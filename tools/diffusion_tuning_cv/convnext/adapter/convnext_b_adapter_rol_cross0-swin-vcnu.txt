[2024-07-29 16:43:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 366): INFO Full config saved to pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/config.json
[2024-07-29 16:43:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: vcnu_swin
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: true
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: lora-style-vcnu-swin-b-finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-29 16:43:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_swin/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/lora-style-vcnu_finetune", "tag": "lora-style-vcnu-swin-b-finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-29 16:43:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 108): INFO Creating model:vcnu_swin/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune
[2024-07-29 16:43:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 110): INFO VCNU_SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (memory_embedding): Memory(
    dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
    (activation): GELU()
    (wm): Linear(in_features=30, out_features=128, bias=True)
    (attn): Linear(in_features=256, out_features=128, bias=True)
    (update_ltm): Linear(in_features=128, out_features=30, bias=True)
    (norm_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (norm_ltm): Identity()
    (proj_drop): Dropout(p=0.0, inplace=False)
  )
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=128, bias=True)
            (attn): Linear(in_features=256, out_features=128, bias=True)
            (update_ltm): Linear(in_features=128, out_features=30, bias=True)
            (norm_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=128, bias=True)
            (attn): Linear(in_features=256, out_features=128, bias=True)
            (update_ltm): Linear(in_features=128, out_features=30, bias=True)
            (norm_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=256, bias=True)
            (attn): Linear(in_features=512, out_features=256, bias=True)
            (update_ltm): Linear(in_features=256, out_features=30, bias=True)
            (norm_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=256, bias=True)
            (attn): Linear(in_features=512, out_features=256, bias=True)
            (update_ltm): Linear(in_features=256, out_features=30, bias=True)
            (norm_attn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=512, bias=True)
            (attn): Linear(in_features=1024, out_features=512, bias=True)
            (update_ltm): Linear(in_features=512, out_features=30, bias=True)
            (norm_attn): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=1024, bias=True)
            (attn): Linear(in_features=2048, out_features=1024, bias=True)
            (update_ltm): Linear(in_features=1024, out_features=30, bias=True)
            (norm_attn): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (t2d_memory_attention): Memory(
            dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=True, ab_norm_ltm_=False,
            (activation): GELU()
            (wm): Linear(in_features=30, out_features=1024, bias=True)
            (attn): Linear(in_features=2048, out_features=1024, bias=True)
            (update_ltm): Identity()
            (norm_attn): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
            (norm_ltm): Identity()
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-29 16:43:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 113): INFO number of params: 102532720
[2024-07-29 16:43:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-29 16:43:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 150): INFO no checkpoint found in pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune, ignoring auto resume
[2024-07-29 16:43:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-07-29 16:43:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-29 16:43:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['memory_embedding.wm.weight', 'memory_embedding.wm.bias', 'memory_embedding.attn.weight', 'memory_embedding.attn.bias', 'memory_embedding.update_ltm.weight', 'memory_embedding.update_ltm.bias', 'memory_embedding.norm_attn.weight', 'memory_embedding.norm_attn.bias', 'layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.t2d_memory_attention.wm.weight', 'layers.0.blocks.0.t2d_memory_attention.wm.bias', 'layers.0.blocks.0.t2d_memory_attention.attn.weight', 'layers.0.blocks.0.t2d_memory_attention.attn.bias', 'layers.0.blocks.0.t2d_memory_attention.update_ltm.weight', 'layers.0.blocks.0.t2d_memory_attention.update_ltm.bias', 'layers.0.blocks.0.t2d_memory_attention.norm_attn.weight', 'layers.0.blocks.0.t2d_memory_attention.norm_attn.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.t2d_memory_attention.wm.weight', 'layers.0.blocks.1.t2d_memory_attention.wm.bias', 'layers.0.blocks.1.t2d_memory_attention.attn.weight', 'layers.0.blocks.1.t2d_memory_attention.attn.bias', 'layers.0.blocks.1.t2d_memory_attention.update_ltm.weight', 'layers.0.blocks.1.t2d_memory_attention.update_ltm.bias', 'layers.0.blocks.1.t2d_memory_attention.norm_attn.weight', 'layers.0.blocks.1.t2d_memory_attention.norm_attn.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.t2d_memory_attention.wm.weight', 'layers.1.blocks.0.t2d_memory_attention.wm.bias', 'layers.1.blocks.0.t2d_memory_attention.attn.weight', 'layers.1.blocks.0.t2d_memory_attention.attn.bias', 'layers.1.blocks.0.t2d_memory_attention.update_ltm.weight', 'layers.1.blocks.0.t2d_memory_attention.update_ltm.bias', 'layers.1.blocks.0.t2d_memory_attention.norm_attn.weight', 'layers.1.blocks.0.t2d_memory_attention.norm_attn.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.t2d_memory_attention.wm.weight', 'layers.1.blocks.1.t2d_memory_attention.wm.bias', 'layers.1.blocks.1.t2d_memory_attention.attn.weight', 'layers.1.blocks.1.t2d_memory_attention.attn.bias', 'layers.1.blocks.1.t2d_memory_attention.update_ltm.weight', 'layers.1.blocks.1.t2d_memory_attention.update_ltm.bias', 'layers.1.blocks.1.t2d_memory_attention.norm_attn.weight', 'layers.1.blocks.1.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.t2d_memory_attention.wm.weight', 'layers.2.blocks.0.t2d_memory_attention.wm.bias', 'layers.2.blocks.0.t2d_memory_attention.attn.weight', 'layers.2.blocks.0.t2d_memory_attention.attn.bias', 'layers.2.blocks.0.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.0.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.0.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.0.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.t2d_memory_attention.wm.weight', 'layers.2.blocks.1.t2d_memory_attention.wm.bias', 'layers.2.blocks.1.t2d_memory_attention.attn.weight', 'layers.2.blocks.1.t2d_memory_attention.attn.bias', 'layers.2.blocks.1.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.1.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.1.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.1.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.t2d_memory_attention.wm.weight', 'layers.2.blocks.2.t2d_memory_attention.wm.bias', 'layers.2.blocks.2.t2d_memory_attention.attn.weight', 'layers.2.blocks.2.t2d_memory_attention.attn.bias', 'layers.2.blocks.2.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.2.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.2.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.2.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.t2d_memory_attention.wm.weight', 'layers.2.blocks.3.t2d_memory_attention.wm.bias', 'layers.2.blocks.3.t2d_memory_attention.attn.weight', 'layers.2.blocks.3.t2d_memory_attention.attn.bias', 'layers.2.blocks.3.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.3.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.3.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.3.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.t2d_memory_attention.wm.weight', 'layers.2.blocks.4.t2d_memory_attention.wm.bias', 'layers.2.blocks.4.t2d_memory_attention.attn.weight', 'layers.2.blocks.4.t2d_memory_attention.attn.bias', 'layers.2.blocks.4.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.4.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.4.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.4.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.t2d_memory_attention.wm.weight', 'layers.2.blocks.5.t2d_memory_attention.wm.bias', 'layers.2.blocks.5.t2d_memory_attention.attn.weight', 'layers.2.blocks.5.t2d_memory_attention.attn.bias', 'layers.2.blocks.5.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.5.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.5.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.5.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.t2d_memory_attention.wm.weight', 'layers.2.blocks.6.t2d_memory_attention.wm.bias', 'layers.2.blocks.6.t2d_memory_attention.attn.weight', 'layers.2.blocks.6.t2d_memory_attention.attn.bias', 'layers.2.blocks.6.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.6.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.6.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.6.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.t2d_memory_attention.wm.weight', 'layers.2.blocks.7.t2d_memory_attention.wm.bias', 'layers.2.blocks.7.t2d_memory_attention.attn.weight', 'layers.2.blocks.7.t2d_memory_attention.attn.bias', 'layers.2.blocks.7.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.7.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.7.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.7.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.t2d_memory_attention.wm.weight', 'layers.2.blocks.8.t2d_memory_attention.wm.bias', 'layers.2.blocks.8.t2d_memory_attention.attn.weight', 'layers.2.blocks.8.t2d_memory_attention.attn.bias', 'layers.2.blocks.8.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.8.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.8.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.8.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.t2d_memory_attention.wm.weight', 'layers.2.blocks.9.t2d_memory_attention.wm.bias', 'layers.2.blocks.9.t2d_memory_attention.attn.weight', 'layers.2.blocks.9.t2d_memory_attention.attn.bias', 'layers.2.blocks.9.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.9.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.9.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.9.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.t2d_memory_attention.wm.weight', 'layers.2.blocks.10.t2d_memory_attention.wm.bias', 'layers.2.blocks.10.t2d_memory_attention.attn.weight', 'layers.2.blocks.10.t2d_memory_attention.attn.bias', 'layers.2.blocks.10.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.10.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.10.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.10.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.t2d_memory_attention.wm.weight', 'layers.2.blocks.11.t2d_memory_attention.wm.bias', 'layers.2.blocks.11.t2d_memory_attention.attn.weight', 'layers.2.blocks.11.t2d_memory_attention.attn.bias', 'layers.2.blocks.11.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.11.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.11.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.11.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.t2d_memory_attention.wm.weight', 'layers.2.blocks.12.t2d_memory_attention.wm.bias', 'layers.2.blocks.12.t2d_memory_attention.attn.weight', 'layers.2.blocks.12.t2d_memory_attention.attn.bias', 'layers.2.blocks.12.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.12.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.12.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.12.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.t2d_memory_attention.wm.weight', 'layers.2.blocks.13.t2d_memory_attention.wm.bias', 'layers.2.blocks.13.t2d_memory_attention.attn.weight', 'layers.2.blocks.13.t2d_memory_attention.attn.bias', 'layers.2.blocks.13.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.13.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.13.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.13.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.t2d_memory_attention.wm.weight', 'layers.2.blocks.14.t2d_memory_attention.wm.bias', 'layers.2.blocks.14.t2d_memory_attention.attn.weight', 'layers.2.blocks.14.t2d_memory_attention.attn.bias', 'layers.2.blocks.14.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.14.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.14.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.14.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.t2d_memory_attention.wm.weight', 'layers.2.blocks.15.t2d_memory_attention.wm.bias', 'layers.2.blocks.15.t2d_memory_attention.attn.weight', 'layers.2.blocks.15.t2d_memory_attention.attn.bias', 'layers.2.blocks.15.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.15.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.15.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.15.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.t2d_memory_attention.wm.weight', 'layers.2.blocks.16.t2d_memory_attention.wm.bias', 'layers.2.blocks.16.t2d_memory_attention.attn.weight', 'layers.2.blocks.16.t2d_memory_attention.attn.bias', 'layers.2.blocks.16.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.16.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.16.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.16.t2d_memory_attention.norm_attn.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.t2d_memory_attention.wm.weight', 'layers.2.blocks.17.t2d_memory_attention.wm.bias', 'layers.2.blocks.17.t2d_memory_attention.attn.weight', 'layers.2.blocks.17.t2d_memory_attention.attn.bias', 'layers.2.blocks.17.t2d_memory_attention.update_ltm.weight', 'layers.2.blocks.17.t2d_memory_attention.update_ltm.bias', 'layers.2.blocks.17.t2d_memory_attention.norm_attn.weight', 'layers.2.blocks.17.t2d_memory_attention.norm_attn.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.t2d_memory_attention.wm.weight', 'layers.3.blocks.0.t2d_memory_attention.wm.bias', 'layers.3.blocks.0.t2d_memory_attention.attn.weight', 'layers.3.blocks.0.t2d_memory_attention.attn.bias', 'layers.3.blocks.0.t2d_memory_attention.update_ltm.weight', 'layers.3.blocks.0.t2d_memory_attention.update_ltm.bias', 'layers.3.blocks.0.t2d_memory_attention.norm_attn.weight', 'layers.3.blocks.0.t2d_memory_attention.norm_attn.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.t2d_memory_attention.wm.weight', 'layers.3.blocks.1.t2d_memory_attention.wm.bias', 'layers.3.blocks.1.t2d_memory_attention.attn.weight', 'layers.3.blocks.1.t2d_memory_attention.attn.bias', 'layers.3.blocks.1.t2d_memory_attention.norm_attn.weight', 'layers.3.blocks.1.t2d_memory_attention.norm_attn.bias'], unexpected_keys=[])
[2024-07-29 16:43:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth'
[2024-07-29 16:43:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.790 (12.790)	Loss 2.5234 (2.5234)	Acc@1 48.438 (48.438)	Acc@5 77.734 (77.734)	Mem 2025MB
[2024-07-29 16:43:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 34.340 Acc@5 55.210
[2024-07-29 16:43:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 34.3%
[2024-07-29 16:43:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 168): INFO Start training
[2024-07-29 16:44:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:25:32 lr 0.000000	 wd 0.0000	time 12.1235 (12.1235)	loss 3.5505 (3.5505)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 16272MB
[2024-07-29 16:45:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:30:34 lr 0.000000	 wd 0.0000	time 0.6206 (0.7639)	loss 3.1475 (3.2881)	grad_norm 28.4434 (inf)	loss_scale 16384.0000 (17195.0891)	mem 17424MB
[2024-07-29 16:46:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:27:07 lr 0.000001	 wd 0.0000	time 0.6245 (0.7069)	loss 2.4731 (3.0403)	grad_norm 17.6184 (inf)	loss_scale 16384.0000 (16791.5622)	mem 17424MB
[2024-07-29 16:47:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:25:14 lr 0.000001	 wd 0.0000	time 0.6229 (0.6879)	loss 2.1690 (2.7919)	grad_norm 9.9764 (inf)	loss_scale 8192.0000 (15621.9535)	mem 17424MB
[2024-07-29 16:48:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:23:45 lr 0.000001	 wd 0.0000	time 0.6154 (0.6782)	loss 2.4538 (2.6250)	grad_norm 6.9085 (inf)	loss_scale 8192.0000 (13769.0973)	mem 17424MB
[2024-07-29 16:49:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:22:26 lr 0.000002	 wd 0.0000	time 0.6245 (0.6725)	loss 2.0997 (2.4945)	grad_norm 6.8761 (inf)	loss_scale 8192.0000 (12655.9042)	mem 17424MB
[2024-07-29 16:50:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:21:12 lr 0.000002	 wd 0.0000	time 0.6290 (0.6690)	loss 1.1838 (2.3974)	grad_norm 6.6147 (inf)	loss_scale 8192.0000 (11913.1581)	mem 17424MB
[2024-07-29 16:51:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:20:00 lr 0.000002	 wd 0.0000	time 0.6255 (0.6664)	loss 1.9838 (2.3122)	grad_norm 6.2364 (inf)	loss_scale 8192.0000 (11382.3224)	mem 17424MB
[2024-07-29 16:52:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:18:51 lr 0.000003	 wd 0.0000	time 0.6272 (0.6645)	loss 2.2264 (2.2479)	grad_norm 4.9200 (inf)	loss_scale 8192.0000 (10984.0300)	mem 17424MB
[2024-07-29 16:53:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:17:42 lr 0.000003	 wd 0.0000	time 0.6167 (0.6630)	loss 1.8357 (2.1865)	grad_norm 5.6323 (inf)	loss_scale 8192.0000 (10674.1487)	mem 17424MB
[2024-07-29 16:55:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:16:34 lr 0.000003	 wd 0.0000	time 0.6165 (0.6618)	loss 1.7567 (2.1417)	grad_norm 5.8674 (inf)	loss_scale 8192.0000 (10426.1818)	mem 17424MB
[2024-07-29 16:56:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:15:26 lr 0.000004	 wd 0.0000	time 0.6162 (0.6608)	loss 1.8864 (2.1021)	grad_norm 4.6075 (inf)	loss_scale 8192.0000 (10223.2589)	mem 17424MB
[2024-07-29 16:57:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:14:19 lr 0.000004	 wd 0.0000	time 0.6178 (0.6600)	loss 1.8480 (2.0714)	grad_norm 7.5495 (inf)	loss_scale 8192.0000 (10054.1282)	mem 17424MB
[2024-07-29 16:58:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:13:12 lr 0.000004	 wd 0.0000	time 0.6212 (0.6594)	loss 1.9563 (2.0430)	grad_norm 6.0115 (inf)	loss_scale 8192.0000 (9910.9977)	mem 17424MB
[2024-07-29 16:59:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:12:05 lr 0.000005	 wd 0.0000	time 0.6237 (0.6588)	loss 1.8380 (2.0169)	grad_norm 7.4734 (inf)	loss_scale 8192.0000 (9788.2998)	mem 17424MB
[2024-07-29 17:00:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:10:59 lr 0.000005	 wd 0.0000	time 0.6223 (0.6583)	loss 1.7189 (1.9924)	grad_norm 8.8431 (inf)	loss_scale 4096.0000 (9583.7122)	mem 17424MB
[2024-07-29 17:01:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:09:53 lr 0.000005	 wd 0.0000	time 0.6539 (0.6579)	loss 1.8422 (1.9715)	grad_norm 5.2306 (inf)	loss_scale 4096.0000 (9240.9444)	mem 17424MB
[2024-07-29 17:02:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:08:47 lr 0.000005	 wd 0.0000	time 0.6232 (0.6576)	loss 1.8048 (1.9508)	grad_norm 6.1355 (inf)	loss_scale 4096.0000 (8938.4785)	mem 17424MB
[2024-07-29 17:03:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:07:41 lr 0.000006	 wd 0.0000	time 0.6209 (0.6573)	loss 1.3819 (1.9327)	grad_norm 6.9074 (inf)	loss_scale 4096.0000 (8669.6013)	mem 17424MB
[2024-07-29 17:04:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:06:35 lr 0.000006	 wd 0.0000	time 0.6237 (0.6570)	loss 1.9139 (1.9149)	grad_norm 6.7791 (inf)	loss_scale 4096.0000 (8429.0121)	mem 17424MB
[2024-07-29 17:05:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:05:29 lr 0.000006	 wd 0.0000	time 0.6150 (0.6567)	loss 1.7522 (1.8965)	grad_norm 5.5349 (inf)	loss_scale 4096.0000 (8212.4698)	mem 17424MB
[2024-07-29 17:06:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:04:23 lr 0.000007	 wd 0.0000	time 0.6266 (0.6564)	loss 1.5542 (1.8832)	grad_norm 5.2806 (inf)	loss_scale 4096.0000 (8016.5407)	mem 17424MB
[2024-07-29 17:08:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:03:18 lr 0.000007	 wd 0.0000	time 0.6204 (0.6562)	loss 1.8354 (1.8696)	grad_norm 6.7474 (inf)	loss_scale 4096.0000 (7838.4153)	mem 17424MB
[2024-07-29 17:09:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:02:12 lr 0.000007	 wd 0.0000	time 0.6264 (0.6560)	loss 1.6537 (1.8554)	grad_norm 5.7780 (inf)	loss_scale 4096.0000 (7675.7723)	mem 17424MB
[2024-07-29 17:10:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:06 lr 0.000008	 wd 0.0000	time 0.6261 (0.6558)	loss 1.6162 (1.8434)	grad_norm 5.7993 (inf)	loss_scale 4096.0000 (7526.6772)	mem 17424MB
[2024-07-29 17:11:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0000	time 0.6222 (0.6555)	loss 1.8586 (1.8321)	grad_norm 5.5604 (inf)	loss_scale 4096.0000 (7389.5050)	mem 17424MB
[2024-07-29 17:11:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 0 training takes 0:27:22
[2024-07-29 17:11:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 145): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_0.pth saving......
[2024-07-29 17:11:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 147): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_0.pth saved !!!
[2024-07-29 17:11:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 9.883 (9.883)	Loss 0.4836 (0.4836)	Acc@1 91.211 (91.211)	Acc@5 97.656 (97.656)	Mem 17424MB
[2024-07-29 17:11:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 81.846 Acc@5 96.208
[2024-07-29 17:11:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 81.8%
[2024-07-29 17:11:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 81.85%
[2024-07-29 17:11:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 17:11:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 17:12:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:51:19 lr 0.000008	 wd 0.0000	time 11.3027 (11.3027)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:13:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:30:14 lr 0.000008	 wd 0.0000	time 0.6214 (0.7556)	loss 1.3296 (1.5830)	grad_norm 7.3583 (5.9339)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:14:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:26:58 lr 0.000009	 wd 0.0000	time 0.6160 (0.7030)	loss 1.5452 (1.5770)	grad_norm 5.8642 (5.8210)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:15:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:25:08 lr 0.000009	 wd 0.0000	time 0.6215 (0.6851)	loss 1.8635 (1.5584)	grad_norm 6.0922 (6.0622)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:16:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:23:41 lr 0.000009	 wd 0.0000	time 0.6190 (0.6764)	loss 1.0110 (1.5414)	grad_norm 6.2367 (6.0795)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:17:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:22:23 lr 0.000010	 wd 0.0000	time 0.6215 (0.6711)	loss 1.7396 (1.5420)	grad_norm 4.9565 (6.0656)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:18:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:21:09 lr 0.000010	 wd 0.0000	time 0.6240 (0.6675)	loss 1.4703 (1.5361)	grad_norm 9.8735 (6.0187)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:19:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:19:58 lr 0.000010	 wd 0.0000	time 0.6189 (0.6652)	loss 1.7641 (1.5309)	grad_norm 5.7041 (5.9726)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:20:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:18:49 lr 0.000011	 wd 0.0000	time 0.6498 (0.6635)	loss 1.7708 (1.5318)	grad_norm 4.9195 (5.9781)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:21:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:17:40 lr 0.000011	 wd 0.0000	time 0.6326 (0.6619)	loss 1.6588 (1.5264)	grad_norm 5.4524 (5.9888)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:22:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:16:32 lr 0.000011	 wd 0.0000	time 0.6201 (0.6607)	loss 1.6336 (1.5211)	grad_norm 5.2736 (6.0158)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:23:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:15:24 lr 0.000012	 wd 0.0000	time 0.6249 (0.6597)	loss 1.0959 (1.5185)	grad_norm 5.3540 (6.0233)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:25:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:14:17 lr 0.000012	 wd 0.0000	time 0.6246 (0.6590)	loss 1.5370 (1.5193)	grad_norm 5.1875 (6.0303)	loss_scale 4096.0000 (4096.0000)	mem 17424MB
[2024-07-29 17:26:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:13:11 lr 0.000012	 wd 0.0000	time 0.6208 (0.6583)	loss 1.6050 (1.5202)	grad_norm 6.3528 (inf)	loss_scale 2048.0000 (3966.9178)	mem 17424MB
[2024-07-29 17:27:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:12:04 lr 0.000012	 wd 0.0000	time 0.6211 (0.6578)	loss 1.6720 (1.5146)	grad_norm 5.6297 (inf)	loss_scale 2048.0000 (3829.9500)	mem 17424MB
[2024-07-29 17:28:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:10:58 lr 0.000013	 wd 0.0000	time 0.6169 (0.6573)	loss 1.0226 (1.5115)	grad_norm 5.8929 (inf)	loss_scale 2048.0000 (3711.2325)	mem 17424MB
[2024-07-29 17:29:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:09:52 lr 0.000013	 wd 0.0000	time 0.6186 (0.6569)	loss 1.0366 (1.5075)	grad_norm 5.7364 (inf)	loss_scale 2048.0000 (3607.3454)	mem 17424MB
[2024-07-29 17:30:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:08:46 lr 0.000013	 wd 0.0000	time 0.6328 (0.6565)	loss 1.5123 (1.5055)	grad_norm 6.2566 (inf)	loss_scale 2048.0000 (3515.6731)	mem 17424MB
[2024-07-29 17:31:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:07:40 lr 0.000014	 wd 0.0000	time 0.6172 (0.6562)	loss 1.4278 (1.5029)	grad_norm 6.7608 (inf)	loss_scale 2048.0000 (3434.1810)	mem 17424MB
[2024-07-29 17:32:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:06:34 lr 0.000014	 wd 0.0000	time 0.6213 (0.6559)	loss 1.5407 (1.5022)	grad_norm 6.6443 (inf)	loss_scale 2048.0000 (3361.2625)	mem 17424MB
[2024-07-29 17:33:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:05:29 lr 0.000014	 wd 0.0000	time 0.6200 (0.6557)	loss 1.5206 (1.4998)	grad_norm 5.5324 (inf)	loss_scale 2048.0000 (3295.6322)	mem 17424MB
[2024-07-29 17:34:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:04:23 lr 0.000015	 wd 0.0000	time 0.6242 (0.6555)	loss 1.5368 (1.4990)	grad_norm 6.2331 (inf)	loss_scale 2048.0000 (3236.2494)	mem 17424MB
[2024-07-29 17:35:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:03:17 lr 0.000015	 wd 0.0000	time 0.6210 (0.6553)	loss 1.0672 (1.4988)	grad_norm 5.8610 (inf)	loss_scale 2048.0000 (3182.2626)	mem 17424MB
[2024-07-29 17:36:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:02:12 lr 0.000015	 wd 0.0000	time 0.6211 (0.6551)	loss 1.5644 (1.4977)	grad_norm 5.9750 (inf)	loss_scale 2048.0000 (3132.9683)	mem 17424MB
[2024-07-29 17:38:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:06 lr 0.000016	 wd 0.0000	time 0.6112 (0.6549)	loss 1.5108 (1.4943)	grad_norm 6.7968 (inf)	loss_scale 2048.0000 (3087.7801)	mem 17424MB
[2024-07-29 17:39:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0000	time 0.6438 (0.6546)	loss 1.1499 (1.4940)	grad_norm 17.6072 (inf)	loss_scale 2048.0000 (3046.2055)	mem 17424MB
[2024-07-29 17:39:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 1 training takes 0:27:20
[2024-07-29 17:39:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.908 (11.908)	Loss 0.4768 (0.4768)	Acc@1 91.016 (91.016)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 17:39:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 82.892 Acc@5 96.782
[2024-07-29 17:39:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 82.9%
[2024-07-29 17:39:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 82.89%
[2024-07-29 17:39:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 17:39:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 17:39:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:10:07 lr 0.000016	 wd 0.0000	time 10.3147 (10.3147)	loss 1.6191 (1.6191)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:40:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:29:48 lr 0.000016	 wd 0.0000	time 0.6241 (0.7448)	loss 1.5647 (1.4268)	grad_norm 5.6709 (6.5404)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:42:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:26:45 lr 0.000017	 wd 0.0000	time 0.6203 (0.6975)	loss 1.4822 (1.4377)	grad_norm 5.8040 (6.5604)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:43:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:25:00 lr 0.000017	 wd 0.0000	time 0.6219 (0.6816)	loss 1.3417 (1.4502)	grad_norm 6.4262 (6.6969)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:44:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:23:35 lr 0.000017	 wd 0.0000	time 0.6275 (0.6736)	loss 1.5633 (1.4483)	grad_norm 7.1773 (6.5647)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:45:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:22:19 lr 0.000018	 wd 0.0000	time 0.6262 (0.6689)	loss 1.6943 (1.4497)	grad_norm 5.2638 (6.4761)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:46:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:21:06 lr 0.000018	 wd 0.0000	time 0.6216 (0.6660)	loss 1.1859 (1.4407)	grad_norm 5.4349 (6.4704)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:47:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:19:55 lr 0.000018	 wd 0.0000	time 0.6195 (0.6637)	loss 1.4292 (1.4430)	grad_norm 5.5013 (6.3989)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:48:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:18:46 lr 0.000019	 wd 0.0000	time 0.6228 (0.6620)	loss 1.5836 (1.4420)	grad_norm 5.9144 (6.3870)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:49:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:17:38 lr 0.000019	 wd 0.0000	time 0.6298 (0.6607)	loss 1.5132 (1.4470)	grad_norm 7.6046 (6.4333)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:50:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:16:30 lr 0.000019	 wd 0.0000	time 0.6164 (0.6597)	loss 1.5755 (1.4494)	grad_norm 6.8143 (6.5242)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:51:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:15:23 lr 0.000020	 wd 0.0000	time 0.6197 (0.6589)	loss 1.3678 (1.4484)	grad_norm 4.7549 (6.5532)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:52:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:14:17 lr 0.000020	 wd 0.0000	time 0.6125 (0.6582)	loss 1.3200 (1.4473)	grad_norm 6.3135 (6.6294)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:53:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:13:10 lr 0.000020	 wd 0.0000	time 0.6244 (0.6576)	loss 1.7487 (1.4494)	grad_norm 5.4879 (6.6672)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:55:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:12:04 lr 0.000020	 wd 0.0000	time 0.6192 (0.6570)	loss 1.4789 (1.4490)	grad_norm 5.7101 (6.6263)	loss_scale 2048.0000 (2048.0000)	mem 17424MB
[2024-07-29 17:56:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:10:57 lr 0.000021	 wd 0.0000	time 0.6126 (0.6566)	loss 1.6576 (1.4455)	grad_norm 4.5849 (inf)	loss_scale 1024.0000 (2000.2452)	mem 17424MB
[2024-07-29 17:57:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:09:52 lr 0.000021	 wd 0.0000	time 0.6207 (0.6564)	loss 1.4316 (1.4449)	grad_norm 7.1649 (inf)	loss_scale 1024.0000 (1939.2680)	mem 17424MB
[2024-07-29 17:58:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:08:46 lr 0.000021	 wd 0.0000	time 0.6370 (0.6560)	loss 1.4843 (1.4435)	grad_norm 6.0551 (inf)	loss_scale 1024.0000 (1885.4603)	mem 17424MB
[2024-07-29 17:59:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:07:40 lr 0.000022	 wd 0.0000	time 0.6518 (0.6557)	loss 1.4172 (1.4428)	grad_norm 6.8136 (inf)	loss_scale 1024.0000 (1837.6280)	mem 17424MB
[2024-07-29 18:00:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:06:34 lr 0.000022	 wd 0.0000	time 0.6207 (0.6555)	loss 1.0752 (1.4410)	grad_norm 5.3640 (inf)	loss_scale 1024.0000 (1794.8280)	mem 17424MB
[2024-07-29 18:01:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:05:28 lr 0.000022	 wd 0.0000	time 0.6210 (0.6552)	loss 1.1964 (1.4383)	grad_norm 14.4399 (inf)	loss_scale 1024.0000 (1756.3058)	mem 17424MB
[2024-07-29 18:02:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:04:23 lr 0.000023	 wd 0.0000	time 0.6202 (0.6550)	loss 1.4062 (1.4377)	grad_norm 5.3264 (inf)	loss_scale 1024.0000 (1721.4507)	mem 17424MB
[2024-07-29 18:03:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:03:17 lr 0.000023	 wd 0.0000	time 0.6210 (0.6548)	loss 1.6642 (1.4372)	grad_norm 5.8308 (inf)	loss_scale 1024.0000 (1689.7628)	mem 17424MB
[2024-07-29 18:04:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:02:12 lr 0.000023	 wd 0.0000	time 0.6219 (0.6547)	loss 1.5770 (1.4367)	grad_norm 6.4308 (inf)	loss_scale 1024.0000 (1660.8292)	mem 17424MB
[2024-07-29 18:05:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:06 lr 0.000024	 wd 0.0000	time 0.6172 (0.6545)	loss 1.5484 (1.4359)	grad_norm 4.9057 (inf)	loss_scale 1024.0000 (1634.3057)	mem 17424MB
[2024-07-29 18:06:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000024	 wd 0.0000	time 0.6266 (0.6542)	loss 1.7061 (1.4358)	grad_norm 5.6870 (inf)	loss_scale 1024.0000 (1609.9032)	mem 17424MB
[2024-07-29 18:07:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 2 training takes 0:27:19
[2024-07-29 18:07:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.134 (11.134)	Loss 0.4773 (0.4773)	Acc@1 90.820 (90.820)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-29 18:07:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 83.216 Acc@5 96.978
[2024-07-29 18:07:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.2%
[2024-07-29 18:07:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 83.22%
[2024-07-29 18:07:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 18:07:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 18:07:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:02:05 lr 0.000024	 wd 0.0000	time 10.1220 (10.1220)	loss 1.0214 (1.0214)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:08:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:29:44 lr 0.000024	 wd 0.0000	time 0.6108 (0.7431)	loss 1.6987 (1.4130)	grad_norm 7.6401 (6.7188)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:09:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:26:43 lr 0.000025	 wd 0.0000	time 0.6252 (0.6965)	loss 1.4363 (1.4196)	grad_norm 4.5434 (6.6810)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:10:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:24:58 lr 0.000025	 wd 0.0000	time 0.6220 (0.6807)	loss 1.5364 (1.4078)	grad_norm 4.1773 (6.5774)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:12:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:23:34 lr 0.000025	 wd 0.0000	time 0.6232 (0.6731)	loss 1.6671 (1.4165)	grad_norm 15.0689 (6.6608)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:13:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:22:18 lr 0.000026	 wd 0.0000	time 0.6270 (0.6686)	loss 1.3754 (1.4129)	grad_norm 6.9690 (6.6502)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:14:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:21:05 lr 0.000026	 wd 0.0000	time 0.6084 (0.6655)	loss 1.5198 (1.4105)	grad_norm 9.6079 (6.6835)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:15:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:19:55 lr 0.000026	 wd 0.0000	time 0.6261 (0.6632)	loss 1.6134 (1.4129)	grad_norm 7.0740 (6.7374)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:16:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:18:45 lr 0.000027	 wd 0.0000	time 0.6203 (0.6614)	loss 1.1631 (1.4114)	grad_norm 7.2608 (6.8354)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:17:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:17:37 lr 0.000027	 wd 0.0000	time 0.6204 (0.6601)	loss 1.5112 (1.4128)	grad_norm 7.6600 (6.8233)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:18:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:16:29 lr 0.000027	 wd 0.0000	time 0.6176 (0.6591)	loss 1.5183 (1.4140)	grad_norm 7.2593 (6.8094)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:19:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:15:22 lr 0.000028	 wd 0.0000	time 0.6228 (0.6582)	loss 1.4033 (1.4165)	grad_norm 6.5191 (6.7780)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:20:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:14:16 lr 0.000028	 wd 0.0000	time 0.6157 (0.6577)	loss 1.1870 (1.4147)	grad_norm 4.1873 (6.7858)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:21:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:13:09 lr 0.000028	 wd 0.0000	time 0.6241 (0.6571)	loss 1.2580 (1.4168)	grad_norm 5.3033 (6.7446)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:22:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:12:03 lr 0.000028	 wd 0.0000	time 0.6193 (0.6566)	loss 1.3403 (1.4168)	grad_norm 5.5754 (6.7775)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:23:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:10:57 lr 0.000029	 wd 0.0000	time 0.6200 (0.6562)	loss 1.5726 (1.4184)	grad_norm 5.6474 (6.7547)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:25:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:09:51 lr 0.000029	 wd 0.0000	time 0.6184 (0.6559)	loss 1.4351 (1.4165)	grad_norm 5.0757 (6.7679)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:26:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:08:45 lr 0.000029	 wd 0.0000	time 0.6258 (0.6556)	loss 1.1203 (1.4151)	grad_norm 6.2464 (6.7833)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:27:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:07:40 lr 0.000030	 wd 0.0000	time 0.6169 (0.6554)	loss 1.6661 (1.4151)	grad_norm 4.8546 (6.7336)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:28:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:06:34 lr 0.000030	 wd 0.0000	time 0.6240 (0.6551)	loss 1.5508 (1.4136)	grad_norm 5.5088 (6.7166)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:29:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:05:28 lr 0.000030	 wd 0.0000	time 0.6036 (0.6549)	loss 1.1386 (1.4142)	grad_norm 7.6415 (6.7140)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:30:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:04:23 lr 0.000031	 wd 0.0000	time 0.6249 (0.6547)	loss 1.2629 (1.4128)	grad_norm 4.9904 (6.6786)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:31:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:03:17 lr 0.000031	 wd 0.0000	time 0.6196 (0.6545)	loss 1.4384 (1.4142)	grad_norm 5.8860 (6.6410)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:32:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:02:12 lr 0.000031	 wd 0.0000	time 0.6180 (0.6543)	loss 1.5819 (1.4144)	grad_norm 5.5798 (6.6108)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:33:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:06 lr 0.000032	 wd 0.0000	time 0.6214 (0.6542)	loss 0.9485 (1.4137)	grad_norm 5.7338 (6.6342)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:34:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000032	 wd 0.0000	time 0.6217 (0.6540)	loss 1.4079 (1.4139)	grad_norm 6.0576 (6.6037)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:34:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 3 training takes 0:27:18
[2024-07-29 18:35:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.039 (11.039)	Loss 0.4421 (0.4421)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 18:35:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 83.388 Acc@5 97.080
[2024-07-29 18:35:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.4%
[2024-07-29 18:35:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 83.39%
[2024-07-29 18:35:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 18:35:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 18:35:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:26:21 lr 0.000032	 wd 0.0000	time 10.7039 (10.7039)	loss 1.4738 (1.4738)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:36:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:30:01 lr 0.000032	 wd 0.0000	time 0.6250 (0.7500)	loss 1.1334 (1.4219)	grad_norm 6.4682 (7.2726)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:37:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:26:52 lr 0.000033	 wd 0.0000	time 0.6215 (0.7003)	loss 1.0724 (1.4242)	grad_norm 8.1191 (6.8122)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:38:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:25:04 lr 0.000033	 wd 0.0000	time 0.6207 (0.6834)	loss 1.1121 (1.4220)	grad_norm 7.1237 (6.6080)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:39:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:23:38 lr 0.000033	 wd 0.0000	time 0.6117 (0.6750)	loss 1.4904 (1.4142)	grad_norm 5.6739 (6.6487)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 18:40:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:22:21 lr 0.000034	 wd 0.0000	time 0.6233 (0.6699)	loss 1.4284 (1.4082)	grad_norm 5.8799 (6.5254)	loss_scale 2048.0000 (1175.2495)	mem 17424MB
[2024-07-29 18:42:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:21:07 lr 0.000034	 wd 0.0000	time 0.6196 (0.6666)	loss 1.6052 (1.4037)	grad_norm 6.5950 (6.5056)	loss_scale 2048.0000 (1320.4659)	mem 17424MB
[2024-07-29 18:43:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:19:57 lr 0.000034	 wd 0.0000	time 0.6273 (0.6643)	loss 1.4233 (1.4055)	grad_norm 5.4702 (6.4371)	loss_scale 2048.0000 (1424.2511)	mem 17424MB
[2024-07-29 18:44:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:18:47 lr 0.000035	 wd 0.0000	time 0.6250 (0.6626)	loss 1.4387 (1.4037)	grad_norm 15.5222 (6.4353)	loss_scale 2048.0000 (1502.1223)	mem 17424MB
[2024-07-29 18:45:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:17:39 lr 0.000035	 wd 0.0000	time 0.6251 (0.6611)	loss 1.6145 (1.4051)	grad_norm 10.7164 (6.4707)	loss_scale 2048.0000 (1562.7081)	mem 17424MB
[2024-07-29 18:46:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:16:31 lr 0.000035	 wd 0.0000	time 0.6211 (0.6601)	loss 1.6497 (1.4049)	grad_norm 4.9226 (6.4998)	loss_scale 2048.0000 (1611.1888)	mem 17424MB
[2024-07-29 18:47:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:15:24 lr 0.000036	 wd 0.0000	time 0.6140 (0.6592)	loss 1.6033 (1.4051)	grad_norm 4.9363 (6.4568)	loss_scale 2048.0000 (1650.8629)	mem 17424MB
[2024-07-29 18:48:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:14:17 lr 0.000036	 wd 0.0000	time 0.6247 (0.6585)	loss 1.2286 (1.4043)	grad_norm 7.4863 (6.4308)	loss_scale 2048.0000 (1683.9301)	mem 17424MB
[2024-07-29 18:49:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:13:10 lr 0.000036	 wd 0.0000	time 0.6203 (0.6579)	loss 1.5822 (1.4035)	grad_norm 6.5626 (6.3974)	loss_scale 2048.0000 (1711.9139)	mem 17424MB
[2024-07-29 18:50:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:12:04 lr 0.000036	 wd 0.0000	time 0.6147 (0.6574)	loss 1.4879 (1.4048)	grad_norm 6.2658 (6.3879)	loss_scale 2048.0000 (1735.9029)	mem 17424MB
[2024-07-29 18:51:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:10:58 lr 0.000037	 wd 0.0000	time 0.6156 (0.6570)	loss 1.3581 (1.4085)	grad_norm 3.6901 (inf)	loss_scale 1024.0000 (1704.8474)	mem 17424MB
[2024-07-29 18:52:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:09:52 lr 0.000037	 wd 0.0000	time 0.6307 (0.6566)	loss 1.1977 (1.4077)	grad_norm 4.5414 (inf)	loss_scale 1024.0000 (1662.3210)	mem 17424MB
[2024-07-29 18:53:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:08:46 lr 0.000037	 wd 0.0000	time 0.6323 (0.6563)	loss 1.6030 (1.4068)	grad_norm 6.0634 (inf)	loss_scale 1024.0000 (1624.7948)	mem 17424MB
[2024-07-29 18:55:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:07:40 lr 0.000038	 wd 0.0000	time 0.6189 (0.6561)	loss 1.6142 (1.4061)	grad_norm 5.2259 (inf)	loss_scale 1024.0000 (1591.4359)	mem 17424MB
[2024-07-29 18:56:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:06:34 lr 0.000038	 wd 0.0000	time 0.6244 (0.6558)	loss 1.5336 (1.4050)	grad_norm 5.4538 (inf)	loss_scale 1024.0000 (1561.5865)	mem 17424MB
[2024-07-29 18:57:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:05:29 lr 0.000038	 wd 0.0000	time 0.6209 (0.6555)	loss 1.3391 (1.4047)	grad_norm 7.7078 (inf)	loss_scale 1024.0000 (1534.7206)	mem 17424MB
[2024-07-29 18:58:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:04:23 lr 0.000039	 wd 0.0000	time 0.6211 (0.6553)	loss 1.3202 (1.4042)	grad_norm 8.3680 (inf)	loss_scale 1024.0000 (1510.4122)	mem 17424MB
[2024-07-29 18:59:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:03:17 lr 0.000039	 wd 0.0000	time 0.6259 (0.6552)	loss 0.9734 (1.4017)	grad_norm 4.8738 (inf)	loss_scale 1024.0000 (1488.3126)	mem 17424MB
[2024-07-29 19:00:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:02:12 lr 0.000039	 wd 0.0000	time 0.6248 (0.6550)	loss 1.3751 (1.4024)	grad_norm 5.6516 (inf)	loss_scale 1024.0000 (1468.1339)	mem 17424MB
[2024-07-29 19:01:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:06 lr 0.000040	 wd 0.0000	time 0.6299 (0.6549)	loss 1.5138 (1.4019)	grad_norm 4.0727 (inf)	loss_scale 1024.0000 (1449.6360)	mem 17424MB
[2024-07-29 19:02:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000040	 wd 0.0000	time 0.6228 (0.6546)	loss 0.9277 (1.4013)	grad_norm 4.5643 (inf)	loss_scale 1024.0000 (1432.6174)	mem 17424MB
[2024-07-29 19:02:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 4 training takes 0:27:20
[2024-07-29 19:02:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.899 (11.899)	Loss 0.4319 (0.4319)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-29 19:03:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 83.620 Acc@5 97.128
[2024-07-29 19:03:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-07-29 19:03:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 83.62%
[2024-07-29 19:03:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 19:03:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 19:03:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][0/2502]	eta 6:53:25 lr 0.000040	 wd 0.0000	time 9.9141 (9.9141)	loss 1.7331 (1.7331)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:04:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:29:41 lr 0.000040	 wd 0.0000	time 0.6241 (0.7416)	loss 1.3915 (1.4148)	grad_norm 5.9142 (6.2640)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:05:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:26:41 lr 0.000040	 wd 0.0000	time 0.6190 (0.6957)	loss 1.4203 (1.4204)	grad_norm 5.1238 (6.0496)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:06:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:24:58 lr 0.000040	 wd 0.0000	time 0.6172 (0.6803)	loss 1.6603 (1.4092)	grad_norm 4.9114 (5.9225)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:07:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:23:33 lr 0.000040	 wd 0.0000	time 0.6247 (0.6724)	loss 1.1549 (1.4125)	grad_norm 4.4729 (5.8874)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:08:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:22:17 lr 0.000040	 wd 0.0000	time 0.6151 (0.6679)	loss 1.6526 (1.4057)	grad_norm 4.1488 (5.8937)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:09:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:21:05 lr 0.000040	 wd 0.0000	time 0.6246 (0.6652)	loss 1.4985 (1.4083)	grad_norm 4.7222 (5.8727)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:10:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:19:54 lr 0.000040	 wd 0.0000	time 0.6169 (0.6629)	loss 1.5792 (1.4063)	grad_norm 6.9742 (5.8691)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:12:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:18:45 lr 0.000040	 wd 0.0000	time 0.6255 (0.6613)	loss 1.1211 (1.4025)	grad_norm 5.7943 (6.1034)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:13:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:17:37 lr 0.000040	 wd 0.0000	time 0.6188 (0.6600)	loss 0.9449 (1.4004)	grad_norm 7.7413 (6.0163)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:14:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:16:29 lr 0.000040	 wd 0.0000	time 0.6206 (0.6590)	loss 1.6326 (1.4023)	grad_norm 20.5369 (6.0752)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:15:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:15:22 lr 0.000040	 wd 0.0000	time 0.6208 (0.6582)	loss 1.5862 (1.3978)	grad_norm 4.5133 (6.0235)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:16:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:14:16 lr 0.000040	 wd 0.0000	time 0.6226 (0.6575)	loss 1.5825 (1.3954)	grad_norm 4.7197 (6.0020)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:17:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:13:09 lr 0.000040	 wd 0.0000	time 0.6265 (0.6570)	loss 1.4879 (1.3950)	grad_norm 4.8998 (5.9659)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:18:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:12:03 lr 0.000040	 wd 0.0000	time 0.6214 (0.6565)	loss 1.5281 (1.3932)	grad_norm 4.8840 (5.9852)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:19:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:10:57 lr 0.000040	 wd 0.0000	time 0.6227 (0.6561)	loss 1.4059 (1.3930)	grad_norm 4.1018 (5.9569)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:20:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:09:51 lr 0.000040	 wd 0.0000	time 0.6283 (0.6558)	loss 1.4919 (1.3942)	grad_norm 5.1505 (5.9345)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:21:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:08:45 lr 0.000040	 wd 0.0000	time 0.6277 (0.6555)	loss 1.5447 (1.3948)	grad_norm 4.9046 (5.9212)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:22:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:07:39 lr 0.000040	 wd 0.0000	time 0.6265 (0.6552)	loss 1.1444 (1.3935)	grad_norm 5.1193 (5.9088)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:23:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:06:34 lr 0.000040	 wd 0.0000	time 0.6244 (0.6550)	loss 1.3459 (1.3959)	grad_norm 5.5588 (5.9103)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:25:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:05:28 lr 0.000040	 wd 0.0000	time 0.6192 (0.6548)	loss 1.6531 (1.3976)	grad_norm 5.1904 (5.9259)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:26:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:04:23 lr 0.000040	 wd 0.0000	time 0.6265 (0.6546)	loss 1.7569 (1.3981)	grad_norm 4.2611 (5.9261)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:27:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:03:17 lr 0.000040	 wd 0.0000	time 0.6218 (0.6544)	loss 1.3356 (1.3959)	grad_norm 4.3546 (5.9252)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:28:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:02:12 lr 0.000040	 wd 0.0000	time 0.6281 (0.6542)	loss 1.0772 (1.3954)	grad_norm 8.0343 (5.9639)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:29:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:06 lr 0.000040	 wd 0.0000	time 0.6194 (0.6541)	loss 1.6012 (1.3952)	grad_norm 5.0519 (5.9978)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:30:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000040	 wd 0.0000	time 0.6133 (0.6539)	loss 1.4917 (1.3950)	grad_norm 4.9313 (5.9857)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:30:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 5 training takes 0:27:18
[2024-07-29 19:30:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.518 (11.518)	Loss 0.4255 (0.4255)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-29 19:31:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 83.738 Acc@5 97.204
[2024-07-29 19:31:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-29 19:31:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 83.74%
[2024-07-29 19:31:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 19:31:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 19:31:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:51:10 lr 0.000040	 wd 0.0000	time 11.2991 (11.2991)	loss 1.5137 (1.5137)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:32:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:30:13 lr 0.000040	 wd 0.0000	time 0.6276 (0.7551)	loss 1.3043 (1.3717)	grad_norm 4.9267 (5.8543)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:33:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:26:58 lr 0.000040	 wd 0.0000	time 0.6303 (0.7032)	loss 1.4432 (1.3902)	grad_norm 4.1614 (6.1190)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:34:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:25:09 lr 0.000040	 wd 0.0000	time 0.6245 (0.6854)	loss 1.6953 (1.3982)	grad_norm 22.8413 (6.2356)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:35:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:23:41 lr 0.000040	 wd 0.0000	time 0.6200 (0.6765)	loss 1.4439 (1.3916)	grad_norm 6.4762 (6.3455)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 19:36:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:22:23 lr 0.000040	 wd 0.0000	time 0.6180 (0.6712)	loss 1.5099 (1.3878)	grad_norm 4.3152 (6.2042)	loss_scale 2048.0000 (1187.5130)	mem 17424MB
[2024-07-29 19:37:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:21:10 lr 0.000040	 wd 0.0000	time 0.6237 (0.6678)	loss 1.5115 (1.3895)	grad_norm 6.9193 (6.0893)	loss_scale 2048.0000 (1330.6889)	mem 17424MB
[2024-07-29 19:38:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:19:59 lr 0.000040	 wd 0.0000	time 0.6235 (0.6654)	loss 1.3795 (1.3887)	grad_norm 5.1679 (6.0717)	loss_scale 2048.0000 (1433.0157)	mem 17424MB
[2024-07-29 19:39:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:18:49 lr 0.000040	 wd 0.0000	time 0.6258 (0.6636)	loss 1.1235 (1.3867)	grad_norm 5.5153 (6.1333)	loss_scale 2048.0000 (1509.7928)	mem 17424MB
[2024-07-29 19:41:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:17:40 lr 0.000040	 wd 0.0000	time 0.6237 (0.6621)	loss 1.6720 (1.3902)	grad_norm 7.9396 (6.1030)	loss_scale 2048.0000 (1569.5272)	mem 17424MB
[2024-07-29 19:42:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:16:32 lr 0.000040	 wd 0.0000	time 0.6268 (0.6610)	loss 1.3712 (1.3856)	grad_norm 5.3808 (6.0704)	loss_scale 2048.0000 (1617.3267)	mem 17424MB
[2024-07-29 19:43:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:15:25 lr 0.000040	 wd 0.0000	time 0.6206 (0.6600)	loss 1.3286 (1.3841)	grad_norm 4.9367 (nan)	loss_scale 1024.0000 (1585.7584)	mem 17424MB
[2024-07-29 19:44:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:14:18 lr 0.000040	 wd 0.0000	time 0.6234 (0.6592)	loss 1.0975 (1.3851)	grad_norm 6.3418 (nan)	loss_scale 1024.0000 (1538.9842)	mem 17424MB
[2024-07-29 19:45:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:13:11 lr 0.000040	 wd 0.0000	time 0.6563 (0.6587)	loss 1.5391 (1.3839)	grad_norm 6.7591 (nan)	loss_scale 1024.0000 (1499.4005)	mem 17424MB
[2024-07-29 19:46:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:12:05 lr 0.000040	 wd 0.0000	time 0.6205 (0.6581)	loss 1.6815 (1.3823)	grad_norm 5.8870 (nan)	loss_scale 1024.0000 (1465.4675)	mem 17424MB
[2024-07-29 19:47:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:10:58 lr 0.000040	 wd 0.0000	time 0.6176 (0.6576)	loss 1.4016 (1.3830)	grad_norm 4.6501 (nan)	loss_scale 1024.0000 (1436.0560)	mem 17424MB
[2024-07-29 19:48:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:09:52 lr 0.000040	 wd 0.0000	time 0.6267 (0.6571)	loss 1.5212 (1.3816)	grad_norm 6.1011 (nan)	loss_scale 1024.0000 (1410.3186)	mem 17424MB
[2024-07-29 19:49:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:08:46 lr 0.000040	 wd 0.0000	time 0.6130 (0.6568)	loss 1.7175 (1.3814)	grad_norm 6.3232 (nan)	loss_scale 1024.0000 (1387.6073)	mem 17424MB
[2024-07-29 19:50:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:07:40 lr 0.000040	 wd 0.0000	time 0.6133 (0.6565)	loss 1.1956 (1.3815)	grad_norm 5.5565 (nan)	loss_scale 1024.0000 (1367.4181)	mem 17424MB
[2024-07-29 19:51:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:06:35 lr 0.000040	 wd 0.0000	time 0.6258 (0.6562)	loss 1.1145 (1.3812)	grad_norm 5.7259 (nan)	loss_scale 1024.0000 (1349.3530)	mem 17424MB
[2024-07-29 19:52:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:05:29 lr 0.000039	 wd 0.0000	time 0.6216 (0.6560)	loss 1.0589 (1.3810)	grad_norm 5.1776 (nan)	loss_scale 1024.0000 (1333.0935)	mem 17424MB
[2024-07-29 19:54:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:04:23 lr 0.000039	 wd 0.0000	time 0.6307 (0.6557)	loss 1.4228 (1.3804)	grad_norm 5.6516 (nan)	loss_scale 1024.0000 (1318.3817)	mem 17424MB
[2024-07-29 19:55:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:03:17 lr 0.000039	 wd 0.0000	time 0.6086 (0.6556)	loss 1.3674 (1.3807)	grad_norm 4.8611 (nan)	loss_scale 1024.0000 (1305.0068)	mem 17424MB
[2024-07-29 19:56:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:02:12 lr 0.000039	 wd 0.0000	time 0.6306 (0.6554)	loss 1.6821 (1.3808)	grad_norm 4.8391 (nan)	loss_scale 1024.0000 (1292.7944)	mem 17424MB
[2024-07-29 19:57:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:06 lr 0.000039	 wd 0.0000	time 0.6125 (0.6552)	loss 1.2446 (1.3796)	grad_norm 3.9791 (nan)	loss_scale 1024.0000 (1281.5993)	mem 17424MB
[2024-07-29 19:58:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000039	 wd 0.0000	time 0.6179 (0.6549)	loss 1.0480 (1.3800)	grad_norm 5.0513 (nan)	loss_scale 1024.0000 (1271.2995)	mem 17424MB
[2024-07-29 19:58:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 6 training takes 0:27:21
[2024-07-29 19:58:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.716 (11.716)	Loss 0.4397 (0.4397)	Acc@1 92.383 (92.383)	Acc@5 98.047 (98.047)	Mem 17424MB
[2024-07-29 19:58:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 83.866 Acc@5 97.174
[2024-07-29 19:58:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-07-29 19:58:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 83.87%
[2024-07-29 19:58:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 19:58:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 19:59:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:15:13 lr 0.000039	 wd 0.0000	time 10.4371 (10.4371)	loss 1.6514 (1.6514)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:00:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:29:53 lr 0.000039	 wd 0.0000	time 0.6117 (0.7465)	loss 1.3121 (1.3902)	grad_norm 4.9287 (5.7624)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:01:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:26:47 lr 0.000039	 wd 0.0000	time 0.6231 (0.6981)	loss 1.5949 (1.3710)	grad_norm 7.4024 (5.7435)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:02:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:25:01 lr 0.000039	 wd 0.0000	time 0.5979 (0.6821)	loss 1.3317 (1.3709)	grad_norm 4.8485 (5.9160)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:03:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:23:36 lr 0.000039	 wd 0.0000	time 0.6272 (0.6739)	loss 1.4564 (1.3756)	grad_norm 4.9185 (5.8540)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:04:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:22:19 lr 0.000039	 wd 0.0000	time 0.6192 (0.6691)	loss 1.3895 (1.3700)	grad_norm 5.4328 (5.7648)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:05:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:21:06 lr 0.000039	 wd 0.0000	time 0.6130 (0.6658)	loss 1.3549 (1.3758)	grad_norm 4.6404 (5.8538)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:06:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:19:55 lr 0.000039	 wd 0.0000	time 0.6348 (0.6635)	loss 1.4263 (1.3778)	grad_norm 5.6824 (5.8356)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:07:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:18:46 lr 0.000039	 wd 0.0000	time 0.6284 (0.6618)	loss 0.9646 (1.3805)	grad_norm 4.7807 (5.7781)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:08:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:17:37 lr 0.000039	 wd 0.0000	time 0.6291 (0.6604)	loss 1.1571 (1.3814)	grad_norm 10.7139 (5.8434)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:09:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:16:30 lr 0.000039	 wd 0.0000	time 0.6228 (0.6594)	loss 1.1728 (1.3778)	grad_norm 8.6262 (5.8961)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:11:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:15:23 lr 0.000039	 wd 0.0000	time 0.6182 (0.6586)	loss 1.4280 (1.3761)	grad_norm 5.3593 (5.8755)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:12:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:14:16 lr 0.000039	 wd 0.0000	time 0.6068 (0.6580)	loss 1.3399 (1.3736)	grad_norm 3.6561 (5.8579)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:13:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:13:10 lr 0.000039	 wd 0.0000	time 0.6213 (0.6575)	loss 1.6048 (1.3749)	grad_norm 4.1156 (5.8262)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:14:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:12:04 lr 0.000039	 wd 0.0000	time 0.6169 (0.6570)	loss 1.4676 (1.3756)	grad_norm 3.5683 (5.8373)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:15:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:10:57 lr 0.000039	 wd 0.0000	time 0.6237 (0.6565)	loss 1.4650 (1.3740)	grad_norm 5.5223 (5.8389)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:16:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:09:51 lr 0.000039	 wd 0.0000	time 0.6227 (0.6561)	loss 1.5037 (1.3757)	grad_norm 4.4934 (5.8219)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:17:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:08:45 lr 0.000039	 wd 0.0000	time 0.6158 (0.6558)	loss 1.4773 (1.3750)	grad_norm 5.4671 (5.8646)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:18:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:07:40 lr 0.000039	 wd 0.0000	time 0.6335 (0.6555)	loss 1.2799 (1.3741)	grad_norm 5.7415 (5.8813)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:19:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:06:34 lr 0.000039	 wd 0.0000	time 0.6091 (0.6552)	loss 1.2998 (1.3737)	grad_norm 7.0111 (5.8684)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:20:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:05:28 lr 0.000039	 wd 0.0000	time 0.6159 (0.6550)	loss 1.5897 (1.3754)	grad_norm 6.8736 (5.8400)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:21:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:04:23 lr 0.000039	 wd 0.0000	time 0.6200 (0.6548)	loss 1.5003 (1.3764)	grad_norm 5.5187 (5.8318)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:22:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:03:17 lr 0.000039	 wd 0.0000	time 0.6209 (0.6547)	loss 1.2080 (1.3752)	grad_norm 4.3188 (5.8530)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:24:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:02:12 lr 0.000039	 wd 0.0000	time 0.6247 (0.6545)	loss 1.5975 (1.3757)	grad_norm 4.3826 (5.8915)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:25:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:06 lr 0.000039	 wd 0.0000	time 0.6527 (0.6544)	loss 1.3487 (1.3753)	grad_norm 5.5165 (5.8793)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:26:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000039	 wd 0.0000	time 0.6336 (0.6541)	loss 1.3397 (1.3751)	grad_norm 4.1399 (5.8772)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:26:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 7 training takes 0:27:19
[2024-07-29 20:26:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.162 (11.162)	Loss 0.4255 (0.4255)	Acc@1 91.992 (91.992)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 20:26:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 83.960 Acc@5 97.236
[2024-07-29 20:26:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-29 20:26:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 83.96%
[2024-07-29 20:26:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 20:26:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:26:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:03:18 lr 0.000039	 wd 0.0000	time 10.1511 (10.1511)	loss 1.5346 (1.5346)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 20:28:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:29:48 lr 0.000039	 wd 0.0000	time 0.6564 (0.7444)	loss 1.6099 (1.4108)	grad_norm 4.3939 (5.5475)	loss_scale 2048.0000 (1835.0891)	mem 17424MB
[2024-07-29 20:29:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:26:45 lr 0.000039	 wd 0.0000	time 0.6177 (0.6975)	loss 1.5234 (1.3836)	grad_norm 4.2568 (5.4338)	loss_scale 2048.0000 (1941.0149)	mem 17424MB
[2024-07-29 20:30:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:25:00 lr 0.000038	 wd 0.0000	time 0.6250 (0.6816)	loss 1.3854 (1.3797)	grad_norm 4.6242 (5.3110)	loss_scale 2048.0000 (1976.5581)	mem 17424MB
[2024-07-29 20:31:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:23:35 lr 0.000038	 wd 0.0000	time 0.6225 (0.6735)	loss 1.4519 (1.3748)	grad_norm 4.9970 (5.3309)	loss_scale 2048.0000 (1994.3741)	mem 17424MB
[2024-07-29 20:32:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:22:19 lr 0.000038	 wd 0.0000	time 0.6156 (0.6689)	loss 1.7067 (1.3772)	grad_norm 4.5208 (5.3624)	loss_scale 2048.0000 (2005.0778)	mem 17424MB
[2024-07-29 20:33:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:21:06 lr 0.000038	 wd 0.0000	time 0.6294 (0.6660)	loss 1.4937 (1.3793)	grad_norm 5.1093 (5.4452)	loss_scale 2048.0000 (2012.2196)	mem 17424MB
[2024-07-29 20:34:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:19:56 lr 0.000038	 wd 0.0000	time 0.6237 (0.6638)	loss 1.5774 (1.3786)	grad_norm 5.0646 (5.3971)	loss_scale 2048.0000 (2017.3238)	mem 17424MB
[2024-07-29 20:35:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:18:47 lr 0.000038	 wd 0.0000	time 0.6163 (0.6622)	loss 1.3960 (1.3767)	grad_norm 3.6597 (5.4065)	loss_scale 2048.0000 (2021.1536)	mem 17424MB
[2024-07-29 20:36:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:17:38 lr 0.000038	 wd 0.0000	time 0.6226 (0.6609)	loss 1.6396 (1.3766)	grad_norm 3.9778 (5.3838)	loss_scale 2048.0000 (2024.1332)	mem 17424MB
[2024-07-29 20:37:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:16:31 lr 0.000038	 wd 0.0000	time 0.6216 (0.6598)	loss 1.4520 (1.3740)	grad_norm 5.3452 (5.3191)	loss_scale 2048.0000 (2026.5175)	mem 17424MB
[2024-07-29 20:38:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:15:23 lr 0.000038	 wd 0.0000	time 0.6197 (0.6589)	loss 1.3207 (1.3704)	grad_norm 6.2353 (5.3094)	loss_scale 2048.0000 (2028.4687)	mem 17424MB
[2024-07-29 20:39:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:14:17 lr 0.000038	 wd 0.0000	time 0.6137 (0.6582)	loss 1.4050 (1.3700)	grad_norm 5.8492 (5.2851)	loss_scale 2048.0000 (2030.0949)	mem 17424MB
[2024-07-29 20:41:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:13:10 lr 0.000038	 wd 0.0000	time 0.6210 (0.6577)	loss 1.1419 (1.3681)	grad_norm 4.8397 (5.3620)	loss_scale 2048.0000 (2031.4712)	mem 17424MB
[2024-07-29 20:42:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:12:04 lr 0.000038	 wd 0.0000	time 0.6251 (0.6572)	loss 0.9179 (1.3711)	grad_norm 4.7759 (5.4046)	loss_scale 2048.0000 (2032.6510)	mem 17424MB
[2024-07-29 20:43:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:10:58 lr 0.000038	 wd 0.0000	time 0.6181 (0.6567)	loss 0.8702 (1.3681)	grad_norm 5.4498 (5.4137)	loss_scale 2048.0000 (2033.6736)	mem 17424MB
[2024-07-29 20:44:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:09:52 lr 0.000038	 wd 0.0000	time 0.6121 (0.6564)	loss 1.3522 (1.3640)	grad_norm 4.0879 (5.4077)	loss_scale 2048.0000 (2034.5684)	mem 17424MB
[2024-07-29 20:45:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:08:46 lr 0.000038	 wd 0.0000	time 0.6161 (0.6560)	loss 1.1604 (1.3652)	grad_norm 5.9896 (5.4059)	loss_scale 2048.0000 (2035.3580)	mem 17424MB
[2024-07-29 20:46:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:07:40 lr 0.000038	 wd 0.0000	time 0.6094 (0.6557)	loss 1.6035 (1.3668)	grad_norm 4.0228 (nan)	loss_scale 1024.0000 (2007.6313)	mem 17424MB
[2024-07-29 20:47:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:06:34 lr 0.000038	 wd 0.0000	time 0.6286 (0.6555)	loss 1.4456 (1.3669)	grad_norm 5.2825 (nan)	loss_scale 1024.0000 (1955.8885)	mem 17424MB
[2024-07-29 20:48:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:05:28 lr 0.000038	 wd 0.0000	time 0.6201 (0.6553)	loss 1.7072 (1.3685)	grad_norm 3.9474 (nan)	loss_scale 1024.0000 (1909.3173)	mem 17424MB
[2024-07-29 20:49:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:04:23 lr 0.000038	 wd 0.0000	time 0.6200 (0.6552)	loss 1.3891 (1.3676)	grad_norm 5.5386 (nan)	loss_scale 1024.0000 (1867.1794)	mem 17424MB
[2024-07-29 20:50:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:03:17 lr 0.000038	 wd 0.0000	time 0.6245 (0.6550)	loss 1.5534 (1.3665)	grad_norm 5.4668 (nan)	loss_scale 1024.0000 (1828.8705)	mem 17424MB
[2024-07-29 20:51:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:02:12 lr 0.000038	 wd 0.0000	time 0.6212 (0.6548)	loss 1.5604 (1.3675)	grad_norm 6.9748 (nan)	loss_scale 512.0000 (1788.9961)	mem 17424MB
[2024-07-29 20:52:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:06 lr 0.000038	 wd 0.0000	time 0.6189 (0.6546)	loss 1.3392 (1.3687)	grad_norm 4.5011 (nan)	loss_scale 512.0000 (1735.8101)	mem 17424MB
[2024-07-29 20:54:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000038	 wd 0.0000	time 0.6203 (0.6543)	loss 1.7625 (1.3704)	grad_norm 6.4978 (nan)	loss_scale 512.0000 (1686.8772)	mem 17424MB
[2024-07-29 20:54:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 8 training takes 0:27:19
[2024-07-29 20:54:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.743 (11.743)	Loss 0.4207 (0.4207)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 20:54:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.140 Acc@5 97.286
[2024-07-29 20:54:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-29 20:54:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.14%
[2024-07-29 20:54:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 20:54:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:54:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:27:45 lr 0.000038	 wd 0.0000	time 10.7375 (10.7375)	loss 1.4207 (1.4207)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 20:55:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:29:59 lr 0.000038	 wd 0.0000	time 0.6215 (0.7493)	loss 1.2265 (1.3255)	grad_norm 4.8239 (5.7362)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 20:56:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:26:51 lr 0.000037	 wd 0.0000	time 0.6249 (0.7001)	loss 1.5215 (1.3413)	grad_norm 7.0906 (5.6182)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 20:58:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:25:04 lr 0.000037	 wd 0.0000	time 0.6240 (0.6835)	loss 1.4882 (1.3545)	grad_norm 3.6810 (5.6188)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 20:59:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:23:38 lr 0.000037	 wd 0.0000	time 0.6224 (0.6750)	loss 1.5005 (1.3527)	grad_norm 9.5540 (5.5909)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:00:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:22:21 lr 0.000037	 wd 0.0000	time 0.6196 (0.6699)	loss 1.3504 (1.3559)	grad_norm 4.7932 (5.5298)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:01:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:21:07 lr 0.000037	 wd 0.0000	time 0.6229 (0.6666)	loss 1.6562 (1.3513)	grad_norm 5.3628 (5.6807)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:02:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:19:57 lr 0.000037	 wd 0.0000	time 0.6210 (0.6646)	loss 1.5959 (1.3533)	grad_norm 4.1979 (5.6957)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:03:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:18:48 lr 0.000037	 wd 0.0000	time 0.6241 (0.6628)	loss 1.3754 (1.3554)	grad_norm 6.1648 (5.6637)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:04:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:17:39 lr 0.000037	 wd 0.0000	time 0.6169 (0.6612)	loss 1.5785 (1.3523)	grad_norm 4.9277 (5.6569)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:05:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:16:31 lr 0.000037	 wd 0.0000	time 0.6220 (0.6602)	loss 1.4804 (1.3549)	grad_norm 7.1414 (5.7364)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:06:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:15:24 lr 0.000037	 wd 0.0000	time 0.6273 (0.6593)	loss 1.4768 (1.3576)	grad_norm 5.2098 (5.6968)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:07:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:14:17 lr 0.000037	 wd 0.0000	time 0.6164 (0.6586)	loss 1.5776 (1.3580)	grad_norm 4.7058 (5.7567)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:08:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:13:10 lr 0.000037	 wd 0.0000	time 0.6323 (0.6580)	loss 0.8701 (1.3554)	grad_norm 3.9293 (5.7380)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:09:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:12:04 lr 0.000037	 wd 0.0000	time 0.6215 (0.6575)	loss 1.5124 (1.3570)	grad_norm 4.3541 (5.7080)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:11:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:10:58 lr 0.000037	 wd 0.0000	time 0.6219 (0.6570)	loss 1.5514 (1.3595)	grad_norm 5.1794 (5.6912)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:12:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:09:52 lr 0.000037	 wd 0.0000	time 0.6256 (0.6565)	loss 1.0104 (1.3587)	grad_norm 5.7550 (5.6827)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:13:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:08:46 lr 0.000037	 wd 0.0000	time 0.6203 (0.6562)	loss 1.0483 (1.3601)	grad_norm 4.2479 (5.7158)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:14:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:07:40 lr 0.000037	 wd 0.0000	time 0.6254 (0.6560)	loss 1.5198 (1.3605)	grad_norm 6.0695 (5.6852)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:15:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:06:34 lr 0.000037	 wd 0.0000	time 0.6191 (0.6557)	loss 1.3548 (1.3600)	grad_norm 6.1624 (5.6805)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:16:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:05:29 lr 0.000037	 wd 0.0000	time 0.6194 (0.6555)	loss 1.5806 (1.3598)	grad_norm 7.6771 (5.6869)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:17:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:04:23 lr 0.000036	 wd 0.0000	time 0.6157 (0.6552)	loss 1.1270 (1.3610)	grad_norm 5.0025 (5.6827)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:18:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:03:17 lr 0.000036	 wd 0.0000	time 0.6280 (0.6550)	loss 1.4757 (1.3618)	grad_norm 5.4185 (5.6631)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:19:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:02:12 lr 0.000036	 wd 0.0000	time 0.6209 (0.6549)	loss 1.4563 (1.3623)	grad_norm 4.9231 (5.6650)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:20:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:06 lr 0.000036	 wd 0.0000	time 0.6241 (0.6548)	loss 1.2366 (1.3625)	grad_norm 4.4815 (5.6654)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:21:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000036	 wd 0.0000	time 0.6233 (0.6545)	loss 0.9740 (1.3636)	grad_norm 7.4321 (5.6690)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:21:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 9 training takes 0:27:20
[2024-07-29 21:22:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.730 (11.730)	Loss 0.4045 (0.4045)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 21:22:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.250 Acc@5 97.306
[2024-07-29 21:22:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.3%
[2024-07-29 21:22:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.25%
[2024-07-29 21:22:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 21:22:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 21:22:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:22:30 lr 0.000036	 wd 0.0000	time 10.6118 (10.6118)	loss 1.2572 (1.2572)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:23:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:29:58 lr 0.000036	 wd 0.0000	time 0.6278 (0.7487)	loss 1.1946 (1.3308)	grad_norm 4.7208 (5.5041)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:24:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:26:50 lr 0.000036	 wd 0.0000	time 0.6261 (0.6995)	loss 1.3292 (1.3529)	grad_norm 14.0571 (5.3601)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:25:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:25:03 lr 0.000036	 wd 0.0000	time 0.6229 (0.6828)	loss 0.7966 (1.3552)	grad_norm 3.8452 (5.5359)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:26:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:23:37 lr 0.000036	 wd 0.0000	time 0.6131 (0.6745)	loss 1.4019 (1.3550)	grad_norm 5.3015 (5.5739)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:28:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:22:20 lr 0.000036	 wd 0.0000	time 0.6196 (0.6696)	loss 1.4080 (1.3484)	grad_norm 4.0156 (5.5820)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:29:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:21:07 lr 0.000036	 wd 0.0000	time 0.6347 (0.6662)	loss 1.5502 (1.3473)	grad_norm 5.4815 (5.6176)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:30:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:19:56 lr 0.000036	 wd 0.0000	time 0.6220 (0.6640)	loss 1.3783 (1.3438)	grad_norm 5.5051 (5.6116)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:31:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:18:47 lr 0.000036	 wd 0.0000	time 0.6270 (0.6623)	loss 1.4150 (1.3454)	grad_norm 5.2827 (5.5714)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:32:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:17:38 lr 0.000036	 wd 0.0000	time 0.6201 (0.6610)	loss 1.4856 (1.3470)	grad_norm 5.7567 (5.5766)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:33:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:16:31 lr 0.000036	 wd 0.0000	time 0.6230 (0.6600)	loss 1.4942 (1.3459)	grad_norm 7.0890 (5.5796)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:34:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:15:24 lr 0.000036	 wd 0.0000	time 0.6164 (0.6591)	loss 1.3140 (1.3440)	grad_norm 4.0433 (5.6346)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:35:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:14:17 lr 0.000035	 wd 0.0000	time 0.6282 (0.6584)	loss 1.4266 (1.3441)	grad_norm 4.3612 (5.6325)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 21:36:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:13:10 lr 0.000035	 wd 0.0000	time 0.6250 (0.6579)	loss 1.6102 (1.3469)	grad_norm 4.9247 (5.6951)	loss_scale 1024.0000 (522.2321)	mem 17424MB
[2024-07-29 21:37:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:12:04 lr 0.000035	 wd 0.0000	time 0.6220 (0.6573)	loss 1.3697 (1.3484)	grad_norm 5.0465 (5.6944)	loss_scale 1024.0000 (558.0471)	mem 17424MB
[2024-07-29 21:38:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:10:58 lr 0.000035	 wd 0.0000	time 0.6207 (0.6568)	loss 1.1705 (1.3482)	grad_norm 4.2735 (5.6745)	loss_scale 1024.0000 (589.0899)	mem 17424MB
[2024-07-29 21:39:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:09:52 lr 0.000035	 wd 0.0000	time 0.6131 (0.6565)	loss 1.1223 (1.3491)	grad_norm 5.0201 (5.6460)	loss_scale 1024.0000 (616.2548)	mem 17424MB
[2024-07-29 21:41:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:08:46 lr 0.000035	 wd 0.0000	time 0.6210 (0.6562)	loss 1.4177 (1.3498)	grad_norm 4.3684 (5.6494)	loss_scale 1024.0000 (640.2257)	mem 17424MB
[2024-07-29 21:42:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:07:40 lr 0.000035	 wd 0.0000	time 0.6512 (0.6559)	loss 1.3257 (1.3504)	grad_norm 8.7545 (5.6526)	loss_scale 1024.0000 (661.5347)	mem 17424MB
[2024-07-29 21:43:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:06:34 lr 0.000035	 wd 0.0000	time 0.6118 (0.6557)	loss 1.4773 (1.3515)	grad_norm 9.1890 (5.6427)	loss_scale 1024.0000 (680.6018)	mem 17424MB
[2024-07-29 21:44:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:05:29 lr 0.000035	 wd 0.0000	time 0.6213 (0.6555)	loss 1.2507 (1.3517)	grad_norm 5.1240 (5.6629)	loss_scale 1024.0000 (697.7631)	mem 17424MB
[2024-07-29 21:45:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:04:23 lr 0.000035	 wd 0.0000	time 0.6246 (0.6553)	loss 1.6661 (1.3534)	grad_norm 4.5102 (5.6412)	loss_scale 1024.0000 (713.2908)	mem 17424MB
[2024-07-29 21:46:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:03:17 lr 0.000035	 wd 0.0000	time 0.6093 (0.6551)	loss 1.0864 (1.3537)	grad_norm 5.2237 (5.6559)	loss_scale 1024.0000 (727.4075)	mem 17424MB
[2024-07-29 21:47:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:02:12 lr 0.000035	 wd 0.0000	time 0.6263 (0.6549)	loss 1.5190 (1.3533)	grad_norm 5.3659 (5.6657)	loss_scale 1024.0000 (740.2973)	mem 17424MB
[2024-07-29 21:48:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:06 lr 0.000035	 wd 0.0000	time 0.6187 (0.6548)	loss 1.5703 (1.3534)	grad_norm 4.4478 (5.6596)	loss_scale 1024.0000 (752.1133)	mem 17424MB
[2024-07-29 21:49:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000035	 wd 0.0000	time 0.6218 (0.6545)	loss 1.3948 (1.3526)	grad_norm 4.4270 (5.6552)	loss_scale 1024.0000 (762.9844)	mem 17424MB
[2024-07-29 21:49:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 10 training takes 0:27:20
[2024-07-29 21:49:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 145): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_10.pth saving......
[2024-07-29 21:49:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 147): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_10.pth saved !!!
[2024-07-29 21:50:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 10.928 (10.928)	Loss 0.4163 (0.4163)	Acc@1 91.602 (91.602)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 21:50:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.168 Acc@5 97.338
[2024-07-29 21:50:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.2%
[2024-07-29 21:50:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.25%
[2024-07-29 21:50:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:30:36 lr 0.000035	 wd 0.0000	time 10.8058 (10.8058)	loss 1.4848 (1.4848)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:51:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:30:12 lr 0.000035	 wd 0.0000	time 0.6239 (0.7545)	loss 1.3770 (1.3379)	grad_norm 7.4853 (5.5876)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:52:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:26:57 lr 0.000034	 wd 0.0000	time 0.6194 (0.7026)	loss 1.4730 (1.3428)	grad_norm 5.0573 (5.4055)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:53:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:25:09 lr 0.000034	 wd 0.0000	time 0.6240 (0.6854)	loss 1.4771 (1.3475)	grad_norm 4.7628 (5.6531)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:54:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:23:41 lr 0.000034	 wd 0.0000	time 0.6280 (0.6763)	loss 1.4514 (1.3498)	grad_norm 4.6623 (5.7397)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:55:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:22:23 lr 0.000034	 wd 0.0000	time 0.6281 (0.6709)	loss 1.4861 (1.3468)	grad_norm 3.9863 (5.6585)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:56:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:21:09 lr 0.000034	 wd 0.0000	time 0.6235 (0.6673)	loss 1.4831 (1.3433)	grad_norm 4.6135 (5.6065)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:58:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:19:57 lr 0.000034	 wd 0.0000	time 0.6188 (0.6647)	loss 1.4642 (1.3441)	grad_norm 3.6984 (5.5595)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 21:59:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:18:48 lr 0.000034	 wd 0.0000	time 0.6220 (0.6628)	loss 1.5459 (1.3462)	grad_norm 5.5883 (5.5452)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:00:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:17:39 lr 0.000034	 wd 0.0000	time 0.6222 (0.6614)	loss 1.5727 (1.3470)	grad_norm 4.8696 (5.5054)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:01:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:16:31 lr 0.000034	 wd 0.0000	time 0.6300 (0.6604)	loss 1.5024 (1.3478)	grad_norm 4.5472 (5.4534)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:02:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:15:24 lr 0.000034	 wd 0.0000	time 0.6093 (0.6594)	loss 1.4503 (1.3497)	grad_norm 5.5777 (5.4356)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:03:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:14:17 lr 0.000034	 wd 0.0000	time 0.6111 (0.6586)	loss 1.4412 (1.3500)	grad_norm 4.8694 (5.4229)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:04:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:13:10 lr 0.000034	 wd 0.0000	time 0.6380 (0.6580)	loss 0.8873 (1.3487)	grad_norm 4.7833 (5.4174)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:05:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:12:04 lr 0.000034	 wd 0.0000	time 0.6271 (0.6575)	loss 0.9470 (1.3484)	grad_norm 5.4552 (5.4299)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:06:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:10:58 lr 0.000034	 wd 0.0000	time 0.6171 (0.6571)	loss 1.4228 (1.3483)	grad_norm 4.8382 (5.4423)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:07:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:09:52 lr 0.000034	 wd 0.0000	time 0.6277 (0.6566)	loss 1.3149 (1.3485)	grad_norm 4.7040 (5.4376)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:08:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:08:46 lr 0.000033	 wd 0.0000	time 0.6151 (0.6564)	loss 1.4106 (1.3477)	grad_norm 16.9159 (5.5181)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:09:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:07:40 lr 0.000033	 wd 0.0000	time 0.6269 (0.6562)	loss 1.3198 (1.3493)	grad_norm 6.2604 (5.5011)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:11:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:06:34 lr 0.000033	 wd 0.0000	time 0.6281 (0.6559)	loss 1.4437 (1.3487)	grad_norm 10.4096 (5.4813)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:12:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:05:29 lr 0.000033	 wd 0.0000	time 0.6152 (0.6557)	loss 1.1191 (1.3486)	grad_norm 3.1008 (5.5125)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:13:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:04:23 lr 0.000033	 wd 0.0000	time 0.6130 (0.6554)	loss 1.5420 (1.3475)	grad_norm 3.8262 (5.5065)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:14:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:03:17 lr 0.000033	 wd 0.0000	time 0.6335 (0.6552)	loss 1.4521 (1.3485)	grad_norm 3.9253 (5.5267)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:15:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:02:12 lr 0.000033	 wd 0.0000	time 0.5951 (0.6550)	loss 1.5461 (1.3487)	grad_norm 5.0100 (5.5186)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:16:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:06 lr 0.000033	 wd 0.0000	time 0.6231 (0.6549)	loss 1.2572 (1.3482)	grad_norm 4.8595 (5.5286)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:17:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000033	 wd 0.0000	time 0.6190 (0.6546)	loss 1.5160 (1.3480)	grad_norm 6.5918 (5.5195)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:17:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 11 training takes 0:27:20
[2024-07-29 22:17:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.658 (11.658)	Loss 0.4124 (0.4124)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-29 22:18:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.390 Acc@5 97.306
[2024-07-29 22:18:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-07-29 22:18:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.39%
[2024-07-29 22:18:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 22:18:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 22:18:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:18:14 lr 0.000033	 wd 0.0000	time 10.5092 (10.5092)	loss 1.4926 (1.4926)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:19:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:29:54 lr 0.000033	 wd 0.0000	time 0.6346 (0.7470)	loss 1.5455 (1.3464)	grad_norm 9.6629 (5.1421)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:20:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:26:50 lr 0.000033	 wd 0.0000	time 0.6242 (0.6996)	loss 0.9306 (1.3583)	grad_norm 3.9005 (5.0730)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 22:21:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:25:04 lr 0.000033	 wd 0.0000	time 0.6176 (0.6831)	loss 1.2199 (1.3587)	grad_norm 7.2892 (5.1616)	loss_scale 2048.0000 (1126.0598)	mem 17424MB
[2024-07-29 22:22:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:23:38 lr 0.000033	 wd 0.0000	time 0.6232 (0.6748)	loss 1.3576 (1.3548)	grad_norm 3.4426 (5.1559)	loss_scale 2048.0000 (1355.9701)	mem 17424MB
[2024-07-29 22:23:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:22:20 lr 0.000032	 wd 0.0000	time 0.6222 (0.6697)	loss 0.9969 (1.3521)	grad_norm 2.9414 (5.1091)	loss_scale 2048.0000 (1494.0998)	mem 17424MB
[2024-07-29 22:24:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:21:07 lr 0.000032	 wd 0.0000	time 0.6363 (0.6665)	loss 1.4100 (1.3496)	grad_norm 3.7094 (5.1436)	loss_scale 2048.0000 (1586.2629)	mem 17424MB
[2024-07-29 22:25:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:19:56 lr 0.000032	 wd 0.0000	time 0.6236 (0.6642)	loss 1.3997 (1.3488)	grad_norm 4.7365 (5.1463)	loss_scale 2048.0000 (1652.1312)	mem 17424MB
[2024-07-29 22:26:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:18:47 lr 0.000032	 wd 0.0000	time 0.6251 (0.6625)	loss 1.5511 (1.3545)	grad_norm 5.1023 (5.1430)	loss_scale 2048.0000 (1701.5531)	mem 17424MB
[2024-07-29 22:28:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:17:39 lr 0.000032	 wd 0.0000	time 0.6230 (0.6612)	loss 1.0120 (1.3562)	grad_norm 5.1736 (5.1789)	loss_scale 2048.0000 (1740.0044)	mem 17424MB
[2024-07-29 22:29:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:16:31 lr 0.000032	 wd 0.0000	time 0.6192 (0.6601)	loss 1.2919 (1.3553)	grad_norm 3.9039 (5.1486)	loss_scale 2048.0000 (1770.7732)	mem 17424MB
[2024-07-29 22:30:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:15:24 lr 0.000032	 wd 0.0000	time 0.6277 (0.6592)	loss 1.4860 (1.3602)	grad_norm 3.4738 (5.1615)	loss_scale 2048.0000 (1795.9528)	mem 17424MB
[2024-07-29 22:31:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:14:17 lr 0.000032	 wd 0.0000	time 0.6143 (0.6586)	loss 1.4115 (1.3582)	grad_norm 5.7296 (5.1782)	loss_scale 2048.0000 (1816.9392)	mem 17424MB
[2024-07-29 22:32:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:13:10 lr 0.000032	 wd 0.0000	time 0.6226 (0.6580)	loss 1.5611 (1.3575)	grad_norm 5.7894 (inf)	loss_scale 1024.0000 (1815.8094)	mem 17424MB
[2024-07-29 22:33:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:12:04 lr 0.000032	 wd 0.0000	time 0.6195 (0.6575)	loss 1.5984 (1.3573)	grad_norm 6.6643 (inf)	loss_scale 1024.0000 (1759.2919)	mem 17424MB
[2024-07-29 22:34:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:10:58 lr 0.000032	 wd 0.0000	time 0.6251 (0.6571)	loss 1.5168 (1.3585)	grad_norm 3.8574 (inf)	loss_scale 1024.0000 (1710.3051)	mem 17424MB
[2024-07-29 22:35:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:09:52 lr 0.000032	 wd 0.0000	time 0.6197 (0.6567)	loss 1.3828 (1.3564)	grad_norm 4.8860 (inf)	loss_scale 1024.0000 (1667.4379)	mem 17424MB
[2024-07-29 22:36:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:08:46 lr 0.000031	 wd 0.0000	time 0.6234 (0.6564)	loss 1.4531 (1.3576)	grad_norm 3.2306 (inf)	loss_scale 1024.0000 (1629.6108)	mem 17424MB
[2024-07-29 22:37:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:07:40 lr 0.000031	 wd 0.0000	time 0.6276 (0.6561)	loss 1.4752 (1.3587)	grad_norm 4.2464 (inf)	loss_scale 1024.0000 (1595.9845)	mem 17424MB
[2024-07-29 22:38:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:06:34 lr 0.000031	 wd 0.0000	time 0.6196 (0.6559)	loss 1.4281 (1.3581)	grad_norm 4.8960 (inf)	loss_scale 1024.0000 (1565.8958)	mem 17424MB
[2024-07-29 22:40:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:05:29 lr 0.000031	 wd 0.0000	time 0.6180 (0.6556)	loss 1.4655 (1.3569)	grad_norm 3.6117 (inf)	loss_scale 1024.0000 (1538.8146)	mem 17424MB
[2024-07-29 22:41:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:04:23 lr 0.000031	 wd 0.0000	time 0.6276 (0.6554)	loss 1.3552 (1.3581)	grad_norm 7.6226 (inf)	loss_scale 1024.0000 (1514.3113)	mem 17424MB
[2024-07-29 22:42:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:03:17 lr 0.000031	 wd 0.0000	time 0.6248 (0.6552)	loss 1.4808 (1.3588)	grad_norm 4.0297 (nan)	loss_scale 512.0000 (1489.2431)	mem 17424MB
[2024-07-29 22:43:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:02:12 lr 0.000031	 wd 0.0000	time 0.6222 (0.6550)	loss 1.5291 (1.3589)	grad_norm 8.1790 (nan)	loss_scale 512.0000 (1446.7727)	mem 17424MB
[2024-07-29 22:44:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:06 lr 0.000031	 wd 0.0000	time 0.6210 (0.6548)	loss 1.4319 (1.3581)	grad_norm 7.1840 (nan)	loss_scale 512.0000 (1407.8401)	mem 17424MB
[2024-07-29 22:45:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000031	 wd 0.0000	time 0.6215 (0.6545)	loss 1.2033 (1.3567)	grad_norm 4.7004 (nan)	loss_scale 512.0000 (1372.0208)	mem 17424MB
[2024-07-29 22:45:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 12 training takes 0:27:20
[2024-07-29 22:45:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.882 (11.882)	Loss 0.4141 (0.4141)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 22:45:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.388 Acc@5 97.316
[2024-07-29 22:45:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-07-29 22:45:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.39%
[2024-07-29 22:46:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:30:30 lr 0.000031	 wd 0.0000	time 10.8037 (10.8037)	loss 1.3238 (1.3238)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:47:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:30:06 lr 0.000031	 wd 0.0000	time 0.6144 (0.7521)	loss 1.4317 (1.3389)	grad_norm 4.3258 (5.1859)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:48:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:26:53 lr 0.000031	 wd 0.0000	time 0.6259 (0.7011)	loss 1.4460 (1.3490)	grad_norm 6.7841 (5.0956)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:49:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:25:06 lr 0.000031	 wd 0.0000	time 0.6069 (0.6842)	loss 1.5682 (1.3509)	grad_norm 5.9792 (5.2167)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:50:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:23:39 lr 0.000030	 wd 0.0000	time 0.6226 (0.6753)	loss 1.4212 (1.3528)	grad_norm 4.5055 (5.0985)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:51:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:22:21 lr 0.000030	 wd 0.0000	time 0.6110 (0.6701)	loss 1.4310 (1.3447)	grad_norm 5.4031 (5.1611)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:52:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:21:08 lr 0.000030	 wd 0.0000	time 0.6147 (0.6668)	loss 1.4494 (1.3508)	grad_norm 5.3027 (5.1568)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:53:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:19:57 lr 0.000030	 wd 0.0000	time 0.6222 (0.6646)	loss 1.6158 (1.3469)	grad_norm 3.5777 (5.2037)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:54:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:18:47 lr 0.000030	 wd 0.0000	time 0.6182 (0.6627)	loss 1.4977 (1.3506)	grad_norm 4.4181 (5.1318)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:55:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:17:39 lr 0.000030	 wd 0.0000	time 0.6118 (0.6613)	loss 1.6349 (1.3475)	grad_norm 4.9334 (5.1354)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:56:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:16:31 lr 0.000030	 wd 0.0000	time 0.6120 (0.6601)	loss 1.5240 (1.3492)	grad_norm 3.8967 (5.1065)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:58:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:15:24 lr 0.000030	 wd 0.0000	time 0.6023 (0.6592)	loss 1.3129 (1.3508)	grad_norm 6.7102 (5.1179)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 22:59:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:14:17 lr 0.000030	 wd 0.0000	time 0.6256 (0.6584)	loss 1.6334 (1.3468)	grad_norm 4.1416 (5.0979)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:00:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:13:10 lr 0.000030	 wd 0.0000	time 0.6185 (0.6578)	loss 1.4502 (1.3466)	grad_norm 3.8790 (5.1535)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:01:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:12:04 lr 0.000030	 wd 0.0000	time 0.6175 (0.6572)	loss 1.6356 (1.3469)	grad_norm 5.2147 (5.1443)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:02:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:10:58 lr 0.000030	 wd 0.0000	time 0.6218 (0.6568)	loss 1.5935 (1.3454)	grad_norm 8.2466 (5.1667)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:03:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:09:52 lr 0.000029	 wd 0.0000	time 0.6147 (0.6564)	loss 1.4793 (1.3451)	grad_norm 5.0911 (5.1596)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:04:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:08:46 lr 0.000029	 wd 0.0000	time 0.6270 (0.6561)	loss 0.9133 (1.3420)	grad_norm 3.8480 (5.2062)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:05:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:07:40 lr 0.000029	 wd 0.0000	time 0.6170 (0.6558)	loss 1.5333 (1.3431)	grad_norm 3.6911 (5.2318)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:06:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:06:34 lr 0.000029	 wd 0.0000	time 0.6300 (0.6555)	loss 1.1381 (1.3425)	grad_norm 6.6554 (5.2359)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:07:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:05:28 lr 0.000029	 wd 0.0000	time 0.6180 (0.6553)	loss 1.3345 (1.3432)	grad_norm 4.2421 (5.2382)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:08:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:04:23 lr 0.000029	 wd 0.0000	time 0.6152 (0.6551)	loss 1.0945 (1.3430)	grad_norm 5.2841 (5.2453)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:09:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:03:17 lr 0.000029	 wd 0.0000	time 0.6247 (0.6549)	loss 1.5681 (1.3441)	grad_norm 4.8805 (5.2912)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:11:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:02:12 lr 0.000029	 wd 0.0000	time 0.6150 (0.6547)	loss 1.3498 (1.3434)	grad_norm 5.2772 (5.3127)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:12:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:06 lr 0.000029	 wd 0.0000	time 0.6130 (0.6546)	loss 1.4779 (1.3440)	grad_norm 4.7996 (5.3043)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:13:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000029	 wd 0.0000	time 0.6192 (0.6543)	loss 1.5824 (1.3432)	grad_norm 4.3028 (5.2972)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:13:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 13 training takes 0:27:19
[2024-07-29 23:13:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.482 (12.482)	Loss 0.4104 (0.4104)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-29 23:13:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.458 Acc@5 97.362
[2024-07-29 23:13:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-29 23:13:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.46%
[2024-07-29 23:13:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 23:13:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 23:13:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:20:28 lr 0.000029	 wd 0.0000	time 10.5628 (10.5628)	loss 1.4255 (1.4255)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:15:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:29:59 lr 0.000029	 wd 0.0000	time 0.6206 (0.7494)	loss 1.5383 (1.3374)	grad_norm 3.6402 (4.8455)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:16:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:26:51 lr 0.000028	 wd 0.0000	time 0.6180 (0.7000)	loss 1.2247 (1.3640)	grad_norm 4.8318 (5.7725)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:17:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:25:04 lr 0.000028	 wd 0.0000	time 0.6161 (0.6834)	loss 1.1300 (1.3500)	grad_norm 5.9034 (5.6944)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:18:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:23:38 lr 0.000028	 wd 0.0000	time 0.6258 (0.6749)	loss 1.2434 (1.3358)	grad_norm 4.1334 (5.6282)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:19:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:22:21 lr 0.000028	 wd 0.0000	time 0.6286 (0.6700)	loss 1.2431 (1.3392)	grad_norm 4.3187 (5.6209)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:20:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:21:08 lr 0.000028	 wd 0.0000	time 0.6240 (0.6667)	loss 1.6357 (1.3436)	grad_norm 5.7510 (5.5193)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:21:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:19:57 lr 0.000028	 wd 0.0000	time 0.6141 (0.6644)	loss 1.3056 (1.3397)	grad_norm 4.7120 (5.4851)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:22:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:18:47 lr 0.000028	 wd 0.0000	time 0.6258 (0.6625)	loss 1.4411 (1.3377)	grad_norm 6.1274 (5.5013)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:23:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:17:39 lr 0.000028	 wd 0.0000	time 0.6115 (0.6612)	loss 1.5418 (1.3359)	grad_norm 4.9736 (5.4840)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:24:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:16:31 lr 0.000028	 wd 0.0000	time 0.6247 (0.6601)	loss 1.3089 (1.3345)	grad_norm 4.4804 (5.5224)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:25:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:15:24 lr 0.000028	 wd 0.0000	time 0.6197 (0.6593)	loss 1.5475 (1.3322)	grad_norm 4.1085 (5.5212)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-29 23:27:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:14:17 lr 0.000028	 wd 0.0000	time 0.6332 (0.6586)	loss 1.4287 (1.3329)	grad_norm 11.7129 (5.5135)	loss_scale 1024.0000 (518.8210)	mem 17424MB
[2024-07-29 23:28:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:13:10 lr 0.000027	 wd 0.0000	time 0.6385 (0.6580)	loss 1.1758 (1.3340)	grad_norm 3.8913 (5.5684)	loss_scale 1024.0000 (557.6510)	mem 17424MB
[2024-07-29 23:29:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:12:04 lr 0.000027	 wd 0.0000	time 0.6178 (0.6575)	loss 1.4685 (1.3325)	grad_norm 5.3543 (5.5002)	loss_scale 1024.0000 (590.9379)	mem 17424MB
[2024-07-29 23:30:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:10:58 lr 0.000027	 wd 0.0000	time 0.6308 (0.6570)	loss 1.3779 (1.3340)	grad_norm 4.4112 (5.5022)	loss_scale 1024.0000 (619.7895)	mem 17424MB
[2024-07-29 23:31:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:09:52 lr 0.000027	 wd 0.0000	time 0.6253 (0.6567)	loss 1.4086 (1.3348)	grad_norm 5.8530 (5.4709)	loss_scale 1024.0000 (645.0369)	mem 17424MB
[2024-07-29 23:32:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:08:46 lr 0.000027	 wd 0.0000	time 0.6222 (0.6564)	loss 1.4455 (1.3349)	grad_norm 4.9231 (5.4756)	loss_scale 1024.0000 (667.3157)	mem 17424MB
[2024-07-29 23:33:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:07:40 lr 0.000027	 wd 0.0000	time 0.6074 (0.6561)	loss 1.3449 (1.3343)	grad_norm 4.2194 (5.4912)	loss_scale 1024.0000 (687.1205)	mem 17424MB
[2024-07-29 23:34:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:06:34 lr 0.000027	 wd 0.0000	time 0.6196 (0.6559)	loss 1.4189 (1.3350)	grad_norm 4.8662 (5.4851)	loss_scale 1024.0000 (704.8417)	mem 17424MB
[2024-07-29 23:35:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:05:29 lr 0.000027	 wd 0.0000	time 0.6132 (0.6557)	loss 0.9529 (1.3364)	grad_norm 4.1194 (5.4666)	loss_scale 1024.0000 (720.7916)	mem 17424MB
[2024-07-29 23:36:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:04:23 lr 0.000027	 wd 0.0000	time 0.6184 (0.6555)	loss 1.4702 (1.3353)	grad_norm 5.4354 (5.4929)	loss_scale 1024.0000 (735.2232)	mem 17424MB
[2024-07-29 23:37:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:03:17 lr 0.000027	 wd 0.0000	time 0.6230 (0.6552)	loss 1.2496 (1.3353)	grad_norm 4.1799 (5.4937)	loss_scale 1024.0000 (748.3435)	mem 17424MB
[2024-07-29 23:38:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:02:12 lr 0.000027	 wd 0.0000	time 0.6229 (0.6551)	loss 1.4625 (1.3366)	grad_norm 5.8682 (5.5247)	loss_scale 1024.0000 (760.3233)	mem 17424MB
[2024-07-29 23:40:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:06 lr 0.000026	 wd 0.0000	time 0.6235 (0.6549)	loss 1.2407 (1.3372)	grad_norm 4.8703 (5.5163)	loss_scale 1024.0000 (771.3053)	mem 17424MB
[2024-07-29 23:41:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000026	 wd 0.0000	time 0.6271 (0.6546)	loss 1.3060 (1.3369)	grad_norm 6.0713 (5.5002)	loss_scale 1024.0000 (781.4090)	mem 17424MB
[2024-07-29 23:41:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 14 training takes 0:27:20
[2024-07-29 23:41:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.346 (11.346)	Loss 0.4072 (0.4072)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-29 23:41:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.598 Acc@5 97.422
[2024-07-29 23:41:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-29 23:41:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.60%
[2024-07-29 23:41:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-29 23:41:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-29 23:41:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][0/2502]	eta 6:51:00 lr 0.000026	 wd 0.0000	time 9.8562 (9.8562)	loss 1.3500 (1.3500)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:42:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:29:39 lr 0.000026	 wd 0.0000	time 0.6258 (0.7410)	loss 0.9801 (1.3228)	grad_norm 4.5020 (4.7960)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:44:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:26:41 lr 0.000026	 wd 0.0000	time 0.6285 (0.6957)	loss 1.4360 (1.3346)	grad_norm 5.2412 (5.2043)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:45:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:24:58 lr 0.000026	 wd 0.0000	time 0.6213 (0.6805)	loss 1.2985 (1.3222)	grad_norm 3.1848 (5.4764)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:46:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:23:33 lr 0.000026	 wd 0.0000	time 0.6199 (0.6725)	loss 1.3048 (1.3241)	grad_norm 5.7127 (5.3015)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:47:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:22:17 lr 0.000026	 wd 0.0000	time 0.6238 (0.6681)	loss 0.8686 (1.3190)	grad_norm 3.0696 (5.5147)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:48:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:21:05 lr 0.000026	 wd 0.0000	time 0.6165 (0.6651)	loss 0.9023 (1.3226)	grad_norm 5.3861 (5.3826)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:49:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:19:54 lr 0.000026	 wd 0.0000	time 0.6200 (0.6630)	loss 1.1846 (1.3260)	grad_norm 7.7520 (5.4021)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:50:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:18:45 lr 0.000026	 wd 0.0000	time 0.6165 (0.6614)	loss 1.3764 (1.3265)	grad_norm 4.6175 (5.3953)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:51:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:17:37 lr 0.000025	 wd 0.0000	time 0.6245 (0.6600)	loss 0.9610 (1.3272)	grad_norm 3.9897 (5.4214)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:52:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:16:29 lr 0.000025	 wd 0.0000	time 0.6264 (0.6590)	loss 1.3896 (1.3278)	grad_norm 4.1916 (5.4055)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:53:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:15:22 lr 0.000025	 wd 0.0000	time 0.6221 (0.6583)	loss 1.2583 (1.3249)	grad_norm 4.2435 (5.3963)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:54:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:14:16 lr 0.000025	 wd 0.0000	time 0.6181 (0.6576)	loss 1.3910 (1.3256)	grad_norm 3.6399 (5.4294)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:55:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:13:09 lr 0.000025	 wd 0.0000	time 0.6093 (0.6571)	loss 1.2584 (1.3230)	grad_norm 4.8548 (5.4536)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:57:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:12:03 lr 0.000025	 wd 0.0000	time 0.6228 (0.6566)	loss 1.4251 (1.3240)	grad_norm 3.4924 (5.4502)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:58:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:10:57 lr 0.000025	 wd 0.0000	time 0.6254 (0.6561)	loss 1.5693 (1.3234)	grad_norm 5.1010 (5.4514)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-29 23:59:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:09:51 lr 0.000025	 wd 0.0000	time 0.6240 (0.6558)	loss 1.1849 (1.3261)	grad_norm 4.4160 (5.4784)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:00:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:08:45 lr 0.000025	 wd 0.0000	time 0.6170 (0.6556)	loss 1.4536 (1.3263)	grad_norm 4.6820 (5.4786)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:01:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:07:40 lr 0.000025	 wd 0.0000	time 0.6404 (0.6554)	loss 0.9955 (1.3251)	grad_norm 5.0163 (5.4810)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:02:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:06:34 lr 0.000024	 wd 0.0000	time 0.6175 (0.6552)	loss 1.4909 (1.3254)	grad_norm 3.8267 (5.4837)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:03:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:05:28 lr 0.000024	 wd 0.0000	time 0.6239 (0.6549)	loss 1.7498 (1.3274)	grad_norm 5.1567 (5.4680)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:04:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:04:23 lr 0.000024	 wd 0.0000	time 0.6185 (0.6548)	loss 1.4845 (1.3273)	grad_norm 4.4009 (5.4555)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:05:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:03:17 lr 0.000024	 wd 0.0000	time 0.6156 (0.6545)	loss 0.9416 (1.3290)	grad_norm 5.0805 (5.4374)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:06:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:02:12 lr 0.000024	 wd 0.0000	time 0.6232 (0.6544)	loss 1.5801 (1.3296)	grad_norm 6.3221 (5.4490)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:07:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:06 lr 0.000024	 wd 0.0000	time 0.6217 (0.6543)	loss 1.2183 (1.3289)	grad_norm 9.7013 (5.4581)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:08:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000024	 wd 0.0000	time 0.6209 (0.6541)	loss 1.2223 (1.3290)	grad_norm 3.5044 (5.4599)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:08:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 15 training takes 0:27:19
[2024-07-30 00:09:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.542 (11.542)	Loss 0.3970 (0.3970)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 00:09:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.682 Acc@5 97.416
[2024-07-30 00:09:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-30 00:09:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.68%
[2024-07-30 00:09:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 00:09:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 00:09:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:02:57 lr 0.000024	 wd 0.0000	time 10.1431 (10.1431)	loss 1.3195 (1.3195)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:10:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:29:47 lr 0.000024	 wd 0.0000	time 0.6223 (0.7441)	loss 1.5101 (1.3301)	grad_norm 3.3860 (5.0928)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:11:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:26:47 lr 0.000024	 wd 0.0000	time 0.6461 (0.6982)	loss 1.2910 (1.3359)	grad_norm 5.0388 (5.0831)	loss_scale 2048.0000 (1125.8905)	mem 17424MB
[2024-07-30 00:12:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:25:01 lr 0.000024	 wd 0.0000	time 0.6234 (0.6821)	loss 1.3347 (1.3466)	grad_norm 4.8424 (4.9887)	loss_scale 2048.0000 (1432.2392)	mem 17424MB
[2024-07-30 00:14:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:23:36 lr 0.000024	 wd 0.0000	time 0.6229 (0.6740)	loss 1.1596 (1.3412)	grad_norm 3.4215 (5.0181)	loss_scale 2048.0000 (1585.7955)	mem 17424MB
[2024-07-30 00:15:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:22:19 lr 0.000023	 wd 0.0000	time 0.6428 (0.6692)	loss 1.3288 (1.3478)	grad_norm 4.5623 (5.3766)	loss_scale 2048.0000 (1678.0519)	mem 17424MB
[2024-07-30 00:16:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:21:07 lr 0.000023	 wd 0.0000	time 0.6267 (0.6662)	loss 0.9693 (1.3388)	grad_norm 4.3219 (5.4588)	loss_scale 2048.0000 (1739.6073)	mem 17424MB
[2024-07-30 00:17:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:19:56 lr 0.000023	 wd 0.0000	time 0.6206 (0.6640)	loss 1.3378 (1.3400)	grad_norm 6.0566 (5.4794)	loss_scale 2048.0000 (1783.6006)	mem 17424MB
[2024-07-30 00:18:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:18:47 lr 0.000023	 wd 0.0000	time 0.6215 (0.6623)	loss 1.3245 (1.3336)	grad_norm 4.8463 (5.5141)	loss_scale 2048.0000 (1816.6092)	mem 17424MB
[2024-07-30 00:19:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:17:39 lr 0.000023	 wd 0.0000	time 0.6260 (0.6611)	loss 1.5267 (1.3319)	grad_norm 3.9484 (5.5186)	loss_scale 2048.0000 (1842.2908)	mem 17424MB
[2024-07-30 00:20:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:16:31 lr 0.000023	 wd 0.0000	time 0.6216 (0.6600)	loss 1.3652 (1.3332)	grad_norm 6.6562 (5.4818)	loss_scale 2048.0000 (1862.8412)	mem 17424MB
[2024-07-30 00:21:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:15:24 lr 0.000023	 wd 0.0000	time 0.6338 (0.6592)	loss 1.4546 (1.3351)	grad_norm 4.5749 (inf)	loss_scale 1024.0000 (1823.8547)	mem 17424MB
[2024-07-30 00:22:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:14:17 lr 0.000023	 wd 0.0000	time 0.6171 (0.6585)	loss 1.5907 (1.3356)	grad_norm 4.3711 (inf)	loss_scale 1024.0000 (1757.2556)	mem 17424MB
[2024-07-30 00:23:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:13:10 lr 0.000023	 wd 0.0000	time 0.6215 (0.6578)	loss 1.1065 (1.3355)	grad_norm 5.1211 (inf)	loss_scale 1024.0000 (1700.8947)	mem 17424MB
[2024-07-30 00:24:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:12:04 lr 0.000023	 wd 0.0000	time 0.6237 (0.6573)	loss 1.2147 (1.3330)	grad_norm 6.7512 (inf)	loss_scale 1024.0000 (1652.5796)	mem 17424MB
[2024-07-30 00:25:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:10:58 lr 0.000022	 wd 0.0000	time 0.6258 (0.6569)	loss 1.3038 (1.3301)	grad_norm 4.7940 (inf)	loss_scale 1024.0000 (1610.7022)	mem 17424MB
[2024-07-30 00:27:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:09:52 lr 0.000022	 wd 0.0000	time 0.6151 (0.6566)	loss 1.3196 (1.3291)	grad_norm 6.5350 (inf)	loss_scale 1024.0000 (1574.0562)	mem 17424MB
[2024-07-30 00:28:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:08:46 lr 0.000022	 wd 0.0000	time 0.6175 (0.6563)	loss 1.1297 (1.3278)	grad_norm 7.6659 (inf)	loss_scale 1024.0000 (1541.7190)	mem 17424MB
[2024-07-30 00:29:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:07:40 lr 0.000022	 wd 0.0000	time 0.6201 (0.6560)	loss 1.3141 (1.3273)	grad_norm 4.9332 (inf)	loss_scale 1024.0000 (1512.9728)	mem 17424MB
[2024-07-30 00:30:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:06:34 lr 0.000022	 wd 0.0000	time 0.6331 (0.6558)	loss 1.1717 (1.3265)	grad_norm 3.8151 (inf)	loss_scale 1024.0000 (1487.2509)	mem 17424MB
[2024-07-30 00:31:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:05:29 lr 0.000022	 wd 0.0000	time 0.6259 (0.6557)	loss 1.3662 (1.3268)	grad_norm 4.8527 (inf)	loss_scale 1024.0000 (1464.1000)	mem 17424MB
[2024-07-30 00:32:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:04:23 lr 0.000022	 wd 0.0000	time 0.6196 (0.6555)	loss 1.3745 (1.3258)	grad_norm 5.6413 (inf)	loss_scale 1024.0000 (1443.1528)	mem 17424MB
[2024-07-30 00:33:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:03:17 lr 0.000022	 wd 0.0000	time 0.6309 (0.6554)	loss 1.3785 (1.3263)	grad_norm 3.8749 (inf)	loss_scale 1024.0000 (1424.1090)	mem 17424MB
[2024-07-30 00:34:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:02:12 lr 0.000022	 wd 0.0000	time 0.6241 (0.6552)	loss 1.2893 (1.3266)	grad_norm 9.5844 (inf)	loss_scale 1024.0000 (1406.7206)	mem 17424MB
[2024-07-30 00:35:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:06 lr 0.000022	 wd 0.0000	time 0.6188 (0.6550)	loss 1.3798 (1.3265)	grad_norm 6.6604 (inf)	loss_scale 1024.0000 (1390.7805)	mem 17424MB
[2024-07-30 00:36:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000021	 wd 0.0000	time 0.6154 (0.6547)	loss 1.3699 (1.3268)	grad_norm 4.2442 (inf)	loss_scale 1024.0000 (1376.1152)	mem 17424MB
[2024-07-30 00:36:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 16 training takes 0:27:20
[2024-07-30 00:37:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.699 (11.699)	Loss 0.3926 (0.3926)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-30 00:37:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.574 Acc@5 97.402
[2024-07-30 00:37:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-30 00:37:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.68%
[2024-07-30 00:37:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:50:42 lr 0.000021	 wd 0.0000	time 11.2878 (11.2878)	loss 1.4917 (1.4917)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:38:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:30:20 lr 0.000021	 wd 0.0000	time 0.6215 (0.7580)	loss 1.6824 (1.3006)	grad_norm 4.4755 (5.4276)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:39:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:27:00 lr 0.000021	 wd 0.0000	time 0.6247 (0.7040)	loss 1.5453 (1.3133)	grad_norm 4.1302 (5.3741)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:40:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:25:09 lr 0.000021	 wd 0.0000	time 0.6263 (0.6856)	loss 1.4213 (1.3124)	grad_norm 5.0674 (5.3974)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:41:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:23:42 lr 0.000021	 wd 0.0000	time 0.6169 (0.6766)	loss 1.1130 (1.3109)	grad_norm 3.4986 (5.4394)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:42:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:22:23 lr 0.000021	 wd 0.0000	time 0.6189 (0.6713)	loss 1.2442 (1.3098)	grad_norm 5.4139 (5.4625)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:44:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:21:10 lr 0.000021	 wd 0.0000	time 0.6269 (0.6677)	loss 1.1578 (1.3157)	grad_norm 4.0971 (5.7616)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:45:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:19:58 lr 0.000021	 wd 0.0000	time 0.6317 (0.6653)	loss 1.3781 (1.3211)	grad_norm 4.1508 (5.7087)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:46:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:18:48 lr 0.000021	 wd 0.0000	time 0.6266 (0.6633)	loss 1.2840 (1.3251)	grad_norm 11.6915 (5.6776)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:47:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:17:40 lr 0.000021	 wd 0.0000	time 0.6220 (0.6618)	loss 1.3697 (1.3268)	grad_norm 5.0939 (5.7005)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:48:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:16:32 lr 0.000020	 wd 0.0000	time 0.6235 (0.6606)	loss 1.4720 (1.3258)	grad_norm 4.1754 (5.6590)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:49:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:15:24 lr 0.000020	 wd 0.0000	time 0.6239 (0.6596)	loss 1.0871 (1.3250)	grad_norm 4.1660 (5.6538)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:50:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:14:17 lr 0.000020	 wd 0.0000	time 0.6167 (0.6588)	loss 1.4904 (1.3274)	grad_norm 4.3335 (5.6306)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:51:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:13:10 lr 0.000020	 wd 0.0000	time 0.6142 (0.6580)	loss 1.2531 (1.3266)	grad_norm 3.9310 (5.6136)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:52:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:12:04 lr 0.000020	 wd 0.0000	time 0.6219 (0.6575)	loss 1.4432 (1.3273)	grad_norm 5.3420 (5.6034)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:53:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:10:58 lr 0.000020	 wd 0.0000	time 0.6225 (0.6569)	loss 1.4381 (1.3281)	grad_norm 5.0278 (5.5991)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:54:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:09:52 lr 0.000020	 wd 0.0000	time 0.6209 (0.6565)	loss 1.4545 (1.3292)	grad_norm 8.0359 (5.5902)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:55:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:08:46 lr 0.000020	 wd 0.0000	time 0.6228 (0.6562)	loss 1.6774 (1.3310)	grad_norm 4.8006 (5.5787)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:57:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:07:40 lr 0.000020	 wd 0.0000	time 0.6237 (0.6559)	loss 1.3671 (1.3315)	grad_norm 4.8877 (5.5582)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:58:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:06:34 lr 0.000020	 wd 0.0000	time 0.6149 (0.6557)	loss 1.1603 (1.3319)	grad_norm 10.2733 (5.5559)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 00:59:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:05:29 lr 0.000019	 wd 0.0000	time 0.6176 (0.6554)	loss 1.3893 (1.3301)	grad_norm 8.1475 (5.5383)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:00:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:04:23 lr 0.000019	 wd 0.0000	time 0.6366 (0.6552)	loss 1.3943 (1.3297)	grad_norm 7.5588 (5.5400)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:01:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:03:17 lr 0.000019	 wd 0.0000	time 0.6204 (0.6550)	loss 1.2677 (1.3308)	grad_norm 9.4310 (5.5512)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:02:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:02:12 lr 0.000019	 wd 0.0000	time 0.6250 (0.6548)	loss 1.4687 (1.3301)	grad_norm 4.8400 (5.6002)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:03:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:06 lr 0.000019	 wd 0.0000	time 0.6189 (0.6546)	loss 1.4128 (1.3287)	grad_norm 4.4862 (5.6001)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:04:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0000	time 0.6161 (0.6543)	loss 1.3943 (1.3291)	grad_norm 3.3202 (5.5991)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:04:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 17 training takes 0:27:19
[2024-07-30 01:04:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.124 (11.124)	Loss 0.4004 (0.4004)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-30 01:05:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.814 Acc@5 97.436
[2024-07-30 01:05:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-30 01:05:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.81%
[2024-07-30 01:05:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 01:05:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 01:05:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:19:57 lr 0.000019	 wd 0.0000	time 10.5506 (10.5506)	loss 1.5925 (1.5925)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:06:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:29:56 lr 0.000019	 wd 0.0000	time 0.6179 (0.7480)	loss 1.5004 (1.3642)	grad_norm 4.9373 (4.9328)	loss_scale 2048.0000 (1672.8713)	mem 17424MB
[2024-07-30 01:07:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:26:50 lr 0.000019	 wd 0.0000	time 0.6165 (0.6997)	loss 1.6058 (1.3421)	grad_norm 4.5243 (4.9627)	loss_scale 2048.0000 (1859.5025)	mem 17424MB
[2024-07-30 01:08:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:25:04 lr 0.000019	 wd 0.0000	time 0.6227 (0.6831)	loss 1.4508 (1.3394)	grad_norm 4.8983 (5.1028)	loss_scale 2048.0000 (1922.1262)	mem 17424MB
[2024-07-30 01:09:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:23:38 lr 0.000019	 wd 0.0000	time 0.6211 (0.6749)	loss 1.4467 (1.3327)	grad_norm 4.3048 (5.1423)	loss_scale 2048.0000 (1953.5162)	mem 17424MB
[2024-07-30 01:10:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:22:20 lr 0.000018	 wd 0.0000	time 0.6256 (0.6698)	loss 1.1875 (1.3249)	grad_norm 6.6465 (5.3707)	loss_scale 2048.0000 (1972.3752)	mem 17424MB
[2024-07-30 01:11:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:21:07 lr 0.000018	 wd 0.0000	time 0.6235 (0.6665)	loss 1.4247 (1.3232)	grad_norm 4.0935 (5.3330)	loss_scale 2048.0000 (1984.9584)	mem 17424MB
[2024-07-30 01:12:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:19:57 lr 0.000018	 wd 0.0000	time 0.6192 (0.6644)	loss 1.4620 (1.3234)	grad_norm 4.1087 (5.3241)	loss_scale 2048.0000 (1993.9515)	mem 17424MB
[2024-07-30 01:14:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:18:47 lr 0.000018	 wd 0.0000	time 0.6293 (0.6626)	loss 1.1126 (1.3216)	grad_norm 4.5006 (5.3067)	loss_scale 2048.0000 (2000.6991)	mem 17424MB
[2024-07-30 01:15:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:17:39 lr 0.000018	 wd 0.0000	time 0.6192 (0.6613)	loss 1.2329 (1.3182)	grad_norm 7.6198 (5.3002)	loss_scale 2048.0000 (2005.9489)	mem 17424MB
[2024-07-30 01:16:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:16:31 lr 0.000018	 wd 0.0000	time 0.6250 (0.6602)	loss 1.0837 (1.3222)	grad_norm 4.6497 (5.3369)	loss_scale 2048.0000 (2010.1499)	mem 17424MB
[2024-07-30 01:17:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:15:24 lr 0.000018	 wd 0.0000	time 0.6237 (0.6593)	loss 1.3008 (1.3215)	grad_norm 4.7312 (5.3171)	loss_scale 2048.0000 (2013.5876)	mem 17424MB
[2024-07-30 01:18:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:14:17 lr 0.000018	 wd 0.0000	time 0.6228 (0.6586)	loss 1.6055 (1.3221)	grad_norm 5.4381 (5.3269)	loss_scale 2048.0000 (2016.4530)	mem 17424MB
[2024-07-30 01:19:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:13:10 lr 0.000018	 wd 0.0000	time 0.6284 (0.6579)	loss 1.5978 (1.3233)	grad_norm 4.5363 (5.3381)	loss_scale 2048.0000 (2018.8778)	mem 17424MB
[2024-07-30 01:20:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:12:04 lr 0.000018	 wd 0.0000	time 0.6209 (0.6573)	loss 1.1565 (1.3234)	grad_norm 4.8555 (5.3449)	loss_scale 2048.0000 (2020.9565)	mem 17424MB
[2024-07-30 01:21:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:10:58 lr 0.000017	 wd 0.0000	time 0.6282 (0.6568)	loss 1.4386 (1.3224)	grad_norm 4.9766 (5.3565)	loss_scale 2048.0000 (2022.7582)	mem 17424MB
[2024-07-30 01:22:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:09:52 lr 0.000017	 wd 0.0000	time 0.6177 (0.6564)	loss 1.5004 (1.3218)	grad_norm 5.5843 (5.4034)	loss_scale 2048.0000 (2024.3348)	mem 17424MB
[2024-07-30 01:23:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:08:46 lr 0.000017	 wd 0.0000	time 0.6293 (0.6561)	loss 1.5305 (1.3211)	grad_norm 4.6402 (5.4215)	loss_scale 2048.0000 (2025.7260)	mem 17424MB
[2024-07-30 01:24:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:07:40 lr 0.000017	 wd 0.0000	time 0.6261 (0.6558)	loss 1.3059 (1.3229)	grad_norm 4.5274 (nan)	loss_scale 1024.0000 (1986.0255)	mem 17424MB
[2024-07-30 01:25:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:06:34 lr 0.000017	 wd 0.0000	time 0.6147 (0.6556)	loss 1.4701 (1.3223)	grad_norm 3.3724 (nan)	loss_scale 1024.0000 (1935.4193)	mem 17424MB
[2024-07-30 01:27:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:05:29 lr 0.000017	 wd 0.0000	time 0.6189 (0.6554)	loss 1.4760 (1.3236)	grad_norm 4.9147 (nan)	loss_scale 1024.0000 (1889.8711)	mem 17424MB
[2024-07-30 01:28:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:04:23 lr 0.000017	 wd 0.0000	time 0.6138 (0.6552)	loss 1.4033 (1.3227)	grad_norm 4.9902 (nan)	loss_scale 1024.0000 (1848.6587)	mem 17424MB
[2024-07-30 01:29:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:03:17 lr 0.000017	 wd 0.0000	time 0.6054 (0.6550)	loss 1.3507 (1.3231)	grad_norm 3.5648 (nan)	loss_scale 1024.0000 (1811.1913)	mem 17424MB
[2024-07-30 01:30:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:02:12 lr 0.000017	 wd 0.0000	time 0.6285 (0.6549)	loss 1.3443 (1.3235)	grad_norm 4.8861 (nan)	loss_scale 1024.0000 (1776.9804)	mem 17424MB
[2024-07-30 01:31:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:06 lr 0.000017	 wd 0.0000	time 0.6248 (0.6547)	loss 1.5099 (1.3242)	grad_norm 6.8762 (nan)	loss_scale 1024.0000 (1745.6193)	mem 17424MB
[2024-07-30 01:32:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0000	time 0.6230 (0.6544)	loss 1.3439 (1.3230)	grad_norm 4.0332 (nan)	loss_scale 1024.0000 (1716.7661)	mem 17424MB
[2024-07-30 01:32:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 18 training takes 0:27:20
[2024-07-30 01:32:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.827 (11.827)	Loss 0.3906 (0.3906)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 01:32:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.816 Acc@5 97.506
[2024-07-30 01:32:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-30 01:32:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.82%
[2024-07-30 01:32:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 01:33:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 01:33:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:14:13 lr 0.000016	 wd 0.0000	time 10.4131 (10.4131)	loss 1.2720 (1.2720)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:34:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:29:52 lr 0.000016	 wd 0.0000	time 0.6166 (0.7463)	loss 1.1303 (1.3357)	grad_norm 9.0093 (5.1097)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:35:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:26:46 lr 0.000016	 wd 0.0000	time 0.6184 (0.6980)	loss 1.3769 (1.3309)	grad_norm 6.1101 (5.3760)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:36:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:25:02 lr 0.000016	 wd 0.0000	time 0.6241 (0.6825)	loss 1.2493 (1.3417)	grad_norm 5.1604 (5.4447)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:37:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:23:37 lr 0.000016	 wd 0.0000	time 0.6219 (0.6743)	loss 1.4438 (1.3371)	grad_norm 4.1913 (5.5269)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:38:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:22:20 lr 0.000016	 wd 0.0000	time 0.6237 (0.6695)	loss 1.4967 (1.3368)	grad_norm 5.8247 (5.4944)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:39:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:21:07 lr 0.000016	 wd 0.0000	time 0.6158 (0.6663)	loss 1.3241 (1.3293)	grad_norm 3.7434 (5.5156)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:40:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:19:56 lr 0.000016	 wd 0.0000	time 0.6226 (0.6639)	loss 1.6428 (1.3302)	grad_norm 4.3989 (5.6176)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:41:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:18:46 lr 0.000016	 wd 0.0000	time 0.6182 (0.6621)	loss 1.4111 (1.3271)	grad_norm 10.3910 (5.5612)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:42:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:17:38 lr 0.000016	 wd 0.0000	time 0.6356 (0.6607)	loss 1.2233 (1.3276)	grad_norm 4.4652 (5.5799)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:44:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:16:30 lr 0.000016	 wd 0.0000	time 0.6252 (0.6596)	loss 1.4796 (1.3288)	grad_norm 5.4901 (5.5874)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:45:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:15:23 lr 0.000015	 wd 0.0000	time 0.6255 (0.6588)	loss 0.8037 (1.3277)	grad_norm 5.4795 (5.5692)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:46:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:14:16 lr 0.000015	 wd 0.0000	time 0.6153 (0.6580)	loss 1.1145 (1.3258)	grad_norm 4.8762 (5.5784)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:47:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:13:10 lr 0.000015	 wd 0.0000	time 0.6311 (0.6574)	loss 1.2868 (1.3260)	grad_norm 5.9193 (5.5584)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:48:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:12:04 lr 0.000015	 wd 0.0000	time 0.6218 (0.6570)	loss 1.5016 (1.3265)	grad_norm 5.1922 (5.5787)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:49:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:10:57 lr 0.000015	 wd 0.0000	time 0.6234 (0.6566)	loss 1.2062 (1.3263)	grad_norm 3.1204 (5.5719)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:50:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:09:51 lr 0.000015	 wd 0.0000	time 0.6174 (0.6562)	loss 1.4417 (1.3266)	grad_norm 4.5645 (5.5575)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:51:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:08:46 lr 0.000015	 wd 0.0000	time 0.6320 (0.6559)	loss 1.4799 (1.3274)	grad_norm 4.2604 (5.5503)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:52:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:07:40 lr 0.000015	 wd 0.0000	time 0.6216 (0.6556)	loss 1.2692 (1.3272)	grad_norm 5.1506 (5.5515)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:53:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:06:34 lr 0.000015	 wd 0.0000	time 0.6254 (0.6553)	loss 1.4062 (1.3282)	grad_norm 4.0555 (5.5278)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:54:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:05:28 lr 0.000015	 wd 0.0000	time 0.6161 (0.6551)	loss 0.9875 (1.3276)	grad_norm 5.0028 (5.5152)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:55:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:04:23 lr 0.000014	 wd 0.0000	time 0.6176 (0.6550)	loss 1.2000 (1.3261)	grad_norm 5.4860 (5.5385)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:57:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:03:17 lr 0.000014	 wd 0.0000	time 0.6161 (0.6548)	loss 1.4339 (1.3244)	grad_norm 3.7935 (5.5255)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:58:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:02:12 lr 0.000014	 wd 0.0000	time 0.6110 (0.6546)	loss 1.2548 (1.3235)	grad_norm 4.1825 (5.5141)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 01:59:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:06 lr 0.000014	 wd 0.0000	time 0.6191 (0.6544)	loss 1.4109 (1.3245)	grad_norm 6.0479 (5.5253)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:00:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0000	time 0.6246 (0.6542)	loss 1.2048 (1.3245)	grad_norm 4.5431 (5.5573)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:00:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 19 training takes 0:27:19
[2024-07-30 02:00:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.107 (11.107)	Loss 0.3914 (0.3914)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 17424MB
[2024-07-30 02:00:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.848 Acc@5 97.506
[2024-07-30 02:00:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-30 02:00:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.85%
[2024-07-30 02:00:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 02:00:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 02:01:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:09:59 lr 0.000014	 wd 0.0000	time 10.3116 (10.3116)	loss 0.9499 (0.9499)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:02:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:29:49 lr 0.000014	 wd 0.0000	time 0.6219 (0.7451)	loss 1.5303 (1.3138)	grad_norm 4.2541 (5.3038)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:03:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:26:45 lr 0.000014	 wd 0.0000	time 0.6192 (0.6976)	loss 1.3707 (1.3098)	grad_norm 4.4058 (5.2008)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:04:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:25:00 lr 0.000014	 wd 0.0000	time 0.6200 (0.6816)	loss 1.5452 (1.3110)	grad_norm 4.7235 (5.2509)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:05:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:23:36 lr 0.000014	 wd 0.0000	time 0.6174 (0.6739)	loss 1.4847 (1.3200)	grad_norm 7.5834 (5.4324)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:06:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:22:19 lr 0.000014	 wd 0.0000	time 0.6047 (0.6692)	loss 1.3343 (1.3235)	grad_norm 3.7921 (5.4118)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:07:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:21:06 lr 0.000014	 wd 0.0000	time 0.6209 (0.6661)	loss 1.3122 (1.3204)	grad_norm 4.8234 (5.4338)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:08:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:19:56 lr 0.000013	 wd 0.0000	time 0.6327 (0.6638)	loss 1.3695 (1.3225)	grad_norm 4.5874 (5.4490)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:09:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:18:46 lr 0.000013	 wd 0.0000	time 0.6230 (0.6621)	loss 1.2268 (1.3200)	grad_norm 3.9131 (5.4135)	loss_scale 2048.0000 (1121.1586)	mem 17424MB
[2024-07-30 02:10:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:17:38 lr 0.000013	 wd 0.0000	time 0.6298 (0.6610)	loss 1.4309 (1.3229)	grad_norm 4.0237 (5.3755)	loss_scale 2048.0000 (1224.0266)	mem 17424MB
[2024-07-30 02:11:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:16:31 lr 0.000013	 wd 0.0000	time 0.6131 (0.6599)	loss 1.3721 (1.3226)	grad_norm 4.5689 (5.3792)	loss_scale 2048.0000 (1306.3417)	mem 17424MB
[2024-07-30 02:12:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:15:24 lr 0.000013	 wd 0.0000	time 0.6248 (0.6591)	loss 1.4005 (1.3197)	grad_norm 5.3872 (5.4143)	loss_scale 2048.0000 (1373.7039)	mem 17424MB
[2024-07-30 02:14:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:14:17 lr 0.000013	 wd 0.0000	time 0.6241 (0.6583)	loss 1.3411 (1.3159)	grad_norm 5.7994 (5.3952)	loss_scale 2048.0000 (1429.8485)	mem 17424MB
[2024-07-30 02:15:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:13:10 lr 0.000013	 wd 0.0000	time 0.6150 (0.6577)	loss 1.1734 (1.3143)	grad_norm 7.1208 (5.4001)	loss_scale 2048.0000 (1477.3620)	mem 17424MB
[2024-07-30 02:16:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:12:04 lr 0.000013	 wd 0.0000	time 0.6201 (0.6572)	loss 1.4272 (1.3160)	grad_norm 5.2098 (5.4215)	loss_scale 2048.0000 (1518.0928)	mem 17424MB
[2024-07-30 02:17:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:10:57 lr 0.000013	 wd 0.0000	time 0.6196 (0.6566)	loss 1.2476 (1.3172)	grad_norm 7.5431 (inf)	loss_scale 1024.0000 (1498.8195)	mem 17424MB
[2024-07-30 02:18:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:09:51 lr 0.000013	 wd 0.0000	time 0.6288 (0.6562)	loss 1.6713 (1.3174)	grad_norm 5.0033 (inf)	loss_scale 1024.0000 (1469.1618)	mem 17424MB
[2024-07-30 02:19:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:08:46 lr 0.000012	 wd 0.0000	time 0.6026 (0.6559)	loss 1.4807 (1.3170)	grad_norm 5.4579 (inf)	loss_scale 1024.0000 (1442.9912)	mem 17424MB
[2024-07-30 02:20:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:07:40 lr 0.000012	 wd 0.0000	time 0.6255 (0.6557)	loss 1.2883 (1.3163)	grad_norm 7.0453 (inf)	loss_scale 1024.0000 (1419.7268)	mem 17424MB
[2024-07-30 02:21:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:06:34 lr 0.000012	 wd 0.0000	time 0.6168 (0.6555)	loss 1.3979 (1.3180)	grad_norm 3.4103 (inf)	loss_scale 1024.0000 (1398.9100)	mem 17424MB
[2024-07-30 02:22:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:05:28 lr 0.000012	 wd 0.0000	time 0.6271 (0.6552)	loss 1.2925 (1.3182)	grad_norm 4.3966 (inf)	loss_scale 1024.0000 (1380.1739)	mem 17424MB
[2024-07-30 02:23:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:04:23 lr 0.000012	 wd 0.0000	time 0.6236 (0.6550)	loss 1.2714 (1.3186)	grad_norm 5.1154 (inf)	loss_scale 1024.0000 (1363.2213)	mem 17424MB
[2024-07-30 02:24:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:03:17 lr 0.000012	 wd 0.0000	time 0.6142 (0.6548)	loss 1.1623 (1.3199)	grad_norm 4.5446 (inf)	loss_scale 1024.0000 (1347.8092)	mem 17424MB
[2024-07-30 02:25:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:02:12 lr 0.000012	 wd 0.0000	time 0.6187 (0.6546)	loss 1.3341 (1.3200)	grad_norm 4.6108 (inf)	loss_scale 1024.0000 (1333.7366)	mem 17424MB
[2024-07-30 02:27:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:06 lr 0.000012	 wd 0.0000	time 0.6218 (0.6544)	loss 1.2196 (1.3204)	grad_norm 11.0919 (inf)	loss_scale 1024.0000 (1320.8363)	mem 17424MB
[2024-07-30 02:28:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0000	time 0.6219 (0.6542)	loss 1.3783 (1.3211)	grad_norm 4.2336 (inf)	loss_scale 1024.0000 (1308.9676)	mem 17424MB
[2024-07-30 02:28:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 20 training takes 0:27:19
[2024-07-30 02:28:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 145): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_20.pth saving......
[2024-07-30 02:28:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 147): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_20.pth saved !!!
[2024-07-30 02:28:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 10.914 (10.914)	Loss 0.3960 (0.3960)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 02:28:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.912 Acc@5 97.500
[2024-07-30 02:28:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-30 02:28:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.91%
[2024-07-30 02:28:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 02:28:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 02:28:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:17:26 lr 0.000012	 wd 0.0000	time 10.4902 (10.4902)	loss 1.4858 (1.4858)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:29:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:29:53 lr 0.000012	 wd 0.0000	time 0.6236 (0.7467)	loss 1.3564 (1.3500)	grad_norm 3.9703 (5.6634)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:31:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:26:47 lr 0.000012	 wd 0.0000	time 0.6271 (0.6985)	loss 0.8664 (1.3230)	grad_norm 12.5068 (6.2594)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:32:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:25:03 lr 0.000012	 wd 0.0000	time 0.6424 (0.6828)	loss 1.2983 (1.3088)	grad_norm 5.5339 (6.1426)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:33:13 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:23:38 lr 0.000011	 wd 0.0000	time 0.6240 (0.6747)	loss 1.5257 (1.3103)	grad_norm 4.1967 (6.0599)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:34:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:22:20 lr 0.000011	 wd 0.0000	time 0.6238 (0.6697)	loss 1.3199 (1.3106)	grad_norm 4.5031 (5.8496)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:35:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:21:07 lr 0.000011	 wd 0.0000	time 0.6313 (0.6665)	loss 1.2103 (1.3118)	grad_norm 3.8096 (5.8651)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:36:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:19:57 lr 0.000011	 wd 0.0000	time 0.6254 (0.6644)	loss 1.3448 (1.3093)	grad_norm 5.5694 (5.7899)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:37:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:18:47 lr 0.000011	 wd 0.0000	time 0.6244 (0.6627)	loss 1.1340 (1.3077)	grad_norm 4.7199 (5.7070)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:38:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:17:39 lr 0.000011	 wd 0.0000	time 0.6185 (0.6613)	loss 1.5134 (1.3070)	grad_norm 4.9029 (5.8215)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:39:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:16:31 lr 0.000011	 wd 0.0000	time 0.6235 (0.6602)	loss 1.0326 (1.3062)	grad_norm 4.7627 (5.8583)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:40:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:15:24 lr 0.000011	 wd 0.0000	time 0.6163 (0.6593)	loss 1.4734 (1.3065)	grad_norm 6.1998 (5.8589)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:41:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:14:17 lr 0.000011	 wd 0.0000	time 0.6211 (0.6586)	loss 1.3893 (1.3082)	grad_norm 4.6210 (5.8333)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:42:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:13:10 lr 0.000011	 wd 0.0000	time 0.6282 (0.6580)	loss 1.3241 (1.3088)	grad_norm 3.3402 (5.8315)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:44:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:12:04 lr 0.000011	 wd 0.0000	time 0.6272 (0.6574)	loss 1.5191 (1.3102)	grad_norm 7.1691 (5.8138)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:45:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:10:58 lr 0.000010	 wd 0.0000	time 0.6283 (0.6570)	loss 1.4798 (1.3090)	grad_norm 5.0997 (5.8502)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:46:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:09:52 lr 0.000010	 wd 0.0000	time 0.6262 (0.6565)	loss 1.2137 (1.3096)	grad_norm 6.3226 (5.8491)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:47:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:08:46 lr 0.000010	 wd 0.0000	time 0.6131 (0.6563)	loss 1.3624 (1.3102)	grad_norm 4.6111 (5.8292)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:48:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:07:40 lr 0.000010	 wd 0.0000	time 0.6201 (0.6560)	loss 1.1629 (1.3108)	grad_norm 4.8579 (5.8546)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:49:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:06:34 lr 0.000010	 wd 0.0000	time 0.6254 (0.6557)	loss 1.1002 (1.3090)	grad_norm 5.2205 (5.8518)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:50:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:05:29 lr 0.000010	 wd 0.0000	time 0.6262 (0.6555)	loss 1.2409 (1.3087)	grad_norm 4.7756 (5.8535)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:51:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:04:23 lr 0.000010	 wd 0.0000	time 0.6206 (0.6553)	loss 1.1068 (1.3092)	grad_norm 5.3307 (5.9446)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:52:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:03:17 lr 0.000010	 wd 0.0000	time 0.6124 (0.6551)	loss 1.3023 (1.3067)	grad_norm 4.1538 (5.9378)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:53:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:02:12 lr 0.000010	 wd 0.0000	time 0.6256 (0.6549)	loss 1.4448 (1.3057)	grad_norm 4.6916 (5.9164)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:54:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:06 lr 0.000010	 wd 0.0000	time 0.6249 (0.6547)	loss 1.4121 (1.3076)	grad_norm 3.3506 (5.8919)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:55:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000010	 wd 0.0000	time 0.6251 (0.6544)	loss 1.1885 (1.3068)	grad_norm 6.8904 (5.8814)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:56:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 21 training takes 0:27:20
[2024-07-30 02:56:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.922 (11.922)	Loss 0.3958 (0.3958)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 02:56:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.894 Acc@5 97.470
[2024-07-30 02:56:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-30 02:56:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.91%
[2024-07-30 02:56:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:05:24 lr 0.000010	 wd 0.0000	time 11.6405 (11.6405)	loss 1.2068 (1.2068)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:57:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:30:23 lr 0.000010	 wd 0.0000	time 0.6227 (0.7593)	loss 1.3381 (1.3206)	grad_norm 5.9936 (6.3216)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:58:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:27:03 lr 0.000009	 wd 0.0000	time 0.6304 (0.7054)	loss 1.0088 (1.3127)	grad_norm 4.8298 (6.4012)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 02:59:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:25:12 lr 0.000009	 wd 0.0000	time 0.6185 (0.6866)	loss 1.4877 (1.3123)	grad_norm 5.6362 (5.9858)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:01:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:23:44 lr 0.000009	 wd 0.0000	time 0.6192 (0.6775)	loss 1.3178 (1.3189)	grad_norm 7.6878 (5.9409)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:02:08 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:22:25 lr 0.000009	 wd 0.0000	time 0.6128 (0.6719)	loss 0.8635 (1.3183)	grad_norm 4.7484 (5.7648)	loss_scale 2048.0000 (1195.6886)	mem 17424MB
[2024-07-30 03:03:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:21:11 lr 0.000009	 wd 0.0000	time 0.6289 (0.6684)	loss 1.2647 (1.3199)	grad_norm 7.4532 (5.6967)	loss_scale 2048.0000 (1337.5042)	mem 17424MB
[2024-07-30 03:04:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:19:59 lr 0.000009	 wd 0.0000	time 0.6266 (0.6658)	loss 0.9062 (1.3155)	grad_norm 5.8327 (5.6552)	loss_scale 2048.0000 (1438.8588)	mem 17424MB
[2024-07-30 03:05:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:18:49 lr 0.000009	 wd 0.0000	time 0.6250 (0.6638)	loss 1.2483 (1.3127)	grad_norm 4.7277 (5.6192)	loss_scale 2048.0000 (1514.9064)	mem 17424MB
[2024-07-30 03:06:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:17:41 lr 0.000009	 wd 0.0000	time 0.6188 (0.6624)	loss 1.4900 (1.3125)	grad_norm 5.4082 (5.7199)	loss_scale 2048.0000 (1574.0733)	mem 17424MB
[2024-07-30 03:07:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:16:33 lr 0.000009	 wd 0.0000	time 0.6236 (0.6612)	loss 1.5218 (1.3140)	grad_norm 5.3702 (5.7498)	loss_scale 2048.0000 (1621.4186)	mem 17424MB
[2024-07-30 03:08:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:15:25 lr 0.000009	 wd 0.0000	time 0.6196 (0.6602)	loss 1.2967 (1.3120)	grad_norm 4.8870 (5.8726)	loss_scale 2048.0000 (1660.1635)	mem 17424MB
[2024-07-30 03:09:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:14:18 lr 0.000009	 wd 0.0000	time 0.6209 (0.6593)	loss 1.3210 (1.3118)	grad_norm 5.3578 (5.8338)	loss_scale 2048.0000 (1692.4563)	mem 17424MB
[2024-07-30 03:10:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:13:11 lr 0.000009	 wd 0.0000	time 0.6328 (0.6586)	loss 0.9509 (1.3116)	grad_norm 6.5775 (5.7964)	loss_scale 2048.0000 (1719.7848)	mem 17424MB
[2024-07-30 03:11:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:12:05 lr 0.000008	 wd 0.0000	time 0.6202 (0.6581)	loss 1.4262 (1.3131)	grad_norm 5.2243 (5.8227)	loss_scale 2048.0000 (1743.2120)	mem 17424MB
[2024-07-30 03:12:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:10:58 lr 0.000008	 wd 0.0000	time 0.6280 (0.6576)	loss 1.2049 (1.3132)	grad_norm 4.7427 (5.7794)	loss_scale 2048.0000 (1763.5177)	mem 17424MB
[2024-07-30 03:14:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:09:52 lr 0.000008	 wd 0.0000	time 0.6235 (0.6572)	loss 1.3564 (1.3124)	grad_norm 4.2898 (5.7668)	loss_scale 2048.0000 (1781.2867)	mem 17424MB
[2024-07-30 03:15:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:08:46 lr 0.000008	 wd 0.0000	time 0.6237 (0.6568)	loss 1.3834 (1.3118)	grad_norm 5.2716 (5.7832)	loss_scale 2048.0000 (1796.9665)	mem 17424MB
[2024-07-30 03:16:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:07:40 lr 0.000008	 wd 0.0000	time 0.6202 (0.6565)	loss 1.6005 (1.3115)	grad_norm 5.2617 (5.7659)	loss_scale 2048.0000 (1810.9051)	mem 17424MB
[2024-07-30 03:17:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:06:35 lr 0.000008	 wd 0.0000	time 0.6182 (0.6562)	loss 1.5270 (1.3109)	grad_norm 4.7882 (5.7502)	loss_scale 2048.0000 (1823.3772)	mem 17424MB
[2024-07-30 03:18:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:05:29 lr 0.000008	 wd 0.0000	time 0.6234 (0.6560)	loss 1.4412 (1.3101)	grad_norm 4.2285 (5.7787)	loss_scale 2048.0000 (1834.6027)	mem 17424MB
[2024-07-30 03:19:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:04:23 lr 0.000008	 wd 0.0000	time 0.6223 (0.6557)	loss 1.3071 (1.3105)	grad_norm 4.7145 (5.7507)	loss_scale 2048.0000 (1844.7596)	mem 17424MB
[2024-07-30 03:20:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:03:17 lr 0.000008	 wd 0.0000	time 0.6131 (0.6555)	loss 1.3860 (1.3089)	grad_norm 4.9419 (5.7333)	loss_scale 2048.0000 (1853.9936)	mem 17424MB
[2024-07-30 03:21:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:02:12 lr 0.000008	 wd 0.0000	time 0.6241 (0.6553)	loss 1.4135 (1.3091)	grad_norm 4.1192 (5.7315)	loss_scale 2048.0000 (1862.4250)	mem 17424MB
[2024-07-30 03:22:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:06 lr 0.000008	 wd 0.0000	time 0.6216 (0.6551)	loss 1.1233 (1.3090)	grad_norm 4.3744 (inf)	loss_scale 1024.0000 (1838.5939)	mem 17424MB
[2024-07-30 03:23:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0000	time 0.6240 (0.6548)	loss 0.9168 (1.3080)	grad_norm 5.4115 (inf)	loss_scale 1024.0000 (1806.0232)	mem 17424MB
[2024-07-30 03:23:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 22 training takes 0:27:21
[2024-07-30 03:24:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.598 (11.598)	Loss 0.3862 (0.3862)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 17424MB
[2024-07-30 03:24:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.910 Acc@5 97.528
[2024-07-30 03:24:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-30 03:24:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 84.91%
[2024-07-30 03:24:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:08:02 lr 0.000008	 wd 0.0000	time 11.7036 (11.7036)	loss 1.1000 (1.1000)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:25:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:30:24 lr 0.000008	 wd 0.0000	time 0.6268 (0.7598)	loss 1.4892 (1.3397)	grad_norm 3.9876 (5.4988)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:26:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:27:02 lr 0.000007	 wd 0.0000	time 0.6125 (0.7049)	loss 1.5456 (1.3217)	grad_norm 5.1876 (5.4327)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:27:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:25:12 lr 0.000007	 wd 0.0000	time 0.6140 (0.6867)	loss 1.4058 (1.3187)	grad_norm 5.6489 (5.5962)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:28:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:23:43 lr 0.000007	 wd 0.0000	time 0.6079 (0.6774)	loss 1.5983 (1.3095)	grad_norm 4.8619 (5.4999)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:30:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:22:25 lr 0.000007	 wd 0.0000	time 0.6180 (0.6719)	loss 1.6228 (1.3123)	grad_norm 5.6891 (5.5377)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 03:31:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:21:10 lr 0.000007	 wd 0.0000	time 0.6258 (0.6681)	loss 1.1212 (1.3135)	grad_norm 4.1717 (inf)	loss_scale 512.0000 (962.6622)	mem 17424MB
[2024-07-30 03:32:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:19:59 lr 0.000007	 wd 0.0000	time 0.6228 (0.6656)	loss 1.4115 (1.3095)	grad_norm 7.3922 (inf)	loss_scale 512.0000 (898.3738)	mem 17424MB
[2024-07-30 03:33:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:18:49 lr 0.000007	 wd 0.0000	time 0.6279 (0.6637)	loss 1.6322 (1.3080)	grad_norm 3.8237 (inf)	loss_scale 512.0000 (850.1373)	mem 17424MB
[2024-07-30 03:34:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:17:40 lr 0.000007	 wd 0.0000	time 0.6276 (0.6622)	loss 1.6281 (1.3047)	grad_norm 7.1925 (inf)	loss_scale 512.0000 (812.6082)	mem 17424MB
[2024-07-30 03:35:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:16:32 lr 0.000007	 wd 0.0000	time 0.6196 (0.6610)	loss 1.0401 (1.3051)	grad_norm 4.9120 (inf)	loss_scale 512.0000 (782.5774)	mem 17424MB
[2024-07-30 03:36:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:15:25 lr 0.000007	 wd 0.0000	time 0.6261 (0.6600)	loss 0.9370 (1.3004)	grad_norm 6.4600 (inf)	loss_scale 512.0000 (758.0018)	mem 17424MB
[2024-07-30 03:37:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:14:18 lr 0.000007	 wd 0.0000	time 0.6160 (0.6593)	loss 1.5739 (1.3016)	grad_norm 5.0234 (inf)	loss_scale 512.0000 (737.5187)	mem 17424MB
[2024-07-30 03:38:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:13:11 lr 0.000007	 wd 0.0000	time 0.6162 (0.6585)	loss 1.3583 (1.3024)	grad_norm 3.4329 (inf)	loss_scale 512.0000 (720.1845)	mem 17424MB
[2024-07-30 03:39:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:12:05 lr 0.000007	 wd 0.0000	time 0.6115 (0.6579)	loss 1.4223 (1.3020)	grad_norm 4.7399 (inf)	loss_scale 512.0000 (705.3248)	mem 17424MB
[2024-07-30 03:40:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:10:58 lr 0.000006	 wd 0.0000	time 0.6169 (0.6574)	loss 1.3012 (1.3033)	grad_norm 6.8122 (inf)	loss_scale 512.0000 (692.4450)	mem 17424MB
[2024-07-30 03:41:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:09:52 lr 0.000006	 wd 0.0000	time 0.6193 (0.6569)	loss 1.0506 (1.3033)	grad_norm 6.9312 (inf)	loss_scale 512.0000 (681.1743)	mem 17424MB
[2024-07-30 03:43:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:08:46 lr 0.000006	 wd 0.0000	time 0.6169 (0.6565)	loss 1.3136 (1.3035)	grad_norm 5.4559 (inf)	loss_scale 512.0000 (671.2287)	mem 17424MB
[2024-07-30 03:44:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:07:40 lr 0.000006	 wd 0.0000	time 0.6117 (0.6562)	loss 1.4380 (1.3032)	grad_norm 6.9461 (inf)	loss_scale 512.0000 (662.3876)	mem 17424MB
[2024-07-30 03:45:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:06:34 lr 0.000006	 wd 0.0000	time 0.6425 (0.6559)	loss 1.1194 (1.3052)	grad_norm 4.3264 (inf)	loss_scale 512.0000 (654.4766)	mem 17424MB
[2024-07-30 03:46:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:05:29 lr 0.000006	 wd 0.0000	time 0.6236 (0.6556)	loss 1.3061 (1.3049)	grad_norm 5.2860 (inf)	loss_scale 512.0000 (647.3563)	mem 17424MB
[2024-07-30 03:47:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:04:23 lr 0.000006	 wd 0.0000	time 0.6128 (0.6553)	loss 1.5487 (1.3053)	grad_norm 4.1653 (inf)	loss_scale 512.0000 (640.9139)	mem 17424MB
[2024-07-30 03:48:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:03:17 lr 0.000006	 wd 0.0000	time 0.6126 (0.6551)	loss 1.4729 (1.3033)	grad_norm 12.3318 (inf)	loss_scale 512.0000 (635.0568)	mem 17424MB
[2024-07-30 03:49:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:02:12 lr 0.000006	 wd 0.0000	time 0.6209 (0.6548)	loss 1.4594 (1.3035)	grad_norm 4.0287 (inf)	loss_scale 512.0000 (629.7088)	mem 17424MB
[2024-07-30 03:50:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:06 lr 0.000006	 wd 0.0000	time 0.6235 (0.6546)	loss 1.7391 (1.3038)	grad_norm 4.3356 (inf)	loss_scale 512.0000 (624.8063)	mem 17424MB
[2024-07-30 03:51:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0000	time 0.6222 (0.6543)	loss 1.4142 (1.3045)	grad_norm 4.9793 (inf)	loss_scale 512.0000 (620.2959)	mem 17424MB
[2024-07-30 03:51:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 23 training takes 0:27:20
[2024-07-30 03:51:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.412 (11.412)	Loss 0.3892 (0.3892)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 03:52:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 85.028 Acc@5 97.536
[2024-07-30 03:52:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-30 03:52:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-30 03:52:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 03:52:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 03:52:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:31:29 lr 0.000006	 wd 0.0000	time 10.8270 (10.8270)	loss 1.3833 (1.3833)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 03:53:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:30:03 lr 0.000006	 wd 0.0000	time 0.6376 (0.7508)	loss 1.1062 (1.3320)	grad_norm 5.0831 (6.3207)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 03:54:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:26:51 lr 0.000006	 wd 0.0000	time 0.6184 (0.7002)	loss 1.3923 (1.3190)	grad_norm 4.4899 (6.3937)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 03:55:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:25:04 lr 0.000006	 wd 0.0000	time 0.6247 (0.6833)	loss 1.5669 (1.3150)	grad_norm 3.7300 (5.9651)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 03:56:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:23:38 lr 0.000005	 wd 0.0000	time 0.6205 (0.6748)	loss 1.3032 (1.3121)	grad_norm 4.7052 (5.8106)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 03:57:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:22:20 lr 0.000005	 wd 0.0000	time 0.6106 (0.6697)	loss 1.4937 (1.3116)	grad_norm 5.8415 (5.6955)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 03:58:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:21:07 lr 0.000005	 wd 0.0000	time 0.6212 (0.6662)	loss 1.3143 (1.3076)	grad_norm 4.0569 (5.7909)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:00:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:19:56 lr 0.000005	 wd 0.0000	time 0.6380 (0.6640)	loss 1.3079 (1.3097)	grad_norm 7.9384 (5.7883)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:01:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:18:46 lr 0.000005	 wd 0.0000	time 0.6121 (0.6622)	loss 1.4565 (1.3106)	grad_norm 4.6576 (5.7971)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:02:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:17:38 lr 0.000005	 wd 0.0000	time 0.6207 (0.6609)	loss 1.4806 (1.3084)	grad_norm 4.9867 (5.7945)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:03:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:16:31 lr 0.000005	 wd 0.0000	time 0.6348 (0.6599)	loss 1.1700 (1.3091)	grad_norm 11.1196 (5.7396)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:04:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:15:23 lr 0.000005	 wd 0.0000	time 0.6165 (0.6590)	loss 1.4144 (1.3091)	grad_norm 5.0737 (5.6645)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:05:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:14:17 lr 0.000005	 wd 0.0000	time 0.6297 (0.6584)	loss 1.0403 (1.3072)	grad_norm 4.0985 (5.6466)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:06:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:13:10 lr 0.000005	 wd 0.0000	time 0.6253 (0.6577)	loss 1.2987 (1.3080)	grad_norm 4.5268 (5.6143)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:07:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:12:04 lr 0.000005	 wd 0.0000	time 0.6242 (0.6572)	loss 1.1460 (1.3070)	grad_norm 4.8537 (5.5929)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:08:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:10:58 lr 0.000005	 wd 0.0000	time 0.6180 (0.6568)	loss 0.8452 (1.3056)	grad_norm 5.1775 (5.5612)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:09:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:09:52 lr 0.000005	 wd 0.0000	time 0.6121 (0.6563)	loss 1.3785 (1.3058)	grad_norm 5.4259 (5.5442)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:10:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:08:46 lr 0.000005	 wd 0.0000	time 0.6315 (0.6560)	loss 1.6290 (1.3070)	grad_norm 4.8915 (5.5552)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:11:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:07:40 lr 0.000005	 wd 0.0000	time 0.6288 (0.6557)	loss 1.0365 (1.3050)	grad_norm 12.9506 (5.6190)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:13:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:06:34 lr 0.000005	 wd 0.0000	time 0.6234 (0.6555)	loss 1.4474 (1.3059)	grad_norm 5.7382 (5.6139)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:14:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:05:28 lr 0.000004	 wd 0.0000	time 0.6171 (0.6552)	loss 1.4063 (1.3055)	grad_norm 4.6757 (5.6005)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 04:15:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:04:23 lr 0.000004	 wd 0.0000	time 0.6146 (0.6551)	loss 1.3844 (1.3048)	grad_norm 8.0373 (5.6035)	loss_scale 1024.0000 (530.0333)	mem 17424MB
[2024-07-30 04:16:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:03:17 lr 0.000004	 wd 0.0000	time 0.6333 (0.6549)	loss 1.3062 (1.3044)	grad_norm 5.2394 (5.5949)	loss_scale 1024.0000 (552.4761)	mem 17424MB
[2024-07-30 04:17:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:02:12 lr 0.000004	 wd 0.0000	time 0.6198 (0.6547)	loss 1.4624 (1.3042)	grad_norm 4.4969 (5.6077)	loss_scale 1024.0000 (572.9683)	mem 17424MB
[2024-07-30 04:18:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:06 lr 0.000004	 wd 0.0000	time 0.6214 (0.6545)	loss 1.5002 (1.3042)	grad_norm 3.8443 (5.6229)	loss_scale 1024.0000 (591.7534)	mem 17424MB
[2024-07-30 04:19:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0000	time 0.6264 (0.6542)	loss 1.1266 (1.3045)	grad_norm 4.5404 (5.6022)	loss_scale 1024.0000 (609.0364)	mem 17424MB
[2024-07-30 04:19:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 24 training takes 0:27:20
[2024-07-30 04:19:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.598 (11.598)	Loss 0.3918 (0.3918)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 04:20:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 85.012 Acc@5 97.540
[2024-07-30 04:20:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-30 04:20:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-30 04:20:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:10:32 lr 0.000004	 wd 0.0000	time 11.7635 (11.7635)	loss 1.1910 (1.1910)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:21:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:30:28 lr 0.000004	 wd 0.0000	time 0.6128 (0.7612)	loss 1.3736 (1.2834)	grad_norm 4.8601 (5.1636)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:22:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:27:04 lr 0.000004	 wd 0.0000	time 0.6243 (0.7057)	loss 1.1976 (1.3025)	grad_norm 3.6489 (5.3861)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:23:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:25:12 lr 0.000004	 wd 0.0000	time 0.6207 (0.6871)	loss 1.0279 (1.2919)	grad_norm 3.3594 (5.3357)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:24:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:23:44 lr 0.000004	 wd 0.0000	time 0.6198 (0.6777)	loss 1.2102 (1.3059)	grad_norm 4.2270 (5.2467)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:25:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:22:25 lr 0.000004	 wd 0.0000	time 0.6166 (0.6721)	loss 1.4487 (1.3082)	grad_norm 3.8981 (5.3065)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:26:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:21:11 lr 0.000004	 wd 0.0000	time 0.6177 (0.6684)	loss 1.2162 (1.3084)	grad_norm 6.2212 (5.2553)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:27:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:19:59 lr 0.000004	 wd 0.0000	time 0.6251 (0.6658)	loss 1.1943 (1.3082)	grad_norm 6.4257 (5.2807)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:29:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:18:49 lr 0.000004	 wd 0.0000	time 0.6186 (0.6637)	loss 1.0195 (1.3088)	grad_norm 5.1572 (5.2478)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:30:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:17:40 lr 0.000004	 wd 0.0000	time 0.6207 (0.6621)	loss 1.3341 (1.3109)	grad_norm 5.3877 (5.2868)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:31:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:16:32 lr 0.000004	 wd 0.0000	time 0.6177 (0.6610)	loss 1.2673 (1.3118)	grad_norm 5.1111 (5.3805)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:32:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:15:25 lr 0.000004	 wd 0.0000	time 0.6195 (0.6600)	loss 1.2558 (1.3097)	grad_norm 5.4107 (5.4050)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:33:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:14:18 lr 0.000004	 wd 0.0000	time 0.6213 (0.6591)	loss 1.2974 (1.3098)	grad_norm 4.6971 (5.4370)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:34:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:13:11 lr 0.000003	 wd 0.0000	time 0.6082 (0.6584)	loss 1.4518 (1.3097)	grad_norm 7.2052 (5.4196)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:35:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:12:04 lr 0.000003	 wd 0.0000	time 0.6232 (0.6578)	loss 1.0985 (1.3095)	grad_norm 2.9875 (5.4319)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:36:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:10:58 lr 0.000003	 wd 0.0000	time 0.6285 (0.6573)	loss 1.2899 (1.3099)	grad_norm 4.3088 (5.4329)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:37:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:09:52 lr 0.000003	 wd 0.0000	time 0.6291 (0.6568)	loss 1.4292 (1.3087)	grad_norm 7.3871 (5.4411)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:38:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:08:46 lr 0.000003	 wd 0.0000	time 0.6214 (0.6564)	loss 1.1793 (1.3077)	grad_norm 6.3324 (5.4571)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:39:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:07:40 lr 0.000003	 wd 0.0000	time 0.6179 (0.6561)	loss 1.3873 (1.3073)	grad_norm 5.4435 (5.4824)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:40:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:06:34 lr 0.000003	 wd 0.0000	time 0.6282 (0.6559)	loss 1.4984 (1.3067)	grad_norm 5.3267 (5.4822)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:42:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:05:29 lr 0.000003	 wd 0.0000	time 0.6187 (0.6557)	loss 1.3054 (1.3065)	grad_norm 10.1511 (5.5730)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:43:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:04:23 lr 0.000003	 wd 0.0000	time 0.6552 (0.6555)	loss 0.7812 (1.3064)	grad_norm 3.6805 (5.5804)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:44:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:03:17 lr 0.000003	 wd 0.0000	time 0.6175 (0.6553)	loss 1.2911 (1.3051)	grad_norm 7.0886 (5.5921)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:45:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:02:12 lr 0.000003	 wd 0.0000	time 0.6285 (0.6550)	loss 1.3759 (1.3043)	grad_norm 5.0481 (5.6351)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:46:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:01:06 lr 0.000003	 wd 0.0000	time 0.6188 (0.6548)	loss 1.4709 (1.3054)	grad_norm 5.4361 (5.6303)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:47:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0000	time 0.5959 (0.6545)	loss 1.6164 (1.3053)	grad_norm 4.9297 (5.6477)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:47:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 25 training takes 0:27:24
[2024-07-30 04:47:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.620 (11.620)	Loss 0.3882 (0.3882)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 04:48:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 84.994 Acc@5 97.538
[2024-07-30 04:48:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-30 04:48:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-30 04:48:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:29:58 lr 0.000003	 wd 0.0000	time 10.7908 (10.7908)	loss 1.2084 (1.2084)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:49:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:30:12 lr 0.000003	 wd 0.0000	time 0.6186 (0.7547)	loss 1.5257 (1.2962)	grad_norm 5.1762 (5.3575)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:50:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:26:56 lr 0.000003	 wd 0.0000	time 0.6251 (0.7021)	loss 1.3742 (1.3240)	grad_norm 4.4210 (5.6232)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:51:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:25:07 lr 0.000003	 wd 0.0000	time 0.6207 (0.6845)	loss 1.3846 (1.3147)	grad_norm 6.2817 (5.6196)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:52:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:23:40 lr 0.000003	 wd 0.0000	time 0.6187 (0.6758)	loss 1.5833 (1.3185)	grad_norm 4.9286 (5.6741)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:53:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:22:22 lr 0.000003	 wd 0.0000	time 0.6186 (0.6705)	loss 1.6300 (1.3205)	grad_norm 4.7929 (5.6071)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:54:46 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:21:08 lr 0.000003	 wd 0.0000	time 0.6192 (0.6671)	loss 1.2000 (1.3140)	grad_norm 4.4402 (5.5512)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:55:51 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:19:57 lr 0.000003	 wd 0.0000	time 0.6217 (0.6646)	loss 1.5248 (1.3094)	grad_norm 6.7250 (5.5148)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:56:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:18:48 lr 0.000002	 wd 0.0000	time 0.6200 (0.6628)	loss 1.4909 (1.3067)	grad_norm 5.4404 (5.5162)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:58:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:17:39 lr 0.000002	 wd 0.0000	time 0.6288 (0.6615)	loss 1.3769 (1.3061)	grad_norm 5.0790 (5.5852)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 04:59:06 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:16:31 lr 0.000002	 wd 0.0000	time 0.6141 (0.6604)	loss 1.4086 (1.3065)	grad_norm 5.4220 (5.5773)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:00:11 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:15:24 lr 0.000002	 wd 0.0000	time 0.6027 (0.6595)	loss 1.3879 (1.3099)	grad_norm 4.6333 (5.6090)	loss_scale 2048.0000 (1096.5450)	mem 17424MB
[2024-07-30 05:01:16 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:14:17 lr 0.000002	 wd 0.0000	time 0.6295 (0.6587)	loss 1.2312 (1.3109)	grad_norm 4.1183 (5.5673)	loss_scale 2048.0000 (1175.7669)	mem 17424MB
[2024-07-30 05:02:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:13:10 lr 0.000002	 wd 0.0000	time 0.6239 (0.6580)	loss 0.8575 (1.3100)	grad_norm 5.9622 (5.5267)	loss_scale 2048.0000 (1242.8101)	mem 17424MB
[2024-07-30 05:03:26 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:12:04 lr 0.000002	 wd 0.0000	time 0.6149 (0.6575)	loss 1.4632 (1.3094)	grad_norm 4.6463 (5.5232)	loss_scale 2048.0000 (1300.2827)	mem 17424MB
[2024-07-30 05:04:31 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:10:58 lr 0.000002	 wd 0.0000	time 0.6187 (0.6570)	loss 1.3828 (1.3080)	grad_norm 7.2930 (5.5411)	loss_scale 2048.0000 (1350.0973)	mem 17424MB
[2024-07-30 05:05:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:09:52 lr 0.000002	 wd 0.0000	time 0.6266 (0.6566)	loss 1.2779 (1.3060)	grad_norm 5.7304 (5.5431)	loss_scale 2048.0000 (1393.6889)	mem 17424MB
[2024-07-30 05:06:41 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:08:46 lr 0.000002	 wd 0.0000	time 0.6137 (0.6564)	loss 1.2808 (1.3064)	grad_norm 5.5558 (5.5468)	loss_scale 2048.0000 (1432.1552)	mem 17424MB
[2024-07-30 05:07:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:07:40 lr 0.000002	 wd 0.0000	time 0.6233 (0.6561)	loss 1.4855 (1.3085)	grad_norm 4.3734 (5.5394)	loss_scale 2048.0000 (1466.3498)	mem 17424MB
[2024-07-30 05:08:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:06:34 lr 0.000002	 wd 0.0000	time 0.6126 (0.6558)	loss 1.0901 (1.3065)	grad_norm 4.2364 (5.5220)	loss_scale 2048.0000 (1496.9469)	mem 17424MB
[2024-07-30 05:09:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:05:29 lr 0.000002	 wd 0.0000	time 0.6268 (0.6556)	loss 1.0392 (1.3060)	grad_norm 5.7163 (5.5058)	loss_scale 2048.0000 (1524.4858)	mem 17424MB
[2024-07-30 05:11:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:04:23 lr 0.000002	 wd 0.0000	time 0.6303 (0.6554)	loss 1.0518 (1.3042)	grad_norm 5.5707 (5.4841)	loss_scale 2048.0000 (1549.4031)	mem 17424MB
[2024-07-30 05:12:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:03:17 lr 0.000002	 wd 0.0000	time 0.6270 (0.6551)	loss 0.9793 (1.3046)	grad_norm 4.1017 (5.4940)	loss_scale 2048.0000 (1572.0563)	mem 17424MB
[2024-07-30 05:13:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:02:12 lr 0.000002	 wd 0.0000	time 0.6286 (0.6550)	loss 0.9712 (1.3034)	grad_norm 4.5540 (5.4889)	loss_scale 2048.0000 (1592.7405)	mem 17424MB
[2024-07-30 05:14:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:01:06 lr 0.000002	 wd 0.0000	time 0.6246 (0.6548)	loss 1.0470 (1.3032)	grad_norm 4.8186 (5.4932)	loss_scale 2048.0000 (1611.7018)	mem 17424MB
[2024-07-30 05:15:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0000	time 0.6264 (0.6545)	loss 0.8792 (1.3014)	grad_norm 4.1327 (inf)	loss_scale 1024.0000 (1590.6597)	mem 17424MB
[2024-07-30 05:15:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 26 training takes 0:27:23
[2024-07-30 05:15:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.687 (11.687)	Loss 0.3867 (0.3867)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 05:16:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 85.028 Acc@5 97.554
[2024-07-30 05:16:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-30 05:16:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-30 05:16:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 05:16:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 05:16:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:17:53 lr 0.000002	 wd 0.0000	time 10.5011 (10.5011)	loss 0.8922 (0.8922)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:17:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:29:53 lr 0.000002	 wd 0.0000	time 0.6218 (0.7468)	loss 1.5207 (1.3247)	grad_norm 4.8466 (5.9395)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:18:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:26:49 lr 0.000002	 wd 0.0000	time 0.6155 (0.6991)	loss 1.3776 (1.3167)	grad_norm 4.8313 (5.7368)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:19:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:25:02 lr 0.000002	 wd 0.0000	time 0.6175 (0.6824)	loss 1.2415 (1.3080)	grad_norm 4.1777 (5.5214)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:20:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:23:37 lr 0.000002	 wd 0.0000	time 0.6254 (0.6742)	loss 1.4317 (1.3020)	grad_norm 4.5203 (5.5552)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:21:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:22:19 lr 0.000002	 wd 0.0000	time 0.6177 (0.6691)	loss 0.7985 (1.2949)	grad_norm 3.9584 (5.6026)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:22:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:21:06 lr 0.000002	 wd 0.0000	time 0.6156 (0.6660)	loss 1.2145 (1.2984)	grad_norm 3.6390 (5.6099)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:23:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:19:56 lr 0.000002	 wd 0.0000	time 0.6247 (0.6638)	loss 1.5125 (1.3049)	grad_norm 3.5016 (5.8658)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:24:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:18:46 lr 0.000002	 wd 0.0000	time 0.6190 (0.6620)	loss 1.4238 (1.3052)	grad_norm 5.1799 (5.8389)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:26:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:17:38 lr 0.000001	 wd 0.0000	time 0.6172 (0.6606)	loss 1.4106 (1.3102)	grad_norm 8.7415 (5.8020)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:27:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:16:30 lr 0.000001	 wd 0.0000	time 0.6261 (0.6596)	loss 1.3507 (1.3089)	grad_norm 5.6165 (5.7471)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:28:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:15:23 lr 0.000001	 wd 0.0000	time 0.6205 (0.6587)	loss 1.3546 (1.3090)	grad_norm 3.7901 (5.7762)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:29:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:14:16 lr 0.000001	 wd 0.0000	time 0.6213 (0.6581)	loss 1.3508 (1.3080)	grad_norm 4.5211 (5.7385)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:30:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:13:10 lr 0.000001	 wd 0.0000	time 0.6158 (0.6574)	loss 1.5050 (1.3054)	grad_norm 7.8465 (5.7277)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:31:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:12:03 lr 0.000001	 wd 0.0000	time 0.6200 (0.6568)	loss 1.2440 (1.3059)	grad_norm 5.5326 (5.8367)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:32:30 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:10:57 lr 0.000001	 wd 0.0000	time 0.6291 (0.6564)	loss 1.3700 (1.3069)	grad_norm 3.8134 (5.8579)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:33:35 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:09:51 lr 0.000001	 wd 0.0000	time 0.6201 (0.6560)	loss 1.5013 (1.3045)	grad_norm 4.9748 (5.8226)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:34:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:08:45 lr 0.000001	 wd 0.0000	time 0.6204 (0.6557)	loss 1.4133 (1.3024)	grad_norm 3.9791 (5.8100)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:35:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:07:40 lr 0.000001	 wd 0.0000	time 0.6174 (0.6555)	loss 1.4750 (1.3036)	grad_norm 3.4778 (5.7799)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:36:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:06:34 lr 0.000001	 wd 0.0000	time 0.6191 (0.6552)	loss 0.9454 (1.3047)	grad_norm 6.5840 (5.7559)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:37:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:05:28 lr 0.000001	 wd 0.0000	time 0.6256 (0.6550)	loss 0.8399 (1.3031)	grad_norm 5.1273 (5.7381)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:39:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:04:23 lr 0.000001	 wd 0.0000	time 0.6201 (0.6548)	loss 1.6238 (1.3027)	grad_norm 3.0210 (5.7169)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:40:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:03:17 lr 0.000001	 wd 0.0000	time 0.6250 (0.6545)	loss 0.9753 (1.3029)	grad_norm 9.2560 (5.7036)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:41:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:02:12 lr 0.000001	 wd 0.0000	time 0.6347 (0.6544)	loss 1.2848 (1.3028)	grad_norm 4.3160 (5.6935)	loss_scale 1024.0000 (1024.0000)	mem 17424MB
[2024-07-30 05:42:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:01:06 lr 0.000001	 wd 0.0000	time 0.6215 (0.6542)	loss 1.1356 (1.3029)	grad_norm 5.2138 (nan)	loss_scale 512.0000 (1017.1762)	mem 17424MB
[2024-07-30 05:43:20 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0000	time 0.6203 (0.6539)	loss 1.5509 (1.3020)	grad_norm 4.2054 (nan)	loss_scale 512.0000 (996.9772)	mem 17424MB
[2024-07-30 05:43:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 27 training takes 0:27:22
[2024-07-30 05:43:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.116 (11.116)	Loss 0.3860 (0.3860)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 05:43:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 85.056 Acc@5 97.542
[2024-07-30 05:43:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-30 05:43:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 85.06%
[2024-07-30 05:43:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 160): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saving......
[2024-07-30 05:44:01 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 162): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-30 05:44:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:17:05 lr 0.000001	 wd 0.0000	time 10.4817 (10.4817)	loss 1.3013 (1.3013)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:45:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:29:54 lr 0.000001	 wd 0.0000	time 0.6130 (0.7473)	loss 0.9445 (1.2872)	grad_norm 5.1663 (5.9728)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:46:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:26:48 lr 0.000001	 wd 0.0000	time 0.6141 (0.6986)	loss 1.0943 (1.2918)	grad_norm 3.8944 (5.8379)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:47:27 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:25:02 lr 0.000001	 wd 0.0000	time 0.6205 (0.6825)	loss 1.4387 (1.3050)	grad_norm 3.1259 (5.6220)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:48:32 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:23:37 lr 0.000001	 wd 0.0000	time 0.6130 (0.6742)	loss 1.3997 (1.3100)	grad_norm 3.8920 (5.5302)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:49:37 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:22:20 lr 0.000001	 wd 0.0000	time 0.6241 (0.6694)	loss 1.4119 (1.3083)	grad_norm 5.2459 (5.5556)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:50:42 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:21:07 lr 0.000001	 wd 0.0000	time 0.6352 (0.6664)	loss 1.3626 (1.3091)	grad_norm 6.8320 (5.5079)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:51:47 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:19:56 lr 0.000001	 wd 0.0000	time 0.6260 (0.6641)	loss 0.9775 (1.3096)	grad_norm 3.6938 (5.5179)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:52:52 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:18:47 lr 0.000001	 wd 0.0000	time 0.6200 (0.6624)	loss 1.4607 (1.3052)	grad_norm 4.8549 (5.4891)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:53:57 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:17:39 lr 0.000001	 wd 0.0000	time 0.6055 (0.6611)	loss 1.3965 (1.3051)	grad_norm 4.2746 (5.4490)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:55:02 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:16:31 lr 0.000001	 wd 0.0000	time 0.6261 (0.6599)	loss 1.5230 (1.3073)	grad_norm 3.6975 (5.4749)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:56:07 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:15:24 lr 0.000001	 wd 0.0000	time 0.6149 (0.6591)	loss 0.9630 (1.3052)	grad_norm 5.7506 (5.4159)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:57:12 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:14:17 lr 0.000001	 wd 0.0000	time 0.6385 (0.6585)	loss 1.0547 (1.3048)	grad_norm 3.8775 (5.4632)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:58:17 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:13:10 lr 0.000001	 wd 0.0000	time 0.6207 (0.6579)	loss 1.4534 (1.3019)	grad_norm 6.1778 (5.4658)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 05:59:22 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:12:04 lr 0.000001	 wd 0.0000	time 0.6032 (0.6575)	loss 0.9293 (1.3029)	grad_norm 3.9473 (5.4657)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:00:28 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:10:58 lr 0.000001	 wd 0.0000	time 0.6565 (0.6572)	loss 1.3233 (1.3010)	grad_norm 6.0944 (5.4721)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:01:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:09:52 lr 0.000001	 wd 0.0000	time 0.6156 (0.6568)	loss 1.2208 (1.3001)	grad_norm 9.9419 (5.4479)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:02:38 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:08:46 lr 0.000001	 wd 0.0000	time 0.6236 (0.6564)	loss 1.4086 (1.3006)	grad_norm 5.4370 (5.4475)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:03:43 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:07:40 lr 0.000001	 wd 0.0000	time 0.6221 (0.6561)	loss 1.2421 (1.3002)	grad_norm 5.1080 (5.4359)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:04:48 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:06:34 lr 0.000001	 wd 0.0000	time 0.6134 (0.6559)	loss 1.5326 (1.2993)	grad_norm 4.7793 (5.4131)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:05:53 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:05:29 lr 0.000001	 wd 0.0000	time 0.6290 (0.6557)	loss 0.8509 (1.2991)	grad_norm 7.3921 (5.4123)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:06:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:04:23 lr 0.000001	 wd 0.0000	time 0.6239 (0.6555)	loss 1.1839 (1.2993)	grad_norm 6.1748 (5.4239)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:08:03 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:03:17 lr 0.000001	 wd 0.0000	time 0.6178 (0.6552)	loss 1.4750 (1.2981)	grad_norm 5.1309 (5.4147)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:09:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:02:12 lr 0.000001	 wd 0.0000	time 0.6170 (0.6551)	loss 1.0021 (1.2981)	grad_norm 5.1754 (5.4053)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:10:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:01:06 lr 0.000001	 wd 0.0000	time 0.6221 (0.6549)	loss 1.4950 (1.2973)	grad_norm 3.5750 (5.4399)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:11:18 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0000	time 0.6242 (0.6546)	loss 1.2066 (1.2962)	grad_norm 4.5216 (5.4502)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:11:25 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 28 training takes 0:27:23
[2024-07-30 06:11:36 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.175 (11.175)	Loss 0.3867 (0.3867)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 06:11:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 85.036 Acc@5 97.538
[2024-07-30 06:11:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-30 06:11:58 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 85.06%
[2024-07-30 06:12:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:43:42 lr 0.000001	 wd 0.0000	time 11.1202 (11.1202)	loss 1.3505 (1.3505)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:13:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:30:12 lr 0.000001	 wd 0.0000	time 0.6371 (0.7546)	loss 1.4210 (1.2694)	grad_norm 4.4804 (4.9699)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:14:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:26:57 lr 0.000001	 wd 0.0000	time 0.6071 (0.7026)	loss 1.0285 (1.2870)	grad_norm 5.9378 (5.4718)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:15:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:25:09 lr 0.000001	 wd 0.0000	time 0.6475 (0.6854)	loss 1.4288 (1.2872)	grad_norm 4.0717 (5.4629)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:16:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:23:42 lr 0.000001	 wd 0.0000	time 0.6205 (0.6767)	loss 1.2390 (1.2977)	grad_norm 5.2779 (5.4983)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:17:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:22:23 lr 0.000001	 wd 0.0000	time 0.6248 (0.6713)	loss 0.8039 (1.3008)	grad_norm 3.7509 (5.7899)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:18:39 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:21:10 lr 0.000000	 wd 0.0000	time 0.6185 (0.6678)	loss 1.1848 (1.3032)	grad_norm 4.6100 (5.9510)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:19:44 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:19:58 lr 0.000000	 wd 0.0000	time 0.6270 (0.6652)	loss 1.6014 (1.3027)	grad_norm 5.6444 (5.8261)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:20:49 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:18:48 lr 0.000000	 wd 0.0000	time 0.6120 (0.6633)	loss 0.7536 (1.3026)	grad_norm 5.3847 (5.7771)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:21:54 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:17:40 lr 0.000000	 wd 0.0000	time 0.6160 (0.6619)	loss 1.3009 (1.3031)	grad_norm 4.9216 (5.7473)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:22:59 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:16:32 lr 0.000000	 wd 0.0000	time 0.6217 (0.6608)	loss 1.3150 (1.3032)	grad_norm 4.9025 (5.6881)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:24:04 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:15:25 lr 0.000000	 wd 0.0000	time 0.6222 (0.6598)	loss 1.4542 (1.3036)	grad_norm 3.5959 (5.6373)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:25:09 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:14:17 lr 0.000000	 wd 0.0000	time 0.6283 (0.6590)	loss 1.3537 (1.3007)	grad_norm 4.6127 (5.6245)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:26:14 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:13:11 lr 0.000000	 wd 0.0000	time 0.6175 (0.6583)	loss 1.4815 (1.3013)	grad_norm 5.9469 (5.6199)	loss_scale 512.0000 (512.0000)	mem 17424MB
[2024-07-30 06:27:19 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:12:04 lr 0.000000	 wd 0.0000	time 0.6204 (0.6577)	loss 1.5432 (1.2996)	grad_norm 6.0882 (5.6054)	loss_scale 1024.0000 (525.1563)	mem 17424MB
[2024-07-30 06:28:24 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:10:58 lr 0.000000	 wd 0.0000	time 0.6258 (0.6572)	loss 1.2425 (1.3014)	grad_norm 6.9003 (5.6372)	loss_scale 1024.0000 (558.3904)	mem 17424MB
[2024-07-30 06:29:29 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:09:52 lr 0.000000	 wd 0.0000	time 0.6168 (0.6569)	loss 1.3722 (1.3038)	grad_norm 8.7070 (5.5977)	loss_scale 1024.0000 (587.4728)	mem 17424MB
[2024-07-30 06:30:34 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:08:46 lr 0.000000	 wd 0.0000	time 0.6259 (0.6565)	loss 1.3166 (1.3006)	grad_norm 4.6537 (5.6608)	loss_scale 1024.0000 (613.1358)	mem 17424MB
[2024-07-30 06:31:40 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:07:40 lr 0.000000	 wd 0.0000	time 0.6037 (0.6562)	loss 1.0206 (1.3019)	grad_norm 7.8496 (5.6310)	loss_scale 1024.0000 (635.9489)	mem 17424MB
[2024-07-30 06:32:45 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:06:34 lr 0.000000	 wd 0.0000	time 0.6258 (0.6560)	loss 1.1375 (1.3017)	grad_norm 8.0300 (5.6406)	loss_scale 1024.0000 (656.3619)	mem 17424MB
[2024-07-30 06:33:50 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:05:29 lr 0.000000	 wd 0.0000	time 0.5998 (0.6557)	loss 1.4576 (1.3018)	grad_norm 5.4147 (inf)	loss_scale 512.0000 (673.7111)	mem 17424MB
[2024-07-30 06:34:55 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:04:23 lr 0.000000	 wd 0.0000	time 0.6199 (0.6556)	loss 1.6029 (1.3034)	grad_norm 6.5466 (inf)	loss_scale 512.0000 (666.0143)	mem 17424MB
[2024-07-30 06:36:00 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:03:17 lr 0.000000	 wd 0.0000	time 0.6218 (0.6554)	loss 1.2331 (1.3046)	grad_norm 3.4850 (inf)	loss_scale 512.0000 (659.0168)	mem 17424MB
[2024-07-30 06:37:05 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:02:12 lr 0.000000	 wd 0.0000	time 0.6305 (0.6552)	loss 1.3197 (1.3050)	grad_norm 6.1845 (inf)	loss_scale 512.0000 (652.6276)	mem 17424MB
[2024-07-30 06:38:10 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:01:06 lr 0.000000	 wd 0.0000	time 0.6179 (0.6550)	loss 1.3835 (1.3057)	grad_norm 4.3563 (inf)	loss_scale 512.0000 (646.7705)	mem 17424MB
[2024-07-30 06:39:15 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0000	time 0.6224 (0.6547)	loss 1.4790 (1.3053)	grad_norm 4.2507 (inf)	loss_scale 512.0000 (641.3818)	mem 17424MB
[2024-07-30 06:39:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 249): INFO EPOCH 29 training takes 0:27:23
[2024-07-30 06:39:21 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 145): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_29.pth saving......
[2024-07-30 06:39:23 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (utils.py 147): INFO pretrain/lora-style-vcnu_finetune/lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune/lora-style-vcnu-swin-b-finetune/ckpt_epoch_29.pth saved !!!
[2024-07-30 06:39:33 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 10.335 (10.335)	Loss 0.3877 (0.3877)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 17424MB
[2024-07-30 06:39:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 296): INFO  * Acc@1 85.046 Acc@5 97.538
[2024-07-30 06:39:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-30 06:39:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 182): INFO Max accuracy: 85.06%
[2024-07-30 06:39:56 lora-style-vcnu_swin_base_patch4_window7_224_22kto1k_finetune] (main.py 189): INFO Training time 13:55:57
