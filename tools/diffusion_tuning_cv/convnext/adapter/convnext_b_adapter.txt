[2024-07-01 21:12:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/config.json
[2024-07-01 21:12:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_convnext_base_224_22kto1k_finetune_efficient
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_convnext
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: adapter_convnext_base_patch4_22kto1k_efficient_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-01 21:12:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_convnext/adapter_convnext_base_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "adapter_convnext_base_patch4_22kto1k_efficient_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-01 21:12:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 108): INFO Creating model:adapter_convnext/adapter_convnext_base_224_22kto1k_finetune_efficient
[2024-07-01 21:39:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/config.json
[2024-07-01 21:39:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_convnext_base_224_22kto1k_finetune_efficient
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_convnext
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: adapter_convnext_base_patch4_22kto1k_efficient_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-01 21:39:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_convnext/adapter_convnext_base_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "adapter_convnext_base_patch4_22kto1k_efficient_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-01 21:39:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 108): INFO Creating model:adapter_convnext/adapter_convnext_base_224_22kto1k_finetune_efficient
[2024-07-01 21:39:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 110): INFO Adapter_ConvNeXt(
  (uma): UMA(filter_strategy1=18, filter_strategy2=6,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
        (adapter): Adapter(
          dim=128, emb_dim=32, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=128, out_features=32, bias=True)
          (up): Linear(in_features=32, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=128, emb_dim=32, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=128, out_features=32, bias=True)
          (up): Linear(in_features=32, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=128, emb_dim=32, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=128, out_features=32, bias=True)
          (up): Linear(in_features=32, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=256, emb_dim=64, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=256, out_features=64, bias=True)
          (up): Linear(in_features=64, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=256, emb_dim=64, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=256, out_features=64, bias=True)
          (up): Linear(in_features=64, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=256, emb_dim=64, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=256, out_features=64, bias=True)
          (up): Linear(in_features=64, out_features=256, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=512, emb_dim=128, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=512, out_features=128, bias=True)
          (up): Linear(in_features=128, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=1024, emb_dim=256, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=1024, out_features=256, bias=True)
          (up): Linear(in_features=256, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=1024, emb_dim=256, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=1024, out_features=256, bias=True)
          (up): Linear(in_features=256, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (adapter): Adapter(
          dim=1024, emb_dim=256, model_style=trans, 
          (activation): GELU()
          (down): Linear(in_features=1024, out_features=256, bias=True)
          (up): Linear(in_features=256, out_features=1024, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (memory_downsampling): ModuleList()
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-01 21:39:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 113): INFO number of params: 6284296
[2024-07-01 21:39:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune, ignoring auto resume
[2024-07-01 21:39:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth for fine-tuning......
[2024-07-01 21:39:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-01 21:39:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['stages.0.0.adapter.down.weight', 'stages.0.0.adapter.down.bias', 'stages.0.0.adapter.up.weight', 'stages.0.0.adapter.up.bias', 'stages.0.1.adapter.down.weight', 'stages.0.1.adapter.down.bias', 'stages.0.1.adapter.up.weight', 'stages.0.1.adapter.up.bias', 'stages.0.2.adapter.down.weight', 'stages.0.2.adapter.down.bias', 'stages.0.2.adapter.up.weight', 'stages.0.2.adapter.up.bias', 'stages.1.0.adapter.down.weight', 'stages.1.0.adapter.down.bias', 'stages.1.0.adapter.up.weight', 'stages.1.0.adapter.up.bias', 'stages.1.1.adapter.down.weight', 'stages.1.1.adapter.down.bias', 'stages.1.1.adapter.up.weight', 'stages.1.1.adapter.up.bias', 'stages.1.2.adapter.down.weight', 'stages.1.2.adapter.down.bias', 'stages.1.2.adapter.up.weight', 'stages.1.2.adapter.up.bias', 'stages.2.0.adapter.down.weight', 'stages.2.0.adapter.down.bias', 'stages.2.0.adapter.up.weight', 'stages.2.0.adapter.up.bias', 'stages.2.1.adapter.down.weight', 'stages.2.1.adapter.down.bias', 'stages.2.1.adapter.up.weight', 'stages.2.1.adapter.up.bias', 'stages.2.2.adapter.down.weight', 'stages.2.2.adapter.down.bias', 'stages.2.2.adapter.up.weight', 'stages.2.2.adapter.up.bias', 'stages.2.3.adapter.down.weight', 'stages.2.3.adapter.down.bias', 'stages.2.3.adapter.up.weight', 'stages.2.3.adapter.up.bias', 'stages.2.4.adapter.down.weight', 'stages.2.4.adapter.down.bias', 'stages.2.4.adapter.up.weight', 'stages.2.4.adapter.up.bias', 'stages.2.5.adapter.down.weight', 'stages.2.5.adapter.down.bias', 'stages.2.5.adapter.up.weight', 'stages.2.5.adapter.up.bias', 'stages.2.6.adapter.down.weight', 'stages.2.6.adapter.down.bias', 'stages.2.6.adapter.up.weight', 'stages.2.6.adapter.up.bias', 'stages.2.7.adapter.down.weight', 'stages.2.7.adapter.down.bias', 'stages.2.7.adapter.up.weight', 'stages.2.7.adapter.up.bias', 'stages.2.8.adapter.down.weight', 'stages.2.8.adapter.down.bias', 'stages.2.8.adapter.up.weight', 'stages.2.8.adapter.up.bias', 'stages.2.9.adapter.down.weight', 'stages.2.9.adapter.down.bias', 'stages.2.9.adapter.up.weight', 'stages.2.9.adapter.up.bias', 'stages.2.10.adapter.down.weight', 'stages.2.10.adapter.down.bias', 'stages.2.10.adapter.up.weight', 'stages.2.10.adapter.up.bias', 'stages.2.11.adapter.down.weight', 'stages.2.11.adapter.down.bias', 'stages.2.11.adapter.up.weight', 'stages.2.11.adapter.up.bias', 'stages.2.12.adapter.down.weight', 'stages.2.12.adapter.down.bias', 'stages.2.12.adapter.up.weight', 'stages.2.12.adapter.up.bias', 'stages.2.13.adapter.down.weight', 'stages.2.13.adapter.down.bias', 'stages.2.13.adapter.up.weight', 'stages.2.13.adapter.up.bias', 'stages.2.14.adapter.down.weight', 'stages.2.14.adapter.down.bias', 'stages.2.14.adapter.up.weight', 'stages.2.14.adapter.up.bias', 'stages.2.15.adapter.down.weight', 'stages.2.15.adapter.down.bias', 'stages.2.15.adapter.up.weight', 'stages.2.15.adapter.up.bias', 'stages.2.16.adapter.down.weight', 'stages.2.16.adapter.down.bias', 'stages.2.16.adapter.up.weight', 'stages.2.16.adapter.up.bias', 'stages.2.17.adapter.down.weight', 'stages.2.17.adapter.down.bias', 'stages.2.17.adapter.up.weight', 'stages.2.17.adapter.up.bias', 'stages.2.18.adapter.down.weight', 'stages.2.18.adapter.down.bias', 'stages.2.18.adapter.up.weight', 'stages.2.18.adapter.up.bias', 'stages.2.19.adapter.down.weight', 'stages.2.19.adapter.down.bias', 'stages.2.19.adapter.up.weight', 'stages.2.19.adapter.up.bias', 'stages.2.20.adapter.down.weight', 'stages.2.20.adapter.down.bias', 'stages.2.20.adapter.up.weight', 'stages.2.20.adapter.up.bias', 'stages.2.21.adapter.down.weight', 'stages.2.21.adapter.down.bias', 'stages.2.21.adapter.up.weight', 'stages.2.21.adapter.up.bias', 'stages.2.22.adapter.down.weight', 'stages.2.22.adapter.down.bias', 'stages.2.22.adapter.up.weight', 'stages.2.22.adapter.up.bias', 'stages.2.23.adapter.down.weight', 'stages.2.23.adapter.down.bias', 'stages.2.23.adapter.up.weight', 'stages.2.23.adapter.up.bias', 'stages.2.24.adapter.down.weight', 'stages.2.24.adapter.down.bias', 'stages.2.24.adapter.up.weight', 'stages.2.24.adapter.up.bias', 'stages.2.25.adapter.down.weight', 'stages.2.25.adapter.down.bias', 'stages.2.25.adapter.up.weight', 'stages.2.25.adapter.up.bias', 'stages.2.26.adapter.down.weight', 'stages.2.26.adapter.down.bias', 'stages.2.26.adapter.up.weight', 'stages.2.26.adapter.up.bias', 'stages.3.0.adapter.down.weight', 'stages.3.0.adapter.down.bias', 'stages.3.0.adapter.up.weight', 'stages.3.0.adapter.up.bias', 'stages.3.1.adapter.down.weight', 'stages.3.1.adapter.down.bias', 'stages.3.1.adapter.up.weight', 'stages.3.1.adapter.up.bias', 'stages.3.2.adapter.down.weight', 'stages.3.2.adapter.down.bias', 'stages.3.2.adapter.up.weight', 'stages.3.2.adapter.up.bias'], unexpected_keys=[])
[2024-07-01 21:39:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth'
[2024-07-01 21:40:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 21.245 (21.245)	Loss 0.3735 (0.3735)	Acc@1 91.016 (91.016)	Acc@5 98.828 (98.828)	Mem 3082MB
[2024-07-01 21:40:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 65.006 Acc@5 77.592
[2024-07-01 21:40:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 162): INFO Accuracy of the network on the 50000 test images: 65.0%
[2024-07-01 21:40:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 168): INFO Start training
[2024-07-01 21:40:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][0/2502]	eta 11:04:16 lr 0.000100	 wd 0.0000	time 15.9299 (15.9299)	loss 1.6934 (1.6934)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 7964MB
[2024-07-01 21:41:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:13:35 lr 0.000100	 wd 0.0000	time 0.1609 (0.3394)	loss 1.3691 (1.5232)	grad_norm 0.4652 (nan)	loss_scale 32768.0000 (33092.4356)	mem 7964MB
[2024-07-01 21:41:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:09:55 lr 0.000100	 wd 0.0000	time 0.1893 (0.2588)	loss 1.1836 (1.4774)	grad_norm 0.4494 (nan)	loss_scale 32768.0000 (32931.0249)	mem 7964MB
[2024-07-01 21:41:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:08:32 lr 0.000100	 wd 0.0000	time 0.1722 (0.2326)	loss 1.6377 (1.4449)	grad_norm 0.4824 (nan)	loss_scale 32768.0000 (32876.8638)	mem 7964MB
[2024-07-01 21:41:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:07:42 lr 0.000100	 wd 0.0000	time 0.1667 (0.2201)	loss 1.4971 (1.4074)	grad_norm 0.4926 (nan)	loss_scale 32768.0000 (32849.7157)	mem 7964MB
[2024-07-01 21:42:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:07:03 lr 0.000100	 wd 0.0000	time 0.1651 (0.2117)	loss 1.1475 (1.3752)	grad_norm 0.5125 (nan)	loss_scale 32768.0000 (32833.4052)	mem 7964MB
[2024-07-01 21:42:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:06:32 lr 0.000100	 wd 0.0000	time 0.1713 (0.2066)	loss 1.2422 (1.3405)	grad_norm 0.5206 (nan)	loss_scale 32768.0000 (32822.5225)	mem 7964MB
[2024-07-01 21:42:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:06:05 lr 0.000100	 wd 0.0000	time 0.1697 (0.2026)	loss 1.0596 (1.3058)	grad_norm 0.5566 (nan)	loss_scale 32768.0000 (32814.7447)	mem 7964MB
[2024-07-01 21:43:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:05:39 lr 0.000100	 wd 0.0000	time 0.1881 (0.1996)	loss 1.0527 (1.2783)	grad_norm 0.5234 (nan)	loss_scale 32768.0000 (32808.9089)	mem 7964MB
[2024-07-01 21:43:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:05:35 lr 0.000100	 wd 0.0000	time 0.1716 (0.2096)	loss 1.1836 (1.2523)	grad_norm 0.5580 (nan)	loss_scale 32768.0000 (32804.3685)	mem 7964MB
[2024-07-01 21:43:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:05:11 lr 0.000100	 wd 0.0000	time 0.1840 (0.2072)	loss 0.9736 (1.2305)	grad_norm 0.5519 (nan)	loss_scale 32768.0000 (32800.7353)	mem 7964MB
[2024-07-01 21:44:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:04:47 lr 0.000100	 wd 0.0000	time 0.1642 (0.2054)	loss 1.0254 (1.2119)	grad_norm 0.5649 (nan)	loss_scale 32768.0000 (32797.7620)	mem 7964MB
[2024-07-01 21:44:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:04:24 lr 0.000100	 wd 0.0000	time 0.1768 (0.2033)	loss 0.9062 (1.1955)	grad_norm 0.5134 (nan)	loss_scale 32768.0000 (32795.2839)	mem 7964MB
[2024-07-01 21:45:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:04:09 lr 0.000100	 wd 0.0000	time 0.1825 (0.2073)	loss 0.9829 (1.1822)	grad_norm 0.5469 (nan)	loss_scale 32768.0000 (32793.1868)	mem 7964MB
[2024-07-01 21:45:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:03:48 lr 0.000100	 wd 0.0000	time 0.1654 (0.2071)	loss 0.9609 (1.1691)	grad_norm 0.5450 (nan)	loss_scale 32768.0000 (32791.3890)	mem 7964MB
[2024-07-01 21:45:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:03:26 lr 0.000100	 wd 0.0000	time 0.1745 (0.2056)	loss 1.0518 (1.1581)	grad_norm 0.5397 (nan)	loss_scale 32768.0000 (32789.8308)	mem 7964MB
[2024-07-01 21:45:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:03:04 lr 0.000100	 wd 0.0000	time 0.1798 (0.2046)	loss 1.1367 (1.1485)	grad_norm 0.5174 (nan)	loss_scale 32768.0000 (32788.4672)	mem 7964MB
[2024-07-01 21:46:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:02:43 lr 0.000100	 wd 0.0000	time 0.1703 (0.2034)	loss 0.9722 (1.1392)	grad_norm 0.4945 (nan)	loss_scale 32768.0000 (32787.2640)	mem 7964MB
[2024-07-01 21:46:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:02:23 lr 0.000100	 wd 0.0000	time 0.3262 (0.2048)	loss 0.9321 (1.1304)	grad_norm 0.5280 (nan)	loss_scale 32768.0000 (32786.1943)	mem 7964MB
[2024-07-01 21:46:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:02 lr 0.000100	 wd 0.0000	time 0.1578 (0.2039)	loss 1.1318 (1.1225)	grad_norm 0.4998 (nan)	loss_scale 32768.0000 (32785.2372)	mem 7964MB
[2024-07-01 21:47:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:01:42 lr 0.000100	 wd 0.0000	time 0.1899 (0.2034)	loss 1.0273 (1.1156)	grad_norm 0.5122 (nan)	loss_scale 32768.0000 (32784.3758)	mem 7964MB
[2024-07-01 21:47:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:21 lr 0.000100	 wd 0.0000	time 0.1884 (0.2029)	loss 0.9907 (1.1094)	grad_norm 0.4875 (nan)	loss_scale 32768.0000 (32783.5964)	mem 7964MB
[2024-07-01 21:47:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:01 lr 0.000100	 wd 0.0000	time 0.1610 (0.2020)	loss 1.0537 (1.1027)	grad_norm 0.4936 (nan)	loss_scale 32768.0000 (32782.8878)	mem 7964MB
[2024-07-01 21:48:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:40 lr 0.000100	 wd 0.0000	time 0.4202 (0.2022)	loss 1.0791 (1.0967)	grad_norm 0.5086 (nan)	loss_scale 32768.0000 (32782.2408)	mem 7964MB
[2024-07-01 21:48:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:20 lr 0.000100	 wd 0.0000	time 0.1779 (0.2023)	loss 1.1465 (1.0916)	grad_norm 0.4697 (nan)	loss_scale 32768.0000 (32781.6476)	mem 7964MB
[2024-07-01 21:48:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000100	 wd 0.0000	time 0.1598 (0.2015)	loss 0.9492 (1.0864)	grad_norm 0.4685 (nan)	loss_scale 32768.0000 (32781.1020)	mem 7964MB
[2024-07-01 21:48:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 0 training takes 0:08:27
[2024-07-01 21:48:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_0.pth saving......
[2024-07-01 21:48:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_0.pth saved !!!
[2024-07-01 21:49:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 15.770 (15.770)	Loss 0.3950 (0.3950)	Acc@1 91.211 (91.211)	Acc@5 98.633 (98.633)	Mem 7964MB
[2024-07-01 21:49:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 83.362 Acc@5 96.944
[2024-07-01 21:49:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.4%
[2024-07-01 21:49:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 83.36%
[2024-07-01 21:49:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 21:49:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 21:49:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][0/2502]	eta 10:45:57 lr 0.000100	 wd 0.0000	time 15.4905 (15.4905)	loss 0.8115 (0.8115)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:50:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:15:26 lr 0.000100	 wd 0.0000	time 0.1861 (0.3859)	loss 0.8687 (0.9503)	grad_norm 0.4797 (0.4741)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:50:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:10:51 lr 0.000100	 wd 0.0000	time 0.1847 (0.2829)	loss 0.8672 (0.9563)	grad_norm 0.4602 (0.4729)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:50:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:09:09 lr 0.000100	 wd 0.0000	time 0.1953 (0.2494)	loss 1.0156 (0.9569)	grad_norm 0.4655 (0.4721)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:50:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:08:06 lr 0.000100	 wd 0.0000	time 0.1683 (0.2317)	loss 0.9302 (0.9578)	grad_norm 0.4798 (0.4714)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:51:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:07:22 lr 0.000100	 wd 0.0000	time 0.1610 (0.2210)	loss 0.8516 (0.9544)	grad_norm 0.4665 (0.4701)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:51:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:06:56 lr 0.000100	 wd 0.0000	time 0.1905 (0.2190)	loss 0.9814 (0.9551)	grad_norm 0.4717 (0.4689)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:51:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:06:25 lr 0.000100	 wd 0.0000	time 0.1564 (0.2138)	loss 0.8691 (0.9536)	grad_norm 0.4660 (0.4679)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:52:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:05:58 lr 0.000100	 wd 0.0000	time 0.2101 (0.2104)	loss 1.0869 (0.9549)	grad_norm 0.4566 (0.4667)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:52:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:05:31 lr 0.000099	 wd 0.0000	time 0.1765 (0.2070)	loss 0.8882 (0.9539)	grad_norm 0.4506 (0.4652)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:52:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:05:07 lr 0.000099	 wd 0.0000	time 0.1768 (0.2046)	loss 0.9707 (0.9534)	grad_norm 0.4484 (0.4640)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:53:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:04:48 lr 0.000099	 wd 0.0000	time 0.1928 (0.2059)	loss 1.0576 (0.9538)	grad_norm 0.4531 (0.4628)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:53:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:04:26 lr 0.000099	 wd 0.0000	time 0.1691 (0.2044)	loss 0.9648 (0.9534)	grad_norm 0.4739 (0.4620)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:53:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:04:03 lr 0.000099	 wd 0.0000	time 0.1771 (0.2028)	loss 0.9380 (0.9535)	grad_norm 0.4463 (0.4610)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:54:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:03:42 lr 0.000099	 wd 0.0000	time 0.1580 (0.2015)	loss 0.9683 (0.9531)	grad_norm 0.4528 (0.4598)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:54:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:03:20 lr 0.000099	 wd 0.0000	time 0.1725 (0.2002)	loss 0.9541 (0.9533)	grad_norm 0.4388 (0.4588)	loss_scale 65536.0000 (32811.6616)	mem 7964MB
[2024-07-01 21:54:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:03:03 lr 0.000099	 wd 0.0000	time 0.1811 (0.2032)	loss 0.8613 (0.9527)	grad_norm 0.4157 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-01 21:55:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:02:42 lr 0.000099	 wd 0.0000	time 0.1927 (0.2025)	loss 0.8242 (0.9522)	grad_norm 0.4469 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-01 21:55:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:02:21 lr 0.000099	 wd 0.0000	time 0.1536 (0.2017)	loss 1.0518 (0.9519)	grad_norm 0.4213 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-01 21:55:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:00 lr 0.000099	 wd 0.0000	time 0.1888 (0.2008)	loss 0.9868 (0.9518)	grad_norm 0.4352 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-01 21:56:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:01:40 lr 0.000099	 wd 0.0000	time 0.3056 (0.2000)	loss 0.8315 (0.9515)	grad_norm 0.4332 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 21:56:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:20 lr 0.000099	 wd 0.0000	time 0.3420 (0.2004)	loss 0.8843 (0.9511)	grad_norm 0.4465 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 21:56:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:00 lr 0.000099	 wd 0.0000	time 0.1593 (0.2005)	loss 0.8086 (0.9498)	grad_norm 0.4107 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 21:57:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:40 lr 0.000099	 wd 0.0000	time 0.1861 (0.1999)	loss 0.9878 (0.9502)	grad_norm 0.4320 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 21:57:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:20 lr 0.000099	 wd 0.0000	time 0.1889 (0.1993)	loss 0.9468 (0.9498)	grad_norm 0.4322 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 21:57:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000099	 wd 0.0000	time 0.1650 (0.1984)	loss 0.9517 (0.9498)	grad_norm 0.4415 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 21:57:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 1 training takes 0:08:19
[2024-07-01 21:58:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 20.300 (20.300)	Loss 0.3914 (0.3914)	Acc@1 91.406 (91.406)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 21:58:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.118 Acc@5 97.168
[2024-07-01 21:58:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-01 21:58:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 84.12%
[2024-07-01 21:58:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 21:58:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 21:58:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][0/2502]	eta 10:11:26 lr 0.000099	 wd 0.0000	time 14.6630 (14.6630)	loss 0.8433 (0.8433)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:58:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:13:11 lr 0.000099	 wd 0.0000	time 0.1818 (0.3296)	loss 1.0371 (0.9385)	grad_norm 0.4304 (0.4212)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:59:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:09:49 lr 0.000099	 wd 0.0000	time 0.1720 (0.2559)	loss 1.0537 (0.9349)	grad_norm 0.4379 (0.4226)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:59:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:08:26 lr 0.000099	 wd 0.0000	time 0.1929 (0.2299)	loss 0.8477 (0.9305)	grad_norm 0.4297 (0.4228)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 21:59:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:07:42 lr 0.000099	 wd 0.0000	time 0.1765 (0.2201)	loss 1.0430 (0.9317)	grad_norm 0.4011 (0.4231)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:00:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:07:04 lr 0.000099	 wd 0.0000	time 0.1664 (0.2121)	loss 0.9580 (0.9373)	grad_norm 0.4349 (0.4224)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:00:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:06:33 lr 0.000099	 wd 0.0000	time 0.1619 (0.2070)	loss 0.8428 (0.9375)	grad_norm 0.4215 (0.4218)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:00:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:06:06 lr 0.000099	 wd 0.0000	time 0.1817 (0.2034)	loss 0.9312 (0.9372)	grad_norm 0.4207 (0.4213)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:00:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:05:41 lr 0.000099	 wd 0.0000	time 0.1743 (0.2004)	loss 0.8447 (0.9351)	grad_norm 0.4126 (0.4205)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:01:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:05:24 lr 0.000098	 wd 0.0000	time 0.1651 (0.2028)	loss 0.9668 (0.9349)	grad_norm 0.4325 (0.4203)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:01:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:05:02 lr 0.000098	 wd 0.0000	time 0.1603 (0.2015)	loss 0.8643 (0.9337)	grad_norm 0.4154 (0.4197)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:01:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:04:40 lr 0.000098	 wd 0.0000	time 0.1798 (0.1998)	loss 0.9585 (0.9333)	grad_norm 0.4255 (0.4194)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:02:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:04:18 lr 0.000098	 wd 0.0000	time 0.1677 (0.1984)	loss 1.1045 (0.9333)	grad_norm 0.4233 (0.4189)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:02:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:03:57 lr 0.000098	 wd 0.0000	time 0.1805 (0.1973)	loss 0.9546 (0.9338)	grad_norm 0.4289 (0.4183)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:02:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:03:37 lr 0.000098	 wd 0.0000	time 0.1596 (0.1973)	loss 1.0127 (0.9337)	grad_norm 0.4060 (0.4177)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:03:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:03:17 lr 0.000098	 wd 0.0000	time 0.1729 (0.1973)	loss 1.0918 (0.9340)	grad_norm 0.4109 (0.4172)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:03:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:02:57 lr 0.000098	 wd 0.0000	time 0.1791 (0.1965)	loss 0.9668 (0.9333)	grad_norm 0.4176 (0.4168)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:03:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:02:37 lr 0.000098	 wd 0.0000	time 0.1727 (0.1959)	loss 0.9336 (0.9338)	grad_norm 0.4148 (0.4164)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:04:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:02:16 lr 0.000098	 wd 0.0000	time 0.1974 (0.1951)	loss 0.9087 (0.9336)	grad_norm 0.3855 (0.4160)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:04:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:01:58 lr 0.000098	 wd 0.0000	time 0.1741 (0.1970)	loss 0.8335 (0.9338)	grad_norm 0.4019 (0.4157)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:04:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:01:38 lr 0.000098	 wd 0.0000	time 0.1716 (0.1970)	loss 0.9155 (0.9336)	grad_norm 0.3883 (0.4152)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:05:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:19 lr 0.000098	 wd 0.0000	time 0.1773 (0.1967)	loss 0.9751 (0.9341)	grad_norm 0.4108 (0.4149)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:05:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:00:59 lr 0.000098	 wd 0.0000	time 0.1702 (0.1963)	loss 0.9004 (0.9337)	grad_norm 0.3977 (0.4143)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:05:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:39 lr 0.000098	 wd 0.0000	time 0.1698 (0.1958)	loss 0.8657 (0.9334)	grad_norm 0.4264 (0.4139)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:06:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:20 lr 0.000098	 wd 0.0000	time 0.1697 (0.1979)	loss 0.8979 (0.9334)	grad_norm 0.4037 (0.4134)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:06:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000098	 wd 0.0000	time 0.1672 (0.1972)	loss 0.9360 (0.9328)	grad_norm 0.4103 (0.4130)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:06:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 2 training takes 0:08:18
[2024-07-01 22:06:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.161 (16.161)	Loss 0.3862 (0.3862)	Acc@1 90.820 (90.820)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 22:07:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.418 Acc@5 97.344
[2024-07-01 22:07:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-07-01 22:07:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 84.42%
[2024-07-01 22:07:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:07:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:07:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][0/2502]	eta 10:30:48 lr 0.000098	 wd 0.0000	time 15.1275 (15.1275)	loss 0.7637 (0.7637)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:07:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:14:03 lr 0.000098	 wd 0.0000	time 0.1622 (0.3512)	loss 1.0596 (0.9238)	grad_norm 0.4235 (0.4045)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:07:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:10:35 lr 0.000097	 wd 0.0000	time 0.1742 (0.2762)	loss 1.0723 (0.9202)	grad_norm 0.4192 (0.4028)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:08:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:08:58 lr 0.000097	 wd 0.0000	time 0.1897 (0.2445)	loss 1.0654 (0.9214)	grad_norm 0.3938 (0.4020)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:08:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:08:02 lr 0.000097	 wd 0.0000	time 0.1656 (0.2297)	loss 1.0117 (0.9212)	grad_norm 0.3802 (0.4007)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:08:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:07:18 lr 0.000097	 wd 0.0000	time 0.1653 (0.2191)	loss 0.9639 (0.9226)	grad_norm nan (nan)	loss_scale 32768.0000 (32898.8104)	mem 7964MB
[2024-07-01 22:09:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:06:45 lr 0.000097	 wd 0.0000	time 0.1765 (0.2129)	loss 0.8691 (0.9230)	grad_norm 0.4284 (nan)	loss_scale 32768.0000 (32877.0449)	mem 7964MB
[2024-07-01 22:09:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:06:22 lr 0.000097	 wd 0.0000	time 0.1707 (0.2124)	loss 0.9736 (0.9239)	grad_norm 0.3988 (nan)	loss_scale 32768.0000 (32861.4893)	mem 7964MB
[2024-07-01 22:09:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:05:55 lr 0.000097	 wd 0.0000	time 0.1600 (0.2088)	loss 0.9150 (0.9227)	grad_norm 0.4141 (nan)	loss_scale 32768.0000 (32849.8177)	mem 7964MB
[2024-07-01 22:10:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:05:29 lr 0.000097	 wd 0.0000	time 0.1855 (0.2059)	loss 0.9663 (0.9229)	grad_norm 0.3951 (nan)	loss_scale 32768.0000 (32840.7370)	mem 7964MB
[2024-07-01 22:10:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:05:05 lr 0.000097	 wd 0.0000	time 0.1709 (0.2034)	loss 1.0762 (0.9245)	grad_norm 0.4218 (nan)	loss_scale 32768.0000 (32833.4705)	mem 7964MB
[2024-07-01 22:10:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:04:42 lr 0.000097	 wd 0.0000	time 0.1615 (0.2012)	loss 0.8198 (0.9235)	grad_norm 0.4151 (nan)	loss_scale 32768.0000 (32827.5241)	mem 7964MB
[2024-07-01 22:11:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:04:25 lr 0.000097	 wd 0.0000	time 0.1692 (0.2037)	loss 0.9858 (0.9227)	grad_norm 0.3851 (nan)	loss_scale 32768.0000 (32822.5679)	mem 7964MB
[2024-07-01 22:11:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:04:03 lr 0.000097	 wd 0.0000	time 0.1681 (0.2023)	loss 1.0332 (0.9236)	grad_norm 0.3719 (nan)	loss_scale 32768.0000 (32818.3736)	mem 7964MB
[2024-07-01 22:11:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:03:41 lr 0.000097	 wd 0.0000	time 0.1761 (0.2009)	loss 0.8931 (0.9233)	grad_norm 0.3747 (nan)	loss_scale 32768.0000 (32814.7780)	mem 7964MB
[2024-07-01 22:12:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:03:19 lr 0.000097	 wd 0.0000	time 0.1679 (0.1996)	loss 0.8467 (0.9242)	grad_norm 0.3753 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-01 22:12:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:02:58 lr 0.000096	 wd 0.0000	time 0.1603 (0.1984)	loss 0.8848 (0.9233)	grad_norm 0.4001 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-01 22:12:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:02:40 lr 0.000096	 wd 0.0000	time 0.1692 (0.1999)	loss 0.9556 (0.9232)	grad_norm 0.3858 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-01 22:13:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:02:19 lr 0.000096	 wd 0.0000	time 0.1678 (0.1994)	loss 0.9287 (0.9230)	grad_norm 0.3700 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-01 22:13:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:01:59 lr 0.000096	 wd 0.0000	time 0.1632 (0.1989)	loss 0.9448 (0.9225)	grad_norm 0.3863 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-01 22:13:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:01:39 lr 0.000096	 wd 0.0000	time 0.1782 (0.1981)	loss 0.8740 (0.9228)	grad_norm 0.3874 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 22:13:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:19 lr 0.000096	 wd 0.0000	time 0.1703 (0.1975)	loss 0.7446 (0.9222)	grad_norm 0.3981 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 22:14:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:00 lr 0.000096	 wd 0.0000	time 0.1771 (0.1999)	loss 0.8472 (0.9217)	grad_norm 0.3925 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 22:14:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:00:40 lr 0.000096	 wd 0.0000	time 0.1702 (0.1995)	loss 1.0068 (0.9219)	grad_norm 0.3842 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 22:15:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:20 lr 0.000096	 wd 0.0000	time 0.1860 (0.1991)	loss 0.8457 (0.9217)	grad_norm 0.3771 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 22:15:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000096	 wd 0.0000	time 0.1531 (0.1981)	loss 0.8262 (0.9222)	grad_norm 0.4027 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 22:15:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 3 training takes 0:08:19
[2024-07-01 22:15:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.347 (16.347)	Loss 0.3877 (0.3877)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 22:15:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.628 Acc@5 97.424
[2024-07-01 22:15:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-01 22:15:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 84.63%
[2024-07-01 22:15:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:16:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:16:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][0/2502]	eta 10:18:33 lr 0.000096	 wd 0.0000	time 14.8337 (14.8337)	loss 0.9307 (0.9307)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:16:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:13:15 lr 0.000096	 wd 0.0000	time 0.1891 (0.3310)	loss 0.8032 (0.9212)	grad_norm 0.3859 (0.3872)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:16:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:09:51 lr 0.000096	 wd 0.0000	time 0.1686 (0.2568)	loss 0.9199 (0.9198)	grad_norm 0.3616 (0.3870)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:17:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:08:29 lr 0.000095	 wd 0.0000	time 0.1780 (0.2312)	loss 0.9209 (0.9193)	grad_norm 0.3950 (0.3876)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:17:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:07:47 lr 0.000095	 wd 0.0000	time 0.1730 (0.2225)	loss 0.8809 (0.9147)	grad_norm 0.3745 (0.3877)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:17:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:07:12 lr 0.000095	 wd 0.0000	time 0.1704 (0.2159)	loss 0.9648 (0.9163)	grad_norm 0.3765 (0.3876)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:18:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:06:40 lr 0.000095	 wd 0.0000	time 0.1675 (0.2104)	loss 0.8823 (0.9186)	grad_norm 0.4027 (0.3876)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:18:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:06:12 lr 0.000095	 wd 0.0000	time 0.1789 (0.2065)	loss 1.0469 (0.9151)	grad_norm 0.3754 (0.3871)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:18:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:05:47 lr 0.000095	 wd 0.0000	time 0.1602 (0.2040)	loss 0.8657 (0.9152)	grad_norm 0.3991 (0.3868)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:19:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:05:26 lr 0.000095	 wd 0.0000	time 0.1769 (0.2036)	loss 0.8799 (0.9164)	grad_norm 0.3915 (0.3865)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:19:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:05:03 lr 0.000095	 wd 0.0000	time 0.1652 (0.2022)	loss 0.9224 (0.9163)	grad_norm 0.3874 (0.3866)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:19:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:04:41 lr 0.000095	 wd 0.0000	time 0.1646 (0.2005)	loss 1.0039 (0.9179)	grad_norm 0.3937 (0.3866)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:19:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:04:19 lr 0.000095	 wd 0.0000	time 0.1782 (0.1993)	loss 0.7563 (0.9177)	grad_norm 0.3728 (0.3863)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:20:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:03:57 lr 0.000095	 wd 0.0000	time 0.1756 (0.1979)	loss 0.8779 (0.9170)	grad_norm 0.3813 (0.3859)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:20:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:03:41 lr 0.000094	 wd 0.0000	time 0.1829 (0.2011)	loss 0.8960 (0.9163)	grad_norm 0.3831 (0.3860)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:21:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:03:23 lr 0.000094	 wd 0.0000	time 0.1802 (0.2029)	loss 0.7598 (0.9163)	grad_norm 0.3842 (0.3859)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:21:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:03:02 lr 0.000094	 wd 0.0000	time 0.1616 (0.2019)	loss 0.8545 (0.9160)	grad_norm 0.3747 (0.3857)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:21:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:02:41 lr 0.000094	 wd 0.0000	time 0.1806 (0.2008)	loss 1.1016 (0.9156)	grad_norm 0.3800 (0.3855)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:22:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:02:20 lr 0.000094	 wd 0.0000	time 0.1807 (0.2000)	loss 0.9355 (0.9161)	grad_norm 0.3775 (0.3854)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:22:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:01 lr 0.000094	 wd 0.0000	time 0.2961 (0.2025)	loss 0.9302 (0.9161)	grad_norm 0.3690 (0.3850)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:22:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:01:41 lr 0.000094	 wd 0.0000	time 0.1844 (0.2026)	loss 0.7822 (0.9156)	grad_norm nan (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 22:23:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:21 lr 0.000094	 wd 0.0000	time 0.1854 (0.2021)	loss 0.8320 (0.9164)	grad_norm 0.3964 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 22:23:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:00 lr 0.000094	 wd 0.0000	time 0.1534 (0.2015)	loss 0.9961 (0.9159)	grad_norm 0.3454 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 22:23:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:40 lr 0.000094	 wd 0.0000	time 0.1655 (0.2008)	loss 0.8784 (0.9160)	grad_norm 0.3832 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 22:24:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:20 lr 0.000093	 wd 0.0000	time 0.1992 (0.2023)	loss 0.8550 (0.9162)	grad_norm 0.3542 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 22:24:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000093	 wd 0.0000	time 0.1616 (0.2017)	loss 0.7803 (0.9161)	grad_norm 0.3772 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 22:24:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 4 training takes 0:08:28
[2024-07-01 22:24:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.467 (16.467)	Loss 0.3726 (0.3726)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 22:24:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.768 Acc@5 97.458
[2024-07-01 22:24:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-01 22:24:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-01 22:24:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:24:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:25:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][0/2502]	eta 9:54:03 lr 0.000093	 wd 0.0000	time 14.2459 (14.2459)	loss 0.9121 (0.9121)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:25:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:13:57 lr 0.000093	 wd 0.0000	time 0.1709 (0.3488)	loss 0.9688 (0.9053)	grad_norm 0.3885 (0.3803)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:25:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:10:27 lr 0.000093	 wd 0.0000	time 0.1561 (0.2726)	loss 0.8447 (0.9080)	grad_norm 0.3853 (0.3814)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:26:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:08:51 lr 0.000093	 wd 0.0000	time 0.1572 (0.2413)	loss 1.0537 (0.9067)	grad_norm 0.3772 (0.3801)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:26:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:08:00 lr 0.000093	 wd 0.0000	time 0.1675 (0.2286)	loss 0.9917 (0.9058)	grad_norm 0.3825 (0.3804)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:26:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:07:16 lr 0.000093	 wd 0.0000	time 0.1540 (0.2181)	loss 0.8750 (0.9059)	grad_norm 0.3974 (0.3800)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:27:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:06:57 lr 0.000093	 wd 0.0000	time 0.1761 (0.2195)	loss 0.8994 (0.9088)	grad_norm 0.3825 (0.3797)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:27:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:06:28 lr 0.000093	 wd 0.0000	time 0.1545 (0.2155)	loss 0.8149 (0.9079)	grad_norm 0.4002 (0.3796)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:27:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:05:59 lr 0.000093	 wd 0.0000	time 0.1759 (0.2114)	loss 0.8604 (0.9079)	grad_norm 0.3867 (0.3797)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:28:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:05:33 lr 0.000092	 wd 0.0000	time 0.1634 (0.2083)	loss 0.9448 (0.9082)	grad_norm 0.3755 (0.3796)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:28:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:05:08 lr 0.000092	 wd 0.0000	time 0.1727 (0.2056)	loss 0.9526 (0.9099)	grad_norm 0.3739 (0.3795)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:28:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:04:46 lr 0.000092	 wd 0.0000	time 0.2068 (0.2045)	loss 0.8521 (0.9111)	grad_norm 0.3765 (0.3795)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:29:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:04:27 lr 0.000092	 wd 0.0000	time 0.1699 (0.2055)	loss 0.9268 (0.9117)	grad_norm 0.3677 (0.3795)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:29:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:04:04 lr 0.000092	 wd 0.0000	time 0.1541 (0.2038)	loss 1.0098 (0.9112)	grad_norm 0.3862 (0.3791)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:29:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:03:43 lr 0.000092	 wd 0.0000	time 0.1816 (0.2026)	loss 0.8877 (0.9103)	grad_norm 0.3840 (0.3790)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:29:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:03:21 lr 0.000092	 wd 0.0000	time 0.1693 (0.2011)	loss 0.8677 (0.9108)	grad_norm 0.3761 (0.3789)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:30:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:03:03 lr 0.000092	 wd 0.0000	time 0.2295 (0.2038)	loss 0.8047 (0.9113)	grad_norm 0.3903 (0.3789)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:30:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:02:43 lr 0.000092	 wd 0.0000	time 0.1727 (0.2043)	loss 0.8896 (0.9116)	grad_norm 0.3684 (0.3789)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:31:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:02:22 lr 0.000091	 wd 0.0000	time 0.1659 (0.2035)	loss 0.9092 (0.9109)	grad_norm 0.3845 (0.3789)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:31:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:02 lr 0.000091	 wd 0.0000	time 0.2109 (0.2028)	loss 0.8760 (0.9107)	grad_norm 0.3689 (0.3790)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:31:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:01:41 lr 0.000091	 wd 0.0000	time 0.1642 (0.2020)	loss 0.9976 (0.9112)	grad_norm 0.3836 (0.3790)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:32:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:22 lr 0.000091	 wd 0.0000	time 0.1737 (0.2051)	loss 0.9883 (0.9107)	grad_norm 0.3897 (0.3789)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:32:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:02 lr 0.000091	 wd 0.0000	time 0.1624 (0.2055)	loss 1.0225 (0.9106)	grad_norm 0.3789 (0.3787)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:32:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:41 lr 0.000091	 wd 0.0000	time 0.1694 (0.2048)	loss 0.7778 (0.9104)	grad_norm 0.4013 (0.3787)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:33:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:20 lr 0.000091	 wd 0.0000	time 0.1670 (0.2041)	loss 0.8188 (0.9099)	grad_norm 0.3864 (0.3786)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:33:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000091	 wd 0.0000	time 0.1585 (0.2031)	loss 0.8052 (0.9093)	grad_norm 0.3758 (0.3784)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:33:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 5 training takes 0:08:31
[2024-07-01 22:33:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 20.642 (20.642)	Loss 0.3713 (0.3713)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 22:34:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.888 Acc@5 97.502
[2024-07-01 22:34:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-01 22:34:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 84.89%
[2024-07-01 22:34:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:34:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:34:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][0/2502]	eta 10:49:19 lr 0.000091	 wd 0.0000	time 15.5713 (15.5713)	loss 0.9453 (0.9453)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:34:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:13:22 lr 0.000090	 wd 0.0000	time 0.1625 (0.3341)	loss 0.8784 (0.9165)	grad_norm 0.3902 (0.3755)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:34:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:09:50 lr 0.000090	 wd 0.0000	time 0.1851 (0.2566)	loss 0.8159 (0.9088)	grad_norm 0.3783 (0.3760)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:35:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:08:32 lr 0.000090	 wd 0.0000	time 0.1789 (0.2327)	loss 0.9453 (0.9099)	grad_norm 0.3708 (0.3759)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:35:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:07:43 lr 0.000090	 wd 0.0000	time 0.1725 (0.2204)	loss 0.8911 (0.9085)	grad_norm 0.3639 (0.3765)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:35:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:07:05 lr 0.000090	 wd 0.0000	time 0.1621 (0.2124)	loss 0.9604 (0.9087)	grad_norm 0.3653 (0.3766)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:36:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:06:36 lr 0.000090	 wd 0.0000	time 0.1674 (0.2085)	loss 0.8901 (0.9082)	grad_norm 0.3866 (0.3768)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:36:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:06:08 lr 0.000090	 wd 0.0000	time 0.1537 (0.2045)	loss 1.0020 (0.9080)	grad_norm 0.3807 (0.3770)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:36:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:05:42 lr 0.000090	 wd 0.0000	time 0.1589 (0.2014)	loss 0.7412 (0.9080)	grad_norm 0.3745 (0.3766)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:37:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:05:27 lr 0.000089	 wd 0.0000	time 0.1703 (0.2042)	loss 0.8950 (0.9076)	grad_norm 0.3605 (0.3765)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:37:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:05:04 lr 0.000089	 wd 0.0000	time 0.1738 (0.2025)	loss 0.8564 (0.9086)	grad_norm 0.3790 (nan)	loss_scale 32768.0000 (32833.4705)	mem 7964MB
[2024-07-01 22:37:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:04:41 lr 0.000089	 wd 0.0000	time 0.1783 (0.2007)	loss 0.8237 (0.9070)	grad_norm 0.3734 (nan)	loss_scale 32768.0000 (32827.5241)	mem 7964MB
[2024-07-01 22:38:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:04:19 lr 0.000089	 wd 0.0000	time 0.1612 (0.1992)	loss 0.8667 (0.9070)	grad_norm 0.3821 (nan)	loss_scale 32768.0000 (32822.5679)	mem 7964MB
[2024-07-01 22:38:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:03:57 lr 0.000089	 wd 0.0000	time 0.1698 (0.1977)	loss 0.7734 (0.9065)	grad_norm 0.3683 (nan)	loss_scale 32768.0000 (32818.3736)	mem 7964MB
[2024-07-01 22:38:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:03:39 lr 0.000089	 wd 0.0000	time 0.1666 (0.1995)	loss 0.9307 (0.9061)	grad_norm 0.3717 (nan)	loss_scale 32768.0000 (32814.7780)	mem 7964MB
[2024-07-01 22:38:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:03:19 lr 0.000089	 wd 0.0000	time 0.1762 (0.1986)	loss 0.8062 (0.9060)	grad_norm 0.3756 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-01 22:39:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:02:58 lr 0.000089	 wd 0.0000	time 0.1650 (0.1979)	loss 0.8154 (0.9054)	grad_norm 0.3618 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-01 22:39:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:02:38 lr 0.000088	 wd 0.0000	time 0.1712 (0.1972)	loss 0.8721 (0.9041)	grad_norm 0.3616 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-01 22:39:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:02:17 lr 0.000088	 wd 0.0000	time 0.1862 (0.1965)	loss 0.8433 (0.9040)	grad_norm 0.3648 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-01 22:40:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:01 lr 0.000088	 wd 0.0000	time 0.1682 (0.2021)	loss 0.9106 (0.9044)	grad_norm 0.3716 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-01 22:40:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:01:41 lr 0.000088	 wd 0.0000	time 0.2255 (0.2017)	loss 0.9429 (0.9050)	grad_norm 0.3802 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 22:41:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:20 lr 0.000088	 wd 0.0000	time 0.1798 (0.2013)	loss 0.8218 (0.9049)	grad_norm 0.3725 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 22:41:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:00 lr 0.000088	 wd 0.0000	time 0.1683 (0.2007)	loss 0.9102 (0.9043)	grad_norm 0.3722 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 22:41:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:40 lr 0.000088	 wd 0.0000	time 0.1803 (0.2025)	loss 0.8452 (0.9041)	grad_norm 0.3699 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 22:42:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:20 lr 0.000087	 wd 0.0000	time 0.1682 (0.2026)	loss 0.9170 (0.9047)	grad_norm 0.3553 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 22:42:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000087	 wd 0.0000	time 0.1766 (0.2017)	loss 0.8291 (0.9053)	grad_norm 0.3852 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 22:42:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 6 training takes 0:08:28
[2024-07-01 22:42:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.183 (17.183)	Loss 0.3713 (0.3713)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 22:42:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.900 Acc@5 97.506
[2024-07-01 22:42:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-01 22:42:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 84.90%
[2024-07-01 22:42:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:42:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:43:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][0/2502]	eta 9:59:03 lr 0.000087	 wd 0.0000	time 14.3658 (14.3658)	loss 0.9126 (0.9126)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:43:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:15:42 lr 0.000087	 wd 0.0000	time 0.1707 (0.3925)	loss 0.9634 (0.9028)	grad_norm 0.3665 (0.3725)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:43:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:11:02 lr 0.000087	 wd 0.0000	time 0.1636 (0.2877)	loss 0.9009 (0.8997)	grad_norm 0.3608 (0.3745)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:44:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:09:17 lr 0.000087	 wd 0.0000	time 0.1640 (0.2532)	loss 0.8618 (0.8971)	grad_norm 0.3810 (0.3743)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:44:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:08:14 lr 0.000087	 wd 0.0000	time 0.1810 (0.2352)	loss 0.8970 (0.9020)	grad_norm 0.4035 (0.3749)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:44:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:07:27 lr 0.000087	 wd 0.0000	time 0.1728 (0.2237)	loss 0.7925 (0.8997)	grad_norm 0.3500 (0.3748)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:45:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:07:07 lr 0.000086	 wd 0.0000	time 0.1713 (0.2249)	loss 1.0254 (0.9018)	grad_norm 0.3597 (0.3745)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:45:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:06:35 lr 0.000086	 wd 0.0000	time 0.1762 (0.2193)	loss 0.8125 (0.9012)	grad_norm 0.3459 (0.3745)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:45:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:06:05 lr 0.000086	 wd 0.0000	time 0.1563 (0.2148)	loss 0.8477 (0.9002)	grad_norm 0.3576 (0.3745)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:46:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:05:37 lr 0.000086	 wd 0.0000	time 0.1570 (0.2109)	loss 0.8721 (0.9010)	grad_norm 0.3701 (0.3744)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:46:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:05:12 lr 0.000086	 wd 0.0000	time 0.1695 (0.2080)	loss 0.7847 (0.9009)	grad_norm 0.3861 (0.3742)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:46:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:04:51 lr 0.000086	 wd 0.0000	time 0.1804 (0.2076)	loss 0.8057 (0.9003)	grad_norm 0.3754 (0.3741)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:47:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:04:27 lr 0.000086	 wd 0.0000	time 0.1684 (0.2057)	loss 0.9395 (0.8997)	grad_norm 0.3743 (0.3740)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:47:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:04:05 lr 0.000085	 wd 0.0000	time 0.1674 (0.2042)	loss 0.9395 (0.8992)	grad_norm 0.3596 (0.3740)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:47:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:03:43 lr 0.000085	 wd 0.0000	time 0.1762 (0.2026)	loss 0.7847 (0.8996)	grad_norm 0.3760 (0.3742)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:48:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:03:21 lr 0.000085	 wd 0.0000	time 0.1661 (0.2012)	loss 0.8521 (0.8992)	grad_norm 0.3805 (0.3741)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:48:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:03:01 lr 0.000085	 wd 0.0000	time 0.1933 (0.2017)	loss 0.9048 (0.8989)	grad_norm 0.3742 (0.3740)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:48:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:02:41 lr 0.000085	 wd 0.0000	time 0.1746 (0.2016)	loss 1.0166 (0.8986)	grad_norm 0.3518 (0.3739)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:49:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:02:20 lr 0.000085	 wd 0.0000	time 0.1753 (0.2007)	loss 0.8643 (0.8986)	grad_norm 0.3719 (0.3738)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:49:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:00 lr 0.000085	 wd 0.0000	time 0.1720 (0.2000)	loss 0.9375 (0.8986)	grad_norm 0.3649 (0.3738)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:49:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:01:40 lr 0.000084	 wd 0.0000	time 0.1787 (0.1994)	loss 0.8521 (0.8981)	grad_norm 0.3784 (0.3736)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:50:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:21 lr 0.000084	 wd 0.0000	time 0.1720 (0.2019)	loss 1.0410 (0.8991)	grad_norm 0.3827 (0.3738)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:50:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:00 lr 0.000084	 wd 0.0000	time 0.1840 (0.2017)	loss 0.7212 (0.8984)	grad_norm 0.4000 (0.3738)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:50:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:40 lr 0.000084	 wd 0.0000	time 0.1617 (0.2012)	loss 0.9854 (0.8984)	grad_norm 0.3677 (0.3738)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:51:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:20 lr 0.000084	 wd 0.0000	time 0.1827 (0.2007)	loss 0.9878 (0.8987)	grad_norm 0.3805 (0.3738)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:51:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000084	 wd 0.0000	time 0.1617 (0.1997)	loss 0.8965 (0.8991)	grad_norm 0.3882 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 22:51:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 7 training takes 0:08:23
[2024-07-01 22:51:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 19.860 (19.860)	Loss 0.3652 (0.3652)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 22:51:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.952 Acc@5 97.538
[2024-07-01 22:51:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-01 22:51:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 84.95%
[2024-07-01 22:51:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:51:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:52:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][0/2502]	eta 10:29:36 lr 0.000084	 wd 0.0000	time 15.0984 (15.0984)	loss 0.8208 (0.8208)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:52:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:13:21 lr 0.000083	 wd 0.0000	time 0.1732 (0.3338)	loss 0.8574 (0.9012)	grad_norm 0.3603 (0.3754)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:52:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:09:50 lr 0.000083	 wd 0.0000	time 0.1535 (0.2567)	loss 0.8809 (0.9003)	grad_norm 0.3718 (0.3743)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:53:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:08:33 lr 0.000083	 wd 0.0000	time 0.1972 (0.2331)	loss 0.8584 (0.8962)	grad_norm 0.3940 (0.3735)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:53:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:07:46 lr 0.000083	 wd 0.0000	time 0.1654 (0.2217)	loss 1.0215 (0.8977)	grad_norm 0.3661 (0.3730)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:53:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:07:06 lr 0.000083	 wd 0.0000	time 0.1746 (0.2132)	loss 0.8286 (0.8988)	grad_norm 0.3677 (0.3729)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:53:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:06:36 lr 0.000083	 wd 0.0000	time 0.1786 (0.2082)	loss 0.8726 (0.8988)	grad_norm 0.3901 (0.3726)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:54:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:06:09 lr 0.000083	 wd 0.0000	time 0.1587 (0.2048)	loss 0.8911 (0.8985)	grad_norm 0.3895 (0.3726)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:54:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:05:43 lr 0.000082	 wd 0.0000	time 0.1636 (0.2018)	loss 0.7764 (0.8981)	grad_norm 0.3773 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:54:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:05:27 lr 0.000082	 wd 0.0000	time 0.1777 (0.2041)	loss 1.0352 (0.8993)	grad_norm 0.3681 (0.3726)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:55:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:05:04 lr 0.000082	 wd 0.0000	time 0.1745 (0.2024)	loss 1.0547 (0.8997)	grad_norm 0.3668 (0.3728)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:55:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:04:41 lr 0.000082	 wd 0.0000	time 0.1590 (0.2008)	loss 0.9756 (0.8993)	grad_norm 0.3789 (0.3729)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:55:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:04:19 lr 0.000082	 wd 0.0000	time 0.1746 (0.1996)	loss 0.8379 (0.8990)	grad_norm 0.3639 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:56:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:03:58 lr 0.000082	 wd 0.0000	time 0.1623 (0.1981)	loss 0.8345 (0.8990)	grad_norm 0.3681 (0.3728)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:56:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:03:43 lr 0.000081	 wd 0.0000	time 0.1779 (0.2028)	loss 0.8403 (0.8989)	grad_norm 0.3641 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:56:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:03:22 lr 0.000081	 wd 0.0000	time 0.1706 (0.2020)	loss 0.8721 (0.8985)	grad_norm 0.3870 (0.3728)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:57:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:03:01 lr 0.000081	 wd 0.0000	time 0.1637 (0.2010)	loss 0.9966 (0.8979)	grad_norm 0.3643 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:57:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:02:40 lr 0.000081	 wd 0.0000	time 0.2077 (0.1999)	loss 0.8647 (0.8987)	grad_norm 0.3542 (0.3728)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:57:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:02:19 lr 0.000081	 wd 0.0000	time 0.1793 (0.1990)	loss 0.8882 (0.8989)	grad_norm 0.3803 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:58:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:01 lr 0.000081	 wd 0.0000	time 0.1705 (0.2017)	loss 0.7915 (0.8983)	grad_norm 0.3510 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:58:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:01:41 lr 0.000080	 wd 0.0000	time 0.1682 (0.2012)	loss 0.8384 (0.8985)	grad_norm 0.3977 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:58:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:20 lr 0.000080	 wd 0.0000	time 0.1699 (0.2008)	loss 0.9053 (0.8986)	grad_norm 0.3734 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:59:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:00 lr 0.000080	 wd 0.0000	time 0.1742 (0.2002)	loss 0.8521 (0.8981)	grad_norm 0.3717 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:59:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:40 lr 0.000080	 wd 0.0000	time 0.5693 (0.2006)	loss 0.9785 (0.8978)	grad_norm 0.3616 (0.3727)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 22:59:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:20 lr 0.000080	 wd 0.0000	time 0.1681 (0.2013)	loss 0.9521 (0.8977)	grad_norm 0.3808 (0.3728)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:00:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000080	 wd 0.0000	time 0.1612 (0.2004)	loss 0.9419 (0.8977)	grad_norm 0.3791 (0.3728)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:00:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 8 training takes 0:08:25
[2024-07-01 23:00:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.869 (16.869)	Loss 0.3667 (0.3667)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 23:00:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.024 Acc@5 97.562
[2024-07-01 23:00:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-01 23:00:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.02%
[2024-07-01 23:00:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:00:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:01:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][0/2502]	eta 10:30:17 lr 0.000080	 wd 0.0000	time 15.1148 (15.1148)	loss 0.8843 (0.8843)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:01:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:15:49 lr 0.000079	 wd 0.0000	time 0.1777 (0.3954)	loss 0.9771 (0.8914)	grad_norm 0.3814 (0.3719)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:01:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:11:05 lr 0.000079	 wd 0.0000	time 0.1731 (0.2889)	loss 0.7646 (0.8893)	grad_norm 0.3672 (0.3726)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:02:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:09:18 lr 0.000079	 wd 0.0000	time 0.1613 (0.2537)	loss 0.9604 (0.8916)	grad_norm 0.3461 (0.3730)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:02:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:08:14 lr 0.000079	 wd 0.0000	time 0.1640 (0.2352)	loss 0.9077 (0.8909)	grad_norm 0.3548 (0.3729)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:02:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:07:27 lr 0.000079	 wd 0.0000	time 0.1679 (0.2237)	loss 0.6812 (0.8957)	grad_norm 0.3779 (0.3732)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:03:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:07:06 lr 0.000079	 wd 0.0000	time 0.3104 (0.2244)	loss 0.7324 (0.8934)	grad_norm 0.3892 (0.3726)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:03:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:06:34 lr 0.000078	 wd 0.0000	time 0.1718 (0.2187)	loss 0.9526 (0.8943)	grad_norm 0.3766 (0.3725)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:03:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:06:05 lr 0.000078	 wd 0.0000	time 0.1725 (0.2146)	loss 0.9214 (0.8953)	grad_norm 0.3497 (0.3726)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:03:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:05:38 lr 0.000078	 wd 0.0000	time 0.1757 (0.2111)	loss 1.0518 (0.8959)	grad_norm 0.3731 (0.3725)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:04:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:05:12 lr 0.000078	 wd 0.0000	time 0.1659 (0.2080)	loss 0.9761 (0.8962)	grad_norm 0.3802 (0.3724)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:04:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:04:51 lr 0.000078	 wd 0.0000	time 0.1851 (0.2077)	loss 0.8223 (0.8961)	grad_norm 0.3899 (0.3722)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:04:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:04:28 lr 0.000078	 wd 0.0000	time 0.1638 (0.2064)	loss 0.9790 (0.8957)	grad_norm 0.3803 (0.3721)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:05:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:04:06 lr 0.000077	 wd 0.0000	time 0.1793 (0.2048)	loss 0.9463 (0.8960)	grad_norm 0.3561 (0.3720)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:05:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:03:44 lr 0.000077	 wd 0.0000	time 0.1646 (0.2034)	loss 0.9258 (0.8963)	grad_norm 0.3558 (0.3720)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:05:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:03:22 lr 0.000077	 wd 0.0000	time 0.1548 (0.2019)	loss 0.9746 (0.8965)	grad_norm 0.3888 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-01 23:06:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:03:04 lr 0.000077	 wd 0.0000	time 0.1631 (0.2042)	loss 0.9009 (0.8961)	grad_norm 0.3592 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-01 23:06:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:02:43 lr 0.000077	 wd 0.0000	time 0.1703 (0.2035)	loss 0.8555 (0.8968)	grad_norm 0.3512 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-01 23:06:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:02:22 lr 0.000077	 wd 0.0000	time 0.1704 (0.2026)	loss 0.7524 (0.8965)	grad_norm 0.3814 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-01 23:07:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:01 lr 0.000076	 wd 0.0000	time 0.1913 (0.2016)	loss 0.9263 (0.8965)	grad_norm 0.3720 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-01 23:07:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:01:41 lr 0.000076	 wd 0.0000	time 0.1846 (0.2013)	loss 0.9370 (0.8974)	grad_norm 0.3592 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 23:08:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:22 lr 0.000076	 wd 0.0000	time 0.1793 (0.2063)	loss 1.0391 (0.8978)	grad_norm 0.3589 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 23:08:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:02 lr 0.000076	 wd 0.0000	time 0.1728 (0.2059)	loss 0.9067 (0.8980)	grad_norm 0.3730 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 23:08:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:41 lr 0.000076	 wd 0.0000	time 0.1647 (0.2051)	loss 0.8818 (0.8976)	grad_norm 0.3943 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 23:08:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:20 lr 0.000075	 wd 0.0000	time 0.1651 (0.2044)	loss 0.8306 (0.8975)	grad_norm 0.3759 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 23:09:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000075	 wd 0.0000	time 0.1796 (0.2034)	loss 0.8672 (0.8976)	grad_norm 0.3544 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 23:09:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 9 training takes 0:08:41
[2024-07-01 23:09:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.226 (16.226)	Loss 0.3635 (0.3635)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 23:09:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 84.988 Acc@5 97.612
[2024-07-01 23:09:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-01 23:09:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.02%
[2024-07-01 23:10:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][0/2502]	eta 11:24:16 lr 0.000075	 wd 0.0000	time 16.4096 (16.4096)	loss 1.0439 (1.0439)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:10:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:13:49 lr 0.000075	 wd 0.0000	time 0.1546 (0.3455)	loss 0.7300 (0.9074)	grad_norm 0.3738 (0.3730)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:10:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:10:03 lr 0.000075	 wd 0.0000	time 0.1653 (0.2621)	loss 0.9731 (0.9001)	grad_norm 0.3811 (0.3729)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:11:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:08:50 lr 0.000075	 wd 0.0000	time 0.1793 (0.2408)	loss 0.8696 (0.8960)	grad_norm 0.3568 (0.3730)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:11:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:07:55 lr 0.000075	 wd 0.0000	time 0.1748 (0.2260)	loss 0.9180 (0.8943)	grad_norm 0.3825 (0.3730)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:11:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:07:13 lr 0.000074	 wd 0.0000	time 0.1827 (0.2167)	loss 0.7749 (0.8954)	grad_norm 0.3843 (0.3732)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:12:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:06:40 lr 0.000074	 wd 0.0000	time 0.1678 (0.2107)	loss 0.9570 (0.8944)	grad_norm 0.3723 (0.3729)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:12:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:06:11 lr 0.000074	 wd 0.0000	time 0.1720 (0.2061)	loss 0.8179 (0.8932)	grad_norm 0.3550 (0.3729)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:12:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:05:53 lr 0.000074	 wd 0.0000	time 0.1891 (0.2076)	loss 0.8857 (0.8935)	grad_norm 0.3725 (0.3733)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:13:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:05:28 lr 0.000074	 wd 0.0000	time 0.1853 (0.2049)	loss 0.8574 (0.8939)	grad_norm 0.3716 (0.3731)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:13:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:05:04 lr 0.000073	 wd 0.0000	time 0.1655 (0.2026)	loss 0.8394 (0.8940)	grad_norm 0.3718 (0.3733)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:13:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:04:41 lr 0.000073	 wd 0.0000	time 0.1613 (0.2008)	loss 0.8037 (0.8941)	grad_norm 0.3707 (0.3732)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:13:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:04:19 lr 0.000073	 wd 0.0000	time 0.1743 (0.1990)	loss 1.0234 (0.8939)	grad_norm 0.3646 (0.3734)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:14:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:03:59 lr 0.000073	 wd 0.0000	time 0.1702 (0.1990)	loss 0.8809 (0.8937)	grad_norm 0.3740 (0.3735)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:14:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:03:38 lr 0.000073	 wd 0.0000	time 0.1737 (0.1987)	loss 0.7979 (0.8937)	grad_norm 0.3983 (0.3734)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:14:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:03:18 lr 0.000073	 wd 0.0000	time 0.1915 (0.1977)	loss 0.8589 (0.8933)	grad_norm 0.3862 (0.3732)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:15:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:02:57 lr 0.000072	 wd 0.0000	time 0.1742 (0.1970)	loss 0.7749 (0.8937)	grad_norm 0.3558 (0.3732)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:15:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:02:37 lr 0.000072	 wd 0.0000	time 0.1753 (0.1960)	loss 0.8794 (0.8939)	grad_norm 0.3585 (0.3731)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:15:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:02:19 lr 0.000072	 wd 0.0000	time 0.1805 (0.1993)	loss 0.9648 (0.8939)	grad_norm 0.3845 (0.3733)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:16:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:01:59 lr 0.000072	 wd 0.0000	time 0.1767 (0.1993)	loss 0.9595 (0.8938)	grad_norm 0.3790 (0.3733)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:16:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:01:39 lr 0.000072	 wd 0.0000	time 0.1898 (0.1987)	loss 0.8901 (0.8942)	grad_norm 0.3751 (0.3735)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:16:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:19 lr 0.000071	 wd 0.0000	time 0.1727 (0.1983)	loss 0.8701 (0.8942)	grad_norm 0.3892 (0.3734)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:17:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:00:59 lr 0.000071	 wd 0.0000	time 0.1996 (0.1977)	loss 0.8726 (0.8947)	grad_norm 0.3663 (0.3733)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:17:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:40 lr 0.000071	 wd 0.0000	time 0.1724 (0.1985)	loss 0.9517 (0.8942)	grad_norm 0.3641 (0.3733)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:17:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:20 lr 0.000071	 wd 0.0000	time 0.1605 (0.1982)	loss 0.8926 (0.8947)	grad_norm 0.3648 (0.3734)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:18:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000071	 wd 0.0000	time 0.1591 (0.1974)	loss 0.9966 (0.8941)	grad_norm 0.3703 (0.3734)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:18:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 10 training takes 0:08:17
[2024-07-01 23:18:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.513 (16.513)	Loss 0.3684 (0.3684)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 23:18:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.084 Acc@5 97.598
[2024-07-01 23:18:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-01 23:18:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.08%
[2024-07-01 23:18:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:18:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:18:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][0/2502]	eta 10:35:42 lr 0.000071	 wd 0.0000	time 15.2447 (15.2447)	loss 0.8130 (0.8130)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:19:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:15:34 lr 0.000070	 wd 0.0000	time 0.1806 (0.3889)	loss 0.9185 (0.8870)	grad_norm 0.3716 (0.3736)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:19:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:10:55 lr 0.000070	 wd 0.0000	time 0.1747 (0.2848)	loss 0.9067 (0.8903)	grad_norm 0.3879 (0.3739)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:19:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:09:11 lr 0.000070	 wd 0.0000	time 0.1600 (0.2504)	loss 1.0215 (0.8910)	grad_norm 0.3783 (0.3743)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:20:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:08:09 lr 0.000070	 wd 0.0000	time 0.1758 (0.2328)	loss 0.9487 (0.8894)	grad_norm 0.3843 (0.3741)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:20:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:07:23 lr 0.000070	 wd 0.0000	time 0.1638 (0.2216)	loss 0.8052 (0.8883)	grad_norm 0.3674 (nan)	loss_scale 32768.0000 (32898.8104)	mem 7964MB
[2024-07-01 23:20:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:07:00 lr 0.000069	 wd 0.0000	time 0.1630 (0.2213)	loss 1.0947 (0.8866)	grad_norm 0.3723 (nan)	loss_scale 32768.0000 (32877.0449)	mem 7964MB
[2024-07-01 23:21:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:06:30 lr 0.000069	 wd 0.0000	time 0.1682 (0.2168)	loss 0.8848 (0.8892)	grad_norm 0.3758 (nan)	loss_scale 32768.0000 (32861.4893)	mem 7964MB
[2024-07-01 23:21:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:06:01 lr 0.000069	 wd 0.0000	time 0.1806 (0.2124)	loss 0.9175 (0.8902)	grad_norm 0.3749 (nan)	loss_scale 32768.0000 (32849.8177)	mem 7964MB
[2024-07-01 23:21:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:05:34 lr 0.000069	 wd 0.0000	time 0.1556 (0.2088)	loss 1.0176 (0.8906)	grad_norm 0.3942 (nan)	loss_scale 32768.0000 (32840.7370)	mem 7964MB
[2024-07-01 23:22:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:05:09 lr 0.000069	 wd 0.0000	time 0.1645 (0.2060)	loss 0.8511 (0.8919)	grad_norm 0.3755 (nan)	loss_scale 32768.0000 (32833.4705)	mem 7964MB
[2024-07-01 23:22:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:04:56 lr 0.000069	 wd 0.0000	time 0.1832 (0.2114)	loss 0.8110 (0.8922)	grad_norm 0.3687 (nan)	loss_scale 32768.0000 (32827.5241)	mem 7964MB
[2024-07-01 23:22:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:04:32 lr 0.000068	 wd 0.0000	time 0.1665 (0.2091)	loss 0.9536 (0.8926)	grad_norm 0.3757 (nan)	loss_scale 32768.0000 (32822.5679)	mem 7964MB
[2024-07-01 23:23:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:04:08 lr 0.000068	 wd 0.0000	time 0.1630 (0.2070)	loss 0.7563 (0.8928)	grad_norm 0.3621 (nan)	loss_scale 32768.0000 (32818.3736)	mem 7964MB
[2024-07-01 23:23:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:03:46 lr 0.000068	 wd 0.0000	time 0.1722 (0.2052)	loss 0.8657 (0.8933)	grad_norm 0.3967 (nan)	loss_scale 32768.0000 (32814.7780)	mem 7964MB
[2024-07-01 23:23:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:03:24 lr 0.000068	 wd 0.0000	time 0.1860 (0.2037)	loss 0.9731 (0.8929)	grad_norm 0.3865 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-01 23:24:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:03:04 lr 0.000068	 wd 0.0000	time 0.1630 (0.2042)	loss 0.9321 (0.8930)	grad_norm 0.4020 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-01 23:24:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:02:42 lr 0.000067	 wd 0.0000	time 0.1707 (0.2031)	loss 0.7964 (0.8932)	grad_norm 0.3753 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-01 23:24:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:02:22 lr 0.000067	 wd 0.0000	time 0.1973 (0.2023)	loss 1.1309 (0.8940)	grad_norm 0.3693 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-01 23:25:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:01 lr 0.000067	 wd 0.0000	time 0.1740 (0.2014)	loss 0.8027 (0.8945)	grad_norm 0.3644 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-01 23:25:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:01:40 lr 0.000067	 wd 0.0000	time 0.1785 (0.2007)	loss 0.9111 (0.8941)	grad_norm 0.3662 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 23:25:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:21 lr 0.000067	 wd 0.0000	time 0.1803 (0.2022)	loss 0.8613 (0.8938)	grad_norm 0.3918 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 23:26:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:00 lr 0.000066	 wd 0.0000	time 0.1719 (0.2019)	loss 0.9287 (0.8937)	grad_norm 0.3837 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 23:26:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:40 lr 0.000066	 wd 0.0000	time 0.2029 (0.2015)	loss 0.8877 (0.8937)	grad_norm 0.3699 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 23:26:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:20 lr 0.000066	 wd 0.0000	time 0.1869 (0.2008)	loss 0.8862 (0.8940)	grad_norm 0.3735 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 23:27:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000066	 wd 0.0000	time 0.1573 (0.1998)	loss 1.0850 (0.8932)	grad_norm 0.3833 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 23:27:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 11 training takes 0:08:32
[2024-07-01 23:27:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.056 (17.056)	Loss 0.3652 (0.3652)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 23:27:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.078 Acc@5 97.608
[2024-07-01 23:27:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-01 23:27:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.08%
[2024-07-01 23:27:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][0/2502]	eta 10:52:45 lr 0.000066	 wd 0.0000	time 15.6536 (15.6536)	loss 0.9385 (0.9385)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:28:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:13:57 lr 0.000066	 wd 0.0000	time 0.1710 (0.3486)	loss 0.8389 (0.8802)	grad_norm 0.3851 (0.3771)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:28:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:10:08 lr 0.000065	 wd 0.0000	time 0.1644 (0.2642)	loss 0.8633 (0.8858)	grad_norm 0.3751 (0.3753)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:28:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:09:13 lr 0.000065	 wd 0.0000	time 0.1614 (0.2514)	loss 0.9277 (0.8956)	grad_norm 0.4044 (0.3752)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:29:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:08:13 lr 0.000065	 wd 0.0000	time 0.1710 (0.2349)	loss 0.8604 (0.8970)	grad_norm 0.3704 (0.3754)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:29:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:07:28 lr 0.000065	 wd 0.0000	time 0.1598 (0.2241)	loss 0.7852 (0.8959)	grad_norm 0.3720 (0.3752)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:29:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:06:53 lr 0.000065	 wd 0.0000	time 0.1600 (0.2172)	loss 0.7891 (0.8958)	grad_norm 0.3795 (0.3756)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:30:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:06:21 lr 0.000064	 wd 0.0000	time 0.1850 (0.2119)	loss 0.8398 (0.8955)	grad_norm 0.3647 (0.3751)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:30:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:06:05 lr 0.000064	 wd 0.0000	time 0.1559 (0.2145)	loss 0.9233 (0.8945)	grad_norm 0.3633 (0.3750)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:30:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:05:38 lr 0.000064	 wd 0.0000	time 0.1824 (0.2115)	loss 0.8271 (0.8937)	grad_norm 0.3862 (0.3754)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:31:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:05:13 lr 0.000064	 wd 0.0000	time 0.1826 (0.2085)	loss 0.9326 (0.8939)	grad_norm 0.3673 (0.3755)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:31:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:04:48 lr 0.000064	 wd 0.0000	time 0.1716 (0.2060)	loss 0.7510 (0.8937)	grad_norm 0.3583 (0.3753)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:31:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:04:25 lr 0.000063	 wd 0.0000	time 0.1889 (0.2038)	loss 0.8896 (0.8930)	grad_norm 0.3899 (0.3753)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:32:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:04:06 lr 0.000063	 wd 0.0000	time 0.2475 (0.2053)	loss 0.7856 (0.8930)	grad_norm 0.3910 (0.3749)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:32:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:03:44 lr 0.000063	 wd 0.0000	time 0.1686 (0.2040)	loss 0.9482 (0.8931)	grad_norm 0.3717 (0.3750)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:32:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:03:22 lr 0.000063	 wd 0.0000	time 0.1968 (0.2026)	loss 0.8647 (0.8940)	grad_norm 0.3778 (0.3751)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:33:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:03:01 lr 0.000063	 wd 0.0000	time 0.1656 (0.2015)	loss 0.8828 (0.8935)	grad_norm 0.3705 (0.3751)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:33:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:02:40 lr 0.000062	 wd 0.0000	time 0.1674 (0.2002)	loss 0.8652 (0.8934)	grad_norm 0.3691 (0.3753)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:33:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:02:21 lr 0.000062	 wd 0.0000	time 0.1734 (0.2011)	loss 0.8052 (0.8937)	grad_norm 0.3791 (0.3756)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:34:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:00 lr 0.000062	 wd 0.0000	time 0.1780 (0.2008)	loss 0.8521 (0.8934)	grad_norm 0.3729 (0.3756)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:34:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:01:40 lr 0.000062	 wd 0.0000	time 0.1745 (0.2003)	loss 0.8340 (0.8936)	grad_norm 0.3773 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 23:34:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:20 lr 0.000062	 wd 0.0000	time 0.1686 (0.1999)	loss 0.8564 (0.8939)	grad_norm 0.3788 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 23:35:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:00 lr 0.000061	 wd 0.0000	time 0.1626 (0.1993)	loss 0.9170 (0.8941)	grad_norm 0.3869 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 23:35:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:40 lr 0.000061	 wd 0.0000	time 1.2714 (0.2013)	loss 0.8828 (0.8937)	grad_norm 0.3510 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 23:35:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:20 lr 0.000061	 wd 0.0000	time 0.1846 (0.2010)	loss 0.8794 (0.8929)	grad_norm 0.3772 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 23:36:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000061	 wd 0.0000	time 0.1614 (0.2001)	loss 0.9189 (0.8927)	grad_norm 0.3801 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 23:36:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 12 training takes 0:08:24
[2024-07-01 23:36:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.750 (16.750)	Loss 0.3596 (0.3596)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 23:36:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.186 Acc@5 97.650
[2024-07-01 23:36:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-01 23:36:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.19%
[2024-07-01 23:36:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:36:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:36:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][0/2502]	eta 14:44:18 lr 0.000061	 wd 0.0000	time 21.2066 (21.2066)	loss 0.8428 (0.8428)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:37:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:15:47 lr 0.000061	 wd 0.0000	time 0.1822 (0.3945)	loss 0.7910 (0.8810)	grad_norm 0.3739 (0.3737)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:37:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:11:00 lr 0.000060	 wd 0.0000	time 0.1720 (0.2871)	loss 0.8047 (0.8896)	grad_norm 0.3846 (0.3736)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:37:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:09:15 lr 0.000060	 wd 0.0000	time 0.1615 (0.2523)	loss 0.7900 (0.8834)	grad_norm 0.3874 (0.3742)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:38:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:08:11 lr 0.000060	 wd 0.0000	time 0.1734 (0.2339)	loss 0.7749 (0.8846)	grad_norm 0.3799 (0.3748)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:38:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:07:28 lr 0.000060	 wd 0.0000	time 0.2871 (0.2240)	loss 0.8428 (0.8850)	grad_norm 0.3740 (0.3755)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:38:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:07:03 lr 0.000060	 wd 0.0000	time 0.1730 (0.2229)	loss 0.8013 (0.8861)	grad_norm 0.3713 (0.3763)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:39:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:06:31 lr 0.000059	 wd 0.0000	time 0.1564 (0.2171)	loss 1.0205 (0.8866)	grad_norm 0.4018 (0.3763)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:39:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:06:02 lr 0.000059	 wd 0.0000	time 0.1659 (0.2128)	loss 0.9268 (0.8867)	grad_norm 0.3815 (0.3763)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:39:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:05:35 lr 0.000059	 wd 0.0000	time 0.1785 (0.2091)	loss 0.9531 (0.8876)	grad_norm 0.3704 (0.3766)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:40:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:05:10 lr 0.000059	 wd 0.0000	time 0.1573 (0.2069)	loss 0.9844 (0.8886)	grad_norm 0.3849 (0.3768)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:40:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:05:02 lr 0.000059	 wd 0.0000	time 0.1662 (0.2158)	loss 0.8623 (0.8881)	grad_norm 0.3714 (0.3767)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:40:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:04:37 lr 0.000058	 wd 0.0000	time 0.1542 (0.2128)	loss 0.9868 (0.8884)	grad_norm 0.3875 (0.3766)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:41:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:04:12 lr 0.000058	 wd 0.0000	time 0.1652 (0.2104)	loss 0.8994 (0.8890)	grad_norm 0.3887 (0.3767)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:41:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:03:49 lr 0.000058	 wd 0.0000	time 0.1800 (0.2082)	loss 1.0537 (0.8887)	grad_norm 0.3617 (0.3768)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:41:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:03:29 lr 0.000058	 wd 0.0000	time 0.1530 (0.2093)	loss 0.9131 (0.8888)	grad_norm 0.3875 (0.3770)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:42:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:03:08 lr 0.000058	 wd 0.0000	time 0.1630 (0.2093)	loss 0.8335 (0.8893)	grad_norm 0.3669 (0.3770)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:42:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:02:46 lr 0.000057	 wd 0.0000	time 0.1646 (0.2080)	loss 0.8486 (0.8894)	grad_norm 0.3666 (0.3770)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:42:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:02:25 lr 0.000057	 wd 0.0000	time 0.1623 (0.2069)	loss 0.8608 (0.8894)	grad_norm 0.3964 (0.3771)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:43:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:03 lr 0.000057	 wd 0.0000	time 0.1659 (0.2057)	loss 0.8140 (0.8881)	grad_norm 0.3921 (0.3772)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:43:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:01:44 lr 0.000057	 wd 0.0000	time 0.1992 (0.2077)	loss 0.8711 (0.8889)	grad_norm 0.3678 (0.3773)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:43:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:23 lr 0.000057	 wd 0.0000	time 0.1706 (0.2071)	loss 0.8877 (0.8895)	grad_norm 0.3457 (0.3773)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:44:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:02 lr 0.000056	 wd 0.0000	time 0.1985 (0.2064)	loss 0.9546 (0.8895)	grad_norm 0.3894 (0.3773)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:44:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:41 lr 0.000056	 wd 0.0000	time 0.1627 (0.2056)	loss 0.8193 (0.8891)	grad_norm 0.3765 (0.3773)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:44:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:20 lr 0.000056	 wd 0.0000	time 0.1782 (0.2048)	loss 0.8999 (0.8895)	grad_norm 0.3564 (0.3774)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:45:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000056	 wd 0.0000	time 0.1608 (0.2037)	loss 0.8311 (0.8895)	grad_norm 0.3717 (0.3774)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:45:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 13 training takes 0:08:44
[2024-07-01 23:45:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.100 (17.100)	Loss 0.3611 (0.3611)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 23:45:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.182 Acc@5 97.652
[2024-07-01 23:45:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-01 23:45:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.19%
[2024-07-01 23:46:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][0/2502]	eta 10:25:14 lr 0.000056	 wd 0.0000	time 14.9940 (14.9940)	loss 0.9878 (0.9878)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:46:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:13:44 lr 0.000055	 wd 0.0000	time 0.1623 (0.3432)	loss 0.8608 (0.8866)	grad_norm 0.3814 (0.3787)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:46:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:10:42 lr 0.000055	 wd 0.0000	time 0.1753 (0.2790)	loss 0.8511 (0.8875)	grad_norm 0.3690 (0.3783)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:47:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:09:08 lr 0.000055	 wd 0.0000	time 0.1545 (0.2491)	loss 1.1328 (0.8901)	grad_norm 0.3694 (0.3776)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:47:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:08:09 lr 0.000055	 wd 0.0000	time 0.1820 (0.2330)	loss 1.0947 (0.8889)	grad_norm 0.3745 (0.3784)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:47:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:07:26 lr 0.000055	 wd 0.0000	time 0.1795 (0.2231)	loss 0.9355 (0.8886)	grad_norm 0.3537 (0.3779)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:47:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:06:52 lr 0.000054	 wd 0.0000	time 0.1632 (0.2167)	loss 0.8599 (0.8890)	grad_norm 0.4105 (0.3782)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:48:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:06:32 lr 0.000054	 wd 0.0000	time 0.1866 (0.2176)	loss 0.8154 (0.8881)	grad_norm 0.3729 (0.3787)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:48:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:06:05 lr 0.000054	 wd 0.0000	time 0.1932 (0.2147)	loss 0.8042 (0.8882)	grad_norm 0.3677 (0.3787)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:48:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:05:38 lr 0.000054	 wd 0.0000	time 0.1733 (0.2111)	loss 0.8789 (0.8878)	grad_norm 0.3964 (0.3788)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:49:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:05:13 lr 0.000054	 wd 0.0000	time 0.1633 (0.2084)	loss 0.9316 (0.8870)	grad_norm 0.4093 (nan)	loss_scale 32768.0000 (32833.4705)	mem 7964MB
[2024-07-01 23:49:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:04:48 lr 0.000053	 wd 0.0000	time 0.1739 (0.2058)	loss 0.8350 (0.8868)	grad_norm 0.3576 (nan)	loss_scale 32768.0000 (32827.5241)	mem 7964MB
[2024-07-01 23:49:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:04:30 lr 0.000053	 wd 0.0000	time 0.1838 (0.2080)	loss 0.8789 (0.8867)	grad_norm 0.3860 (nan)	loss_scale 32768.0000 (32822.5679)	mem 7964MB
[2024-07-01 23:50:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:04:08 lr 0.000053	 wd 0.0000	time 0.1743 (0.2069)	loss 1.0947 (0.8864)	grad_norm 0.3919 (nan)	loss_scale 32768.0000 (32818.3736)	mem 7964MB
[2024-07-01 23:50:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:03:46 lr 0.000053	 wd 0.0000	time 0.1671 (0.2051)	loss 0.9004 (0.8867)	grad_norm 0.3570 (nan)	loss_scale 32768.0000 (32814.7780)	mem 7964MB
[2024-07-01 23:50:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:03:24 lr 0.000053	 wd 0.0000	time 0.1675 (0.2038)	loss 0.8696 (0.8870)	grad_norm 0.3849 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-01 23:51:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:03:02 lr 0.000052	 wd 0.0000	time 0.1781 (0.2025)	loss 0.8662 (0.8867)	grad_norm 0.3871 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-01 23:51:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:02:42 lr 0.000052	 wd 0.0000	time 0.2075 (0.2028)	loss 1.0605 (0.8872)	grad_norm 0.4053 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-01 23:51:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:02:21 lr 0.000052	 wd 0.0000	time 0.1752 (0.2023)	loss 0.6953 (0.8874)	grad_norm 0.3702 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-01 23:52:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:01 lr 0.000052	 wd 0.0000	time 0.1586 (0.2014)	loss 0.8408 (0.8872)	grad_norm 0.3785 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-01 23:52:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:01:40 lr 0.000052	 wd 0.0000	time 0.1699 (0.2008)	loss 0.8105 (0.8869)	grad_norm 0.3819 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-01 23:52:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:20 lr 0.000051	 wd 0.0000	time 0.1701 (0.2002)	loss 0.9253 (0.8870)	grad_norm 0.3734 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-01 23:53:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:01 lr 0.000051	 wd 0.0000	time 0.1791 (0.2030)	loss 0.8809 (0.8877)	grad_norm 0.3664 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-01 23:53:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:40 lr 0.000051	 wd 0.0000	time 0.1870 (0.2023)	loss 0.8599 (0.8874)	grad_norm 0.3863 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-01 23:53:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:20 lr 0.000051	 wd 0.0000	time 0.1708 (0.2017)	loss 0.9702 (0.8880)	grad_norm 0.3874 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-01 23:54:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000051	 wd 0.0000	time 0.3249 (0.2009)	loss 0.8789 (0.8882)	grad_norm 0.3836 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-01 23:54:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 14 training takes 0:08:26
[2024-07-01 23:54:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.510 (17.510)	Loss 0.3584 (0.3584)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-01 23:54:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.240 Acc@5 97.624
[2024-07-01 23:54:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-01 23:54:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-01 23:54:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:54:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:55:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][0/2502]	eta 9:43:05 lr 0.000051	 wd 0.0000	time 13.9828 (13.9828)	loss 0.8003 (0.8003)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:55:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:13:22 lr 0.000050	 wd 0.0000	time 0.1778 (0.3340)	loss 0.9834 (0.8933)	grad_norm 0.3834 (0.3787)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:55:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:09:58 lr 0.000050	 wd 0.0000	time 0.1642 (0.2600)	loss 1.0039 (0.8890)	grad_norm 0.3805 (0.3810)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:56:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:08:34 lr 0.000050	 wd 0.0000	time 0.1753 (0.2334)	loss 0.9414 (0.8869)	grad_norm 0.3720 (0.3812)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:56:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:07:43 lr 0.000050	 wd 0.0000	time 0.1612 (0.2203)	loss 1.0293 (0.8884)	grad_norm 0.3897 (0.3814)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:56:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:07:09 lr 0.000049	 wd 0.0000	time 0.1840 (0.2148)	loss 0.7866 (0.8903)	grad_norm 0.3877 (0.3817)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:56:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:06:38 lr 0.000049	 wd 0.0000	time 0.1792 (0.2095)	loss 0.8447 (0.8899)	grad_norm 0.3947 (0.3813)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:57:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:06:14 lr 0.000049	 wd 0.0000	time 0.1816 (0.2078)	loss 0.8311 (0.8872)	grad_norm 0.3828 (0.3816)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:57:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:05:47 lr 0.000049	 wd 0.0000	time 0.1725 (0.2042)	loss 1.0312 (0.8877)	grad_norm 0.3884 (0.3816)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:57:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:05:22 lr 0.000049	 wd 0.0000	time 0.1659 (0.2015)	loss 0.8071 (0.8878)	grad_norm 0.3966 (0.3819)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:58:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:05:06 lr 0.000048	 wd 0.0000	time 0.1863 (0.2042)	loss 0.9268 (0.8874)	grad_norm 0.3717 (0.3820)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:58:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:04:43 lr 0.000048	 wd 0.0000	time 0.1595 (0.2021)	loss 0.9219 (0.8854)	grad_norm 0.3982 (0.3816)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:58:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:04:21 lr 0.000048	 wd 0.0000	time 0.1870 (0.2008)	loss 0.8188 (0.8864)	grad_norm 0.3710 (0.3814)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:59:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:03:59 lr 0.000048	 wd 0.0000	time 0.1542 (0.1993)	loss 0.9126 (0.8859)	grad_norm 0.3706 (0.3816)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:59:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:03:38 lr 0.000048	 wd 0.0000	time 0.1835 (0.1979)	loss 0.8110 (0.8863)	grad_norm 0.3855 (0.3818)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-01 23:59:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:03:22 lr 0.000047	 wd 0.0000	time 0.1696 (0.2024)	loss 0.9395 (0.8858)	grad_norm 0.3811 (0.3820)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:00:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:03:01 lr 0.000047	 wd 0.0000	time 0.1681 (0.2013)	loss 0.9380 (0.8859)	grad_norm 0.3793 (0.3822)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:00:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:02:40 lr 0.000047	 wd 0.0000	time 0.1713 (0.2004)	loss 0.8086 (0.8860)	grad_norm 0.3669 (0.3821)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:00:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:02:20 lr 0.000047	 wd 0.0000	time 0.1807 (0.1994)	loss 1.0254 (0.8866)	grad_norm 0.4122 (0.3821)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:01:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:01 lr 0.000047	 wd 0.0000	time 0.1693 (0.2023)	loss 0.9761 (0.8860)	grad_norm 0.3942 (0.3822)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:01:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:01:41 lr 0.000046	 wd 0.0000	time 0.1763 (0.2029)	loss 1.0381 (0.8868)	grad_norm 0.4153 (0.3822)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:01:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:21 lr 0.000046	 wd 0.0000	time 0.1684 (0.2023)	loss 0.8418 (0.8867)	grad_norm 0.3748 (0.3821)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:02:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:00 lr 0.000046	 wd 0.0000	time 0.1686 (0.2018)	loss 0.9243 (0.8870)	grad_norm 0.3721 (0.3819)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:02:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:40 lr 0.000046	 wd 0.0000	time 0.1714 (0.2010)	loss 0.9136 (0.8870)	grad_norm 0.4038 (0.3820)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:03:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:20 lr 0.000046	 wd 0.0000	time 0.1734 (0.2045)	loss 0.9326 (0.8870)	grad_norm 0.3821 (0.3822)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:03:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000045	 wd 0.0000	time 0.1582 (0.2034)	loss 1.1016 (0.8872)	grad_norm 0.3828 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 00:03:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 15 training takes 0:08:33
[2024-07-02 00:03:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_15.pth saving......
[2024-07-02 00:03:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_15.pth saved !!!
[2024-07-02 00:03:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.298 (16.298)	Loss 0.3640 (0.3640)	Acc@1 91.602 (91.602)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 00:03:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.186 Acc@5 97.646
[2024-07-02 00:03:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-02 00:03:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-02 00:04:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][0/2502]	eta 10:40:47 lr 0.000045	 wd 0.0000	time 15.3667 (15.3667)	loss 0.7529 (0.7529)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:04:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:14:31 lr 0.000045	 wd 0.0000	time 0.1699 (0.3630)	loss 1.0107 (0.8981)	grad_norm 0.3926 (0.3853)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:04:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:10:39 lr 0.000045	 wd 0.0000	time 0.1773 (0.2780)	loss 0.9014 (0.8871)	grad_norm 0.3911 (0.3834)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:05:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:09:01 lr 0.000045	 wd 0.0000	time 0.1711 (0.2460)	loss 0.9219 (0.8881)	grad_norm 0.3796 (0.3822)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:05:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:08:03 lr 0.000045	 wd 0.0000	time 0.1929 (0.2300)	loss 0.9053 (0.8875)	grad_norm 0.3869 (0.3824)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:05:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:07:19 lr 0.000044	 wd 0.0000	time 0.1645 (0.2196)	loss 0.9771 (0.8872)	grad_norm 0.3742 (0.3826)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:06:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:06:44 lr 0.000044	 wd 0.0000	time 0.1734 (0.2125)	loss 0.9302 (0.8876)	grad_norm 0.3780 (0.3828)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:06:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:06:40 lr 0.000044	 wd 0.0000	time 0.1782 (0.2222)	loss 0.9087 (0.8880)	grad_norm 0.3785 (0.3826)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:06:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:06:10 lr 0.000044	 wd 0.0000	time 0.1859 (0.2175)	loss 0.8970 (0.8872)	grad_norm 0.3833 (0.3827)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:07:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:05:42 lr 0.000043	 wd 0.0000	time 0.1758 (0.2140)	loss 0.7969 (0.8887)	grad_norm 0.3846 (0.3829)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:07:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:05:16 lr 0.000043	 wd 0.0000	time 0.1739 (0.2104)	loss 0.8784 (0.8889)	grad_norm 0.3893 (0.3832)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:07:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:04:56 lr 0.000043	 wd 0.0000	time 0.1547 (0.2113)	loss 0.9785 (0.8892)	grad_norm 0.3943 (0.3835)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:08:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:04:35 lr 0.000043	 wd 0.0000	time 0.1674 (0.2113)	loss 0.9214 (0.8887)	grad_norm 0.3869 (0.3833)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:08:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:04:12 lr 0.000043	 wd 0.0000	time 0.1720 (0.2098)	loss 0.9907 (0.8881)	grad_norm 0.3807 (0.3833)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:08:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:03:49 lr 0.000042	 wd 0.0000	time 0.1717 (0.2085)	loss 0.8140 (0.8873)	grad_norm 0.3673 (0.3834)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:09:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:03:27 lr 0.000042	 wd 0.0000	time 0.1835 (0.2070)	loss 0.8281 (0.8869)	grad_norm 0.3644 (0.3836)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:09:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:03:17 lr 0.000042	 wd 0.0000	time 0.1789 (0.2186)	loss 0.7891 (0.8866)	grad_norm 0.3848 (0.3835)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:10:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:02:53 lr 0.000042	 wd 0.0000	time 0.1813 (0.2168)	loss 0.8799 (0.8864)	grad_norm 0.3871 (0.3837)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:10:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:02:31 lr 0.000042	 wd 0.0000	time 0.1794 (0.2152)	loss 1.0371 (0.8866)	grad_norm 0.3613 (0.3837)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:10:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:08 lr 0.000041	 wd 0.0000	time 0.2031 (0.2136)	loss 1.0088 (0.8856)	grad_norm 0.3776 (0.3837)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:11:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:01:48 lr 0.000041	 wd 0.0000	time 0.3211 (0.2157)	loss 0.8428 (0.8853)	grad_norm 0.3852 (0.3836)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:11:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:27 lr 0.000041	 wd 0.0000	time 0.1557 (0.2164)	loss 0.6855 (0.8850)	grad_norm 0.3784 (0.3837)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:11:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:04 lr 0.000041	 wd 0.0000	time 0.1791 (0.2152)	loss 0.7852 (0.8850)	grad_norm 0.3847 (0.3838)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:12:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:43 lr 0.000041	 wd 0.0000	time 0.1908 (0.2139)	loss 0.6890 (0.8844)	grad_norm 0.4036 (0.3837)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:12:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:21 lr 0.000040	 wd 0.0000	time 0.1722 (0.2128)	loss 0.7754 (0.8843)	grad_norm 0.3882 (0.3836)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:12:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.1561 (0.2118)	loss 0.8394 (0.8839)	grad_norm 0.4187 (0.3837)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:12:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 16 training takes 0:08:59
[2024-07-02 00:13:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.231 (17.231)	Loss 0.3604 (0.3604)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 00:13:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.216 Acc@5 97.652
[2024-07-02 00:13:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-02 00:13:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-02 00:13:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][0/2502]	eta 11:08:25 lr 0.000040	 wd 0.0000	time 16.0292 (16.0292)	loss 0.7749 (0.7749)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:13:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:13:55 lr 0.000040	 wd 0.0000	time 0.1538 (0.3477)	loss 0.9932 (0.8848)	grad_norm 0.3673 (0.3831)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:14:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:10:47 lr 0.000040	 wd 0.0000	time 0.1910 (0.2814)	loss 0.8818 (0.8883)	grad_norm 0.3576 (0.3838)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:14:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:09:12 lr 0.000040	 wd 0.0000	time 0.1660 (0.2508)	loss 0.8936 (0.8842)	grad_norm 0.4019 (0.3843)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:14:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:08:10 lr 0.000039	 wd 0.0000	time 0.1535 (0.2334)	loss 0.9473 (0.8828)	grad_norm 0.3677 (0.3845)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:15:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:07:26 lr 0.000039	 wd 0.0000	time 0.1703 (0.2230)	loss 0.7612 (0.8809)	grad_norm 0.4167 (0.3847)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:15:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:06:49 lr 0.000039	 wd 0.0000	time 0.1747 (0.2154)	loss 0.8057 (0.8826)	grad_norm 0.4019 (0.3849)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:15:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:06:35 lr 0.000039	 wd 0.0000	time 0.1846 (0.2197)	loss 0.9004 (0.8817)	grad_norm 0.3861 (0.3850)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:16:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:06:09 lr 0.000039	 wd 0.0000	time 0.1554 (0.2169)	loss 0.7866 (0.8819)	grad_norm 0.3768 (0.3851)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:16:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:05:42 lr 0.000038	 wd 0.0000	time 0.1626 (0.2136)	loss 0.9932 (0.8822)	grad_norm 0.3799 (0.3848)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:16:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:05:16 lr 0.000038	 wd 0.0000	time 0.1744 (0.2108)	loss 0.9067 (0.8821)	grad_norm 0.3841 (0.3850)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:17:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:04:51 lr 0.000038	 wd 0.0000	time 0.1539 (0.2081)	loss 0.7505 (0.8834)	grad_norm 0.3866 (0.3851)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:17:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:04:35 lr 0.000038	 wd 0.0000	time 0.1816 (0.2113)	loss 0.7793 (0.8849)	grad_norm 0.4019 (0.3854)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:17:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:04:13 lr 0.000038	 wd 0.0000	time 0.1990 (0.2107)	loss 0.9058 (0.8840)	grad_norm 0.3614 (0.3853)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:18:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:03:50 lr 0.000037	 wd 0.0000	time 0.1720 (0.2088)	loss 0.9116 (0.8849)	grad_norm 0.3599 (0.3854)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:18:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:03:27 lr 0.000037	 wd 0.0000	time 0.1714 (0.2072)	loss 0.8716 (0.8847)	grad_norm 0.3869 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-02 00:18:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:03:05 lr 0.000037	 wd 0.0000	time 0.1758 (0.2056)	loss 1.0420 (0.8844)	grad_norm 0.4007 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-02 00:19:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:02:47 lr 0.000037	 wd 0.0000	time 0.1651 (0.2083)	loss 1.0225 (0.8846)	grad_norm 0.3892 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-02 00:19:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:02:25 lr 0.000037	 wd 0.0000	time 0.1713 (0.2071)	loss 0.7817 (0.8861)	grad_norm 0.3932 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-02 00:19:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:03 lr 0.000036	 wd 0.0000	time 0.1802 (0.2060)	loss 0.7539 (0.8865)	grad_norm 0.3743 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-02 00:20:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:01:42 lr 0.000036	 wd 0.0000	time 0.1974 (0.2050)	loss 0.8115 (0.8860)	grad_norm 0.3807 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-02 00:20:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:22 lr 0.000036	 wd 0.0000	time 0.1605 (0.2041)	loss 0.8179 (0.8855)	grad_norm 0.3745 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-02 00:20:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:01 lr 0.000036	 wd 0.0000	time 0.1534 (0.2051)	loss 0.9512 (0.8854)	grad_norm 0.3835 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-02 00:21:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:41 lr 0.000036	 wd 0.0000	time 0.1657 (0.2046)	loss 0.9146 (0.8849)	grad_norm 0.3774 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-02 00:21:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:20 lr 0.000035	 wd 0.0000	time 0.2227 (0.2039)	loss 0.8354 (0.8852)	grad_norm 0.3848 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-02 00:21:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.1618 (0.2032)	loss 0.9263 (0.8855)	grad_norm 0.3809 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 00:21:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 17 training takes 0:08:31
[2024-07-02 00:22:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.408 (17.408)	Loss 0.3572 (0.3572)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 00:22:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.236 Acc@5 97.654
[2024-07-02 00:22:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-02 00:22:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-02 00:22:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][0/2502]	eta 11:00:46 lr 0.000035	 wd 0.0000	time 15.8460 (15.8460)	loss 0.9175 (0.9175)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:23:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:13:38 lr 0.000035	 wd 0.0000	time 0.1581 (0.3407)	loss 0.9941 (0.8878)	grad_norm 0.3927 (0.3873)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:23:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:10:03 lr 0.000035	 wd 0.0000	time 0.1646 (0.2621)	loss 0.8315 (0.8830)	grad_norm 0.4023 (0.3868)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:23:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:08:35 lr 0.000035	 wd 0.0000	time 0.1790 (0.2341)	loss 0.9370 (0.8872)	grad_norm 0.3895 (0.3878)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:23:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:07:55 lr 0.000034	 wd 0.0000	time 0.1561 (0.2261)	loss 0.7979 (0.8889)	grad_norm 0.3979 (0.3877)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:24:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:07:20 lr 0.000034	 wd 0.0000	time 0.1773 (0.2198)	loss 0.7104 (0.8886)	grad_norm 0.3818 (0.3867)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:24:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:06:45 lr 0.000034	 wd 0.0000	time 0.1726 (0.2131)	loss 0.9556 (0.8850)	grad_norm 0.3938 (0.3869)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:24:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:06:17 lr 0.000034	 wd 0.0000	time 0.1664 (0.2093)	loss 0.9604 (0.8820)	grad_norm 0.3687 (0.3865)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:25:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:05:50 lr 0.000034	 wd 0.0000	time 0.1642 (0.2058)	loss 1.0830 (0.8802)	grad_norm 0.3730 (0.3864)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:25:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:05:25 lr 0.000033	 wd 0.0000	time 0.2257 (0.2034)	loss 0.9609 (0.8808)	grad_norm 0.3947 (0.3865)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:25:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:05:10 lr 0.000033	 wd 0.0000	time 0.1871 (0.2067)	loss 0.8442 (0.8817)	grad_norm 0.3896 (0.3867)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:26:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:04:47 lr 0.000033	 wd 0.0000	time 0.1610 (0.2049)	loss 0.7949 (0.8829)	grad_norm 0.3782 (0.3867)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:26:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:04:24 lr 0.000033	 wd 0.0000	time 0.1698 (0.2031)	loss 0.8882 (0.8825)	grad_norm 0.3747 (0.3868)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:26:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:04:02 lr 0.000033	 wd 0.0000	time 0.1714 (0.2016)	loss 0.8960 (0.8839)	grad_norm 0.3781 (0.3870)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:27:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:03:41 lr 0.000032	 wd 0.0000	time 0.3582 (0.2009)	loss 1.0029 (0.8833)	grad_norm 0.3939 (0.3871)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:27:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:03:22 lr 0.000032	 wd 0.0000	time 0.1975 (0.2020)	loss 0.8604 (0.8842)	grad_norm 0.3904 (0.3870)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:27:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:03:01 lr 0.000032	 wd 0.0000	time 0.1560 (0.2010)	loss 0.8613 (0.8834)	grad_norm 0.3866 (0.3871)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:28:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:02:40 lr 0.000032	 wd 0.0000	time 0.1625 (0.2003)	loss 0.9712 (0.8835)	grad_norm 0.3979 (0.3872)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:28:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:02:19 lr 0.000032	 wd 0.0000	time 0.1684 (0.1993)	loss 0.8271 (0.8835)	grad_norm 0.4062 (0.3873)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:28:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:00 lr 0.000032	 wd 0.0000	time 0.1620 (0.2006)	loss 0.8315 (0.8831)	grad_norm 0.3912 (0.3874)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:29:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:01:42 lr 0.000031	 wd 0.0000	time 0.1849 (0.2038)	loss 0.8882 (0.8838)	grad_norm 0.3645 (0.3875)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:29:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:21 lr 0.000031	 wd 0.0000	time 0.1864 (0.2031)	loss 0.7471 (0.8839)	grad_norm 0.3950 (0.3876)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:29:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:01 lr 0.000031	 wd 0.0000	time 0.1722 (0.2024)	loss 0.8965 (0.8837)	grad_norm 0.3833 (0.3877)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:30:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:40 lr 0.000031	 wd 0.0000	time 0.1718 (0.2017)	loss 0.7646 (0.8841)	grad_norm 0.3689 (0.3878)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:30:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:20 lr 0.000031	 wd 0.0000	time 0.1674 (0.2037)	loss 0.8740 (0.8842)	grad_norm 0.3942 (0.3880)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:30:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000030	 wd 0.0000	time 0.1576 (0.2033)	loss 0.8735 (0.8842)	grad_norm 0.3913 (0.3880)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:31:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 18 training takes 0:08:33
[2024-07-02 00:31:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.257 (17.257)	Loss 0.3586 (0.3586)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 00:31:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.242 Acc@5 97.656
[2024-07-02 00:31:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-02 00:31:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-02 00:31:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 00:31:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 00:31:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][0/2502]	eta 10:25:52 lr 0.000030	 wd 0.0000	time 15.0088 (15.0088)	loss 0.8638 (0.8638)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:32:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:14:55 lr 0.000030	 wd 0.0000	time 0.1937 (0.3729)	loss 1.0283 (0.8741)	grad_norm 0.3857 (0.3878)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:32:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:10:56 lr 0.000030	 wd 0.0000	time 0.1821 (0.2853)	loss 0.7085 (0.8721)	grad_norm 0.4068 (0.3883)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:32:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:09:11 lr 0.000030	 wd 0.0000	time 0.1695 (0.2502)	loss 0.9780 (0.8774)	grad_norm 0.3866 (0.3881)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:33:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:08:11 lr 0.000030	 wd 0.0000	time 0.1621 (0.2336)	loss 0.8252 (0.8803)	grad_norm 0.3706 (0.3878)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:33:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:07:25 lr 0.000029	 wd 0.0000	time 0.1699 (0.2224)	loss 0.9497 (0.8802)	grad_norm 0.3916 (nan)	loss_scale 32768.0000 (32898.8104)	mem 7964MB
[2024-07-02 00:33:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:07:02 lr 0.000029	 wd 0.0000	time 0.2108 (0.2221)	loss 0.7544 (0.8817)	grad_norm 0.3804 (nan)	loss_scale 32768.0000 (32877.0449)	mem 7964MB
[2024-07-02 00:34:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:06:32 lr 0.000029	 wd 0.0000	time 0.1745 (0.2180)	loss 0.9995 (0.8829)	grad_norm 0.3796 (nan)	loss_scale 32768.0000 (32861.4893)	mem 7964MB
[2024-07-02 00:34:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:06:03 lr 0.000029	 wd 0.0000	time 0.1625 (0.2136)	loss 0.8530 (0.8841)	grad_norm 0.4001 (nan)	loss_scale 32768.0000 (32849.8177)	mem 7964MB
[2024-07-02 00:34:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:05:36 lr 0.000029	 wd 0.0000	time 0.1778 (0.2103)	loss 0.8613 (0.8838)	grad_norm 0.3847 (nan)	loss_scale 32768.0000 (32840.7370)	mem 7964MB
[2024-07-02 00:35:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:05:11 lr 0.000028	 wd 0.0000	time 0.1589 (0.2074)	loss 0.9565 (0.8845)	grad_norm 0.3785 (nan)	loss_scale 32768.0000 (32833.4705)	mem 7964MB
[2024-07-02 00:35:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:04:48 lr 0.000028	 wd 0.0000	time 0.2071 (0.2059)	loss 0.8252 (0.8840)	grad_norm 0.3929 (nan)	loss_scale 32768.0000 (32827.5241)	mem 7964MB
[2024-07-02 00:35:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:04:30 lr 0.000028	 wd 0.0000	time 0.1719 (0.2078)	loss 0.8667 (0.8842)	grad_norm 0.3936 (nan)	loss_scale 32768.0000 (32822.5679)	mem 7964MB
[2024-07-02 00:36:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:04:07 lr 0.000028	 wd 0.0000	time 0.1646 (0.2059)	loss 0.8271 (0.8848)	grad_norm 0.3855 (nan)	loss_scale 32768.0000 (32818.3736)	mem 7964MB
[2024-07-02 00:36:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:03:45 lr 0.000028	 wd 0.0000	time 0.1643 (0.2044)	loss 0.8271 (0.8843)	grad_norm 0.3873 (nan)	loss_scale 32768.0000 (32814.7780)	mem 7964MB
[2024-07-02 00:36:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:03:23 lr 0.000028	 wd 0.0000	time 0.1643 (0.2030)	loss 0.8784 (0.8847)	grad_norm 0.3719 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-02 00:36:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:03:02 lr 0.000027	 wd 0.0000	time 0.2084 (0.2028)	loss 0.8843 (0.8851)	grad_norm 0.4093 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-02 00:37:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:02:44 lr 0.000027	 wd 0.0000	time 0.1598 (0.2051)	loss 0.9795 (0.8850)	grad_norm 0.3869 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-02 00:37:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:02:23 lr 0.000027	 wd 0.0000	time 0.1597 (0.2040)	loss 0.8135 (0.8853)	grad_norm 0.3783 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-02 00:37:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:02 lr 0.000027	 wd 0.0000	time 0.2146 (0.2034)	loss 0.8472 (0.8854)	grad_norm 0.3890 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-02 00:38:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:01:41 lr 0.000027	 wd 0.0000	time 0.1595 (0.2024)	loss 0.9365 (0.8851)	grad_norm 0.3908 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-02 00:38:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:22 lr 0.000026	 wd 0.0000	time 1.3879 (0.2047)	loss 0.9863 (0.8848)	grad_norm 0.3879 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-02 00:39:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:01 lr 0.000026	 wd 0.0000	time 0.1720 (0.2042)	loss 0.8779 (0.8841)	grad_norm 0.4300 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-02 00:39:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:41 lr 0.000026	 wd 0.0000	time 0.1748 (0.2035)	loss 0.9189 (0.8838)	grad_norm 0.3898 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-02 00:39:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:20 lr 0.000026	 wd 0.0000	time 0.1745 (0.2030)	loss 0.9780 (0.8837)	grad_norm 0.4051 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-02 00:39:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.1611 (0.2019)	loss 0.7080 (0.8837)	grad_norm 0.3906 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 00:40:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 19 training takes 0:08:28
[2024-07-02 00:40:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 19.903 (19.903)	Loss 0.3601 (0.3601)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 00:40:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.228 Acc@5 97.698
[2024-07-02 00:40:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-02 00:40:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-02 00:40:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][0/2502]	eta 10:51:28 lr 0.000026	 wd 0.0000	time 15.6228 (15.6228)	loss 0.8491 (0.8491)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:41:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:13:47 lr 0.000026	 wd 0.0000	time 0.1663 (0.3447)	loss 1.0166 (0.8876)	grad_norm 0.3878 (0.3934)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:41:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:10:04 lr 0.000025	 wd 0.0000	time 0.1568 (0.2624)	loss 0.9482 (0.8788)	grad_norm 0.3931 (0.3921)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:41:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:08:38 lr 0.000025	 wd 0.0000	time 0.1716 (0.2354)	loss 0.7607 (0.8747)	grad_norm 0.3973 (0.3920)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:42:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:07:58 lr 0.000025	 wd 0.0000	time 0.1756 (0.2277)	loss 0.8623 (0.8751)	grad_norm 0.3961 (0.3915)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:42:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:07:16 lr 0.000025	 wd 0.0000	time 0.1745 (0.2182)	loss 0.8643 (0.8745)	grad_norm 0.3729 (0.3922)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:42:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:06:43 lr 0.000025	 wd 0.0000	time 0.1627 (0.2121)	loss 0.7939 (0.8750)	grad_norm 0.3970 (0.3918)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:42:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:06:13 lr 0.000025	 wd 0.0000	time 0.1879 (0.2075)	loss 0.7900 (0.8755)	grad_norm 0.3766 (0.3919)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:43:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:05:47 lr 0.000024	 wd 0.0000	time 0.1693 (0.2040)	loss 1.0117 (0.8760)	grad_norm 0.3777 (0.3919)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:43:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:05:30 lr 0.000024	 wd 0.0000	time 0.1795 (0.2060)	loss 0.7485 (0.8761)	grad_norm 0.3739 (0.3919)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:43:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:05:06 lr 0.000024	 wd 0.0000	time 0.2023 (0.2038)	loss 0.8076 (0.8758)	grad_norm 0.3838 (0.3916)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:44:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:04:43 lr 0.000024	 wd 0.0000	time 0.1735 (0.2019)	loss 0.8096 (0.8760)	grad_norm 0.3817 (0.3916)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:44:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:04:20 lr 0.000024	 wd 0.0000	time 0.1718 (0.2004)	loss 0.7344 (0.8757)	grad_norm 0.3785 (0.3916)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:44:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:03:58 lr 0.000023	 wd 0.0000	time 0.1649 (0.1988)	loss 0.8848 (0.8757)	grad_norm 0.3911 (0.3914)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:45:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:03:45 lr 0.000023	 wd 0.0000	time 0.1640 (0.2050)	loss 0.9316 (0.8761)	grad_norm 0.4051 (0.3914)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:45:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:03:23 lr 0.000023	 wd 0.0000	time 0.1736 (0.2034)	loss 0.9570 (0.8757)	grad_norm 0.3958 (0.3913)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:45:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:03:02 lr 0.000023	 wd 0.0000	time 0.1901 (0.2024)	loss 0.9150 (0.8751)	grad_norm 0.4030 (0.3914)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:46:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:02:41 lr 0.000023	 wd 0.0000	time 0.1741 (0.2014)	loss 0.9678 (0.8752)	grad_norm 0.3845 (0.3913)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:46:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:02:20 lr 0.000023	 wd 0.0000	time 0.1603 (0.2003)	loss 0.8047 (0.8759)	grad_norm 0.3881 (0.3913)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:46:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:01 lr 0.000022	 wd 0.0000	time 0.1810 (0.2012)	loss 0.9951 (0.8765)	grad_norm 0.4022 (0.3914)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:47:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:01:40 lr 0.000022	 wd 0.0000	time 0.1980 (0.2007)	loss 0.9023 (0.8764)	grad_norm 0.3823 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-02 00:47:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:20 lr 0.000022	 wd 0.0000	time 0.1740 (0.2002)	loss 0.9287 (0.8769)	grad_norm 0.3969 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-02 00:47:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:00 lr 0.000022	 wd 0.0000	time 0.1546 (0.1995)	loss 0.8936 (0.8774)	grad_norm 0.3980 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-02 00:48:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:40 lr 0.000022	 wd 0.0000	time 0.1651 (0.1988)	loss 0.8188 (0.8783)	grad_norm 0.4147 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-02 00:48:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:20 lr 0.000022	 wd 0.0000	time 0.1541 (0.1998)	loss 0.8433 (0.8784)	grad_norm 0.3936 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-02 00:48:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.1558 (0.1991)	loss 0.7896 (0.8782)	grad_norm 0.3732 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 00:48:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 20 training takes 0:08:24
[2024-07-02 00:49:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.194 (17.194)	Loss 0.3582 (0.3582)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 00:49:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.232 Acc@5 97.678
[2024-07-02 00:49:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-02 00:49:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-02 00:49:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][0/2502]	eta 10:44:50 lr 0.000021	 wd 0.0000	time 15.4640 (15.4640)	loss 0.9150 (0.9150)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:50:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:15:40 lr 0.000021	 wd 0.0000	time 0.1860 (0.3916)	loss 0.8999 (0.8899)	grad_norm 0.3893 (0.3923)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:50:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:11:02 lr 0.000021	 wd 0.0000	time 0.1620 (0.2880)	loss 0.9287 (0.8824)	grad_norm 0.3825 (0.3917)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:50:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:09:16 lr 0.000021	 wd 0.0000	time 0.1790 (0.2527)	loss 0.9175 (0.8802)	grad_norm 0.3860 (0.3923)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:51:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:08:13 lr 0.000021	 wd 0.0000	time 0.1803 (0.2348)	loss 0.9888 (0.8806)	grad_norm 0.3877 (0.3918)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:51:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:07:27 lr 0.000021	 wd 0.0000	time 0.1550 (0.2236)	loss 0.8179 (0.8809)	grad_norm 0.3845 (0.3921)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:51:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:06:58 lr 0.000020	 wd 0.0000	time 0.1613 (0.2199)	loss 0.8794 (0.8800)	grad_norm 0.3996 (0.3926)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:51:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:06:30 lr 0.000020	 wd 0.0000	time 0.1582 (0.2167)	loss 1.0098 (0.8803)	grad_norm 0.4070 (0.3925)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:52:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:06:02 lr 0.000020	 wd 0.0000	time 0.1563 (0.2128)	loss 0.9062 (0.8804)	grad_norm 0.4145 (0.3926)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:52:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:05:36 lr 0.000020	 wd 0.0000	time 0.1811 (0.2101)	loss 0.8369 (0.8814)	grad_norm 0.3810 (0.3924)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:52:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:05:11 lr 0.000020	 wd 0.0000	time 0.1707 (0.2071)	loss 0.8374 (0.8809)	grad_norm 0.3723 (0.3923)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:53:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:04:52 lr 0.000020	 wd 0.0000	time 0.1851 (0.2083)	loss 0.9077 (0.8814)	grad_norm 0.3888 (0.3925)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:53:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:04:30 lr 0.000019	 wd 0.0000	time 0.1699 (0.2076)	loss 0.8569 (0.8828)	grad_norm 0.3896 (0.3925)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:53:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:04:07 lr 0.000019	 wd 0.0000	time 0.3759 (0.2060)	loss 0.9639 (0.8824)	grad_norm 0.3820 (0.3924)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:54:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:03:45 lr 0.000019	 wd 0.0000	time 0.1628 (0.2045)	loss 0.8901 (0.8824)	grad_norm 0.3870 (0.3923)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:54:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:03:23 lr 0.000019	 wd 0.0000	time 0.1724 (0.2030)	loss 0.9214 (0.8821)	grad_norm 0.3816 (0.3922)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:54:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:03:02 lr 0.000019	 wd 0.0000	time 0.1568 (0.2025)	loss 0.9087 (0.8815)	grad_norm 0.4034 (0.3923)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:55:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:02:41 lr 0.000019	 wd 0.0000	time 0.1539 (0.2019)	loss 0.8848 (0.8821)	grad_norm 0.3883 (0.3924)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:55:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:02:21 lr 0.000018	 wd 0.0000	time 0.1597 (0.2011)	loss 0.8965 (0.8821)	grad_norm 0.3906 (0.3925)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:55:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:00 lr 0.000018	 wd 0.0000	time 0.1949 (0.2002)	loss 0.9062 (0.8822)	grad_norm 0.3865 (0.3926)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:56:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:01:40 lr 0.000018	 wd 0.0000	time 0.1537 (0.1996)	loss 0.7500 (0.8820)	grad_norm 0.3997 (0.3928)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:56:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:20 lr 0.000018	 wd 0.0000	time 0.1750 (0.1996)	loss 0.7793 (0.8815)	grad_norm 0.3768 (0.3930)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:56:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:00 lr 0.000018	 wd 0.0000	time 0.1776 (0.1994)	loss 0.7881 (0.8809)	grad_norm 0.3947 (0.3929)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:57:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:40 lr 0.000018	 wd 0.0000	time 0.1659 (0.1987)	loss 0.9399 (0.8808)	grad_norm 0.3911 (0.3929)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:57:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:20 lr 0.000018	 wd 0.0000	time 0.1643 (0.1986)	loss 0.8662 (0.8809)	grad_norm 0.3843 (0.3931)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:57:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0000	time 0.1711 (0.1978)	loss 0.9004 (0.8810)	grad_norm 0.3637 (0.3931)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:57:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 21 training takes 0:08:21
[2024-07-02 00:58:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 24.965 (24.965)	Loss 0.3564 (0.3564)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 00:58:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.288 Acc@5 97.678
[2024-07-02 00:58:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 00:58:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.29%
[2024-07-02 00:58:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 00:58:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 00:58:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][0/2502]	eta 10:07:40 lr 0.000017	 wd 0.0000	time 14.5724 (14.5724)	loss 0.8369 (0.8369)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:59:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:13:09 lr 0.000017	 wd 0.0000	time 0.1705 (0.3287)	loss 0.9253 (0.8846)	grad_norm 0.3970 (0.3949)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:59:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:09:46 lr 0.000017	 wd 0.0000	time 0.1661 (0.2547)	loss 0.7983 (0.8804)	grad_norm 0.3830 (0.3938)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 00:59:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:08:51 lr 0.000017	 wd 0.0000	time 0.2208 (0.2414)	loss 0.8809 (0.8799)	grad_norm 0.3890 (0.3941)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:00:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:08:02 lr 0.000017	 wd 0.0000	time 0.1825 (0.2294)	loss 0.8091 (0.8850)	grad_norm 0.3828 (0.3945)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:00:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:07:19 lr 0.000017	 wd 0.0000	time 0.1630 (0.2195)	loss 0.6973 (0.8830)	grad_norm 0.3810 (0.3940)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:00:40 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:06:46 lr 0.000016	 wd 0.0000	time 0.1643 (0.2139)	loss 0.9302 (0.8842)	grad_norm 0.3719 (0.3942)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:00:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:06:18 lr 0.000016	 wd 0.0000	time 0.1677 (0.2100)	loss 0.9932 (0.8827)	grad_norm 0.3802 (0.3947)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:01:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:05:59 lr 0.000016	 wd 0.0000	time 0.1853 (0.2109)	loss 0.8984 (0.8828)	grad_norm 0.3888 (0.3946)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:01:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:05:36 lr 0.000016	 wd 0.0000	time 0.1709 (0.2103)	loss 0.8486 (0.8828)	grad_norm 0.3733 (0.3948)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:01:59 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:05:11 lr 0.000016	 wd 0.0000	time 0.1682 (0.2074)	loss 0.8989 (0.8824)	grad_norm 0.3790 (nan)	loss_scale 32768.0000 (32833.4705)	mem 7964MB
[2024-07-02 01:02:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:04:47 lr 0.000016	 wd 0.0000	time 0.1622 (0.2053)	loss 0.7783 (0.8809)	grad_norm 0.3789 (nan)	loss_scale 32768.0000 (32827.5241)	mem 7964MB
[2024-07-02 01:02:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:04:24 lr 0.000016	 wd 0.0000	time 0.1786 (0.2031)	loss 0.8521 (0.8813)	grad_norm 0.3918 (nan)	loss_scale 32768.0000 (32822.5679)	mem 7964MB
[2024-07-02 01:02:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:04:02 lr 0.000015	 wd 0.0000	time 0.2436 (0.2021)	loss 0.8794 (0.8800)	grad_norm 0.3884 (nan)	loss_scale 32768.0000 (32818.3736)	mem 7964MB
[2024-07-02 01:03:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:03:42 lr 0.000015	 wd 0.0000	time 0.1637 (0.2016)	loss 0.8682 (0.8797)	grad_norm 0.3910 (nan)	loss_scale 32768.0000 (32814.7780)	mem 7964MB
[2024-07-02 01:03:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:03:20 lr 0.000015	 wd 0.0000	time 0.1692 (0.2004)	loss 0.6934 (0.8797)	grad_norm 0.3844 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-02 01:03:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:02:59 lr 0.000015	 wd 0.0000	time 0.1752 (0.1995)	loss 0.8340 (0.8800)	grad_norm 0.3954 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-02 01:04:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:02:39 lr 0.000015	 wd 0.0000	time 0.1707 (0.1985)	loss 0.8984 (0.8795)	grad_norm 0.4027 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-02 01:04:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:02:18 lr 0.000015	 wd 0.0000	time 0.1756 (0.1976)	loss 0.9326 (0.8795)	grad_norm 0.3906 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-02 01:04:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:00 lr 0.000015	 wd 0.0000	time 0.1560 (0.1996)	loss 0.8921 (0.8799)	grad_norm 0.3908 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-02 01:05:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:01:40 lr 0.000014	 wd 0.0000	time 0.1978 (0.1995)	loss 0.9668 (0.8797)	grad_norm 0.3631 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-02 01:05:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:20 lr 0.000014	 wd 0.0000	time 0.1861 (0.1990)	loss 0.8291 (0.8801)	grad_norm 0.3983 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-02 01:05:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:00:59 lr 0.000014	 wd 0.0000	time 0.1589 (0.1984)	loss 0.6904 (0.8800)	grad_norm 0.3865 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-02 01:06:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:39 lr 0.000014	 wd 0.0000	time 0.2006 (0.1979)	loss 0.9414 (0.8799)	grad_norm 0.3980 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-02 01:06:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:20 lr 0.000014	 wd 0.0000	time 0.1702 (0.2003)	loss 0.7686 (0.8800)	grad_norm 0.3927 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-02 01:06:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.1622 (0.1996)	loss 0.8486 (0.8795)	grad_norm 0.4008 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 01:07:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 22 training takes 0:08:28
[2024-07-02 01:07:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.913 (16.913)	Loss 0.3562 (0.3562)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 01:07:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.314 Acc@5 97.684
[2024-07-02 01:07:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 01:07:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.31%
[2024-07-02 01:07:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 01:07:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 01:08:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][0/2502]	eta 18:00:20 lr 0.000014	 wd 0.0000	time 25.9075 (25.9075)	loss 0.7769 (0.7769)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:08:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:17:42 lr 0.000014	 wd 0.0000	time 0.1778 (0.4423)	loss 0.8892 (0.8786)	grad_norm 0.3958 (0.3976)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:08:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:11:58 lr 0.000013	 wd 0.0000	time 0.1762 (0.3123)	loss 0.9365 (0.8753)	grad_norm 0.4243 (0.3958)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:08:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:09:50 lr 0.000013	 wd 0.0000	time 0.1743 (0.2682)	loss 0.7803 (0.8734)	grad_norm 0.3881 (0.3958)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:09:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:08:42 lr 0.000013	 wd 0.0000	time 0.1699 (0.2487)	loss 0.9927 (0.8751)	grad_norm 0.3967 (0.3965)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:09:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:07:58 lr 0.000013	 wd 0.0000	time 0.2060 (0.2391)	loss 0.9053 (0.8793)	grad_norm 0.3877 (0.3968)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:09:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:07:18 lr 0.000013	 wd 0.0000	time 0.2131 (0.2306)	loss 0.8950 (0.8802)	grad_norm 0.3992 (0.3968)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:10:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:06:42 lr 0.000013	 wd 0.0000	time 0.1760 (0.2235)	loss 0.9639 (0.8810)	grad_norm 0.3765 (0.3963)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:10:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:06:11 lr 0.000013	 wd 0.0000	time 0.1644 (0.2185)	loss 0.9902 (0.8806)	grad_norm 0.3941 (0.3961)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:10:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:05:43 lr 0.000012	 wd 0.0000	time 0.1825 (0.2144)	loss 1.1143 (0.8792)	grad_norm 0.3966 (0.3960)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:11:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:05:27 lr 0.000012	 wd 0.0000	time 0.1702 (0.2182)	loss 0.7402 (0.8797)	grad_norm 0.3976 (0.3960)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:11:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:05:01 lr 0.000012	 wd 0.0000	time 0.1679 (0.2151)	loss 0.7197 (0.8788)	grad_norm 0.4040 (0.3960)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:11:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:04:36 lr 0.000012	 wd 0.0000	time 0.1587 (0.2125)	loss 0.9414 (0.8791)	grad_norm 0.3798 (0.3960)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:12:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:04:12 lr 0.000012	 wd 0.0000	time 0.1730 (0.2102)	loss 0.9590 (0.8794)	grad_norm 0.4178 (0.3962)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:12:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:03:49 lr 0.000012	 wd 0.0000	time 0.1668 (0.2081)	loss 0.9521 (0.8796)	grad_norm 0.3980 (0.3962)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:12:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:03:30 lr 0.000012	 wd 0.0000	time 0.1552 (0.2102)	loss 0.7568 (0.8804)	grad_norm 0.4040 (0.3963)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:13:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:03:09 lr 0.000012	 wd 0.0000	time 0.2014 (0.2097)	loss 0.8965 (0.8804)	grad_norm 0.3835 (0.3964)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:13:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:02:47 lr 0.000011	 wd 0.0000	time 0.1764 (0.2085)	loss 0.7954 (0.8801)	grad_norm 0.3992 (0.3965)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:13:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:02:25 lr 0.000011	 wd 0.0000	time 0.1807 (0.2071)	loss 0.8994 (0.8804)	grad_norm 0.3846 (0.3966)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:14:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:02:03 lr 0.000011	 wd 0.0000	time 0.1657 (0.2060)	loss 0.7964 (0.8799)	grad_norm 0.3860 (0.3964)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:14:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:01:44 lr 0.000011	 wd 0.0000	time 0.1791 (0.2087)	loss 0.9849 (0.8797)	grad_norm 0.3929 (0.3964)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:14:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:23 lr 0.000011	 wd 0.0000	time 0.1578 (0.2078)	loss 1.0205 (0.8797)	grad_norm 0.3642 (0.3964)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:15:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:02 lr 0.000011	 wd 0.0000	time 0.1685 (0.2071)	loss 0.9717 (0.8792)	grad_norm 0.4091 (0.3963)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:15:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:41 lr 0.000011	 wd 0.0000	time 0.1954 (0.2062)	loss 0.8584 (0.8793)	grad_norm 0.3983 (0.3962)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:15:48 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:20 lr 0.000011	 wd 0.0000	time 0.1802 (0.2053)	loss 0.8970 (0.8794)	grad_norm 0.3874 (0.3963)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:16:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.1564 (0.2080)	loss 0.8096 (0.8793)	grad_norm 0.4015 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 01:16:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 23 training takes 0:08:49
[2024-07-02 01:16:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.026 (17.026)	Loss 0.3572 (0.3572)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 01:16:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.284 Acc@5 97.694
[2024-07-02 01:16:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 01:16:58 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.31%
[2024-07-02 01:17:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][0/2502]	eta 10:46:44 lr 0.000010	 wd 0.0000	time 15.5096 (15.5096)	loss 0.8784 (0.8784)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:17:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:14:54 lr 0.000010	 wd 0.0000	time 0.1789 (0.3725)	loss 0.9795 (0.8750)	grad_norm 0.3732 (0.3974)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:17:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:10:46 lr 0.000010	 wd 0.0000	time 0.1537 (0.2810)	loss 0.8984 (0.8768)	grad_norm 0.4035 (0.3964)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:18:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:09:04 lr 0.000010	 wd 0.0000	time 0.1636 (0.2473)	loss 0.8071 (0.8797)	grad_norm 0.4217 (0.3966)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:18:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:08:05 lr 0.000010	 wd 0.0000	time 0.1664 (0.2310)	loss 0.7417 (0.8808)	grad_norm 0.3885 (0.3965)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:18:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:07:21 lr 0.000010	 wd 0.0000	time 0.1855 (0.2206)	loss 0.8975 (0.8826)	grad_norm 0.4448 (0.3968)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:19:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:06:46 lr 0.000010	 wd 0.0000	time 0.2105 (0.2137)	loss 0.8989 (0.8803)	grad_norm 0.3712 (0.3973)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:19:28 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:06:24 lr 0.000010	 wd 0.0000	time 0.1982 (0.2131)	loss 0.8369 (0.8796)	grad_norm 0.3825 (0.3968)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:19:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:05:56 lr 0.000010	 wd 0.0000	time 0.1741 (0.2094)	loss 0.7974 (0.8790)	grad_norm 0.4027 (0.3967)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:20:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:05:31 lr 0.000009	 wd 0.0000	time 0.1611 (0.2067)	loss 0.9512 (0.8796)	grad_norm 0.4001 (0.3965)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:20:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:05:06 lr 0.000009	 wd 0.0000	time 0.1721 (0.2043)	loss 0.8237 (0.8804)	grad_norm 0.3957 (0.3965)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:20:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:04:43 lr 0.000009	 wd 0.0000	time 0.1618 (0.2021)	loss 0.8154 (0.8804)	grad_norm 0.3928 (0.3969)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:21:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:04:28 lr 0.000009	 wd 0.0000	time 0.1629 (0.2061)	loss 0.8296 (0.8800)	grad_norm 0.3690 (0.3970)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:21:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:04:05 lr 0.000009	 wd 0.0000	time 0.1740 (0.2046)	loss 0.7441 (0.8786)	grad_norm 0.4005 (0.3969)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:21:44 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:03:45 lr 0.000009	 wd 0.0000	time 0.1763 (0.2042)	loss 0.7935 (0.8790)	grad_norm 0.3817 (0.3969)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:22:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:03:23 lr 0.000009	 wd 0.0000	time 0.1697 (0.2027)	loss 0.8325 (0.8790)	grad_norm 0.3876 (0.3968)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:22:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:03:04 lr 0.000009	 wd 0.0000	time 0.1771 (0.2045)	loss 0.9028 (0.8790)	grad_norm 0.3848 (0.3970)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:22:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:02:44 lr 0.000008	 wd 0.0000	time 0.1649 (0.2051)	loss 0.9678 (0.8793)	grad_norm 0.4048 (0.3969)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:23:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:02:23 lr 0.000008	 wd 0.0000	time 0.1660 (0.2041)	loss 0.7808 (0.8797)	grad_norm 0.3904 (0.3969)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:23:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:02:02 lr 0.000008	 wd 0.0000	time 0.1671 (0.2031)	loss 0.7456 (0.8793)	grad_norm 0.3810 (0.3970)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:23:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:01:41 lr 0.000008	 wd 0.0000	time 0.1795 (0.2023)	loss 0.7139 (0.8792)	grad_norm 0.3992 (0.3971)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:24:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:23 lr 0.000008	 wd 0.0000	time 0.1819 (0.2066)	loss 0.9702 (0.8790)	grad_norm 0.4058 (0.3971)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:24:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:02 lr 0.000008	 wd 0.0000	time 0.1947 (0.2063)	loss 0.8779 (0.8789)	grad_norm 0.3885 (0.3972)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:24:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:41 lr 0.000008	 wd 0.0000	time 0.1962 (0.2065)	loss 0.9229 (0.8790)	grad_norm 0.3790 (0.3971)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:25:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:20 lr 0.000008	 wd 0.0000	time 0.1709 (0.2057)	loss 1.0781 (0.8789)	grad_norm 0.3866 (0.3970)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:25:30 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.1593 (0.2046)	loss 0.9937 (0.8788)	grad_norm 0.3838 (0.3970)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:25:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 24 training takes 0:08:43
[2024-07-02 01:26:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 18.332 (18.332)	Loss 0.3574 (0.3574)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 01:26:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.324 Acc@5 97.692
[2024-07-02 01:26:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 01:26:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-02 01:26:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 01:26:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 01:26:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][0/2502]	eta 9:33:58 lr 0.000008	 wd 0.0000	time 13.7642 (13.7642)	loss 0.8721 (0.8721)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:26:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:14:37 lr 0.000008	 wd 0.0000	time 0.1582 (0.3653)	loss 0.8364 (0.8932)	grad_norm 0.3802 (0.3990)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:27:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:10:26 lr 0.000007	 wd 0.0000	time 0.1742 (0.2723)	loss 0.8237 (0.8903)	grad_norm 0.3946 (0.3979)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:27:37 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:09:26 lr 0.000007	 wd 0.0000	time 0.1942 (0.2571)	loss 0.8994 (0.8828)	grad_norm 0.3964 (0.3976)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:27:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:08:20 lr 0.000007	 wd 0.0000	time 0.1701 (0.2383)	loss 0.7412 (0.8836)	grad_norm 0.3996 (0.3980)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:28:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:07:34 lr 0.000007	 wd 0.0000	time 0.1631 (0.2273)	loss 0.8687 (0.8828)	grad_norm 0.3751 (0.3978)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:28:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:06:57 lr 0.000007	 wd 0.0000	time 0.1692 (0.2195)	loss 0.7412 (0.8822)	grad_norm 0.4125 (0.3977)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:28:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:06:25 lr 0.000007	 wd 0.0000	time 0.1541 (0.2141)	loss 0.8291 (0.8814)	grad_norm 0.3980 (0.3976)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:29:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:06:08 lr 0.000007	 wd 0.0000	time 0.1643 (0.2164)	loss 0.9766 (0.8809)	grad_norm 0.3986 (0.3976)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:29:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:05:40 lr 0.000007	 wd 0.0000	time 0.1879 (0.2128)	loss 0.7319 (0.8806)	grad_norm 0.3869 (0.3975)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:29:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:05:15 lr 0.000007	 wd 0.0000	time 0.1688 (0.2099)	loss 0.9360 (0.8811)	grad_norm 0.4233 (0.3979)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:30:08 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:04:50 lr 0.000007	 wd 0.0000	time 0.1850 (0.2072)	loss 1.0078 (0.8806)	grad_norm 0.3950 (0.3979)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:30:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:04:26 lr 0.000006	 wd 0.0000	time 0.1676 (0.2049)	loss 1.0078 (0.8814)	grad_norm 0.3996 (0.3978)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:30:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:04:06 lr 0.000006	 wd 0.0000	time 0.3331 (0.2054)	loss 0.8789 (0.8803)	grad_norm 0.3823 (0.3978)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:31:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:03:44 lr 0.000006	 wd 0.0000	time 0.1659 (0.2040)	loss 0.9380 (0.8800)	grad_norm 0.3784 (0.3977)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:31:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:03:23 lr 0.000006	 wd 0.0000	time 0.1574 (0.2028)	loss 0.9766 (0.8809)	grad_norm 0.3920 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-02 01:31:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:03:02 lr 0.000006	 wd 0.0000	time 0.1866 (0.2020)	loss 0.7505 (0.8804)	grad_norm 0.3819 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-02 01:32:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:02:40 lr 0.000006	 wd 0.0000	time 0.1887 (0.2007)	loss 0.8223 (0.8811)	grad_norm 0.3840 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-02 01:32:23 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:02:21 lr 0.000006	 wd 0.0000	time 0.1564 (0.2013)	loss 0.9077 (0.8805)	grad_norm 0.3716 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-02 01:32:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:02:00 lr 0.000006	 wd 0.0000	time 0.1757 (0.2004)	loss 0.9849 (0.8806)	grad_norm 0.3902 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-02 01:33:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:01:40 lr 0.000006	 wd 0.0000	time 0.1993 (0.1999)	loss 1.0234 (0.8806)	grad_norm 0.3955 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-02 01:33:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:20 lr 0.000006	 wd 0.0000	time 0.1990 (0.1993)	loss 0.8960 (0.8800)	grad_norm 0.3878 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-02 01:33:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:00 lr 0.000006	 wd 0.0000	time 0.1803 (0.1989)	loss 0.8379 (0.8797)	grad_norm 0.3716 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-02 01:34:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:40 lr 0.000005	 wd 0.0000	time 0.1757 (0.2020)	loss 0.7866 (0.8795)	grad_norm 0.3879 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-02 01:34:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:20 lr 0.000005	 wd 0.0000	time 0.1916 (0.2014)	loss 0.8330 (0.8795)	grad_norm 0.3799 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-02 01:34:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0000	time 0.1655 (0.2004)	loss 0.8867 (0.8799)	grad_norm 0.4006 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 01:34:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 25 training takes 0:08:29
[2024-07-02 01:35:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.650 (16.650)	Loss 0.3574 (0.3574)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 01:35:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.308 Acc@5 97.690
[2024-07-02 01:35:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 01:35:25 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-02 01:35:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][0/2502]	eta 12:17:40 lr 0.000005	 wd 0.0000	time 17.6900 (17.6900)	loss 1.0332 (1.0332)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:36:02 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:14:34 lr 0.000005	 wd 0.0000	time 0.1685 (0.3642)	loss 0.7886 (0.8890)	grad_norm 0.3875 (0.4031)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:36:20 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:10:28 lr 0.000005	 wd 0.0000	time 0.1825 (0.2730)	loss 0.8569 (0.8849)	grad_norm 0.4083 (0.4011)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:36:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:08:53 lr 0.000005	 wd 0.0000	time 0.1613 (0.2422)	loss 0.8750 (0.8863)	grad_norm 0.4145 (0.4006)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:36:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:07:55 lr 0.000005	 wd 0.0000	time 0.1810 (0.2261)	loss 0.9038 (0.8836)	grad_norm 0.3963 (0.3996)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:37:18 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:07:31 lr 0.000005	 wd 0.0000	time 0.1618 (0.2255)	loss 0.8149 (0.8814)	grad_norm 0.3688 (0.3989)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:37:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:06:55 lr 0.000005	 wd 0.0000	time 0.1832 (0.2186)	loss 0.8647 (0.8824)	grad_norm 0.3979 (0.3990)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:37:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:06:24 lr 0.000005	 wd 0.0000	time 0.1752 (0.2134)	loss 0.8276 (0.8805)	grad_norm 0.3826 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:38:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:05:57 lr 0.000005	 wd 0.0000	time 0.1676 (0.2103)	loss 0.8604 (0.8814)	grad_norm 0.4115 (0.3984)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:38:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:05:31 lr 0.000005	 wd 0.0000	time 0.1640 (0.2069)	loss 0.7935 (0.8799)	grad_norm 0.4042 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:38:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:05:10 lr 0.000004	 wd 0.0000	time 0.1757 (0.2067)	loss 0.9414 (0.8803)	grad_norm 0.3819 (0.3987)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:39:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:04:46 lr 0.000004	 wd 0.0000	time 0.1555 (0.2046)	loss 0.9795 (0.8810)	grad_norm 0.3832 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:39:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:04:23 lr 0.000004	 wd 0.0000	time 0.1825 (0.2027)	loss 0.8970 (0.8807)	grad_norm 0.3985 (0.3987)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:39:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:04:01 lr 0.000004	 wd 0.0000	time 0.1923 (0.2011)	loss 0.7490 (0.8810)	grad_norm 0.3878 (0.3987)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:40:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:03:39 lr 0.000004	 wd 0.0000	time 0.1660 (0.1996)	loss 0.9829 (0.8806)	grad_norm 0.3946 (0.3988)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:40:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:03:27 lr 0.000004	 wd 0.0000	time 0.1797 (0.2074)	loss 0.8608 (0.8809)	grad_norm 0.3862 (0.3988)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:40:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:03:05 lr 0.000004	 wd 0.0000	time 0.1642 (0.2057)	loss 0.7383 (0.8799)	grad_norm 0.3709 (0.3987)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:41:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:02:43 lr 0.000004	 wd 0.0000	time 0.1628 (0.2045)	loss 0.8613 (0.8806)	grad_norm 0.3806 (0.3987)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:41:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:02:22 lr 0.000004	 wd 0.0000	time 0.1667 (0.2033)	loss 0.9214 (0.8811)	grad_norm 0.3974 (0.3988)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:41:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:02:01 lr 0.000004	 wd 0.0000	time 0.1860 (0.2022)	loss 0.8882 (0.8805)	grad_norm 0.3954 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:42:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:01:42 lr 0.000004	 wd 0.0000	time 0.1757 (0.2045)	loss 0.9404 (0.8804)	grad_norm 0.4025 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:42:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:21 lr 0.000004	 wd 0.0000	time 0.1543 (0.2036)	loss 0.8872 (0.8806)	grad_norm 0.4106 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:42:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:01 lr 0.000004	 wd 0.0000	time 0.1762 (0.2031)	loss 0.9351 (0.8806)	grad_norm 0.4005 (0.3985)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:43:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:40 lr 0.000004	 wd 0.0000	time 0.1939 (0.2023)	loss 0.8428 (0.8802)	grad_norm 0.4051 (0.3985)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:43:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:20 lr 0.000003	 wd 0.0000	time 0.1713 (0.2036)	loss 0.8735 (0.8806)	grad_norm 0.4081 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:43:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.1597 (0.2031)	loss 0.9292 (0.8807)	grad_norm 0.3920 (0.3986)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:44:04 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 26 training takes 0:08:39
[2024-07-02 01:44:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 17.779 (17.779)	Loss 0.3567 (0.3567)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 01:44:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.304 Acc@5 97.692
[2024-07-02 01:44:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 01:44:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-02 01:44:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][0/2502]	eta 9:23:13 lr 0.000003	 wd 0.0000	time 13.5066 (13.5066)	loss 0.9297 (0.9297)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:45:21 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:15:36 lr 0.000003	 wd 0.0000	time 0.1933 (0.3901)	loss 0.8975 (0.8854)	grad_norm 0.3979 (0.3976)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:45:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:11:02 lr 0.000003	 wd 0.0000	time 0.1801 (0.2876)	loss 0.7275 (0.8798)	grad_norm 0.4109 (0.3980)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:45:57 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:09:15 lr 0.000003	 wd 0.0000	time 0.1682 (0.2523)	loss 0.8911 (0.8799)	grad_norm 0.4133 (0.3985)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:46:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:08:12 lr 0.000003	 wd 0.0000	time 0.1683 (0.2344)	loss 0.8188 (0.8790)	grad_norm 0.3957 (0.3987)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:46:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:07:26 lr 0.000003	 wd 0.0000	time 0.1790 (0.2231)	loss 0.7412 (0.8782)	grad_norm 0.4275 (nan)	loss_scale 32768.0000 (32898.8104)	mem 7964MB
[2024-07-02 01:46:55 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:07:01 lr 0.000003	 wd 0.0000	time 0.1786 (0.2216)	loss 0.8120 (0.8805)	grad_norm 0.4052 (nan)	loss_scale 32768.0000 (32877.0449)	mem 7964MB
[2024-07-02 01:47:14 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:06:31 lr 0.000003	 wd 0.0000	time 0.1620 (0.2173)	loss 0.8232 (0.8800)	grad_norm 0.4128 (nan)	loss_scale 32768.0000 (32861.4893)	mem 7964MB
[2024-07-02 01:47:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:06:02 lr 0.000003	 wd 0.0000	time 0.1665 (0.2129)	loss 0.9023 (0.8794)	grad_norm 0.3878 (nan)	loss_scale 32768.0000 (32849.8177)	mem 7964MB
[2024-07-02 01:47:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:05:36 lr 0.000003	 wd 0.0000	time 0.1536 (0.2098)	loss 0.7310 (0.8789)	grad_norm 0.3866 (nan)	loss_scale 32768.0000 (32840.7370)	mem 7964MB
[2024-07-02 01:48:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:05:10 lr 0.000003	 wd 0.0000	time 0.1680 (0.2070)	loss 0.8916 (0.8785)	grad_norm 0.3834 (nan)	loss_scale 32768.0000 (32833.4705)	mem 7964MB
[2024-07-02 01:48:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:04:51 lr 0.000003	 wd 0.0000	time 0.1721 (0.2082)	loss 0.7749 (0.8783)	grad_norm 0.4073 (nan)	loss_scale 32768.0000 (32827.5241)	mem 7964MB
[2024-07-02 01:48:52 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:04:31 lr 0.000003	 wd 0.0000	time 0.1558 (0.2083)	loss 0.8926 (0.8775)	grad_norm 0.3928 (nan)	loss_scale 32768.0000 (32822.5679)	mem 7964MB
[2024-07-02 01:49:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:04:08 lr 0.000003	 wd 0.0000	time 0.1813 (0.2065)	loss 0.8525 (0.8776)	grad_norm 0.4126 (nan)	loss_scale 32768.0000 (32818.3736)	mem 7964MB
[2024-07-02 01:49:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:03:45 lr 0.000003	 wd 0.0000	time 0.1766 (0.2049)	loss 0.8057 (0.8775)	grad_norm 0.3963 (nan)	loss_scale 32768.0000 (32814.7780)	mem 7964MB
[2024-07-02 01:49:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:03:23 lr 0.000003	 wd 0.0000	time 0.1633 (0.2032)	loss 0.9844 (0.8775)	grad_norm 0.3880 (nan)	loss_scale 32768.0000 (32811.6616)	mem 7964MB
[2024-07-02 01:50:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:03:04 lr 0.000003	 wd 0.0000	time 0.1644 (0.2048)	loss 0.8896 (0.8776)	grad_norm 0.3926 (nan)	loss_scale 32768.0000 (32808.9344)	mem 7964MB
[2024-07-02 01:50:29 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:02:43 lr 0.000002	 wd 0.0000	time 0.2375 (0.2041)	loss 0.8818 (0.8777)	grad_norm 0.3999 (nan)	loss_scale 32768.0000 (32806.5279)	mem 7964MB
[2024-07-02 01:50:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:02:22 lr 0.000002	 wd 0.0000	time 0.1978 (0.2030)	loss 0.7725 (0.8781)	grad_norm 0.3945 (nan)	loss_scale 32768.0000 (32804.3887)	mem 7964MB
[2024-07-02 01:51:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:02:01 lr 0.000002	 wd 0.0000	time 0.1625 (0.2022)	loss 0.8115 (0.8782)	grad_norm 0.3931 (nan)	loss_scale 32768.0000 (32802.4745)	mem 7964MB
[2024-07-02 01:51:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:01:41 lr 0.000002	 wd 0.0000	time 0.1757 (0.2013)	loss 0.7588 (0.8779)	grad_norm 0.3774 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-02 01:51:46 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:21 lr 0.000002	 wd 0.0000	time 0.1617 (0.2023)	loss 1.0264 (0.8788)	grad_norm 0.4154 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-02 01:52:07 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:01 lr 0.000002	 wd 0.0000	time 0.1585 (0.2022)	loss 0.7393 (0.8783)	grad_norm 0.3911 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-02 01:52:26 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:40 lr 0.000002	 wd 0.0000	time 0.2047 (0.2019)	loss 0.8467 (0.8787)	grad_norm 0.3956 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-02 01:52:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:20 lr 0.000002	 wd 0.0000	time 0.1619 (0.2014)	loss 0.8921 (0.8783)	grad_norm 0.4082 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-02 01:53:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.1626 (0.2004)	loss 0.8589 (0.8785)	grad_norm 0.3926 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 01:53:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 27 training takes 0:08:30
[2024-07-02 01:53:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 19.185 (19.185)	Loss 0.3577 (0.3577)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 01:53:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.326 Acc@5 97.694
[2024-07-02 01:53:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 01:53:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 01:53:49 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 01:53:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 01:54:05 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][0/2502]	eta 10:04:03 lr 0.000002	 wd 0.0000	time 14.4860 (14.4860)	loss 0.8516 (0.8516)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:54:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:13:19 lr 0.000002	 wd 0.0000	time 0.1615 (0.3328)	loss 0.9038 (0.8742)	grad_norm 0.4255 (0.4002)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:54:42 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:09:50 lr 0.000002	 wd 0.0000	time 0.1810 (0.2567)	loss 0.8418 (0.8802)	grad_norm 0.4005 (0.3995)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:55:06 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:09:07 lr 0.000002	 wd 0.0000	time 0.1830 (0.2487)	loss 0.9961 (0.8819)	grad_norm 0.3974 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:55:24 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:08:10 lr 0.000002	 wd 0.0000	time 0.1637 (0.2335)	loss 0.7861 (0.8769)	grad_norm 0.4004 (0.3995)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:55:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:07:27 lr 0.000002	 wd 0.0000	time 0.2080 (0.2235)	loss 0.8765 (0.8795)	grad_norm 0.3947 (0.3999)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:56:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:06:51 lr 0.000002	 wd 0.0000	time 0.1643 (0.2165)	loss 0.9175 (0.8808)	grad_norm 0.3929 (0.3992)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:56:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:06:20 lr 0.000002	 wd 0.0000	time 0.1839 (0.2112)	loss 0.8604 (0.8798)	grad_norm 0.3946 (0.3991)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:56:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:06:22 lr 0.000002	 wd 0.0000	time 0.1670 (0.2246)	loss 0.8428 (0.8805)	grad_norm 0.3896 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:57:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:05:52 lr 0.000002	 wd 0.0000	time 0.1742 (0.2197)	loss 0.8291 (0.8805)	grad_norm 0.4003 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:57:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:05:24 lr 0.000002	 wd 0.0000	time 0.1809 (0.2161)	loss 0.8516 (0.8804)	grad_norm 0.3800 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:57:45 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:04:58 lr 0.000002	 wd 0.0000	time 0.1882 (0.2129)	loss 0.8164 (0.8787)	grad_norm 0.4003 (0.3992)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:58:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:04:33 lr 0.000002	 wd 0.0000	time 0.1880 (0.2103)	loss 0.8745 (0.8788)	grad_norm 0.4087 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:58:32 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:04:19 lr 0.000002	 wd 0.0000	time 0.1982 (0.2162)	loss 0.9648 (0.8792)	grad_norm 0.4084 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:58:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:03:55 lr 0.000002	 wd 0.0000	time 0.1657 (0.2138)	loss 0.7993 (0.8785)	grad_norm 0.3982 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:59:09 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:03:32 lr 0.000002	 wd 0.0000	time 0.1811 (0.2120)	loss 0.7944 (0.8786)	grad_norm 0.4118 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:59:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:03:09 lr 0.000002	 wd 0.0000	time 0.1676 (0.2100)	loss 1.0107 (0.8789)	grad_norm 0.4138 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 01:59:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:02:51 lr 0.000001	 wd 0.0000	time 0.1907 (0.2135)	loss 0.9160 (0.8789)	grad_norm 0.4198 (0.3992)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:00:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:02:30 lr 0.000001	 wd 0.0000	time 0.1877 (0.2139)	loss 0.7808 (0.8786)	grad_norm 0.4176 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:00:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:02:07 lr 0.000001	 wd 0.0000	time 0.1738 (0.2125)	loss 0.8765 (0.8795)	grad_norm 0.3910 (0.3996)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:00:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:01:46 lr 0.000001	 wd 0.0000	time 0.1606 (0.2112)	loss 0.7891 (0.8787)	grad_norm 0.3962 (nan)	loss_scale 32768.0000 (32800.7516)	mem 7964MB
[2024-07-02 02:01:12 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:24 lr 0.000001	 wd 0.0000	time 0.1691 (0.2100)	loss 0.8564 (0.8785)	grad_norm 0.4200 (nan)	loss_scale 32768.0000 (32799.1928)	mem 7964MB
[2024-07-02 02:01:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:04 lr 0.000001	 wd 0.0000	time 0.1691 (0.2146)	loss 0.8940 (0.8780)	grad_norm 0.4100 (nan)	loss_scale 32768.0000 (32797.7756)	mem 7964MB
[2024-07-02 02:02:03 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:43 lr 0.000001	 wd 0.0000	time 0.1790 (0.2138)	loss 0.8203 (0.8778)	grad_norm 0.3897 (nan)	loss_scale 32768.0000 (32796.4815)	mem 7964MB
[2024-07-02 02:02:22 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:21 lr 0.000001	 wd 0.0000	time 0.1820 (0.2127)	loss 0.9731 (0.8774)	grad_norm 0.4009 (nan)	loss_scale 32768.0000 (32795.2953)	mem 7964MB
[2024-07-02 02:02:39 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1654 (0.2113)	loss 0.8472 (0.8775)	grad_norm 0.3820 (nan)	loss_scale 32768.0000 (32794.2039)	mem 7964MB
[2024-07-02 02:02:47 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 28 training takes 0:08:56
[2024-07-02 02:03:10 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 23.053 (23.053)	Loss 0.3574 (0.3574)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 02:03:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.298 Acc@5 97.686
[2024-07-02 02:03:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 02:03:27 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 02:03:43 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][0/2502]	eta 10:38:43 lr 0.000001	 wd 0.0000	time 15.3173 (15.3173)	loss 0.9551 (0.9551)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:04:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:13:30 lr 0.000001	 wd 0.0000	time 0.1639 (0.3373)	loss 0.8638 (0.8870)	grad_norm 0.4093 (0.4005)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:04:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:09:56 lr 0.000001	 wd 0.0000	time 0.1697 (0.2592)	loss 0.8916 (0.8872)	grad_norm 0.4173 (0.3998)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:04:41 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:09:00 lr 0.000001	 wd 0.0000	time 0.1952 (0.2457)	loss 0.8423 (0.8873)	grad_norm 0.3995 (0.3988)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:05:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:08:12 lr 0.000001	 wd 0.0000	time 0.1669 (0.2342)	loss 0.8682 (0.8865)	grad_norm 0.3938 (0.3988)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:05:19 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:07:27 lr 0.000001	 wd 0.0000	time 0.2158 (0.2238)	loss 0.8301 (0.8861)	grad_norm 0.4093 (0.3997)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:05:38 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:06:52 lr 0.000001	 wd 0.0000	time 0.1830 (0.2167)	loss 0.7461 (0.8851)	grad_norm 0.4096 (0.3992)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:05:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:06:20 lr 0.000001	 wd 0.0000	time 0.1673 (0.2114)	loss 0.8433 (0.8840)	grad_norm 0.3841 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:06:15 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:05:55 lr 0.000001	 wd 0.0000	time 0.1874 (0.2089)	loss 0.8228 (0.8843)	grad_norm 0.3791 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:06:36 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:05:36 lr 0.000001	 wd 0.0000	time 0.1635 (0.2098)	loss 0.8750 (0.8844)	grad_norm 0.4110 (0.3995)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:06:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:05:10 lr 0.000001	 wd 0.0000	time 0.1638 (0.2068)	loss 0.8193 (0.8846)	grad_norm 0.4199 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:07:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:04:47 lr 0.000001	 wd 0.0000	time 0.1667 (0.2048)	loss 0.9941 (0.8843)	grad_norm 0.3826 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:07:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:04:23 lr 0.000001	 wd 0.0000	time 0.1795 (0.2027)	loss 0.7583 (0.8829)	grad_norm 0.4022 (0.3995)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:07:56 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:04:07 lr 0.000001	 wd 0.0000	time 0.1873 (0.2061)	loss 0.9043 (0.8830)	grad_norm 0.4054 (0.3996)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:08:16 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:03:46 lr 0.000001	 wd 0.0000	time 0.2063 (0.2058)	loss 0.9731 (0.8827)	grad_norm 0.4142 (0.3995)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:08:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:03:24 lr 0.000001	 wd 0.0000	time 0.1692 (0.2042)	loss 0.8096 (0.8813)	grad_norm 0.4020 (0.3996)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:08:53 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:03:03 lr 0.000001	 wd 0.0000	time 0.1863 (0.2031)	loss 0.9116 (0.8807)	grad_norm 0.3987 (0.3996)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:09:11 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:02:41 lr 0.000001	 wd 0.0000	time 0.1769 (0.2019)	loss 0.9263 (0.8796)	grad_norm 0.4103 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:09:34 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:02:23 lr 0.000001	 wd 0.0000	time 0.2863 (0.2038)	loss 0.7632 (0.8789)	grad_norm 0.3824 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:09:54 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:02:02 lr 0.000001	 wd 0.0000	time 0.1642 (0.2035)	loss 0.7261 (0.8789)	grad_norm 0.4056 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:10:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:01:41 lr 0.000001	 wd 0.0000	time 0.1824 (0.2026)	loss 0.8447 (0.8793)	grad_norm 0.4086 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:10:31 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:21 lr 0.000001	 wd 0.0000	time 0.1688 (0.2018)	loss 0.8169 (0.8795)	grad_norm 0.3815 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:10:50 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:00 lr 0.000001	 wd 0.0000	time 0.1662 (0.2012)	loss 0.7812 (0.8789)	grad_norm 0.3983 (0.3993)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:11:13 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:40 lr 0.000001	 wd 0.0000	time 0.1904 (0.2024)	loss 0.7266 (0.8792)	grad_norm 0.4195 (0.3994)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:11:33 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:20 lr 0.000001	 wd 0.0000	time 0.1672 (0.2024)	loss 0.7812 (0.8792)	grad_norm 0.4171 (0.3995)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:11:51 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1530 (0.2014)	loss 0.9512 (0.8790)	grad_norm 0.4074 (0.3995)	loss_scale 32768.0000 (32768.0000)	mem 7964MB
[2024-07-02 02:12:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 249): INFO EPOCH 29 training takes 0:08:32
[2024-07-02 02:12:00 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_29.pth saving......
[2024-07-02 02:12:01 adapter_convnext_base_224_22kto1k_finetune_efficient] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_convnext_base_224_22kto1k_finetune_efficient/adapter_convnext_base_patch4_22kto1k_efficient_finetune/ckpt_epoch_29.pth saved !!!
[2024-07-02 02:12:17 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 289): INFO Test: [0/98]	Time 16.210 (16.210)	Loss 0.3574 (0.3574)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 7964MB
[2024-07-02 02:12:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 296): INFO  * Acc@1 85.308 Acc@5 97.686
[2024-07-02 02:12:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 02:12:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 02:12:35 adapter_convnext_base_224_22kto1k_finetune_efficient] (main.py 189): INFO Training time 4:32:04
