[2024-07-01 20:21:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/config.json
[2024-07-01 20:21:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: vcnu_convnext
  VCNU_CONVNEXT:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: vcnu_convnext_base_patch4_22kto1k_finetune_nonorm
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-01 20:21:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_convnext/vcnu_convnext_base_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "vcnu_convnext_base_patch4_22kto1k_finetune_nonorm", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-01 20:21:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 108): INFO Creating model:vcnu_convnext/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm
[2024-07-01 20:21:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 110): INFO VCNU_ConvNeXt(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
        (t2d_memory_attention): Memory(
          dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=128, bias=True)
          (attn): Linear(in_features=256, out_features=128, bias=True)
          (update_ltm): Linear(in_features=128, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=128, bias=True)
          (attn): Linear(in_features=256, out_features=128, bias=True)
          (update_ltm): Linear(in_features=128, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=128, bias=True)
          (attn): Linear(in_features=256, out_features=128, bias=True)
          (update_ltm): Linear(in_features=128, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=256, bias=True)
          (attn): Linear(in_features=512, out_features=256, bias=True)
          (update_ltm): Linear(in_features=256, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=256, bias=True)
          (attn): Linear(in_features=512, out_features=256, bias=True)
          (update_ltm): Linear(in_features=256, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=256, bias=True)
          (attn): Linear(in_features=512, out_features=256, bias=True)
          (update_ltm): Linear(in_features=256, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=1024, bias=True)
          (attn): Linear(in_features=2048, out_features=1024, bias=True)
          (update_ltm): Linear(in_features=1024, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=1024, bias=True)
          (attn): Linear(in_features=2048, out_features=1024, bias=True)
          (update_ltm): Linear(in_features=1024, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=1024, bias=True)
          (attn): Linear(in_features=2048, out_features=1024, bias=True)
          (update_ltm): Identity()
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (memory_downsampling): ModuleList(
    (0): MemoryDownSampling(
      (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (1): MemoryDownSampling(
      (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (2): MemoryDownSampling(
      (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-01 20:21:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 113): INFO number of params: 110619522
[2024-07-01 20:21:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm, ignoring auto resume
[2024-07-01 20:21:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/convnext-b/convnext_base_22k_224.pth for fine-tuning......
[2024-07-01 20:21:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-01 20:21:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['stages.0.0.t2d_memory_attention.wm.weight', 'stages.0.0.t2d_memory_attention.wm.bias', 'stages.0.0.t2d_memory_attention.attn.weight', 'stages.0.0.t2d_memory_attention.attn.bias', 'stages.0.0.t2d_memory_attention.update_ltm.weight', 'stages.0.0.t2d_memory_attention.update_ltm.bias', 'stages.0.1.t2d_memory_attention.wm.weight', 'stages.0.1.t2d_memory_attention.wm.bias', 'stages.0.1.t2d_memory_attention.attn.weight', 'stages.0.1.t2d_memory_attention.attn.bias', 'stages.0.1.t2d_memory_attention.update_ltm.weight', 'stages.0.1.t2d_memory_attention.update_ltm.bias', 'stages.0.2.t2d_memory_attention.wm.weight', 'stages.0.2.t2d_memory_attention.wm.bias', 'stages.0.2.t2d_memory_attention.attn.weight', 'stages.0.2.t2d_memory_attention.attn.bias', 'stages.0.2.t2d_memory_attention.update_ltm.weight', 'stages.0.2.t2d_memory_attention.update_ltm.bias', 'stages.1.0.t2d_memory_attention.wm.weight', 'stages.1.0.t2d_memory_attention.wm.bias', 'stages.1.0.t2d_memory_attention.attn.weight', 'stages.1.0.t2d_memory_attention.attn.bias', 'stages.1.0.t2d_memory_attention.update_ltm.weight', 'stages.1.0.t2d_memory_attention.update_ltm.bias', 'stages.1.1.t2d_memory_attention.wm.weight', 'stages.1.1.t2d_memory_attention.wm.bias', 'stages.1.1.t2d_memory_attention.attn.weight', 'stages.1.1.t2d_memory_attention.attn.bias', 'stages.1.1.t2d_memory_attention.update_ltm.weight', 'stages.1.1.t2d_memory_attention.update_ltm.bias', 'stages.1.2.t2d_memory_attention.wm.weight', 'stages.1.2.t2d_memory_attention.wm.bias', 'stages.1.2.t2d_memory_attention.attn.weight', 'stages.1.2.t2d_memory_attention.attn.bias', 'stages.1.2.t2d_memory_attention.update_ltm.weight', 'stages.1.2.t2d_memory_attention.update_ltm.bias', 'stages.2.0.t2d_memory_attention.wm.weight', 'stages.2.0.t2d_memory_attention.wm.bias', 'stages.2.0.t2d_memory_attention.attn.weight', 'stages.2.0.t2d_memory_attention.attn.bias', 'stages.2.0.t2d_memory_attention.update_ltm.weight', 'stages.2.0.t2d_memory_attention.update_ltm.bias', 'stages.2.1.t2d_memory_attention.wm.weight', 'stages.2.1.t2d_memory_attention.wm.bias', 'stages.2.1.t2d_memory_attention.attn.weight', 'stages.2.1.t2d_memory_attention.attn.bias', 'stages.2.1.t2d_memory_attention.update_ltm.weight', 'stages.2.1.t2d_memory_attention.update_ltm.bias', 'stages.2.2.t2d_memory_attention.wm.weight', 'stages.2.2.t2d_memory_attention.wm.bias', 'stages.2.2.t2d_memory_attention.attn.weight', 'stages.2.2.t2d_memory_attention.attn.bias', 'stages.2.2.t2d_memory_attention.update_ltm.weight', 'stages.2.2.t2d_memory_attention.update_ltm.bias', 'stages.2.3.t2d_memory_attention.wm.weight', 'stages.2.3.t2d_memory_attention.wm.bias', 'stages.2.3.t2d_memory_attention.attn.weight', 'stages.2.3.t2d_memory_attention.attn.bias', 'stages.2.3.t2d_memory_attention.update_ltm.weight', 'stages.2.3.t2d_memory_attention.update_ltm.bias', 'stages.2.4.t2d_memory_attention.wm.weight', 'stages.2.4.t2d_memory_attention.wm.bias', 'stages.2.4.t2d_memory_attention.attn.weight', 'stages.2.4.t2d_memory_attention.attn.bias', 'stages.2.4.t2d_memory_attention.update_ltm.weight', 'stages.2.4.t2d_memory_attention.update_ltm.bias', 'stages.2.5.t2d_memory_attention.wm.weight', 'stages.2.5.t2d_memory_attention.wm.bias', 'stages.2.5.t2d_memory_attention.attn.weight', 'stages.2.5.t2d_memory_attention.attn.bias', 'stages.2.5.t2d_memory_attention.update_ltm.weight', 'stages.2.5.t2d_memory_attention.update_ltm.bias', 'stages.2.6.t2d_memory_attention.wm.weight', 'stages.2.6.t2d_memory_attention.wm.bias', 'stages.2.6.t2d_memory_attention.attn.weight', 'stages.2.6.t2d_memory_attention.attn.bias', 'stages.2.6.t2d_memory_attention.update_ltm.weight', 'stages.2.6.t2d_memory_attention.update_ltm.bias', 'stages.2.7.t2d_memory_attention.wm.weight', 'stages.2.7.t2d_memory_attention.wm.bias', 'stages.2.7.t2d_memory_attention.attn.weight', 'stages.2.7.t2d_memory_attention.attn.bias', 'stages.2.7.t2d_memory_attention.update_ltm.weight', 'stages.2.7.t2d_memory_attention.update_ltm.bias', 'stages.2.8.t2d_memory_attention.wm.weight', 'stages.2.8.t2d_memory_attention.wm.bias', 'stages.2.8.t2d_memory_attention.attn.weight', 'stages.2.8.t2d_memory_attention.attn.bias', 'stages.2.8.t2d_memory_attention.update_ltm.weight', 'stages.2.8.t2d_memory_attention.update_ltm.bias', 'stages.2.9.t2d_memory_attention.wm.weight', 'stages.2.9.t2d_memory_attention.wm.bias', 'stages.2.9.t2d_memory_attention.attn.weight', 'stages.2.9.t2d_memory_attention.attn.bias', 'stages.2.9.t2d_memory_attention.update_ltm.weight', 'stages.2.9.t2d_memory_attention.update_ltm.bias', 'stages.2.10.t2d_memory_attention.wm.weight', 'stages.2.10.t2d_memory_attention.wm.bias', 'stages.2.10.t2d_memory_attention.attn.weight', 'stages.2.10.t2d_memory_attention.attn.bias', 'stages.2.10.t2d_memory_attention.update_ltm.weight', 'stages.2.10.t2d_memory_attention.update_ltm.bias', 'stages.2.11.t2d_memory_attention.wm.weight', 'stages.2.11.t2d_memory_attention.wm.bias', 'stages.2.11.t2d_memory_attention.attn.weight', 'stages.2.11.t2d_memory_attention.attn.bias', 'stages.2.11.t2d_memory_attention.update_ltm.weight', 'stages.2.11.t2d_memory_attention.update_ltm.bias', 'stages.2.12.t2d_memory_attention.wm.weight', 'stages.2.12.t2d_memory_attention.wm.bias', 'stages.2.12.t2d_memory_attention.attn.weight', 'stages.2.12.t2d_memory_attention.attn.bias', 'stages.2.12.t2d_memory_attention.update_ltm.weight', 'stages.2.12.t2d_memory_attention.update_ltm.bias', 'stages.2.13.t2d_memory_attention.wm.weight', 'stages.2.13.t2d_memory_attention.wm.bias', 'stages.2.13.t2d_memory_attention.attn.weight', 'stages.2.13.t2d_memory_attention.attn.bias', 'stages.2.13.t2d_memory_attention.update_ltm.weight', 'stages.2.13.t2d_memory_attention.update_ltm.bias', 'stages.2.14.t2d_memory_attention.wm.weight', 'stages.2.14.t2d_memory_attention.wm.bias', 'stages.2.14.t2d_memory_attention.attn.weight', 'stages.2.14.t2d_memory_attention.attn.bias', 'stages.2.14.t2d_memory_attention.update_ltm.weight', 'stages.2.14.t2d_memory_attention.update_ltm.bias', 'stages.2.15.t2d_memory_attention.wm.weight', 'stages.2.15.t2d_memory_attention.wm.bias', 'stages.2.15.t2d_memory_attention.attn.weight', 'stages.2.15.t2d_memory_attention.attn.bias', 'stages.2.15.t2d_memory_attention.update_ltm.weight', 'stages.2.15.t2d_memory_attention.update_ltm.bias', 'stages.2.16.t2d_memory_attention.wm.weight', 'stages.2.16.t2d_memory_attention.wm.bias', 'stages.2.16.t2d_memory_attention.attn.weight', 'stages.2.16.t2d_memory_attention.attn.bias', 'stages.2.16.t2d_memory_attention.update_ltm.weight', 'stages.2.16.t2d_memory_attention.update_ltm.bias', 'stages.2.17.t2d_memory_attention.wm.weight', 'stages.2.17.t2d_memory_attention.wm.bias', 'stages.2.17.t2d_memory_attention.attn.weight', 'stages.2.17.t2d_memory_attention.attn.bias', 'stages.2.17.t2d_memory_attention.update_ltm.weight', 'stages.2.17.t2d_memory_attention.update_ltm.bias', 'stages.2.18.t2d_memory_attention.wm.weight', 'stages.2.18.t2d_memory_attention.wm.bias', 'stages.2.18.t2d_memory_attention.attn.weight', 'stages.2.18.t2d_memory_attention.attn.bias', 'stages.2.18.t2d_memory_attention.update_ltm.weight', 'stages.2.18.t2d_memory_attention.update_ltm.bias', 'stages.2.19.t2d_memory_attention.wm.weight', 'stages.2.19.t2d_memory_attention.wm.bias', 'stages.2.19.t2d_memory_attention.attn.weight', 'stages.2.19.t2d_memory_attention.attn.bias', 'stages.2.19.t2d_memory_attention.update_ltm.weight', 'stages.2.19.t2d_memory_attention.update_ltm.bias', 'stages.2.20.t2d_memory_attention.wm.weight', 'stages.2.20.t2d_memory_attention.wm.bias', 'stages.2.20.t2d_memory_attention.attn.weight', 'stages.2.20.t2d_memory_attention.attn.bias', 'stages.2.20.t2d_memory_attention.update_ltm.weight', 'stages.2.20.t2d_memory_attention.update_ltm.bias', 'stages.2.21.t2d_memory_attention.wm.weight', 'stages.2.21.t2d_memory_attention.wm.bias', 'stages.2.21.t2d_memory_attention.attn.weight', 'stages.2.21.t2d_memory_attention.attn.bias', 'stages.2.21.t2d_memory_attention.update_ltm.weight', 'stages.2.21.t2d_memory_attention.update_ltm.bias', 'stages.2.22.t2d_memory_attention.wm.weight', 'stages.2.22.t2d_memory_attention.wm.bias', 'stages.2.22.t2d_memory_attention.attn.weight', 'stages.2.22.t2d_memory_attention.attn.bias', 'stages.2.22.t2d_memory_attention.update_ltm.weight', 'stages.2.22.t2d_memory_attention.update_ltm.bias', 'stages.2.23.t2d_memory_attention.wm.weight', 'stages.2.23.t2d_memory_attention.wm.bias', 'stages.2.23.t2d_memory_attention.attn.weight', 'stages.2.23.t2d_memory_attention.attn.bias', 'stages.2.23.t2d_memory_attention.update_ltm.weight', 'stages.2.23.t2d_memory_attention.update_ltm.bias', 'stages.2.24.t2d_memory_attention.wm.weight', 'stages.2.24.t2d_memory_attention.wm.bias', 'stages.2.24.t2d_memory_attention.attn.weight', 'stages.2.24.t2d_memory_attention.attn.bias', 'stages.2.24.t2d_memory_attention.update_ltm.weight', 'stages.2.24.t2d_memory_attention.update_ltm.bias', 'stages.2.25.t2d_memory_attention.wm.weight', 'stages.2.25.t2d_memory_attention.wm.bias', 'stages.2.25.t2d_memory_attention.attn.weight', 'stages.2.25.t2d_memory_attention.attn.bias', 'stages.2.25.t2d_memory_attention.update_ltm.weight', 'stages.2.25.t2d_memory_attention.update_ltm.bias', 'stages.2.26.t2d_memory_attention.wm.weight', 'stages.2.26.t2d_memory_attention.wm.bias', 'stages.2.26.t2d_memory_attention.attn.weight', 'stages.2.26.t2d_memory_attention.attn.bias', 'stages.2.26.t2d_memory_attention.update_ltm.weight', 'stages.2.26.t2d_memory_attention.update_ltm.bias', 'stages.3.0.t2d_memory_attention.wm.weight', 'stages.3.0.t2d_memory_attention.wm.bias', 'stages.3.0.t2d_memory_attention.attn.weight', 'stages.3.0.t2d_memory_attention.attn.bias', 'stages.3.0.t2d_memory_attention.update_ltm.weight', 'stages.3.0.t2d_memory_attention.update_ltm.bias', 'stages.3.1.t2d_memory_attention.wm.weight', 'stages.3.1.t2d_memory_attention.wm.bias', 'stages.3.1.t2d_memory_attention.attn.weight', 'stages.3.1.t2d_memory_attention.attn.bias', 'stages.3.1.t2d_memory_attention.update_ltm.weight', 'stages.3.1.t2d_memory_attention.update_ltm.bias', 'stages.3.2.t2d_memory_attention.wm.weight', 'stages.3.2.t2d_memory_attention.wm.bias', 'stages.3.2.t2d_memory_attention.attn.weight', 'stages.3.2.t2d_memory_attention.attn.bias'], unexpected_keys=[])
[2024-07-01 20:21:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/convnext-b/convnext_base_22k_224.pth'
[2024-07-01 20:22:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 15.846 (15.846)	Loss 0.5791 (0.5791)	Acc@1 86.133 (86.133)	Acc@5 97.461 (97.461)	Mem 3685MB
[2024-07-01 20:22:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 57.424 Acc@5 73.284
[2024-07-01 20:22:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 162): INFO Accuracy of the network on the 50000 test images: 57.4%
[2024-07-01 20:22:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 168): INFO Start training
[2024-07-01 20:22:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:40:36 lr 0.000100	 wd 0.0000	time 13.9236 (13.9236)	loss 2.0625 (2.0625)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 14840MB
[2024-07-01 20:23:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:26:13 lr 0.000100	 wd 0.0000	time 0.4864 (0.6550)	loss 1.2061 (1.4875)	grad_norm 4.3898 (nan)	loss_scale 16384.0000 (17195.0891)	mem 14840MB
[2024-07-01 20:24:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:22:29 lr 0.000100	 wd 0.0000	time 0.4830 (0.5862)	loss 1.1523 (1.3751)	grad_norm 3.4934 (nan)	loss_scale 16384.0000 (16791.5622)	mem 14840MB
[2024-07-01 20:25:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:20:41 lr 0.000100	 wd 0.0000	time 0.4944 (0.5639)	loss 1.1191 (1.2973)	grad_norm 4.8862 (nan)	loss_scale 8192.0000 (16438.4319)	mem 14840MB
[2024-07-01 20:26:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:19:22 lr 0.000100	 wd 0.0000	time 0.4797 (0.5530)	loss 1.5020 (1.2444)	grad_norm 3.7592 (nan)	loss_scale 8192.0000 (14381.9651)	mem 14840MB
[2024-07-01 20:26:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:18:12 lr 0.000100	 wd 0.0000	time 0.4935 (0.5456)	loss 0.9326 (1.2075)	grad_norm 3.6816 (nan)	loss_scale 8192.0000 (13146.4431)	mem 14840MB
[2024-07-01 20:27:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:17:08 lr 0.000100	 wd 0.0000	time 0.4769 (0.5406)	loss 1.0410 (1.1817)	grad_norm 3.1460 (nan)	loss_scale 8192.0000 (12322.0765)	mem 14840MB
[2024-07-01 20:28:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:16:08 lr 0.000100	 wd 0.0000	time 0.4866 (0.5373)	loss 0.9097 (1.1597)	grad_norm 2.9444 (nan)	loss_scale 8192.0000 (11732.9073)	mem 14840MB
[2024-07-01 20:29:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:15:10 lr 0.000100	 wd 0.0000	time 0.5202 (0.5349)	loss 0.9951 (1.1440)	grad_norm 3.1456 (nan)	loss_scale 8192.0000 (11290.8464)	mem 14840MB
[2024-07-01 20:30:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:14:14 lr 0.000100	 wd 0.0000	time 0.4851 (0.5331)	loss 1.0459 (1.1305)	grad_norm 3.0212 (nan)	loss_scale 8192.0000 (10946.9123)	mem 14840MB
[2024-07-01 20:31:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:13:19 lr 0.000100	 wd 0.0000	time 0.4590 (0.5320)	loss 1.0225 (1.1185)	grad_norm 2.4065 (nan)	loss_scale 8192.0000 (10671.6963)	mem 14840MB
[2024-07-01 20:32:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:12:24 lr 0.000100	 wd 0.0000	time 0.4870 (0.5307)	loss 0.9648 (1.1086)	grad_norm 3.2314 (nan)	loss_scale 8192.0000 (10446.4741)	mem 14840MB
[2024-07-01 20:33:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:11:32 lr 0.000100	 wd 0.0000	time 0.5078 (0.5318)	loss 0.9946 (1.0999)	grad_norm 3.3419 (nan)	loss_scale 8192.0000 (10258.7577)	mem 14840MB
[2024-07-01 20:33:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:10:38 lr 0.000100	 wd 0.0000	time 0.4770 (0.5309)	loss 0.9307 (1.0933)	grad_norm 3.0114 (nan)	loss_scale 8192.0000 (10099.8985)	mem 14840MB
[2024-07-01 20:34:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:09:44 lr 0.000100	 wd 0.0000	time 0.4626 (0.5300)	loss 0.9888 (1.0868)	grad_norm 2.2270 (nan)	loss_scale 8192.0000 (9963.7173)	mem 14840MB
[2024-07-01 20:35:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:08:50 lr 0.000100	 wd 0.0000	time 0.4759 (0.5299)	loss 1.0049 (1.0816)	grad_norm 3.3267 (nan)	loss_scale 8192.0000 (9845.6815)	mem 14840MB
[2024-07-01 20:36:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:07:57 lr 0.000100	 wd 0.0000	time 0.4891 (0.5294)	loss 1.3096 (1.0762)	grad_norm 2.9399 (nan)	loss_scale 8192.0000 (9742.3910)	mem 14840MB
[2024-07-01 20:37:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:07:04 lr 0.000100	 wd 0.0000	time 0.4875 (0.5288)	loss 0.9263 (1.0713)	grad_norm 2.8772 (nan)	loss_scale 8192.0000 (9651.2451)	mem 14840MB
[2024-07-01 20:38:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:06:10 lr 0.000100	 wd 0.0000	time 0.4849 (0.5283)	loss 0.8838 (1.0662)	grad_norm 3.0961 (nan)	loss_scale 8192.0000 (9570.2210)	mem 14840MB
[2024-07-01 20:39:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:05:18 lr 0.000100	 wd 0.0000	time 0.4896 (0.5294)	loss 1.2080 (1.0622)	grad_norm 2.9050 (nan)	loss_scale 8192.0000 (9497.7212)	mem 14840MB
[2024-07-01 20:40:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:04:25 lr 0.000100	 wd 0.0000	time 0.4776 (0.5288)	loss 1.0586 (1.0588)	grad_norm 2.9045 (nan)	loss_scale 8192.0000 (9432.4678)	mem 14840MB
[2024-07-01 20:40:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:03:32 lr 0.000100	 wd 0.0000	time 0.4922 (0.5289)	loss 0.9360 (1.0543)	grad_norm 2.9738 (nan)	loss_scale 8192.0000 (9373.4260)	mem 14840MB
[2024-07-01 20:41:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:02:39 lr 0.000100	 wd 0.0000	time 0.4904 (0.5286)	loss 0.9517 (1.0508)	grad_norm 3.1631 (nan)	loss_scale 8192.0000 (9319.7492)	mem 14840MB
[2024-07-01 20:42:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:46 lr 0.000100	 wd 0.0000	time 0.5157 (0.5281)	loss 1.0615 (1.0470)	grad_norm 2.6482 (nan)	loss_scale 8192.0000 (9270.7379)	mem 14840MB
[2024-07-01 20:43:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:53 lr 0.000100	 wd 0.0000	time 0.4916 (0.5277)	loss 0.9370 (1.0443)	grad_norm 2.4418 (nan)	loss_scale 8192.0000 (9225.8092)	mem 14840MB
[2024-07-01 20:44:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000100	 wd 0.0000	time 0.4704 (0.5267)	loss 1.2139 (1.0417)	grad_norm 2.5785 (nan)	loss_scale 8192.0000 (9184.4734)	mem 14840MB
[2024-07-01 20:44:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 0 training takes 0:22:00
[2024-07-01 20:44:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 145): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_0.pth saving......
[2024-07-01 20:44:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 147): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_0.pth saved !!!
[2024-07-01 20:44:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 13.100 (13.100)	Loss 0.3809 (0.3809)	Acc@1 91.016 (91.016)	Acc@5 98.633 (98.633)	Mem 14840MB
[2024-07-01 20:44:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 83.426 Acc@5 96.982
[2024-07-01 20:44:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.4%
[2024-07-01 20:44:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 83.43%
[2024-07-01 20:44:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-01 20:44:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-01 20:45:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][0/2502]	eta 9:03:52 lr 0.000100	 wd 0.0000	time 13.0427 (13.0427)	loss 0.8789 (0.8789)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:46:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:25:44 lr 0.000100	 wd 0.0000	time 0.5023 (0.6430)	loss 0.8696 (0.9442)	grad_norm 2.6026 (2.6627)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:46:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:22:50 lr 0.000100	 wd 0.0000	time 0.4968 (0.5955)	loss 0.9058 (0.9541)	grad_norm 2.6033 (2.6385)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:47:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:20:54 lr 0.000100	 wd 0.0000	time 0.4979 (0.5699)	loss 1.1436 (0.9489)	grad_norm 2.8426 (2.6686)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:48:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:19:30 lr 0.000100	 wd 0.0000	time 0.4987 (0.5568)	loss 0.8486 (0.9544)	grad_norm 2.9449 (2.6663)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:49:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:18:20 lr 0.000100	 wd 0.0000	time 0.4812 (0.5495)	loss 0.9731 (0.9530)	grad_norm 2.4765 (2.6575)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:50:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:17:15 lr 0.000100	 wd 0.0000	time 0.4749 (0.5446)	loss 1.0303 (0.9546)	grad_norm 2.6359 (2.6444)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:51:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:16:14 lr 0.000100	 wd 0.0000	time 0.4813 (0.5408)	loss 0.8789 (0.9547)	grad_norm 2.5162 (2.6485)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:52:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:15:18 lr 0.000100	 wd 0.0000	time 0.4901 (0.5399)	loss 0.9727 (0.9551)	grad_norm 2.6258 (2.6464)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:53:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:14:23 lr 0.000099	 wd 0.0000	time 0.4929 (0.5388)	loss 0.9819 (0.9544)	grad_norm 2.3232 (2.6386)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:53:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:13:26 lr 0.000099	 wd 0.0000	time 0.4649 (0.5370)	loss 0.9526 (0.9537)	grad_norm 3.3422 (2.6367)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:54:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:12:30 lr 0.000099	 wd 0.0000	time 0.4776 (0.5354)	loss 1.1934 (0.9539)	grad_norm 2.5198 (2.6364)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:55:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:11:35 lr 0.000099	 wd 0.0000	time 0.4348 (0.5343)	loss 1.0547 (0.9534)	grad_norm 2.5120 (2.6322)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:56:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:10:41 lr 0.000099	 wd 0.0000	time 0.5032 (0.5335)	loss 0.8740 (0.9535)	grad_norm 2.1372 (2.6241)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:57:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:09:46 lr 0.000099	 wd 0.0000	time 0.5214 (0.5324)	loss 0.9390 (0.9527)	grad_norm 2.1114 (2.6132)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:58:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:08:52 lr 0.000099	 wd 0.0000	time 0.4870 (0.5317)	loss 0.9780 (0.9534)	grad_norm 2.3753 (2.6119)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:59:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:07:59 lr 0.000099	 wd 0.0000	time 0.4888 (0.5311)	loss 0.9307 (0.9524)	grad_norm 3.5353 (2.6091)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 20:59:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:07:05 lr 0.000099	 wd 0.0000	time 0.4838 (0.5309)	loss 0.8750 (0.9522)	grad_norm 2.2516 (2.6021)	loss_scale 8192.0000 (8192.0000)	mem 14840MB
[2024-07-01 21:00:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:06:12 lr 0.000099	 wd 0.0000	time 0.5063 (0.5310)	loss 1.0381 (0.9526)	grad_norm 2.5092 (2.6013)	loss_scale 16384.0000 (8237.4858)	mem 14840MB
[2024-07-01 21:01:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:05:20 lr 0.000099	 wd 0.0000	time 0.4923 (0.5318)	loss 1.0098 (0.9532)	grad_norm 2.3312 (2.5953)	loss_scale 16384.0000 (8666.0242)	mem 14840MB
[2024-07-01 21:02:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:04:26 lr 0.000099	 wd 0.0000	time 0.4917 (0.5310)	loss 0.8667 (0.9537)	grad_norm 2.5730 (2.5924)	loss_scale 16384.0000 (9051.7301)	mem 14840MB
[2024-07-01 21:03:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:03:33 lr 0.000099	 wd 0.0000	time 0.4884 (0.5304)	loss 0.8682 (0.9536)	grad_norm 2.3322 (2.5914)	loss_scale 16384.0000 (9400.7197)	mem 14840MB
[2024-07-01 21:04:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:02:40 lr 0.000099	 wd 0.0000	time 0.4909 (0.5300)	loss 0.8555 (0.9533)	grad_norm 2.3863 (2.5888)	loss_scale 16384.0000 (9717.9973)	mem 14840MB
[2024-07-01 21:05:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:46 lr 0.000099	 wd 0.0000	time 0.5232 (0.5295)	loss 0.9819 (0.9536)	grad_norm 2.3903 (2.5887)	loss_scale 16384.0000 (10007.6975)	mem 14840MB
[2024-07-01 21:06:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:53 lr 0.000099	 wd 0.0000	time 0.4882 (0.5291)	loss 1.0459 (0.9544)	grad_norm 2.4090 (2.5872)	loss_scale 16384.0000 (10273.2661)	mem 14840MB
[2024-07-01 21:06:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000099	 wd 0.0000	time 0.4652 (0.5281)	loss 0.9136 (0.9540)	grad_norm 2.3853 (2.5833)	loss_scale 16384.0000 (10517.5978)	mem 14840MB
[2024-07-01 21:06:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 1 training takes 0:22:03
[2024-07-01 21:07:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 13.261 (13.261)	Loss 0.3906 (0.3906)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 14840MB
[2024-07-01 21:07:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 83.880 Acc@5 97.188
[2024-07-01 21:07:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-07-01 21:07:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 83.88%
[2024-07-01 21:07:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-01 21:07:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-01 21:07:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:37:31 lr 0.000099	 wd 0.0000	time 10.9717 (10.9717)	loss 0.7817 (0.7817)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:08:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:25:47 lr 0.000099	 wd 0.0000	time 0.4716 (0.6442)	loss 1.0010 (0.9370)	grad_norm 2.3473 (2.5474)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:09:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:22:15 lr 0.000099	 wd 0.0000	time 0.4836 (0.5802)	loss 0.9116 (0.9294)	grad_norm 2.4765 (2.5581)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:10:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:20:32 lr 0.000099	 wd 0.0000	time 0.4992 (0.5596)	loss 1.0020 (0.9253)	grad_norm 2.5425 (2.5404)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:11:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:19:13 lr 0.000099	 wd 0.0000	time 0.4300 (0.5486)	loss 0.9883 (0.9309)	grad_norm 2.6813 (2.5356)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:12:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:18:06 lr 0.000099	 wd 0.0000	time 0.4918 (0.5427)	loss 0.9487 (0.9329)	grad_norm 2.7707 (2.5382)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:12:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:17:03 lr 0.000099	 wd 0.0000	time 0.4700 (0.5380)	loss 0.8809 (0.9316)	grad_norm 2.4322 (2.5370)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:13:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:16:04 lr 0.000099	 wd 0.0000	time 0.4919 (0.5350)	loss 0.9565 (0.9310)	grad_norm 2.1631 (2.5263)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:14:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:15:06 lr 0.000099	 wd 0.0000	time 0.4844 (0.5328)	loss 0.9751 (0.9300)	grad_norm 2.3621 (2.5157)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:15:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:14:10 lr 0.000098	 wd 0.0000	time 0.4815 (0.5310)	loss 0.9526 (0.9306)	grad_norm 2.4888 (2.5140)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:16:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:13:15 lr 0.000098	 wd 0.0000	time 0.4813 (0.5299)	loss 0.8208 (0.9294)	grad_norm 2.1270 (2.5063)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:17:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:12:22 lr 0.000098	 wd 0.0000	time 0.4949 (0.5298)	loss 0.8965 (0.9288)	grad_norm 2.3116 (2.5025)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:18:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:11:28 lr 0.000098	 wd 0.0000	time 0.4642 (0.5288)	loss 0.9624 (0.9282)	grad_norm 2.8067 (2.4982)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:19:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:10:37 lr 0.000098	 wd 0.0000	time 0.5200 (0.5303)	loss 0.9834 (0.9287)	grad_norm 2.7390 (2.4948)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:19:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:09:43 lr 0.000098	 wd 0.0000	time 0.4909 (0.5293)	loss 0.9419 (0.9295)	grad_norm 2.0298 (2.4905)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:20:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:08:49 lr 0.000098	 wd 0.0000	time 0.4864 (0.5285)	loss 1.0137 (0.9297)	grad_norm 2.1080 (2.4832)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:21:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:07:56 lr 0.000098	 wd 0.0000	time 0.4880 (0.5280)	loss 1.0586 (0.9300)	grad_norm 2.6151 (2.4861)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:22:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:07:03 lr 0.000098	 wd 0.0000	time 0.5116 (0.5275)	loss 1.0547 (0.9307)	grad_norm 2.6894 (2.4915)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:23:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:06:12 lr 0.000098	 wd 0.0000	time 0.4874 (0.5300)	loss 0.7710 (0.9313)	grad_norm 2.2406 (2.4912)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:24:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:05:18 lr 0.000098	 wd 0.0000	time 0.4758 (0.5293)	loss 0.8252 (0.9313)	grad_norm 2.8320 (2.4906)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:25:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:04:25 lr 0.000098	 wd 0.0000	time 0.5175 (0.5288)	loss 0.8174 (0.9307)	grad_norm 2.5224 (2.4922)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:26:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:03:32 lr 0.000098	 wd 0.0000	time 0.5085 (0.5287)	loss 0.9961 (0.9317)	grad_norm 2.5045 (2.4903)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:26:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:02:39 lr 0.000098	 wd 0.0000	time 0.4868 (0.5286)	loss 0.8452 (0.9312)	grad_norm 2.6956 (2.4860)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:27:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:46 lr 0.000098	 wd 0.0000	time 0.4832 (0.5282)	loss 0.8325 (0.9311)	grad_norm 2.4820 (2.4814)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:28:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:53 lr 0.000098	 wd 0.0000	time 0.4839 (0.5278)	loss 0.9302 (0.9309)	grad_norm 2.5694 (2.4805)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:29:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000098	 wd 0.0000	time 0.4659 (0.5268)	loss 0.9946 (0.9306)	grad_norm 2.7633 (2.4801)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:29:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 2 training takes 0:22:00
[2024-07-01 21:29:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 13.915 (13.915)	Loss 0.3777 (0.3777)	Acc@1 91.602 (91.602)	Acc@5 98.828 (98.828)	Mem 14840MB
[2024-07-01 21:29:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.100 Acc@5 97.290
[2024-07-01 21:29:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-01 21:29:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 84.10%
[2024-07-01 21:29:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-01 21:30:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-01 21:30:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:23:05 lr 0.000098	 wd 0.0000	time 12.0647 (12.0647)	loss 0.8242 (0.8242)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:31:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:25:15 lr 0.000098	 wd 0.0000	time 0.4927 (0.6309)	loss 0.8066 (0.8989)	grad_norm 2.2743 (2.4673)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:31:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:22:01 lr 0.000097	 wd 0.0000	time 0.4686 (0.5742)	loss 0.9795 (0.9033)	grad_norm 2.6065 (2.4307)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:32:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:20:21 lr 0.000097	 wd 0.0000	time 0.4783 (0.5548)	loss 0.9185 (0.9070)	grad_norm 3.1616 (2.4555)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:33:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:19:06 lr 0.000097	 wd 0.0000	time 0.4744 (0.5454)	loss 0.9404 (0.9054)	grad_norm 2.3757 (2.5107)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:34:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:18:00 lr 0.000097	 wd 0.0000	time 0.4776 (0.5398)	loss 0.9717 (0.9096)	grad_norm 2.4380 (2.4871)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:35:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:16:59 lr 0.000097	 wd 0.0000	time 0.5004 (0.5358)	loss 0.9204 (0.9141)	grad_norm 2.1970 (2.5059)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:36:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:16:03 lr 0.000097	 wd 0.0000	time 0.4920 (0.5346)	loss 0.9702 (0.9161)	grad_norm 3.4171 (2.4925)	loss_scale 16384.0000 (16384.0000)	mem 14840MB
[2024-07-01 21:37:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:15:06 lr 0.000097	 wd 0.0000	time 0.4693 (0.5325)	loss 0.9536 (0.9148)	grad_norm 2.5135 (2.4941)	loss_scale 32768.0000 (16670.3620)	mem 14840MB
[2024-07-01 21:38:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:14:10 lr 0.000097	 wd 0.0000	time 0.4630 (0.5307)	loss 1.0547 (0.9145)	grad_norm 2.6312 (2.4933)	loss_scale 32768.0000 (18457.0033)	mem 14840MB
[2024-07-01 21:38:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:13:15 lr 0.000097	 wd 0.0000	time 0.4802 (0.5296)	loss 0.9570 (0.9164)	grad_norm 2.2852 (2.4848)	loss_scale 32768.0000 (19886.6733)	mem 14840MB
[2024-07-01 21:39:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:12:20 lr 0.000097	 wd 0.0000	time 0.4722 (0.5285)	loss 0.8047 (0.9172)	grad_norm 2.2742 (2.4842)	loss_scale 32768.0000 (21056.6394)	mem 14840MB
[2024-07-01 21:40:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:11:26 lr 0.000097	 wd 0.0000	time 0.5013 (0.5274)	loss 0.9380 (0.9168)	grad_norm 2.3004 (2.4757)	loss_scale 32768.0000 (22031.7735)	mem 14840MB
[2024-07-01 21:41:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:10:33 lr 0.000097	 wd 0.0000	time 0.4899 (0.5268)	loss 0.8638 (0.9182)	grad_norm 2.3142 (2.4698)	loss_scale 32768.0000 (22857.0023)	mem 14840MB
[2024-07-01 21:42:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:09:40 lr 0.000097	 wd 0.0000	time 0.4862 (0.5270)	loss 0.9175 (0.9183)	grad_norm 2.1972 (2.4688)	loss_scale 32768.0000 (23564.4254)	mem 14840MB
[2024-07-01 21:43:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:08:47 lr 0.000097	 wd 0.0000	time 0.5000 (0.5264)	loss 0.8154 (0.9186)	grad_norm 1.9277 (inf)	loss_scale 16384.0000 (23697.3111)	mem 14840MB
[2024-07-01 21:44:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:07:54 lr 0.000096	 wd 0.0000	time 0.4885 (0.5260)	loss 1.0156 (0.9173)	grad_norm 2.6687 (inf)	loss_scale 16384.0000 (23240.5147)	mem 14840MB
[2024-07-01 21:44:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:07:01 lr 0.000096	 wd 0.0000	time 0.4836 (0.5257)	loss 0.8037 (0.9175)	grad_norm 2.3796 (inf)	loss_scale 16384.0000 (22837.4274)	mem 14840MB
[2024-07-01 21:45:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:06:08 lr 0.000096	 wd 0.0000	time 0.4803 (0.5253)	loss 1.0205 (0.9175)	grad_norm 2.9480 (inf)	loss_scale 16384.0000 (22479.1027)	mem 14840MB
[2024-07-01 21:46:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:05:16 lr 0.000096	 wd 0.0000	time 0.4864 (0.5249)	loss 0.8271 (0.9169)	grad_norm 2.2860 (inf)	loss_scale 16384.0000 (22158.4766)	mem 14840MB
[2024-07-01 21:47:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:04:23 lr 0.000096	 wd 0.0000	time 0.5070 (0.5246)	loss 0.8545 (0.9167)	grad_norm 2.1201 (inf)	loss_scale 16384.0000 (21869.8971)	mem 14840MB
[2024-07-01 21:48:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:03:30 lr 0.000096	 wd 0.0000	time 0.4818 (0.5243)	loss 0.8140 (0.9168)	grad_norm 2.4229 (inf)	loss_scale 16384.0000 (21608.7882)	mem 14840MB
[2024-07-01 21:49:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:38 lr 0.000096	 wd 0.0000	time 0.5011 (0.5241)	loss 0.8906 (0.9165)	grad_norm 2.4115 (inf)	loss_scale 16384.0000 (21371.4057)	mem 14840MB
[2024-07-01 21:50:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:45 lr 0.000096	 wd 0.0000	time 0.4972 (0.5239)	loss 1.0225 (0.9169)	grad_norm 2.2608 (inf)	loss_scale 16384.0000 (21154.6562)	mem 14840MB
[2024-07-02 08:45:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/config.json
[2024-07-02 08:45:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: vcnu_convnext
  VCNU_CONVNEXT:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: vcnu_convnext_base_patch4_22kto1k_finetune_nonorm
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-02 08:45:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_convnext/vcnu_convnext_base_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weight/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "vcnu_convnext_base_patch4_22kto1k_finetune_nonorm", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-02 08:45:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 108): INFO Creating model:vcnu_convnext/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm
[2024-07-02 08:45:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 110): INFO VCNU_ConvNeXt(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
        (t2d_memory_attention): Memory(
          dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=128, bias=True)
          (attn): Linear(in_features=256, out_features=128, bias=True)
          (update_ltm): Linear(in_features=128, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=128, bias=True)
          (attn): Linear(in_features=256, out_features=128, bias=True)
          (update_ltm): Linear(in_features=128, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=128, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=128, bias=True)
          (attn): Linear(in_features=256, out_features=128, bias=True)
          (update_ltm): Linear(in_features=128, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=256, bias=True)
          (attn): Linear(in_features=512, out_features=256, bias=True)
          (update_ltm): Linear(in_features=256, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=256, bias=True)
          (attn): Linear(in_features=512, out_features=256, bias=True)
          (update_ltm): Linear(in_features=256, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=256, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=256, bias=True)
          (attn): Linear(in_features=512, out_features=256, bias=True)
          (update_ltm): Linear(in_features=256, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=512, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=512, bias=True)
          (attn): Linear(in_features=1024, out_features=512, bias=True)
          (update_ltm): Linear(in_features=512, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=1024, bias=True)
          (attn): Linear(in_features=2048, out_features=1024, bias=True)
          (update_ltm): Linear(in_features=1024, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=1024, bias=True)
          (attn): Linear(in_features=2048, out_features=1024, bias=True)
          (update_ltm): Linear(in_features=1024, out_features=30, bias=True)
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
        (t2d_memory_attention): Memory(
          dim=1024, memory_dim=30, model_style=trans, ab_norm_attn_=False, ab_norm_ltm_=False,
          (activation): GELU()
          (wm): Linear(in_features=30, out_features=1024, bias=True)
          (attn): Linear(in_features=2048, out_features=1024, bias=True)
          (update_ltm): Identity()
          (norm_attn): Identity()
          (norm_ltm): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (memory_downsampling): ModuleList(
    (0): MemoryDownSampling(
      (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (1): MemoryDownSampling(
      (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (2): MemoryDownSampling(
      (memory_pooling): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-02 08:45:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 113): INFO number of params: 110619522
[2024-07-02 08:45:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 148): INFO auto resuming from pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth
[2024-07-02 08:45:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 19): INFO ==============> Resuming form pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth....................
[2024-07-02 08:45:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 26): INFO <All keys matched successfully>
[2024-07-02 08:45:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 36): INFO => loaded successfully 'pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth' (epoch 2)
[2024-07-02 08:45:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 18.108 (18.108)	Loss 0.3777 (0.3777)	Acc@1 91.602 (91.602)	Acc@5 98.828 (98.828)	Mem 4528MB
[2024-07-02 08:45:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.098 Acc@5 97.290
[2024-07-02 08:45:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 155): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-02 08:45:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 168): INFO Start training
[2024-07-02 08:46:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][0/2502]	eta 14:42:28 lr 0.000098	 wd 0.0000	time 21.1627 (21.1627)	loss 0.9067 (0.9067)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:47:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:27:05 lr 0.000098	 wd 0.0000	time 0.4125 (0.6769)	loss 0.9160 (0.9046)	grad_norm 2.0359 (2.5253)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:47:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:21:27 lr 0.000097	 wd 0.0000	time 0.4096 (0.5595)	loss 0.9556 (0.9055)	grad_norm 2.0527 (2.5072)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:48:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:19:05 lr 0.000097	 wd 0.0000	time 0.4107 (0.5203)	loss 1.0625 (0.9063)	grad_norm 2.5695 (2.5061)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:49:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:17:36 lr 0.000097	 wd 0.0000	time 0.4059 (0.5025)	loss 0.9570 (0.9087)	grad_norm 2.2045 (2.4830)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:50:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:16:21 lr 0.000097	 wd 0.0000	time 0.3922 (0.4900)	loss 0.8408 (0.9104)	grad_norm 2.2957 (2.4648)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:50:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:15:14 lr 0.000097	 wd 0.0000	time 0.4228 (0.4810)	loss 0.8628 (0.9121)	grad_norm 2.2792 (2.4550)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:51:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:14:14 lr 0.000097	 wd 0.0000	time 0.4057 (0.4743)	loss 0.9365 (0.9143)	grad_norm 3.5803 (2.4652)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 08:52:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:13:20 lr 0.000097	 wd 0.0000	time 0.4504 (0.4701)	loss 0.9004 (0.9148)	grad_norm 2.8950 (2.4683)	loss_scale 32768.0000 (16670.3620)	mem 15686MB
[2024-07-02 08:52:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:12:26 lr 0.000097	 wd 0.0000	time 0.3502 (0.4658)	loss 1.1064 (0.9146)	grad_norm 2.3587 (2.4578)	loss_scale 32768.0000 (18457.0033)	mem 15686MB
[2024-07-02 08:53:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:11:34 lr 0.000097	 wd 0.0000	time 0.3823 (0.4625)	loss 0.9189 (0.9154)	grad_norm 2.5132 (inf)	loss_scale 16384.0000 (18773.6743)	mem 15686MB
[2024-07-02 08:54:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:10:44 lr 0.000097	 wd 0.0000	time 0.3757 (0.4599)	loss 0.7422 (0.9162)	grad_norm 2.4957 (inf)	loss_scale 16384.0000 (18556.6285)	mem 15686MB
[2024-07-02 08:55:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:09:56 lr 0.000097	 wd 0.0000	time 0.4076 (0.4580)	loss 1.0449 (0.9165)	grad_norm 1.9609 (inf)	loss_scale 16384.0000 (18375.7269)	mem 15686MB
[2024-07-02 08:55:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:09:08 lr 0.000097	 wd 0.0000	time 0.3949 (0.4559)	loss 0.9639 (0.9170)	grad_norm 2.1559 (inf)	loss_scale 16384.0000 (18222.6349)	mem 15686MB
[2024-07-02 08:56:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:08:20 lr 0.000097	 wd 0.0000	time 0.3748 (0.4544)	loss 1.0078 (0.9176)	grad_norm 2.6744 (inf)	loss_scale 16384.0000 (18091.3976)	mem 15686MB
[2024-07-02 08:57:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:07:33 lr 0.000097	 wd 0.0000	time 0.4380 (0.4529)	loss 0.7554 (0.9179)	grad_norm 2.0319 (inf)	loss_scale 16384.0000 (17977.6469)	mem 15686MB
[2024-07-02 08:58:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:06:47 lr 0.000096	 wd 0.0000	time 0.3900 (0.4520)	loss 0.9707 (0.9173)	grad_norm 2.5410 (inf)	loss_scale 16384.0000 (17878.1062)	mem 15686MB
[2024-07-02 08:58:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:06:01 lr 0.000096	 wd 0.0000	time 0.4217 (0.4509)	loss 0.9497 (0.9178)	grad_norm 2.6981 (inf)	loss_scale 16384.0000 (17790.2693)	mem 15686MB
[2024-07-02 08:59:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:05:15 lr 0.000096	 wd 0.0000	time 0.4260 (0.4499)	loss 0.9443 (0.9183)	grad_norm 2.4582 (inf)	loss_scale 16384.0000 (17712.1866)	mem 15686MB
[2024-07-02 09:00:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:04:30 lr 0.000096	 wd 0.0000	time 0.4458 (0.4491)	loss 0.9697 (0.9182)	grad_norm 2.3592 (inf)	loss_scale 16384.0000 (17642.3188)	mem 15686MB
[2024-07-02 09:00:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:03:45 lr 0.000096	 wd 0.0000	time 0.4003 (0.4483)	loss 0.9102 (0.9180)	grad_norm 1.9397 (inf)	loss_scale 16384.0000 (17579.4343)	mem 15686MB
[2024-07-02 09:01:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:59 lr 0.000096	 wd 0.0000	time 0.3876 (0.4475)	loss 0.8345 (0.9175)	grad_norm 2.2894 (inf)	loss_scale 16384.0000 (17522.5359)	mem 15686MB
[2024-07-02 09:02:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:14 lr 0.000096	 wd 0.0000	time 0.3988 (0.4467)	loss 0.8940 (0.9173)	grad_norm 2.3911 (inf)	loss_scale 16384.0000 (17470.8078)	mem 15686MB
[2024-07-02 09:03:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:30 lr 0.000096	 wd 0.0000	time 0.4192 (0.4462)	loss 0.9844 (0.9173)	grad_norm 2.5280 (inf)	loss_scale 16384.0000 (17423.5758)	mem 15686MB
[2024-07-02 09:03:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:45 lr 0.000096	 wd 0.0000	time 0.3889 (0.4457)	loss 0.9180 (0.9171)	grad_norm 2.0548 (inf)	loss_scale 16384.0000 (17380.2782)	mem 15686MB
[2024-07-02 09:04:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000096	 wd 0.0000	time 0.3833 (0.4447)	loss 0.8354 (0.9173)	grad_norm 2.5707 (inf)	loss_scale 16384.0000 (17340.4430)	mem 15686MB
[2024-07-02 09:04:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 3 training takes 0:18:34
[2024-07-02 09:04:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.757 (12.757)	Loss 0.3855 (0.3855)	Acc@1 91.406 (91.406)	Acc@5 98.828 (98.828)	Mem 15686MB
[2024-07-02 09:04:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.436 Acc@5 97.356
[2024-07-02 09:04:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-07-02 09:04:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 84.44%
[2024-07-02 09:04:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 09:05:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 09:05:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][0/2502]	eta 5:57:08 lr 0.000096	 wd 0.0000	time 8.5644 (8.5644)	loss 0.8647 (0.8647)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:05:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:20:33 lr 0.000096	 wd 0.0000	time 0.3596 (0.5137)	loss 0.8643 (0.9060)	grad_norm 2.2201 (2.4798)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:06:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:18:04 lr 0.000096	 wd 0.0000	time 0.3815 (0.4712)	loss 1.0635 (0.9128)	grad_norm 2.1784 (2.4877)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:07:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:16:47 lr 0.000095	 wd 0.0000	time 0.3704 (0.4576)	loss 0.7964 (0.9060)	grad_norm 2.2979 (2.4282)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:08:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:15:49 lr 0.000095	 wd 0.0000	time 0.4140 (0.4515)	loss 0.8271 (0.9043)	grad_norm 2.3308 (2.4432)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:08:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:14:54 lr 0.000095	 wd 0.0000	time 0.3900 (0.4467)	loss 0.9639 (0.9056)	grad_norm 3.0573 (2.4340)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:09:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:14:03 lr 0.000095	 wd 0.0000	time 0.4134 (0.4437)	loss 0.8013 (0.9069)	grad_norm 2.8740 (2.4460)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:10:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:13:15 lr 0.000095	 wd 0.0000	time 0.3458 (0.4414)	loss 1.0908 (0.9052)	grad_norm 2.0988 (2.4402)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:10:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:12:29 lr 0.000095	 wd 0.0000	time 0.3832 (0.4403)	loss 0.7764 (0.9038)	grad_norm 2.4184 (2.4314)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:11:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:11:43 lr 0.000095	 wd 0.0000	time 0.4071 (0.4392)	loss 1.1133 (0.9043)	grad_norm 2.2593 (2.4345)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:12:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:10:57 lr 0.000095	 wd 0.0000	time 0.3965 (0.4380)	loss 0.9224 (0.9050)	grad_norm 2.0906 (2.4330)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:13:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:10:13 lr 0.000095	 wd 0.0000	time 0.4354 (0.4375)	loss 1.0557 (0.9052)	grad_norm 2.3581 (2.4252)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:13:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:09:29 lr 0.000095	 wd 0.0000	time 0.3918 (0.4371)	loss 0.8428 (0.9053)	grad_norm 2.1783 (2.4222)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:14:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:08:44 lr 0.000095	 wd 0.0000	time 0.3908 (0.4365)	loss 0.8735 (0.9044)	grad_norm 1.9510 (2.4182)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:15:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:08:00 lr 0.000094	 wd 0.0000	time 0.3857 (0.4360)	loss 0.8184 (0.9037)	grad_norm 2.1319 (2.4148)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:15:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:07:16 lr 0.000094	 wd 0.0000	time 0.4004 (0.4355)	loss 0.8770 (0.9036)	grad_norm 2.6372 (2.4120)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:16:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:06:32 lr 0.000094	 wd 0.0000	time 0.4192 (0.4352)	loss 0.9512 (0.9036)	grad_norm 2.6124 (2.4140)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:17:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:05:48 lr 0.000094	 wd 0.0000	time 0.3883 (0.4350)	loss 1.0146 (0.9041)	grad_norm 2.1708 (2.4157)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:18:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:05:05 lr 0.000094	 wd 0.0000	time 0.3930 (0.4347)	loss 1.0537 (0.9048)	grad_norm 2.4183 (2.4183)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:18:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:04:21 lr 0.000094	 wd 0.0000	time 0.3914 (0.4346)	loss 0.9336 (0.9046)	grad_norm 2.3441 (2.4129)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:19:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:03:38 lr 0.000094	 wd 0.0000	time 0.4041 (0.4344)	loss 0.8926 (0.9038)	grad_norm 1.9993 (2.4093)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:20:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:54 lr 0.000094	 wd 0.0000	time 0.4021 (0.4342)	loss 0.7783 (0.9041)	grad_norm 2.2477 (2.4081)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:20:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:02:11 lr 0.000094	 wd 0.0000	time 0.4219 (0.4340)	loss 0.8125 (0.9037)	grad_norm 1.9983 (2.4059)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:21:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:27 lr 0.000094	 wd 0.0000	time 0.3996 (0.4339)	loss 0.9014 (0.9042)	grad_norm 2.3796 (2.3987)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:22:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:44 lr 0.000093	 wd 0.0000	time 0.4095 (0.4338)	loss 0.8433 (0.9048)	grad_norm 2.3735 (2.3951)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:23:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000093	 wd 0.0000	time 0.3900 (0.4332)	loss 0.8672 (0.9047)	grad_norm 2.2485 (2.3945)	loss_scale 32768.0000 (16842.5686)	mem 15686MB
[2024-07-02 09:23:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 4 training takes 0:18:06
[2024-07-02 09:23:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 13.164 (13.164)	Loss 0.3706 (0.3706)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 15686MB
[2024-07-02 09:23:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.640 Acc@5 97.364
[2024-07-02 09:23:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-02 09:23:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 84.64%
[2024-07-02 09:23:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 09:23:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 09:23:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][0/2502]	eta 5:53:18 lr 0.000093	 wd 0.0000	time 8.4726 (8.4726)	loss 0.9404 (0.9404)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 15686MB
[2024-07-02 09:24:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:20:22 lr 0.000093	 wd 0.0000	time 0.4010 (0.5089)	loss 0.8008 (0.9065)	grad_norm 2.7968 (2.3443)	loss_scale 32768.0000 (32768.0000)	mem 15686MB
[2024-07-02 09:25:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:18:00 lr 0.000093	 wd 0.0000	time 0.3988 (0.4694)	loss 0.8145 (0.9015)	grad_norm 2.0884 (2.3519)	loss_scale 32768.0000 (32768.0000)	mem 15686MB
[2024-07-02 09:25:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:16:41 lr 0.000093	 wd 0.0000	time 0.4075 (0.4550)	loss 1.0010 (0.8967)	grad_norm 2.3303 (2.3354)	loss_scale 32768.0000 (32768.0000)	mem 15686MB
[2024-07-02 09:26:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:15:43 lr 0.000093	 wd 0.0000	time 0.3357 (0.4487)	loss 0.9014 (0.8941)	grad_norm 2.3420 (inf)	loss_scale 16384.0000 (29744.5187)	mem 15686MB
[2024-07-02 09:27:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:14:50 lr 0.000093	 wd 0.0000	time 0.4005 (0.4449)	loss 1.0537 (0.8928)	grad_norm 1.9698 (inf)	loss_scale 16384.0000 (27077.7485)	mem 15686MB
[2024-07-02 09:28:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:14:00 lr 0.000093	 wd 0.0000	time 0.3907 (0.4418)	loss 0.9326 (0.8948)	grad_norm 2.1564 (inf)	loss_scale 16384.0000 (25298.4226)	mem 15686MB
[2024-07-02 09:28:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:13:12 lr 0.000093	 wd 0.0000	time 0.3963 (0.4398)	loss 0.8032 (0.8938)	grad_norm 2.3623 (inf)	loss_scale 16384.0000 (24026.7504)	mem 15686MB
[2024-07-02 09:29:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:12:26 lr 0.000093	 wd 0.0000	time 0.3549 (0.4388)	loss 0.9194 (0.8935)	grad_norm 2.0040 (inf)	loss_scale 16384.0000 (23072.5993)	mem 15686MB
[2024-07-02 09:30:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:11:41 lr 0.000092	 wd 0.0000	time 0.3930 (0.4381)	loss 0.9150 (0.8944)	grad_norm 2.4850 (inf)	loss_scale 16384.0000 (22330.2464)	mem 15686MB
[2024-07-02 09:30:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:10:56 lr 0.000092	 wd 0.0000	time 0.4244 (0.4373)	loss 0.8081 (0.8945)	grad_norm 2.2301 (inf)	loss_scale 16384.0000 (21736.2158)	mem 15686MB
[2024-07-02 09:31:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:10:11 lr 0.000092	 wd 0.0000	time 0.4030 (0.4364)	loss 0.9888 (0.8962)	grad_norm 2.4655 (inf)	loss_scale 16384.0000 (21250.0926)	mem 15686MB
[2024-07-02 09:32:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:09:27 lr 0.000092	 wd 0.0000	time 0.3978 (0.4362)	loss 0.9116 (0.8966)	grad_norm 2.0136 (inf)	loss_scale 16384.0000 (20844.9226)	mem 15686MB
[2024-07-02 09:33:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:08:43 lr 0.000092	 wd 0.0000	time 0.4364 (0.4356)	loss 1.0137 (0.8960)	grad_norm 2.3396 (inf)	loss_scale 16384.0000 (20502.0384)	mem 15686MB
[2024-07-02 09:33:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:07:59 lr 0.000092	 wd 0.0000	time 0.4100 (0.4352)	loss 0.8613 (0.8959)	grad_norm 2.8006 (inf)	loss_scale 16384.0000 (20208.1028)	mem 15686MB
[2024-07-02 09:34:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:07:15 lr 0.000092	 wd 0.0000	time 0.4274 (0.4348)	loss 0.8442 (0.8953)	grad_norm 2.6432 (inf)	loss_scale 16384.0000 (19953.3324)	mem 15686MB
[2024-07-02 09:35:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:06:32 lr 0.000092	 wd 0.0000	time 0.3532 (0.4348)	loss 0.8696 (0.8958)	grad_norm 2.7619 (inf)	loss_scale 16384.0000 (19730.3885)	mem 15686MB
[2024-07-02 09:35:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:05:48 lr 0.000092	 wd 0.0000	time 0.4164 (0.4346)	loss 0.9551 (0.8965)	grad_norm 2.3561 (inf)	loss_scale 16384.0000 (19533.6578)	mem 15686MB
[2024-07-02 09:36:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:05:04 lr 0.000091	 wd 0.0000	time 0.3913 (0.4344)	loss 1.0352 (0.8965)	grad_norm 2.3287 (inf)	loss_scale 16384.0000 (19358.7740)	mem 15686MB
[2024-07-02 09:37:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:04:21 lr 0.000091	 wd 0.0000	time 0.4128 (0.4342)	loss 0.8730 (0.8966)	grad_norm 2.7326 (inf)	loss_scale 16384.0000 (19202.2893)	mem 15686MB
[2024-07-02 09:38:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:03:37 lr 0.000091	 wd 0.0000	time 0.4002 (0.4340)	loss 0.8965 (0.8969)	grad_norm 2.1177 (inf)	loss_scale 16384.0000 (19061.4453)	mem 15686MB
[2024-07-02 09:38:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:54 lr 0.000091	 wd 0.0000	time 0.3774 (0.4339)	loss 0.9351 (0.8968)	grad_norm 2.1300 (inf)	loss_scale 16384.0000 (18934.0086)	mem 15686MB
[2024-07-02 09:39:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:02:10 lr 0.000091	 wd 0.0000	time 0.3980 (0.4337)	loss 0.8726 (0.8974)	grad_norm 2.3660 (inf)	loss_scale 16384.0000 (18818.1517)	mem 15686MB
[2024-07-02 09:40:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:27 lr 0.000091	 wd 0.0000	time 0.3875 (0.4337)	loss 0.7207 (0.8976)	grad_norm 2.3031 (inf)	loss_scale 16384.0000 (18712.3651)	mem 15686MB
[2024-07-02 09:41:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:44 lr 0.000091	 wd 0.0000	time 0.4349 (0.4336)	loss 0.8335 (0.8969)	grad_norm 2.1542 (inf)	loss_scale 16384.0000 (18615.3903)	mem 15686MB
[2024-07-02 09:41:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000091	 wd 0.0000	time 0.3899 (0.4331)	loss 0.8853 (0.8966)	grad_norm 2.2668 (inf)	loss_scale 16384.0000 (18526.1703)	mem 15686MB
[2024-07-02 09:41:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 5 training takes 0:18:06
[2024-07-02 09:41:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.741 (12.741)	Loss 0.3735 (0.3735)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 15686MB
[2024-07-02 09:42:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.708 Acc@5 97.370
[2024-07-02 09:42:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-02 09:42:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 84.71%
[2024-07-02 09:42:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 09:42:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 09:42:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][0/2502]	eta 6:24:52 lr 0.000091	 wd 0.0000	time 9.2296 (9.2296)	loss 0.9434 (0.9434)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:43:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:20:35 lr 0.000090	 wd 0.0000	time 0.3946 (0.5144)	loss 0.8394 (0.8738)	grad_norm 2.2289 (2.3465)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:43:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:18:05 lr 0.000090	 wd 0.0000	time 0.4016 (0.4714)	loss 0.8594 (0.8797)	grad_norm 2.2789 (2.3317)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:44:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:16:46 lr 0.000090	 wd 0.0000	time 0.3958 (0.4569)	loss 0.8496 (0.8836)	grad_norm 2.3803 (2.3288)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:45:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:15:46 lr 0.000090	 wd 0.0000	time 0.4316 (0.4503)	loss 0.8730 (0.8835)	grad_norm 2.4729 (2.3170)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:45:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:14:52 lr 0.000090	 wd 0.0000	time 0.4190 (0.4459)	loss 0.9004 (0.8846)	grad_norm 2.4567 (2.3171)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:46:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:14:02 lr 0.000090	 wd 0.0000	time 0.4297 (0.4432)	loss 0.8208 (0.8868)	grad_norm 2.2245 (2.3161)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:47:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:13:14 lr 0.000090	 wd 0.0000	time 0.3871 (0.4410)	loss 0.8672 (0.8884)	grad_norm 2.0817 (2.3171)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:48:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:12:28 lr 0.000090	 wd 0.0000	time 0.3874 (0.4398)	loss 0.6890 (0.8883)	grad_norm 2.1380 (2.3309)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:48:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:11:42 lr 0.000089	 wd 0.0000	time 0.3986 (0.4387)	loss 0.8193 (0.8891)	grad_norm 1.9743 (2.3283)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:49:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:10:57 lr 0.000089	 wd 0.0000	time 0.3825 (0.4379)	loss 0.7510 (0.8897)	grad_norm 2.1096 (2.3250)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:50:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:10:12 lr 0.000089	 wd 0.0000	time 0.3972 (0.4371)	loss 0.7539 (0.8893)	grad_norm 3.0077 (2.3171)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:50:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:09:28 lr 0.000089	 wd 0.0000	time 0.3842 (0.4366)	loss 0.7266 (0.8889)	grad_norm 3.0071 (2.3179)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:51:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:08:44 lr 0.000089	 wd 0.0000	time 0.3870 (0.4361)	loss 0.8125 (0.8882)	grad_norm 2.6443 (2.3148)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:52:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:08:00 lr 0.000089	 wd 0.0000	time 0.3980 (0.4360)	loss 0.8848 (0.8880)	grad_norm 2.2322 (2.3145)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:53:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:07:16 lr 0.000089	 wd 0.0000	time 0.3936 (0.4355)	loss 0.7383 (0.8885)	grad_norm 2.0474 (2.3173)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:53:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:06:32 lr 0.000089	 wd 0.0000	time 0.3947 (0.4355)	loss 0.8232 (0.8881)	grad_norm 2.2929 (2.3166)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:54:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:05:49 lr 0.000088	 wd 0.0000	time 0.3795 (0.4352)	loss 0.7949 (0.8875)	grad_norm 2.4777 (2.3164)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:55:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:05:05 lr 0.000088	 wd 0.0000	time 0.3918 (0.4350)	loss 0.7812 (0.8871)	grad_norm 2.1417 (2.3190)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 09:56:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:04:21 lr 0.000088	 wd 0.0000	time 0.3805 (0.4348)	loss 0.8750 (0.8867)	grad_norm 2.0543 (inf)	loss_scale 16384.0000 (16849.4056)	mem 15686MB
[2024-07-02 09:56:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:03:38 lr 0.000088	 wd 0.0000	time 0.3893 (0.4346)	loss 0.9502 (0.8867)	grad_norm 2.0335 (inf)	loss_scale 16384.0000 (16826.1469)	mem 15686MB
[2024-07-02 09:57:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:54 lr 0.000088	 wd 0.0000	time 0.4002 (0.4344)	loss 0.8096 (0.8866)	grad_norm 2.0795 (inf)	loss_scale 16384.0000 (16805.1023)	mem 15686MB
[2024-07-02 09:58:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:02:11 lr 0.000088	 wd 0.0000	time 0.3934 (0.4342)	loss 0.8628 (0.8868)	grad_norm 2.1286 (inf)	loss_scale 16384.0000 (16785.9700)	mem 15686MB
[2024-07-02 09:58:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:27 lr 0.000088	 wd 0.0000	time 0.4244 (0.4342)	loss 0.8203 (0.8868)	grad_norm 2.3274 (inf)	loss_scale 16384.0000 (16768.5007)	mem 15686MB
[2024-07-02 09:59:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:44 lr 0.000087	 wd 0.0000	time 0.3958 (0.4340)	loss 0.8247 (0.8871)	grad_norm 2.0178 (inf)	loss_scale 16384.0000 (16752.4865)	mem 15686MB
[2024-07-02 10:00:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000087	 wd 0.0000	time 0.3810 (0.4335)	loss 0.9482 (0.8876)	grad_norm 2.1095 (inf)	loss_scale 16384.0000 (16737.7529)	mem 15686MB
[2024-07-02 10:00:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 6 training takes 0:18:06
[2024-07-02 10:00:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.131 (12.131)	Loss 0.3967 (0.3967)	Acc@1 91.211 (91.211)	Acc@5 98.438 (98.438)	Mem 15686MB
[2024-07-02 10:00:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.712 Acc@5 97.424
[2024-07-02 10:00:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-02 10:00:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 84.71%
[2024-07-02 10:00:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 10:00:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 10:00:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][0/2502]	eta 6:13:50 lr 0.000087	 wd 0.0000	time 8.9650 (8.9650)	loss 1.0342 (1.0342)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:01:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:20:29 lr 0.000087	 wd 0.0000	time 0.3867 (0.5120)	loss 0.9155 (0.8723)	grad_norm 2.1895 (2.2814)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:02:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:18:02 lr 0.000087	 wd 0.0000	time 0.3950 (0.4704)	loss 0.9331 (0.8669)	grad_norm 2.4363 (2.2840)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:03:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:16:45 lr 0.000087	 wd 0.0000	time 0.4042 (0.4565)	loss 0.8403 (0.8663)	grad_norm 2.1567 (2.2883)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:03:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:15:44 lr 0.000087	 wd 0.0000	time 0.3986 (0.4495)	loss 0.8872 (0.8705)	grad_norm 2.2922 (2.3011)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:04:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:14:51 lr 0.000087	 wd 0.0000	time 0.4095 (0.4455)	loss 0.7544 (0.8684)	grad_norm 1.9824 (2.2992)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:05:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:14:02 lr 0.000086	 wd 0.0000	time 0.4092 (0.4430)	loss 0.7993 (0.8694)	grad_norm 2.2404 (2.3079)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:06:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:13:14 lr 0.000086	 wd 0.0000	time 0.3940 (0.4409)	loss 0.8789 (0.8689)	grad_norm 2.6408 (2.3284)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:06:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:12:28 lr 0.000086	 wd 0.0000	time 0.3862 (0.4395)	loss 0.8545 (0.8688)	grad_norm 1.8676 (2.3314)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:07:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:11:42 lr 0.000086	 wd 0.0000	time 0.3978 (0.4384)	loss 0.8096 (0.8697)	grad_norm 2.1071 (2.3281)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:08:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:10:57 lr 0.000086	 wd 0.0000	time 0.4124 (0.4377)	loss 0.9634 (0.8698)	grad_norm 2.1465 (2.3306)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:08:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:10:12 lr 0.000086	 wd 0.0000	time 0.3980 (0.4372)	loss 0.7666 (0.8706)	grad_norm 2.2783 (2.3330)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:09:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:09:28 lr 0.000086	 wd 0.0000	time 0.3968 (0.4367)	loss 0.9126 (0.8704)	grad_norm 2.5528 (2.3355)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:10:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:08:44 lr 0.000085	 wd 0.0000	time 0.4022 (0.4361)	loss 0.9526 (0.8703)	grad_norm 2.0886 (2.3352)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:11:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:08:00 lr 0.000085	 wd 0.0000	time 0.3920 (0.4357)	loss 0.7065 (0.8712)	grad_norm 2.0359 (2.3320)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:11:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:07:16 lr 0.000085	 wd 0.0000	time 0.3937 (0.4354)	loss 0.8662 (0.8721)	grad_norm 2.7583 (2.3337)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:12:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:06:32 lr 0.000085	 wd 0.0000	time 0.4097 (0.4353)	loss 0.8887 (0.8724)	grad_norm 2.5428 (2.3318)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:13:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:05:49 lr 0.000085	 wd 0.0000	time 0.3980 (0.4352)	loss 0.9678 (0.8728)	grad_norm 2.3444 (2.3355)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:13:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:05:05 lr 0.000085	 wd 0.0000	time 0.4062 (0.4348)	loss 0.8589 (0.8721)	grad_norm 2.2788 (2.3334)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:14:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:04:21 lr 0.000085	 wd 0.0000	time 0.3972 (0.4346)	loss 0.8945 (0.8731)	grad_norm 3.0790 (2.3379)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 10:15:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:03:38 lr 0.000084	 wd 0.0000	time 0.3770 (0.4347)	loss 0.8369 (0.8733)	grad_norm 3.1088 (inf)	loss_scale 8192.0000 (16154.7386)	mem 15686MB
[2024-07-02 10:16:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:54 lr 0.000084	 wd 0.0000	time 0.3891 (0.4345)	loss 1.0322 (0.8741)	grad_norm 2.5010 (inf)	loss_scale 8192.0000 (15775.7411)	mem 15686MB
[2024-07-02 10:16:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:02:11 lr 0.000084	 wd 0.0000	time 0.3945 (0.4342)	loss 0.9224 (0.8744)	grad_norm 3.0714 (inf)	loss_scale 8192.0000 (15431.1822)	mem 15686MB
[2024-07-02 10:17:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:27 lr 0.000084	 wd 0.0000	time 0.3954 (0.4341)	loss 0.9873 (0.8745)	grad_norm 2.0915 (inf)	loss_scale 8192.0000 (15116.5719)	mem 15686MB
[2024-07-02 10:18:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:44 lr 0.000084	 wd 0.0000	time 0.3964 (0.4340)	loss 0.8784 (0.8746)	grad_norm 2.5036 (inf)	loss_scale 8192.0000 (14828.1683)	mem 15686MB
[2024-07-02 10:18:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000084	 wd 0.0000	time 0.3799 (0.4335)	loss 0.8926 (0.8748)	grad_norm 2.2680 (inf)	loss_scale 8192.0000 (14562.8277)	mem 15686MB
[2024-07-02 10:18:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 7 training takes 0:18:06
[2024-07-02 10:19:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.293 (12.293)	Loss 0.3569 (0.3569)	Acc@1 91.406 (91.406)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 10:19:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.740 Acc@5 97.438
[2024-07-02 10:19:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-02 10:19:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 84.74%
[2024-07-02 10:19:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 10:19:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 10:19:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][0/2502]	eta 6:09:37 lr 0.000084	 wd 0.0000	time 8.8640 (8.8640)	loss 0.8823 (0.8823)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:20:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:20:30 lr 0.000083	 wd 0.0000	time 0.3866 (0.5122)	loss 0.8867 (0.8639)	grad_norm 2.3976 (2.2998)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:21:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:18:02 lr 0.000083	 wd 0.0000	time 0.3898 (0.4701)	loss 0.9316 (0.8656)	grad_norm 2.6403 (2.3454)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:21:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:16:44 lr 0.000083	 wd 0.0000	time 0.3837 (0.4561)	loss 0.9497 (0.8611)	grad_norm 3.0765 (2.3424)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:22:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:15:44 lr 0.000083	 wd 0.0000	time 0.3955 (0.4493)	loss 0.9287 (0.8656)	grad_norm 2.2452 (2.3488)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:23:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:14:50 lr 0.000083	 wd 0.0000	time 0.4009 (0.4448)	loss 0.8540 (0.8672)	grad_norm 2.3016 (2.3680)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:23:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:14:01 lr 0.000083	 wd 0.0000	time 0.3731 (0.4423)	loss 0.8721 (0.8696)	grad_norm 2.6392 (2.3497)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:24:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:13:14 lr 0.000083	 wd 0.0000	time 0.4028 (0.4408)	loss 1.0098 (0.8674)	grad_norm 2.1793 (2.3347)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:25:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:12:28 lr 0.000082	 wd 0.0000	time 0.4097 (0.4395)	loss 0.8589 (0.8656)	grad_norm 2.2339 (2.3274)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:26:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:11:42 lr 0.000082	 wd 0.0000	time 0.4027 (0.4384)	loss 1.0107 (0.8664)	grad_norm 2.1320 (2.3242)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:26:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:10:57 lr 0.000082	 wd 0.0000	time 0.4187 (0.4378)	loss 0.8398 (0.8670)	grad_norm 2.4920 (2.3191)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:27:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:10:13 lr 0.000082	 wd 0.0000	time 0.4034 (0.4373)	loss 0.8931 (0.8670)	grad_norm 2.2409 (2.3215)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:28:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:09:28 lr 0.000082	 wd 0.0000	time 0.4224 (0.4368)	loss 0.9824 (0.8658)	grad_norm 2.0734 (2.3226)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:28:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:08:44 lr 0.000082	 wd 0.0000	time 0.4055 (0.4363)	loss 0.7944 (0.8656)	grad_norm 2.1510 (2.3212)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:29:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:08:00 lr 0.000081	 wd 0.0000	time 0.3964 (0.4359)	loss 0.9673 (0.8666)	grad_norm 2.2392 (2.3175)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:30:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:07:16 lr 0.000081	 wd 0.0000	time 0.4167 (0.4355)	loss 0.8955 (0.8671)	grad_norm 1.9411 (2.3207)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:31:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:06:32 lr 0.000081	 wd 0.0000	time 0.3920 (0.4352)	loss 0.8955 (0.8673)	grad_norm 2.3986 (2.3223)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:31:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:05:48 lr 0.000081	 wd 0.0000	time 0.3881 (0.4349)	loss 0.7959 (0.8675)	grad_norm 1.9704 (2.3172)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:32:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:05:05 lr 0.000081	 wd 0.0000	time 0.4278 (0.4349)	loss 0.7920 (0.8680)	grad_norm 2.4742 (2.3147)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:33:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:04:21 lr 0.000081	 wd 0.0000	time 0.3982 (0.4347)	loss 0.7485 (0.8675)	grad_norm 2.0879 (2.3106)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:33:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:03:38 lr 0.000080	 wd 0.0000	time 0.4063 (0.4346)	loss 0.8696 (0.8675)	grad_norm 2.1650 (2.3110)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:34:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:54 lr 0.000080	 wd 0.0000	time 0.3942 (0.4345)	loss 0.9316 (0.8684)	grad_norm 2.0320 (2.3124)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:35:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:02:11 lr 0.000080	 wd 0.0000	time 0.3943 (0.4344)	loss 0.8638 (0.8683)	grad_norm 1.9180 (2.3077)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:36:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:27 lr 0.000080	 wd 0.0000	time 0.3961 (0.4343)	loss 0.9072 (0.8677)	grad_norm 1.9963 (2.3044)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:36:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:44 lr 0.000080	 wd 0.0000	time 0.3948 (0.4341)	loss 0.9048 (0.8678)	grad_norm 2.1758 (2.3059)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:37:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000080	 wd 0.0000	time 0.3873 (0.4336)	loss 0.9258 (0.8678)	grad_norm 2.1793 (2.3068)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:37:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 8 training takes 0:18:07
[2024-07-02 10:37:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.236 (12.236)	Loss 0.3650 (0.3650)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 15686MB
[2024-07-02 10:38:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.944 Acc@5 97.436
[2024-07-02 10:38:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-02 10:38:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 84.94%
[2024-07-02 10:38:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 10:38:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 10:38:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][0/2502]	eta 6:12:56 lr 0.000080	 wd 0.0000	time 8.9436 (8.9436)	loss 0.8887 (0.8887)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:38:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:20:31 lr 0.000079	 wd 0.0000	time 0.3922 (0.5128)	loss 0.8105 (0.8622)	grad_norm 2.6984 (2.3138)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:39:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:18:04 lr 0.000079	 wd 0.0000	time 0.3906 (0.4713)	loss 0.8359 (0.8630)	grad_norm 2.2355 (2.3407)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:40:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:16:46 lr 0.000079	 wd 0.0000	time 0.3671 (0.4570)	loss 1.0273 (0.8635)	grad_norm 2.7341 (2.3417)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:41:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:15:45 lr 0.000079	 wd 0.0000	time 0.3998 (0.4500)	loss 0.9570 (0.8609)	grad_norm 1.9523 (2.3177)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:41:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:14:52 lr 0.000079	 wd 0.0000	time 0.3945 (0.4457)	loss 0.8350 (0.8616)	grad_norm 2.2704 (2.2953)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:42:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:14:02 lr 0.000079	 wd 0.0000	time 0.3404 (0.4430)	loss 0.8247 (0.8619)	grad_norm 2.2747 (2.2978)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:43:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:13:14 lr 0.000078	 wd 0.0000	time 0.3920 (0.4409)	loss 0.9126 (0.8636)	grad_norm 2.2435 (2.3056)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:43:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:12:27 lr 0.000078	 wd 0.0000	time 0.4031 (0.4394)	loss 0.9053 (0.8642)	grad_norm 2.5030 (2.3026)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:44:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:11:42 lr 0.000078	 wd 0.0000	time 0.4212 (0.4385)	loss 0.8491 (0.8637)	grad_norm 2.1917 (2.2980)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:45:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:10:57 lr 0.000078	 wd 0.0000	time 0.3932 (0.4375)	loss 0.9458 (0.8638)	grad_norm 2.1574 (2.2956)	loss_scale 16384.0000 (8683.0290)	mem 15686MB
[2024-07-02 10:46:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:10:12 lr 0.000078	 wd 0.0000	time 0.3999 (0.4367)	loss 0.9341 (0.8630)	grad_norm 2.2171 (2.3001)	loss_scale 16384.0000 (9382.4814)	mem 15686MB
[2024-07-02 10:46:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:09:28 lr 0.000078	 wd 0.0000	time 0.4003 (0.4363)	loss 0.8247 (0.8624)	grad_norm 2.7225 (2.2988)	loss_scale 16384.0000 (9965.4555)	mem 15686MB
[2024-07-02 10:47:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:08:44 lr 0.000077	 wd 0.0000	time 0.3650 (0.4360)	loss 0.8384 (0.8630)	grad_norm 2.3616 (2.2958)	loss_scale 16384.0000 (10458.8101)	mem 15686MB
[2024-07-02 10:48:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:07:59 lr 0.000077	 wd 0.0000	time 0.4007 (0.4355)	loss 0.9326 (0.8631)	grad_norm 1.9615 (2.2950)	loss_scale 16384.0000 (10881.7359)	mem 15686MB
[2024-07-02 10:48:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:07:16 lr 0.000077	 wd 0.0000	time 0.4208 (0.4353)	loss 0.8662 (0.8642)	grad_norm 1.8403 (2.2919)	loss_scale 16384.0000 (11248.3091)	mem 15686MB
[2024-07-02 10:49:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:06:32 lr 0.000077	 wd 0.0000	time 0.3893 (0.4351)	loss 0.9023 (0.8636)	grad_norm 2.3180 (2.2940)	loss_scale 16384.0000 (11569.0893)	mem 15686MB
[2024-07-02 10:50:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:05:48 lr 0.000077	 wd 0.0000	time 0.4023 (0.4349)	loss 0.9287 (0.8640)	grad_norm 2.3663 (2.2919)	loss_scale 16384.0000 (11852.1529)	mem 15686MB
[2024-07-02 10:51:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:05:05 lr 0.000077	 wd 0.0000	time 0.3939 (0.4346)	loss 0.9360 (0.8637)	grad_norm 2.8940 (2.2904)	loss_scale 16384.0000 (12103.7823)	mem 15686MB
[2024-07-02 10:51:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:04:21 lr 0.000076	 wd 0.0000	time 0.3975 (0.4345)	loss 0.9482 (0.8633)	grad_norm 2.1523 (2.2909)	loss_scale 16384.0000 (12328.9385)	mem 15686MB
[2024-07-02 10:52:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:03:38 lr 0.000076	 wd 0.0000	time 0.4133 (0.4344)	loss 0.8569 (0.8639)	grad_norm 2.1918 (2.2961)	loss_scale 16384.0000 (12531.5902)	mem 15686MB
[2024-07-02 10:53:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:54 lr 0.000076	 wd 0.0000	time 0.4061 (0.4343)	loss 0.7397 (0.8641)	grad_norm 2.7957 (2.2937)	loss_scale 16384.0000 (12714.9510)	mem 15686MB
[2024-07-02 10:53:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:11 lr 0.000076	 wd 0.0000	time 0.3994 (0.4341)	loss 0.9028 (0.8641)	grad_norm 2.3014 (2.2971)	loss_scale 16384.0000 (12881.6502)	mem 15686MB
[2024-07-02 10:54:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:27 lr 0.000076	 wd 0.0000	time 0.3982 (0.4339)	loss 0.7632 (0.8637)	grad_norm 2.2654 (2.2952)	loss_scale 16384.0000 (13033.8601)	mem 15686MB
[2024-07-02 10:55:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:44 lr 0.000075	 wd 0.0000	time 0.3868 (0.4338)	loss 0.8247 (0.8634)	grad_norm 1.7357 (2.2976)	loss_scale 16384.0000 (13173.3911)	mem 15686MB
[2024-07-02 10:56:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000075	 wd 0.0000	time 0.3848 (0.4333)	loss 0.8794 (0.8639)	grad_norm 2.0579 (inf)	loss_scale 8192.0000 (13242.8053)	mem 15686MB
[2024-07-02 10:56:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 9 training takes 0:18:06
[2024-07-02 10:56:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.346 (12.346)	Loss 0.3508 (0.3508)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 15686MB
[2024-07-02 10:56:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.034 Acc@5 97.440
[2024-07-02 10:56:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-02 10:56:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-02 10:56:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 10:56:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 10:56:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][0/2502]	eta 6:13:31 lr 0.000075	 wd 0.0000	time 8.9574 (8.9574)	loss 1.0342 (1.0342)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:57:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:20:33 lr 0.000075	 wd 0.0000	time 0.4063 (0.5137)	loss 0.8940 (0.8527)	grad_norm 2.2467 (2.2503)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:58:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:18:03 lr 0.000075	 wd 0.0000	time 0.3907 (0.4705)	loss 0.7456 (0.8534)	grad_norm 2.3843 (2.2174)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:58:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:16:46 lr 0.000075	 wd 0.0000	time 0.3919 (0.4570)	loss 0.9009 (0.8506)	grad_norm 2.1454 (2.2686)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 10:59:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:15:46 lr 0.000075	 wd 0.0000	time 0.4360 (0.4503)	loss 0.8540 (0.8520)	grad_norm 2.5210 (2.2649)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:00:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:14:52 lr 0.000074	 wd 0.0000	time 0.3849 (0.4456)	loss 0.7739 (0.8537)	grad_norm 2.0341 (2.2578)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:01:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:14:01 lr 0.000074	 wd 0.0000	time 0.3960 (0.4427)	loss 0.8096 (0.8546)	grad_norm 2.1050 (2.2603)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:01:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:13:14 lr 0.000074	 wd 0.0000	time 0.3885 (0.4410)	loss 0.8042 (0.8542)	grad_norm 2.2927 (2.2554)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:02:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:12:28 lr 0.000074	 wd 0.0000	time 0.3889 (0.4398)	loss 0.8369 (0.8548)	grad_norm 2.2428 (2.2578)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:03:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:11:43 lr 0.000074	 wd 0.0000	time 0.4183 (0.4392)	loss 0.8105 (0.8552)	grad_norm 2.8439 (2.2600)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:03:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:10:58 lr 0.000073	 wd 0.0000	time 0.3967 (0.4382)	loss 0.7783 (0.8543)	grad_norm 2.3782 (2.2565)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:04:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:10:13 lr 0.000073	 wd 0.0000	time 0.4537 (0.4375)	loss 0.9102 (0.8541)	grad_norm 2.3235 (2.2687)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:05:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:09:28 lr 0.000073	 wd 0.0000	time 0.4253 (0.4370)	loss 1.0684 (0.8537)	grad_norm 2.4448 (2.2736)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:06:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:08:44 lr 0.000073	 wd 0.0000	time 0.3956 (0.4364)	loss 0.9692 (0.8546)	grad_norm 2.1421 (2.2777)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:06:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:08:00 lr 0.000073	 wd 0.0000	time 0.3729 (0.4359)	loss 0.8120 (0.8549)	grad_norm 2.4172 (2.2775)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:07:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:07:16 lr 0.000073	 wd 0.0000	time 0.4133 (0.4356)	loss 0.8560 (0.8540)	grad_norm 2.1638 (2.2717)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:08:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:06:32 lr 0.000072	 wd 0.0000	time 0.3457 (0.4353)	loss 0.7852 (0.8539)	grad_norm 2.0403 (2.2736)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:08:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:05:48 lr 0.000072	 wd 0.0000	time 0.4184 (0.4351)	loss 0.8433 (0.8539)	grad_norm 3.0337 (2.2739)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:09:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:05:05 lr 0.000072	 wd 0.0000	time 0.3942 (0.4350)	loss 0.9951 (0.8543)	grad_norm 2.4661 (2.2757)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:10:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:04:21 lr 0.000072	 wd 0.0000	time 0.4104 (0.4348)	loss 0.8501 (0.8542)	grad_norm 2.4139 (2.2740)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:11:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:03:38 lr 0.000072	 wd 0.0000	time 0.4162 (0.4346)	loss 1.0303 (0.8544)	grad_norm 2.2378 (2.2810)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:11:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:54 lr 0.000071	 wd 0.0000	time 0.3957 (0.4342)	loss 0.9951 (0.8542)	grad_norm 3.4466 (2.2836)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:12:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:02:11 lr 0.000071	 wd 0.0000	time 0.3960 (0.4341)	loss 0.7349 (0.8544)	grad_norm 2.4944 (2.2911)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:13:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:27 lr 0.000071	 wd 0.0000	time 0.4147 (0.4341)	loss 0.8472 (0.8543)	grad_norm 2.3006 (2.2926)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:14:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:44 lr 0.000071	 wd 0.0000	time 0.4256 (0.4340)	loss 0.9834 (0.8543)	grad_norm 2.0401 (2.2881)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:14:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000071	 wd 0.0000	time 0.3831 (0.4336)	loss 0.8003 (0.8542)	grad_norm 2.3793 (2.2866)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:14:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 10 training takes 0:18:07
[2024-07-02 11:14:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.415 (12.415)	Loss 0.3589 (0.3589)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 11:15:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.884 Acc@5 97.424
[2024-07-02 11:15:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-02 11:15:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-02 11:15:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:31:16 lr 0.000071	 wd 0.0000	time 10.8218 (10.8218)	loss 0.8525 (0.8525)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:16:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:21:41 lr 0.000070	 wd 0.0000	time 0.4113 (0.5418)	loss 0.7705 (0.8393)	grad_norm 2.1191 (2.2541)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:16:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:18:36 lr 0.000070	 wd 0.0000	time 0.3939 (0.4849)	loss 0.7178 (0.8467)	grad_norm 2.1293 (2.2582)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:17:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:17:07 lr 0.000070	 wd 0.0000	time 0.4039 (0.4666)	loss 0.9023 (0.8456)	grad_norm 2.3687 (2.2523)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:18:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:16:01 lr 0.000070	 wd 0.0000	time 0.3747 (0.4574)	loss 0.7666 (0.8455)	grad_norm 2.4038 (2.2716)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:18:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:15:05 lr 0.000070	 wd 0.0000	time 0.3867 (0.4523)	loss 0.8120 (0.8429)	grad_norm 2.3305 (2.2771)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:19:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:14:13 lr 0.000069	 wd 0.0000	time 0.3946 (0.4489)	loss 0.9922 (0.8423)	grad_norm 2.6743 (2.2725)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:20:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:13:23 lr 0.000069	 wd 0.0000	time 0.3893 (0.4459)	loss 0.9004 (0.8433)	grad_norm 1.9779 (2.2810)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:21:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:12:35 lr 0.000069	 wd 0.0000	time 0.3688 (0.4437)	loss 1.0684 (0.8455)	grad_norm 2.1572 (2.2859)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:21:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:11:48 lr 0.000069	 wd 0.0000	time 0.4138 (0.4425)	loss 0.8687 (0.8473)	grad_norm 2.2079 (2.2823)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:22:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:11:02 lr 0.000069	 wd 0.0000	time 0.3962 (0.4412)	loss 0.7598 (0.8460)	grad_norm 2.2425 (2.2822)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:23:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:10:16 lr 0.000069	 wd 0.0000	time 0.3799 (0.4401)	loss 0.8018 (0.8454)	grad_norm 2.2679 (2.2819)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:23:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:09:31 lr 0.000068	 wd 0.0000	time 0.3977 (0.4393)	loss 0.9077 (0.8455)	grad_norm 2.0933 (2.2832)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:24:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:08:47 lr 0.000068	 wd 0.0000	time 0.4355 (0.4389)	loss 0.7861 (0.8450)	grad_norm 2.5217 (2.2768)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:25:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:08:03 lr 0.000068	 wd 0.0000	time 0.4068 (0.4384)	loss 0.8232 (0.8453)	grad_norm 2.6179 (2.2720)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:26:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:07:18 lr 0.000068	 wd 0.0000	time 0.3914 (0.4379)	loss 0.8354 (0.8456)	grad_norm 2.2229 (2.2673)	loss_scale 16384.0000 (8312.0693)	mem 15686MB
[2024-07-02 11:26:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:06:34 lr 0.000068	 wd 0.0000	time 0.3808 (0.4375)	loss 0.8809 (0.8460)	grad_norm 2.4854 (2.2711)	loss_scale 16384.0000 (8816.2498)	mem 15686MB
[2024-07-02 11:27:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:05:50 lr 0.000067	 wd 0.0000	time 0.3707 (0.4371)	loss 0.7988 (0.8456)	grad_norm 2.4632 (2.2719)	loss_scale 16384.0000 (9261.1499)	mem 15686MB
[2024-07-02 11:28:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:05:06 lr 0.000067	 wd 0.0000	time 0.4178 (0.4368)	loss 0.8203 (0.8464)	grad_norm 2.1212 (2.2759)	loss_scale 16384.0000 (9656.6441)	mem 15686MB
[2024-07-02 11:29:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:04:22 lr 0.000067	 wd 0.0000	time 0.3830 (0.4365)	loss 0.7861 (0.8467)	grad_norm 2.1885 (2.2800)	loss_scale 16384.0000 (10010.5292)	mem 15686MB
[2024-07-02 11:29:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:03:38 lr 0.000067	 wd 0.0000	time 0.4011 (0.4362)	loss 0.8008 (0.8467)	grad_norm 1.9637 (2.2798)	loss_scale 16384.0000 (10329.0435)	mem 15686MB
[2024-07-02 11:30:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:55 lr 0.000067	 wd 0.0000	time 0.3934 (0.4361)	loss 0.8247 (0.8465)	grad_norm 2.5420 (2.2784)	loss_scale 16384.0000 (10617.2375)	mem 15686MB
[2024-07-02 11:31:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:11 lr 0.000066	 wd 0.0000	time 0.4035 (0.4359)	loss 0.9346 (0.8465)	grad_norm 1.9526 (2.2741)	loss_scale 16384.0000 (10879.2440)	mem 15686MB
[2024-07-02 11:31:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:28 lr 0.000066	 wd 0.0000	time 0.3864 (0.4357)	loss 0.7480 (0.8468)	grad_norm 2.5874 (2.2749)	loss_scale 16384.0000 (11118.4772)	mem 15686MB
[2024-07-02 11:32:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:44 lr 0.000066	 wd 0.0000	time 0.3965 (0.4355)	loss 0.8545 (0.8471)	grad_norm 2.0541 (2.2748)	loss_scale 16384.0000 (11337.7826)	mem 15686MB
[2024-07-02 11:33:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000066	 wd 0.0000	time 0.3884 (0.4349)	loss 0.8286 (0.8467)	grad_norm 2.2740 (2.2726)	loss_scale 16384.0000 (11539.5506)	mem 15686MB
[2024-07-02 11:33:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 11 training takes 0:18:10
[2024-07-02 11:33:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.206 (12.206)	Loss 0.3767 (0.3767)	Acc@1 91.211 (91.211)	Acc@5 98.828 (98.828)	Mem 15686MB
[2024-07-02 11:33:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 84.976 Acc@5 97.424
[2024-07-02 11:33:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-02 11:33:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-02 11:34:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:00:57 lr 0.000066	 wd 0.0000	time 11.5338 (11.5338)	loss 0.8560 (0.8560)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 11:34:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:21:39 lr 0.000066	 wd 0.0000	time 0.3918 (0.5409)	loss 0.8242 (0.8304)	grad_norm 2.0255 (inf)	loss_scale 8192.0000 (15897.3465)	mem 15686MB
[2024-07-02 11:35:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:18:36 lr 0.000065	 wd 0.0000	time 0.3978 (0.4850)	loss 0.8105 (0.8351)	grad_norm 1.9868 (inf)	loss_scale 8192.0000 (12063.8408)	mem 15686MB
[2024-07-02 11:36:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:17:06 lr 0.000065	 wd 0.0000	time 0.3728 (0.4660)	loss 0.7964 (0.8406)	grad_norm 2.1741 (inf)	loss_scale 8192.0000 (10777.5150)	mem 15686MB
[2024-07-02 11:36:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:16:00 lr 0.000065	 wd 0.0000	time 0.4003 (0.4570)	loss 0.8921 (0.8447)	grad_norm 2.4307 (inf)	loss_scale 8192.0000 (10132.7481)	mem 15686MB
[2024-07-02 11:37:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:15:05 lr 0.000065	 wd 0.0000	time 0.4083 (0.4521)	loss 0.8257 (0.8447)	grad_norm 2.3088 (inf)	loss_scale 8192.0000 (9745.3733)	mem 15686MB
[2024-07-02 11:38:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:14:12 lr 0.000065	 wd 0.0000	time 0.3913 (0.4484)	loss 0.9370 (0.8428)	grad_norm 2.6361 (inf)	loss_scale 8192.0000 (9486.9085)	mem 15686MB
[2024-07-02 11:39:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:13:24 lr 0.000064	 wd 0.0000	time 0.3936 (0.4462)	loss 0.7656 (0.8424)	grad_norm 2.2393 (inf)	loss_scale 8192.0000 (9302.1854)	mem 15686MB
[2024-07-02 11:39:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:12:35 lr 0.000064	 wd 0.0000	time 0.3924 (0.4441)	loss 0.9497 (0.8437)	grad_norm 2.0555 (inf)	loss_scale 8192.0000 (9163.5855)	mem 15686MB
[2024-07-02 11:40:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:11:48 lr 0.000064	 wd 0.0000	time 0.3985 (0.4425)	loss 0.7783 (0.8427)	grad_norm 2.3376 (inf)	loss_scale 8192.0000 (9055.7514)	mem 15686MB
[2024-07-02 11:41:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:11:03 lr 0.000064	 wd 0.0000	time 0.3949 (0.4414)	loss 0.8188 (0.8444)	grad_norm 2.6358 (inf)	loss_scale 8192.0000 (8969.4625)	mem 15686MB
[2024-07-02 11:41:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:10:17 lr 0.000064	 wd 0.0000	time 0.4176 (0.4405)	loss 0.7754 (0.8446)	grad_norm 2.3035 (inf)	loss_scale 8192.0000 (8898.8483)	mem 15686MB
[2024-07-02 11:42:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:09:32 lr 0.000063	 wd 0.0000	time 0.3916 (0.4396)	loss 0.8486 (0.8434)	grad_norm 2.7749 (inf)	loss_scale 8192.0000 (8839.9933)	mem 15686MB
[2024-07-02 11:43:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:08:47 lr 0.000063	 wd 0.0000	time 0.3640 (0.4390)	loss 0.7759 (0.8432)	grad_norm 2.4422 (inf)	loss_scale 8192.0000 (8790.1860)	mem 15686MB
[2024-07-02 11:44:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:08:03 lr 0.000063	 wd 0.0000	time 0.3859 (0.4384)	loss 0.8418 (0.8431)	grad_norm 2.2168 (inf)	loss_scale 8192.0000 (8747.4889)	mem 15686MB
[2024-07-02 11:44:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:07:18 lr 0.000063	 wd 0.0000	time 0.4396 (0.4380)	loss 0.8813 (0.8432)	grad_norm 1.9364 (inf)	loss_scale 8192.0000 (8710.4810)	mem 15686MB
[2024-07-02 11:45:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:06:34 lr 0.000063	 wd 0.0000	time 0.4129 (0.4375)	loss 0.8188 (0.8428)	grad_norm 2.2224 (inf)	loss_scale 8192.0000 (8678.0962)	mem 15686MB
[2024-07-02 11:46:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:05:50 lr 0.000062	 wd 0.0000	time 0.3865 (0.4371)	loss 0.7681 (0.8434)	grad_norm 2.0765 (inf)	loss_scale 8192.0000 (8649.5191)	mem 15686MB
[2024-07-02 11:46:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:05:06 lr 0.000062	 wd 0.0000	time 0.3850 (0.4369)	loss 0.7388 (0.8439)	grad_norm 2.1170 (inf)	loss_scale 8192.0000 (8624.1155)	mem 15686MB
[2024-07-02 11:47:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:04:22 lr 0.000062	 wd 0.0000	time 0.3926 (0.4367)	loss 0.7832 (0.8445)	grad_norm 2.3577 (inf)	loss_scale 8192.0000 (8601.3845)	mem 15686MB
[2024-07-02 11:48:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:03:39 lr 0.000062	 wd 0.0000	time 0.3952 (0.4364)	loss 0.7559 (0.8441)	grad_norm 2.0204 (inf)	loss_scale 8192.0000 (8580.9255)	mem 15686MB
[2024-07-02 11:49:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:55 lr 0.000062	 wd 0.0000	time 0.3940 (0.4361)	loss 0.8833 (0.8447)	grad_norm 2.2980 (inf)	loss_scale 8192.0000 (8562.4141)	mem 15686MB
[2024-07-02 11:49:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:02:11 lr 0.000061	 wd 0.0000	time 0.3988 (0.4360)	loss 0.8613 (0.8452)	grad_norm 2.3345 (inf)	loss_scale 8192.0000 (8545.5847)	mem 15686MB
[2024-07-02 11:50:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:28 lr 0.000061	 wd 0.0000	time 0.4324 (0.4357)	loss 0.8325 (0.8450)	grad_norm 2.0999 (inf)	loss_scale 8192.0000 (8530.2182)	mem 15686MB
[2024-07-02 11:51:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:44 lr 0.000061	 wd 0.0000	time 0.3968 (0.4355)	loss 0.7988 (0.8444)	grad_norm 3.0843 (inf)	loss_scale 8192.0000 (8516.1316)	mem 15686MB
[2024-07-02 11:51:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000061	 wd 0.0000	time 0.3845 (0.4349)	loss 0.8867 (0.8445)	grad_norm 2.2699 (inf)	loss_scale 8192.0000 (8503.1715)	mem 15686MB
[2024-07-02 11:51:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 12 training takes 0:18:10
[2024-07-02 11:52:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.636 (12.636)	Loss 0.3826 (0.3826)	Acc@1 90.625 (90.625)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 11:52:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.018 Acc@5 97.472
[2024-07-02 11:52:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-02 11:52:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.03%
[2024-07-02 11:52:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:32:32 lr 0.000061	 wd 0.0000	time 12.2911 (12.2911)	loss 0.7129 (0.7129)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:53:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:22:06 lr 0.000061	 wd 0.0000	time 0.4022 (0.5521)	loss 0.7832 (0.8420)	grad_norm 2.7030 (2.2485)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:54:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:18:51 lr 0.000060	 wd 0.0000	time 0.3772 (0.4914)	loss 0.9038 (0.8423)	grad_norm 2.2358 (2.3138)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:54:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:17:17 lr 0.000060	 wd 0.0000	time 0.4072 (0.4712)	loss 0.7729 (0.8344)	grad_norm 2.2502 (2.2958)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:55:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:16:09 lr 0.000060	 wd 0.0000	time 0.4134 (0.4610)	loss 0.8203 (0.8352)	grad_norm 2.4829 (2.2963)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:56:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:15:10 lr 0.000060	 wd 0.0000	time 0.3894 (0.4546)	loss 0.7681 (0.8341)	grad_norm 3.1116 (2.2852)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:56:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:14:17 lr 0.000060	 wd 0.0000	time 0.4193 (0.4509)	loss 0.6318 (0.8350)	grad_norm 1.9214 (2.2748)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:57:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:13:27 lr 0.000059	 wd 0.0000	time 0.3863 (0.4481)	loss 0.8838 (0.8355)	grad_norm 2.1021 (2.2797)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:58:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:12:39 lr 0.000059	 wd 0.0000	time 0.4152 (0.4460)	loss 0.7886 (0.8356)	grad_norm 1.9654 (2.2708)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:59:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:11:51 lr 0.000059	 wd 0.0000	time 0.3933 (0.4444)	loss 0.7773 (0.8339)	grad_norm 2.8759 (2.2812)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 11:59:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:11:05 lr 0.000059	 wd 0.0000	time 0.3849 (0.4428)	loss 0.9365 (0.8342)	grad_norm 2.4242 (2.2779)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 12:00:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:10:19 lr 0.000059	 wd 0.0000	time 0.4004 (0.4417)	loss 0.7886 (0.8346)	grad_norm 2.0408 (2.2732)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 12:01:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:09:33 lr 0.000058	 wd 0.0000	time 0.3788 (0.4408)	loss 0.9575 (0.8350)	grad_norm 2.2686 (2.2686)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 12:01:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:08:48 lr 0.000058	 wd 0.0000	time 0.3876 (0.4399)	loss 0.8311 (0.8363)	grad_norm 2.3781 (2.2721)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 12:02:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:08:04 lr 0.000058	 wd 0.0000	time 0.3939 (0.4395)	loss 0.7471 (0.8364)	grad_norm 2.3472 (2.2735)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 12:03:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:07:20 lr 0.000058	 wd 0.0000	time 0.4212 (0.4394)	loss 0.8115 (0.8366)	grad_norm 2.8451 (2.2751)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 12:04:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:06:36 lr 0.000058	 wd 0.0000	time 0.4045 (0.4392)	loss 0.7725 (0.8365)	grad_norm 2.1077 (2.2736)	loss_scale 16384.0000 (8232.9344)	mem 15686MB
[2024-07-02 12:04:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:05:52 lr 0.000057	 wd 0.0000	time 0.3932 (0.4391)	loss 0.7905 (0.8364)	grad_norm 2.2555 (2.2720)	loss_scale 16384.0000 (8712.1270)	mem 15686MB
[2024-07-02 12:05:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:05:07 lr 0.000057	 wd 0.0000	time 0.4257 (0.4387)	loss 0.7935 (0.8371)	grad_norm 2.6127 (2.2743)	loss_scale 16384.0000 (9138.1055)	mem 15686MB
[2024-07-02 12:06:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:04:23 lr 0.000057	 wd 0.0000	time 0.3690 (0.4383)	loss 0.8237 (0.8373)	grad_norm 2.9795 (2.2692)	loss_scale 16384.0000 (9519.2678)	mem 15686MB
[2024-07-02 12:07:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:03:39 lr 0.000057	 wd 0.0000	time 0.4086 (0.4380)	loss 0.8496 (0.8373)	grad_norm 1.7993 (2.2741)	loss_scale 16384.0000 (9862.3328)	mem 15686MB
[2024-07-02 12:07:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:55 lr 0.000057	 wd 0.0000	time 0.4077 (0.4377)	loss 1.0508 (0.8376)	grad_norm 2.9223 (2.2829)	loss_scale 16384.0000 (10172.7406)	mem 15686MB
[2024-07-02 12:08:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:12 lr 0.000056	 wd 0.0000	time 0.4066 (0.4376)	loss 0.8833 (0.8374)	grad_norm 2.2423 (2.2849)	loss_scale 16384.0000 (10454.9423)	mem 15686MB
[2024-07-02 12:09:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:28 lr 0.000056	 wd 0.0000	time 0.3937 (0.4372)	loss 0.9458 (0.8376)	grad_norm 1.9744 (2.2796)	loss_scale 16384.0000 (10712.6154)	mem 15686MB
[2024-07-02 12:09:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:44 lr 0.000056	 wd 0.0000	time 0.3789 (0.4370)	loss 0.8213 (0.8374)	grad_norm 2.0908 (2.2777)	loss_scale 16384.0000 (10948.8247)	mem 15686MB
[2024-07-02 12:10:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000056	 wd 0.0000	time 0.3911 (0.4364)	loss 0.9087 (0.8375)	grad_norm 1.8351 (2.2742)	loss_scale 16384.0000 (11166.1447)	mem 15686MB
[2024-07-02 12:10:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 13 training takes 0:18:14
[2024-07-02 12:10:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.665 (12.665)	Loss 0.3916 (0.3916)	Acc@1 91.211 (91.211)	Acc@5 98.828 (98.828)	Mem 15686MB
[2024-07-02 12:11:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.142 Acc@5 97.470
[2024-07-02 12:11:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-02 12:11:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.14%
[2024-07-02 12:11:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 12:11:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 12:11:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][0/2502]	eta 6:06:13 lr 0.000056	 wd 0.0000	time 8.7826 (8.7826)	loss 0.8203 (0.8203)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:12:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:20:32 lr 0.000055	 wd 0.0000	time 0.4272 (0.5132)	loss 0.8818 (0.8372)	grad_norm 2.2763 (2.3661)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:12:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:18:02 lr 0.000055	 wd 0.0000	time 0.3792 (0.4704)	loss 0.7266 (0.8430)	grad_norm 2.3739 (2.3280)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:13:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:16:44 lr 0.000055	 wd 0.0000	time 0.4077 (0.4563)	loss 0.9053 (0.8409)	grad_norm 1.8333 (2.3084)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:14:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:15:45 lr 0.000055	 wd 0.0000	time 0.3811 (0.4497)	loss 0.9678 (0.8383)	grad_norm 2.4968 (2.3188)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:14:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:14:51 lr 0.000055	 wd 0.0000	time 0.3943 (0.4455)	loss 0.8906 (0.8382)	grad_norm 1.8514 (2.3064)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:15:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:14:08 lr 0.000054	 wd 0.0000	time 0.4203 (0.4459)	loss 0.8892 (0.8370)	grad_norm 2.7353 (2.3033)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:16:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:13:19 lr 0.000054	 wd 0.0000	time 0.4153 (0.4438)	loss 0.7876 (0.8347)	grad_norm 2.5935 (2.3015)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:17:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:12:32 lr 0.000054	 wd 0.0000	time 0.3955 (0.4421)	loss 0.7920 (0.8343)	grad_norm 2.3834 (2.2898)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:17:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:11:45 lr 0.000054	 wd 0.0000	time 0.4172 (0.4407)	loss 0.7988 (0.8349)	grad_norm 1.8431 (2.2809)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:18:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:11:00 lr 0.000054	 wd 0.0000	time 0.4078 (0.4395)	loss 0.9072 (0.8355)	grad_norm 2.1466 (2.2894)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:19:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:10:15 lr 0.000053	 wd 0.0000	time 0.3852 (0.4388)	loss 0.7925 (0.8336)	grad_norm 2.1883 (2.2933)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:19:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:09:30 lr 0.000053	 wd 0.0000	time 0.4282 (0.4380)	loss 0.8477 (0.8335)	grad_norm 2.3839 (2.2854)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:20:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:08:45 lr 0.000053	 wd 0.0000	time 0.4068 (0.4375)	loss 0.8052 (0.8327)	grad_norm 1.9059 (2.2775)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:21:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:08:01 lr 0.000053	 wd 0.0000	time 0.4214 (0.4370)	loss 0.8242 (0.8326)	grad_norm 2.3320 (2.2786)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:22:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:07:17 lr 0.000053	 wd 0.0000	time 0.4187 (0.4366)	loss 0.9023 (0.8323)	grad_norm 2.1823 (2.2842)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:22:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:06:33 lr 0.000052	 wd 0.0000	time 0.4124 (0.4365)	loss 0.7686 (0.8324)	grad_norm 2.0187 (2.2846)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:23:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:05:49 lr 0.000052	 wd 0.0000	time 0.4291 (0.4362)	loss 0.7700 (0.8322)	grad_norm 1.9588 (2.2790)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:24:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:05:06 lr 0.000052	 wd 0.0000	time 0.3896 (0.4361)	loss 0.7627 (0.8323)	grad_norm 2.1576 (2.2765)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:24:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:04:22 lr 0.000052	 wd 0.0000	time 0.4132 (0.4358)	loss 0.7520 (0.8316)	grad_norm 2.0943 (2.2778)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:25:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:03:38 lr 0.000052	 wd 0.0000	time 0.4002 (0.4356)	loss 0.8257 (0.8318)	grad_norm 2.2217 (2.2764)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:26:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:55 lr 0.000051	 wd 0.0000	time 0.3973 (0.4355)	loss 0.8511 (0.8317)	grad_norm 2.2354 (2.2765)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:27:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:11 lr 0.000051	 wd 0.0000	time 0.3943 (0.4353)	loss 0.7441 (0.8323)	grad_norm 2.1617 (2.2837)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:27:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:27 lr 0.000051	 wd 0.0000	time 0.3861 (0.4351)	loss 0.8403 (0.8321)	grad_norm 2.1408 (2.2891)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:28:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:44 lr 0.000051	 wd 0.0000	time 0.3886 (0.4350)	loss 1.0137 (0.8326)	grad_norm 2.3036 (2.2873)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:29:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000051	 wd 0.0000	time 0.3844 (0.4343)	loss 0.7612 (0.8327)	grad_norm 2.4347 (2.2860)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:29:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 14 training takes 0:18:08
[2024-07-02 12:29:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 11.930 (11.930)	Loss 0.3916 (0.3916)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 15686MB
[2024-07-02 12:29:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.142 Acc@5 97.440
[2024-07-02 12:29:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-02 12:29:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.14%
[2024-07-02 12:29:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 12:29:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 12:29:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][0/2502]	eta 6:11:16 lr 0.000051	 wd 0.0000	time 8.9033 (8.9033)	loss 0.8213 (0.8213)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:30:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:20:30 lr 0.000050	 wd 0.0000	time 0.4117 (0.5123)	loss 0.9517 (0.8291)	grad_norm 2.0892 (2.2835)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:31:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:18:04 lr 0.000050	 wd 0.0000	time 0.4116 (0.4711)	loss 0.7539 (0.8255)	grad_norm 2.0682 (2.2784)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:32:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:16:45 lr 0.000050	 wd 0.0000	time 0.3878 (0.4567)	loss 0.8506 (0.8268)	grad_norm 2.6990 (2.2544)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:32:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:15:45 lr 0.000050	 wd 0.0000	time 0.4058 (0.4496)	loss 0.7817 (0.8256)	grad_norm 2.8236 (2.2766)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:33:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:14:51 lr 0.000049	 wd 0.0000	time 0.4064 (0.4454)	loss 0.9043 (0.8255)	grad_norm 2.0895 (2.2899)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:34:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:14:02 lr 0.000049	 wd 0.0000	time 0.3749 (0.4427)	loss 0.7593 (0.8274)	grad_norm 2.1486 (2.2827)	loss_scale 32768.0000 (16711.1348)	mem 15686MB
[2024-07-02 12:34:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:13:14 lr 0.000049	 wd 0.0000	time 0.4156 (0.4412)	loss 0.8862 (0.8268)	grad_norm 2.3831 (inf)	loss_scale 16384.0000 (17272.1484)	mem 15686MB
[2024-07-02 12:35:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:12:28 lr 0.000049	 wd 0.0000	time 0.4008 (0.4399)	loss 0.9155 (0.8263)	grad_norm 2.3317 (inf)	loss_scale 16384.0000 (17161.2684)	mem 15686MB
[2024-07-02 12:36:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:11:43 lr 0.000049	 wd 0.0000	time 0.4123 (0.4389)	loss 0.7710 (0.8265)	grad_norm 2.0948 (inf)	loss_scale 16384.0000 (17075.0011)	mem 15686MB
[2024-07-02 12:37:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:10:57 lr 0.000048	 wd 0.0000	time 0.3897 (0.4378)	loss 0.7515 (0.8266)	grad_norm 2.2294 (inf)	loss_scale 16384.0000 (17005.9700)	mem 15686MB
[2024-07-02 12:37:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:10:12 lr 0.000048	 wd 0.0000	time 0.4129 (0.4371)	loss 0.7275 (0.8248)	grad_norm 2.0295 (inf)	loss_scale 16384.0000 (16949.4787)	mem 15686MB
[2024-07-02 12:38:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:09:28 lr 0.000048	 wd 0.0000	time 0.4116 (0.4366)	loss 0.8555 (0.8248)	grad_norm 2.6036 (inf)	loss_scale 16384.0000 (16902.3947)	mem 15686MB
[2024-07-02 12:39:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:08:44 lr 0.000048	 wd 0.0000	time 0.3948 (0.4361)	loss 0.8623 (0.8254)	grad_norm 2.1111 (inf)	loss_scale 16384.0000 (16862.5488)	mem 15686MB
[2024-07-02 12:39:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:08:00 lr 0.000048	 wd 0.0000	time 0.4159 (0.4358)	loss 0.8687 (0.8261)	grad_norm 1.8955 (inf)	loss_scale 16384.0000 (16828.3911)	mem 15686MB
[2024-07-02 12:40:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:07:16 lr 0.000047	 wd 0.0000	time 0.4344 (0.4355)	loss 0.9653 (0.8257)	grad_norm 2.2358 (inf)	loss_scale 16384.0000 (16798.7848)	mem 15686MB
[2024-07-02 12:41:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:06:32 lr 0.000047	 wd 0.0000	time 0.4281 (0.4352)	loss 0.8486 (0.8255)	grad_norm 1.7722 (inf)	loss_scale 16384.0000 (16772.8770)	mem 15686MB
[2024-07-02 12:42:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:05:48 lr 0.000047	 wd 0.0000	time 0.3954 (0.4351)	loss 0.8364 (0.8255)	grad_norm 1.9541 (inf)	loss_scale 16384.0000 (16750.0153)	mem 15686MB
[2024-07-02 12:42:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:05:05 lr 0.000047	 wd 0.0000	time 0.4042 (0.4349)	loss 1.0039 (0.8257)	grad_norm 2.3803 (inf)	loss_scale 16384.0000 (16729.6924)	mem 15686MB
[2024-07-02 12:43:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:04:21 lr 0.000047	 wd 0.0000	time 0.4240 (0.4347)	loss 0.8369 (0.8258)	grad_norm 2.2190 (inf)	loss_scale 16384.0000 (16711.5076)	mem 15686MB
[2024-07-02 12:44:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:03:38 lr 0.000046	 wd 0.0000	time 0.4232 (0.4347)	loss 0.8657 (0.8262)	grad_norm 2.3203 (inf)	loss_scale 16384.0000 (16695.1404)	mem 15686MB
[2024-07-02 12:45:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:54 lr 0.000046	 wd 0.0000	time 0.4031 (0.4345)	loss 0.9351 (0.8261)	grad_norm 2.4495 (inf)	loss_scale 16384.0000 (16680.3313)	mem 15686MB
[2024-07-02 12:45:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:11 lr 0.000046	 wd 0.0000	time 0.3932 (0.4343)	loss 0.8022 (0.8267)	grad_norm 2.0507 (inf)	loss_scale 16384.0000 (16666.8678)	mem 15686MB
[2024-07-02 12:46:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:27 lr 0.000046	 wd 0.0000	time 0.3966 (0.4342)	loss 0.8276 (0.8265)	grad_norm 1.9452 (inf)	loss_scale 16384.0000 (16654.5745)	mem 15686MB
[2024-07-02 12:47:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:44 lr 0.000046	 wd 0.0000	time 0.3937 (0.4341)	loss 0.9521 (0.8266)	grad_norm 2.0043 (inf)	loss_scale 16384.0000 (16643.3053)	mem 15686MB
[2024-07-02 12:47:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000045	 wd 0.0000	time 0.3859 (0.4336)	loss 0.8638 (0.8266)	grad_norm 2.3129 (inf)	loss_scale 16384.0000 (16632.9372)	mem 15686MB
[2024-07-02 12:47:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 15 training takes 0:18:07
[2024-07-02 12:47:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 145): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_15.pth saving......
[2024-07-02 12:47:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 147): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_15.pth saved !!!
[2024-07-02 12:48:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 10.950 (10.950)	Loss 0.3850 (0.3850)	Acc@1 91.797 (91.797)	Acc@5 98.438 (98.438)	Mem 15686MB
[2024-07-02 12:48:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.326 Acc@5 97.524
[2024-07-02 12:48:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 12:48:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 12:48:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 12:48:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 12:48:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][0/2502]	eta 6:18:12 lr 0.000045	 wd 0.0000	time 9.0696 (9.0696)	loss 0.7197 (0.7197)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:49:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:20:40 lr 0.000045	 wd 0.0000	time 0.3870 (0.5164)	loss 0.9712 (0.8208)	grad_norm 2.4706 (2.2729)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:49:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:18:09 lr 0.000045	 wd 0.0000	time 0.3961 (0.4731)	loss 0.8896 (0.8146)	grad_norm 2.0614 (2.3006)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:50:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:16:49 lr 0.000045	 wd 0.0000	time 0.3938 (0.4584)	loss 0.7793 (0.8188)	grad_norm 2.0661 (2.2795)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:51:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:15:48 lr 0.000045	 wd 0.0000	time 0.3927 (0.4511)	loss 0.9067 (0.8191)	grad_norm 1.9419 (2.2783)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:52:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:14:55 lr 0.000044	 wd 0.0000	time 0.3911 (0.4471)	loss 0.7324 (0.8206)	grad_norm 2.1219 (2.2658)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:52:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:14:04 lr 0.000044	 wd 0.0000	time 0.3940 (0.4441)	loss 0.8882 (0.8231)	grad_norm 3.1587 (2.2647)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:53:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:13:16 lr 0.000044	 wd 0.0000	time 0.4157 (0.4422)	loss 0.7793 (0.8227)	grad_norm 2.7750 (2.2713)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:54:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:12:30 lr 0.000044	 wd 0.0000	time 0.3912 (0.4408)	loss 0.7607 (0.8222)	grad_norm 1.7895 (2.2634)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:54:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:11:44 lr 0.000043	 wd 0.0000	time 0.4248 (0.4396)	loss 0.7275 (0.8219)	grad_norm 2.2147 (2.2585)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:55:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:10:58 lr 0.000043	 wd 0.0000	time 0.4003 (0.4385)	loss 0.7490 (0.8212)	grad_norm 1.9998 (2.2660)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:56:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:10:13 lr 0.000043	 wd 0.0000	time 0.3974 (0.4378)	loss 0.7798 (0.8215)	grad_norm 2.2959 (2.2699)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:57:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:09:29 lr 0.000043	 wd 0.0000	time 0.3950 (0.4372)	loss 0.8965 (0.8224)	grad_norm 1.9730 (2.2712)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:57:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:08:45 lr 0.000043	 wd 0.0000	time 0.3811 (0.4368)	loss 0.7666 (0.8217)	grad_norm 2.4135 (2.2670)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:58:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:08:00 lr 0.000042	 wd 0.0000	time 0.3991 (0.4364)	loss 0.7856 (0.8214)	grad_norm 2.0992 (2.2701)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 12:59:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:07:17 lr 0.000042	 wd 0.0000	time 0.3991 (0.4362)	loss 0.7886 (0.8216)	grad_norm 1.8863 (2.2737)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 13:00:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:06:33 lr 0.000042	 wd 0.0000	time 0.4079 (0.4360)	loss 0.8179 (0.8209)	grad_norm 1.9715 (2.2700)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 13:00:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:05:49 lr 0.000042	 wd 0.0000	time 0.3949 (0.4358)	loss 0.7700 (0.8217)	grad_norm 2.6024 (2.2702)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 13:01:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:05:05 lr 0.000042	 wd 0.0000	time 0.3953 (0.4355)	loss 0.9062 (0.8214)	grad_norm 2.0110 (2.2742)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 13:02:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:04:22 lr 0.000041	 wd 0.0000	time 0.3974 (0.4353)	loss 0.8647 (0.8214)	grad_norm 2.3110 (2.2746)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 13:02:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:03:38 lr 0.000041	 wd 0.0000	time 0.3892 (0.4352)	loss 0.8281 (0.8217)	grad_norm 2.4228 (2.2742)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 13:03:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:54 lr 0.000041	 wd 0.0000	time 0.4070 (0.4350)	loss 0.7383 (0.8213)	grad_norm 2.2569 (2.2728)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 13:04:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:02:11 lr 0.000041	 wd 0.0000	time 0.3867 (0.4348)	loss 0.8652 (0.8212)	grad_norm 2.1885 (inf)	loss_scale 8192.0000 (16458.4389)	mem 15686MB
[2024-07-02 13:05:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:27 lr 0.000041	 wd 0.0000	time 0.3851 (0.4348)	loss 0.7549 (0.8209)	grad_norm 2.2605 (inf)	loss_scale 8192.0000 (16099.1847)	mem 15686MB
[2024-07-02 13:05:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:44 lr 0.000040	 wd 0.0000	time 0.3749 (0.4346)	loss 0.7256 (0.8213)	grad_norm 2.1750 (inf)	loss_scale 8192.0000 (15769.8559)	mem 15686MB
[2024-07-02 13:06:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.3854 (0.4342)	loss 0.8296 (0.8206)	grad_norm 2.2119 (inf)	loss_scale 8192.0000 (15466.8629)	mem 15686MB
[2024-07-02 13:06:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 16 training takes 0:18:08
[2024-07-02 13:06:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.280 (12.280)	Loss 0.3875 (0.3875)	Acc@1 91.211 (91.211)	Acc@5 98.438 (98.438)	Mem 15686MB
[2024-07-02 13:06:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.254 Acc@5 97.490
[2024-07-02 13:06:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 13:06:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 13:07:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:14:45 lr 0.000040	 wd 0.0000	time 11.8647 (11.8647)	loss 0.8394 (0.8394)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:07:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:21:48 lr 0.000040	 wd 0.0000	time 0.4089 (0.5447)	loss 0.8823 (0.8277)	grad_norm 2.4328 (2.2342)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:08:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:18:44 lr 0.000040	 wd 0.0000	time 0.4238 (0.4883)	loss 0.9600 (0.8283)	grad_norm 2.2317 (2.2398)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:09:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:17:12 lr 0.000040	 wd 0.0000	time 0.3978 (0.4688)	loss 0.8481 (0.8261)	grad_norm 2.2942 (2.2535)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:10:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:16:04 lr 0.000039	 wd 0.0000	time 0.4117 (0.4587)	loss 0.7925 (0.8241)	grad_norm 1.9966 (2.2624)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:10:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:15:06 lr 0.000039	 wd 0.0000	time 0.3963 (0.4526)	loss 0.7578 (0.8206)	grad_norm 2.9253 (2.2678)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:11:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:14:14 lr 0.000039	 wd 0.0000	time 0.3963 (0.4491)	loss 0.8184 (0.8207)	grad_norm 2.6122 (2.2801)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:12:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:13:23 lr 0.000039	 wd 0.0000	time 0.3848 (0.4459)	loss 0.8008 (0.8203)	grad_norm 2.7387 (2.2811)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:12:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:12:35 lr 0.000039	 wd 0.0000	time 0.4171 (0.4441)	loss 0.8413 (0.8205)	grad_norm 1.9828 (2.2857)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:13:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:11:48 lr 0.000038	 wd 0.0000	time 0.3783 (0.4424)	loss 0.8086 (0.8208)	grad_norm 1.8764 (2.2851)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:14:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:11:02 lr 0.000038	 wd 0.0000	time 0.4075 (0.4410)	loss 0.7949 (0.8196)	grad_norm 2.0952 (2.2883)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:15:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:10:17 lr 0.000038	 wd 0.0000	time 0.3880 (0.4401)	loss 0.8242 (0.8197)	grad_norm 2.1987 (2.2798)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:15:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:09:31 lr 0.000038	 wd 0.0000	time 0.3711 (0.4392)	loss 0.8555 (0.8200)	grad_norm 1.9538 (2.2782)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:16:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:08:47 lr 0.000038	 wd 0.0000	time 0.3940 (0.4385)	loss 0.8179 (0.8199)	grad_norm 2.2855 (2.2840)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:17:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:08:02 lr 0.000037	 wd 0.0000	time 0.3784 (0.4380)	loss 0.7417 (0.8205)	grad_norm 2.2804 (2.2876)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:17:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:07:18 lr 0.000037	 wd 0.0000	time 0.4038 (0.4375)	loss 0.8301 (0.8202)	grad_norm 1.8194 (2.2918)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:18:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:06:34 lr 0.000037	 wd 0.0000	time 0.3898 (0.4371)	loss 0.8096 (0.8196)	grad_norm 2.5103 (2.2942)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:19:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:05:50 lr 0.000037	 wd 0.0000	time 0.3937 (0.4369)	loss 0.9819 (0.8187)	grad_norm 2.8535 (2.2950)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:20:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:05:06 lr 0.000037	 wd 0.0000	time 0.4315 (0.4367)	loss 0.8687 (0.8191)	grad_norm 1.8966 (2.2947)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:20:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:04:22 lr 0.000036	 wd 0.0000	time 0.3897 (0.4364)	loss 0.8413 (0.8190)	grad_norm 2.4846 (2.2947)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:21:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:03:38 lr 0.000036	 wd 0.0000	time 0.4168 (0.4360)	loss 0.7412 (0.8186)	grad_norm 2.3872 (2.3019)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 13:22:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:55 lr 0.000036	 wd 0.0000	time 0.3934 (0.4360)	loss 0.7812 (0.8180)	grad_norm 1.9183 (inf)	loss_scale 4096.0000 (8000.9443)	mem 15686MB
[2024-07-02 13:22:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:11 lr 0.000036	 wd 0.0000	time 0.3984 (0.4357)	loss 0.8037 (0.8185)	grad_norm 2.1677 (inf)	loss_scale 4096.0000 (7823.5275)	mem 15686MB
[2024-07-02 13:23:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:27 lr 0.000036	 wd 0.0000	time 0.4043 (0.4356)	loss 0.8037 (0.8183)	grad_norm 2.4349 (inf)	loss_scale 4096.0000 (7661.5315)	mem 15686MB
[2024-07-02 13:24:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:44 lr 0.000035	 wd 0.0000	time 0.4248 (0.4355)	loss 0.8438 (0.8181)	grad_norm 2.2435 (inf)	loss_scale 4096.0000 (7513.0296)	mem 15686MB
[2024-07-02 13:25:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.4002 (0.4350)	loss 0.7271 (0.8180)	grad_norm 2.4598 (inf)	loss_scale 4096.0000 (7376.4030)	mem 15686MB
[2024-07-02 13:25:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 17 training takes 0:18:10
[2024-07-02 13:25:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 11.963 (11.963)	Loss 0.3789 (0.3789)	Acc@1 91.211 (91.211)	Acc@5 98.438 (98.438)	Mem 15686MB
[2024-07-02 13:25:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.220 Acc@5 97.452
[2024-07-02 13:25:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-02 13:25:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 13:25:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:35:52 lr 0.000035	 wd 0.0000	time 12.3713 (12.3713)	loss 0.8657 (0.8657)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:26:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:22:00 lr 0.000035	 wd 0.0000	time 0.3865 (0.5497)	loss 0.8130 (0.8188)	grad_norm 2.0943 (2.3029)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:27:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:18:45 lr 0.000035	 wd 0.0000	time 0.3919 (0.4889)	loss 0.8218 (0.8094)	grad_norm 2.2969 (2.2850)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:27:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:17:14 lr 0.000035	 wd 0.0000	time 0.4269 (0.4697)	loss 0.8574 (0.8107)	grad_norm 2.0801 (2.2850)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:28:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:16:06 lr 0.000034	 wd 0.0000	time 0.4301 (0.4596)	loss 0.7388 (0.8107)	grad_norm 2.5092 (2.3131)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:29:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:15:07 lr 0.000034	 wd 0.0000	time 0.4149 (0.4534)	loss 0.7983 (0.8101)	grad_norm 1.9470 (2.3002)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:30:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:14:14 lr 0.000034	 wd 0.0000	time 0.4006 (0.4494)	loss 0.7935 (0.8104)	grad_norm 2.1124 (2.3128)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:30:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:13:24 lr 0.000034	 wd 0.0000	time 0.3914 (0.4465)	loss 0.9702 (0.8098)	grad_norm 1.9148 (2.2935)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:31:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:12:36 lr 0.000034	 wd 0.0000	time 0.3876 (0.4445)	loss 0.6636 (0.8086)	grad_norm 1.9930 (2.2993)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:32:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:11:49 lr 0.000033	 wd 0.0000	time 0.3819 (0.4430)	loss 0.7134 (0.8086)	grad_norm 2.2561 (2.2957)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:32:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:11:03 lr 0.000033	 wd 0.0000	time 0.4223 (0.4417)	loss 1.0020 (0.8086)	grad_norm 1.8988 (2.2948)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:33:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:10:18 lr 0.000033	 wd 0.0000	time 0.3989 (0.4408)	loss 0.8687 (0.8104)	grad_norm 2.2077 (2.2954)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:34:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:09:32 lr 0.000033	 wd 0.0000	time 0.3879 (0.4399)	loss 1.0029 (0.8101)	grad_norm 2.0878 (2.2897)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:35:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:08:47 lr 0.000033	 wd 0.0000	time 0.4032 (0.4392)	loss 0.8257 (0.8101)	grad_norm 2.2650 (2.2899)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:35:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:08:03 lr 0.000032	 wd 0.0000	time 0.4177 (0.4387)	loss 0.7827 (0.8096)	grad_norm 2.8989 (2.2871)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:36:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:07:18 lr 0.000032	 wd 0.0000	time 0.3969 (0.4380)	loss 0.7852 (0.8098)	grad_norm 2.7403 (2.2878)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:37:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:06:34 lr 0.000032	 wd 0.0000	time 0.3913 (0.4375)	loss 0.8862 (0.8099)	grad_norm 2.8541 (2.2890)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:37:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:05:50 lr 0.000032	 wd 0.0000	time 0.4021 (0.4372)	loss 0.7271 (0.8098)	grad_norm 3.4698 (2.2965)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:38:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:05:06 lr 0.000032	 wd 0.0000	time 0.4095 (0.4368)	loss 0.7769 (0.8096)	grad_norm 2.2879 (2.2993)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:39:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:04:22 lr 0.000032	 wd 0.0000	time 0.3938 (0.4365)	loss 0.8242 (0.8087)	grad_norm 2.1695 (2.3030)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:40:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:03:39 lr 0.000031	 wd 0.0000	time 0.4158 (0.4363)	loss 0.7422 (0.8094)	grad_norm 2.5065 (2.3034)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:40:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:55 lr 0.000031	 wd 0.0000	time 0.3900 (0.4361)	loss 0.8491 (0.8093)	grad_norm 2.3202 (2.3037)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:41:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:02:11 lr 0.000031	 wd 0.0000	time 0.4062 (0.4359)	loss 0.7690 (0.8091)	grad_norm 2.7072 (2.3041)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:42:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:28 lr 0.000031	 wd 0.0000	time 0.3756 (0.4358)	loss 0.7588 (0.8089)	grad_norm 2.0682 (2.3025)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:43:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:44 lr 0.000031	 wd 0.0000	time 0.3811 (0.4357)	loss 0.7749 (0.8092)	grad_norm 2.8743 (2.3015)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:43:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000030	 wd 0.0000	time 0.3870 (0.4351)	loss 0.6924 (0.8093)	grad_norm 2.5796 (2.3011)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:43:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 18 training takes 0:18:10
[2024-07-02 13:43:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 11.672 (11.672)	Loss 0.3828 (0.3828)	Acc@1 91.602 (91.602)	Acc@5 98.438 (98.438)	Mem 15686MB
[2024-07-02 13:44:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.270 Acc@5 97.454
[2024-07-02 13:44:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 13:44:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 13:44:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:52:12 lr 0.000030	 wd 0.0000	time 11.3241 (11.3241)	loss 0.8828 (0.8828)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:45:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:21:36 lr 0.000030	 wd 0.0000	time 0.4097 (0.5397)	loss 0.8042 (0.8053)	grad_norm 1.9433 (2.2827)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:45:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:18:33 lr 0.000030	 wd 0.0000	time 0.3891 (0.4836)	loss 0.7148 (0.8009)	grad_norm 2.0990 (2.2677)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:46:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:17:04 lr 0.000030	 wd 0.0000	time 0.3451 (0.4652)	loss 0.7158 (0.8057)	grad_norm 2.0047 (2.3008)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:47:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:15:58 lr 0.000030	 wd 0.0000	time 0.4139 (0.4560)	loss 0.7192 (0.8071)	grad_norm 1.9343 (2.2986)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:47:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:15:02 lr 0.000029	 wd 0.0000	time 0.4209 (0.4508)	loss 0.8413 (0.8087)	grad_norm 2.9590 (2.2922)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:48:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:14:10 lr 0.000029	 wd 0.0000	time 0.3924 (0.4471)	loss 0.9370 (0.8091)	grad_norm 2.6085 (2.3002)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:49:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:13:21 lr 0.000029	 wd 0.0000	time 0.3956 (0.4447)	loss 0.8672 (0.8094)	grad_norm 2.1146 (2.3006)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:50:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:12:33 lr 0.000029	 wd 0.0000	time 0.3745 (0.4428)	loss 0.8408 (0.8102)	grad_norm 2.3667 (2.3072)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:50:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:11:47 lr 0.000029	 wd 0.0000	time 0.3628 (0.4413)	loss 0.7891 (0.8089)	grad_norm 2.4109 (2.3074)	loss_scale 4096.0000 (4096.0000)	mem 15686MB
[2024-07-02 13:51:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:11:01 lr 0.000028	 wd 0.0000	time 0.4193 (0.4403)	loss 0.7690 (0.8105)	grad_norm 2.5356 (2.3085)	loss_scale 8192.0000 (4104.1838)	mem 15686MB
[2024-07-02 13:52:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:10:16 lr 0.000028	 wd 0.0000	time 0.3998 (0.4394)	loss 0.6841 (0.8109)	grad_norm 2.5052 (2.3165)	loss_scale 8192.0000 (4475.4659)	mem 15686MB
[2024-07-02 13:52:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:09:31 lr 0.000028	 wd 0.0000	time 0.4455 (0.4388)	loss 0.7891 (0.8111)	grad_norm 2.0533 (2.3228)	loss_scale 8192.0000 (4784.9192)	mem 15686MB
[2024-07-02 13:53:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:08:46 lr 0.000028	 wd 0.0000	time 0.3919 (0.4381)	loss 0.7827 (0.8107)	grad_norm 2.3813 (2.3243)	loss_scale 8192.0000 (5046.8009)	mem 15686MB
[2024-07-02 13:54:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:08:02 lr 0.000028	 wd 0.0000	time 0.3532 (0.4376)	loss 0.8555 (0.8106)	grad_norm 2.4293 (2.3248)	loss_scale 8192.0000 (5271.2976)	mem 15686MB
[2024-07-02 13:55:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:07:18 lr 0.000028	 wd 0.0000	time 0.4016 (0.4373)	loss 0.8066 (0.8105)	grad_norm 2.4653 (2.3236)	loss_scale 8192.0000 (5465.8814)	mem 15686MB
[2024-07-02 13:55:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:06:34 lr 0.000027	 wd 0.0000	time 0.4166 (0.4370)	loss 0.8428 (0.8101)	grad_norm 2.3437 (2.3182)	loss_scale 8192.0000 (5636.1574)	mem 15686MB
[2024-07-02 13:56:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:05:50 lr 0.000027	 wd 0.0000	time 0.4251 (0.4366)	loss 0.7676 (0.8101)	grad_norm 2.1037 (2.3182)	loss_scale 8192.0000 (5786.4127)	mem 15686MB
[2024-07-02 13:57:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:05:06 lr 0.000027	 wd 0.0000	time 0.4229 (0.4363)	loss 0.8398 (0.8108)	grad_norm 2.1483 (2.3213)	loss_scale 8192.0000 (5919.9822)	mem 15686MB
[2024-07-02 13:57:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:04:22 lr 0.000027	 wd 0.0000	time 0.4129 (0.4359)	loss 0.8521 (0.8111)	grad_norm 2.7471 (2.3216)	loss_scale 8192.0000 (6039.4992)	mem 15686MB
[2024-07-02 13:58:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:03:38 lr 0.000027	 wd 0.0000	time 0.3675 (0.4356)	loss 0.8608 (0.8112)	grad_norm 2.0713 (2.3203)	loss_scale 8192.0000 (6147.0705)	mem 15686MB
[2024-07-02 13:59:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:55 lr 0.000026	 wd 0.0000	time 0.3998 (0.4355)	loss 0.9248 (0.8111)	grad_norm 1.8589 (2.3237)	loss_scale 8192.0000 (6244.4017)	mem 15686MB
[2024-07-02 14:00:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:02:11 lr 0.000026	 wd 0.0000	time 0.4409 (0.4352)	loss 0.7783 (0.8101)	grad_norm 3.8599 (2.3248)	loss_scale 8192.0000 (6332.8887)	mem 15686MB
[2024-07-02 14:00:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:27 lr 0.000026	 wd 0.0000	time 0.4258 (0.4351)	loss 0.8682 (0.8103)	grad_norm 2.1487 (2.3248)	loss_scale 8192.0000 (6413.6845)	mem 15686MB
[2024-07-02 14:01:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:44 lr 0.000026	 wd 0.0000	time 0.4309 (0.4349)	loss 0.8433 (0.8095)	grad_norm 2.2407 (2.3235)	loss_scale 8192.0000 (6487.7501)	mem 15686MB
[2024-07-02 14:02:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.3887 (0.4344)	loss 0.8438 (0.8094)	grad_norm 2.5361 (2.3253)	loss_scale 8192.0000 (6555.8928)	mem 15686MB
[2024-07-02 14:02:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 19 training takes 0:18:09
[2024-07-02 14:02:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.571 (12.571)	Loss 0.3694 (0.3694)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 14:02:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.300 Acc@5 97.492
[2024-07-02 14:02:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-02 14:02:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-02 14:02:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:22:56 lr 0.000026	 wd 0.0000	time 10.6219 (10.6219)	loss 0.8213 (0.8213)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:03:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:21:40 lr 0.000026	 wd 0.0000	time 0.3709 (0.5412)	loss 0.7505 (0.8092)	grad_norm 2.2194 (2.3703)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:04:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:18:36 lr 0.000025	 wd 0.0000	time 0.3756 (0.4851)	loss 0.9424 (0.8015)	grad_norm 2.3598 (2.3604)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:05:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:17:08 lr 0.000025	 wd 0.0000	time 0.3959 (0.4672)	loss 0.7686 (0.8027)	grad_norm 3.4493 (2.3607)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:05:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:16:02 lr 0.000025	 wd 0.0000	time 0.3722 (0.4577)	loss 0.6523 (0.8037)	grad_norm 2.1922 (2.3548)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:06:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:15:04 lr 0.000025	 wd 0.0000	time 0.4027 (0.4518)	loss 0.7993 (0.8050)	grad_norm 2.3741 (2.3448)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:07:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:14:12 lr 0.000025	 wd 0.0000	time 0.4534 (0.4481)	loss 0.7983 (0.8044)	grad_norm 2.4408 (2.3354)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:07:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:13:22 lr 0.000025	 wd 0.0000	time 0.3826 (0.4452)	loss 0.7017 (0.8044)	grad_norm 2.3295 (2.3219)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:08:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:12:34 lr 0.000024	 wd 0.0000	time 0.3914 (0.4433)	loss 0.9150 (0.8048)	grad_norm 2.1644 (2.3220)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:09:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:11:48 lr 0.000024	 wd 0.0000	time 0.3892 (0.4420)	loss 0.8218 (0.8036)	grad_norm 1.8073 (2.3275)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:10:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:11:01 lr 0.000024	 wd 0.0000	time 0.4099 (0.4407)	loss 0.8994 (0.8042)	grad_norm 2.1183 (2.3272)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:10:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:10:16 lr 0.000024	 wd 0.0000	time 0.3959 (0.4395)	loss 0.8037 (0.8035)	grad_norm 2.1294 (2.3204)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:11:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:09:31 lr 0.000024	 wd 0.0000	time 0.3835 (0.4389)	loss 0.7480 (0.8042)	grad_norm 2.1247 (2.3122)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:12:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:08:46 lr 0.000023	 wd 0.0000	time 0.3914 (0.4382)	loss 0.8003 (0.8046)	grad_norm 2.5669 (2.3114)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:12:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:08:02 lr 0.000023	 wd 0.0000	time 0.4021 (0.4377)	loss 0.7720 (0.8045)	grad_norm 2.2372 (2.3177)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:13:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:07:18 lr 0.000023	 wd 0.0000	time 0.4486 (0.4372)	loss 0.7642 (0.8055)	grad_norm 2.2821 (2.3197)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:14:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:06:34 lr 0.000023	 wd 0.0000	time 0.3917 (0.4368)	loss 0.8667 (0.8047)	grad_norm 2.5115 (2.3189)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:15:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:05:50 lr 0.000023	 wd 0.0000	time 0.4147 (0.4365)	loss 0.8110 (0.8051)	grad_norm 2.2297 (2.3174)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:15:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:05:06 lr 0.000023	 wd 0.0000	time 0.3982 (0.4362)	loss 0.7446 (0.8055)	grad_norm 3.2912 (2.3223)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:16:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:04:22 lr 0.000022	 wd 0.0000	time 0.4152 (0.4359)	loss 0.7227 (0.8058)	grad_norm 2.1035 (2.3231)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:17:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:03:38 lr 0.000022	 wd 0.0000	time 0.3820 (0.4358)	loss 0.8101 (0.8058)	grad_norm 2.4344 (2.3224)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:18:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:55 lr 0.000022	 wd 0.0000	time 0.4003 (0.4356)	loss 0.7920 (0.8055)	grad_norm 3.0876 (2.3232)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:18:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:02:11 lr 0.000022	 wd 0.0000	time 0.3801 (0.4354)	loss 0.8604 (0.8058)	grad_norm 1.8909 (2.3189)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:19:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:27 lr 0.000022	 wd 0.0000	time 0.3940 (0.4353)	loss 0.7949 (0.8064)	grad_norm 1.9849 (2.3215)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:20:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:44 lr 0.000022	 wd 0.0000	time 0.3983 (0.4352)	loss 0.8369 (0.8066)	grad_norm 2.0111 (2.3213)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:20:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.3884 (0.4346)	loss 0.8599 (0.8069)	grad_norm 2.2123 (2.3208)	loss_scale 16384.0000 (8205.1020)	mem 15686MB
[2024-07-02 14:20:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 20 training takes 0:18:09
[2024-07-02 14:21:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.259 (12.259)	Loss 0.3787 (0.3787)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 14:21:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.358 Acc@5 97.520
[2024-07-02 14:21:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-02 14:21:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.36%
[2024-07-02 14:21:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 14:21:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 14:21:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][0/2502]	eta 5:54:28 lr 0.000021	 wd 0.0000	time 8.5007 (8.5007)	loss 0.9287 (0.9287)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:22:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:20:20 lr 0.000021	 wd 0.0000	time 0.4125 (0.5080)	loss 0.8628 (0.8035)	grad_norm 2.1032 (2.2639)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:22:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:17:57 lr 0.000021	 wd 0.0000	time 0.3966 (0.4683)	loss 0.8569 (0.8030)	grad_norm 2.4087 (2.2717)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:23:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:16:42 lr 0.000021	 wd 0.0000	time 0.3810 (0.4550)	loss 0.7876 (0.8063)	grad_norm 1.9023 (2.2826)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:24:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:15:43 lr 0.000021	 wd 0.0000	time 0.3929 (0.4491)	loss 0.7915 (0.8050)	grad_norm 1.7495 (2.2786)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:25:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:14:52 lr 0.000021	 wd 0.0000	time 0.3874 (0.4457)	loss 0.8994 (0.8039)	grad_norm 2.6108 (2.2787)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:25:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:14:02 lr 0.000020	 wd 0.0000	time 0.4015 (0.4431)	loss 0.7500 (0.8027)	grad_norm 2.2282 (2.2781)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:26:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:13:15 lr 0.000020	 wd 0.0000	time 0.4143 (0.4414)	loss 0.9185 (0.8018)	grad_norm 2.6930 (2.2993)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:27:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:12:28 lr 0.000020	 wd 0.0000	time 0.4244 (0.4400)	loss 0.8027 (0.8018)	grad_norm 2.0442 (2.2926)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:28:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:11:42 lr 0.000020	 wd 0.0000	time 0.3729 (0.4387)	loss 0.9365 (0.8034)	grad_norm 2.1566 (2.2956)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:28:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:10:57 lr 0.000020	 wd 0.0000	time 0.3991 (0.4379)	loss 0.7598 (0.8027)	grad_norm 2.0129 (2.2968)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:29:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:10:12 lr 0.000020	 wd 0.0000	time 0.4040 (0.4371)	loss 0.7588 (0.8025)	grad_norm 2.1136 (2.3011)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:30:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:09:28 lr 0.000019	 wd 0.0000	time 0.3956 (0.4365)	loss 0.7095 (0.8025)	grad_norm 3.0171 (2.3039)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:30:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:08:43 lr 0.000019	 wd 0.0000	time 0.4062 (0.4359)	loss 0.8657 (0.8026)	grad_norm 1.7776 (2.3127)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:31:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:07:59 lr 0.000019	 wd 0.0000	time 0.3976 (0.4355)	loss 0.7266 (0.8029)	grad_norm 2.3853 (2.3106)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:32:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:07:16 lr 0.000019	 wd 0.0000	time 0.4010 (0.4351)	loss 0.7896 (0.8022)	grad_norm 1.9959 (2.3141)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:33:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:06:32 lr 0.000019	 wd 0.0000	time 0.4151 (0.4350)	loss 0.8145 (0.8029)	grad_norm 2.2126 (2.3118)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:33:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:05:48 lr 0.000019	 wd 0.0000	time 0.4233 (0.4347)	loss 0.6694 (0.8030)	grad_norm 2.0307 (2.3120)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:34:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:05:04 lr 0.000018	 wd 0.0000	time 0.3821 (0.4344)	loss 0.7207 (0.8027)	grad_norm 2.2340 (2.3118)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:35:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:04:21 lr 0.000018	 wd 0.0000	time 0.4030 (0.4342)	loss 0.7954 (0.8024)	grad_norm 2.2416 (2.3125)	loss_scale 16384.0000 (16384.0000)	mem 15686MB
[2024-07-02 14:35:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:03:37 lr 0.000018	 wd 0.0000	time 0.3866 (0.4340)	loss 0.7700 (0.8023)	grad_norm 2.2182 (inf)	loss_scale 8192.0000 (16015.5442)	mem 15686MB
[2024-07-02 14:36:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:54 lr 0.000018	 wd 0.0000	time 0.3781 (0.4339)	loss 0.8120 (0.8019)	grad_norm 2.3483 (inf)	loss_scale 8192.0000 (15643.1718)	mem 15686MB
[2024-07-02 14:37:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:02:11 lr 0.000018	 wd 0.0000	time 0.3732 (0.4338)	loss 0.7568 (0.8022)	grad_norm 2.9489 (inf)	loss_scale 8192.0000 (15304.6361)	mem 15686MB
[2024-07-02 14:38:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:27 lr 0.000018	 wd 0.0000	time 0.4148 (0.4337)	loss 0.8501 (0.8022)	grad_norm 2.2674 (inf)	loss_scale 8192.0000 (14995.5254)	mem 15686MB
[2024-07-02 14:38:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:44 lr 0.000018	 wd 0.0000	time 0.3910 (0.4337)	loss 0.8462 (0.8024)	grad_norm 2.2139 (inf)	loss_scale 8192.0000 (14712.1633)	mem 15686MB
[2024-07-02 14:39:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0000	time 0.3908 (0.4332)	loss 0.9297 (0.8031)	grad_norm 1.9820 (inf)	loss_scale 8192.0000 (14451.4610)	mem 15686MB
[2024-07-02 14:39:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 21 training takes 0:18:06
[2024-07-02 14:39:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 11.819 (11.819)	Loss 0.3826 (0.3826)	Acc@1 91.406 (91.406)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 14:39:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.362 Acc@5 97.462
[2024-07-02 14:39:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-02 14:39:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.36%
[2024-07-02 14:39:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 14:40:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 14:40:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][0/2502]	eta 5:45:39 lr 0.000017	 wd 0.0000	time 8.2893 (8.2893)	loss 0.9272 (0.9272)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:40:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:20:18 lr 0.000017	 wd 0.0000	time 0.3969 (0.5073)	loss 0.8291 (0.8110)	grad_norm 2.8436 (2.3533)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:41:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:17:54 lr 0.000017	 wd 0.0000	time 0.3926 (0.4668)	loss 0.7759 (0.8014)	grad_norm 3.1477 (2.3422)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:42:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:16:38 lr 0.000017	 wd 0.0000	time 0.3959 (0.4533)	loss 0.9741 (0.8006)	grad_norm 2.2810 (2.3618)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:43:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:15:39 lr 0.000017	 wd 0.0000	time 0.4069 (0.4470)	loss 0.8711 (0.7996)	grad_norm 2.1553 (2.3594)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:43:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:14:47 lr 0.000017	 wd 0.0000	time 0.3882 (0.4432)	loss 0.7129 (0.7972)	grad_norm 2.5431 (2.3483)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:44:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:13:59 lr 0.000016	 wd 0.0000	time 0.3907 (0.4412)	loss 0.7388 (0.7994)	grad_norm 2.3289 (2.3361)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:45:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:13:12 lr 0.000016	 wd 0.0000	time 0.4208 (0.4396)	loss 0.8091 (0.7986)	grad_norm 2.6323 (2.3301)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:45:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:12:26 lr 0.000016	 wd 0.0000	time 0.3563 (0.4385)	loss 0.8091 (0.7975)	grad_norm 2.2112 (2.3217)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:46:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:11:41 lr 0.000016	 wd 0.0000	time 0.3820 (0.4377)	loss 0.7905 (0.7972)	grad_norm 2.4261 (2.3175)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:47:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:10:56 lr 0.000016	 wd 0.0000	time 0.3868 (0.4368)	loss 0.7021 (0.7964)	grad_norm 2.1084 (2.3223)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:48:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:10:11 lr 0.000016	 wd 0.0000	time 0.3768 (0.4363)	loss 0.7485 (0.7973)	grad_norm 3.0072 (2.3207)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:48:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:09:27 lr 0.000016	 wd 0.0000	time 0.3965 (0.4359)	loss 0.7900 (0.7974)	grad_norm 2.4472 (2.3195)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:49:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:08:43 lr 0.000015	 wd 0.0000	time 0.4183 (0.4357)	loss 0.8853 (0.7976)	grad_norm 2.0092 (2.3188)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:50:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:07:59 lr 0.000015	 wd 0.0000	time 0.3836 (0.4353)	loss 0.7778 (0.7980)	grad_norm 2.1931 (2.3247)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:50:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:07:15 lr 0.000015	 wd 0.0000	time 0.3969 (0.4351)	loss 0.6621 (0.7991)	grad_norm 2.2539 (2.3238)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:51:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:06:32 lr 0.000015	 wd 0.0000	time 0.3963 (0.4348)	loss 0.8521 (0.7991)	grad_norm 2.0708 (2.3265)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:52:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:05:48 lr 0.000015	 wd 0.0000	time 0.4334 (0.4346)	loss 0.7393 (0.7992)	grad_norm 2.4038 (2.3245)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:53:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:05:04 lr 0.000015	 wd 0.0000	time 0.3911 (0.4344)	loss 1.0498 (0.7995)	grad_norm 2.1440 (2.3274)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:53:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:04:21 lr 0.000015	 wd 0.0000	time 0.4124 (0.4342)	loss 0.7637 (0.7997)	grad_norm 2.5887 (2.3273)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:54:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:03:37 lr 0.000014	 wd 0.0000	time 0.4383 (0.4340)	loss 0.6987 (0.7994)	grad_norm 2.3023 (2.3276)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:55:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:54 lr 0.000014	 wd 0.0000	time 0.4020 (0.4338)	loss 0.8198 (0.7997)	grad_norm 2.2158 (2.3266)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:55:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:10 lr 0.000014	 wd 0.0000	time 0.4276 (0.4337)	loss 0.7095 (0.8001)	grad_norm 2.0956 (2.3307)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:56:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:27 lr 0.000014	 wd 0.0000	time 0.3949 (0.4336)	loss 0.8579 (0.8000)	grad_norm 2.3855 (2.3322)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:57:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:44 lr 0.000014	 wd 0.0000	time 0.3984 (0.4335)	loss 0.7798 (0.8002)	grad_norm 2.2577 (2.3337)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:58:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.3875 (0.4330)	loss 0.8179 (0.8000)	grad_norm 2.5609 (2.3355)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:58:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 22 training takes 0:18:05
[2024-07-02 14:58:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 11.364 (11.364)	Loss 0.3782 (0.3782)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 14:58:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.396 Acc@5 97.528
[2024-07-02 14:58:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-02 14:58:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.40%
[2024-07-02 14:58:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 14:58:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 14:58:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][0/2502]	eta 6:01:10 lr 0.000014	 wd 0.0000	time 8.6613 (8.6613)	loss 0.7949 (0.7949)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 14:59:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:20:36 lr 0.000014	 wd 0.0000	time 0.4057 (0.5146)	loss 0.7651 (0.8019)	grad_norm 2.0700 (2.3028)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:00:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:18:05 lr 0.000013	 wd 0.0000	time 0.3989 (0.4716)	loss 0.8477 (0.7957)	grad_norm 2.4164 (2.3047)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:00:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:16:46 lr 0.000013	 wd 0.0000	time 0.3905 (0.4573)	loss 0.7734 (0.7971)	grad_norm 2.4035 (2.3188)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:01:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:15:45 lr 0.000013	 wd 0.0000	time 0.3974 (0.4499)	loss 0.7793 (0.7991)	grad_norm 2.5230 (2.3074)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:02:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:14:53 lr 0.000013	 wd 0.0000	time 0.4002 (0.4461)	loss 0.8740 (0.8006)	grad_norm 2.3894 (2.3219)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:03:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:14:03 lr 0.000013	 wd 0.0000	time 0.4227 (0.4433)	loss 0.9141 (0.7996)	grad_norm 2.0931 (2.3215)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:03:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:13:15 lr 0.000013	 wd 0.0000	time 0.3903 (0.4413)	loss 0.7651 (0.7990)	grad_norm 2.2169 (2.3300)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:04:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:12:28 lr 0.000013	 wd 0.0000	time 0.4334 (0.4395)	loss 0.9658 (0.7983)	grad_norm 2.0164 (2.3285)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:05:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:11:42 lr 0.000012	 wd 0.0000	time 0.4193 (0.4384)	loss 1.0244 (0.7990)	grad_norm 2.5625 (2.3355)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:05:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:10:57 lr 0.000012	 wd 0.0000	time 0.4093 (0.4374)	loss 0.7417 (0.7974)	grad_norm 2.1806 (2.3363)	loss_scale 16384.0000 (8961.2787)	mem 15686MB
[2024-07-02 15:06:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:10:12 lr 0.000012	 wd 0.0000	time 0.3925 (0.4369)	loss 0.7759 (0.7977)	grad_norm 2.9912 (inf)	loss_scale 8192.0000 (8995.5749)	mem 15686MB
[2024-07-02 15:07:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:09:27 lr 0.000012	 wd 0.0000	time 0.4206 (0.4362)	loss 0.7104 (0.7980)	grad_norm 2.0894 (inf)	loss_scale 8192.0000 (8928.6661)	mem 15686MB
[2024-07-02 15:08:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:08:43 lr 0.000012	 wd 0.0000	time 0.4254 (0.4358)	loss 0.9766 (0.7968)	grad_norm 2.2208 (inf)	loss_scale 8192.0000 (8872.0430)	mem 15686MB
[2024-07-02 15:08:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:07:59 lr 0.000012	 wd 0.0000	time 0.4005 (0.4354)	loss 0.8218 (0.7972)	grad_norm 2.3257 (inf)	loss_scale 8192.0000 (8823.5032)	mem 15686MB
[2024-07-02 15:09:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:07:16 lr 0.000012	 wd 0.0000	time 0.3963 (0.4352)	loss 0.8442 (0.7978)	grad_norm 2.4999 (inf)	loss_scale 8192.0000 (8781.4310)	mem 15686MB
[2024-07-02 15:10:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:06:32 lr 0.000012	 wd 0.0000	time 0.3964 (0.4349)	loss 0.7954 (0.7978)	grad_norm 2.5304 (inf)	loss_scale 8192.0000 (8744.6146)	mem 15686MB
[2024-07-02 15:10:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:05:48 lr 0.000011	 wd 0.0000	time 0.3845 (0.4348)	loss 0.7378 (0.7978)	grad_norm 2.1336 (inf)	loss_scale 8192.0000 (8712.1270)	mem 15686MB
[2024-07-02 15:11:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:05:05 lr 0.000011	 wd 0.0000	time 0.3807 (0.4346)	loss 0.8257 (0.7972)	grad_norm 2.3026 (inf)	loss_scale 8192.0000 (8683.2471)	mem 15686MB
[2024-07-02 15:12:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:04:21 lr 0.000011	 wd 0.0000	time 0.4438 (0.4345)	loss 0.8232 (0.7972)	grad_norm 2.6185 (inf)	loss_scale 8192.0000 (8657.4056)	mem 15686MB
[2024-07-02 15:13:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:03:38 lr 0.000011	 wd 0.0000	time 0.3924 (0.4343)	loss 0.7603 (0.7967)	grad_norm 2.8061 (inf)	loss_scale 8192.0000 (8634.1469)	mem 15686MB
[2024-07-02 15:13:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:54 lr 0.000011	 wd 0.0000	time 0.4020 (0.4342)	loss 0.8096 (0.7962)	grad_norm 2.1163 (inf)	loss_scale 8192.0000 (8613.1023)	mem 15686MB
[2024-07-02 15:14:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:02:11 lr 0.000011	 wd 0.0000	time 0.3937 (0.4341)	loss 0.8311 (0.7962)	grad_norm 2.1838 (inf)	loss_scale 8192.0000 (8593.9700)	mem 15686MB
[2024-07-02 15:15:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:27 lr 0.000011	 wd 0.0000	time 0.4241 (0.4341)	loss 0.6958 (0.7963)	grad_norm 1.9035 (inf)	loss_scale 8192.0000 (8576.5007)	mem 15686MB
[2024-07-02 15:15:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:44 lr 0.000011	 wd 0.0000	time 0.3973 (0.4340)	loss 0.7720 (0.7968)	grad_norm 2.4458 (inf)	loss_scale 8192.0000 (8560.4865)	mem 15686MB
[2024-07-02 15:16:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.3851 (0.4335)	loss 0.8447 (0.7966)	grad_norm 2.3386 (inf)	loss_scale 8192.0000 (8545.7529)	mem 15686MB
[2024-07-02 15:16:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 23 training takes 0:18:06
[2024-07-02 15:16:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.306 (12.306)	Loss 0.3765 (0.3765)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 15:17:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.422 Acc@5 97.530
[2024-07-02 15:17:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-02 15:17:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.42%
[2024-07-02 15:17:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 15:17:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 15:17:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][0/2502]	eta 6:26:47 lr 0.000010	 wd 0.0000	time 9.2755 (9.2755)	loss 0.7188 (0.7188)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:18:03 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:20:40 lr 0.000010	 wd 0.0000	time 0.3762 (0.5164)	loss 0.9067 (0.8009)	grad_norm 2.1527 (2.2924)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:18:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:18:10 lr 0.000010	 wd 0.0000	time 0.4168 (0.4738)	loss 0.7266 (0.7991)	grad_norm 2.7005 (2.3754)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:19:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:16:49 lr 0.000010	 wd 0.0000	time 0.4031 (0.4585)	loss 0.7944 (0.7959)	grad_norm 2.5486 (2.3630)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:20:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:15:48 lr 0.000010	 wd 0.0000	time 0.4024 (0.4511)	loss 0.7798 (0.7966)	grad_norm 3.0461 (2.3454)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:20:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:14:54 lr 0.000010	 wd 0.0000	time 0.3869 (0.4470)	loss 0.9385 (0.7966)	grad_norm 2.6034 (2.3741)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:21:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:14:05 lr 0.000010	 wd 0.0000	time 0.3964 (0.4443)	loss 0.7207 (0.7954)	grad_norm 2.9176 (2.3569)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:22:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:13:16 lr 0.000010	 wd 0.0000	time 0.3856 (0.4418)	loss 0.7598 (0.7948)	grad_norm 2.1978 (2.3749)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:23:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:12:29 lr 0.000010	 wd 0.0000	time 0.4021 (0.4403)	loss 0.7881 (0.7951)	grad_norm 2.2588 (2.3651)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:23:47 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:11:43 lr 0.000009	 wd 0.0000	time 0.4222 (0.4392)	loss 0.7920 (0.7966)	grad_norm 2.5211 (2.3773)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:24:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:10:58 lr 0.000009	 wd 0.0000	time 0.3834 (0.4384)	loss 0.9321 (0.7969)	grad_norm 1.9431 (2.3713)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:25:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:10:13 lr 0.000009	 wd 0.0000	time 0.3973 (0.4375)	loss 0.8901 (0.7964)	grad_norm 2.2128 (2.3705)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:25:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:09:28 lr 0.000009	 wd 0.0000	time 0.4062 (0.4369)	loss 0.6763 (0.7965)	grad_norm 1.9928 (2.3653)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:26:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:08:44 lr 0.000009	 wd 0.0000	time 0.4008 (0.4365)	loss 0.6587 (0.7964)	grad_norm 2.2551 (2.3649)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:27:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:08:00 lr 0.000009	 wd 0.0000	time 0.3847 (0.4359)	loss 0.9043 (0.7963)	grad_norm 2.6236 (2.3676)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:28:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:07:16 lr 0.000009	 wd 0.0000	time 0.4344 (0.4356)	loss 0.6943 (0.7965)	grad_norm 2.1977 (2.3624)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:28:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:06:32 lr 0.000009	 wd 0.0000	time 0.4197 (0.4355)	loss 0.7651 (0.7966)	grad_norm 2.5987 (2.3663)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:29:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:05:49 lr 0.000008	 wd 0.0000	time 0.4112 (0.4353)	loss 0.9146 (0.7966)	grad_norm 2.3521 (2.3630)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:30:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:05:05 lr 0.000008	 wd 0.0000	time 0.4001 (0.4352)	loss 0.7446 (0.7965)	grad_norm 2.4934 (2.3584)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:30:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:04:21 lr 0.000008	 wd 0.0000	time 0.3990 (0.4350)	loss 0.6919 (0.7967)	grad_norm 2.5054 (2.3642)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:31:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:03:38 lr 0.000008	 wd 0.0000	time 0.4053 (0.4349)	loss 0.8428 (0.7970)	grad_norm 2.5185 (2.3631)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:32:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:54 lr 0.000008	 wd 0.0000	time 0.3737 (0.4348)	loss 0.8174 (0.7969)	grad_norm 2.3457 (2.3662)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:33:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:02:11 lr 0.000008	 wd 0.0000	time 0.3967 (0.4347)	loss 0.9531 (0.7974)	grad_norm 2.9610 (2.3679)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:33:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:27 lr 0.000008	 wd 0.0000	time 0.3896 (0.4345)	loss 0.8765 (0.7974)	grad_norm 2.5776 (2.3686)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:34:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:44 lr 0.000008	 wd 0.0000	time 0.4091 (0.4345)	loss 0.8188 (0.7973)	grad_norm 2.1828 (2.3657)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:35:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.3879 (0.4339)	loss 0.8276 (0.7976)	grad_norm 2.9720 (2.3635)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:35:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 24 training takes 0:18:07
[2024-07-02 15:35:31 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.003 (12.003)	Loss 0.3767 (0.3767)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 15:35:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.454 Acc@5 97.488
[2024-07-02 15:35:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-02 15:35:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.45%
[2024-07-02 15:35:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 15:35:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 15:35:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][0/2502]	eta 6:13:51 lr 0.000008	 wd 0.0000	time 8.9654 (8.9654)	loss 0.8286 (0.8286)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:36:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:20:32 lr 0.000008	 wd 0.0000	time 0.3874 (0.5131)	loss 0.7485 (0.7931)	grad_norm 2.0330 (2.3386)	loss_scale 16384.0000 (15491.8020)	mem 15686MB
[2024-07-02 15:37:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:18:03 lr 0.000007	 wd 0.0000	time 0.4245 (0.4708)	loss 0.8296 (0.7957)	grad_norm 2.1172 (inf)	loss_scale 8192.0000 (12593.6716)	mem 15686MB
[2024-07-02 15:38:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:16:46 lr 0.000007	 wd 0.0000	time 0.3963 (0.4570)	loss 0.7656 (0.7907)	grad_norm 2.6144 (inf)	loss_scale 8192.0000 (11131.3223)	mem 15686MB
[2024-07-02 15:38:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:15:45 lr 0.000007	 wd 0.0000	time 0.3983 (0.4497)	loss 0.6323 (0.7934)	grad_norm 2.5120 (inf)	loss_scale 8192.0000 (10398.3242)	mem 15686MB
[2024-07-02 15:39:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:14:52 lr 0.000007	 wd 0.0000	time 0.3978 (0.4456)	loss 0.6567 (0.7934)	grad_norm 2.0393 (inf)	loss_scale 8192.0000 (9957.9401)	mem 15686MB
[2024-07-02 15:40:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:14:02 lr 0.000007	 wd 0.0000	time 0.4050 (0.4431)	loss 0.8149 (0.7935)	grad_norm 2.3017 (inf)	loss_scale 8192.0000 (9664.1065)	mem 15686MB
[2024-07-02 15:40:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:13:14 lr 0.000007	 wd 0.0000	time 0.4367 (0.4411)	loss 0.7334 (0.7938)	grad_norm 2.7949 (inf)	loss_scale 8192.0000 (9454.1056)	mem 15686MB
[2024-07-02 15:41:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:12:27 lr 0.000007	 wd 0.0000	time 0.3779 (0.4394)	loss 0.8784 (0.7938)	grad_norm 2.8872 (inf)	loss_scale 8192.0000 (9296.5393)	mem 15686MB
[2024-07-02 15:42:23 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:11:41 lr 0.000007	 wd 0.0000	time 0.3885 (0.4380)	loss 0.9131 (0.7948)	grad_norm 2.2684 (inf)	loss_scale 8192.0000 (9173.9489)	mem 15686MB
[2024-07-02 15:43:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:10:57 lr 0.000007	 wd 0.0000	time 0.4298 (0.4375)	loss 0.8213 (0.7952)	grad_norm 2.0940 (inf)	loss_scale 8192.0000 (9075.8521)	mem 15686MB
[2024-07-02 15:43:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:10:12 lr 0.000007	 wd 0.0000	time 0.3948 (0.4370)	loss 0.9038 (0.7943)	grad_norm 2.1369 (inf)	loss_scale 8192.0000 (8995.5749)	mem 15686MB
[2024-07-02 15:44:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:09:28 lr 0.000006	 wd 0.0000	time 0.3966 (0.4366)	loss 0.8467 (0.7933)	grad_norm 2.5602 (inf)	loss_scale 8192.0000 (8928.6661)	mem 15686MB
[2024-07-02 15:45:16 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:08:44 lr 0.000006	 wd 0.0000	time 0.4301 (0.4362)	loss 0.9590 (0.7929)	grad_norm 2.4032 (inf)	loss_scale 8192.0000 (8872.0430)	mem 15686MB
[2024-07-02 15:45:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:08:00 lr 0.000006	 wd 0.0000	time 0.3848 (0.4360)	loss 0.7769 (0.7931)	grad_norm 2.5586 (inf)	loss_scale 8192.0000 (8823.5032)	mem 15686MB
[2024-07-02 15:46:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:07:16 lr 0.000006	 wd 0.0000	time 0.4155 (0.4357)	loss 0.7617 (0.7939)	grad_norm 2.7711 (inf)	loss_scale 8192.0000 (8781.4310)	mem 15686MB
[2024-07-02 15:47:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:06:32 lr 0.000006	 wd 0.0000	time 0.3897 (0.4355)	loss 0.7485 (0.7942)	grad_norm 2.3344 (inf)	loss_scale 8192.0000 (8744.6146)	mem 15686MB
[2024-07-02 15:48:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:05:49 lr 0.000006	 wd 0.0000	time 0.4013 (0.4352)	loss 0.6553 (0.7943)	grad_norm 2.0589 (inf)	loss_scale 8192.0000 (8712.1270)	mem 15686MB
[2024-07-02 15:48:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:05:05 lr 0.000006	 wd 0.0000	time 0.4313 (0.4350)	loss 0.7988 (0.7944)	grad_norm 2.4263 (inf)	loss_scale 8192.0000 (8683.2471)	mem 15686MB
[2024-07-02 15:49:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:04:21 lr 0.000006	 wd 0.0000	time 0.4137 (0.4350)	loss 0.9170 (0.7949)	grad_norm 2.6116 (inf)	loss_scale 8192.0000 (8657.4056)	mem 15686MB
[2024-07-02 15:50:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:03:38 lr 0.000006	 wd 0.0000	time 0.3991 (0.4349)	loss 1.0098 (0.7950)	grad_norm 2.4962 (inf)	loss_scale 8192.0000 (8634.1469)	mem 15686MB
[2024-07-02 15:51:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:54 lr 0.000006	 wd 0.0000	time 0.4487 (0.4347)	loss 0.7754 (0.7952)	grad_norm 2.2315 (inf)	loss_scale 8192.0000 (8613.1023)	mem 15686MB
[2024-07-02 15:51:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:02:11 lr 0.000006	 wd 0.0000	time 0.4074 (0.4346)	loss 0.8047 (0.7948)	grad_norm 2.1368 (inf)	loss_scale 8192.0000 (8593.9700)	mem 15686MB
[2024-07-02 15:52:28 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:27 lr 0.000005	 wd 0.0000	time 0.3992 (0.4345)	loss 0.7295 (0.7949)	grad_norm 2.3294 (inf)	loss_scale 8192.0000 (8576.5007)	mem 15686MB
[2024-07-02 15:53:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:44 lr 0.000005	 wd 0.0000	time 0.4159 (0.4343)	loss 0.7891 (0.7949)	grad_norm 2.2062 (inf)	loss_scale 8192.0000 (8560.4865)	mem 15686MB
[2024-07-02 15:53:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0000	time 0.4153 (0.4338)	loss 0.7529 (0.7948)	grad_norm 2.4880 (inf)	loss_scale 8192.0000 (8545.7529)	mem 15686MB
[2024-07-02 15:53:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 25 training takes 0:18:07
[2024-07-02 15:54:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 11.432 (11.432)	Loss 0.3840 (0.3840)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 15:54:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.486 Acc@5 97.506
[2024-07-02 15:54:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-02 15:54:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.49%
[2024-07-02 15:54:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 15:54:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 15:54:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][0/2502]	eta 6:00:21 lr 0.000005	 wd 0.0000	time 8.6416 (8.6416)	loss 0.7012 (0.7012)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:55:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:20:21 lr 0.000005	 wd 0.0000	time 0.4016 (0.5086)	loss 0.8242 (0.7837)	grad_norm 2.8695 (2.3691)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:56:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:17:58 lr 0.000005	 wd 0.0000	time 0.4447 (0.4686)	loss 0.7500 (0.7885)	grad_norm 2.7632 (2.3909)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:56:43 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:16:42 lr 0.000005	 wd 0.0000	time 0.3958 (0.4554)	loss 0.7227 (0.7900)	grad_norm 2.1295 (2.3694)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:57:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:15:43 lr 0.000005	 wd 0.0000	time 0.4107 (0.4487)	loss 0.7495 (0.7897)	grad_norm 1.9975 (2.3552)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:58:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:14:50 lr 0.000005	 wd 0.0000	time 0.3889 (0.4446)	loss 0.8286 (0.7919)	grad_norm 1.9059 (2.3575)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:58:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:14:01 lr 0.000005	 wd 0.0000	time 0.4292 (0.4423)	loss 0.6621 (0.7903)	grad_norm 2.1917 (2.3547)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 15:59:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:13:13 lr 0.000005	 wd 0.0000	time 0.3956 (0.4404)	loss 0.8423 (0.7912)	grad_norm 2.1410 (2.3546)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:00:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:12:27 lr 0.000005	 wd 0.0000	time 0.3946 (0.4391)	loss 0.7490 (0.7911)	grad_norm 2.6004 (2.3495)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:01:01 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:11:41 lr 0.000005	 wd 0.0000	time 0.3988 (0.4380)	loss 0.6616 (0.7911)	grad_norm 2.0376 (2.3408)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:01:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:10:56 lr 0.000004	 wd 0.0000	time 0.4082 (0.4371)	loss 0.8359 (0.7900)	grad_norm 3.6421 (2.3477)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:02:27 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:10:12 lr 0.000004	 wd 0.0000	time 0.3986 (0.4367)	loss 0.8213 (0.7890)	grad_norm 2.0496 (2.3429)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:03:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:09:27 lr 0.000004	 wd 0.0000	time 0.4078 (0.4361)	loss 0.6719 (0.7892)	grad_norm 2.1542 (2.3414)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:03:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:08:43 lr 0.000004	 wd 0.0000	time 0.4032 (0.4355)	loss 0.6377 (0.7902)	grad_norm 2.1670 (2.3375)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:04:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:07:59 lr 0.000004	 wd 0.0000	time 0.4225 (0.4351)	loss 0.9546 (0.7909)	grad_norm 2.6587 (2.3329)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:05:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:07:15 lr 0.000004	 wd 0.0000	time 0.3979 (0.4351)	loss 0.7778 (0.7907)	grad_norm 2.4918 (2.3317)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:06:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:06:32 lr 0.000004	 wd 0.0000	time 0.4186 (0.4349)	loss 0.7744 (0.7901)	grad_norm 2.4086 (2.3331)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:06:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:05:48 lr 0.000004	 wd 0.0000	time 0.4076 (0.4347)	loss 0.8569 (0.7909)	grad_norm 2.1990 (inf)	loss_scale 8192.0000 (8394.2716)	mem 15686MB
[2024-07-02 16:07:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:05:05 lr 0.000004	 wd 0.0000	time 0.4138 (0.4345)	loss 0.7451 (0.7911)	grad_norm 2.3508 (inf)	loss_scale 8192.0000 (8383.0405)	mem 15686MB
[2024-07-02 16:08:12 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:04:21 lr 0.000004	 wd 0.0000	time 0.4061 (0.4344)	loss 0.7617 (0.7905)	grad_norm 2.3129 (inf)	loss_scale 8192.0000 (8372.9911)	mem 15686MB
[2024-07-02 16:08:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:03:37 lr 0.000004	 wd 0.0000	time 0.4062 (0.4342)	loss 0.8110 (0.7910)	grad_norm 2.0275 (inf)	loss_scale 8192.0000 (8363.9460)	mem 15686MB
[2024-07-02 16:09:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:54 lr 0.000004	 wd 0.0000	time 0.3944 (0.4346)	loss 0.7603 (0.7914)	grad_norm 2.1860 (inf)	loss_scale 8192.0000 (8355.7620)	mem 15686MB
[2024-07-02 16:10:24 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:02:11 lr 0.000004	 wd 0.0000	time 0.3862 (0.4353)	loss 0.7544 (0.7923)	grad_norm 2.0920 (inf)	loss_scale 8192.0000 (8348.3217)	mem 15686MB
[2024-07-02 16:11:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:28 lr 0.000004	 wd 0.0000	time 0.3997 (0.4360)	loss 0.7881 (0.7920)	grad_norm 2.2172 (inf)	loss_scale 8192.0000 (8341.5280)	mem 15686MB
[2024-07-02 16:11:55 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:44 lr 0.000003	 wd 0.0000	time 0.4651 (0.4367)	loss 0.7075 (0.7922)	grad_norm 2.7291 (inf)	loss_scale 8192.0000 (8335.3003)	mem 15686MB
[2024-07-02 16:12:39 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.4106 (0.4369)	loss 0.7339 (0.7925)	grad_norm 2.5906 (inf)	loss_scale 8192.0000 (8329.5706)	mem 15686MB
[2024-07-02 16:12:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 26 training takes 0:18:15
[2024-07-02 16:12:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 11.100 (11.100)	Loss 0.3840 (0.3840)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 16:13:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.466 Acc@5 97.510
[2024-07-02 16:13:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-02 16:13:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.49%
[2024-07-02 16:13:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][0/2502]	eta 6:52:21 lr 0.000003	 wd 0.0000	time 9.8886 (9.8886)	loss 0.7939 (0.7939)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:14:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:22:36 lr 0.000003	 wd 0.0000	time 0.4236 (0.5647)	loss 0.8228 (0.7995)	grad_norm 2.4834 (2.2922)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:14:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:19:26 lr 0.000003	 wd 0.0000	time 0.4376 (0.5069)	loss 0.6997 (0.7941)	grad_norm 2.9706 (2.3117)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:15:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:17:51 lr 0.000003	 wd 0.0000	time 0.3529 (0.4868)	loss 0.7949 (0.7943)	grad_norm 2.7761 (2.3190)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:16:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:16:43 lr 0.000003	 wd 0.0000	time 0.4201 (0.4772)	loss 0.7739 (0.7960)	grad_norm 2.4660 (2.3238)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:17:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:15:44 lr 0.000003	 wd 0.0000	time 0.4123 (0.4717)	loss 0.7446 (0.7956)	grad_norm 2.2412 (2.3110)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:17:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:14:50 lr 0.000003	 wd 0.0000	time 0.4303 (0.4682)	loss 0.6670 (0.7953)	grad_norm 2.2534 (2.3118)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:18:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:13:57 lr 0.000003	 wd 0.0000	time 0.4265 (0.4649)	loss 0.6919 (0.7937)	grad_norm 1.6982 (2.3078)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:19:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:13:08 lr 0.000003	 wd 0.0000	time 0.3985 (0.4630)	loss 0.9482 (0.7936)	grad_norm 2.4674 (2.3072)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:20:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:12:19 lr 0.000003	 wd 0.0000	time 0.4136 (0.4616)	loss 0.8472 (0.7935)	grad_norm 2.0087 (2.3042)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:20:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:11:30 lr 0.000003	 wd 0.0000	time 0.3861 (0.4599)	loss 0.7959 (0.7935)	grad_norm 2.1118 (2.3134)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:21:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:10:43 lr 0.000003	 wd 0.0000	time 0.4084 (0.4589)	loss 0.7993 (0.7933)	grad_norm 2.6141 (2.3134)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:22:19 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:09:56 lr 0.000003	 wd 0.0000	time 0.4096 (0.4580)	loss 0.7485 (0.7931)	grad_norm 2.4725 (2.3187)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:23:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:09:09 lr 0.000003	 wd 0.0000	time 0.4213 (0.4574)	loss 0.8228 (0.7927)	grad_norm 2.3306 (2.3149)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:23:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:08:23 lr 0.000003	 wd 0.0000	time 0.4221 (0.4569)	loss 0.7378 (0.7924)	grad_norm 2.3012 (2.3136)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:24:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:07:37 lr 0.000003	 wd 0.0000	time 0.3548 (0.4563)	loss 0.8022 (0.7929)	grad_norm 2.1559 (2.3168)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:25:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:06:51 lr 0.000003	 wd 0.0000	time 0.3835 (0.4558)	loss 0.7861 (0.7939)	grad_norm 2.0627 (2.3134)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:26:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:06:05 lr 0.000002	 wd 0.0000	time 0.3781 (0.4558)	loss 0.9131 (0.7935)	grad_norm 2.8751 (2.3152)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:26:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:05:19 lr 0.000002	 wd 0.0000	time 0.4264 (0.4557)	loss 0.7666 (0.7935)	grad_norm 1.8542 (2.3164)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:27:35 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:04:34 lr 0.000002	 wd 0.0000	time 0.4150 (0.4556)	loss 0.8281 (0.7945)	grad_norm 2.3492 (2.3181)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:28:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:03:48 lr 0.000002	 wd 0.0000	time 0.4039 (0.4555)	loss 0.7070 (0.7944)	grad_norm 2.8833 (2.3171)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:29:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:03:03 lr 0.000002	 wd 0.0000	time 0.4472 (0.4553)	loss 0.8550 (0.7946)	grad_norm 2.1465 (2.3152)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:29:51 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:02:17 lr 0.000002	 wd 0.0000	time 0.4010 (0.4553)	loss 0.7197 (0.7939)	grad_norm 2.3617 (2.3144)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:30:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:31 lr 0.000002	 wd 0.0000	time 0.4219 (0.4550)	loss 0.9238 (0.7943)	grad_norm 2.3687 (2.3157)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:31:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:46 lr 0.000002	 wd 0.0000	time 0.3914 (0.4547)	loss 0.7632 (0.7944)	grad_norm 2.4353 (2.3174)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:32:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.3805 (0.4542)	loss 0.6714 (0.7944)	grad_norm 2.4517 (2.3188)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:32:40 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 27 training takes 0:19:31
[2024-07-02 16:32:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 14.010 (14.010)	Loss 0.3823 (0.3823)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 16:33:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.478 Acc@5 97.520
[2024-07-02 16:33:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-02 16:33:07 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.49%
[2024-07-02 16:33:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][0/2502]	eta 6:33:45 lr 0.000002	 wd 0.0000	time 9.4427 (9.4427)	loss 0.8530 (0.8530)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:34:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:22:26 lr 0.000002	 wd 0.0000	time 0.4109 (0.5606)	loss 0.7085 (0.7945)	grad_norm 2.1964 (2.3445)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:34:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:19:21 lr 0.000002	 wd 0.0000	time 0.4049 (0.5047)	loss 0.6895 (0.7920)	grad_norm 2.4283 (2.3712)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:35:33 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:17:48 lr 0.000002	 wd 0.0000	time 0.4072 (0.4851)	loss 0.8281 (0.7929)	grad_norm 2.2394 (2.3809)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:36:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:16:37 lr 0.000002	 wd 0.0000	time 0.4066 (0.4747)	loss 0.8286 (0.7910)	grad_norm 2.1136 (2.3658)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:37:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:15:36 lr 0.000002	 wd 0.0000	time 0.4191 (0.4676)	loss 0.7256 (0.7890)	grad_norm 2.6496 (2.3598)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:37:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:14:41 lr 0.000002	 wd 0.0000	time 0.4186 (0.4635)	loss 0.8008 (0.7896)	grad_norm 2.2500 (2.3538)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:38:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:13:49 lr 0.000002	 wd 0.0000	time 0.3980 (0.4602)	loss 0.7900 (0.7904)	grad_norm 2.3422 (2.3505)	loss_scale 16384.0000 (8729.5635)	mem 15686MB
[2024-07-02 16:39:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:12:59 lr 0.000002	 wd 0.0000	time 0.3904 (0.4577)	loss 0.7959 (0.7914)	grad_norm 2.0940 (2.3389)	loss_scale 16384.0000 (9685.1735)	mem 15686MB
[2024-07-02 16:39:58 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:12:10 lr 0.000002	 wd 0.0000	time 0.3991 (0.4557)	loss 0.7725 (0.7915)	grad_norm 2.0576 (2.3344)	loss_scale 16384.0000 (10428.6615)	mem 15686MB
[2024-07-02 16:40:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:11:21 lr 0.000002	 wd 0.0000	time 0.4291 (0.4538)	loss 0.7817 (0.7915)	grad_norm 2.4049 (2.3387)	loss_scale 16384.0000 (11023.6004)	mem 15686MB
[2024-07-02 16:41:26 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:10:34 lr 0.000002	 wd 0.0000	time 0.3911 (0.4526)	loss 0.7617 (0.7906)	grad_norm 2.5017 (2.3331)	loss_scale 16384.0000 (11510.4668)	mem 15686MB
[2024-07-02 16:42:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:09:48 lr 0.000002	 wd 0.0000	time 0.3904 (0.4519)	loss 0.7559 (0.7905)	grad_norm 2.5362 (2.3305)	loss_scale 16384.0000 (11916.2565)	mem 15686MB
[2024-07-02 16:42:54 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:09:02 lr 0.000002	 wd 0.0000	time 0.4184 (0.4511)	loss 0.8672 (0.7905)	grad_norm 1.9019 (2.3281)	loss_scale 16384.0000 (12259.6649)	mem 15686MB
[2024-07-02 16:43:38 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:08:16 lr 0.000002	 wd 0.0000	time 0.3827 (0.4502)	loss 0.7666 (0.7895)	grad_norm 2.4592 (2.3305)	loss_scale 16384.0000 (12554.0500)	mem 15686MB
[2024-07-02 16:44:22 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:07:30 lr 0.000002	 wd 0.0000	time 0.4235 (0.4497)	loss 0.8735 (0.7885)	grad_norm 3.8476 (inf)	loss_scale 8192.0000 (12601.8175)	mem 15686MB
[2024-07-02 16:45:06 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:06:44 lr 0.000002	 wd 0.0000	time 0.3809 (0.4489)	loss 0.7925 (0.7895)	grad_norm 2.4446 (inf)	loss_scale 8192.0000 (12326.3760)	mem 15686MB
[2024-07-02 16:45:50 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:05:59 lr 0.000001	 wd 0.0000	time 0.3795 (0.4485)	loss 0.6367 (0.7898)	grad_norm 2.2067 (inf)	loss_scale 8192.0000 (12083.3204)	mem 15686MB
[2024-07-02 16:46:34 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:05:14 lr 0.000001	 wd 0.0000	time 0.4302 (0.4479)	loss 0.7534 (0.7896)	grad_norm 2.4842 (inf)	loss_scale 8192.0000 (11867.2560)	mem 15686MB
[2024-07-02 16:47:18 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:04:29 lr 0.000001	 wd 0.0000	time 0.3903 (0.4476)	loss 0.7959 (0.7900)	grad_norm 2.6875 (inf)	loss_scale 8192.0000 (11673.9232)	mem 15686MB
[2024-07-02 16:48:02 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:03:44 lr 0.000001	 wd 0.0000	time 0.3773 (0.4471)	loss 0.7637 (0.7898)	grad_norm 2.6641 (inf)	loss_scale 8192.0000 (11499.9140)	mem 15686MB
[2024-07-02 16:48:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:59 lr 0.000001	 wd 0.0000	time 0.4405 (0.4467)	loss 0.8027 (0.7903)	grad_norm 2.0910 (inf)	loss_scale 8192.0000 (11342.4693)	mem 15686MB
[2024-07-02 16:49:30 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:02:14 lr 0.000001	 wd 0.0000	time 0.4178 (0.4466)	loss 0.7749 (0.7906)	grad_norm 2.2153 (inf)	loss_scale 8192.0000 (11199.3312)	mem 15686MB
[2024-07-02 16:50:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:30 lr 0.000001	 wd 0.0000	time 0.4114 (0.4464)	loss 0.7275 (0.7911)	grad_norm 2.3250 (inf)	loss_scale 8192.0000 (11068.6345)	mem 15686MB
[2024-07-02 16:50:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:45 lr 0.000001	 wd 0.0000	time 0.4300 (0.4463)	loss 1.0439 (0.7904)	grad_norm 2.3732 (inf)	loss_scale 8192.0000 (10948.8247)	mem 15686MB
[2024-07-02 16:51:42 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.3372 (0.4457)	loss 0.7021 (0.7906)	grad_norm 2.0660 (inf)	loss_scale 8192.0000 (10838.5958)	mem 15686MB
[2024-07-02 16:51:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 28 training takes 0:18:37
[2024-07-02 16:51:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 12.317 (12.317)	Loss 0.3840 (0.3840)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 16:52:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.500 Acc@5 97.502
[2024-07-02 16:52:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-02 16:52:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.50%
[2024-07-02 16:52:11 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 160): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saving......
[2024-07-02 16:52:14 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 162): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_best.pth saved !!!
[2024-07-02 16:52:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][0/2502]	eta 6:55:45 lr 0.000001	 wd 0.0000	time 9.9702 (9.9702)	loss 0.8721 (0.8721)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:53:08 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:21:16 lr 0.000001	 wd 0.0000	time 0.4034 (0.5316)	loss 0.7251 (0.7982)	grad_norm 2.5364 (2.3524)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:53:52 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:18:38 lr 0.000001	 wd 0.0000	time 0.4113 (0.4857)	loss 0.7749 (0.7922)	grad_norm 2.2798 (2.3498)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:54:36 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:17:14 lr 0.000001	 wd 0.0000	time 0.4173 (0.4697)	loss 0.7539 (0.7935)	grad_norm 2.5076 (2.3266)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:55:20 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:16:12 lr 0.000001	 wd 0.0000	time 0.4280 (0.4625)	loss 0.7900 (0.7916)	grad_norm 2.2750 (2.3176)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:56:04 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:15:17 lr 0.000001	 wd 0.0000	time 0.3999 (0.4582)	loss 0.7773 (0.7919)	grad_norm 2.1862 (2.3259)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:56:48 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:14:25 lr 0.000001	 wd 0.0000	time 0.4129 (0.4551)	loss 0.7578 (0.7899)	grad_norm 2.4062 (2.3180)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:57:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:13:36 lr 0.000001	 wd 0.0000	time 0.4111 (0.4534)	loss 0.8906 (0.7883)	grad_norm 2.5322 (2.3126)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:58:17 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:12:49 lr 0.000001	 wd 0.0000	time 0.3986 (0.4519)	loss 0.7241 (0.7894)	grad_norm 2.7856 (2.3137)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:59:00 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:12:01 lr 0.000001	 wd 0.0000	time 0.4027 (0.4505)	loss 0.8105 (0.7891)	grad_norm 2.2676 (2.3205)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 16:59:45 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:11:15 lr 0.000001	 wd 0.0000	time 0.4237 (0.4499)	loss 0.8584 (0.7903)	grad_norm 1.8959 (2.3176)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:00:29 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:10:29 lr 0.000001	 wd 0.0000	time 0.4182 (0.4491)	loss 0.7446 (0.7915)	grad_norm 1.9830 (2.3205)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:01:13 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:09:43 lr 0.000001	 wd 0.0000	time 0.3872 (0.4481)	loss 0.8086 (0.7908)	grad_norm 2.4083 (2.3200)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:01:57 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:08:57 lr 0.000001	 wd 0.0000	time 0.4075 (0.4474)	loss 0.7695 (0.7915)	grad_norm 2.3419 (2.3192)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:02:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:08:12 lr 0.000001	 wd 0.0000	time 0.4121 (0.4469)	loss 0.9009 (0.7909)	grad_norm 2.0747 (2.3164)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:03:25 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:07:27 lr 0.000001	 wd 0.0000	time 0.3950 (0.4465)	loss 0.6812 (0.7915)	grad_norm 2.6330 (2.3212)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:04:09 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:06:42 lr 0.000001	 wd 0.0000	time 0.4091 (0.4463)	loss 0.7520 (0.7911)	grad_norm 2.8129 (2.3195)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:04:53 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:05:57 lr 0.000001	 wd 0.0000	time 0.4039 (0.4458)	loss 0.7588 (0.7906)	grad_norm 2.3341 (2.3204)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:05:37 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:05:12 lr 0.000001	 wd 0.0000	time 0.4165 (0.4457)	loss 0.6104 (0.7904)	grad_norm 2.8201 (2.3214)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:06:21 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:04:28 lr 0.000001	 wd 0.0000	time 0.4068 (0.4453)	loss 0.8086 (0.7903)	grad_norm 2.3757 (2.3198)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:07:05 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:03:43 lr 0.000001	 wd 0.0000	time 0.4031 (0.4450)	loss 0.6924 (0.7905)	grad_norm 1.9389 (2.3235)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:07:49 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:58 lr 0.000001	 wd 0.0000	time 0.4119 (0.4446)	loss 0.7026 (0.7907)	grad_norm 2.0869 (2.3255)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:08:32 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:02:14 lr 0.000001	 wd 0.0000	time 0.3911 (0.4440)	loss 0.7417 (0.7901)	grad_norm 2.0867 (2.3226)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:09:15 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:29 lr 0.000001	 wd 0.0000	time 0.3913 (0.4436)	loss 0.8076 (0.7901)	grad_norm 2.1354 (2.3235)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:09:59 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:45 lr 0.000001	 wd 0.0000	time 0.3923 (0.4432)	loss 0.8101 (0.7905)	grad_norm 2.3844 (2.3245)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:10:41 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.3911 (0.4424)	loss 0.7139 (0.7904)	grad_norm 2.0835 (2.3232)	loss_scale 8192.0000 (8192.0000)	mem 15686MB
[2024-07-02 17:10:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 249): INFO EPOCH 29 training takes 0:18:29
[2024-07-02 17:10:44 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 145): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_29.pth saving......
[2024-07-02 17:10:46 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (utils.py 147): INFO pretrain/vcnu_finetune/vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm/vcnu_convnext_base_patch4_22kto1k_finetune_nonorm/ckpt_epoch_29.pth saved !!!
[2024-07-02 17:10:56 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 289): INFO Test: [0/98]	Time 10.644 (10.644)	Loss 0.3835 (0.3835)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 15686MB
[2024-07-02 17:11:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 296): INFO  * Acc@1 85.442 Acc@5 97.508
[2024-07-02 17:11:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-02 17:11:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 182): INFO Max accuracy: 85.50%
[2024-07-02 17:11:10 vcnu_convnext_base_224_22kto1kto1k_finetune_nonorm] (main.py 189): INFO Training time 8:25:11
