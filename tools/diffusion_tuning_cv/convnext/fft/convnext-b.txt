[2024-06-29 13:12:57 convnext_base_224_22kto1kto1k_finetune] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/config.json
[2024-06-29 13:12:57 convnext_base_224_22kto1kto1k_finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: convnext_base_224_22kto1kto1k_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weights/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: convnext
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: convnext_base_22kto1k_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-06-29 13:12:57 convnext_base_224_22kto1kto1k_finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_convnext/convnext_base_224_22kto1kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weights/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "convnext_base_22kto1k_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-06-29 13:13:02 convnext_base_224_22kto1kto1k_finetune] (main.py 108): INFO Creating model:convnext/convnext_base_224_22kto1kto1k_finetune
[2024-06-29 13:13:03 convnext_base_224_22kto1kto1k_finetune] (main.py 110): INFO ConvNeXt(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-06-29 13:13:03 convnext_base_224_22kto1kto1k_finetune] (main.py 113): INFO number of params: 88591464
[2024-06-29 13:13:04 convnext_base_224_22kto1kto1k_finetune] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune, ignoring auto resume
[2024-06-29 13:13:04 convnext_base_224_22kto1kto1k_finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weights/convnext-b/convnext_base_22k_224.pth for fine-tuning......
[2024-06-29 13:13:04 convnext_base_224_22kto1kto1k_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-06-29 13:13:05 convnext_base_224_22kto1kto1k_finetune] (utils.py 127): WARNING <All keys matched successfully>
[2024-06-29 13:13:05 convnext_base_224_22kto1kto1k_finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/pretrain_weights/convnext-b/convnext_base_22k_224.pth'
[2024-06-29 13:13:19 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 14.801 (14.801)	Loss 0.3726 (0.3726)	Acc@1 91.406 (91.406)	Acc@5 98.828 (98.828)	Mem 3484MB
[2024-06-29 13:13:31 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 65.036 Acc@5 77.622
[2024-06-29 13:13:31 convnext_base_224_22kto1kto1k_finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 65.0%
[2024-06-29 13:13:31 convnext_base_224_22kto1kto1k_finetune] (main.py 168): INFO Start training
[2024-06-29 13:13:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:12:03 lr 0.000100	 wd 0.0000	time 13.2389 (13.2389)	loss 1.6660 (1.6660)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 11634MB
[2024-06-29 13:14:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:17:43 lr 0.000100	 wd 0.0000	time 0.3016 (0.4429)	loss 1.3555 (1.4415)	grad_norm 3.4754 (nan)	loss_scale 16384.0000 (18168.3960)	mem 11634MB
[2024-06-29 13:14:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:45 lr 0.000100	 wd 0.0000	time 0.2874 (0.3845)	loss 1.0996 (1.3569)	grad_norm 3.7735 (nan)	loss_scale 16384.0000 (17280.6368)	mem 11634MB
[2024-06-29 13:15:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:17 lr 0.000100	 wd 0.0000	time 0.2925 (0.3622)	loss 1.1855 (1.2909)	grad_norm 2.8043 (nan)	loss_scale 16384.0000 (16982.7508)	mem 11634MB
[2024-06-29 13:15:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:12:18 lr 0.000100	 wd 0.0000	time 0.3081 (0.3512)	loss 1.2979 (1.2405)	grad_norm 3.5765 (nan)	loss_scale 16384.0000 (16833.4364)	mem 11634MB
[2024-06-29 13:16:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:26 lr 0.000100	 wd 0.0000	time 0.3157 (0.3432)	loss 0.9316 (1.2044)	grad_norm 3.4442 (nan)	loss_scale 16384.0000 (16743.7285)	mem 11634MB
[2024-06-29 13:16:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:10:43 lr 0.000100	 wd 0.0000	time 0.2471 (0.3386)	loss 0.9712 (1.1762)	grad_norm 2.9314 (nan)	loss_scale 16384.0000 (16683.8735)	mem 11634MB
[2024-06-29 13:17:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:10:07 lr 0.000100	 wd 0.0000	time 0.2365 (0.3370)	loss 1.0684 (1.1553)	grad_norm 5.2694 (nan)	loss_scale 16384.0000 (16641.0956)	mem 11634MB
[2024-06-29 13:17:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:28 lr 0.000100	 wd 0.0000	time 0.2791 (0.3341)	loss 1.0107 (1.1370)	grad_norm 2.9127 (nan)	loss_scale 16384.0000 (16608.9988)	mem 11634MB
[2024-06-29 13:18:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:08:51 lr 0.000100	 wd 0.0000	time 0.2872 (0.3320)	loss 1.0049 (1.1217)	grad_norm 3.3701 (nan)	loss_scale 16384.0000 (16584.0266)	mem 11634MB
[2024-06-29 13:19:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:08:19 lr 0.000100	 wd 0.0000	time 0.2922 (0.3323)	loss 1.0566 (1.1096)	grad_norm 2.9259 (nan)	loss_scale 16384.0000 (16564.0440)	mem 11634MB
[2024-06-29 13:19:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:07:43 lr 0.000100	 wd 0.0000	time 0.2972 (0.3307)	loss 0.8877 (1.0991)	grad_norm 3.7191 (nan)	loss_scale 16384.0000 (16547.6912)	mem 11634MB
[2024-06-29 13:20:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:07:09 lr 0.000100	 wd 0.0000	time 0.2694 (0.3296)	loss 0.9170 (1.0901)	grad_norm 2.2115 (nan)	loss_scale 16384.0000 (16534.0616)	mem 11634MB
[2024-06-29 13:20:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:36 lr 0.000100	 wd 0.0000	time 0.3839 (0.3303)	loss 0.8037 (1.0828)	grad_norm 2.8579 (nan)	loss_scale 16384.0000 (16522.5273)	mem 11634MB
[2024-06-29 13:21:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:04 lr 0.000100	 wd 0.0000	time 0.3010 (0.3310)	loss 0.9526 (1.0755)	grad_norm 3.0028 (nan)	loss_scale 16384.0000 (16512.6395)	mem 11634MB
[2024-06-29 13:21:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:34 lr 0.000100	 wd 0.0000	time 0.4141 (0.3337)	loss 1.0947 (1.0695)	grad_norm 2.6420 (nan)	loss_scale 16384.0000 (16504.0693)	mem 11634MB
[2024-06-29 13:22:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:05:03 lr 0.000100	 wd 0.0000	time 0.3476 (0.3363)	loss 1.1084 (1.0633)	grad_norm 2.8604 (nan)	loss_scale 16384.0000 (16496.5696)	mem 11634MB
[2024-06-29 13:23:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:30 lr 0.000100	 wd 0.0000	time 0.3607 (0.3374)	loss 0.8560 (1.0578)	grad_norm 2.9966 (nan)	loss_scale 16384.0000 (16489.9518)	mem 11634MB
[2024-06-29 13:23:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:58 lr 0.000100	 wd 0.0000	time 0.3436 (0.3399)	loss 0.9263 (1.0529)	grad_norm 2.2993 (nan)	loss_scale 16384.0000 (16484.0689)	mem 11634MB
[2024-06-29 13:24:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:26 lr 0.000100	 wd 0.0000	time 0.3028 (0.3429)	loss 0.9375 (1.0480)	grad_norm 2.0810 (nan)	loss_scale 16384.0000 (16478.8048)	mem 11634MB
[2024-06-29 13:25:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:53 lr 0.000100	 wd 0.0000	time 0.3888 (0.3452)	loss 1.0459 (1.0448)	grad_norm 2.5620 (nan)	loss_scale 16384.0000 (16474.0670)	mem 11634MB
[2024-06-29 13:25:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:19 lr 0.000100	 wd 0.0000	time 0.3398 (0.3461)	loss 0.9346 (1.0413)	grad_norm 2.0843 (nan)	loss_scale 16384.0000 (16469.7801)	mem 11634MB
[2024-06-29 13:26:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:45 lr 0.000100	 wd 0.0000	time 0.2941 (0.3488)	loss 0.9551 (1.0375)	grad_norm 2.3505 (nan)	loss_scale 16384.0000 (16465.8828)	mem 11634MB
[2024-06-29 13:26:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:10 lr 0.000100	 wd 0.0000	time 0.3782 (0.3505)	loss 1.0576 (1.0339)	grad_norm 2.2363 (nan)	loss_scale 16384.0000 (16462.3242)	mem 11634MB
[2024-06-29 13:27:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:36 lr 0.000100	 wd 0.0000	time 0.4209 (0.3536)	loss 0.8794 (1.0307)	grad_norm 3.0701 (nan)	loss_scale 16384.0000 (16459.0621)	mem 11634MB
[2024-06-29 13:28:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000100	 wd 0.0000	time 0.2714 (0.3526)	loss 1.1230 (1.0276)	grad_norm 2.4699 (nan)	loss_scale 16384.0000 (16456.0608)	mem 11634MB
[2024-06-29 13:28:16 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 0 training takes 0:14:44
[2024-06-29 13:28:16 convnext_base_224_22kto1kto1k_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_0.pth saving......
[2024-06-29 13:28:17 convnext_base_224_22kto1kto1k_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_0.pth saved !!!
[2024-06-29 13:28:28 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 10.567 (10.567)	Loss 0.3921 (0.3921)	Acc@1 91.211 (91.211)	Acc@5 98.828 (98.828)	Mem 11634MB
[2024-06-29 13:28:38 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 83.806 Acc@5 97.084
[2024-06-29 13:28:38 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-06-29 13:28:38 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 83.81%
[2024-06-29 13:28:38 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 13:28:39 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 13:28:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][0/2502]	eta 8:03:30 lr 0.000100	 wd 0.0000	time 11.5948 (11.5948)	loss 0.9155 (0.9155)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:29:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:16:56 lr 0.000100	 wd 0.0000	time 0.2862 (0.4230)	loss 0.8784 (0.9427)	grad_norm 3.3356 (2.4802)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:29:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:14:19 lr 0.000100	 wd 0.0000	time 0.3015 (0.3734)	loss 1.0820 (0.9502)	grad_norm 2.6370 (2.4398)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:30:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:12:59 lr 0.000100	 wd 0.0000	time 0.3045 (0.3541)	loss 1.0762 (0.9470)	grad_norm 2.0199 (2.4064)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:30:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:12:04 lr 0.000100	 wd 0.0000	time 0.2452 (0.3446)	loss 0.8140 (0.9474)	grad_norm 2.4313 (2.4068)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:31:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:11:21 lr 0.000100	 wd 0.0000	time 0.2557 (0.3402)	loss 0.9521 (0.9445)	grad_norm 2.2836 (2.4130)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:32:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:10:39 lr 0.000100	 wd 0.0000	time 0.2972 (0.3361)	loss 1.0420 (0.9452)	grad_norm 2.4800 (2.4050)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:32:33 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:10:00 lr 0.000100	 wd 0.0000	time 0.2909 (0.3332)	loss 0.8193 (0.9441)	grad_norm 2.4021 (2.4123)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:33:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:22 lr 0.000100	 wd 0.0000	time 0.2936 (0.3306)	loss 0.8418 (0.9451)	grad_norm 2.4183 (2.3957)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:33:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:08:46 lr 0.000099	 wd 0.0000	time 0.2821 (0.3287)	loss 1.0176 (0.9442)	grad_norm 1.9254 (2.3929)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:34:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:08:11 lr 0.000099	 wd 0.0000	time 0.2688 (0.3269)	loss 0.9380 (0.9435)	grad_norm 2.7092 (inf)	loss_scale 8192.0000 (16302.1618)	mem 11634MB
[2024-06-29 13:34:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:07:38 lr 0.000099	 wd 0.0000	time 0.2680 (0.3269)	loss 1.0791 (0.9432)	grad_norm 2.7631 (inf)	loss_scale 8192.0000 (15565.5441)	mem 11634MB
[2024-06-29 13:35:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:04 lr 0.000099	 wd 0.0000	time 0.2814 (0.3257)	loss 0.9883 (0.9427)	grad_norm 2.0688 (inf)	loss_scale 8192.0000 (14951.5937)	mem 11634MB
[2024-06-29 13:35:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:30 lr 0.000099	 wd 0.0000	time 0.2773 (0.3252)	loss 0.9966 (0.9433)	grad_norm 2.1294 (inf)	loss_scale 8192.0000 (14432.0246)	mem 11634MB
[2024-06-29 13:36:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:05:57 lr 0.000099	 wd 0.0000	time 0.2963 (0.3246)	loss 0.9771 (0.9434)	grad_norm 2.2935 (inf)	loss_scale 8192.0000 (13986.6267)	mem 11634MB
[2024-06-29 13:36:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:24 lr 0.000099	 wd 0.0000	time 0.2846 (0.3242)	loss 0.9995 (0.9432)	grad_norm 2.0343 (inf)	loss_scale 8192.0000 (13600.5756)	mem 11634MB
[2024-06-29 13:37:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:51 lr 0.000099	 wd 0.0000	time 0.2402 (0.3237)	loss 1.0400 (0.9435)	grad_norm 2.0858 (inf)	loss_scale 8192.0000 (13262.7508)	mem 11634MB
[2024-06-29 13:37:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:19 lr 0.000099	 wd 0.0000	time 0.3109 (0.3234)	loss 0.7842 (0.9433)	grad_norm 2.3894 (inf)	loss_scale 8192.0000 (12964.6467)	mem 11634MB
[2024-06-29 13:38:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:47 lr 0.000099	 wd 0.0000	time 0.3118 (0.3235)	loss 1.0850 (0.9438)	grad_norm 2.2295 (inf)	loss_scale 8192.0000 (12699.6469)	mem 11634MB
[2024-06-29 13:38:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:14 lr 0.000099	 wd 0.0000	time 0.2740 (0.3239)	loss 0.9502 (0.9441)	grad_norm 2.7303 (inf)	loss_scale 8192.0000 (12462.5271)	mem 11634MB
[2024-06-29 13:39:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:42 lr 0.000099	 wd 0.0000	time 0.2925 (0.3239)	loss 0.8169 (0.9435)	grad_norm 2.1549 (inf)	loss_scale 8192.0000 (12249.1074)	mem 11634MB
[2024-06-29 13:40:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:10 lr 0.000099	 wd 0.0000	time 0.2814 (0.3244)	loss 0.8613 (0.9433)	grad_norm 2.3318 (inf)	loss_scale 8192.0000 (12056.0038)	mem 11634MB
[2024-06-29 13:40:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:38 lr 0.000099	 wd 0.0000	time 0.3063 (0.3253)	loss 0.7529 (0.9429)	grad_norm 2.4099 (inf)	loss_scale 8192.0000 (11880.4471)	mem 11634MB
[2024-06-29 13:41:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:06 lr 0.000099	 wd 0.0000	time 0.2918 (0.3269)	loss 1.1621 (0.9431)	grad_norm 2.3259 (inf)	loss_scale 8192.0000 (11720.1495)	mem 11634MB
[2024-06-29 13:41:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:33 lr 0.000099	 wd 0.0000	time 0.3936 (0.3268)	loss 0.9658 (0.9426)	grad_norm 2.6794 (inf)	loss_scale 8192.0000 (11573.2045)	mem 11634MB
[2024-06-29 13:42:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000099	 wd 0.0000	time 0.2754 (0.3258)	loss 1.1182 (0.9425)	grad_norm 2.0802 (inf)	loss_scale 8192.0000 (11438.0104)	mem 11634MB
[2024-06-29 13:42:17 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 1 training takes 0:13:37
[2024-06-29 13:42:29 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.006 (12.006)	Loss 0.3899 (0.3899)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 13:42:40 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 84.228 Acc@5 97.248
[2024-06-29 13:42:40 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.2%
[2024-06-29 13:42:40 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 84.23%
[2024-06-29 13:42:40 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 13:42:42 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 13:42:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:27:14 lr 0.000099	 wd 0.0000	time 10.7251 (10.7251)	loss 0.8535 (0.8535)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:43:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:16:43 lr 0.000099	 wd 0.0000	time 0.2940 (0.4176)	loss 0.9214 (0.9246)	grad_norm 2.4274 (2.2663)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:43:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:01 lr 0.000099	 wd 0.0000	time 0.2897 (0.3656)	loss 0.9331 (0.9167)	grad_norm 2.3822 (2.2172)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:44:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:12:45 lr 0.000099	 wd 0.0000	time 0.2792 (0.3479)	loss 1.0166 (0.9168)	grad_norm 1.9729 (2.2300)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:44:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:11:51 lr 0.000099	 wd 0.0000	time 0.2610 (0.3385)	loss 0.8281 (0.9166)	grad_norm 2.6270 (2.2407)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:45:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:11:12 lr 0.000099	 wd 0.0000	time 0.2793 (0.3361)	loss 1.1045 (0.9147)	grad_norm 2.3110 (2.2482)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:46:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:10:39 lr 0.000099	 wd 0.0000	time 0.2901 (0.3360)	loss 0.9775 (0.9162)	grad_norm 2.1203 (2.2631)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:46:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:10:01 lr 0.000099	 wd 0.0000	time 0.2707 (0.3338)	loss 1.0303 (0.9169)	grad_norm 2.1748 (2.2636)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:47:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:23 lr 0.000099	 wd 0.0000	time 0.3300 (0.3313)	loss 0.8354 (0.9157)	grad_norm 2.1592 (2.2638)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:47:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:08:50 lr 0.000098	 wd 0.0000	time 0.2863 (0.3312)	loss 0.8843 (0.9169)	grad_norm 2.1851 (2.2568)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:48:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:08:15 lr 0.000098	 wd 0.0000	time 0.2963 (0.3299)	loss 0.8208 (0.9172)	grad_norm 2.1254 (2.2510)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:48:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:42 lr 0.000098	 wd 0.0000	time 0.2825 (0.3297)	loss 0.9253 (0.9168)	grad_norm 2.2183 (2.2467)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:49:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:07:09 lr 0.000098	 wd 0.0000	time 0.3274 (0.3300)	loss 1.0674 (0.9160)	grad_norm 2.0874 (2.2399)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:49:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:37 lr 0.000098	 wd 0.0000	time 0.3140 (0.3308)	loss 0.9824 (0.9173)	grad_norm 2.5147 (2.2487)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:50:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:06:05 lr 0.000098	 wd 0.0000	time 0.3288 (0.3317)	loss 0.9277 (0.9176)	grad_norm 2.2302 (2.2527)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:51:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:33 lr 0.000098	 wd 0.0000	time 0.3150 (0.3329)	loss 0.8755 (0.9174)	grad_norm 1.6205 (2.2514)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:51:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:05:00 lr 0.000098	 wd 0.0000	time 0.3537 (0.3335)	loss 0.9321 (0.9175)	grad_norm 2.9302 (2.2483)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:52:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:27 lr 0.000098	 wd 0.0000	time 0.2860 (0.3333)	loss 0.9609 (0.9178)	grad_norm 1.8561 (2.2395)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:52:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:53 lr 0.000098	 wd 0.0000	time 0.2948 (0.3324)	loss 0.8169 (0.9173)	grad_norm 1.8842 (2.2341)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:53:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:19 lr 0.000098	 wd 0.0000	time 0.2934 (0.3319)	loss 0.8457 (0.9177)	grad_norm 2.1712 (2.2347)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:53:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:47 lr 0.000098	 wd 0.0000	time 0.3243 (0.3334)	loss 0.8984 (0.9174)	grad_norm 2.2631 (2.2342)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:54:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:14 lr 0.000098	 wd 0.0000	time 0.3780 (0.3353)	loss 1.0205 (0.9179)	grad_norm 1.8381 (2.2307)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:55:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:41 lr 0.000098	 wd 0.0000	time 0.5380 (0.3372)	loss 0.8623 (0.9176)	grad_norm 2.0676 (2.2265)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:55:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:08 lr 0.000098	 wd 0.0000	time 0.3764 (0.3387)	loss 1.0098 (0.9172)	grad_norm 2.0077 (2.2209)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:56:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:34 lr 0.000098	 wd 0.0000	time 0.3202 (0.3410)	loss 1.1680 (0.9174)	grad_norm 2.2137 (2.2261)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 13:56:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000098	 wd 0.0000	time 0.2806 (0.3401)	loss 0.8672 (0.9176)	grad_norm 2.2400 (2.2249)	loss_scale 16384.0000 (8231.3059)	mem 11634MB
[2024-06-29 13:56:55 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 2 training takes 0:14:13
[2024-06-29 13:57:07 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.160 (12.160)	Loss 0.3589 (0.3589)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 11634MB
[2024-06-29 13:57:18 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 84.460 Acc@5 97.408
[2024-06-29 13:57:18 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-06-29 13:57:18 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 84.46%
[2024-06-29 13:57:18 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 13:57:20 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 13:57:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:09:20 lr 0.000098	 wd 0.0000	time 10.2960 (10.2960)	loss 0.7783 (0.7783)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:58:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:16:37 lr 0.000098	 wd 0.0000	time 0.2736 (0.4151)	loss 0.8633 (0.9016)	grad_norm 2.2534 (2.1832)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:58:33 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:13:55 lr 0.000097	 wd 0.0000	time 0.2732 (0.3632)	loss 0.8682 (0.9022)	grad_norm 2.0776 (2.1982)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:59:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:12:42 lr 0.000097	 wd 0.0000	time 0.2992 (0.3465)	loss 1.0088 (0.9054)	grad_norm 2.3178 (2.1800)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 13:59:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:11:50 lr 0.000097	 wd 0.0000	time 0.2972 (0.3381)	loss 0.9258 (0.9046)	grad_norm 1.9521 (2.1913)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:00:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:11:06 lr 0.000097	 wd 0.0000	time 0.2871 (0.3328)	loss 0.8755 (0.9041)	grad_norm 2.6228 (2.1745)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:00:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:10:26 lr 0.000097	 wd 0.0000	time 0.2892 (0.3294)	loss 0.8506 (0.9043)	grad_norm 2.5284 (2.1667)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:01:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:09:49 lr 0.000097	 wd 0.0000	time 0.2846 (0.3270)	loss 1.1289 (0.9045)	grad_norm 2.3227 (2.1607)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:01:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:13 lr 0.000097	 wd 0.0000	time 0.2915 (0.3253)	loss 0.9946 (0.9053)	grad_norm 2.1228 (2.1630)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:02:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:08:38 lr 0.000097	 wd 0.0000	time 0.2863 (0.3238)	loss 1.0078 (0.9058)	grad_norm 2.2045 (2.1636)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:02:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:08:04 lr 0.000097	 wd 0.0000	time 0.2990 (0.3227)	loss 0.9849 (0.9073)	grad_norm 2.4048 (2.1663)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:03:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:31 lr 0.000097	 wd 0.0000	time 0.2690 (0.3221)	loss 0.8545 (0.9084)	grad_norm 2.3195 (2.1643)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:03:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:06:58 lr 0.000097	 wd 0.0000	time 0.2896 (0.3212)	loss 1.0059 (0.9086)	grad_norm 2.5526 (2.1651)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:04:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:25 lr 0.000097	 wd 0.0000	time 0.2890 (0.3211)	loss 1.0029 (0.9088)	grad_norm 2.5349 (2.1666)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:04:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:05:53 lr 0.000097	 wd 0.0000	time 0.3031 (0.3209)	loss 0.8350 (0.9094)	grad_norm 1.9081 (2.1617)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:05:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:21 lr 0.000097	 wd 0.0000	time 0.3190 (0.3210)	loss 0.7783 (0.9101)	grad_norm 1.9437 (2.1605)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:05:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:04:49 lr 0.000096	 wd 0.0000	time 0.3102 (0.3209)	loss 0.8696 (0.9095)	grad_norm 1.8582 (2.1594)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:06:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:17 lr 0.000096	 wd 0.0000	time 0.3033 (0.3210)	loss 0.8633 (0.9093)	grad_norm 2.1478 (2.1563)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:06:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:45 lr 0.000096	 wd 0.0000	time 0.4096 (0.3214)	loss 0.9858 (0.9089)	grad_norm 2.2661 (2.1538)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:07:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:14 lr 0.000096	 wd 0.0000	time 0.3040 (0.3236)	loss 0.8887 (0.9085)	grad_norm 2.2378 (2.1556)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:08:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:43 lr 0.000096	 wd 0.0000	time 0.3042 (0.3261)	loss 0.9277 (0.9088)	grad_norm 2.3673 (2.1528)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:08:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:12 lr 0.000096	 wd 0.0000	time 0.3041 (0.3289)	loss 0.8403 (0.9087)	grad_norm 2.1021 (2.1488)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:09:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:40 lr 0.000096	 wd 0.0000	time 0.4202 (0.3313)	loss 0.9312 (0.9084)	grad_norm 1.8622 (2.1440)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:10:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:07 lr 0.000096	 wd 0.0000	time 0.3332 (0.3337)	loss 0.9458 (0.9084)	grad_norm 2.1236 (2.1454)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:10:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:34 lr 0.000096	 wd 0.0000	time 0.2409 (0.3339)	loss 0.8960 (0.9080)	grad_norm 2.0184 (2.1432)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:11:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000096	 wd 0.0000	time 0.2796 (0.3326)	loss 0.9746 (0.9081)	grad_norm 1.9209 (2.1448)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:11:14 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 3 training takes 0:13:54
[2024-06-29 14:11:27 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.941 (12.941)	Loss 0.3784 (0.3784)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 11634MB
[2024-06-29 14:11:37 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 84.756 Acc@5 97.422
[2024-06-29 14:11:37 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-06-29 14:11:37 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 84.76%
[2024-06-29 14:11:37 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 14:11:39 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 14:11:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:12:15 lr 0.000096	 wd 0.0000	time 10.3657 (10.3657)	loss 0.9385 (0.9385)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:12:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:16:28 lr 0.000096	 wd 0.0000	time 0.2843 (0.4116)	loss 0.8296 (0.8891)	grad_norm 2.3195 (2.2329)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:12:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:13:56 lr 0.000096	 wd 0.0000	time 0.2774 (0.3635)	loss 1.0234 (0.8984)	grad_norm 2.2337 (2.1684)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:13:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:12:42 lr 0.000095	 wd 0.0000	time 0.2686 (0.3464)	loss 0.8706 (0.8986)	grad_norm 2.1616 (2.1556)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:13:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:11:49 lr 0.000095	 wd 0.0000	time 0.2860 (0.3377)	loss 0.9604 (0.8943)	grad_norm 1.6672 (2.1254)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:14:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:11:06 lr 0.000095	 wd 0.0000	time 0.2905 (0.3327)	loss 1.0127 (0.8939)	grad_norm 2.9717 (2.1161)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:14:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:10:27 lr 0.000095	 wd 0.0000	time 0.2830 (0.3301)	loss 0.8716 (0.8963)	grad_norm 2.0082 (2.1127)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:15:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:09:51 lr 0.000095	 wd 0.0000	time 0.2859 (0.3283)	loss 1.0527 (0.8941)	grad_norm 2.5191 (2.1232)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:16:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:15 lr 0.000095	 wd 0.0000	time 0.2796 (0.3265)	loss 0.7856 (0.8931)	grad_norm 1.6649 (2.1104)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:16:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:08:40 lr 0.000095	 wd 0.0000	time 0.2983 (0.3251)	loss 0.9360 (0.8939)	grad_norm 2.2385 (2.1095)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:17:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:08:09 lr 0.000095	 wd 0.0000	time 0.2797 (0.3256)	loss 0.8628 (0.8939)	grad_norm 1.8890 (2.1144)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:17:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:07:35 lr 0.000095	 wd 0.0000	time 0.2896 (0.3248)	loss 0.9805 (0.8949)	grad_norm 2.0636 (2.1158)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:18:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:07:01 lr 0.000095	 wd 0.0000	time 0.2883 (0.3239)	loss 0.7930 (0.8952)	grad_norm 1.9387 (2.1083)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:18:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:28 lr 0.000095	 wd 0.0000	time 0.3085 (0.3232)	loss 0.7515 (0.8945)	grad_norm 2.1583 (2.1137)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:19:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:05:55 lr 0.000094	 wd 0.0000	time 0.3163 (0.3226)	loss 0.8779 (0.8939)	grad_norm 2.2485 (2.1222)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:19:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:23 lr 0.000094	 wd 0.0000	time 0.3232 (0.3226)	loss 0.7925 (0.8941)	grad_norm 1.8325 (inf)	loss_scale 16384.0000 (16493.1539)	mem 11634MB
[2024-06-29 14:20:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:04:51 lr 0.000094	 wd 0.0000	time 0.3625 (0.3237)	loss 0.9512 (0.8945)	grad_norm 1.7977 (inf)	loss_scale 8192.0000 (16005.3567)	mem 11634MB
[2024-06-29 14:20:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:20 lr 0.000094	 wd 0.0000	time 0.3265 (0.3250)	loss 0.9209 (0.8949)	grad_norm 1.9352 (inf)	loss_scale 8192.0000 (15546.0176)	mem 11634MB
[2024-06-29 14:21:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:49 lr 0.000094	 wd 0.0000	time 0.3354 (0.3269)	loss 0.9282 (0.8959)	grad_norm 2.0562 (inf)	loss_scale 8192.0000 (15137.6880)	mem 11634MB
[2024-06-29 14:22:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:17 lr 0.000094	 wd 0.0000	time 0.3137 (0.3288)	loss 0.8618 (0.8955)	grad_norm 2.2209 (inf)	loss_scale 8192.0000 (14772.3177)	mem 11634MB
[2024-06-29 14:22:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:44 lr 0.000094	 wd 0.0000	time 0.3354 (0.3285)	loss 0.7646 (0.8951)	grad_norm 2.0020 (inf)	loss_scale 8192.0000 (14443.4663)	mem 11634MB
[2024-06-29 14:23:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:11 lr 0.000094	 wd 0.0000	time 0.2981 (0.3283)	loss 0.8110 (0.8952)	grad_norm 2.1142 (inf)	loss_scale 8192.0000 (14145.9191)	mem 11634MB
[2024-06-29 14:23:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:39 lr 0.000094	 wd 0.0000	time 0.2973 (0.3281)	loss 0.8916 (0.8951)	grad_norm 1.7589 (inf)	loss_scale 8192.0000 (13875.4094)	mem 11634MB
[2024-06-29 14:24:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:06 lr 0.000094	 wd 0.0000	time 0.3088 (0.3280)	loss 0.8315 (0.8954)	grad_norm 2.5741 (inf)	loss_scale 8192.0000 (13628.4120)	mem 11634MB
[2024-06-29 14:24:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:33 lr 0.000093	 wd 0.0000	time 0.4102 (0.3284)	loss 0.8027 (0.8954)	grad_norm 1.8264 (inf)	loss_scale 8192.0000 (13401.9892)	mem 11634MB
[2024-06-29 14:25:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000093	 wd 0.0000	time 0.2674 (0.3282)	loss 0.7925 (0.8953)	grad_norm 2.1966 (inf)	loss_scale 8192.0000 (13193.6729)	mem 11634MB
[2024-06-29 14:25:23 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 4 training takes 0:13:43
[2024-06-29 14:25:35 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.993 (11.993)	Loss 0.3684 (0.3684)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 14:25:45 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 84.936 Acc@5 97.458
[2024-06-29 14:25:45 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-06-29 14:25:45 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 84.94%
[2024-06-29 14:25:45 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 14:25:48 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 14:25:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:08:02 lr 0.000093	 wd 0.0000	time 10.2648 (10.2648)	loss 1.0537 (1.0537)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:26:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:16:36 lr 0.000093	 wd 0.0000	time 0.2984 (0.4149)	loss 0.8281 (0.8830)	grad_norm 2.3270 (2.1338)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:27:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:13:55 lr 0.000093	 wd 0.0000	time 0.2907 (0.3630)	loss 0.8301 (0.8889)	grad_norm 2.4577 (2.1514)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:27:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:12:43 lr 0.000093	 wd 0.0000	time 0.2932 (0.3466)	loss 0.8389 (0.8863)	grad_norm 1.7903 (2.1784)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:28:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:11:49 lr 0.000093	 wd 0.0000	time 0.2920 (0.3376)	loss 0.7856 (0.8850)	grad_norm 1.9671 (2.1516)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:28:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:11:04 lr 0.000093	 wd 0.0000	time 0.2842 (0.3321)	loss 0.8809 (0.8848)	grad_norm 2.1533 (2.1394)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:29:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:10:25 lr 0.000093	 wd 0.0000	time 0.2787 (0.3287)	loss 1.1719 (0.8875)	grad_norm 1.7889 (2.1365)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:29:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:09:49 lr 0.000093	 wd 0.0000	time 0.2888 (0.3271)	loss 0.9082 (0.8870)	grad_norm 2.7870 (2.1342)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:30:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:13 lr 0.000093	 wd 0.0000	time 0.2843 (0.3252)	loss 0.9155 (0.8880)	grad_norm 2.3740 (2.1302)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:30:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:08:38 lr 0.000092	 wd 0.0000	time 0.2813 (0.3236)	loss 0.8564 (0.8878)	grad_norm 2.0416 (2.1303)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:31:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:08:04 lr 0.000092	 wd 0.0000	time 0.2550 (0.3226)	loss 0.8086 (0.8877)	grad_norm 2.2635 (2.1265)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:31:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:07:31 lr 0.000092	 wd 0.0000	time 0.2964 (0.3217)	loss 0.8184 (0.8884)	grad_norm 2.0417 (2.1328)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:32:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:06:58 lr 0.000092	 wd 0.0000	time 0.2849 (0.3211)	loss 1.0303 (0.8885)	grad_norm 1.6091 (2.1310)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:32:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:25 lr 0.000092	 wd 0.0000	time 0.3029 (0.3205)	loss 0.8813 (0.8885)	grad_norm 1.9197 (2.1258)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:33:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:05:52 lr 0.000092	 wd 0.0000	time 0.2953 (0.3201)	loss 0.9043 (0.8877)	grad_norm 2.3765 (2.1233)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:33:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:20 lr 0.000092	 wd 0.0000	time 0.2975 (0.3200)	loss 0.9292 (0.8875)	grad_norm 2.6823 (2.1138)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:34:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:04:48 lr 0.000092	 wd 0.0000	time 0.2920 (0.3197)	loss 0.8311 (0.8870)	grad_norm 1.9960 (2.1111)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:34:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:16 lr 0.000092	 wd 0.0000	time 0.3011 (0.3196)	loss 0.9707 (0.8868)	grad_norm 1.7941 (2.1163)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:35:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:44 lr 0.000091	 wd 0.0000	time 0.2801 (0.3195)	loss 0.8394 (0.8866)	grad_norm 1.7865 (2.1144)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:35:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:12 lr 0.000091	 wd 0.0000	time 0.3054 (0.3195)	loss 1.0215 (0.8869)	grad_norm 2.4647 (2.1165)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:36:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:40 lr 0.000091	 wd 0.0000	time 0.3107 (0.3196)	loss 1.0684 (0.8869)	grad_norm 2.0665 (2.1232)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:37:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:08 lr 0.000091	 wd 0.0000	time 0.3110 (0.3208)	loss 0.9224 (0.8861)	grad_norm 2.0493 (2.1232)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:37:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:37 lr 0.000091	 wd 0.0000	time 0.3146 (0.3235)	loss 0.8843 (0.8862)	grad_norm 2.1347 (2.1214)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:38:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:05 lr 0.000091	 wd 0.0000	time 0.3060 (0.3261)	loss 0.7314 (0.8856)	grad_norm 2.1644 (2.1195)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:38:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:33 lr 0.000091	 wd 0.0000	time 0.3346 (0.3285)	loss 0.9761 (0.8854)	grad_norm 2.4104 (2.1178)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:39:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000091	 wd 0.0000	time 0.2696 (0.3281)	loss 0.8672 (0.8856)	grad_norm 2.4466 (2.1186)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:39:31 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 5 training takes 0:13:43
[2024-06-29 14:39:44 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.694 (12.694)	Loss 0.3616 (0.3616)	Acc@1 91.797 (91.797)	Acc@5 99.023 (99.023)	Mem 11634MB
[2024-06-29 14:39:54 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.020 Acc@5 97.566
[2024-06-29 14:39:54 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-06-29 14:39:54 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.02%
[2024-06-29 14:39:54 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 14:39:56 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 14:40:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:10:39 lr 0.000091	 wd 0.0000	time 10.3277 (10.3277)	loss 0.9160 (0.9160)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:40:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:16:27 lr 0.000090	 wd 0.0000	time 0.3048 (0.4113)	loss 0.8135 (0.8801)	grad_norm 2.3272 (2.1111)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:41:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:13:54 lr 0.000090	 wd 0.0000	time 0.2821 (0.3625)	loss 0.7832 (0.8758)	grad_norm 2.5923 (2.1261)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:41:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:12:45 lr 0.000090	 wd 0.0000	time 0.2975 (0.3474)	loss 0.8877 (0.8767)	grad_norm 1.9960 (2.1131)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:42:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:12:02 lr 0.000090	 wd 0.0000	time 0.2694 (0.3436)	loss 0.8662 (0.8773)	grad_norm 1.7929 (2.0925)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:42:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:11:17 lr 0.000090	 wd 0.0000	time 0.2935 (0.3385)	loss 0.9180 (0.8760)	grad_norm 1.8161 (2.0802)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 14:43:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:10:35 lr 0.000090	 wd 0.0000	time 0.2800 (0.3341)	loss 0.9365 (0.8761)	grad_norm 1.9248 (2.0799)	loss_scale 16384.0000 (9527.8003)	mem 11634MB
[2024-06-29 14:43:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:09:56 lr 0.000090	 wd 0.0000	time 0.2825 (0.3313)	loss 0.8940 (0.8777)	grad_norm 1.7643 (2.0665)	loss_scale 16384.0000 (10505.8602)	mem 11634MB
[2024-06-29 14:44:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:19 lr 0.000090	 wd 0.0000	time 0.2833 (0.3289)	loss 0.7910 (0.8791)	grad_norm 1.9960 (2.0686)	loss_scale 16384.0000 (11239.7104)	mem 11634MB
[2024-06-29 14:44:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:08:45 lr 0.000089	 wd 0.0000	time 0.2675 (0.3281)	loss 0.8657 (0.8788)	grad_norm 2.2085 (2.0756)	loss_scale 16384.0000 (11810.6637)	mem 11634MB
[2024-06-29 14:45:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:08:11 lr 0.000089	 wd 0.0000	time 0.2887 (0.3272)	loss 0.8281 (0.8791)	grad_norm 1.6284 (2.0752)	loss_scale 16384.0000 (12267.5405)	mem 11634MB
[2024-06-29 14:45:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:37 lr 0.000089	 wd 0.0000	time 0.2730 (0.3265)	loss 0.7910 (0.8783)	grad_norm 1.8914 (2.0749)	loss_scale 16384.0000 (12641.4242)	mem 11634MB
[2024-06-29 14:46:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:07:04 lr 0.000089	 wd 0.0000	time 0.2482 (0.3263)	loss 0.7168 (0.8785)	grad_norm 2.0604 (2.0771)	loss_scale 16384.0000 (12953.0458)	mem 11634MB
[2024-06-29 14:46:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:31 lr 0.000089	 wd 0.0000	time 0.2801 (0.3254)	loss 0.8789 (0.8796)	grad_norm 2.4221 (2.0787)	loss_scale 16384.0000 (13216.7625)	mem 11634MB
[2024-06-29 14:47:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:05:58 lr 0.000089	 wd 0.0000	time 0.2975 (0.3251)	loss 1.1152 (0.8782)	grad_norm 2.4977 (2.0773)	loss_scale 16384.0000 (13442.8323)	mem 11634MB
[2024-06-29 14:48:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:25 lr 0.000089	 wd 0.0000	time 0.3117 (0.3250)	loss 0.7129 (0.8780)	grad_norm 1.7805 (2.0796)	loss_scale 16384.0000 (13638.7795)	mem 11634MB
[2024-06-29 14:48:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:04:52 lr 0.000089	 wd 0.0000	time 0.2865 (0.3246)	loss 0.8931 (0.8779)	grad_norm 1.9568 (2.0821)	loss_scale 16384.0000 (13810.2486)	mem 11634MB
[2024-06-29 14:49:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:20 lr 0.000088	 wd 0.0000	time 0.3168 (0.3248)	loss 1.0107 (0.8767)	grad_norm 1.9358 (2.0788)	loss_scale 16384.0000 (13961.5567)	mem 11634MB
[2024-06-29 14:49:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:48 lr 0.000088	 wd 0.0000	time 0.2933 (0.3259)	loss 0.7334 (0.8761)	grad_norm 1.8665 (2.0882)	loss_scale 16384.0000 (14096.0622)	mem 11634MB
[2024-06-29 14:50:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:16 lr 0.000088	 wd 0.0000	time 0.3620 (0.3271)	loss 0.9692 (0.8762)	grad_norm 3.1596 (2.0916)	loss_scale 16384.0000 (14216.4166)	mem 11634MB
[2024-06-29 14:50:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:45 lr 0.000088	 wd 0.0000	time 0.3934 (0.3297)	loss 1.0752 (0.8772)	grad_norm 2.6028 (2.0916)	loss_scale 16384.0000 (14324.7416)	mem 11634MB
[2024-06-29 14:51:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:13 lr 0.000088	 wd 0.0000	time 0.3006 (0.3324)	loss 0.8857 (0.8773)	grad_norm 2.8030 (2.0966)	loss_scale 16384.0000 (14422.7549)	mem 11634MB
[2024-06-29 14:52:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:40 lr 0.000088	 wd 0.0000	time 0.2967 (0.3341)	loss 0.9429 (0.8775)	grad_norm 2.4522 (2.0937)	loss_scale 16384.0000 (14511.8619)	mem 11634MB
[2024-06-29 14:52:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:07 lr 0.000088	 wd 0.0000	time 0.4103 (0.3348)	loss 0.9141 (0.8774)	grad_norm 2.0172 (2.0938)	loss_scale 16384.0000 (14593.2238)	mem 11634MB
[2024-06-29 14:53:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:34 lr 0.000087	 wd 0.0000	time 0.3373 (0.3368)	loss 0.8711 (0.8769)	grad_norm 2.1657 (2.0923)	loss_scale 16384.0000 (14667.8084)	mem 11634MB
[2024-06-29 14:53:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000087	 wd 0.0000	time 0.2669 (0.3363)	loss 0.8027 (0.8764)	grad_norm 1.9261 (2.0909)	loss_scale 16384.0000 (14736.4286)	mem 11634MB
[2024-06-29 14:54:00 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 6 training takes 0:14:03
[2024-06-29 14:54:12 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.680 (11.680)	Loss 0.3601 (0.3601)	Acc@1 91.797 (91.797)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-06-29 14:54:23 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.006 Acc@5 97.584
[2024-06-29 14:54:23 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-06-29 14:54:23 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.02%
[2024-06-29 14:54:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:28:13 lr 0.000087	 wd 0.0000	time 12.1877 (12.1877)	loss 1.0049 (1.0049)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:55:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:17:22 lr 0.000087	 wd 0.0000	time 0.3014 (0.4341)	loss 1.0596 (0.8791)	grad_norm 1.9488 (2.0768)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:55:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:19 lr 0.000087	 wd 0.0000	time 0.2825 (0.3734)	loss 0.8662 (0.8660)	grad_norm 2.0180 (2.0768)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:56:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:13:05 lr 0.000087	 wd 0.0000	time 0.2894 (0.3569)	loss 0.8384 (0.8655)	grad_norm 2.2420 (2.0472)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:56:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:06 lr 0.000087	 wd 0.0000	time 0.2927 (0.3456)	loss 0.9331 (0.8654)	grad_norm 2.0203 (2.0484)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:57:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:18 lr 0.000087	 wd 0.0000	time 0.3096 (0.3389)	loss 0.8364 (0.8653)	grad_norm 2.0993 (2.0480)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:57:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:10:37 lr 0.000086	 wd 0.0000	time 0.2785 (0.3351)	loss 0.8042 (0.8659)	grad_norm 2.8963 (2.0505)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:58:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:09:59 lr 0.000086	 wd 0.0000	time 0.3185 (0.3326)	loss 0.7979 (0.8654)	grad_norm 1.7942 (2.0545)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:58:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:22 lr 0.000086	 wd 0.0000	time 0.2792 (0.3304)	loss 0.8535 (0.8658)	grad_norm 1.8157 (2.0503)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:59:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:08:46 lr 0.000086	 wd 0.0000	time 0.3032 (0.3287)	loss 0.6895 (0.8663)	grad_norm 2.0451 (2.0528)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 14:59:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:12 lr 0.000086	 wd 0.0000	time 0.2943 (0.3280)	loss 0.8164 (0.8670)	grad_norm 2.1345 (2.0483)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:00:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:38 lr 0.000086	 wd 0.0000	time 0.2670 (0.3269)	loss 0.8516 (0.8660)	grad_norm 1.8336 (2.0464)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:00:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:07:04 lr 0.000086	 wd 0.0000	time 0.2716 (0.3261)	loss 0.7563 (0.8666)	grad_norm 1.9322 (2.0493)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:01:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:30 lr 0.000085	 wd 0.0000	time 0.2862 (0.3253)	loss 0.9067 (0.8664)	grad_norm 2.1428 (2.0479)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:01:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:05:57 lr 0.000085	 wd 0.0000	time 0.2582 (0.3245)	loss 0.9434 (0.8676)	grad_norm 2.1167 (inf)	loss_scale 8192.0000 (15998.0814)	mem 11634MB
[2024-06-29 15:02:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:24 lr 0.000085	 wd 0.0000	time 0.2892 (0.3240)	loss 0.8228 (0.8671)	grad_norm 2.0952 (inf)	loss_scale 8192.0000 (15478.0227)	mem 11634MB
[2024-06-29 15:03:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:04:51 lr 0.000085	 wd 0.0000	time 0.2766 (0.3235)	loss 0.9497 (0.8677)	grad_norm 2.0106 (inf)	loss_scale 8192.0000 (15022.9307)	mem 11634MB
[2024-06-29 15:03:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:19 lr 0.000085	 wd 0.0000	time 0.2838 (0.3230)	loss 0.8193 (0.8674)	grad_norm 1.7086 (inf)	loss_scale 8192.0000 (14621.3474)	mem 11634MB
[2024-06-29 15:04:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:46 lr 0.000085	 wd 0.0000	time 0.3089 (0.3228)	loss 0.8389 (0.8666)	grad_norm 2.2212 (inf)	loss_scale 8192.0000 (14264.3598)	mem 11634MB
[2024-06-29 15:04:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:14 lr 0.000085	 wd 0.0000	time 0.2984 (0.3225)	loss 0.9971 (0.8678)	grad_norm 2.3428 (inf)	loss_scale 8192.0000 (13944.9300)	mem 11634MB
[2024-06-29 15:05:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:42 lr 0.000084	 wd 0.0000	time 0.3027 (0.3238)	loss 0.8042 (0.8676)	grad_norm 2.0508 (inf)	loss_scale 8192.0000 (13657.4273)	mem 11634MB
[2024-06-29 15:05:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:10 lr 0.000084	 wd 0.0000	time 0.3128 (0.3238)	loss 0.8960 (0.8679)	grad_norm 1.8487 (inf)	loss_scale 8192.0000 (13397.2927)	mem 11634MB
[2024-06-29 15:06:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:37 lr 0.000084	 wd 0.0000	time 0.3151 (0.3238)	loss 0.8486 (0.8675)	grad_norm 2.2353 (inf)	loss_scale 8192.0000 (13160.7960)	mem 11634MB
[2024-06-29 15:06:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:05 lr 0.000084	 wd 0.0000	time 0.2741 (0.3237)	loss 0.8589 (0.8674)	grad_norm 1.9833 (inf)	loss_scale 8192.0000 (12944.8553)	mem 11634MB
[2024-06-29 15:07:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:33 lr 0.000084	 wd 0.0000	time 0.2815 (0.3238)	loss 0.8057 (0.8679)	grad_norm 1.9175 (inf)	loss_scale 8192.0000 (12746.9021)	mem 11634MB
[2024-06-29 15:07:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000084	 wd 0.0000	time 0.2662 (0.3231)	loss 0.8071 (0.8684)	grad_norm 2.1237 (inf)	loss_scale 8192.0000 (12564.7789)	mem 11634MB
[2024-06-29 15:07:53 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 7 training takes 0:13:30
[2024-06-29 15:08:06 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.671 (12.671)	Loss 0.3440 (0.3440)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 15:08:16 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 84.972 Acc@5 97.566
[2024-06-29 15:08:16 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-06-29 15:08:16 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.02%
[2024-06-29 15:08:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:32:48 lr 0.000084	 wd 0.0000	time 12.2976 (12.2976)	loss 0.9600 (0.9600)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:08:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:17:17 lr 0.000083	 wd 0.0000	time 0.2892 (0.4321)	loss 0.8237 (0.8673)	grad_norm 2.4246 (2.0960)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:09:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:14:17 lr 0.000083	 wd 0.0000	time 0.2805 (0.3723)	loss 0.8174 (0.8616)	grad_norm 2.1368 (2.0825)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:10:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:13:06 lr 0.000083	 wd 0.0000	time 0.3148 (0.3570)	loss 0.9150 (0.8590)	grad_norm 1.9066 (2.0508)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:10:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:12:06 lr 0.000083	 wd 0.0000	time 0.2925 (0.3458)	loss 0.8926 (0.8591)	grad_norm 1.9056 (2.0479)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:11:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:11:19 lr 0.000083	 wd 0.0000	time 0.2712 (0.3393)	loss 0.9106 (0.8606)	grad_norm 2.0014 (2.0450)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:11:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:10:36 lr 0.000083	 wd 0.0000	time 0.2906 (0.3347)	loss 0.7231 (0.8608)	grad_norm 1.8332 (2.0405)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:12:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:09:57 lr 0.000083	 wd 0.0000	time 0.3059 (0.3316)	loss 0.9814 (0.8593)	grad_norm 1.8645 (2.0536)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:12:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:20 lr 0.000082	 wd 0.0000	time 0.2733 (0.3291)	loss 0.9702 (0.8605)	grad_norm 1.8266 (2.0553)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:13:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:08:44 lr 0.000082	 wd 0.0000	time 0.3099 (0.3272)	loss 0.9458 (0.8601)	grad_norm 2.0511 (2.0476)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:13:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:09 lr 0.000082	 wd 0.0000	time 0.2889 (0.3259)	loss 0.8936 (0.8615)	grad_norm 1.7179 (2.0542)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:14:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:35 lr 0.000082	 wd 0.0000	time 0.2885 (0.3248)	loss 0.8228 (0.8616)	grad_norm 2.5967 (2.0628)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:14:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:01 lr 0.000082	 wd 0.0000	time 0.2848 (0.3239)	loss 0.8721 (0.8620)	grad_norm 2.7670 (2.0584)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:15:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:28 lr 0.000082	 wd 0.0000	time 0.2920 (0.3234)	loss 0.8174 (0.8624)	grad_norm 1.9003 (2.0567)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:15:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:05:56 lr 0.000081	 wd 0.0000	time 0.2603 (0.3233)	loss 0.8198 (0.8632)	grad_norm 2.0847 (2.0554)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:16:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:23 lr 0.000081	 wd 0.0000	time 0.3012 (0.3229)	loss 0.7407 (0.8633)	grad_norm 1.7121 (2.0519)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:16:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:04:51 lr 0.000081	 wd 0.0000	time 0.3573 (0.3228)	loss 0.8838 (0.8632)	grad_norm 2.5284 (2.0519)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:17:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:19 lr 0.000081	 wd 0.0000	time 0.2923 (0.3239)	loss 0.7759 (0.8638)	grad_norm 1.9216 (2.0554)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:18:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:48 lr 0.000081	 wd 0.0000	time 0.2369 (0.3248)	loss 0.7910 (0.8645)	grad_norm 3.0117 (2.0594)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:18:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:16 lr 0.000081	 wd 0.0000	time 0.3471 (0.3261)	loss 0.7568 (0.8638)	grad_norm 1.9151 (2.0566)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:19:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:45 lr 0.000080	 wd 0.0000	time 0.3275 (0.3295)	loss 0.9229 (0.8635)	grad_norm 2.1289 (2.0581)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:19:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:13 lr 0.000080	 wd 0.0000	time 0.2825 (0.3327)	loss 0.9614 (0.8637)	grad_norm 1.9870 (2.0554)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:20:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:41 lr 0.000080	 wd 0.0000	time 0.3146 (0.3353)	loss 0.9258 (0.8637)	grad_norm 2.0758 (2.0577)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:21:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:07 lr 0.000080	 wd 0.0000	time 0.3072 (0.3364)	loss 0.8950 (0.8635)	grad_norm 2.1404 (2.0561)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:21:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:34 lr 0.000080	 wd 0.0000	time 0.2638 (0.3365)	loss 0.8667 (0.8633)	grad_norm 2.5330 (2.0596)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:22:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000080	 wd 0.0000	time 0.2720 (0.3352)	loss 0.9277 (0.8635)	grad_norm 2.2214 (2.0609)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:22:17 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 8 training takes 0:14:01
[2024-06-29 15:22:29 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.290 (12.290)	Loss 0.3655 (0.3655)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 15:22:40 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.222 Acc@5 97.576
[2024-06-29 15:22:40 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-06-29 15:22:40 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.22%
[2024-06-29 15:22:40 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 15:22:42 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 15:22:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:25:35 lr 0.000080	 wd 0.0000	time 10.6855 (10.6855)	loss 0.7446 (0.7446)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:23:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:16:35 lr 0.000079	 wd 0.0000	time 0.2840 (0.4142)	loss 0.8574 (0.8485)	grad_norm 1.9724 (2.0311)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:23:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:14:08 lr 0.000079	 wd 0.0000	time 0.2759 (0.3685)	loss 0.7837 (0.8471)	grad_norm 2.1430 (2.0340)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:24:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:12:51 lr 0.000079	 wd 0.0000	time 0.2895 (0.3502)	loss 0.8979 (0.8536)	grad_norm 2.0982 (2.0239)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 15:24:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:11:55 lr 0.000079	 wd 0.0000	time 0.2831 (0.3406)	loss 0.7607 (0.8521)	grad_norm 2.0355 (2.0170)	loss_scale 16384.0000 (9622.0249)	mem 11634MB
[2024-06-29 15:25:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:11:10 lr 0.000079	 wd 0.0000	time 0.3066 (0.3351)	loss 0.7573 (0.8510)	grad_norm 1.6790 (2.0056)	loss_scale 16384.0000 (10971.7206)	mem 11634MB
[2024-06-29 15:26:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:10:29 lr 0.000079	 wd 0.0000	time 0.2763 (0.3312)	loss 0.8457 (0.8495)	grad_norm 2.1193 (2.0157)	loss_scale 16384.0000 (11872.2662)	mem 11634MB
[2024-06-29 15:26:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:09:51 lr 0.000078	 wd 0.0000	time 0.2742 (0.3285)	loss 0.9546 (0.8509)	grad_norm 2.7516 (2.0185)	loss_scale 16384.0000 (12515.8802)	mem 11634MB
[2024-06-29 15:27:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:15 lr 0.000078	 wd 0.0000	time 0.2995 (0.3266)	loss 0.9941 (0.8513)	grad_norm 1.6227 (2.0230)	loss_scale 16384.0000 (12998.7915)	mem 11634MB
[2024-06-29 15:27:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:08:44 lr 0.000078	 wd 0.0000	time 0.2565 (0.3272)	loss 0.9658 (0.8526)	grad_norm 1.8813 (2.0263)	loss_scale 16384.0000 (13374.5083)	mem 11634MB
[2024-06-29 15:28:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:08:09 lr 0.000078	 wd 0.0000	time 0.2843 (0.3258)	loss 0.8589 (0.8531)	grad_norm 2.0259 (2.0259)	loss_scale 16384.0000 (13675.1568)	mem 11634MB
[2024-06-29 15:28:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:38 lr 0.000078	 wd 0.0000	time 0.2826 (0.3268)	loss 0.9067 (0.8525)	grad_norm 2.0822 (2.0298)	loss_scale 16384.0000 (13921.1916)	mem 11634MB
[2024-06-29 15:29:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:07:04 lr 0.000078	 wd 0.0000	time 0.2966 (0.3258)	loss 0.8682 (0.8539)	grad_norm 2.1040 (2.0242)	loss_scale 16384.0000 (14126.2548)	mem 11634MB
[2024-06-29 15:29:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:30 lr 0.000077	 wd 0.0000	time 0.3018 (0.3250)	loss 0.8594 (0.8542)	grad_norm 2.0006 (2.0192)	loss_scale 16384.0000 (14299.7940)	mem 11634MB
[2024-06-29 15:30:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:05:57 lr 0.000077	 wd 0.0000	time 0.3090 (0.3244)	loss 0.9106 (0.8544)	grad_norm 1.8696 (2.0184)	loss_scale 16384.0000 (14448.5596)	mem 11634MB
[2024-06-29 15:30:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:24 lr 0.000077	 wd 0.0000	time 0.2817 (0.3238)	loss 0.8257 (0.8553)	grad_norm 1.7151 (2.0210)	loss_scale 16384.0000 (14577.5030)	mem 11634MB
[2024-06-29 15:31:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:04:52 lr 0.000077	 wd 0.0000	time 0.2837 (0.3247)	loss 0.8467 (0.8539)	grad_norm 2.1841 (2.0228)	loss_scale 16384.0000 (14690.3385)	mem 11634MB
[2024-06-29 15:31:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:19 lr 0.000077	 wd 0.0000	time 0.2823 (0.3241)	loss 0.8281 (0.8538)	grad_norm 2.0667 (2.0230)	loss_scale 16384.0000 (14789.9071)	mem 11634MB
[2024-06-29 15:32:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:47 lr 0.000077	 wd 0.0000	time 0.2900 (0.3237)	loss 0.8594 (0.8541)	grad_norm 1.7176 (2.0214)	loss_scale 16384.0000 (14878.4187)	mem 11634MB
[2024-06-29 15:32:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:14 lr 0.000076	 wd 0.0000	time 0.2837 (0.3238)	loss 0.8604 (0.8545)	grad_norm 2.0443 (2.0225)	loss_scale 16384.0000 (14957.6181)	mem 11634MB
[2024-06-29 15:33:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:42 lr 0.000076	 wd 0.0000	time 0.2572 (0.3235)	loss 0.7891 (0.8554)	grad_norm 2.1919 (2.0223)	loss_scale 16384.0000 (15028.9015)	mem 11634MB
[2024-06-29 15:34:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:09 lr 0.000076	 wd 0.0000	time 0.2925 (0.3231)	loss 0.8218 (0.8554)	grad_norm 2.0949 (2.0265)	loss_scale 16384.0000 (15093.3993)	mem 11634MB
[2024-06-29 15:34:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:37 lr 0.000076	 wd 0.0000	time 0.3325 (0.3239)	loss 0.6704 (0.8550)	grad_norm 2.0954 (2.0266)	loss_scale 16384.0000 (15152.0363)	mem 11634MB
[2024-06-29 15:35:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:06 lr 0.000076	 wd 0.0000	time 0.3729 (0.3270)	loss 0.8301 (0.8548)	grad_norm 1.7443 (2.0270)	loss_scale 16384.0000 (15205.5767)	mem 11634MB
[2024-06-29 15:35:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:33 lr 0.000075	 wd 0.0000	time 0.3439 (0.3289)	loss 0.9155 (0.8551)	grad_norm 1.8919 (2.0297)	loss_scale 16384.0000 (15254.6572)	mem 11634MB
[2024-06-29 15:36:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000075	 wd 0.0000	time 0.2803 (0.3291)	loss 0.8521 (0.8555)	grad_norm 1.8347 (2.0263)	loss_scale 16384.0000 (15299.8129)	mem 11634MB
[2024-06-29 15:36:28 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 9 training takes 0:13:45
[2024-06-29 15:36:40 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.202 (12.202)	Loss 0.3679 (0.3679)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 15:36:50 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.204 Acc@5 97.554
[2024-06-29 15:36:50 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-06-29 15:36:50 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.22%
[2024-06-29 15:37:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][0/2502]	eta 8:11:05 lr 0.000075	 wd 0.0000	time 11.7768 (11.7768)	loss 0.9111 (0.9111)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:37:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:17:30 lr 0.000075	 wd 0.0000	time 0.2914 (0.4374)	loss 0.9321 (0.8624)	grad_norm 1.8459 (2.0895)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:38:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:14:29 lr 0.000075	 wd 0.0000	time 0.2721 (0.3775)	loss 0.8110 (0.8565)	grad_norm 2.2016 (2.1042)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:38:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:13:09 lr 0.000075	 wd 0.0000	time 0.3276 (0.3585)	loss 0.8398 (0.8539)	grad_norm 1.9646 (2.0756)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:39:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:12:09 lr 0.000075	 wd 0.0000	time 0.2914 (0.3472)	loss 0.8398 (0.8541)	grad_norm 1.7719 (2.0572)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:39:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:11:24 lr 0.000074	 wd 0.0000	time 0.3000 (0.3419)	loss 0.7983 (0.8560)	grad_norm 1.9021 (2.0593)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:40:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:10:43 lr 0.000074	 wd 0.0000	time 0.2852 (0.3384)	loss 0.8022 (0.8552)	grad_norm 1.9794 (2.0734)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:40:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:10:05 lr 0.000074	 wd 0.0000	time 0.3188 (0.3363)	loss 0.7979 (0.8525)	grad_norm 1.7098 (2.0702)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:41:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:27 lr 0.000074	 wd 0.0000	time 0.3227 (0.3337)	loss 0.8091 (0.8534)	grad_norm 1.8418 (2.0550)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:41:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:08:51 lr 0.000074	 wd 0.0000	time 0.2931 (0.3316)	loss 0.7920 (0.8524)	grad_norm 1.7183 (2.0572)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:42:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:08:15 lr 0.000073	 wd 0.0000	time 0.2679 (0.3297)	loss 0.9595 (0.8517)	grad_norm 1.5682 (2.0531)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:42:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:07:40 lr 0.000073	 wd 0.0000	time 0.3095 (0.3282)	loss 0.8931 (0.8512)	grad_norm 2.0081 (2.0557)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:43:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:07:06 lr 0.000073	 wd 0.0000	time 0.2802 (0.3277)	loss 0.9399 (0.8508)	grad_norm 1.8970 (2.0568)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:43:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:35 lr 0.000073	 wd 0.0000	time 0.3017 (0.3288)	loss 0.7856 (0.8508)	grad_norm 1.9899 (2.0558)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:44:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:06:03 lr 0.000073	 wd 0.0000	time 0.3404 (0.3295)	loss 0.8193 (0.8510)	grad_norm 2.6695 (2.0535)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:45:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:29 lr 0.000073	 wd 0.0000	time 0.2970 (0.3287)	loss 0.8291 (0.8508)	grad_norm 1.9171 (2.0473)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:45:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:04:56 lr 0.000072	 wd 0.0000	time 0.3124 (0.3293)	loss 0.6689 (0.8502)	grad_norm 2.1948 (2.0485)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:46:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:23 lr 0.000072	 wd 0.0000	time 0.2961 (0.3286)	loss 0.7915 (0.8500)	grad_norm 2.3763 (2.0529)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:46:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:50 lr 0.000072	 wd 0.0000	time 0.2585 (0.3278)	loss 0.9624 (0.8502)	grad_norm 2.1219 (2.0594)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:47:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:16 lr 0.000072	 wd 0.0000	time 0.2977 (0.3272)	loss 0.9595 (0.8517)	grad_norm 2.0964 (inf)	loss_scale 16384.0000 (16573.6097)	mem 11634MB
[2024-06-29 15:47:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:44 lr 0.000072	 wd 0.0000	time 0.3040 (0.3283)	loss 1.0713 (0.8522)	grad_norm 2.3185 (inf)	loss_scale 16384.0000 (16564.1339)	mem 11634MB
[2024-06-29 15:48:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:12 lr 0.000071	 wd 0.0000	time 0.2954 (0.3284)	loss 0.8589 (0.8523)	grad_norm 2.1548 (inf)	loss_scale 16384.0000 (16555.5602)	mem 11634MB
[2024-06-29 15:48:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:39 lr 0.000071	 wd 0.0000	time 0.3110 (0.3282)	loss 0.8647 (0.8526)	grad_norm 1.9523 (inf)	loss_scale 16384.0000 (16547.7656)	mem 11634MB
[2024-06-29 15:49:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:06 lr 0.000071	 wd 0.0000	time 0.2957 (0.3281)	loss 0.9414 (0.8526)	grad_norm 1.7905 (inf)	loss_scale 16384.0000 (16540.6484)	mem 11634MB
[2024-06-29 15:49:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:33 lr 0.000071	 wd 0.0000	time 0.3087 (0.3281)	loss 0.9434 (0.8526)	grad_norm 2.3593 (inf)	loss_scale 16384.0000 (16534.1241)	mem 11634MB
[2024-06-29 15:50:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000071	 wd 0.0000	time 0.2818 (0.3273)	loss 0.7358 (0.8522)	grad_norm 2.1083 (inf)	loss_scale 16384.0000 (16528.1216)	mem 11634MB
[2024-06-29 15:50:31 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 10 training takes 0:13:41
[2024-06-29 15:50:44 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.320 (12.320)	Loss 0.3726 (0.3726)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 15:50:54 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.148 Acc@5 97.552
[2024-06-29 15:50:54 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-06-29 15:50:54 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.22%
[2024-06-29 15:51:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:03:34 lr 0.000071	 wd 0.0000	time 11.5965 (11.5965)	loss 0.7988 (0.7988)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:51:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:17:23 lr 0.000070	 wd 0.0000	time 0.2964 (0.4342)	loss 0.7812 (0.8436)	grad_norm 2.0908 (2.0829)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:52:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:14:23 lr 0.000070	 wd 0.0000	time 0.3021 (0.3750)	loss 0.8740 (0.8437)	grad_norm 2.0940 (2.0706)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:52:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:13:01 lr 0.000070	 wd 0.0000	time 0.2839 (0.3550)	loss 0.8257 (0.8412)	grad_norm 2.0103 (2.0645)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:53:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:12:03 lr 0.000070	 wd 0.0000	time 0.2910 (0.3441)	loss 0.8062 (0.8383)	grad_norm 1.9506 (2.0513)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:53:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:11:15 lr 0.000070	 wd 0.0000	time 0.2805 (0.3376)	loss 0.8364 (0.8367)	grad_norm 2.1281 (2.0536)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:54:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:10:33 lr 0.000069	 wd 0.0000	time 0.2937 (0.3333)	loss 0.8838 (0.8376)	grad_norm 1.9217 (2.0442)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:54:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:09:55 lr 0.000069	 wd 0.0000	time 0.2976 (0.3306)	loss 0.8276 (0.8389)	grad_norm 2.4518 (2.0450)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:55:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:18 lr 0.000069	 wd 0.0000	time 0.2893 (0.3284)	loss 0.9194 (0.8400)	grad_norm 1.8881 (2.0424)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:55:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:08:43 lr 0.000069	 wd 0.0000	time 0.2870 (0.3267)	loss 0.8608 (0.8429)	grad_norm 2.0211 (2.0466)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:56:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:08:10 lr 0.000069	 wd 0.0000	time 0.2357 (0.3265)	loss 0.8330 (0.8432)	grad_norm 1.7662 (2.0450)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:56:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:35 lr 0.000069	 wd 0.0000	time 0.2387 (0.3252)	loss 0.8530 (0.8426)	grad_norm 1.9946 (2.0438)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:57:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:07:02 lr 0.000068	 wd 0.0000	time 0.2891 (0.3242)	loss 0.9390 (0.8424)	grad_norm 2.1379 (2.0471)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:57:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:28 lr 0.000068	 wd 0.0000	time 0.2923 (0.3235)	loss 0.8022 (0.8431)	grad_norm 1.5554 (2.0514)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:58:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:05:55 lr 0.000068	 wd 0.0000	time 0.3193 (0.3228)	loss 0.8296 (0.8432)	grad_norm 1.7042 (2.0460)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:58:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:23 lr 0.000068	 wd 0.0000	time 0.3010 (0.3224)	loss 0.8218 (0.8426)	grad_norm 1.6608 (2.0483)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 15:59:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:04:50 lr 0.000068	 wd 0.0000	time 0.2715 (0.3221)	loss 0.7236 (0.8426)	grad_norm 2.2761 (2.0501)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:00:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:18 lr 0.000067	 wd 0.0000	time 0.3995 (0.3225)	loss 0.8228 (0.8420)	grad_norm 1.7650 (2.0501)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:00:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:48 lr 0.000067	 wd 0.0000	time 0.3538 (0.3253)	loss 0.8179 (0.8428)	grad_norm 1.7590 (2.0494)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:01:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:17 lr 0.000067	 wd 0.0000	time 0.4416 (0.3281)	loss 0.8013 (0.8435)	grad_norm 1.7931 (2.0420)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:01:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:45 lr 0.000067	 wd 0.0000	time 0.2912 (0.3298)	loss 0.6328 (0.8433)	grad_norm 2.2906 (2.0416)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:02:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:12 lr 0.000067	 wd 0.0000	time 0.3362 (0.3301)	loss 0.9307 (0.8429)	grad_norm 2.3918 (2.0385)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:03:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:39 lr 0.000066	 wd 0.0000	time 0.2906 (0.3303)	loss 0.9150 (0.8432)	grad_norm 1.7290 (2.0414)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:03:33 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:06 lr 0.000066	 wd 0.0000	time 0.2856 (0.3299)	loss 0.8379 (0.8432)	grad_norm 2.2279 (2.0436)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:04:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:33 lr 0.000066	 wd 0.0000	time 0.3100 (0.3298)	loss 0.8184 (0.8429)	grad_norm 2.2734 (2.0462)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:04:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000066	 wd 0.0000	time 0.2610 (0.3285)	loss 0.8145 (0.8429)	grad_norm 1.9081 (2.0447)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:04:38 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 11 training takes 0:13:44
[2024-06-29 16:04:51 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.448 (12.448)	Loss 0.3552 (0.3552)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 16:05:01 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.116 Acc@5 97.506
[2024-06-29 16:05:01 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-06-29 16:05:01 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.22%
[2024-06-29 16:05:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:20:59 lr 0.000066	 wd 0.0000	time 10.5753 (10.5753)	loss 0.7905 (0.7905)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:05:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:18:11 lr 0.000066	 wd 0.0000	time 0.2796 (0.4545)	loss 0.8008 (0.8375)	grad_norm 2.0459 (2.0404)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:06:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:14:41 lr 0.000065	 wd 0.0000	time 0.2955 (0.3828)	loss 0.7886 (0.8341)	grad_norm 1.8498 (2.0503)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:06:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:13:10 lr 0.000065	 wd 0.0000	time 0.2992 (0.3589)	loss 0.7734 (0.8417)	grad_norm 2.3271 (2.0602)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:07:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:12:17 lr 0.000065	 wd 0.0000	time 0.2853 (0.3510)	loss 0.7881 (0.8405)	grad_norm 1.6858 (2.0573)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:07:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:26 lr 0.000065	 wd 0.0000	time 0.2936 (0.3429)	loss 0.8794 (0.8424)	grad_norm 1.8035 (2.0409)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:08:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:10:41 lr 0.000065	 wd 0.0000	time 0.3035 (0.3375)	loss 0.8740 (0.8393)	grad_norm 1.9575 (2.0230)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:08:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:10:01 lr 0.000064	 wd 0.0000	time 0.2683 (0.3338)	loss 0.8032 (0.8410)	grad_norm 3.1322 (2.0460)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:09:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:23 lr 0.000064	 wd 0.0000	time 0.2843 (0.3311)	loss 1.0674 (0.8406)	grad_norm 2.2186 (2.0396)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:09:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:08:46 lr 0.000064	 wd 0.0000	time 0.2899 (0.3289)	loss 0.9482 (0.8403)	grad_norm 1.6149 (inf)	loss_scale 16384.0000 (16674.9478)	mem 11634MB
[2024-06-29 16:10:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:08:11 lr 0.000064	 wd 0.0000	time 0.2954 (0.3273)	loss 0.7856 (0.8408)	grad_norm 2.6799 (inf)	loss_scale 16384.0000 (16645.8821)	mem 11634MB
[2024-06-29 16:11:00 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:37 lr 0.000064	 wd 0.0000	time 0.2981 (0.3261)	loss 0.7935 (0.8423)	grad_norm 1.6674 (inf)	loss_scale 16384.0000 (16622.0963)	mem 11634MB
[2024-06-29 16:11:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:07:03 lr 0.000063	 wd 0.0000	time 0.2848 (0.3251)	loss 0.9360 (0.8425)	grad_norm 1.9281 (inf)	loss_scale 16384.0000 (16602.2714)	mem 11634MB
[2024-06-29 16:12:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:29 lr 0.000063	 wd 0.0000	time 0.3028 (0.3243)	loss 0.8330 (0.8427)	grad_norm 2.3982 (inf)	loss_scale 16384.0000 (16585.4942)	mem 11634MB
[2024-06-29 16:12:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:05:56 lr 0.000063	 wd 0.0000	time 0.2949 (0.3237)	loss 1.0088 (0.8433)	grad_norm 3.0324 (inf)	loss_scale 16384.0000 (16571.1121)	mem 11634MB
[2024-06-29 16:13:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:23 lr 0.000063	 wd 0.0000	time 0.2915 (0.3233)	loss 0.8486 (0.8435)	grad_norm 1.9586 (inf)	loss_scale 16384.0000 (16558.6462)	mem 11634MB
[2024-06-29 16:13:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:04:51 lr 0.000063	 wd 0.0000	time 0.3124 (0.3230)	loss 0.8784 (0.8425)	grad_norm 2.0410 (inf)	loss_scale 16384.0000 (16547.7377)	mem 11634MB
[2024-06-29 16:14:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:18 lr 0.000062	 wd 0.0000	time 0.2424 (0.3227)	loss 0.9131 (0.8422)	grad_norm 1.8862 (inf)	loss_scale 16384.0000 (16538.1117)	mem 11634MB
[2024-06-29 16:14:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:46 lr 0.000062	 wd 0.0000	time 0.2814 (0.3223)	loss 0.7065 (0.8423)	grad_norm 2.3350 (inf)	loss_scale 16384.0000 (16529.5547)	mem 11634MB
[2024-06-29 16:15:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:14 lr 0.000062	 wd 0.0000	time 0.3710 (0.3226)	loss 0.7861 (0.8426)	grad_norm 1.7291 (inf)	loss_scale 16384.0000 (16521.8979)	mem 11634MB
[2024-06-29 16:15:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:42 lr 0.000062	 wd 0.0000	time 0.2993 (0.3245)	loss 0.8022 (0.8425)	grad_norm 1.7869 (inf)	loss_scale 16384.0000 (16515.0065)	mem 11634MB
[2024-06-29 16:16:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:10 lr 0.000062	 wd 0.0000	time 0.2404 (0.3245)	loss 0.9238 (0.8428)	grad_norm 2.4230 (inf)	loss_scale 16384.0000 (16508.7711)	mem 11634MB
[2024-06-29 16:16:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:37 lr 0.000061	 wd 0.0000	time 0.2740 (0.3244)	loss 0.9521 (0.8429)	grad_norm 1.7067 (inf)	loss_scale 16384.0000 (16503.1022)	mem 11634MB
[2024-06-29 16:17:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:05 lr 0.000061	 wd 0.0000	time 0.2637 (0.3243)	loss 0.7397 (0.8427)	grad_norm 1.8948 (inf)	loss_scale 16384.0000 (16497.9261)	mem 11634MB
[2024-06-29 16:18:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:33 lr 0.000061	 wd 0.0000	time 0.2494 (0.3250)	loss 0.7798 (0.8425)	grad_norm 1.6233 (inf)	loss_scale 16384.0000 (16493.1812)	mem 11634MB
[2024-06-29 16:18:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000061	 wd 0.0000	time 0.2617 (0.3241)	loss 0.8208 (0.8427)	grad_norm 1.7910 (inf)	loss_scale 16384.0000 (16488.8157)	mem 11634MB
[2024-06-29 16:18:35 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 12 training takes 0:13:33
[2024-06-29 16:18:47 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.270 (12.270)	Loss 0.3718 (0.3718)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 11634MB
[2024-06-29 16:18:57 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.226 Acc@5 97.564
[2024-06-29 16:18:57 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-06-29 16:18:57 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.23%
[2024-06-29 16:18:57 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 16:18:59 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 16:19:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:30:20 lr 0.000061	 wd 0.0000	time 10.7997 (10.7997)	loss 0.7129 (0.7129)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:19:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:16:41 lr 0.000061	 wd 0.0000	time 0.3047 (0.4170)	loss 0.7764 (0.8351)	grad_norm 2.2792 (2.0216)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:20:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:14:25 lr 0.000060	 wd 0.0000	time 0.2566 (0.3761)	loss 0.7104 (0.8296)	grad_norm 1.9057 (1.9863)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:20:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:13:02 lr 0.000060	 wd 0.0000	time 0.2909 (0.3554)	loss 0.7993 (0.8261)	grad_norm 1.9583 (2.0070)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:21:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:12:04 lr 0.000060	 wd 0.0000	time 0.3014 (0.3447)	loss 0.9126 (0.8306)	grad_norm 2.2351 (2.0169)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:21:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:11:16 lr 0.000060	 wd 0.0000	time 0.2943 (0.3381)	loss 0.8657 (0.8295)	grad_norm 2.3532 (2.0200)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:22:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:10:34 lr 0.000060	 wd 0.0000	time 0.2599 (0.3336)	loss 0.8369 (0.8280)	grad_norm 2.0451 (2.0163)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:22:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:09:57 lr 0.000059	 wd 0.0000	time 0.3035 (0.3315)	loss 0.7949 (0.8285)	grad_norm 2.4920 (2.0113)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:23:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:21 lr 0.000059	 wd 0.0000	time 0.3324 (0.3297)	loss 0.8232 (0.8289)	grad_norm 1.7114 (inf)	loss_scale 8192.0000 (15770.3670)	mem 11634MB
[2024-06-29 16:23:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:08:45 lr 0.000059	 wd 0.0000	time 0.2902 (0.3282)	loss 0.8188 (0.8304)	grad_norm 2.7635 (inf)	loss_scale 8192.0000 (14929.2608)	mem 11634MB
[2024-06-29 16:24:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:08:10 lr 0.000059	 wd 0.0000	time 0.2883 (0.3268)	loss 0.8677 (0.8329)	grad_norm 2.4177 (inf)	loss_scale 8192.0000 (14256.2078)	mem 11634MB
[2024-06-29 16:24:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:37 lr 0.000059	 wd 0.0000	time 0.3078 (0.3261)	loss 0.8320 (0.8328)	grad_norm 2.1041 (inf)	loss_scale 8192.0000 (13705.4169)	mem 11634MB
[2024-06-29 16:25:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:03 lr 0.000058	 wd 0.0000	time 0.2789 (0.3253)	loss 0.9180 (0.8324)	grad_norm 1.9113 (inf)	loss_scale 8192.0000 (13246.3480)	mem 11634MB
[2024-06-29 16:26:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:30 lr 0.000058	 wd 0.0000	time 0.2900 (0.3251)	loss 0.9102 (0.8328)	grad_norm 1.8185 (inf)	loss_scale 8192.0000 (12857.8509)	mem 11634MB
[2024-06-29 16:26:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:05:58 lr 0.000058	 wd 0.0000	time 0.2982 (0.3254)	loss 0.8716 (0.8317)	grad_norm 2.1091 (inf)	loss_scale 8192.0000 (12524.8137)	mem 11634MB
[2024-06-29 16:27:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:25 lr 0.000058	 wd 0.0000	time 0.3259 (0.3250)	loss 0.7690 (0.8327)	grad_norm 2.1950 (inf)	loss_scale 8192.0000 (12236.1519)	mem 11634MB
[2024-06-29 16:27:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:04:52 lr 0.000058	 wd 0.0000	time 0.2833 (0.3245)	loss 0.6655 (0.8332)	grad_norm 1.7395 (inf)	loss_scale 8192.0000 (11983.5503)	mem 11634MB
[2024-06-29 16:28:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:19 lr 0.000057	 wd 0.0000	time 0.3177 (0.3241)	loss 0.8198 (0.8342)	grad_norm 1.9392 (inf)	loss_scale 8192.0000 (11760.6490)	mem 11634MB
[2024-06-29 16:28:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:47 lr 0.000057	 wd 0.0000	time 0.3184 (0.3237)	loss 0.8281 (0.8349)	grad_norm 1.8874 (inf)	loss_scale 8192.0000 (11562.5008)	mem 11634MB
[2024-06-29 16:29:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:14 lr 0.000057	 wd 0.0000	time 0.3000 (0.3235)	loss 0.7441 (0.8343)	grad_norm 1.9855 (inf)	loss_scale 8192.0000 (11385.1994)	mem 11634MB
[2024-06-29 16:29:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:42 lr 0.000057	 wd 0.0000	time 0.3074 (0.3234)	loss 0.8511 (0.8340)	grad_norm 2.1474 (inf)	loss_scale 8192.0000 (11225.6192)	mem 11634MB
[2024-06-29 16:30:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:09 lr 0.000057	 wd 0.0000	time 0.2951 (0.3233)	loss 0.8218 (0.8343)	grad_norm 1.8926 (inf)	loss_scale 8192.0000 (11081.2299)	mem 11634MB
[2024-06-29 16:30:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:37 lr 0.000056	 wd 0.0000	time 0.2978 (0.3231)	loss 0.7739 (0.8340)	grad_norm 2.3612 (inf)	loss_scale 8192.0000 (10949.9609)	mem 11634MB
[2024-06-29 16:31:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:05 lr 0.000056	 wd 0.0000	time 0.4011 (0.3237)	loss 0.8599 (0.8341)	grad_norm 2.1066 (inf)	loss_scale 8192.0000 (10830.1017)	mem 11634MB
[2024-06-29 16:31:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:33 lr 0.000056	 wd 0.0000	time 0.3308 (0.3248)	loss 0.8027 (0.8342)	grad_norm 1.7949 (inf)	loss_scale 8192.0000 (10720.2266)	mem 11634MB
[2024-06-29 16:32:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000056	 wd 0.0000	time 0.2596 (0.3241)	loss 0.7671 (0.8341)	grad_norm 1.9852 (inf)	loss_scale 8192.0000 (10619.1379)	mem 11634MB
[2024-06-29 16:32:33 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 13 training takes 0:13:33
[2024-06-29 16:32:44 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.397 (11.397)	Loss 0.3718 (0.3718)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 16:32:55 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.366 Acc@5 97.528
[2024-06-29 16:32:55 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 16:32:55 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.37%
[2024-06-29 16:32:55 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 16:32:57 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 16:33:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:35:18 lr 0.000056	 wd 0.0000	time 10.9188 (10.9188)	loss 0.7954 (0.7954)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:33:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:16:52 lr 0.000055	 wd 0.0000	time 0.2912 (0.4214)	loss 0.8271 (0.8298)	grad_norm 1.7637 (1.9893)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:34:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:14:13 lr 0.000055	 wd 0.0000	time 0.2972 (0.3706)	loss 0.7944 (0.8254)	grad_norm 1.8696 (2.0262)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:34:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:12:57 lr 0.000055	 wd 0.0000	time 0.3093 (0.3531)	loss 0.9106 (0.8275)	grad_norm 1.9849 (2.0485)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:35:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:12:03 lr 0.000055	 wd 0.0000	time 0.3051 (0.3444)	loss 0.9712 (0.8285)	grad_norm 1.7382 (2.0356)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:35:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:11:17 lr 0.000055	 wd 0.0000	time 0.2876 (0.3384)	loss 0.8267 (0.8274)	grad_norm 1.9038 (2.0237)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:36:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:10:36 lr 0.000054	 wd 0.0000	time 0.3303 (0.3348)	loss 0.7896 (0.8292)	grad_norm 1.9605 (2.0314)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:36:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:09:58 lr 0.000054	 wd 0.0000	time 0.3145 (0.3322)	loss 0.7202 (0.8309)	grad_norm 1.7161 (2.0312)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:37:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:23 lr 0.000054	 wd 0.0000	time 0.2896 (0.3311)	loss 0.8149 (0.8291)	grad_norm 1.9493 (2.0280)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:37:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:08:49 lr 0.000054	 wd 0.0000	time 0.3181 (0.3303)	loss 0.8994 (0.8285)	grad_norm 1.9816 (2.0226)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:38:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:08:13 lr 0.000054	 wd 0.0000	time 0.2850 (0.3288)	loss 0.9365 (0.8281)	grad_norm 2.2752 (2.0250)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:38:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:40 lr 0.000053	 wd 0.0000	time 0.2749 (0.3283)	loss 0.7266 (0.8285)	grad_norm 1.8350 (2.0306)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:39:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:07:06 lr 0.000053	 wd 0.0000	time 0.2861 (0.3273)	loss 0.8667 (0.8289)	grad_norm 1.9311 (2.0347)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:40:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:35 lr 0.000053	 wd 0.0000	time 0.3247 (0.3292)	loss 0.8628 (0.8287)	grad_norm 1.7859 (2.0299)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:40:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:06:03 lr 0.000053	 wd 0.0000	time 0.3081 (0.3295)	loss 0.8398 (0.8280)	grad_norm 2.8071 (2.0284)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:41:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:30 lr 0.000053	 wd 0.0000	time 0.2881 (0.3297)	loss 0.9043 (0.8281)	grad_norm 2.0020 (2.0338)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:41:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:04:58 lr 0.000052	 wd 0.0000	time 0.3110 (0.3310)	loss 0.8677 (0.8287)	grad_norm 1.8794 (2.0363)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:42:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:26 lr 0.000052	 wd 0.0000	time 0.3240 (0.3325)	loss 0.9175 (0.8288)	grad_norm 1.9128 (2.0375)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:43:00 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:54 lr 0.000052	 wd 0.0000	time 0.3155 (0.3346)	loss 0.9316 (0.8288)	grad_norm 2.1963 (2.0392)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:43:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:22 lr 0.000052	 wd 0.0000	time 0.3271 (0.3364)	loss 0.8213 (0.8287)	grad_norm 1.7887 (2.0386)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:44:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:49 lr 0.000052	 wd 0.0000	time 0.2864 (0.3384)	loss 0.8574 (0.8289)	grad_norm 2.9760 (2.0432)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:44:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:15 lr 0.000051	 wd 0.0000	time 0.3129 (0.3383)	loss 0.8081 (0.8295)	grad_norm 1.8981 (2.0446)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:45:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:42 lr 0.000051	 wd 0.0000	time 0.3072 (0.3386)	loss 0.7583 (0.8302)	grad_norm 2.8651 (2.0497)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 16:45:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:08 lr 0.000051	 wd 0.0000	time 0.2819 (0.3384)	loss 0.8940 (0.8305)	grad_norm 2.0691 (2.0494)	loss_scale 16384.0000 (8412.7319)	mem 11634MB
[2024-06-29 16:46:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:34 lr 0.000051	 wd 0.0000	time 0.3771 (0.3378)	loss 0.7910 (0.8307)	grad_norm 1.7105 (2.0477)	loss_scale 16384.0000 (8744.7297)	mem 11634MB
[2024-06-29 16:46:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000051	 wd 0.0000	time 0.2695 (0.3365)	loss 0.8076 (0.8308)	grad_norm 1.5060 (2.0497)	loss_scale 16384.0000 (9050.1783)	mem 11634MB
[2024-06-29 16:47:01 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 14 training takes 0:14:04
[2024-06-29 16:47:13 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.498 (11.498)	Loss 0.3560 (0.3560)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 16:47:24 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.378 Acc@5 97.554
[2024-06-29 16:47:24 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 16:47:24 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.38%
[2024-06-29 16:47:24 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 16:47:26 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 16:47:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:38:39 lr 0.000051	 wd 0.0000	time 10.9990 (10.9990)	loss 0.7158 (0.7158)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:48:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:16:45 lr 0.000050	 wd 0.0000	time 0.2902 (0.4184)	loss 0.8516 (0.8208)	grad_norm 2.0983 (2.1185)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:48:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:01 lr 0.000050	 wd 0.0000	time 0.2715 (0.3654)	loss 1.0381 (0.8285)	grad_norm 2.2019 (2.1213)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:49:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:12:44 lr 0.000050	 wd 0.0000	time 0.2812 (0.3474)	loss 0.7275 (0.8281)	grad_norm 1.8394 (2.0872)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:49:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:11:52 lr 0.000050	 wd 0.0000	time 0.2936 (0.3388)	loss 0.6543 (0.8281)	grad_norm 1.6804 (2.0658)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:50:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:11:07 lr 0.000049	 wd 0.0000	time 0.3051 (0.3336)	loss 0.7695 (0.8276)	grad_norm 2.0674 (2.0556)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:50:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:10:28 lr 0.000049	 wd 0.0000	time 0.3028 (0.3303)	loss 0.7124 (0.8279)	grad_norm 1.9448 (2.0516)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 16:51:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:09:50 lr 0.000049	 wd 0.0000	time 0.3001 (0.3279)	loss 0.8218 (0.8269)	grad_norm 1.7577 (inf)	loss_scale 8192.0000 (16290.5107)	mem 11634MB
[2024-06-29 16:51:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:14 lr 0.000049	 wd 0.0000	time 0.2776 (0.3260)	loss 0.7627 (0.8259)	grad_norm 2.0791 (inf)	loss_scale 8192.0000 (15279.4607)	mem 11634MB
[2024-06-29 16:52:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:08:39 lr 0.000049	 wd 0.0000	time 0.2824 (0.3246)	loss 0.7925 (0.8258)	grad_norm 1.9185 (inf)	loss_scale 8192.0000 (14492.8391)	mem 11634MB
[2024-06-29 16:52:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:08:05 lr 0.000048	 wd 0.0000	time 0.2990 (0.3233)	loss 0.8032 (0.8264)	grad_norm 1.9018 (inf)	loss_scale 8192.0000 (13863.3846)	mem 11634MB
[2024-06-29 16:53:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:07:32 lr 0.000048	 wd 0.0000	time 0.3015 (0.3224)	loss 0.9111 (0.8251)	grad_norm 1.9885 (inf)	loss_scale 8192.0000 (13348.2725)	mem 11634MB
[2024-06-29 16:53:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:06:59 lr 0.000048	 wd 0.0000	time 0.2895 (0.3220)	loss 0.7124 (0.8246)	grad_norm 1.5913 (inf)	loss_scale 8192.0000 (12918.9409)	mem 11634MB
[2024-06-29 16:54:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:26 lr 0.000048	 wd 0.0000	time 0.2869 (0.3215)	loss 0.8809 (0.8250)	grad_norm 1.9234 (inf)	loss_scale 8192.0000 (12555.6095)	mem 11634MB
[2024-06-29 16:54:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:05:53 lr 0.000048	 wd 0.0000	time 0.2942 (0.3211)	loss 0.8369 (0.8247)	grad_norm 1.8472 (inf)	loss_scale 8192.0000 (12244.1456)	mem 11634MB
[2024-06-29 16:55:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:22 lr 0.000047	 wd 0.0000	time 0.2819 (0.3214)	loss 0.8169 (0.8244)	grad_norm 2.4588 (inf)	loss_scale 8192.0000 (11974.1825)	mem 11634MB
[2024-06-29 16:56:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:04:50 lr 0.000047	 wd 0.0000	time 0.3020 (0.3218)	loss 0.8896 (0.8244)	grad_norm 2.0466 (inf)	loss_scale 8192.0000 (11737.9438)	mem 11634MB
[2024-06-29 16:56:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:18 lr 0.000047	 wd 0.0000	time 0.3495 (0.3224)	loss 0.8887 (0.8240)	grad_norm 1.7340 (inf)	loss_scale 8192.0000 (11529.4815)	mem 11634MB
[2024-06-29 16:57:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:47 lr 0.000047	 wd 0.0000	time 0.3032 (0.3235)	loss 0.8320 (0.8253)	grad_norm 1.8512 (inf)	loss_scale 8192.0000 (11344.1688)	mem 11634MB
[2024-06-29 16:57:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:15 lr 0.000047	 wd 0.0000	time 0.4328 (0.3241)	loss 0.8652 (0.8251)	grad_norm 1.8371 (inf)	loss_scale 8192.0000 (11178.3524)	mem 11634MB
[2024-06-29 16:58:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:43 lr 0.000046	 wd 0.0000	time 0.3120 (0.3265)	loss 0.9131 (0.8251)	grad_norm 2.0686 (inf)	loss_scale 8192.0000 (11029.1094)	mem 11634MB
[2024-06-29 16:58:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:12 lr 0.000046	 wd 0.0000	time 0.3979 (0.3290)	loss 0.8657 (0.8250)	grad_norm 2.0941 (inf)	loss_scale 8192.0000 (10894.0733)	mem 11634MB
[2024-06-29 16:59:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:40 lr 0.000046	 wd 0.0000	time 0.3769 (0.3325)	loss 0.8247 (0.8253)	grad_norm 2.5315 (inf)	loss_scale 8192.0000 (10771.3076)	mem 11634MB
[2024-06-29 17:00:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:07 lr 0.000046	 wd 0.0000	time 0.3148 (0.3351)	loss 0.9331 (0.8249)	grad_norm 2.3587 (inf)	loss_scale 8192.0000 (10659.2125)	mem 11634MB
[2024-06-29 17:00:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:34 lr 0.000046	 wd 0.0000	time 0.3119 (0.3370)	loss 0.8042 (0.8248)	grad_norm 1.7536 (inf)	loss_scale 4096.0000 (10440.4498)	mem 11634MB
[2024-06-29 17:01:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000045	 wd 0.0000	time 0.2675 (0.3372)	loss 1.0469 (0.8251)	grad_norm 2.1589 (inf)	loss_scale 4096.0000 (10186.7733)	mem 11634MB
[2024-06-29 17:01:32 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 15 training takes 0:14:06
[2024-06-29 17:01:32 convnext_base_224_22kto1kto1k_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_15.pth saving......
[2024-06-29 17:01:33 convnext_base_224_22kto1kto1k_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_15.pth saved !!!
[2024-06-29 17:01:44 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 10.854 (10.854)	Loss 0.3679 (0.3679)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 17:01:55 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.484 Acc@5 97.556
[2024-06-29 17:01:55 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-06-29 17:01:55 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 17:01:55 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 17:01:56 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 17:02:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:25:21 lr 0.000045	 wd 0.0000	time 10.6800 (10.6800)	loss 0.7803 (0.7803)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:02:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:16:47 lr 0.000045	 wd 0.0000	time 0.2922 (0.4195)	loss 0.9492 (0.8263)	grad_norm 2.4127 (2.0781)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:03:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:14:05 lr 0.000045	 wd 0.0000	time 0.2814 (0.3674)	loss 0.8403 (0.8275)	grad_norm 1.8417 (2.0835)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:03:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:12:50 lr 0.000045	 wd 0.0000	time 0.3031 (0.3499)	loss 0.8223 (0.8264)	grad_norm 2.0882 (2.0625)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:04:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:11:58 lr 0.000045	 wd 0.0000	time 0.2898 (0.3420)	loss 0.8892 (0.8245)	grad_norm 1.7476 (2.0584)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:04:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:11:13 lr 0.000044	 wd 0.0000	time 0.3211 (0.3364)	loss 0.7275 (0.8224)	grad_norm 2.3670 (2.0693)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:05:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:10:33 lr 0.000044	 wd 0.0000	time 0.3102 (0.3330)	loss 0.7939 (0.8235)	grad_norm 2.3885 (2.0817)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:05:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:09:55 lr 0.000044	 wd 0.0000	time 0.2804 (0.3302)	loss 0.8457 (0.8232)	grad_norm 1.8053 (2.0688)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:06:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:19 lr 0.000044	 wd 0.0000	time 0.2603 (0.3286)	loss 0.9370 (0.8228)	grad_norm 2.0385 (2.0652)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:06:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:08:43 lr 0.000043	 wd 0.0000	time 0.2869 (0.3270)	loss 0.7812 (0.8246)	grad_norm 3.0521 (2.0716)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:07:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:08:09 lr 0.000043	 wd 0.0000	time 0.2797 (0.3258)	loss 0.7866 (0.8257)	grad_norm 1.9963 (2.0636)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:07:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:35 lr 0.000043	 wd 0.0000	time 0.2865 (0.3248)	loss 0.7407 (0.8246)	grad_norm 1.8559 (2.0676)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:08:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:01 lr 0.000043	 wd 0.0000	time 0.2812 (0.3239)	loss 0.8892 (0.8254)	grad_norm 2.0085 (2.0684)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:08:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:28 lr 0.000043	 wd 0.0000	time 0.3029 (0.3232)	loss 0.7983 (0.8236)	grad_norm 2.3982 (2.0739)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:09:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:05:56 lr 0.000042	 wd 0.0000	time 0.2941 (0.3233)	loss 0.8125 (0.8229)	grad_norm 2.4434 (2.0727)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:10:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:23 lr 0.000042	 wd 0.0000	time 0.2797 (0.3230)	loss 0.8638 (0.8226)	grad_norm 1.8516 (2.0770)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:10:33 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:04:51 lr 0.000042	 wd 0.0000	time 0.2901 (0.3227)	loss 0.8218 (0.8224)	grad_norm 2.1281 (2.0799)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:11:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:18 lr 0.000042	 wd 0.0000	time 0.2991 (0.3228)	loss 0.6812 (0.8233)	grad_norm 2.0240 (2.0827)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:11:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:46 lr 0.000042	 wd 0.0000	time 0.3043 (0.3227)	loss 0.8760 (0.8237)	grad_norm 1.8220 (2.0809)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:12:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:14 lr 0.000041	 wd 0.0000	time 0.2981 (0.3227)	loss 0.9409 (0.8242)	grad_norm 2.3363 (2.0782)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:12:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:41 lr 0.000041	 wd 0.0000	time 0.2987 (0.3226)	loss 0.7451 (0.8236)	grad_norm 2.0245 (2.0827)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:13:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:09 lr 0.000041	 wd 0.0000	time 0.3034 (0.3225)	loss 0.7373 (0.8235)	grad_norm 1.6681 (2.0831)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:13:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:37 lr 0.000041	 wd 0.0000	time 0.2875 (0.3225)	loss 0.7495 (0.8236)	grad_norm 1.8627 (2.0858)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:14:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:05 lr 0.000041	 wd 0.0000	time 0.2911 (0.3239)	loss 0.8564 (0.8236)	grad_norm 1.9208 (2.0836)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:14:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:33 lr 0.000040	 wd 0.0000	time 0.4498 (0.3250)	loss 0.7456 (0.8232)	grad_norm 2.0013 (2.0818)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:15:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.2631 (0.3251)	loss 0.8276 (0.8228)	grad_norm 2.2508 (2.0847)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:15:32 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 16 training takes 0:13:35
[2024-06-29 17:15:45 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.535 (12.535)	Loss 0.3538 (0.3538)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 17:15:55 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.450 Acc@5 97.556
[2024-06-29 17:15:55 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 17:15:55 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 17:16:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:31:21 lr 0.000040	 wd 0.0000	time 12.2629 (12.2629)	loss 0.7383 (0.7383)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:16:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:17:30 lr 0.000040	 wd 0.0000	time 0.2849 (0.4373)	loss 0.9082 (0.8158)	grad_norm 1.7709 (2.0392)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:17:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:23 lr 0.000040	 wd 0.0000	time 0.2784 (0.3750)	loss 0.9507 (0.8248)	grad_norm 1.8134 (2.0544)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:17:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:13:00 lr 0.000040	 wd 0.0000	time 0.2759 (0.3543)	loss 0.8525 (0.8240)	grad_norm 1.8959 (2.0766)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:18:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:12:02 lr 0.000039	 wd 0.0000	time 0.2835 (0.3437)	loss 0.7505 (0.8173)	grad_norm 2.4261 (2.0830)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:18:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:14 lr 0.000039	 wd 0.0000	time 0.2896 (0.3369)	loss 0.7529 (0.8187)	grad_norm 1.6679 (2.0886)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:19:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:10:33 lr 0.000039	 wd 0.0000	time 0.3125 (0.3329)	loss 0.8652 (0.8197)	grad_norm 2.7837 (2.0946)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:19:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:09:54 lr 0.000039	 wd 0.0000	time 0.2775 (0.3300)	loss 0.7500 (0.8190)	grad_norm 1.7981 (2.0941)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:20:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:17 lr 0.000039	 wd 0.0000	time 0.3138 (0.3278)	loss 0.7373 (0.8192)	grad_norm 1.8277 (2.0997)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:20:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:08:42 lr 0.000038	 wd 0.0000	time 0.2747 (0.3262)	loss 0.8091 (0.8195)	grad_norm 1.8218 (2.0966)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:21:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:08 lr 0.000038	 wd 0.0000	time 0.2792 (0.3252)	loss 0.8794 (0.8193)	grad_norm 1.9703 (2.0883)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:21:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:34 lr 0.000038	 wd 0.0000	time 0.2803 (0.3244)	loss 0.8931 (0.8204)	grad_norm 1.9577 (2.0858)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:22:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:01 lr 0.000038	 wd 0.0000	time 0.2448 (0.3234)	loss 0.8994 (0.8208)	grad_norm 2.1134 (2.0828)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:22:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:27 lr 0.000038	 wd 0.0000	time 0.2937 (0.3228)	loss 0.9609 (0.8197)	grad_norm 2.0425 (2.0886)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 17:23:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:05:55 lr 0.000037	 wd 0.0000	time 0.3007 (0.3222)	loss 0.7520 (0.8203)	grad_norm 1.9808 (2.0905)	loss_scale 8192.0000 (4306.5011)	mem 11634MB
[2024-06-29 17:23:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:22 lr 0.000037	 wd 0.0000	time 0.2733 (0.3218)	loss 0.7866 (0.8206)	grad_norm 2.4953 (2.0911)	loss_scale 8192.0000 (4565.3618)	mem 11634MB
[2024-06-29 17:24:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:50 lr 0.000037	 wd 0.0000	time 0.2861 (0.3217)	loss 0.7104 (0.8203)	grad_norm 2.3382 (2.0947)	loss_scale 8192.0000 (4791.8851)	mem 11634MB
[2024-06-29 17:25:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:19 lr 0.000037	 wd 0.0000	time 0.3792 (0.3233)	loss 0.9658 (0.8201)	grad_norm 1.9935 (2.0974)	loss_scale 8192.0000 (4991.7743)	mem 11634MB
[2024-06-29 17:25:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:47 lr 0.000037	 wd 0.0000	time 0.3243 (0.3244)	loss 0.7515 (0.8203)	grad_norm 1.8874 (2.0935)	loss_scale 8192.0000 (5169.4659)	mem 11634MB
[2024-06-29 17:26:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:15 lr 0.000036	 wd 0.0000	time 0.2896 (0.3242)	loss 0.7637 (0.8203)	grad_norm 2.4845 (2.0907)	loss_scale 8192.0000 (5328.4629)	mem 11634MB
[2024-06-29 17:26:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:43 lr 0.000036	 wd 0.0000	time 0.2959 (0.3257)	loss 0.7891 (0.8198)	grad_norm 2.0706 (2.0903)	loss_scale 8192.0000 (5471.5682)	mem 11634MB
[2024-06-29 17:27:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:11 lr 0.000036	 wd 0.0000	time 0.3178 (0.3279)	loss 0.7700 (0.8198)	grad_norm 2.4228 (2.0914)	loss_scale 8192.0000 (5601.0509)	mem 11634MB
[2024-06-29 17:27:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:39 lr 0.000036	 wd 0.0000	time 0.3455 (0.3280)	loss 0.8281 (0.8199)	grad_norm 1.7924 (2.0920)	loss_scale 8192.0000 (5718.7678)	mem 11634MB
[2024-06-29 17:28:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:06 lr 0.000036	 wd 0.0000	time 0.3058 (0.3277)	loss 0.7798 (0.8196)	grad_norm 2.2842 (2.0891)	loss_scale 8192.0000 (5826.2529)	mem 11634MB
[2024-06-29 17:29:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:33 lr 0.000035	 wd 0.0000	time 0.3554 (0.3277)	loss 0.7188 (0.8203)	grad_norm 2.0854 (2.0869)	loss_scale 8192.0000 (5924.7847)	mem 11634MB
[2024-06-29 17:29:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.2707 (0.3270)	loss 0.9741 (0.8207)	grad_norm 1.8253 (2.0829)	loss_scale 8192.0000 (6015.4370)	mem 11634MB
[2024-06-29 17:29:35 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 17 training takes 0:13:40
[2024-06-29 17:29:47 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.410 (12.410)	Loss 0.3535 (0.3535)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 17:29:58 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.386 Acc@5 97.552
[2024-06-29 17:29:58 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 17:29:58 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 17:30:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:05:40 lr 0.000035	 wd 0.0000	time 11.6467 (11.6467)	loss 0.8525 (0.8525)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:30:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:17:25 lr 0.000035	 wd 0.0000	time 0.2853 (0.4355)	loss 0.7959 (0.8122)	grad_norm 2.0750 (2.1539)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:31:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:21 lr 0.000035	 wd 0.0000	time 0.3028 (0.3742)	loss 0.9116 (0.8098)	grad_norm 2.2070 (2.0965)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:31:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:12:59 lr 0.000035	 wd 0.0000	time 0.3113 (0.3542)	loss 0.8989 (0.8140)	grad_norm 2.1789 (2.0966)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:32:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:12:07 lr 0.000034	 wd 0.0000	time 0.2810 (0.3461)	loss 0.8530 (0.8167)	grad_norm 2.2034 (2.1054)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:32:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:11:21 lr 0.000034	 wd 0.0000	time 0.3186 (0.3406)	loss 0.7554 (0.8169)	grad_norm 1.8668 (2.0944)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:33:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:10:39 lr 0.000034	 wd 0.0000	time 0.2835 (0.3362)	loss 0.7334 (0.8161)	grad_norm 1.8442 (2.0807)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:33:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:09:59 lr 0.000034	 wd 0.0000	time 0.2875 (0.3327)	loss 0.9497 (0.8150)	grad_norm 1.9059 (2.0774)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:34:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:23 lr 0.000034	 wd 0.0000	time 0.2757 (0.3312)	loss 0.8687 (0.8126)	grad_norm 2.5227 (2.0721)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:34:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:08:48 lr 0.000033	 wd 0.0000	time 0.2847 (0.3298)	loss 0.8433 (0.8128)	grad_norm 2.3604 (2.0764)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:35:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:08:13 lr 0.000033	 wd 0.0000	time 0.2900 (0.3285)	loss 0.7646 (0.8127)	grad_norm 1.7595 (2.0770)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:36:00 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:40 lr 0.000033	 wd 0.0000	time 0.3039 (0.3288)	loss 0.8003 (0.8137)	grad_norm 2.1666 (2.0766)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:36:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:07:08 lr 0.000033	 wd 0.0000	time 0.2665 (0.3288)	loss 1.0928 (0.8133)	grad_norm 2.1476 (2.0812)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:37:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:34 lr 0.000033	 wd 0.0000	time 0.2574 (0.3285)	loss 0.7300 (0.8141)	grad_norm 1.9978 (2.0750)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:37:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:06:02 lr 0.000032	 wd 0.0000	time 0.2846 (0.3287)	loss 0.7588 (0.8131)	grad_norm 2.2552 (2.0767)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:38:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:28 lr 0.000032	 wd 0.0000	time 0.3052 (0.3281)	loss 0.7549 (0.8133)	grad_norm 3.4027 (2.0784)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:38:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:04:56 lr 0.000032	 wd 0.0000	time 0.3024 (0.3289)	loss 0.8188 (0.8132)	grad_norm 2.1541 (2.0780)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:39:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:23 lr 0.000032	 wd 0.0000	time 0.2721 (0.3290)	loss 0.9434 (0.8133)	grad_norm 2.0347 (2.0788)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:39:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:50 lr 0.000032	 wd 0.0000	time 0.3031 (0.3290)	loss 0.8687 (0.8135)	grad_norm 1.7568 (2.0761)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:40:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:18 lr 0.000032	 wd 0.0000	time 0.2810 (0.3300)	loss 0.9780 (0.8134)	grad_norm 2.0670 (2.0736)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:40:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:45 lr 0.000031	 wd 0.0000	time 0.3345 (0.3295)	loss 0.9282 (0.8143)	grad_norm 1.9766 (2.0725)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:41:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:12 lr 0.000031	 wd 0.0000	time 0.3021 (0.3295)	loss 0.7188 (0.8143)	grad_norm 1.7107 (2.0806)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:42:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:39 lr 0.000031	 wd 0.0000	time 0.2600 (0.3291)	loss 0.7217 (0.8147)	grad_norm 1.7423 (2.0812)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:42:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:06 lr 0.000031	 wd 0.0000	time 0.3379 (0.3289)	loss 0.7554 (0.8150)	grad_norm 2.2005 (2.0847)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:43:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:33 lr 0.000031	 wd 0.0000	time 0.3357 (0.3294)	loss 0.8418 (0.8148)	grad_norm 2.4100 (2.0843)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:43:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000030	 wd 0.0000	time 0.2695 (0.3291)	loss 0.8101 (0.8151)	grad_norm 1.8742 (2.0859)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:43:43 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 18 training takes 0:13:45
[2024-06-29 17:43:55 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.626 (11.626)	Loss 0.3638 (0.3638)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 17:44:06 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.406 Acc@5 97.562
[2024-06-29 17:44:06 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 17:44:06 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 17:44:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:29:52 lr 0.000030	 wd 0.0000	time 10.7885 (10.7885)	loss 0.7661 (0.7661)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:44:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:13 lr 0.000030	 wd 0.0000	time 0.2910 (0.4303)	loss 0.9771 (0.8221)	grad_norm 2.2994 (2.0614)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:45:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:14:14 lr 0.000030	 wd 0.0000	time 0.3101 (0.3713)	loss 0.7021 (0.8226)	grad_norm 2.1766 (2.0721)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:45:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:12:59 lr 0.000030	 wd 0.0000	time 0.2824 (0.3542)	loss 0.7578 (0.8200)	grad_norm 1.7546 (2.0513)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 17:46:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:12:02 lr 0.000030	 wd 0.0000	time 0.2560 (0.3437)	loss 0.8667 (0.8176)	grad_norm 1.7733 (2.0597)	loss_scale 16384.0000 (9744.5985)	mem 11634MB
[2024-06-29 17:46:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:15 lr 0.000029	 wd 0.0000	time 0.3089 (0.3376)	loss 0.8599 (0.8152)	grad_norm 1.8785 (2.0658)	loss_scale 16384.0000 (11069.8283)	mem 11634MB
[2024-06-29 17:47:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:10:34 lr 0.000029	 wd 0.0000	time 0.2743 (0.3335)	loss 0.7773 (0.8152)	grad_norm 1.9780 (2.0634)	loss_scale 16384.0000 (11954.0499)	mem 11634MB
[2024-06-29 17:47:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:09:59 lr 0.000029	 wd 0.0000	time 0.2977 (0.3326)	loss 0.8730 (0.8152)	grad_norm 2.0464 (2.0663)	loss_scale 16384.0000 (12585.9971)	mem 11634MB
[2024-06-29 17:48:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:22 lr 0.000029	 wd 0.0000	time 0.3020 (0.3304)	loss 0.7104 (0.8146)	grad_norm 1.8473 (2.0672)	loss_scale 16384.0000 (13060.1548)	mem 11634MB
[2024-06-29 17:49:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:08:46 lr 0.000029	 wd 0.0000	time 0.3090 (0.3285)	loss 0.7349 (0.8133)	grad_norm 2.8490 (2.0706)	loss_scale 16384.0000 (13429.0610)	mem 11634MB
[2024-06-29 17:49:33 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:11 lr 0.000028	 wd 0.0000	time 0.2845 (0.3270)	loss 0.7314 (0.8129)	grad_norm 2.1286 (2.0708)	loss_scale 16384.0000 (13724.2597)	mem 11634MB
[2024-06-29 17:50:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:36 lr 0.000028	 wd 0.0000	time 0.2853 (0.3259)	loss 0.9077 (0.8131)	grad_norm 2.0150 (2.0726)	loss_scale 16384.0000 (13965.8347)	mem 11634MB
[2024-06-29 17:50:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:03 lr 0.000028	 wd 0.0000	time 0.2933 (0.3250)	loss 0.8159 (0.8129)	grad_norm 1.8264 (2.0761)	loss_scale 16384.0000 (14167.1807)	mem 11634MB
[2024-06-29 17:51:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:30 lr 0.000028	 wd 0.0000	time 0.2798 (0.3245)	loss 0.6816 (0.8136)	grad_norm 1.7582 (2.0733)	loss_scale 16384.0000 (14337.5742)	mem 11634MB
[2024-06-29 17:51:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:05:58 lr 0.000028	 wd 0.0000	time 0.3014 (0.3256)	loss 0.7334 (0.8140)	grad_norm 1.6757 (2.0712)	loss_scale 16384.0000 (14483.6431)	mem 11634MB
[2024-06-29 17:52:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:27 lr 0.000028	 wd 0.0000	time 0.2739 (0.3268)	loss 0.8545 (0.8146)	grad_norm 2.0097 (2.0703)	loss_scale 16384.0000 (14610.2492)	mem 11634MB
[2024-06-29 17:52:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:54 lr 0.000027	 wd 0.0000	time 0.3180 (0.3263)	loss 0.7642 (0.8146)	grad_norm 1.7038 (2.0714)	loss_scale 16384.0000 (14721.0394)	mem 11634MB
[2024-06-29 17:53:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:21 lr 0.000027	 wd 0.0000	time 0.2835 (0.3264)	loss 0.7056 (0.8142)	grad_norm 2.1048 (2.0718)	loss_scale 16384.0000 (14818.8031)	mem 11634MB
[2024-06-29 17:53:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:49 lr 0.000027	 wd 0.0000	time 0.3266 (0.3262)	loss 0.6958 (0.8146)	grad_norm 2.3156 (2.0782)	loss_scale 16384.0000 (14905.7102)	mem 11634MB
[2024-06-29 17:54:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:16 lr 0.000027	 wd 0.0000	time 0.3412 (0.3258)	loss 0.7988 (0.8143)	grad_norm 2.4864 (2.0812)	loss_scale 16384.0000 (14983.4740)	mem 11634MB
[2024-06-29 17:54:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:43 lr 0.000027	 wd 0.0000	time 0.3172 (0.3261)	loss 0.8867 (0.8134)	grad_norm 1.9398 (2.0869)	loss_scale 16384.0000 (15053.4653)	mem 11634MB
[2024-06-29 17:55:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:11 lr 0.000026	 wd 0.0000	time 0.5465 (0.3276)	loss 0.9590 (0.8131)	grad_norm 2.0665 (2.0896)	loss_scale 16384.0000 (15116.7939)	mem 11634MB
[2024-06-29 17:56:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:39 lr 0.000026	 wd 0.0000	time 0.3004 (0.3286)	loss 0.8130 (0.8128)	grad_norm 2.5112 (2.0914)	loss_scale 16384.0000 (15174.3680)	mem 11634MB
[2024-06-29 17:56:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:06 lr 0.000026	 wd 0.0000	time 0.2863 (0.3302)	loss 0.8726 (0.8131)	grad_norm 1.9017 (2.0892)	loss_scale 16384.0000 (15226.9379)	mem 11634MB
[2024-06-29 17:57:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:33 lr 0.000026	 wd 0.0000	time 0.3124 (0.3322)	loss 0.9907 (0.8129)	grad_norm 2.7688 (2.0934)	loss_scale 16384.0000 (15275.1287)	mem 11634MB
[2024-06-29 17:57:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.2624 (0.3310)	loss 0.7319 (0.8126)	grad_norm 1.7417 (2.0949)	loss_scale 16384.0000 (15319.4658)	mem 11634MB
[2024-06-29 17:57:57 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 19 training takes 0:13:50
[2024-06-29 17:58:08 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.015 (11.015)	Loss 0.3616 (0.3616)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 17:58:20 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.440 Acc@5 97.590
[2024-06-29 17:58:20 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 17:58:20 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 17:58:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][0/2502]	eta 8:20:34 lr 0.000026	 wd 0.0000	time 12.0042 (12.0042)	loss 0.9131 (0.9131)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 17:59:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:17:18 lr 0.000026	 wd 0.0000	time 0.3014 (0.4324)	loss 0.8335 (0.8180)	grad_norm 2.0994 (2.1948)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 17:59:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:14:19 lr 0.000025	 wd 0.0000	time 0.3146 (0.3732)	loss 0.8672 (0.8131)	grad_norm 1.7176 (2.1227)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:00:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:12:59 lr 0.000025	 wd 0.0000	time 0.2832 (0.3538)	loss 0.7246 (0.8118)	grad_norm 2.3984 (2.1354)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:00:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:12:02 lr 0.000025	 wd 0.0000	time 0.2797 (0.3435)	loss 0.7007 (0.8082)	grad_norm 2.0725 (2.1337)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:01:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:11:15 lr 0.000025	 wd 0.0000	time 0.2937 (0.3373)	loss 0.8511 (0.8090)	grad_norm 1.8594 (2.1380)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:01:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:10:34 lr 0.000025	 wd 0.0000	time 0.3022 (0.3335)	loss 0.7612 (0.8089)	grad_norm 2.0990 (2.1394)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:02:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:09:55 lr 0.000025	 wd 0.0000	time 0.2851 (0.3304)	loss 0.7178 (0.8088)	grad_norm 2.0865 (2.1375)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:02:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:18 lr 0.000024	 wd 0.0000	time 0.2901 (0.3282)	loss 0.8154 (0.8098)	grad_norm 2.0123 (2.1315)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:03:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:08:44 lr 0.000024	 wd 0.0000	time 0.2887 (0.3271)	loss 0.9292 (0.8093)	grad_norm 2.3645 (2.1270)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:03:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:08:09 lr 0.000024	 wd 0.0000	time 0.3103 (0.3259)	loss 0.7681 (0.8109)	grad_norm 1.9548 (2.1245)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:04:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:07:35 lr 0.000024	 wd 0.0000	time 0.3066 (0.3248)	loss 0.7285 (0.8106)	grad_norm 2.6235 (2.1246)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:04:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:07:02 lr 0.000024	 wd 0.0000	time 0.2971 (0.3245)	loss 0.7134 (0.8104)	grad_norm 2.4417 (2.1145)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:05:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:30 lr 0.000023	 wd 0.0000	time 0.3001 (0.3248)	loss 0.7568 (0.8093)	grad_norm 2.5575 (2.1129)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:05:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:05:57 lr 0.000023	 wd 0.0000	time 0.2979 (0.3246)	loss 0.8252 (0.8096)	grad_norm 2.7350 (2.1120)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:06:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:25 lr 0.000023	 wd 0.0000	time 0.2960 (0.3245)	loss 0.8916 (0.8093)	grad_norm 3.3436 (2.1099)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:07:00 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:04:52 lr 0.000023	 wd 0.0000	time 0.2835 (0.3242)	loss 0.8467 (0.8083)	grad_norm 1.7970 (2.1066)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:07:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:19 lr 0.000023	 wd 0.0000	time 0.2917 (0.3237)	loss 1.0146 (0.8083)	grad_norm 2.0351 (2.1080)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:08:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:46 lr 0.000023	 wd 0.0000	time 0.2929 (0.3232)	loss 0.8340 (0.8082)	grad_norm 2.3124 (2.1087)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:08:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:14 lr 0.000022	 wd 0.0000	time 0.3517 (0.3232)	loss 0.8252 (0.8082)	grad_norm 2.5957 (inf)	loss_scale 16384.0000 (16677.0331)	mem 11634MB
[2024-06-29 18:09:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:42 lr 0.000022	 wd 0.0000	time 0.2973 (0.3231)	loss 0.8770 (0.8080)	grad_norm 2.1996 (inf)	loss_scale 16384.0000 (16662.3888)	mem 11634MB
[2024-06-29 18:09:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:09 lr 0.000022	 wd 0.0000	time 0.2917 (0.3233)	loss 0.8657 (0.8081)	grad_norm 2.2532 (inf)	loss_scale 16384.0000 (16649.1385)	mem 11634MB
[2024-06-29 18:10:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:38 lr 0.000022	 wd 0.0000	time 0.3889 (0.3253)	loss 0.7954 (0.8079)	grad_norm 2.1415 (inf)	loss_scale 16384.0000 (16637.0922)	mem 11634MB
[2024-06-29 18:10:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:06 lr 0.000022	 wd 0.0000	time 0.3467 (0.3269)	loss 0.8101 (0.8082)	grad_norm 1.9232 (inf)	loss_scale 16384.0000 (16626.0930)	mem 11634MB
[2024-06-29 18:11:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:33 lr 0.000022	 wd 0.0000	time 0.3238 (0.3292)	loss 0.7842 (0.8086)	grad_norm 2.1322 (inf)	loss_scale 16384.0000 (16616.0100)	mem 11634MB
[2024-06-29 18:12:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.2729 (0.3287)	loss 0.7739 (0.8084)	grad_norm 1.8481 (inf)	loss_scale 16384.0000 (16606.7333)	mem 11634MB
[2024-06-29 18:12:06 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 20 training takes 0:13:45
[2024-06-29 18:12:17 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.292 (11.292)	Loss 0.3608 (0.3608)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 18:12:31 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.376 Acc@5 97.574
[2024-06-29 18:12:31 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 18:12:31 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 18:12:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:15:07 lr 0.000021	 wd 0.0000	time 11.8735 (11.8735)	loss 0.8608 (0.8608)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:13:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:17:07 lr 0.000021	 wd 0.0000	time 0.2966 (0.4279)	loss 0.7983 (0.8070)	grad_norm 2.4043 (2.2309)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:13:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:14:11 lr 0.000021	 wd 0.0000	time 0.2376 (0.3699)	loss 0.8071 (0.8105)	grad_norm 1.9523 (2.1605)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:14:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:12:53 lr 0.000021	 wd 0.0000	time 0.2898 (0.3513)	loss 0.7583 (0.8035)	grad_norm 1.9981 (2.1210)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:14:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:11:58 lr 0.000021	 wd 0.0000	time 0.2876 (0.3417)	loss 0.7095 (0.8020)	grad_norm 2.1780 (2.1232)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:15:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:13 lr 0.000021	 wd 0.0000	time 0.2910 (0.3364)	loss 0.9214 (0.8040)	grad_norm 2.2604 (2.1276)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:15:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:32 lr 0.000020	 wd 0.0000	time 0.3074 (0.3325)	loss 0.8555 (0.8039)	grad_norm 1.9149 (2.1392)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:16:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:09:54 lr 0.000020	 wd 0.0000	time 0.3120 (0.3296)	loss 0.7954 (0.8036)	grad_norm 2.1768 (2.1269)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:16:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:19 lr 0.000020	 wd 0.0000	time 0.2976 (0.3289)	loss 0.8032 (0.8034)	grad_norm 1.8022 (2.1199)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:17:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:08:43 lr 0.000020	 wd 0.0000	time 0.2908 (0.3271)	loss 0.8062 (0.8033)	grad_norm 1.9009 (2.1241)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:17:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:08:10 lr 0.000020	 wd 0.0000	time 0.2968 (0.3267)	loss 0.7466 (0.8026)	grad_norm 2.0715 (2.1256)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:18:29 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:36 lr 0.000020	 wd 0.0000	time 0.2645 (0.3257)	loss 0.7505 (0.8037)	grad_norm 1.8552 (2.1294)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:19:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:07:02 lr 0.000019	 wd 0.0000	time 0.3043 (0.3249)	loss 0.8701 (0.8035)	grad_norm 2.9207 (2.1368)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:19:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:29 lr 0.000019	 wd 0.0000	time 0.2855 (0.3242)	loss 0.9277 (0.8034)	grad_norm 1.7924 (2.1340)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:20:04 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:05:56 lr 0.000019	 wd 0.0000	time 0.2903 (0.3235)	loss 0.7417 (0.8035)	grad_norm 2.0774 (2.1353)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:20:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:23 lr 0.000019	 wd 0.0000	time 0.3090 (0.3231)	loss 0.7720 (0.8035)	grad_norm 1.9196 (2.1404)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:21:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:04:51 lr 0.000019	 wd 0.0000	time 0.2928 (0.3227)	loss 0.7842 (0.8027)	grad_norm 2.4616 (2.1377)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:21:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:18 lr 0.000019	 wd 0.0000	time 0.2904 (0.3227)	loss 0.7817 (0.8024)	grad_norm 2.2294 (2.1385)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:22:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:46 lr 0.000018	 wd 0.0000	time 0.2858 (0.3226)	loss 0.7944 (0.8019)	grad_norm 2.4278 (2.1411)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:22:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:14 lr 0.000018	 wd 0.0000	time 0.3247 (0.3229)	loss 0.6997 (0.8016)	grad_norm 1.8498 (2.1468)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:23:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:42 lr 0.000018	 wd 0.0000	time 0.2967 (0.3233)	loss 0.7681 (0.8018)	grad_norm 2.0336 (2.1480)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:23:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:10 lr 0.000018	 wd 0.0000	time 0.2876 (0.3236)	loss 0.8184 (0.8019)	grad_norm 2.4043 (2.1440)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:24:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:38 lr 0.000018	 wd 0.0000	time 0.3493 (0.3248)	loss 0.6719 (0.8018)	grad_norm 1.8294 (2.1443)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:25:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:05 lr 0.000018	 wd 0.0000	time 0.3332 (0.3264)	loss 0.8838 (0.8025)	grad_norm 2.1762 (2.1461)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:25:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:33 lr 0.000018	 wd 0.0000	time 0.3038 (0.3289)	loss 0.7905 (0.8026)	grad_norm 2.0077 (2.1456)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:26:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0000	time 0.2389 (0.3287)	loss 0.8701 (0.8029)	grad_norm 1.9428 (2.1452)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:26:17 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 21 training takes 0:13:46
[2024-06-29 18:26:28 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.496 (11.496)	Loss 0.3586 (0.3586)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 18:26:42 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.438 Acc@5 97.582
[2024-06-29 18:26:42 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-06-29 18:26:42 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 18:26:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:44:36 lr 0.000017	 wd 0.0000	time 11.1418 (11.1418)	loss 0.8433 (0.8433)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:27:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:17:18 lr 0.000017	 wd 0.0000	time 0.3018 (0.4325)	loss 0.8760 (0.8132)	grad_norm 1.6939 (2.1731)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:28:00 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:14:44 lr 0.000017	 wd 0.0000	time 0.2667 (0.3841)	loss 0.6875 (0.8103)	grad_norm 2.1232 (2.1083)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:28:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:13:15 lr 0.000017	 wd 0.0000	time 0.2779 (0.3612)	loss 0.9014 (0.8074)	grad_norm 2.5824 (2.1512)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:29:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:12:15 lr 0.000017	 wd 0.0000	time 0.2832 (0.3500)	loss 0.8613 (0.8091)	grad_norm 1.9293 (2.1405)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:29:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:25 lr 0.000017	 wd 0.0000	time 0.2898 (0.3426)	loss 0.8877 (0.8108)	grad_norm 2.1732 (2.1432)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:30:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:10:42 lr 0.000016	 wd 0.0000	time 0.2994 (0.3376)	loss 0.9487 (0.8105)	grad_norm 2.7009 (2.1360)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:30:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:10:03 lr 0.000016	 wd 0.0000	time 0.2849 (0.3351)	loss 0.7939 (0.8109)	grad_norm 2.2956 (2.1412)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:31:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:25 lr 0.000016	 wd 0.0000	time 0.2652 (0.3324)	loss 0.7349 (0.8114)	grad_norm 1.9607 (2.1386)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:31:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:08:49 lr 0.000016	 wd 0.0000	time 0.3015 (0.3304)	loss 0.8008 (0.8103)	grad_norm 1.7596 (inf)	loss_scale 16384.0000 (16820.4218)	mem 11634MB
[2024-06-29 18:32:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:08:14 lr 0.000016	 wd 0.0000	time 0.2503 (0.3289)	loss 0.6870 (0.8111)	grad_norm 2.2255 (inf)	loss_scale 16384.0000 (16776.8232)	mem 11634MB
[2024-06-29 18:32:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:07:39 lr 0.000016	 wd 0.0000	time 0.2935 (0.3276)	loss 0.7393 (0.8110)	grad_norm 2.1117 (inf)	loss_scale 16384.0000 (16741.1444)	mem 11634MB
[2024-06-29 18:33:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:07:05 lr 0.000016	 wd 0.0000	time 0.2755 (0.3264)	loss 0.8105 (0.8110)	grad_norm 1.9531 (inf)	loss_scale 16384.0000 (16711.4072)	mem 11634MB
[2024-06-29 18:33:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:31 lr 0.000015	 wd 0.0000	time 0.3045 (0.3255)	loss 0.7905 (0.8104)	grad_norm 2.2801 (inf)	loss_scale 16384.0000 (16686.2414)	mem 11634MB
[2024-06-29 18:34:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:05:57 lr 0.000015	 wd 0.0000	time 0.3133 (0.3248)	loss 0.8184 (0.8101)	grad_norm 2.3240 (inf)	loss_scale 16384.0000 (16664.6681)	mem 11634MB
[2024-06-29 18:34:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:25 lr 0.000015	 wd 0.0000	time 0.2967 (0.3246)	loss 0.6255 (0.8102)	grad_norm 2.2953 (inf)	loss_scale 16384.0000 (16645.9694)	mem 11634MB
[2024-06-29 18:35:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:04:53 lr 0.000015	 wd 0.0000	time 0.3074 (0.3249)	loss 0.9058 (0.8101)	grad_norm 2.3309 (inf)	loss_scale 16384.0000 (16629.6065)	mem 11634MB
[2024-06-29 18:35:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:20 lr 0.000015	 wd 0.0000	time 0.2952 (0.3249)	loss 0.6685 (0.8091)	grad_norm 1.9788 (inf)	loss_scale 16384.0000 (16615.1675)	mem 11634MB
[2024-06-29 18:36:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:47 lr 0.000015	 wd 0.0000	time 0.2971 (0.3247)	loss 0.9556 (0.8083)	grad_norm 1.8619 (inf)	loss_scale 16384.0000 (16602.3320)	mem 11634MB
[2024-06-29 18:36:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:15 lr 0.000015	 wd 0.0000	time 0.3125 (0.3245)	loss 0.8403 (0.8088)	grad_norm 2.1927 (inf)	loss_scale 16384.0000 (16590.8469)	mem 11634MB
[2024-06-29 18:37:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:42 lr 0.000014	 wd 0.0000	time 0.2882 (0.3244)	loss 0.7983 (0.8087)	grad_norm 2.3883 (inf)	loss_scale 16384.0000 (16580.5097)	mem 11634MB
[2024-06-29 18:38:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:10 lr 0.000014	 wd 0.0000	time 0.2760 (0.3257)	loss 0.7539 (0.8086)	grad_norm 1.8943 (inf)	loss_scale 16384.0000 (16571.1566)	mem 11634MB
[2024-06-29 18:38:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:38 lr 0.000014	 wd 0.0000	time 0.3183 (0.3271)	loss 0.6660 (0.8079)	grad_norm 1.7238 (inf)	loss_scale 16384.0000 (16562.6533)	mem 11634MB
[2024-06-29 18:39:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:06 lr 0.000014	 wd 0.0000	time 0.3734 (0.3304)	loss 0.7974 (0.8074)	grad_norm 2.5862 (inf)	loss_scale 16384.0000 (16554.8892)	mem 11634MB
[2024-06-29 18:40:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:33 lr 0.000014	 wd 0.0000	time 0.3327 (0.3333)	loss 0.7598 (0.8079)	grad_norm 2.0689 (inf)	loss_scale 16384.0000 (16547.7718)	mem 11634MB
[2024-06-29 18:40:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.2700 (0.3332)	loss 0.7695 (0.8077)	grad_norm 1.9497 (inf)	loss_scale 16384.0000 (16541.2235)	mem 11634MB
[2024-06-29 18:40:42 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 22 training takes 0:13:59
[2024-06-29 18:40:55 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.308 (12.308)	Loss 0.3604 (0.3604)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 18:41:09 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.474 Acc@5 97.540
[2024-06-29 18:41:09 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-06-29 18:41:09 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.48%
[2024-06-29 18:41:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:04:27 lr 0.000014	 wd 0.0000	time 10.1787 (10.1787)	loss 0.8320 (0.8320)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:41:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:17:05 lr 0.000014	 wd 0.0000	time 0.2706 (0.4271)	loss 0.8687 (0.8013)	grad_norm 2.4816 (2.1605)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:42:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:14:10 lr 0.000013	 wd 0.0000	time 0.2846 (0.3693)	loss 0.7324 (0.7982)	grad_norm 2.4176 (2.1384)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:42:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:12:51 lr 0.000013	 wd 0.0000	time 0.2995 (0.3502)	loss 0.7515 (0.8003)	grad_norm 2.0554 (2.1410)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:43:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:11:58 lr 0.000013	 wd 0.0000	time 0.2907 (0.3418)	loss 0.8984 (0.8028)	grad_norm 1.8351 (2.1345)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:43:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:12 lr 0.000013	 wd 0.0000	time 0.2672 (0.3361)	loss 0.9150 (0.8054)	grad_norm 2.3813 (2.1476)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 18:44:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:10:32 lr 0.000013	 wd 0.0000	time 0.2607 (0.3325)	loss 0.9546 (0.8055)	grad_norm 3.0211 (inf)	loss_scale 8192.0000 (15975.0815)	mem 11634MB
[2024-06-29 18:45:00 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:09:55 lr 0.000013	 wd 0.0000	time 0.2922 (0.3307)	loss 0.8716 (0.8063)	grad_norm 1.9018 (inf)	loss_scale 8192.0000 (14864.7989)	mem 11634MB
[2024-06-29 18:45:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:19 lr 0.000013	 wd 0.0000	time 0.3279 (0.3285)	loss 0.7310 (0.8048)	grad_norm 1.9498 (inf)	loss_scale 8192.0000 (14031.7403)	mem 11634MB
[2024-06-29 18:46:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:08:43 lr 0.000012	 wd 0.0000	time 0.2385 (0.3268)	loss 1.0234 (0.8040)	grad_norm 2.3347 (inf)	loss_scale 8192.0000 (13383.6004)	mem 11634MB
[2024-06-29 18:46:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:08 lr 0.000012	 wd 0.0000	time 0.2985 (0.3254)	loss 0.7148 (0.8047)	grad_norm 2.5130 (inf)	loss_scale 8192.0000 (12864.9590)	mem 11634MB
[2024-06-29 18:47:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:34 lr 0.000012	 wd 0.0000	time 0.3002 (0.3242)	loss 0.7900 (0.8037)	grad_norm 2.2502 (inf)	loss_scale 8192.0000 (12440.5304)	mem 11634MB
[2024-06-29 18:47:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:01 lr 0.000012	 wd 0.0000	time 0.2736 (0.3239)	loss 0.7588 (0.8037)	grad_norm 2.4304 (inf)	loss_scale 8192.0000 (12086.7810)	mem 11634MB
[2024-06-29 18:48:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:28 lr 0.000012	 wd 0.0000	time 0.2922 (0.3232)	loss 0.8428 (0.8026)	grad_norm 1.7588 (inf)	loss_scale 8192.0000 (11787.4128)	mem 11634MB
[2024-06-29 18:48:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:05:55 lr 0.000012	 wd 0.0000	time 0.3124 (0.3226)	loss 0.8423 (0.8023)	grad_norm 2.7981 (inf)	loss_scale 8192.0000 (11530.7809)	mem 11634MB
[2024-06-29 18:49:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:22 lr 0.000012	 wd 0.0000	time 0.3251 (0.3221)	loss 0.9209 (0.8018)	grad_norm 3.3369 (inf)	loss_scale 8192.0000 (11308.3438)	mem 11634MB
[2024-06-29 18:49:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:04:50 lr 0.000012	 wd 0.0000	time 0.2938 (0.3225)	loss 0.7769 (0.8018)	grad_norm 2.4039 (inf)	loss_scale 8192.0000 (11113.6939)	mem 11634MB
[2024-06-29 18:50:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:19 lr 0.000011	 wd 0.0000	time 0.2904 (0.3233)	loss 0.6865 (0.8021)	grad_norm 1.9943 (inf)	loss_scale 8192.0000 (10941.9306)	mem 11634MB
[2024-06-29 18:50:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:47 lr 0.000011	 wd 0.0000	time 0.2690 (0.3239)	loss 0.8477 (0.8023)	grad_norm 2.2430 (inf)	loss_scale 8192.0000 (10789.2415)	mem 11634MB
[2024-06-29 18:51:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:15 lr 0.000011	 wd 0.0000	time 0.2800 (0.3242)	loss 0.7388 (0.8021)	grad_norm 1.8501 (inf)	loss_scale 8192.0000 (10652.6165)	mem 11634MB
[2024-06-29 18:52:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:43 lr 0.000011	 wd 0.0000	time 0.3745 (0.3260)	loss 0.8755 (0.8015)	grad_norm 2.2754 (inf)	loss_scale 8192.0000 (10529.6472)	mem 11634MB
[2024-06-29 18:52:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:11 lr 0.000011	 wd 0.0000	time 0.3118 (0.3280)	loss 0.8193 (0.8018)	grad_norm 3.1962 (inf)	loss_scale 8192.0000 (10418.3836)	mem 11634MB
[2024-06-29 18:53:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:39 lr 0.000011	 wd 0.0000	time 0.4480 (0.3292)	loss 0.8271 (0.8018)	grad_norm 1.8166 (inf)	loss_scale 8192.0000 (10317.2303)	mem 11634MB
[2024-06-29 18:53:52 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:06 lr 0.000011	 wd 0.0000	time 0.3124 (0.3316)	loss 0.8540 (0.8020)	grad_norm 2.4780 (inf)	loss_scale 8192.0000 (10224.8692)	mem 11634MB
[2024-06-29 18:54:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:33 lr 0.000011	 wd 0.0000	time 0.3113 (0.3324)	loss 0.8008 (0.8020)	grad_norm 2.0705 (inf)	loss_scale 8192.0000 (10140.2016)	mem 11634MB
[2024-06-29 18:54:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.2806 (0.3319)	loss 0.8315 (0.8017)	grad_norm 1.9576 (inf)	loss_scale 8192.0000 (10062.3047)	mem 11634MB
[2024-06-29 18:55:04 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 23 training takes 0:13:55
[2024-06-29 18:55:16 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.644 (11.644)	Loss 0.3628 (0.3628)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 18:55:32 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.574 Acc@5 97.556
[2024-06-29 18:55:32 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-06-29 18:55:32 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.57%
[2024-06-29 18:55:32 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 18:55:34 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 18:55:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:23:12 lr 0.000010	 wd 0.0000	time 10.6284 (10.6284)	loss 0.7319 (0.7319)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:56:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:16:37 lr 0.000010	 wd 0.0000	time 0.2832 (0.4153)	loss 0.8486 (0.7887)	grad_norm 2.2903 (2.1868)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:56:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:13:57 lr 0.000010	 wd 0.0000	time 0.3015 (0.3640)	loss 0.8525 (0.7981)	grad_norm 1.9031 (2.1765)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:57:19 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:12:47 lr 0.000010	 wd 0.0000	time 0.2814 (0.3487)	loss 0.7500 (0.7970)	grad_norm 2.3379 (2.1822)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:57:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:11:54 lr 0.000010	 wd 0.0000	time 0.3041 (0.3400)	loss 0.8208 (0.7972)	grad_norm 2.1771 (2.1627)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:58:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:11:09 lr 0.000010	 wd 0.0000	time 0.2915 (0.3344)	loss 0.7686 (0.7980)	grad_norm 2.5078 (2.1699)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:58:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:10:29 lr 0.000010	 wd 0.0000	time 0.2919 (0.3308)	loss 0.7334 (0.7980)	grad_norm 2.2233 (2.1578)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:59:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:09:52 lr 0.000010	 wd 0.0000	time 0.2823 (0.3286)	loss 0.7485 (0.7980)	grad_norm 2.1340 (2.1485)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 18:59:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:16 lr 0.000010	 wd 0.0000	time 0.2856 (0.3267)	loss 0.7246 (0.7992)	grad_norm 2.0049 (2.1457)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:00:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:08:41 lr 0.000009	 wd 0.0000	time 0.2997 (0.3258)	loss 0.8403 (0.7997)	grad_norm 2.0767 (2.1476)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:01:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:08:09 lr 0.000009	 wd 0.0000	time 0.3351 (0.3262)	loss 0.8589 (0.8012)	grad_norm 1.9819 (2.1476)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:01:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:07:35 lr 0.000009	 wd 0.0000	time 0.2761 (0.3250)	loss 0.7139 (0.7995)	grad_norm 2.0062 (2.1456)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:02:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:01 lr 0.000009	 wd 0.0000	time 0.3096 (0.3241)	loss 0.6924 (0.7991)	grad_norm 2.2834 (2.1473)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:02:35 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:28 lr 0.000009	 wd 0.0000	time 0.2833 (0.3233)	loss 0.8730 (0.8001)	grad_norm 2.5122 (2.1502)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:03:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:05:55 lr 0.000009	 wd 0.0000	time 0.2892 (0.3227)	loss 0.8657 (0.8004)	grad_norm 2.3625 (2.1465)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:03:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:22 lr 0.000009	 wd 0.0000	time 0.2979 (0.3223)	loss 0.6831 (0.8009)	grad_norm 2.2401 (2.1368)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:04:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:04:50 lr 0.000009	 wd 0.0000	time 0.2828 (0.3222)	loss 0.7808 (0.8009)	grad_norm 2.2923 (2.1328)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:04:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:18 lr 0.000008	 wd 0.0000	time 0.2935 (0.3218)	loss 0.8096 (0.8014)	grad_norm 1.7269 (2.1321)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:05:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:46 lr 0.000008	 wd 0.0000	time 0.3189 (0.3223)	loss 0.7393 (0.8009)	grad_norm 2.3793 (inf)	loss_scale 4096.0000 (8114.6741)	mem 11634MB
[2024-06-29 19:05:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:14 lr 0.000008	 wd 0.0000	time 0.2873 (0.3234)	loss 0.5977 (0.8012)	grad_norm 2.3939 (inf)	loss_scale 4096.0000 (7903.2762)	mem 11634MB
[2024-06-29 19:06:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:42 lr 0.000008	 wd 0.0000	time 0.2957 (0.3233)	loss 0.8701 (0.8006)	grad_norm 2.2230 (inf)	loss_scale 4096.0000 (7713.0075)	mem 11634MB
[2024-06-29 19:06:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:09 lr 0.000008	 wd 0.0000	time 0.3023 (0.3230)	loss 0.7656 (0.8003)	grad_norm 2.4280 (inf)	loss_scale 4096.0000 (7540.8510)	mem 11634MB
[2024-06-29 19:07:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:37 lr 0.000008	 wd 0.0000	time 0.2970 (0.3228)	loss 0.8447 (0.7999)	grad_norm 2.0260 (inf)	loss_scale 4096.0000 (7384.3380)	mem 11634MB
[2024-06-29 19:08:00 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:05 lr 0.000008	 wd 0.0000	time 0.3917 (0.3241)	loss 0.8940 (0.8002)	grad_norm 2.0735 (inf)	loss_scale 4096.0000 (7241.4289)	mem 11634MB
[2024-06-29 19:08:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:33 lr 0.000008	 wd 0.0000	time 0.2819 (0.3263)	loss 0.9321 (0.8003)	grad_norm 1.9699 (inf)	loss_scale 4096.0000 (7110.4240)	mem 11634MB
[2024-06-29 19:09:10 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.2735 (0.3261)	loss 0.9043 (0.8003)	grad_norm 2.2116 (inf)	loss_scale 4096.0000 (6989.8952)	mem 11634MB
[2024-06-29 19:09:16 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 24 training takes 0:13:41
[2024-06-29 19:09:27 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 10.868 (10.868)	Loss 0.3616 (0.3616)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 19:09:44 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.552 Acc@5 97.574
[2024-06-29 19:09:44 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-06-29 19:09:44 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.57%
[2024-06-29 19:09:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:05:18 lr 0.000008	 wd 0.0000	time 11.6383 (11.6383)	loss 0.7695 (0.7695)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:10:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:16:59 lr 0.000008	 wd 0.0000	time 0.2982 (0.4243)	loss 0.8218 (0.7957)	grad_norm 2.0192 (2.1738)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:10:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:14:08 lr 0.000007	 wd 0.0000	time 0.2876 (0.3688)	loss 0.7397 (0.7958)	grad_norm 2.1956 (2.2000)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:11:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:13:05 lr 0.000007	 wd 0.0000	time 0.3115 (0.3566)	loss 0.8311 (0.7980)	grad_norm 2.0094 (2.1783)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:12:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:12:06 lr 0.000007	 wd 0.0000	time 0.2864 (0.3456)	loss 0.8486 (0.7971)	grad_norm 2.1913 (2.1756)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:12:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:11:20 lr 0.000007	 wd 0.0000	time 0.2779 (0.3399)	loss 0.7441 (0.7978)	grad_norm 1.6731 (2.1624)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:13:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:10:37 lr 0.000007	 wd 0.0000	time 0.2945 (0.3353)	loss 0.8086 (0.7988)	grad_norm 1.8563 (2.1568)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:13:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:09:58 lr 0.000007	 wd 0.0000	time 0.2965 (0.3322)	loss 0.7964 (0.7981)	grad_norm 2.2027 (2.1613)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:14:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:21 lr 0.000007	 wd 0.0000	time 0.2922 (0.3301)	loss 0.8594 (0.7977)	grad_norm 1.9414 (2.1689)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:14:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:08:46 lr 0.000007	 wd 0.0000	time 0.2613 (0.3285)	loss 0.7627 (0.7973)	grad_norm 2.1763 (2.1635)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:15:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:08:11 lr 0.000007	 wd 0.0000	time 0.3013 (0.3272)	loss 0.9023 (0.7972)	grad_norm 2.3647 (2.1653)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:15:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:07:37 lr 0.000007	 wd 0.0000	time 0.2874 (0.3261)	loss 0.7510 (0.7969)	grad_norm 2.0572 (2.1644)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:16:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:07:04 lr 0.000006	 wd 0.0000	time 0.2566 (0.3263)	loss 1.0078 (0.7980)	grad_norm 2.2336 (2.1572)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:16:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:31 lr 0.000006	 wd 0.0000	time 0.2954 (0.3259)	loss 0.7959 (0.7981)	grad_norm 1.8557 (2.1520)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:17:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:05:58 lr 0.000006	 wd 0.0000	time 0.2869 (0.3251)	loss 0.7729 (0.7978)	grad_norm 1.6491 (2.1461)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:17:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:25 lr 0.000006	 wd 0.0000	time 0.2987 (0.3245)	loss 0.8472 (0.7979)	grad_norm 1.9235 (2.1473)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:18:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:04:53 lr 0.000006	 wd 0.0000	time 0.3237 (0.3253)	loss 0.8579 (0.7975)	grad_norm 2.1427 (2.1441)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:18:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:21 lr 0.000006	 wd 0.0000	time 0.2833 (0.3262)	loss 0.7944 (0.7969)	grad_norm 2.0139 (2.1434)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:19:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:48 lr 0.000006	 wd 0.0000	time 0.2943 (0.3258)	loss 0.9888 (0.7978)	grad_norm 2.9384 (2.1410)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:20:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:16 lr 0.000006	 wd 0.0000	time 0.3130 (0.3257)	loss 1.0918 (0.7981)	grad_norm 2.1348 (2.1415)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:20:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:43 lr 0.000006	 wd 0.0000	time 0.2716 (0.3258)	loss 0.8408 (0.7980)	grad_norm 2.3651 (2.1446)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:21:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:10 lr 0.000006	 wd 0.0000	time 0.2955 (0.3258)	loss 0.8403 (0.7980)	grad_norm 1.7783 (2.1425)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:21:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:38 lr 0.000006	 wd 0.0000	time 0.3111 (0.3270)	loss 0.7114 (0.7973)	grad_norm 2.2006 (2.1414)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:22:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:06 lr 0.000005	 wd 0.0000	time 0.3026 (0.3277)	loss 0.7725 (0.7972)	grad_norm 1.8822 (2.1448)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:22:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:33 lr 0.000005	 wd 0.0000	time 0.2989 (0.3290)	loss 0.7686 (0.7971)	grad_norm 1.6933 (2.1443)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:23:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0000	time 0.2681 (0.3287)	loss 0.7607 (0.7968)	grad_norm 2.1404 (2.1451)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:23:32 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 25 training takes 0:13:48
[2024-06-29 19:23:44 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.900 (11.900)	Loss 0.3604 (0.3604)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 19:24:00 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.592 Acc@5 97.588
[2024-06-29 19:24:00 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-06-29 19:24:00 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.59%
[2024-06-29 19:24:00 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 19:24:03 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 19:24:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:06:36 lr 0.000005	 wd 0.0000	time 10.2302 (10.2302)	loss 0.7769 (0.7769)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:24:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:16:25 lr 0.000005	 wd 0.0000	time 0.2963 (0.4104)	loss 0.9106 (0.7992)	grad_norm 1.9508 (2.1115)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:25:15 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:13:53 lr 0.000005	 wd 0.0000	time 0.3011 (0.3619)	loss 0.8306 (0.7993)	grad_norm 2.9412 (2.1577)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:25:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:12:41 lr 0.000005	 wd 0.0000	time 0.2809 (0.3457)	loss 0.7515 (0.7992)	grad_norm 2.0516 (2.1539)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:26:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:11:49 lr 0.000005	 wd 0.0000	time 0.2952 (0.3376)	loss 0.8384 (0.7973)	grad_norm 1.8699 (2.1570)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:26:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:11:06 lr 0.000005	 wd 0.0000	time 0.3004 (0.3327)	loss 0.9258 (0.7984)	grad_norm 2.0378 (2.1413)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:27:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:10:28 lr 0.000005	 wd 0.0000	time 0.2808 (0.3304)	loss 0.8008 (0.7983)	grad_norm 2.0298 (2.1405)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:27:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:09:51 lr 0.000005	 wd 0.0000	time 0.3252 (0.3280)	loss 0.9404 (0.7969)	grad_norm 1.7339 (2.1294)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-06-29 19:28:24 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:09:15 lr 0.000005	 wd 0.0000	time 0.2883 (0.3264)	loss 0.7603 (0.7965)	grad_norm 1.7712 (2.1289)	loss_scale 8192.0000 (4290.3171)	mem 11634MB
[2024-06-29 19:28:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:08:40 lr 0.000005	 wd 0.0000	time 0.2762 (0.3249)	loss 0.8496 (0.7959)	grad_norm 1.9058 (2.1286)	loss_scale 8192.0000 (4723.3563)	mem 11634MB
[2024-06-29 19:29:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:06 lr 0.000004	 wd 0.0000	time 0.2959 (0.3237)	loss 0.9058 (0.7962)	grad_norm 2.0395 (2.1334)	loss_scale 8192.0000 (5069.8741)	mem 11634MB
[2024-06-29 19:29:58 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:07:33 lr 0.000004	 wd 0.0000	time 0.3061 (0.3232)	loss 0.7974 (0.7960)	grad_norm 1.9542 (2.1372)	loss_scale 8192.0000 (5353.4460)	mem 11634MB
[2024-06-29 19:30:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:07:00 lr 0.000004	 wd 0.0000	time 0.2889 (0.3226)	loss 0.7300 (0.7968)	grad_norm 2.2084 (2.1326)	loss_scale 8192.0000 (5589.7952)	mem 11634MB
[2024-06-29 19:31:01 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:26 lr 0.000004	 wd 0.0000	time 0.2993 (0.3219)	loss 0.7329 (0.7967)	grad_norm 2.4430 (2.1339)	loss_scale 8192.0000 (5789.8109)	mem 11634MB
[2024-06-29 19:31:33 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:05:54 lr 0.000004	 wd 0.0000	time 0.3179 (0.3213)	loss 0.8838 (0.7972)	grad_norm 1.7798 (2.1390)	loss_scale 8192.0000 (5961.2734)	mem 11634MB
[2024-06-29 19:32:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:22 lr 0.000004	 wd 0.0000	time 0.2911 (0.3222)	loss 0.7388 (0.7974)	grad_norm 2.3168 (2.1389)	loss_scale 8192.0000 (6109.8894)	mem 11634MB
[2024-06-29 19:32:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:04:51 lr 0.000004	 wd 0.0000	time 0.2901 (0.3228)	loss 0.7222 (0.7963)	grad_norm 2.2876 (2.1416)	loss_scale 8192.0000 (6239.9400)	mem 11634MB
[2024-06-29 19:33:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:18 lr 0.000004	 wd 0.0000	time 0.2874 (0.3225)	loss 0.7188 (0.7957)	grad_norm 2.3798 (2.1420)	loss_scale 8192.0000 (6354.6996)	mem 11634MB
[2024-06-29 19:33:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:46 lr 0.000004	 wd 0.0000	time 0.2868 (0.3226)	loss 0.7764 (0.7956)	grad_norm 1.9334 (2.1398)	loss_scale 8192.0000 (6456.7152)	mem 11634MB
[2024-06-29 19:34:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:14 lr 0.000004	 wd 0.0000	time 0.2761 (0.3236)	loss 0.7075 (0.7956)	grad_norm 2.1196 (2.1399)	loss_scale 8192.0000 (6547.9979)	mem 11634MB
[2024-06-29 19:34:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:44 lr 0.000004	 wd 0.0000	time 0.2510 (0.3268)	loss 0.8071 (0.7957)	grad_norm 1.9264 (2.1367)	loss_scale 8192.0000 (6630.1569)	mem 11634MB
[2024-06-29 19:35:32 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:11 lr 0.000004	 wd 0.0000	time 0.3197 (0.3280)	loss 0.8335 (0.7958)	grad_norm 2.0915 (2.1362)	loss_scale 8192.0000 (6704.4950)	mem 11634MB
[2024-06-29 19:36:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:39 lr 0.000004	 wd 0.0000	time 0.2929 (0.3297)	loss 0.7798 (0.7959)	grad_norm 1.8302 (2.1368)	loss_scale 8192.0000 (6772.0781)	mem 11634MB
[2024-06-29 19:36:45 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:06 lr 0.000004	 wd 0.0000	time 0.3112 (0.3313)	loss 0.7524 (0.7961)	grad_norm 2.1783 (2.1350)	loss_scale 8192.0000 (6833.7870)	mem 11634MB
[2024-06-29 19:37:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:33 lr 0.000003	 wd 0.0000	time 0.2798 (0.3327)	loss 0.8140 (0.7965)	grad_norm 2.4771 (2.1333)	loss_scale 8192.0000 (6890.3557)	mem 11634MB
[2024-06-29 19:37:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.2715 (0.3321)	loss 0.7144 (0.7972)	grad_norm 3.0480 (2.1350)	loss_scale 8192.0000 (6942.4006)	mem 11634MB
[2024-06-29 19:38:01 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 26 training takes 0:13:57
[2024-06-29 19:38:13 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.162 (12.162)	Loss 0.3618 (0.3618)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 19:38:29 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.598 Acc@5 97.578
[2024-06-29 19:38:29 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-06-29 19:38:29 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.60%
[2024-06-29 19:38:29 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 19:38:30 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 19:38:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][0/2502]	eta 6:53:29 lr 0.000003	 wd 0.0000	time 9.9158 (9.9158)	loss 0.7231 (0.7231)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:39:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:16:18 lr 0.000003	 wd 0.0000	time 0.2788 (0.4072)	loss 0.8306 (0.7968)	grad_norm 3.3224 (2.1928)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:39:43 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:13:48 lr 0.000003	 wd 0.0000	time 0.2884 (0.3599)	loss 0.7607 (0.7948)	grad_norm 2.2839 (2.1721)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:40:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:12:39 lr 0.000003	 wd 0.0000	time 0.2875 (0.3449)	loss 0.8057 (0.7909)	grad_norm 2.2429 (2.1707)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:40:46 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:11:48 lr 0.000003	 wd 0.0000	time 0.3189 (0.3371)	loss 0.8125 (0.7921)	grad_norm 1.8904 (2.1492)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:41:17 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:11:05 lr 0.000003	 wd 0.0000	time 0.2710 (0.3324)	loss 0.7192 (0.7902)	grad_norm 1.7949 (2.1374)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:41:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:10:25 lr 0.000003	 wd 0.0000	time 0.2402 (0.3290)	loss 0.7476 (0.7936)	grad_norm 2.3021 (2.1289)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:42:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:09:49 lr 0.000003	 wd 0.0000	time 0.3166 (0.3272)	loss 0.7944 (0.7958)	grad_norm 2.4416 (2.1258)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:42:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:14 lr 0.000003	 wd 0.0000	time 0.2951 (0.3256)	loss 0.8589 (0.7945)	grad_norm 2.0001 (2.1297)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:43:23 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:08:39 lr 0.000003	 wd 0.0000	time 0.2397 (0.3244)	loss 0.7632 (0.7964)	grad_norm 1.9126 (2.1287)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:43:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:08:05 lr 0.000003	 wd 0.0000	time 0.2949 (0.3234)	loss 0.9033 (0.7954)	grad_norm 2.1664 (2.1364)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:44:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:07:32 lr 0.000003	 wd 0.0000	time 0.3078 (0.3229)	loss 0.7197 (0.7959)	grad_norm 3.0819 (2.1336)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:44:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:01 lr 0.000003	 wd 0.0000	time 0.3159 (0.3234)	loss 0.7539 (0.7954)	grad_norm 2.0409 (2.1374)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:45:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:28 lr 0.000003	 wd 0.0000	time 0.2949 (0.3234)	loss 0.8247 (0.7954)	grad_norm 2.3073 (2.1381)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:46:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:05:55 lr 0.000003	 wd 0.0000	time 0.2838 (0.3229)	loss 0.6914 (0.7955)	grad_norm 2.4425 (2.1348)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:46:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:24 lr 0.000003	 wd 0.0000	time 0.3302 (0.3235)	loss 0.8403 (0.7961)	grad_norm 2.4597 (2.1315)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:47:08 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:04:51 lr 0.000003	 wd 0.0000	time 0.3381 (0.3232)	loss 0.7485 (0.7966)	grad_norm 1.9998 (2.1304)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:47:40 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:19 lr 0.000002	 wd 0.0000	time 0.2984 (0.3230)	loss 0.8721 (0.7961)	grad_norm 1.7528 (2.1291)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:48:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:47 lr 0.000002	 wd 0.0000	time 0.3012 (0.3236)	loss 0.7729 (0.7961)	grad_norm 2.2688 (2.1272)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:48:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:15 lr 0.000002	 wd 0.0000	time 0.2715 (0.3251)	loss 0.8677 (0.7960)	grad_norm 1.9718 (2.1270)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:49:26 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:44 lr 0.000002	 wd 0.0000	time 0.3283 (0.3277)	loss 0.6797 (0.7957)	grad_norm 2.0137 (2.1272)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:50:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:12 lr 0.000002	 wd 0.0000	time 0.2870 (0.3294)	loss 0.9292 (0.7968)	grad_norm 2.2366 (2.1286)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:50:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:39 lr 0.000002	 wd 0.0000	time 0.3595 (0.3310)	loss 0.7100 (0.7967)	grad_norm 1.7417 (2.1262)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 19:51:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:07 lr 0.000002	 wd 0.0000	time 0.3065 (0.3328)	loss 0.8364 (0.7970)	grad_norm 1.8975 (2.1303)	loss_scale 16384.0000 (8334.4076)	mem 11634MB
[2024-06-29 19:51:51 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:34 lr 0.000002	 wd 0.0000	time 0.2691 (0.3334)	loss 0.8320 (0.7970)	grad_norm 1.8685 (2.1330)	loss_scale 16384.0000 (8669.6676)	mem 11634MB
[2024-06-29 19:52:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.2712 (0.3321)	loss 0.7363 (0.7973)	grad_norm 1.7332 (2.1338)	loss_scale 16384.0000 (8978.1176)	mem 11634MB
[2024-06-29 19:52:28 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 27 training takes 0:13:57
[2024-06-29 19:52:40 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.240 (12.240)	Loss 0.3599 (0.3599)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 19:52:56 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.596 Acc@5 97.588
[2024-06-29 19:52:56 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-06-29 19:52:56 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.60%
[2024-06-29 19:53:07 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:45:31 lr 0.000002	 wd 0.0000	time 11.1638 (11.1638)	loss 0.7827 (0.7827)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 19:53:39 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:17:09 lr 0.000002	 wd 0.0000	time 0.2741 (0.4286)	loss 0.8154 (0.8055)	grad_norm 2.4805 (2.1476)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 19:54:11 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:14:19 lr 0.000002	 wd 0.0000	time 0.2959 (0.3733)	loss 0.8428 (0.8005)	grad_norm 2.7165 (2.1431)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 19:54:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:12:56 lr 0.000002	 wd 0.0000	time 0.2630 (0.3526)	loss 0.8169 (0.8001)	grad_norm 1.5833 (2.1194)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 19:55:13 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:11:59 lr 0.000002	 wd 0.0000	time 0.2877 (0.3424)	loss 0.6948 (0.7987)	grad_norm 2.0395 (2.1172)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 19:55:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:13 lr 0.000002	 wd 0.0000	time 0.2835 (0.3365)	loss 0.9004 (0.7992)	grad_norm 2.2319 (2.1153)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 19:56:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:10:32 lr 0.000002	 wd 0.0000	time 0.2773 (0.3326)	loss 0.8032 (0.7988)	grad_norm 2.2475 (2.1190)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-06-29 19:56:47 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:09:54 lr 0.000002	 wd 0.0000	time 0.2720 (0.3297)	loss 0.7715 (0.7974)	grad_norm 2.4094 (inf)	loss_scale 8192.0000 (15449.1070)	mem 11634MB
[2024-06-29 19:57:18 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:17 lr 0.000002	 wd 0.0000	time 0.2926 (0.3276)	loss 0.7715 (0.7963)	grad_norm 2.2855 (inf)	loss_scale 8192.0000 (14543.1011)	mem 11634MB
[2024-06-29 19:57:49 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:08:41 lr 0.000002	 wd 0.0000	time 0.2749 (0.3258)	loss 0.8506 (0.7950)	grad_norm 2.7260 (inf)	loss_scale 8192.0000 (13838.2064)	mem 11634MB
[2024-06-29 19:58:21 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:08 lr 0.000002	 wd 0.0000	time 0.2743 (0.3250)	loss 0.8794 (0.7945)	grad_norm 1.8766 (inf)	loss_scale 8192.0000 (13274.1499)	mem 11634MB
[2024-06-29 19:58:53 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:07:35 lr 0.000002	 wd 0.0000	time 0.2987 (0.3248)	loss 0.7437 (0.7941)	grad_norm 2.4406 (inf)	loss_scale 8192.0000 (12812.5559)	mem 11634MB
[2024-06-29 19:59:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:07:01 lr 0.000002	 wd 0.0000	time 0.3070 (0.3240)	loss 0.9087 (0.7933)	grad_norm 2.6983 (inf)	loss_scale 8192.0000 (12427.8301)	mem 11634MB
[2024-06-29 19:59:57 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:29 lr 0.000002	 wd 0.0000	time 0.2742 (0.3237)	loss 0.7485 (0.7932)	grad_norm 2.2149 (inf)	loss_scale 8192.0000 (12102.2475)	mem 11634MB
[2024-06-29 20:00:30 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:05:57 lr 0.000002	 wd 0.0000	time 0.2377 (0.3241)	loss 0.7407 (0.7934)	grad_norm 2.1149 (inf)	loss_scale 8192.0000 (11823.1435)	mem 11634MB
[2024-06-29 20:01:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:24 lr 0.000002	 wd 0.0000	time 0.3059 (0.3239)	loss 0.7627 (0.7934)	grad_norm 1.6920 (inf)	loss_scale 8192.0000 (11581.2285)	mem 11634MB
[2024-06-29 20:01:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:04:51 lr 0.000002	 wd 0.0000	time 0.3067 (0.3235)	loss 0.6787 (0.7937)	grad_norm 1.7133 (inf)	loss_scale 8192.0000 (11369.5340)	mem 11634MB
[2024-06-29 20:02:06 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:19 lr 0.000001	 wd 0.0000	time 0.3002 (0.3233)	loss 0.6880 (0.7941)	grad_norm 1.8708 (inf)	loss_scale 8192.0000 (11182.7302)	mem 11634MB
[2024-06-29 20:02:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:46 lr 0.000001	 wd 0.0000	time 0.2955 (0.3231)	loss 0.6973 (0.7943)	grad_norm 2.6595 (inf)	loss_scale 8192.0000 (11016.6707)	mem 11634MB
[2024-06-29 20:03:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:14 lr 0.000001	 wd 0.0000	time 0.2890 (0.3229)	loss 0.8467 (0.7944)	grad_norm 1.8841 (inf)	loss_scale 8192.0000 (10868.0821)	mem 11634MB
[2024-06-29 20:03:42 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:42 lr 0.000001	 wd 0.0000	time 0.2992 (0.3231)	loss 0.7612 (0.7943)	grad_norm 2.1428 (inf)	loss_scale 8192.0000 (10734.3448)	mem 11634MB
[2024-06-29 20:04:14 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:09 lr 0.000001	 wd 0.0000	time 0.3030 (0.3230)	loss 0.8667 (0.7946)	grad_norm 2.1529 (inf)	loss_scale 8192.0000 (10613.3384)	mem 11634MB
[2024-06-29 20:04:50 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:38 lr 0.000001	 wd 0.0000	time 0.3173 (0.3247)	loss 0.8730 (0.7946)	grad_norm 2.1855 (inf)	loss_scale 8192.0000 (10503.3276)	mem 11634MB
[2024-06-29 20:05:27 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:05 lr 0.000001	 wd 0.0000	time 0.3940 (0.3266)	loss 0.7778 (0.7950)	grad_norm 2.5063 (inf)	loss_scale 8192.0000 (10402.8787)	mem 11634MB
[2024-06-29 20:06:03 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:33 lr 0.000001	 wd 0.0000	time 0.3293 (0.3277)	loss 0.8418 (0.7952)	grad_norm 2.4168 (inf)	loss_scale 8192.0000 (10310.7972)	mem 11634MB
[2024-06-29 20:06:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.2690 (0.3280)	loss 0.7617 (0.7958)	grad_norm 2.2193 (inf)	loss_scale 8192.0000 (10226.0792)	mem 11634MB
[2024-06-29 20:06:42 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 28 training takes 0:13:46
[2024-06-29 20:06:54 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 12.256 (12.256)	Loss 0.3596 (0.3596)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 20:07:10 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.610 Acc@5 97.572
[2024-06-29 20:07:10 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-06-29 20:07:10 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.61%
[2024-06-29 20:07:10 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 20:07:12 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 20:07:22 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:19:31 lr 0.000001	 wd 0.0000	time 10.5400 (10.5400)	loss 0.7686 (0.7686)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:07:54 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:16:34 lr 0.000001	 wd 0.0000	time 0.3118 (0.4142)	loss 0.8872 (0.7961)	grad_norm 2.1488 (2.0937)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:08:25 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:13:55 lr 0.000001	 wd 0.0000	time 0.2880 (0.3628)	loss 0.7539 (0.8033)	grad_norm 1.8676 (2.0842)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:08:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:12:43 lr 0.000001	 wd 0.0000	time 0.2611 (0.3467)	loss 0.7705 (0.7995)	grad_norm 2.4828 (2.0907)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:09:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:11:51 lr 0.000001	 wd 0.0000	time 0.2895 (0.3386)	loss 0.7236 (0.8007)	grad_norm 2.1666 (2.0960)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:09:59 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:11:09 lr 0.000001	 wd 0.0000	time 0.2876 (0.3342)	loss 0.7598 (0.7995)	grad_norm 1.9130 (2.1033)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:10:31 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:10:30 lr 0.000001	 wd 0.0000	time 0.2687 (0.3313)	loss 0.7749 (0.7971)	grad_norm 1.9835 (2.0925)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:11:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:09:52 lr 0.000001	 wd 0.0000	time 0.3037 (0.3290)	loss 0.9067 (0.7958)	grad_norm 1.7997 (2.0793)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:11:34 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:16 lr 0.000001	 wd 0.0000	time 0.3005 (0.3271)	loss 0.7104 (0.7976)	grad_norm 2.0586 (2.0871)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:12:05 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:08:41 lr 0.000001	 wd 0.0000	time 0.2966 (0.3255)	loss 0.8262 (0.7974)	grad_norm 2.1404 (2.0924)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:12:37 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:08:07 lr 0.000001	 wd 0.0000	time 0.3002 (0.3244)	loss 0.6987 (0.7967)	grad_norm 1.7462 (2.0926)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:13:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:34 lr 0.000001	 wd 0.0000	time 0.2935 (0.3244)	loss 0.7896 (0.7971)	grad_norm 2.1396 (2.0931)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:13:41 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:07:01 lr 0.000001	 wd 0.0000	time 0.2738 (0.3239)	loss 0.7832 (0.7959)	grad_norm 2.3397 (2.0972)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:14:12 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:28 lr 0.000001	 wd 0.0000	time 0.2858 (0.3233)	loss 0.7031 (0.7971)	grad_norm 2.3003 (2.1017)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:14:44 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:05:55 lr 0.000001	 wd 0.0000	time 0.2821 (0.3228)	loss 0.9023 (0.7974)	grad_norm 1.7373 (2.1012)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:15:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:23 lr 0.000001	 wd 0.0000	time 0.2942 (0.3225)	loss 0.7192 (0.7970)	grad_norm 1.9969 (2.0990)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:15:48 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:04:50 lr 0.000001	 wd 0.0000	time 0.2890 (0.3223)	loss 0.7822 (0.7968)	grad_norm 1.9766 (2.1016)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:16:20 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:18 lr 0.000001	 wd 0.0000	time 0.2998 (0.3223)	loss 0.9907 (0.7974)	grad_norm 2.0616 (2.1068)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:16:55 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:47 lr 0.000001	 wd 0.0000	time 0.2832 (0.3237)	loss 0.6904 (0.7978)	grad_norm 2.1361 (2.1104)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:17:28 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:15 lr 0.000001	 wd 0.0000	time 0.2650 (0.3242)	loss 0.7368 (0.7985)	grad_norm 2.0560 (2.1087)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:18:02 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:43 lr 0.000001	 wd 0.0000	time 0.3169 (0.3250)	loss 0.8452 (0.7985)	grad_norm 2.2198 (2.1062)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:18:38 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:11 lr 0.000001	 wd 0.0000	time 0.4232 (0.3265)	loss 0.8208 (0.7985)	grad_norm 2.0800 (2.1049)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-06-29 20:19:16 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:39 lr 0.000001	 wd 0.0000	time 0.2952 (0.3292)	loss 0.9307 (0.7980)	grad_norm 1.8151 (2.1037)	loss_scale 16384.0000 (8497.1995)	mem 11634MB
[2024-06-29 20:19:56 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:07 lr 0.000001	 wd 0.0000	time 0.3752 (0.3321)	loss 0.7812 (0.7978)	grad_norm 2.3482 (2.1059)	loss_scale 16384.0000 (8839.9548)	mem 11634MB
[2024-06-29 20:20:36 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0000	time 0.2932 (0.3348)	loss 0.7725 (0.7980)	grad_norm 1.9095 (2.1040)	loss_scale 16384.0000 (9154.1591)	mem 11634MB
[2024-06-29 20:21:09 convnext_base_224_22kto1kto1k_finetune] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.2691 (0.3345)	loss 0.9702 (0.7979)	grad_norm 1.8169 (2.1061)	loss_scale 16384.0000 (9443.2371)	mem 11634MB
[2024-06-29 20:21:16 convnext_base_224_22kto1kto1k_finetune] (main.py 249): INFO EPOCH 29 training takes 0:14:03
[2024-06-29 20:21:16 convnext_base_224_22kto1kto1k_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_29.pth saving......
[2024-06-29 20:21:17 convnext_base_224_22kto1kto1k_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_29.pth saved !!!
[2024-06-29 20:21:29 convnext_base_224_22kto1kto1k_finetune] (main.py 289): INFO Test: [0/98]	Time 11.382 (11.382)	Loss 0.3599 (0.3599)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-06-29 20:21:44 convnext_base_224_22kto1kto1k_finetune] (main.py 296): INFO  * Acc@1 85.630 Acc@5 97.572
[2024-06-29 20:21:44 convnext_base_224_22kto1kto1k_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-06-29 20:21:44 convnext_base_224_22kto1kto1k_finetune] (main.py 182): INFO Max accuracy: 85.63%
[2024-06-29 20:21:44 convnext_base_224_22kto1kto1k_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 20:21:46 convnext_base_224_22kto1kto1k_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_base_224_22kto1kto1k_finetune/convnext_base_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 20:21:46 convnext_base_224_22kto1kto1k_finetune] (main.py 189): INFO Training time 7:08:14
