[2024-05-28 09:27:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 355): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 7
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x
  NUM_CLASSES: 1000
  PRETRAINED: /root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_large_patch4_window7_224_22k.pth
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SMT:
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    HEAD_CONV: 3
    IN_CHANS: 3
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    NUM_STAGES: 4
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 192
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 24
    - 48
    - 96
    - 192
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 6
    - 12
    - 24
    - 48
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin_mam_v2
OUTPUT: /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x/swin_mam_v2_large_p4_w7_224_22kto1k
PRINT_FREQ: 100
SAVE_FREQ: 30
SEED: 0
TAG: swin_mam_v2_large_p4_w7_224_22kto1k
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-05-28 09:27:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 356): INFO {"cfg": "/root/mam_classifier/configs/swin_mam_v2/swin_mam_v2_large_patch4_window7_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_large_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "/mnt/data/pretrain_weights/swin-mam/", "tag": "swin_mam_v2_large_p4_w7_224_22kto1k", "eval": false, "throughput": false, "local_rank": 7, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-05-28 09:27:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 93): INFO Creating model:swin_mam_v2/swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x
[2024-05-28 09:27:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 95): INFO MemorySwinTransformerV2(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=192, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=192, input_resolution=(56, 56), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=192, out_features=24, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=192, input_resolution=(56, 56), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=24, out_features=192, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=384, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=384, input_resolution=(28, 28), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=384, input_resolution=(28, 28), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=768, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): MemorySwinTransformerBlock(
          dim=768, input_resolution=(14, 14), num_heads=24, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=768
        (reduction): Linear(in_features=3072, out_features=1536, bias=False)
        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1536, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=1536, input_resolution=(7, 7), num_heads=48, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=1536, window_size=(7, 7), num_heads=48
            (qkv): Linear(in_features=1536, out_features=4608, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1536, out_features=1536, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=1536, out_features=192, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=6144, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=6144, out_features=1536, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=1536, input_resolution=(7, 7), num_heads=48, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=1536, window_size=(7, 7), num_heads=48
            (qkv): Linear(in_features=1536, out_features=4608, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1536, out_features=1536, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=192, out_features=1536, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1536, out_features=6144, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=6144, out_features=1536, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1536, out_features=1000, bias=True)
)
[2024-05-28 09:27:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 98): INFO number of params: 199692196
[2024-05-28 09:27:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 101): INFO number of GFLOPs: 34.487049216
[2024-05-28 09:27:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 136): INFO no checkpoint found in /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x/swin_mam_v2_large_p4_w7_224_22kto1k, ignoring auto resume
[2024-05-28 09:27:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (utils.py 46): INFO ==============> Loading weight /root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_large_patch4_window7_224_22k.pth for fine-tuning......
[2024-05-28 09:27:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-05-28 09:27:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.0.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.0.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.1.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.1.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.1.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.2.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.2.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.2.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.3.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.3.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.3.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.4.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.4.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.4.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.5.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.5.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.5.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.6.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.6.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.6.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.7.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.7.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.7.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.8.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.8.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.8.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.9.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.9.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.9.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.10.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.10.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.10.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.11.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.11.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.11.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.12.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.12.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.12.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.13.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.13.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.13.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.14.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.14.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.14.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.15.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.15.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.15.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.16.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.16.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.16.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.17.attn.proj_latent_space_to_feature.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.3.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.3.blocks.1.attn.proj_latent_space_to_feature.bias'], unexpected_keys=[])
[2024-05-28 09:27:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (utils.py 129): INFO => loaded successfully '/root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_large_patch4_window7_224_22k.pth'
[2024-05-28 09:29:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 73.516 (73.516)	Loss 0.3250 (0.3250)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 3102MB
[2024-05-28 09:30:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 83.892 Acc@5 96.964
[2024-05-28 09:30:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 148): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-05-28 09:30:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 154): INFO Start training
[2024-05-28 09:30:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][0/2502]	eta 1 day, 0:44:03 lr 0.000000	 wd 0.0000	time 35.5889 (35.5889)	loss 1.2570 (1.2570)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17820MB
[2024-05-28 09:31:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][100/2502]	eta 0:36:01 lr 0.000000	 wd 0.0000	time 0.5227 (0.9000)	loss 1.6433 (1.4271)	grad_norm 4.0312 (nan)	loss_scale 4096.0000 (6732.0396)	mem 20118MB
[2024-05-28 09:32:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][200/2502]	eta 0:27:43 lr 0.000001	 wd 0.0000	time 0.5086 (0.7226)	loss 1.3557 (1.4292)	grad_norm 5.7117 (nan)	loss_scale 2048.0000 (5298.3085)	mem 20118MB
[2024-05-28 09:33:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][300/2502]	eta 0:24:22 lr 0.000001	 wd 0.0000	time 0.5148 (0.6641)	loss 1.6419 (1.4142)	grad_norm 4.0741 (nan)	loss_scale 2048.0000 (4218.4718)	mem 20118MB
[2024-05-28 09:34:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][400/2502]	eta 0:22:14 lr 0.000001	 wd 0.0000	time 0.5141 (0.6347)	loss 1.5639 (1.4097)	grad_norm 7.4393 (nan)	loss_scale 2048.0000 (3677.2070)	mem 20118MB
[2024-05-28 09:35:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][500/2502]	eta 0:20:35 lr 0.000002	 wd 0.0000	time 0.5207 (0.6170)	loss 1.1528 (1.4011)	grad_norm 5.3049 (nan)	loss_scale 2048.0000 (3352.0160)	mem 20118MB
[2024-05-28 09:36:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][600/2502]	eta 0:19:10 lr 0.000002	 wd 0.0000	time 0.5154 (0.6050)	loss 1.3747 (1.3948)	grad_norm 4.0222 (nan)	loss_scale 2048.0000 (3135.0416)	mem 20118MB
[2024-05-28 09:37:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][700/2502]	eta 0:17:54 lr 0.000002	 wd 0.0000	time 0.5133 (0.5963)	loss 0.9606 (1.3836)	grad_norm 4.8127 (nan)	loss_scale 2048.0000 (2979.9715)	mem 20118MB
[2024-05-28 09:38:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][800/2502]	eta 0:17:19 lr 0.000003	 wd 0.0000	time 0.6079 (0.6107)	loss 0.9343 (1.3793)	grad_norm 3.5426 (nan)	loss_scale 1024.0000 (2735.7803)	mem 20118MB
[2024-05-28 09:39:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][900/2502]	eta 0:16:48 lr 0.000003	 wd 0.0000	time 0.5446 (0.6297)	loss 1.4912 (1.3755)	grad_norm 3.6013 (nan)	loss_scale 1024.0000 (2545.7936)	mem 20118MB
[2024-05-28 09:40:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1000/2502]	eta 0:15:33 lr 0.000003	 wd 0.0000	time 0.5150 (0.6212)	loss 1.5187 (1.3669)	grad_norm 4.1811 (nan)	loss_scale 1024.0000 (2393.7662)	mem 20118MB
[2024-05-28 09:41:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1100/2502]	eta 0:14:21 lr 0.000004	 wd 0.0000	time 0.5257 (0.6143)	loss 1.4013 (1.3611)	grad_norm 4.0343 (nan)	loss_scale 1024.0000 (2269.3551)	mem 20118MB
[2024-05-28 09:42:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1200/2502]	eta 0:13:12 lr 0.000004	 wd 0.0000	time 0.5296 (0.6086)	loss 1.5460 (1.3585)	grad_norm 5.4155 (nan)	loss_scale 1024.0000 (2165.6619)	mem 20118MB
[2024-05-28 09:43:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1300/2502]	eta 0:12:05 lr 0.000004	 wd 0.0000	time 0.5166 (0.6037)	loss 1.1478 (1.3553)	grad_norm 3.6157 (nan)	loss_scale 1024.0000 (2077.9093)	mem 20118MB
[2024-05-28 09:44:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1400/2502]	eta 0:11:00 lr 0.000005	 wd 0.0000	time 0.5151 (0.5994)	loss 1.5711 (1.3524)	grad_norm 5.2623 (nan)	loss_scale 1024.0000 (2002.6838)	mem 20118MB
[2024-05-28 09:45:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1500/2502]	eta 0:10:05 lr 0.000005	 wd 0.0000	time 0.5539 (0.6043)	loss 1.3313 (1.3521)	grad_norm 3.9905 (nan)	loss_scale 1024.0000 (1937.4817)	mem 20118MB
[2024-05-28 09:46:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1600/2502]	eta 0:09:09 lr 0.000005	 wd 0.0000	time 0.5086 (0.6088)	loss 0.9644 (1.3510)	grad_norm 3.1972 (nan)	loss_scale 1024.0000 (1880.4247)	mem 20118MB
[2024-05-28 09:47:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1700/2502]	eta 0:08:07 lr 0.000005	 wd 0.0000	time 0.5178 (0.6073)	loss 1.2548 (1.3474)	grad_norm 4.2992 (nan)	loss_scale 1024.0000 (1830.0764)	mem 20118MB
[2024-05-28 09:48:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1800/2502]	eta 0:07:08 lr 0.000006	 wd 0.0000	time 0.5242 (0.6102)	loss 1.5138 (1.3448)	grad_norm 3.6631 (nan)	loss_scale 1024.0000 (1785.3193)	mem 20118MB
[2024-05-28 09:49:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][1900/2502]	eta 0:06:10 lr 0.000006	 wd 0.0000	time 0.9985 (0.6158)	loss 1.3472 (1.3453)	grad_norm 4.5193 (nan)	loss_scale 1024.0000 (1745.2709)	mem 20118MB
[2024-05-28 09:51:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][2000/2502]	eta 0:05:16 lr 0.000006	 wd 0.0000	time 0.5119 (0.6304)	loss 1.3958 (1.3438)	grad_norm 4.1331 (nan)	loss_scale 1024.0000 (1709.2254)	mem 20118MB
[2024-05-28 09:51:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][2100/2502]	eta 0:04:11 lr 0.000007	 wd 0.0000	time 0.5175 (0.6262)	loss 1.5366 (1.3412)	grad_norm 4.4290 (nan)	loss_scale 1024.0000 (1676.6111)	mem 20118MB
[2024-05-28 09:52:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][2200/2502]	eta 0:03:07 lr 0.000007	 wd 0.0000	time 0.5074 (0.6224)	loss 1.4206 (1.3391)	grad_norm 5.6548 (nan)	loss_scale 1024.0000 (1646.9605)	mem 20118MB
[2024-05-28 09:53:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][2300/2502]	eta 0:02:05 lr 0.000007	 wd 0.0000	time 0.5161 (0.6189)	loss 1.3114 (1.3375)	grad_norm 3.8635 (nan)	loss_scale 1024.0000 (1619.8870)	mem 20118MB
[2024-05-28 09:54:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][2400/2502]	eta 0:01:02 lr 0.000008	 wd 0.0000	time 0.5268 (0.6158)	loss 1.4192 (1.3356)	grad_norm 5.9758 (nan)	loss_scale 1024.0000 (1595.0687)	mem 20118MB
[2024-05-28 09:55:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0000	time 0.5144 (0.6127)	loss 1.2971 (1.3350)	grad_norm 4.8029 (nan)	loss_scale 1024.0000 (1572.2351)	mem 20118MB
[2024-05-28 09:55:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 0 training takes 0:25:38
[2024-05-28 09:57:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 97.522 (97.522)	Loss 0.4119 (0.4119)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 20118MB
[2024-05-28 09:58:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 84.752 Acc@5 97.446
[2024-05-28 09:58:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-05-28 09:58:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 84.75%
[2024-05-28 09:58:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][0/2502]	eta 16:56:16 lr 0.000008	 wd 0.0000	time 24.3712 (24.3712)	loss 1.3485 (1.3485)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 09:59:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][100/2502]	eta 0:31:17 lr 0.000008	 wd 0.0000	time 0.5169 (0.7816)	loss 1.3858 (1.3166)	grad_norm 4.0594 (4.5768)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:00:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][200/2502]	eta 0:25:27 lr 0.000009	 wd 0.0000	time 0.5241 (0.6635)	loss 1.4904 (1.3127)	grad_norm 13.3831 (4.8527)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:01:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][300/2502]	eta 0:22:54 lr 0.000009	 wd 0.0000	time 0.5392 (0.6243)	loss 1.5503 (1.3175)	grad_norm 1.9824 (4.5629)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:02:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][400/2502]	eta 0:21:09 lr 0.000009	 wd 0.0000	time 0.5126 (0.6038)	loss 1.1108 (1.3121)	grad_norm 4.1108 (4.5425)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:03:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][500/2502]	eta 0:19:44 lr 0.000010	 wd 0.0000	time 0.5151 (0.5918)	loss 1.6024 (1.3131)	grad_norm 3.0828 (inf)	loss_scale 512.0000 (956.5509)	mem 20118MB
[2024-05-28 10:03:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][600/2502]	eta 0:18:30 lr 0.000010	 wd 0.0000	time 0.5286 (0.5838)	loss 1.2156 (1.3144)	grad_norm 6.2881 (inf)	loss_scale 512.0000 (882.5824)	mem 20118MB
[2024-05-28 10:04:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][700/2502]	eta 0:17:39 lr 0.000010	 wd 0.0000	time 0.5560 (0.5878)	loss 1.2704 (1.3147)	grad_norm 4.9081 (inf)	loss_scale 512.0000 (829.7175)	mem 20118MB
[2024-05-28 10:06:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][800/2502]	eta 0:17:29 lr 0.000011	 wd 0.0000	time 2.3979 (0.6169)	loss 1.2614 (1.3097)	grad_norm 3.0140 (inf)	loss_scale 512.0000 (790.0524)	mem 20118MB
[2024-05-28 10:07:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][900/2502]	eta 0:17:08 lr 0.000011	 wd 0.0000	time 0.5154 (0.6417)	loss 1.0675 (1.3097)	grad_norm 5.5292 (inf)	loss_scale 512.0000 (759.1920)	mem 20118MB
[2024-05-28 10:08:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1000/2502]	eta 0:15:48 lr 0.000011	 wd 0.0000	time 0.5211 (0.6317)	loss 0.9438 (1.3069)	grad_norm 3.6034 (inf)	loss_scale 512.0000 (734.4975)	mem 20118MB
[2024-05-28 10:09:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1100/2502]	eta 0:14:34 lr 0.000012	 wd 0.0000	time 0.5348 (0.6239)	loss 1.4720 (1.3080)	grad_norm 4.6215 (inf)	loss_scale 512.0000 (714.2888)	mem 20118MB
[2024-05-28 10:10:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1200/2502]	eta 0:13:23 lr 0.000012	 wd 0.0000	time 0.5272 (0.6172)	loss 1.1071 (1.3059)	grad_norm 4.3061 (inf)	loss_scale 512.0000 (697.4455)	mem 20118MB
[2024-05-28 10:11:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1300/2502]	eta 0:12:15 lr 0.000012	 wd 0.0000	time 0.5092 (0.6116)	loss 1.3214 (1.3012)	grad_norm 4.2836 (inf)	loss_scale 512.0000 (683.1914)	mem 20118MB
[2024-05-28 10:12:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1400/2502]	eta 0:11:08 lr 0.000012	 wd 0.0000	time 0.5255 (0.6068)	loss 1.3612 (1.3011)	grad_norm 4.9334 (inf)	loss_scale 512.0000 (670.9722)	mem 20118MB
[2024-05-28 10:13:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1500/2502]	eta 0:10:19 lr 0.000013	 wd 0.0000	time 0.5362 (0.6185)	loss 1.5029 (1.2996)	grad_norm 3.4836 (inf)	loss_scale 512.0000 (660.3811)	mem 20118MB
[2024-05-28 10:14:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1600/2502]	eta 0:09:30 lr 0.000013	 wd 0.0000	time 1.0113 (0.6329)	loss 1.1657 (1.2997)	grad_norm 3.8885 (inf)	loss_scale 512.0000 (651.1131)	mem 20118MB
[2024-05-28 10:16:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1700/2502]	eta 0:08:31 lr 0.000013	 wd 0.0000	time 0.5340 (0.6379)	loss 1.3373 (1.2998)	grad_norm 3.5440 (inf)	loss_scale 512.0000 (642.9347)	mem 20118MB
[2024-05-28 10:17:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1800/2502]	eta 0:07:24 lr 0.000014	 wd 0.0000	time 0.5141 (0.6326)	loss 0.9253 (1.2979)	grad_norm 3.5066 (inf)	loss_scale 512.0000 (635.6646)	mem 20118MB
[2024-05-28 10:17:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][1900/2502]	eta 0:06:18 lr 0.000014	 wd 0.0000	time 0.5109 (0.6279)	loss 1.0243 (1.2994)	grad_norm 5.1773 (inf)	loss_scale 512.0000 (629.1594)	mem 20118MB
[2024-05-28 10:18:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][2000/2502]	eta 0:05:13 lr 0.000014	 wd 0.0000	time 0.5159 (0.6237)	loss 1.4089 (1.2982)	grad_norm 3.3927 (inf)	loss_scale 512.0000 (623.3043)	mem 20118MB
[2024-05-28 10:19:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][2100/2502]	eta 0:04:09 lr 0.000015	 wd 0.0000	time 0.5105 (0.6198)	loss 1.0845 (1.2973)	grad_norm 5.7928 (inf)	loss_scale 512.0000 (618.0067)	mem 20118MB
[2024-05-28 10:20:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][2200/2502]	eta 0:03:06 lr 0.000015	 wd 0.0000	time 0.5118 (0.6163)	loss 1.2609 (1.2980)	grad_norm 5.5339 (inf)	loss_scale 512.0000 (613.1904)	mem 20118MB
[2024-05-28 10:21:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][2300/2502]	eta 0:02:04 lr 0.000015	 wd 0.0000	time 0.5071 (0.6186)	loss 1.5511 (1.2985)	grad_norm 3.5326 (inf)	loss_scale 512.0000 (608.7927)	mem 20118MB
[2024-05-28 10:22:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][2400/2502]	eta 0:01:03 lr 0.000016	 wd 0.0000	time 0.5542 (0.6220)	loss 1.0414 (1.2993)	grad_norm 3.3989 (inf)	loss_scale 512.0000 (604.7613)	mem 20118MB
[2024-05-28 10:23:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0000	time 0.5288 (0.6196)	loss 1.4268 (1.2988)	grad_norm 3.3787 (inf)	loss_scale 512.0000 (601.0524)	mem 20118MB
[2024-05-28 10:24:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 1 training takes 0:25:55
[2024-05-28 10:25:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 73.844 (73.844)	Loss 0.3931 (0.3931)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 10:25:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.210 Acc@5 97.632
[2024-05-28 10:25:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-05-28 10:25:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.21%
[2024-05-28 10:26:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][0/2502]	eta 1 day, 2:39:20 lr 0.000016	 wd 0.0000	time 38.3534 (38.3534)	loss 1.4737 (1.4737)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:27:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][100/2502]	eta 0:36:48 lr 0.000016	 wd 0.0000	time 0.5389 (0.9194)	loss 1.3746 (1.2878)	grad_norm 4.5010 (4.1564)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:27:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][200/2502]	eta 0:28:07 lr 0.000017	 wd 0.0000	time 0.5126 (0.7330)	loss 1.4977 (1.3046)	grad_norm 4.2997 (4.2997)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:28:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][300/2502]	eta 0:24:35 lr 0.000017	 wd 0.0000	time 0.5497 (0.6702)	loss 0.8151 (1.3025)	grad_norm 6.8347 (4.3585)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:29:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][400/2502]	eta 0:22:23 lr 0.000017	 wd 0.0000	time 0.5098 (0.6391)	loss 1.4060 (1.2969)	grad_norm 3.7902 (4.5298)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:30:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][500/2502]	eta 0:20:42 lr 0.000018	 wd 0.0000	time 0.5291 (0.6204)	loss 0.9992 (1.2977)	grad_norm 3.3586 (4.4661)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:31:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][600/2502]	eta 0:19:15 lr 0.000018	 wd 0.0000	time 0.5215 (0.6078)	loss 1.4299 (1.2973)	grad_norm 5.3460 (4.4501)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:32:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][700/2502]	eta 0:18:19 lr 0.000018	 wd 0.0000	time 0.5820 (0.6100)	loss 1.4773 (1.2985)	grad_norm 5.6182 (4.4310)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:33:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][800/2502]	eta 0:17:14 lr 0.000019	 wd 0.0000	time 0.5608 (0.6079)	loss 1.3666 (1.2980)	grad_norm 4.7907 (4.4401)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:34:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][900/2502]	eta 0:16:48 lr 0.000019	 wd 0.0000	time 0.5112 (0.6296)	loss 1.2856 (1.2976)	grad_norm 5.5753 (4.4711)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:36:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1000/2502]	eta 0:16:05 lr 0.000019	 wd 0.0000	time 0.5229 (0.6426)	loss 1.2377 (1.3001)	grad_norm 3.2135 (4.4357)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:37:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1100/2502]	eta 0:14:54 lr 0.000020	 wd 0.0000	time 0.5280 (0.6382)	loss 1.2722 (1.2982)	grad_norm 3.4932 (4.4133)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:38:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1200/2502]	eta 0:13:52 lr 0.000020	 wd 0.0000	time 0.5136 (0.6395)	loss 1.4430 (1.2978)	grad_norm 7.3025 (4.3932)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:39:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1300/2502]	eta 0:12:50 lr 0.000020	 wd 0.0000	time 0.5984 (0.6406)	loss 1.3810 (1.2990)	grad_norm 4.5971 (4.4120)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:40:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1400/2502]	eta 0:11:41 lr 0.000020	 wd 0.0000	time 0.5302 (0.6370)	loss 1.2618 (1.2977)	grad_norm 3.8587 (4.3765)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:42:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1500/2502]	eta 0:11:19 lr 0.000021	 wd 0.0000	time 0.5263 (0.6782)	loss 1.2686 (1.2975)	grad_norm 4.5618 (4.3596)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:43:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1600/2502]	eta 0:10:04 lr 0.000021	 wd 0.0000	time 0.5305 (0.6697)	loss 1.1810 (1.2991)	grad_norm 3.9384 (4.3692)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:44:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1700/2502]	eta 0:08:51 lr 0.000021	 wd 0.0000	time 0.5353 (0.6623)	loss 1.5611 (1.2968)	grad_norm 3.0055 (4.3677)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:45:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1800/2502]	eta 0:07:40 lr 0.000022	 wd 0.0000	time 0.5106 (0.6558)	loss 1.0298 (1.2945)	grad_norm 3.6123 (4.3550)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:46:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][1900/2502]	eta 0:06:31 lr 0.000022	 wd 0.0000	time 0.5465 (0.6498)	loss 1.4441 (1.2947)	grad_norm 3.0582 (4.3472)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 10:47:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][2000/2502]	eta 0:05:23 lr 0.000022	 wd 0.0000	time 0.5327 (0.6445)	loss 1.4016 (1.2936)	grad_norm 3.5080 (4.3315)	loss_scale 1024.0000 (529.3993)	mem 20118MB
[2024-05-28 10:48:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][2100/2502]	eta 0:04:19 lr 0.000023	 wd 0.0000	time 0.6599 (0.6448)	loss 1.3440 (1.2932)	grad_norm 3.7560 (4.3370)	loss_scale 1024.0000 (552.9405)	mem 20118MB
[2024-05-28 10:49:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][2200/2502]	eta 0:03:17 lr 0.000023	 wd 0.0000	time 0.6327 (0.6541)	loss 1.3292 (1.2934)	grad_norm 8.2191 (4.3643)	loss_scale 1024.0000 (574.3426)	mem 20118MB
[2024-05-28 10:50:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][2300/2502]	eta 0:02:11 lr 0.000023	 wd 0.0000	time 1.1962 (0.6519)	loss 1.2024 (1.2930)	grad_norm 3.1769 (4.3861)	loss_scale 1024.0000 (593.8844)	mem 20118MB
[2024-05-28 10:51:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][2400/2502]	eta 0:01:06 lr 0.000024	 wd 0.0000	time 0.5782 (0.6512)	loss 1.1001 (1.2924)	grad_norm 3.3579 (4.3849)	loss_scale 1024.0000 (611.7984)	mem 20118MB
[2024-05-28 10:52:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000024	 wd 0.0000	time 0.5159 (0.6473)	loss 1.4754 (1.2928)	grad_norm 5.2895 (4.3965)	loss_scale 1024.0000 (628.2799)	mem 20118MB
[2024-05-28 10:52:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 2 training takes 0:27:20
[2024-05-28 10:54:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 69.385 (69.385)	Loss 0.3953 (0.3953)	Acc@1 91.797 (91.797)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 10:54:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.292 Acc@5 97.706
[2024-05-28 10:54:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-05-28 10:54:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.29%
[2024-05-28 10:54:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][0/2502]	eta 15:12:30 lr 0.000024	 wd 0.0000	time 21.8825 (21.8825)	loss 1.4829 (1.4829)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:55:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][100/2502]	eta 0:30:21 lr 0.000024	 wd 0.0000	time 0.5292 (0.7582)	loss 1.5730 (1.2659)	grad_norm 4.1564 (4.2262)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:56:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][200/2502]	eta 0:24:58 lr 0.000025	 wd 0.0000	time 0.5223 (0.6510)	loss 1.2409 (1.2678)	grad_norm 4.5986 (4.1385)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:57:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][300/2502]	eta 0:22:35 lr 0.000025	 wd 0.0000	time 0.5464 (0.6157)	loss 1.3626 (1.2851)	grad_norm 2.5406 (4.2468)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:58:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][400/2502]	eta 0:20:56 lr 0.000025	 wd 0.0000	time 0.5223 (0.5977)	loss 0.8785 (1.2848)	grad_norm 3.1341 (4.2188)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 10:59:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][500/2502]	eta 0:21:02 lr 0.000026	 wd 0.0000	time 0.5992 (0.6306)	loss 1.3007 (1.2840)	grad_norm 3.6736 (4.2079)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:00:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][600/2502]	eta 0:20:13 lr 0.000026	 wd 0.0000	time 0.5502 (0.6380)	loss 1.4582 (1.2861)	grad_norm 3.7898 (4.2226)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:01:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][700/2502]	eta 0:18:45 lr 0.000026	 wd 0.0000	time 0.5247 (0.6243)	loss 1.0605 (1.2890)	grad_norm 4.5079 (4.2229)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:02:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][800/2502]	eta 0:17:25 lr 0.000027	 wd 0.0000	time 0.5522 (0.6143)	loss 1.1841 (1.2906)	grad_norm 4.3322 (4.2225)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:03:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][900/2502]	eta 0:16:11 lr 0.000027	 wd 0.0000	time 0.5082 (0.6064)	loss 0.9932 (1.2888)	grad_norm 4.8055 (4.2847)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:04:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1000/2502]	eta 0:15:17 lr 0.000027	 wd 0.0000	time 0.5493 (0.6106)	loss 1.3616 (1.2893)	grad_norm 6.2902 (4.3703)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:05:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1100/2502]	eta 0:14:29 lr 0.000028	 wd 0.0000	time 0.5809 (0.6203)	loss 1.2240 (1.2904)	grad_norm 3.3156 (4.3748)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:06:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1200/2502]	eta 0:13:26 lr 0.000028	 wd 0.0000	time 0.5698 (0.6196)	loss 1.2487 (1.2883)	grad_norm 3.0427 (4.3673)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:07:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1300/2502]	eta 0:12:31 lr 0.000028	 wd 0.0000	time 0.6633 (0.6250)	loss 1.3012 (1.2889)	grad_norm 7.5325 (4.3720)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:09:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1400/2502]	eta 0:11:33 lr 0.000028	 wd 0.0000	time 0.5963 (0.6293)	loss 1.2424 (1.2896)	grad_norm 3.9972 (4.3927)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:10:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1500/2502]	eta 0:10:34 lr 0.000029	 wd 0.0000	time 0.5153 (0.6337)	loss 1.1280 (1.2877)	grad_norm 2.7687 (4.3987)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:11:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1600/2502]	eta 0:09:32 lr 0.000029	 wd 0.0000	time 0.5552 (0.6352)	loss 1.3082 (1.2883)	grad_norm 3.1823 (4.4199)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:12:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1700/2502]	eta 0:08:30 lr 0.000029	 wd 0.0000	time 0.8985 (0.6369)	loss 1.0068 (1.2854)	grad_norm 5.4995 (4.4123)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:13:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1800/2502]	eta 0:07:25 lr 0.000030	 wd 0.0000	time 0.5211 (0.6345)	loss 1.4039 (1.2848)	grad_norm 6.2598 (4.4128)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:14:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][1900/2502]	eta 0:06:23 lr 0.000030	 wd 0.0000	time 0.6274 (0.6368)	loss 0.8821 (1.2839)	grad_norm 4.3129 (4.4131)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:15:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][2000/2502]	eta 0:05:20 lr 0.000030	 wd 0.0000	time 0.5842 (0.6377)	loss 0.9599 (1.2838)	grad_norm 3.9689 (4.4123)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:16:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][2100/2502]	eta 0:04:15 lr 0.000031	 wd 0.0000	time 0.5094 (0.6355)	loss 1.4198 (1.2840)	grad_norm 2.7757 (4.4375)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:17:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][2200/2502]	eta 0:03:11 lr 0.000031	 wd 0.0000	time 0.5954 (0.6356)	loss 1.3878 (1.2846)	grad_norm 4.6031 (4.4201)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:18:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][2300/2502]	eta 0:02:08 lr 0.000031	 wd 0.0000	time 0.5223 (0.6337)	loss 1.1497 (1.2847)	grad_norm 6.0981 (4.4135)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:19:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][2400/2502]	eta 0:01:04 lr 0.000032	 wd 0.0000	time 0.5825 (0.6365)	loss 1.2476 (1.2839)	grad_norm 2.8272 (4.3946)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:20:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000032	 wd 0.0000	time 0.5127 (0.6330)	loss 1.0667 (1.2845)	grad_norm 5.6853 (4.3927)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:20:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 3 training takes 0:26:36
[2024-05-28 11:22:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 66.984 (66.984)	Loss 0.4072 (0.4072)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 11:22:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.314 Acc@5 97.734
[2024-05-28 11:22:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-05-28 11:22:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.31%
[2024-05-28 11:22:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][0/2502]	eta 19:17:27 lr 0.000032	 wd 0.0000	time 27.7569 (27.7569)	loss 1.1062 (1.1062)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:23:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][100/2502]	eta 0:33:11 lr 0.000032	 wd 0.0000	time 0.5073 (0.8289)	loss 1.0824 (1.2943)	grad_norm 4.6524 (4.9671)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:24:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][200/2502]	eta 0:26:21 lr 0.000033	 wd 0.0000	time 0.5319 (0.6870)	loss 1.3733 (1.2804)	grad_norm 3.7992 (5.0949)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:25:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][300/2502]	eta 0:23:28 lr 0.000033	 wd 0.0000	time 0.5423 (0.6394)	loss 1.3017 (1.2818)	grad_norm 4.5143 (4.9875)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:26:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][400/2502]	eta 0:21:33 lr 0.000033	 wd 0.0000	time 0.5261 (0.6155)	loss 1.1555 (1.2796)	grad_norm 4.3955 (4.7320)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:27:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][500/2502]	eta 0:20:32 lr 0.000034	 wd 0.0000	time 0.5391 (0.6156)	loss 1.4854 (1.2724)	grad_norm 5.6636 (4.7386)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:28:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][600/2502]	eta 0:19:25 lr 0.000034	 wd 0.0000	time 0.6110 (0.6126)	loss 1.3708 (1.2776)	grad_norm 4.0646 (4.6703)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:29:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][700/2502]	eta 0:19:22 lr 0.000034	 wd 0.0000	time 0.5371 (0.6450)	loss 1.3144 (1.2751)	grad_norm 3.9932 (4.6650)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:31:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][800/2502]	eta 0:18:29 lr 0.000035	 wd 0.0000	time 0.6382 (0.6521)	loss 1.4320 (1.2788)	grad_norm 3.0919 (4.8971)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:32:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][900/2502]	eta 0:17:45 lr 0.000035	 wd 0.0000	time 0.7780 (0.6651)	loss 1.1936 (1.2819)	grad_norm 3.9727 (4.8381)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:33:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1000/2502]	eta 0:16:29 lr 0.000035	 wd 0.0000	time 0.5371 (0.6585)	loss 1.3842 (1.2807)	grad_norm 3.3986 (4.7679)	loss_scale 2048.0000 (1097.6543)	mem 20118MB
[2024-05-28 11:34:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1100/2502]	eta 0:15:20 lr 0.000036	 wd 0.0000	time 0.7051 (0.6567)	loss 1.2448 (1.2777)	grad_norm 3.1369 (4.7434)	loss_scale 2048.0000 (1183.9709)	mem 20118MB
[2024-05-28 11:35:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1200/2502]	eta 0:14:15 lr 0.000036	 wd 0.0000	time 0.5591 (0.6573)	loss 0.7165 (1.2753)	grad_norm 5.4678 (4.7144)	loss_scale 2048.0000 (1255.9134)	mem 20118MB
[2024-05-28 11:36:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1300/2502]	eta 0:13:04 lr 0.000036	 wd 0.0000	time 0.5149 (0.6529)	loss 1.6164 (1.2781)	grad_norm 3.5355 (4.6703)	loss_scale 2048.0000 (1316.7963)	mem 20118MB
[2024-05-28 11:38:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1400/2502]	eta 0:12:22 lr 0.000036	 wd 0.0000	time 0.6577 (0.6736)	loss 1.4279 (1.2785)	grad_norm 7.4234 (4.6947)	loss_scale 2048.0000 (1368.9879)	mem 20118MB
[2024-05-28 11:39:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1500/2502]	eta 0:11:15 lr 0.000037	 wd 0.0000	time 0.5286 (0.6738)	loss 1.1906 (1.2778)	grad_norm 4.1805 (inf)	loss_scale 1024.0000 (1355.5550)	mem 20118MB
[2024-05-28 11:40:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1600/2502]	eta 0:10:00 lr 0.000037	 wd 0.0000	time 0.5264 (0.6656)	loss 1.4318 (1.2774)	grad_norm 4.4212 (inf)	loss_scale 1024.0000 (1334.8457)	mem 20118MB
[2024-05-28 11:41:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1700/2502]	eta 0:08:48 lr 0.000037	 wd 0.0000	time 0.5234 (0.6584)	loss 1.5587 (1.2787)	grad_norm 2.9823 (inf)	loss_scale 1024.0000 (1316.5714)	mem 20118MB
[2024-05-28 11:42:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1800/2502]	eta 0:07:37 lr 0.000038	 wd 0.0000	time 0.5676 (0.6522)	loss 1.1688 (1.2773)	grad_norm 3.5993 (inf)	loss_scale 1024.0000 (1300.3265)	mem 20118MB
[2024-05-28 11:42:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][1900/2502]	eta 0:06:29 lr 0.000038	 wd 0.0000	time 0.5324 (0.6465)	loss 1.3177 (1.2780)	grad_norm 3.0993 (inf)	loss_scale 1024.0000 (1285.7906)	mem 20118MB
[2024-05-28 11:43:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][2000/2502]	eta 0:05:23 lr 0.000038	 wd 0.0000	time 0.6122 (0.6437)	loss 1.1783 (1.2777)	grad_norm 3.2476 (inf)	loss_scale 1024.0000 (1272.7076)	mem 20118MB
[2024-05-28 11:44:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][2100/2502]	eta 0:04:17 lr 0.000039	 wd 0.0000	time 0.5926 (0.6406)	loss 1.3178 (1.2769)	grad_norm 5.7804 (inf)	loss_scale 1024.0000 (1260.8701)	mem 20118MB
[2024-05-28 11:45:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][2200/2502]	eta 0:03:13 lr 0.000039	 wd 0.0000	time 0.5286 (0.6403)	loss 1.2416 (1.2777)	grad_norm 7.9062 (inf)	loss_scale 1024.0000 (1250.1081)	mem 20118MB
[2024-05-28 11:47:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][2300/2502]	eta 0:02:09 lr 0.000039	 wd 0.0000	time 0.6017 (0.6417)	loss 1.3740 (1.2776)	grad_norm 3.4960 (inf)	loss_scale 1024.0000 (1240.2816)	mem 20118MB
[2024-05-28 11:48:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][2400/2502]	eta 0:01:06 lr 0.000040	 wd 0.0000	time 0.5242 (0.6546)	loss 1.3034 (1.2782)	grad_norm 2.7001 (inf)	loss_scale 1024.0000 (1231.2736)	mem 20118MB
[2024-05-28 11:49:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000040	 wd 0.0000	time 0.5143 (0.6501)	loss 1.5239 (1.2765)	grad_norm 3.5306 (inf)	loss_scale 1024.0000 (1222.9860)	mem 20118MB
[2024-05-28 11:49:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 4 training takes 0:27:10
[2024-05-28 11:50:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 41.368 (41.368)	Loss 0.3906 (0.3906)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 11:50:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.490 Acc@5 97.782
[2024-05-28 11:50:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-05-28 11:50:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.49%
[2024-05-28 11:50:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][0/2502]	eta 15:29:58 lr 0.000040	 wd 0.0000	time 22.3015 (22.3015)	loss 1.3357 (1.3357)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:51:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][100/2502]	eta 0:30:22 lr 0.000040	 wd 0.0000	time 0.5364 (0.7588)	loss 1.5845 (1.2521)	grad_norm 5.7056 (4.2162)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:52:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][200/2502]	eta 0:25:00 lr 0.000040	 wd 0.0000	time 0.5491 (0.6516)	loss 1.0087 (1.2721)	grad_norm 4.0510 (4.2824)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:53:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][300/2502]	eta 0:22:36 lr 0.000040	 wd 0.0000	time 0.5307 (0.6161)	loss 1.4250 (1.2695)	grad_norm 6.5782 (4.3425)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:54:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][400/2502]	eta 0:20:57 lr 0.000040	 wd 0.0000	time 0.5121 (0.5981)	loss 1.3154 (1.2711)	grad_norm 5.2840 (4.4498)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:55:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][500/2502]	eta 0:20:08 lr 0.000040	 wd 0.0000	time 0.5182 (0.6037)	loss 1.3372 (1.2696)	grad_norm 2.9693 (4.4057)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:56:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][600/2502]	eta 0:19:03 lr 0.000040	 wd 0.0000	time 0.5236 (0.6010)	loss 0.9379 (1.2688)	grad_norm 3.0814 (4.3404)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:57:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][700/2502]	eta 0:18:23 lr 0.000040	 wd 0.0000	time 0.5328 (0.6122)	loss 1.4100 (1.2721)	grad_norm 4.9912 (4.3527)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:58:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][800/2502]	eta 0:17:24 lr 0.000040	 wd 0.0000	time 0.5536 (0.6136)	loss 1.1026 (1.2759)	grad_norm 6.1277 (4.4029)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 11:59:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][900/2502]	eta 0:16:20 lr 0.000040	 wd 0.0000	time 0.5339 (0.6119)	loss 1.3496 (1.2751)	grad_norm 3.7503 (4.3766)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:00:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1000/2502]	eta 0:15:28 lr 0.000040	 wd 0.0000	time 0.5912 (0.6182)	loss 1.3893 (1.2747)	grad_norm 3.2607 (4.3298)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:01:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1100/2502]	eta 0:14:32 lr 0.000040	 wd 0.0000	time 0.5986 (0.6220)	loss 0.9586 (1.2768)	grad_norm 3.4843 (4.2963)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:02:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1200/2502]	eta 0:13:27 lr 0.000040	 wd 0.0000	time 0.5727 (0.6204)	loss 1.3233 (1.2787)	grad_norm 3.0649 (4.2794)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:04:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1300/2502]	eta 0:12:39 lr 0.000040	 wd 0.0000	time 0.5989 (0.6317)	loss 1.2276 (1.2770)	grad_norm 3.9816 (4.2789)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:05:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1400/2502]	eta 0:11:38 lr 0.000040	 wd 0.0000	time 0.6419 (0.6334)	loss 1.0845 (1.2752)	grad_norm 3.1875 (4.3024)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:06:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1500/2502]	eta 0:10:33 lr 0.000040	 wd 0.0000	time 0.5557 (0.6325)	loss 1.4442 (1.2767)	grad_norm 4.7211 (4.3040)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:07:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1600/2502]	eta 0:09:39 lr 0.000040	 wd 0.0000	time 0.9578 (0.6423)	loss 0.9963 (1.2781)	grad_norm 3.6951 (4.3384)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:08:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1700/2502]	eta 0:08:36 lr 0.000040	 wd 0.0000	time 0.6183 (0.6438)	loss 1.3271 (1.2785)	grad_norm 2.9573 (4.3253)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:09:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1800/2502]	eta 0:07:29 lr 0.000040	 wd 0.0000	time 0.5250 (0.6408)	loss 1.4243 (1.2783)	grad_norm 5.2446 (4.3339)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:11:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][1900/2502]	eta 0:06:30 lr 0.000040	 wd 0.0000	time 0.5146 (0.6484)	loss 1.3146 (1.2779)	grad_norm 3.9092 (4.3084)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:12:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][2000/2502]	eta 0:05:25 lr 0.000040	 wd 0.0000	time 0.5239 (0.6489)	loss 1.3700 (1.2772)	grad_norm 3.0019 (4.2922)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:13:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][2100/2502]	eta 0:04:19 lr 0.000040	 wd 0.0000	time 0.5789 (0.6460)	loss 1.0796 (1.2767)	grad_norm 3.2534 (4.2924)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:14:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][2200/2502]	eta 0:03:16 lr 0.000040	 wd 0.0000	time 0.5644 (0.6509)	loss 1.1900 (1.2783)	grad_norm 2.8329 (4.3240)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:15:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][2300/2502]	eta 0:02:11 lr 0.000040	 wd 0.0000	time 0.5087 (0.6506)	loss 1.4487 (1.2774)	grad_norm 3.2575 (4.3169)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:16:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][2400/2502]	eta 0:01:06 lr 0.000040	 wd 0.0000	time 0.5109 (0.6479)	loss 1.2234 (1.2769)	grad_norm 3.5112 (4.3202)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:17:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000040	 wd 0.0000	time 0.5130 (0.6547)	loss 1.2577 (1.2767)	grad_norm 3.2441 (4.3182)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:17:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 5 training takes 0:27:26
[2024-05-28 12:18:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 48.801 (48.801)	Loss 0.3826 (0.3826)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 20118MB
[2024-05-28 12:19:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.566 Acc@5 97.810
[2024-05-28 12:19:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-05-28 12:19:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.57%
[2024-05-28 12:19:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][0/2502]	eta 15:38:45 lr 0.000040	 wd 0.0000	time 22.5122 (22.5122)	loss 0.8008 (0.8008)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:20:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][100/2502]	eta 0:30:28 lr 0.000040	 wd 0.0000	time 0.5453 (0.7612)	loss 1.5757 (1.2465)	grad_norm 3.3878 (4.6664)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:21:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][200/2502]	eta 0:25:02 lr 0.000040	 wd 0.0000	time 0.5310 (0.6526)	loss 1.3440 (1.2610)	grad_norm 2.4701 (4.6059)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:22:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][300/2502]	eta 0:22:37 lr 0.000040	 wd 0.0000	time 0.5475 (0.6167)	loss 1.3679 (1.2712)	grad_norm 5.7036 (4.7395)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:23:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][400/2502]	eta 0:20:58 lr 0.000040	 wd 0.0000	time 0.5268 (0.5987)	loss 1.2220 (1.2696)	grad_norm 6.8751 (4.7105)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 12:23:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][500/2502]	eta 0:19:37 lr 0.000040	 wd 0.0000	time 0.5165 (0.5881)	loss 1.3230 (1.2726)	grad_norm 2.9985 (4.5575)	loss_scale 2048.0000 (1207.9521)	mem 20118MB
[2024-05-28 12:24:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][600/2502]	eta 0:18:24 lr 0.000040	 wd 0.0000	time 0.5138 (0.5808)	loss 1.4463 (1.2770)	grad_norm 3.5813 (4.5443)	loss_scale 2048.0000 (1347.7271)	mem 20118MB
[2024-05-28 12:26:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][700/2502]	eta 0:19:42 lr 0.000040	 wd 0.0000	time 0.5134 (0.6564)	loss 1.1053 (1.2766)	grad_norm 4.1277 (4.5335)	loss_scale 2048.0000 (1447.6234)	mem 20118MB
[2024-05-28 12:27:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][800/2502]	eta 0:18:14 lr 0.000040	 wd 0.0000	time 0.5496 (0.6430)	loss 0.9092 (1.2776)	grad_norm 4.6041 (4.5246)	loss_scale 2048.0000 (1522.5768)	mem 20118MB
[2024-05-28 12:28:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][900/2502]	eta 0:16:52 lr 0.000040	 wd 0.0000	time 0.5177 (0.6322)	loss 0.9456 (1.2761)	grad_norm 5.0548 (4.5117)	loss_scale 2048.0000 (1580.8923)	mem 20118MB
[2024-05-28 12:29:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1000/2502]	eta 0:15:36 lr 0.000040	 wd 0.0000	time 0.5146 (0.6234)	loss 1.1984 (1.2787)	grad_norm 3.1890 (4.4735)	loss_scale 2048.0000 (1627.5564)	mem 20118MB
[2024-05-28 12:30:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1100/2502]	eta 0:14:23 lr 0.000040	 wd 0.0000	time 0.5151 (0.6160)	loss 0.9322 (1.2783)	grad_norm 4.7999 (4.4326)	loss_scale 2048.0000 (1665.7439)	mem 20118MB
[2024-05-28 12:31:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1200/2502]	eta 0:13:14 lr 0.000040	 wd 0.0000	time 0.5559 (0.6101)	loss 1.2915 (1.2753)	grad_norm 2.6462 (4.4129)	loss_scale 2048.0000 (1697.5720)	mem 20118MB
[2024-05-28 12:32:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1300/2502]	eta 0:12:14 lr 0.000040	 wd 0.0000	time 0.5343 (0.6112)	loss 1.5026 (1.2745)	grad_norm 4.1325 (4.4179)	loss_scale 2048.0000 (1724.5073)	mem 20118MB
[2024-05-28 12:33:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1400/2502]	eta 0:11:14 lr 0.000040	 wd 0.0000	time 0.6987 (0.6122)	loss 1.5130 (1.2755)	grad_norm 3.7239 (4.4119)	loss_scale 2048.0000 (1747.5974)	mem 20118MB
[2024-05-28 12:34:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1500/2502]	eta 0:10:13 lr 0.000040	 wd 0.0000	time 0.5179 (0.6118)	loss 1.6366 (1.2756)	grad_norm 3.7337 (4.3737)	loss_scale 2048.0000 (1767.6109)	mem 20118MB
[2024-05-28 12:35:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1600/2502]	eta 0:09:22 lr 0.000040	 wd 0.0000	time 0.5819 (0.6240)	loss 1.3794 (1.2750)	grad_norm 4.1403 (4.3622)	loss_scale 2048.0000 (1785.1243)	mem 20118MB
[2024-05-28 12:36:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1700/2502]	eta 0:08:20 lr 0.000040	 wd 0.0000	time 0.7466 (0.6241)	loss 0.8285 (1.2751)	grad_norm 6.5227 (4.3490)	loss_scale 2048.0000 (1800.5785)	mem 20118MB
[2024-05-28 12:37:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1800/2502]	eta 0:07:18 lr 0.000040	 wd 0.0000	time 0.5115 (0.6246)	loss 1.3507 (1.2750)	grad_norm 3.3129 (4.3328)	loss_scale 2048.0000 (1814.3165)	mem 20118MB
[2024-05-28 12:38:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][1900/2502]	eta 0:06:16 lr 0.000040	 wd 0.0000	time 0.5877 (0.6257)	loss 1.2514 (1.2744)	grad_norm 3.7069 (4.3345)	loss_scale 2048.0000 (1826.6092)	mem 20118MB
[2024-05-28 12:39:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][2000/2502]	eta 0:05:13 lr 0.000039	 wd 0.0000	time 0.5710 (0.6240)	loss 1.4301 (1.2735)	grad_norm 7.5770 (4.3383)	loss_scale 2048.0000 (1837.6732)	mem 20118MB
[2024-05-28 12:41:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][2100/2502]	eta 0:04:12 lr 0.000039	 wd 0.0000	time 0.5215 (0.6290)	loss 0.9813 (1.2744)	grad_norm 3.3216 (4.3252)	loss_scale 2048.0000 (1847.6840)	mem 20118MB
[2024-05-28 12:42:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][2200/2502]	eta 0:03:10 lr 0.000039	 wd 0.0000	time 0.5098 (0.6293)	loss 1.3160 (1.2741)	grad_norm 5.4755 (4.3229)	loss_scale 2048.0000 (1856.7851)	mem 20118MB
[2024-05-28 12:43:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][2300/2502]	eta 0:02:06 lr 0.000039	 wd 0.0000	time 0.5583 (0.6275)	loss 1.4408 (1.2739)	grad_norm 3.7584 (4.3091)	loss_scale 2048.0000 (1865.0952)	mem 20118MB
[2024-05-28 12:44:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][2400/2502]	eta 0:01:04 lr 0.000039	 wd 0.0000	time 0.5689 (0.6284)	loss 1.3012 (1.2718)	grad_norm 2.6715 (4.2978)	loss_scale 2048.0000 (1872.7130)	mem 20118MB
[2024-05-28 12:45:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000039	 wd 0.0000	time 0.5144 (0.6258)	loss 1.3141 (1.2705)	grad_norm 4.4077 (4.2865)	loss_scale 2048.0000 (1879.7217)	mem 20118MB
[2024-05-28 12:45:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 6 training takes 0:26:22
[2024-05-28 12:46:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 73.480 (73.480)	Loss 0.3904 (0.3904)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 20118MB
[2024-05-28 12:46:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.704 Acc@5 97.768
[2024-05-28 12:46:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-05-28 12:46:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.70%
[2024-05-28 12:47:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][0/2502]	eta 16:38:28 lr 0.000039	 wd 0.0000	time 23.9441 (23.9441)	loss 1.0632 (1.0632)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 12:48:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][100/2502]	eta 0:31:06 lr 0.000039	 wd 0.0000	time 0.5351 (0.7770)	loss 1.2388 (1.2508)	grad_norm 5.2807 (4.2536)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 12:49:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][200/2502]	eta 0:25:21 lr 0.000039	 wd 0.0000	time 0.5288 (0.6612)	loss 1.4536 (1.2884)	grad_norm 3.2752 (4.5383)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 12:50:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][300/2502]	eta 0:22:51 lr 0.000039	 wd 0.0000	time 0.5285 (0.6230)	loss 1.3213 (1.2823)	grad_norm 3.6509 (4.4124)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 12:50:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][400/2502]	eta 0:21:08 lr 0.000039	 wd 0.0000	time 0.5141 (0.6034)	loss 1.3764 (1.2763)	grad_norm inf (inf)	loss_scale 1024.0000 (2042.8928)	mem 20118MB
[2024-05-28 12:52:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][500/2502]	eta 0:20:23 lr 0.000039	 wd 0.0000	time 0.5131 (0.6111)	loss 1.3953 (1.2750)	grad_norm 4.3128 (inf)	loss_scale 1024.0000 (1839.5210)	mem 20118MB
[2024-05-28 12:53:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][600/2502]	eta 0:19:19 lr 0.000039	 wd 0.0000	time 0.5805 (0.6099)	loss 1.5139 (1.2744)	grad_norm 3.2879 (inf)	loss_scale 1024.0000 (1703.8270)	mem 20118MB
[2024-05-28 12:54:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][700/2502]	eta 0:18:59 lr 0.000039	 wd 0.0000	time 0.5085 (0.6321)	loss 1.3477 (1.2723)	grad_norm 3.7201 (inf)	loss_scale 1024.0000 (1606.8474)	mem 20118MB
[2024-05-28 12:55:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][800/2502]	eta 0:17:53 lr 0.000039	 wd 0.0000	time 0.5815 (0.6308)	loss 1.5308 (1.2717)	grad_norm 3.9204 (inf)	loss_scale 1024.0000 (1534.0824)	mem 20118MB
[2024-05-28 12:56:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][900/2502]	eta 0:16:44 lr 0.000039	 wd 0.0000	time 0.5707 (0.6269)	loss 1.1963 (1.2708)	grad_norm 5.9343 (inf)	loss_scale 1024.0000 (1477.4695)	mem 20118MB
[2024-05-28 12:57:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1000/2502]	eta 0:15:45 lr 0.000039	 wd 0.0000	time 0.5648 (0.6294)	loss 1.4297 (1.2686)	grad_norm 5.5391 (inf)	loss_scale 1024.0000 (1432.1678)	mem 20118MB
[2024-05-28 12:58:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1100/2502]	eta 0:14:46 lr 0.000039	 wd 0.0000	time 0.5369 (0.6324)	loss 0.8077 (1.2682)	grad_norm 2.8629 (inf)	loss_scale 1024.0000 (1395.0954)	mem 20118MB
[2024-05-28 12:59:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1200/2502]	eta 0:13:39 lr 0.000039	 wd 0.0000	time 0.5411 (0.6298)	loss 1.1372 (1.2660)	grad_norm 31.3490 (inf)	loss_scale 1024.0000 (1364.1965)	mem 20118MB
[2024-05-28 13:00:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1300/2502]	eta 0:12:43 lr 0.000039	 wd 0.0000	time 0.6228 (0.6353)	loss 1.3874 (1.2678)	grad_norm 4.1618 (inf)	loss_scale 1024.0000 (1338.0477)	mem 20118MB
[2024-05-28 13:01:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1400/2502]	eta 0:11:44 lr 0.000039	 wd 0.0000	time 0.5682 (0.6391)	loss 1.4839 (1.2673)	grad_norm 4.2040 (inf)	loss_scale 1024.0000 (1315.6317)	mem 20118MB
[2024-05-28 13:02:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1500/2502]	eta 0:10:37 lr 0.000039	 wd 0.0000	time 0.5329 (0.6365)	loss 0.8301 (1.2667)	grad_norm 3.5658 (inf)	loss_scale 1024.0000 (1296.2025)	mem 20118MB
[2024-05-28 13:03:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1600/2502]	eta 0:09:36 lr 0.000039	 wd 0.0000	time 0.5783 (0.6389)	loss 1.4155 (1.2690)	grad_norm 3.6770 (inf)	loss_scale 1024.0000 (1279.2005)	mem 20118MB
[2024-05-28 13:05:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1700/2502]	eta 0:08:35 lr 0.000039	 wd 0.0000	time 0.6251 (0.6425)	loss 1.1974 (1.2704)	grad_norm 5.0767 (inf)	loss_scale 1024.0000 (1264.1975)	mem 20118MB
[2024-05-28 13:06:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1800/2502]	eta 0:07:29 lr 0.000039	 wd 0.0000	time 0.5575 (0.6398)	loss 1.3001 (1.2702)	grad_norm 5.4801 (inf)	loss_scale 1024.0000 (1250.8606)	mem 20118MB
[2024-05-28 13:07:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][1900/2502]	eta 0:06:25 lr 0.000039	 wd 0.0000	time 0.5750 (0.6412)	loss 0.9404 (1.2707)	grad_norm 3.8453 (inf)	loss_scale 1024.0000 (1238.9269)	mem 20118MB
[2024-05-28 13:08:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][2000/2502]	eta 0:05:24 lr 0.000039	 wd 0.0000	time 0.7407 (0.6463)	loss 1.4181 (1.2704)	grad_norm 3.4430 (inf)	loss_scale 1024.0000 (1228.1859)	mem 20118MB
[2024-05-28 13:09:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][2100/2502]	eta 0:04:19 lr 0.000039	 wd 0.0000	time 0.5296 (0.6466)	loss 1.4396 (1.2717)	grad_norm 4.4624 (inf)	loss_scale 1024.0000 (1218.4674)	mem 20118MB
[2024-05-28 13:10:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][2200/2502]	eta 0:03:15 lr 0.000039	 wd 0.0000	time 0.5953 (0.6470)	loss 1.2296 (1.2711)	grad_norm 3.3193 (inf)	loss_scale 1024.0000 (1209.6320)	mem 20118MB
[2024-05-28 13:11:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][2300/2502]	eta 0:02:11 lr 0.000039	 wd 0.0000	time 0.5358 (0.6520)	loss 1.1813 (1.2721)	grad_norm 3.8841 (inf)	loss_scale 1024.0000 (1201.5645)	mem 20118MB
[2024-05-28 13:12:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][2400/2502]	eta 0:01:06 lr 0.000039	 wd 0.0000	time 0.5300 (0.6491)	loss 1.3484 (1.2721)	grad_norm 2.7239 (inf)	loss_scale 1024.0000 (1194.1691)	mem 20118MB
[2024-05-28 13:13:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000039	 wd 0.0000	time 0.5159 (0.6475)	loss 1.1051 (1.2717)	grad_norm 3.2861 (inf)	loss_scale 1024.0000 (1187.3651)	mem 20118MB
[2024-05-28 13:14:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 7 training takes 0:27:09
[2024-05-28 13:15:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 69.734 (69.734)	Loss 0.3857 (0.3857)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 20118MB
[2024-05-28 13:15:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.720 Acc@5 97.804
[2024-05-28 13:15:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-05-28 13:15:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.72%
[2024-05-28 13:16:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][0/2502]	eta 21:49:39 lr 0.000039	 wd 0.0000	time 31.4065 (31.4065)	loss 1.4303 (1.4303)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:16:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][100/2502]	eta 0:34:41 lr 0.000039	 wd 0.0000	time 0.5202 (0.8664)	loss 1.2878 (1.2843)	grad_norm 9.1801 (4.2635)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:17:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][200/2502]	eta 0:27:06 lr 0.000039	 wd 0.0000	time 0.5275 (0.7064)	loss 1.4138 (1.2766)	grad_norm 3.0875 (4.2564)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:18:46 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][300/2502]	eta 0:23:57 lr 0.000038	 wd 0.0000	time 0.5370 (0.6526)	loss 1.3617 (1.2737)	grad_norm 3.5114 (4.1274)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:19:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][400/2502]	eta 0:21:55 lr 0.000038	 wd 0.0000	time 0.5637 (0.6257)	loss 1.0257 (1.2670)	grad_norm 4.7046 (4.1381)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:20:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][500/2502]	eta 0:20:20 lr 0.000038	 wd 0.0000	time 0.5288 (0.6097)	loss 1.1349 (1.2686)	grad_norm 8.9651 (4.2340)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:21:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][600/2502]	eta 0:19:09 lr 0.000038	 wd 0.0000	time 1.0504 (0.6045)	loss 1.1564 (1.2614)	grad_norm 3.6205 (4.2449)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:22:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][700/2502]	eta 0:18:39 lr 0.000038	 wd 0.0000	time 0.5169 (0.6210)	loss 1.4810 (1.2622)	grad_norm 3.7447 (4.3057)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:23:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][800/2502]	eta 0:17:57 lr 0.000038	 wd 0.0000	time 0.6019 (0.6332)	loss 1.3400 (1.2653)	grad_norm 3.1721 (4.3109)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:24:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][900/2502]	eta 0:16:50 lr 0.000038	 wd 0.0000	time 0.5351 (0.6307)	loss 1.5213 (1.2653)	grad_norm 4.1807 (4.3181)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:25:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1000/2502]	eta 0:15:40 lr 0.000038	 wd 0.0000	time 0.5312 (0.6263)	loss 1.3369 (1.2666)	grad_norm 3.5606 (4.3299)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:27:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1100/2502]	eta 0:14:58 lr 0.000038	 wd 0.0000	time 0.5879 (0.6409)	loss 0.9316 (1.2660)	grad_norm 4.0108 (4.3439)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:28:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1200/2502]	eta 0:13:53 lr 0.000038	 wd 0.0000	time 0.6266 (0.6401)	loss 1.1258 (1.2656)	grad_norm 4.6690 (4.3205)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:29:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1300/2502]	eta 0:12:44 lr 0.000038	 wd 0.0000	time 0.5216 (0.6360)	loss 1.0089 (1.2647)	grad_norm 4.1110 (4.3317)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:30:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1400/2502]	eta 0:11:55 lr 0.000038	 wd 0.0000	time 0.5491 (0.6489)	loss 0.9884 (1.2645)	grad_norm 5.8335 (4.3164)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:31:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1500/2502]	eta 0:10:51 lr 0.000038	 wd 0.0000	time 0.5212 (0.6498)	loss 1.4262 (1.2654)	grad_norm 5.7507 (4.3288)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:32:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1600/2502]	eta 0:09:42 lr 0.000038	 wd 0.0000	time 0.5140 (0.6460)	loss 1.2416 (1.2641)	grad_norm 3.2733 (4.3309)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:33:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1700/2502]	eta 0:08:41 lr 0.000038	 wd 0.0000	time 0.5904 (0.6503)	loss 1.3903 (1.2630)	grad_norm 3.0520 (4.3077)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:35:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1800/2502]	eta 0:07:38 lr 0.000038	 wd 0.0000	time 0.5355 (0.6529)	loss 1.5113 (1.2625)	grad_norm 3.6656 (4.3081)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 13:36:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][1900/2502]	eta 0:06:31 lr 0.000038	 wd 0.0000	time 0.5497 (0.6497)	loss 1.4164 (1.2626)	grad_norm 3.7657 (4.3160)	loss_scale 2048.0000 (1026.1547)	mem 20118MB
[2024-05-28 13:37:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][2000/2502]	eta 0:05:28 lr 0.000038	 wd 0.0000	time 0.5198 (0.6535)	loss 1.1617 (1.2620)	grad_norm 2.1552 (4.3099)	loss_scale 2048.0000 (1077.2214)	mem 20118MB
[2024-05-28 13:38:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][2100/2502]	eta 0:04:22 lr 0.000038	 wd 0.0000	time 0.6613 (0.6533)	loss 1.3492 (1.2620)	grad_norm 4.6840 (4.3438)	loss_scale 2048.0000 (1123.4269)	mem 20118MB
[2024-05-28 13:39:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][2200/2502]	eta 0:03:16 lr 0.000038	 wd 0.0000	time 0.5210 (0.6503)	loss 1.1718 (1.2619)	grad_norm 2.7290 (4.3327)	loss_scale 2048.0000 (1165.4339)	mem 20118MB
[2024-05-28 13:40:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][2300/2502]	eta 0:02:12 lr 0.000038	 wd 0.0000	time 0.5821 (0.6553)	loss 1.4882 (1.2610)	grad_norm 6.0443 (4.3296)	loss_scale 2048.0000 (1203.7897)	mem 20118MB
[2024-05-28 13:41:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][2400/2502]	eta 0:01:06 lr 0.000038	 wd 0.0000	time 0.5086 (0.6542)	loss 1.2440 (1.2624)	grad_norm 2.9948 (4.3319)	loss_scale 2048.0000 (1238.9504)	mem 20118MB
[2024-05-28 13:42:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000038	 wd 0.0000	time 0.5186 (0.6504)	loss 1.0032 (1.2625)	grad_norm 2.8043 (4.3367)	loss_scale 2048.0000 (1271.2995)	mem 20118MB
[2024-05-28 13:42:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 8 training takes 0:27:14
[2024-05-28 13:43:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 65.578 (65.578)	Loss 0.3894 (0.3894)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20118MB
[2024-05-28 13:44:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.828 Acc@5 97.734
[2024-05-28 13:44:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-05-28 13:44:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.83%
[2024-05-28 13:44:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][0/2502]	eta 1 day, 6:40:40 lr 0.000038	 wd 0.0000	time 44.1410 (44.1410)	loss 1.2549 (1.2549)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 13:45:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][100/2502]	eta 0:39:04 lr 0.000038	 wd 0.0000	time 0.5194 (0.9760)	loss 1.2919 (1.2593)	grad_norm 2.9170 (4.5366)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 13:46:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][200/2502]	eta 0:29:13 lr 0.000037	 wd 0.0000	time 0.5335 (0.7619)	loss 1.0118 (1.2643)	grad_norm 5.7747 (4.2809)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 13:47:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][300/2502]	eta 0:25:18 lr 0.000037	 wd 0.0000	time 0.5194 (0.6897)	loss 1.4947 (1.2673)	grad_norm 5.2066 (4.1836)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 13:48:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][400/2502]	eta 0:22:54 lr 0.000037	 wd 0.0000	time 0.5191 (0.6540)	loss 1.4558 (1.2637)	grad_norm 8.3871 (4.1392)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 13:49:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][500/2502]	eta 0:21:05 lr 0.000037	 wd 0.0000	time 0.5096 (0.6319)	loss 1.2662 (1.2654)	grad_norm 3.0332 (inf)	loss_scale 1024.0000 (1868.1357)	mem 20118MB
[2024-05-28 13:50:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][600/2502]	eta 0:19:34 lr 0.000037	 wd 0.0000	time 0.5208 (0.6177)	loss 1.4947 (1.2684)	grad_norm 12.8884 (inf)	loss_scale 1024.0000 (1727.6805)	mem 20118MB
[2024-05-28 13:51:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][700/2502]	eta 0:18:14 lr 0.000037	 wd 0.0000	time 0.5737 (0.6073)	loss 1.1560 (1.2651)	grad_norm 2.7271 (inf)	loss_scale 1024.0000 (1627.2981)	mem 20118MB
[2024-05-28 13:52:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][800/2502]	eta 0:17:17 lr 0.000037	 wd 0.0000	time 0.5388 (0.6099)	loss 1.4371 (1.2625)	grad_norm 2.9909 (inf)	loss_scale 1024.0000 (1551.9800)	mem 20118MB
[2024-05-28 13:53:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][900/2502]	eta 0:16:41 lr 0.000037	 wd 0.0000	time 0.5270 (0.6249)	loss 1.1684 (1.2571)	grad_norm 3.1579 (inf)	loss_scale 1024.0000 (1493.3807)	mem 20118MB
[2024-05-28 13:54:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1000/2502]	eta 0:15:33 lr 0.000037	 wd 0.0000	time 0.6173 (0.6215)	loss 1.1548 (1.2546)	grad_norm 3.7223 (inf)	loss_scale 1024.0000 (1446.4895)	mem 20118MB
[2024-05-28 13:55:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1100/2502]	eta 0:14:59 lr 0.000037	 wd 0.0000	time 0.5096 (0.6419)	loss 1.4071 (1.2576)	grad_norm 3.3290 (inf)	loss_scale 1024.0000 (1408.1163)	mem 20118MB
[2024-05-28 13:57:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1200/2502]	eta 0:14:22 lr 0.000037	 wd 0.0000	time 0.6199 (0.6625)	loss 1.2760 (1.2561)	grad_norm 3.9037 (inf)	loss_scale 1024.0000 (1376.1332)	mem 20118MB
[2024-05-28 13:58:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1300/2502]	eta 0:13:17 lr 0.000037	 wd 0.0000	time 0.5125 (0.6637)	loss 1.1145 (1.2552)	grad_norm 5.2273 (inf)	loss_scale 1024.0000 (1349.0669)	mem 20118MB
[2024-05-28 13:59:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1400/2502]	eta 0:12:01 lr 0.000037	 wd 0.0000	time 0.5152 (0.6551)	loss 1.3721 (1.2532)	grad_norm 3.3556 (inf)	loss_scale 1024.0000 (1325.8644)	mem 20118MB
[2024-05-28 14:00:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1500/2502]	eta 0:10:48 lr 0.000037	 wd 0.0000	time 0.5374 (0.6476)	loss 1.3729 (1.2512)	grad_norm 4.1133 (inf)	loss_scale 1024.0000 (1305.7535)	mem 20118MB
[2024-05-28 14:01:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1600/2502]	eta 0:09:38 lr 0.000037	 wd 0.0000	time 0.5248 (0.6412)	loss 1.3490 (1.2529)	grad_norm 2.8246 (inf)	loss_scale 1024.0000 (1288.1549)	mem 20118MB
[2024-05-28 14:02:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1700/2502]	eta 0:08:29 lr 0.000037	 wd 0.0000	time 0.5097 (0.6356)	loss 1.2222 (1.2529)	grad_norm 5.0780 (inf)	loss_scale 1024.0000 (1272.6255)	mem 20118MB
[2024-05-28 14:03:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1800/2502]	eta 0:07:30 lr 0.000037	 wd 0.0000	time 0.5195 (0.6424)	loss 1.3749 (1.2536)	grad_norm 3.3030 (inf)	loss_scale 1024.0000 (1258.8207)	mem 20118MB
[2024-05-28 14:04:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][1900/2502]	eta 0:06:25 lr 0.000037	 wd 0.0000	time 0.9983 (0.6405)	loss 1.1408 (1.2518)	grad_norm 2.9944 (inf)	loss_scale 1024.0000 (1246.4682)	mem 20118MB
[2024-05-28 14:05:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][2000/2502]	eta 0:05:23 lr 0.000037	 wd 0.0000	time 0.5144 (0.6452)	loss 1.1795 (1.2520)	grad_norm 4.0732 (inf)	loss_scale 1024.0000 (1235.3503)	mem 20118MB
[2024-05-28 14:06:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][2100/2502]	eta 0:04:18 lr 0.000036	 wd 0.0000	time 0.5370 (0.6440)	loss 1.3088 (1.2525)	grad_norm 4.0877 (inf)	loss_scale 1024.0000 (1225.2908)	mem 20118MB
[2024-05-28 14:07:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][2200/2502]	eta 0:03:13 lr 0.000036	 wd 0.0000	time 0.6745 (0.6412)	loss 1.4951 (1.2508)	grad_norm 2.8976 (inf)	loss_scale 1024.0000 (1216.1454)	mem 20118MB
[2024-05-28 14:08:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][2300/2502]	eta 0:02:10 lr 0.000036	 wd 0.0000	time 0.5196 (0.6443)	loss 1.1124 (1.2502)	grad_norm 4.0151 (inf)	loss_scale 1024.0000 (1207.7949)	mem 20118MB
[2024-05-28 14:10:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][2400/2502]	eta 0:01:06 lr 0.000036	 wd 0.0000	time 0.5753 (0.6491)	loss 1.2363 (1.2498)	grad_norm 11.6282 (inf)	loss_scale 1024.0000 (1200.1399)	mem 20118MB
[2024-05-28 14:11:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000036	 wd 0.0000	time 0.5197 (0.6458)	loss 1.3675 (1.2505)	grad_norm 2.6371 (inf)	loss_scale 1024.0000 (1193.0972)	mem 20118MB
[2024-05-28 14:11:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 9 training takes 0:27:14
[2024-05-28 14:12:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 64.447 (64.447)	Loss 0.3887 (0.3887)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 20118MB
[2024-05-28 14:12:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.862 Acc@5 97.788
[2024-05-28 14:12:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-05-28 14:12:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.86%
[2024-05-28 14:13:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][0/2502]	eta 16:27:37 lr 0.000036	 wd 0.0000	time 23.6843 (23.6843)	loss 0.8786 (0.8786)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:14:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][100/2502]	eta 0:31:00 lr 0.000036	 wd 0.0000	time 0.5477 (0.7745)	loss 0.9994 (1.2384)	grad_norm 5.1354 (4.3477)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:15:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][200/2502]	eta 0:25:19 lr 0.000036	 wd 0.0000	time 0.5232 (0.6599)	loss 1.4990 (1.2536)	grad_norm 4.4481 (4.0943)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:15:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][300/2502]	eta 0:22:47 lr 0.000036	 wd 0.0000	time 0.5094 (0.6211)	loss 1.5493 (1.2608)	grad_norm 4.0980 (4.3755)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:16:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][400/2502]	eta 0:21:05 lr 0.000036	 wd 0.0000	time 0.5180 (0.6021)	loss 0.7758 (1.2506)	grad_norm 15.2771 (4.3028)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:17:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][500/2502]	eta 0:19:45 lr 0.000036	 wd 0.0000	time 0.6515 (0.5923)	loss 0.9718 (1.2516)	grad_norm 4.4806 (4.2875)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:18:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][600/2502]	eta 0:18:57 lr 0.000036	 wd 0.0000	time 0.5220 (0.5979)	loss 1.0122 (1.2482)	grad_norm 5.2154 (4.2251)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:20:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][700/2502]	eta 0:18:51 lr 0.000036	 wd 0.0000	time 0.5854 (0.6279)	loss 1.5751 (1.2509)	grad_norm 5.1694 (4.2635)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:21:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][800/2502]	eta 0:17:48 lr 0.000036	 wd 0.0000	time 0.9251 (0.6276)	loss 1.2540 (1.2552)	grad_norm 3.8754 (4.3903)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:22:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][900/2502]	eta 0:16:42 lr 0.000036	 wd 0.0000	time 0.5832 (0.6261)	loss 1.2313 (1.2551)	grad_norm 4.4456 (4.4138)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:23:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1000/2502]	eta 0:16:07 lr 0.000036	 wd 0.0000	time 0.6117 (0.6443)	loss 1.1228 (1.2543)	grad_norm 3.9191 (4.3869)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:24:46 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1100/2502]	eta 0:15:09 lr 0.000036	 wd 0.0000	time 0.5246 (0.6488)	loss 1.4893 (1.2534)	grad_norm 4.8870 (4.3930)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:25:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1200/2502]	eta 0:13:58 lr 0.000035	 wd 0.0000	time 0.5213 (0.6438)	loss 1.2206 (1.2503)	grad_norm 3.6543 (4.3669)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:26:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1300/2502]	eta 0:12:52 lr 0.000035	 wd 0.0000	time 0.6271 (0.6431)	loss 1.3637 (1.2509)	grad_norm 3.1767 (4.3426)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:28:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1400/2502]	eta 0:12:02 lr 0.000035	 wd 0.0000	time 0.5231 (0.6552)	loss 0.9084 (1.2508)	grad_norm 3.3076 (4.3245)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:29:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1500/2502]	eta 0:10:55 lr 0.000035	 wd 0.0000	time 0.5616 (0.6539)	loss 1.1951 (1.2499)	grad_norm 3.5019 (4.3287)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:30:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1600/2502]	eta 0:10:03 lr 0.000035	 wd 0.0000	time 0.5201 (0.6687)	loss 1.3515 (1.2473)	grad_norm 7.4972 (4.3334)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:31:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1700/2502]	eta 0:08:51 lr 0.000035	 wd 0.0000	time 0.5305 (0.6623)	loss 1.1478 (1.2478)	grad_norm 2.7885 (4.3326)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:32:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1800/2502]	eta 0:07:40 lr 0.000035	 wd 0.0000	time 0.5329 (0.6558)	loss 1.3617 (1.2488)	grad_norm 3.8421 (4.3951)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:33:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][1900/2502]	eta 0:06:31 lr 0.000035	 wd 0.0000	time 0.5294 (0.6500)	loss 1.4085 (1.2487)	grad_norm 4.5228 (4.4027)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 14:34:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][2000/2502]	eta 0:05:23 lr 0.000035	 wd 0.0000	time 0.5147 (0.6447)	loss 1.3680 (1.2493)	grad_norm 4.2848 (4.3998)	loss_scale 2048.0000 (1070.0570)	mem 20118MB
[2024-05-28 14:35:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][2100/2502]	eta 0:04:17 lr 0.000035	 wd 0.0000	time 0.5275 (0.6399)	loss 1.0503 (1.2508)	grad_norm 5.2514 (4.4048)	loss_scale 2048.0000 (1116.6035)	mem 20118MB
[2024-05-28 14:36:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][2200/2502]	eta 0:03:12 lr 0.000035	 wd 0.0000	time 0.5459 (0.6382)	loss 0.9050 (1.2496)	grad_norm 3.7559 (4.4058)	loss_scale 2048.0000 (1158.9205)	mem 20118MB
[2024-05-28 14:37:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][2300/2502]	eta 0:02:08 lr 0.000035	 wd 0.0000	time 0.5254 (0.6357)	loss 1.2255 (1.2502)	grad_norm 6.6224 (4.4135)	loss_scale 2048.0000 (1197.5593)	mem 20118MB
[2024-05-28 14:38:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][2400/2502]	eta 0:01:05 lr 0.000035	 wd 0.0000	time 0.5491 (0.6421)	loss 1.2925 (1.2499)	grad_norm 4.6307 (4.4055)	loss_scale 2048.0000 (1232.9796)	mem 20118MB
[2024-05-28 14:39:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000035	 wd 0.0000	time 0.5189 (0.6387)	loss 1.3534 (1.2519)	grad_norm 3.5217 (4.4154)	loss_scale 2048.0000 (1265.5674)	mem 20118MB
[2024-05-28 14:39:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 10 training takes 0:26:57
[2024-05-28 14:41:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 73.971 (73.971)	Loss 0.3750 (0.3750)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 20118MB
[2024-05-28 14:41:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.886 Acc@5 97.828
[2024-05-28 14:41:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-05-28 14:41:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.89%
[2024-05-28 14:41:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][0/2502]	eta 15:50:27 lr 0.000035	 wd 0.0000	time 22.7926 (22.7926)	loss 1.3127 (1.3127)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:42:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][100/2502]	eta 0:30:37 lr 0.000035	 wd 0.0000	time 0.5137 (0.7651)	loss 1.2255 (1.2294)	grad_norm 4.4442 (4.0326)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:43:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][200/2502]	eta 0:25:09 lr 0.000034	 wd 0.0000	time 0.5166 (0.6557)	loss 1.2880 (1.2638)	grad_norm 3.1928 (3.9812)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:44:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][300/2502]	eta 0:22:42 lr 0.000034	 wd 0.0000	time 0.5104 (0.6189)	loss 1.0659 (1.2594)	grad_norm 3.0138 (4.1261)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:45:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][400/2502]	eta 0:21:01 lr 0.000034	 wd 0.0000	time 0.5078 (0.5999)	loss 1.2791 (1.2488)	grad_norm 3.8664 (4.3844)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:46:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][500/2502]	eta 0:19:39 lr 0.000034	 wd 0.0000	time 0.5167 (0.5890)	loss 1.3116 (1.2477)	grad_norm 2.6471 (4.3336)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:47:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][600/2502]	eta 0:18:25 lr 0.000034	 wd 0.0000	time 0.5131 (0.5814)	loss 1.5482 (1.2498)	grad_norm 5.2428 (4.4250)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:48:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][700/2502]	eta 0:17:38 lr 0.000034	 wd 0.0000	time 0.5246 (0.5875)	loss 1.2744 (1.2530)	grad_norm 4.5948 (4.4145)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:49:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][800/2502]	eta 0:16:42 lr 0.000034	 wd 0.0000	time 0.5270 (0.5891)	loss 1.2394 (1.2501)	grad_norm 3.6656 (4.3949)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 14:50:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][900/2502]	eta 0:15:48 lr 0.000034	 wd 0.0000	time 0.6703 (0.5922)	loss 1.1749 (1.2483)	grad_norm 4.2202 (inf)	loss_scale 1024.0000 (2043.4539)	mem 20118MB
[2024-05-28 14:51:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1000/2502]	eta 0:15:21 lr 0.000034	 wd 0.0000	time 0.5678 (0.6132)	loss 1.3321 (1.2451)	grad_norm 2.9321 (inf)	loss_scale 1024.0000 (1941.6104)	mem 20118MB
[2024-05-28 14:52:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1100/2502]	eta 0:14:18 lr 0.000034	 wd 0.0000	time 0.5672 (0.6125)	loss 1.4058 (1.2468)	grad_norm 4.6709 (inf)	loss_scale 1024.0000 (1858.2670)	mem 20118MB
[2024-05-28 14:53:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1200/2502]	eta 0:13:25 lr 0.000034	 wd 0.0000	time 0.5310 (0.6183)	loss 1.2888 (1.2460)	grad_norm 3.5657 (inf)	loss_scale 1024.0000 (1788.8027)	mem 20118MB
[2024-05-28 14:54:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1300/2502]	eta 0:12:29 lr 0.000034	 wd 0.0000	time 0.5171 (0.6234)	loss 1.1907 (1.2452)	grad_norm 2.8579 (inf)	loss_scale 1024.0000 (1730.0169)	mem 20118MB
[2024-05-28 14:56:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1400/2502]	eta 0:11:36 lr 0.000034	 wd 0.0000	time 0.5400 (0.6322)	loss 1.0649 (1.2458)	grad_norm 3.7987 (inf)	loss_scale 1024.0000 (1679.6231)	mem 20118MB
[2024-05-28 14:57:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1500/2502]	eta 0:10:39 lr 0.000034	 wd 0.0000	time 0.5625 (0.6386)	loss 1.4820 (1.2480)	grad_norm 3.2343 (inf)	loss_scale 1024.0000 (1635.9440)	mem 20118MB
[2024-05-28 14:59:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1600/2502]	eta 0:10:04 lr 0.000034	 wd 0.0000	time 0.5405 (0.6697)	loss 1.3553 (1.2473)	grad_norm 4.3353 (inf)	loss_scale 1024.0000 (1597.7214)	mem 20118MB
[2024-05-28 15:00:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1700/2502]	eta 0:08:51 lr 0.000033	 wd 0.0000	time 0.5177 (0.6624)	loss 1.3422 (1.2468)	grad_norm 2.7880 (inf)	loss_scale 1024.0000 (1563.9929)	mem 20118MB
[2024-05-28 15:01:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1800/2502]	eta 0:07:40 lr 0.000033	 wd 0.0000	time 0.5098 (0.6558)	loss 1.3546 (1.2473)	grad_norm 4.0269 (inf)	loss_scale 1024.0000 (1534.0100)	mem 20118MB
[2024-05-28 15:01:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][1900/2502]	eta 0:06:31 lr 0.000033	 wd 0.0000	time 0.5312 (0.6499)	loss 1.5219 (1.2467)	grad_norm 4.5842 (inf)	loss_scale 1024.0000 (1507.1815)	mem 20118MB
[2024-05-28 15:02:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][2000/2502]	eta 0:05:23 lr 0.000033	 wd 0.0000	time 0.5363 (0.6447)	loss 1.2312 (1.2466)	grad_norm 2.9677 (inf)	loss_scale 1024.0000 (1483.0345)	mem 20118MB
[2024-05-28 15:03:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][2100/2502]	eta 0:04:17 lr 0.000033	 wd 0.0000	time 0.5200 (0.6398)	loss 1.2355 (1.2462)	grad_norm 3.6846 (inf)	loss_scale 1024.0000 (1461.1861)	mem 20118MB
[2024-05-28 15:04:46 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][2200/2502]	eta 0:03:12 lr 0.000033	 wd 0.0000	time 0.5131 (0.6384)	loss 1.0968 (1.2468)	grad_norm 3.2975 (inf)	loss_scale 1024.0000 (1441.3230)	mem 20118MB
[2024-05-28 15:05:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][2300/2502]	eta 0:02:09 lr 0.000033	 wd 0.0000	time 5.4878 (0.6398)	loss 1.4386 (1.2470)	grad_norm 3.0847 (inf)	loss_scale 1024.0000 (1423.1864)	mem 20118MB
[2024-05-28 15:06:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][2400/2502]	eta 0:01:05 lr 0.000033	 wd 0.0000	time 0.5146 (0.6406)	loss 1.3625 (1.2483)	grad_norm 2.7861 (inf)	loss_scale 1024.0000 (1406.5606)	mem 20118MB
[2024-05-28 15:07:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000033	 wd 0.0000	time 0.5125 (0.6372)	loss 1.2878 (1.2496)	grad_norm 5.5896 (inf)	loss_scale 1024.0000 (1391.2643)	mem 20118MB
[2024-05-28 15:08:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 11 training takes 0:26:53
[2024-05-28 15:09:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 62.696 (62.696)	Loss 0.3806 (0.3806)	Acc@1 93.555 (93.555)	Acc@5 98.633 (98.633)	Mem 20118MB
[2024-05-28 15:09:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.966 Acc@5 97.802
[2024-05-28 15:09:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-05-28 15:09:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.97%
[2024-05-28 15:09:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][0/2502]	eta 15:10:47 lr 0.000033	 wd 0.0000	time 21.8416 (21.8416)	loss 1.4385 (1.4385)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:10:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][100/2502]	eta 0:30:15 lr 0.000033	 wd 0.0000	time 0.5474 (0.7558)	loss 1.4171 (1.2399)	grad_norm 3.3211 (3.8717)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:11:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][200/2502]	eta 0:24:58 lr 0.000033	 wd 0.0000	time 0.5139 (0.6510)	loss 0.7592 (1.2423)	grad_norm 3.9787 (4.1915)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:12:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][300/2502]	eta 0:22:35 lr 0.000033	 wd 0.0000	time 0.5276 (0.6156)	loss 1.3781 (1.2450)	grad_norm 3.9218 (4.1866)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:13:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][400/2502]	eta 0:20:55 lr 0.000033	 wd 0.0000	time 0.5309 (0.5975)	loss 1.2375 (1.2513)	grad_norm 3.3089 (4.2141)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:14:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][500/2502]	eta 0:20:44 lr 0.000032	 wd 0.0000	time 0.5241 (0.6216)	loss 1.0424 (1.2501)	grad_norm 2.7859 (4.1449)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:15:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][600/2502]	eta 0:20:14 lr 0.000032	 wd 0.0000	time 1.0389 (0.6384)	loss 1.0154 (1.2524)	grad_norm 5.3153 (4.1145)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:17:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][700/2502]	eta 0:20:13 lr 0.000032	 wd 0.0000	time 0.5170 (0.6736)	loss 1.3756 (1.2507)	grad_norm 3.1867 (4.1145)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:18:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][800/2502]	eta 0:18:38 lr 0.000032	 wd 0.0000	time 0.5283 (0.6575)	loss 1.3166 (1.2483)	grad_norm 4.0636 (4.1065)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:19:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][900/2502]	eta 0:17:13 lr 0.000032	 wd 0.0000	time 0.5107 (0.6449)	loss 1.2305 (1.2484)	grad_norm 3.0674 (4.1218)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:20:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1000/2502]	eta 0:15:53 lr 0.000032	 wd 0.0000	time 0.5122 (0.6347)	loss 1.1917 (1.2463)	grad_norm 5.7542 (4.0947)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:21:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1100/2502]	eta 0:14:38 lr 0.000032	 wd 0.0000	time 0.5076 (0.6264)	loss 0.7295 (1.2459)	grad_norm 3.7104 (4.1436)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:21:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1200/2502]	eta 0:13:26 lr 0.000032	 wd 0.0000	time 0.5295 (0.6194)	loss 1.3941 (1.2485)	grad_norm 4.5366 (4.1696)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:22:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1300/2502]	eta 0:12:22 lr 0.000032	 wd 0.0000	time 0.5239 (0.6180)	loss 1.3506 (1.2463)	grad_norm 5.8911 (4.2181)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:23:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1400/2502]	eta 0:11:18 lr 0.000032	 wd 0.0000	time 0.6561 (0.6159)	loss 1.2198 (1.2469)	grad_norm 2.9104 (4.2354)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:25:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1500/2502]	eta 0:10:48 lr 0.000032	 wd 0.0000	time 0.5190 (0.6470)	loss 1.2774 (1.2482)	grad_norm 2.8572 (4.1961)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:26:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1600/2502]	eta 0:09:38 lr 0.000032	 wd 0.0000	time 0.5407 (0.6408)	loss 1.2061 (1.2482)	grad_norm 3.1535 (4.1990)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:27:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1700/2502]	eta 0:08:29 lr 0.000031	 wd 0.0000	time 0.5096 (0.6351)	loss 0.9695 (1.2511)	grad_norm 4.2282 (4.1995)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:28:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1800/2502]	eta 0:07:22 lr 0.000031	 wd 0.0000	time 0.5249 (0.6300)	loss 1.4708 (1.2527)	grad_norm 4.5903 (4.2624)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:29:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][1900/2502]	eta 0:06:16 lr 0.000031	 wd 0.0000	time 0.5219 (0.6256)	loss 1.4556 (1.2524)	grad_norm 7.7724 (4.2533)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:30:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][2000/2502]	eta 0:05:11 lr 0.000031	 wd 0.0000	time 0.5189 (0.6214)	loss 0.7764 (1.2502)	grad_norm 3.7191 (4.2640)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:31:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][2100/2502]	eta 0:04:09 lr 0.000031	 wd 0.0000	time 0.5240 (0.6211)	loss 1.4630 (1.2493)	grad_norm 3.8570 (4.2510)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:32:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][2200/2502]	eta 0:03:07 lr 0.000031	 wd 0.0000	time 0.8639 (0.6207)	loss 1.2732 (1.2494)	grad_norm 3.4909 (4.2399)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:33:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][2300/2502]	eta 0:02:07 lr 0.000031	 wd 0.0000	time 0.5291 (0.6297)	loss 1.0785 (1.2488)	grad_norm 3.9368 (4.2247)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 15:34:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][2400/2502]	eta 0:01:04 lr 0.000031	 wd 0.0000	time 0.6310 (0.6291)	loss 1.1195 (1.2486)	grad_norm 4.6232 (4.2321)	loss_scale 2048.0000 (1026.5589)	mem 20118MB
[2024-05-28 15:35:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000031	 wd 0.0000	time 0.5152 (0.6268)	loss 1.3321 (1.2487)	grad_norm 3.6922 (4.2361)	loss_scale 2048.0000 (1067.4002)	mem 20118MB
[2024-05-28 15:35:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 12 training takes 0:26:24
[2024-05-28 15:36:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 56.559 (56.559)	Loss 0.3723 (0.3723)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 15:37:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.926 Acc@5 97.874
[2024-05-28 15:37:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-05-28 15:37:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.97%
[2024-05-28 15:37:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][0/2502]	eta 20:22:50 lr 0.000031	 wd 0.0000	time 29.3249 (29.3249)	loss 1.2338 (1.2338)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 15:38:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][100/2502]	eta 0:33:18 lr 0.000031	 wd 0.0000	time 0.5333 (0.8320)	loss 1.1876 (1.2188)	grad_norm 5.6034 (4.3729)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 15:39:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][200/2502]	eta 0:26:24 lr 0.000031	 wd 0.0000	time 0.5273 (0.6885)	loss 1.0665 (1.2386)	grad_norm 4.0376 (nan)	loss_scale 1024.0000 (1721.9502)	mem 20118MB
[2024-05-28 15:40:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][300/2502]	eta 0:23:29 lr 0.000031	 wd 0.0000	time 0.5220 (0.6401)	loss 1.0209 (1.2412)	grad_norm 4.7194 (nan)	loss_scale 1024.0000 (1490.0731)	mem 20118MB
[2024-05-28 15:41:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][400/2502]	eta 0:21:36 lr 0.000030	 wd 0.0000	time 0.5148 (0.6167)	loss 1.3637 (1.2271)	grad_norm 3.9023 (nan)	loss_scale 1024.0000 (1373.8454)	mem 20118MB
[2024-05-28 15:42:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][500/2502]	eta 0:20:05 lr 0.000030	 wd 0.0000	time 0.5210 (0.6023)	loss 0.8216 (1.2312)	grad_norm 4.1921 (nan)	loss_scale 1024.0000 (1304.0160)	mem 20118MB
[2024-05-28 15:43:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][600/2502]	eta 0:19:47 lr 0.000030	 wd 0.0000	time 0.5468 (0.6241)	loss 1.0357 (1.2323)	grad_norm 3.0954 (nan)	loss_scale 1024.0000 (1257.4243)	mem 20118MB
[2024-05-28 15:44:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][700/2502]	eta 0:19:09 lr 0.000030	 wd 0.0000	time 0.5592 (0.6382)	loss 1.4613 (1.2330)	grad_norm 3.0698 (nan)	loss_scale 1024.0000 (1224.1255)	mem 20118MB
[2024-05-28 15:45:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][800/2502]	eta 0:18:10 lr 0.000030	 wd 0.0000	time 0.5627 (0.6407)	loss 1.2263 (1.2337)	grad_norm 4.3703 (nan)	loss_scale 1024.0000 (1199.1411)	mem 20118MB
[2024-05-28 15:46:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][900/2502]	eta 0:16:59 lr 0.000030	 wd 0.0000	time 0.5136 (0.6366)	loss 1.0885 (1.2348)	grad_norm 3.1128 (nan)	loss_scale 1024.0000 (1179.7026)	mem 20118MB
[2024-05-28 15:48:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1000/2502]	eta 0:16:15 lr 0.000030	 wd 0.0000	time 0.5089 (0.6496)	loss 1.1516 (1.2338)	grad_norm 2.6951 (nan)	loss_scale 1024.0000 (1164.1479)	mem 20118MB
[2024-05-28 15:49:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1100/2502]	eta 0:15:06 lr 0.000030	 wd 0.0000	time 0.6747 (0.6464)	loss 1.4205 (1.2325)	grad_norm 3.9409 (nan)	loss_scale 1024.0000 (1151.4187)	mem 20118MB
[2024-05-28 15:50:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1200/2502]	eta 0:13:56 lr 0.000030	 wd 0.0000	time 0.5166 (0.6428)	loss 1.5884 (1.2327)	grad_norm 3.6324 (nan)	loss_scale 1024.0000 (1140.8093)	mem 20118MB
[2024-05-28 15:51:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1300/2502]	eta 0:12:53 lr 0.000030	 wd 0.0000	time 0.5173 (0.6437)	loss 1.4069 (1.2331)	grad_norm 4.8682 (nan)	loss_scale 1024.0000 (1131.8309)	mem 20118MB
[2024-05-28 15:52:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1400/2502]	eta 0:11:49 lr 0.000030	 wd 0.0000	time 0.6719 (0.6437)	loss 1.2505 (1.2347)	grad_norm 5.8901 (nan)	loss_scale 1024.0000 (1124.1342)	mem 20118MB
[2024-05-28 15:53:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1500/2502]	eta 0:10:44 lr 0.000030	 wd 0.0000	time 0.5270 (0.6432)	loss 1.1260 (1.2350)	grad_norm 2.0595 (nan)	loss_scale 1024.0000 (1117.4630)	mem 20118MB
[2024-05-28 15:54:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1600/2502]	eta 0:09:51 lr 0.000029	 wd 0.0000	time 0.6318 (0.6555)	loss 0.7046 (1.2345)	grad_norm 2.8557 (nan)	loss_scale 1024.0000 (1111.6252)	mem 20118MB
[2024-05-28 15:55:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1700/2502]	eta 0:08:49 lr 0.000029	 wd 0.0000	time 0.5337 (0.6607)	loss 1.3378 (1.2370)	grad_norm 3.1893 (nan)	loss_scale 1024.0000 (1106.4738)	mem 20118MB
[2024-05-28 15:56:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1800/2502]	eta 0:07:40 lr 0.000029	 wd 0.0000	time 0.5216 (0.6567)	loss 1.4195 (1.2368)	grad_norm 4.6183 (nan)	loss_scale 1024.0000 (1101.8945)	mem 20118MB
[2024-05-28 15:58:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][1900/2502]	eta 0:06:35 lr 0.000029	 wd 0.0000	time 0.6021 (0.6577)	loss 1.0559 (1.2368)	grad_norm 3.5212 (nan)	loss_scale 1024.0000 (1097.7969)	mem 20118MB
[2024-05-28 15:59:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][2000/2502]	eta 0:05:30 lr 0.000029	 wd 0.0000	time 0.5324 (0.6581)	loss 1.3188 (1.2361)	grad_norm 4.3436 (nan)	loss_scale 1024.0000 (1094.1089)	mem 20118MB
[2024-05-28 16:00:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][2100/2502]	eta 0:04:23 lr 0.000029	 wd 0.0000	time 0.5386 (0.6549)	loss 1.4709 (1.2368)	grad_norm 3.4023 (nan)	loss_scale 1024.0000 (1090.7720)	mem 20118MB
[2024-05-28 16:01:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][2200/2502]	eta 0:03:17 lr 0.000029	 wd 0.0000	time 0.5148 (0.6540)	loss 0.9437 (1.2367)	grad_norm 3.6335 (nan)	loss_scale 1024.0000 (1087.7383)	mem 20118MB
[2024-05-28 16:02:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][2300/2502]	eta 0:02:12 lr 0.000029	 wd 0.0000	time 0.7410 (0.6536)	loss 1.1172 (1.2373)	grad_norm 3.5683 (nan)	loss_scale 1024.0000 (1084.9683)	mem 20118MB
[2024-05-28 16:03:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][2400/2502]	eta 0:01:06 lr 0.000029	 wd 0.0000	time 0.5117 (0.6516)	loss 1.2544 (1.2377)	grad_norm 6.7236 (nan)	loss_scale 1024.0000 (1082.4290)	mem 20118MB
[2024-05-28 16:04:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000029	 wd 0.0000	time 0.5140 (0.6476)	loss 1.3793 (1.2383)	grad_norm 2.8943 (nan)	loss_scale 1024.0000 (1080.0928)	mem 20118MB
[2024-05-28 16:04:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 13 training takes 0:27:18
[2024-05-28 16:05:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 67.296 (67.296)	Loss 0.3730 (0.3730)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 16:05:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 85.952 Acc@5 97.900
[2024-05-28 16:05:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-05-28 16:05:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 85.97%
[2024-05-28 16:06:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][0/2502]	eta 11:15:53 lr 0.000029	 wd 0.0000	time 16.2085 (16.2085)	loss 0.9434 (0.9434)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:07:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][100/2502]	eta 0:28:12 lr 0.000029	 wd 0.0000	time 0.5391 (0.7048)	loss 1.2865 (1.2440)	grad_norm 3.1808 (3.8417)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:07:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][200/2502]	eta 0:23:57 lr 0.000028	 wd 0.0000	time 0.5130 (0.6242)	loss 1.3029 (1.2241)	grad_norm 4.0726 (3.8492)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:08:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][300/2502]	eta 0:21:55 lr 0.000028	 wd 0.0000	time 0.5408 (0.5975)	loss 1.2283 (1.2352)	grad_norm 3.5938 (3.8836)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:09:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][400/2502]	eta 0:20:27 lr 0.000028	 wd 0.0000	time 0.5265 (0.5842)	loss 1.4139 (1.2382)	grad_norm 6.5391 (4.0155)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:10:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][500/2502]	eta 0:19:38 lr 0.000028	 wd 0.0000	time 0.5190 (0.5887)	loss 1.4873 (1.2404)	grad_norm 15.8239 (3.9813)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:11:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][600/2502]	eta 0:18:39 lr 0.000028	 wd 0.0000	time 0.5470 (0.5887)	loss 1.2226 (1.2419)	grad_norm 4.0007 (3.9495)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:12:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][700/2502]	eta 0:17:53 lr 0.000028	 wd 0.0000	time 0.6053 (0.5958)	loss 1.3412 (1.2431)	grad_norm 3.8058 (3.9113)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:14:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][800/2502]	eta 0:17:37 lr 0.000028	 wd 0.0000	time 0.6025 (0.6213)	loss 1.5258 (1.2433)	grad_norm 4.0868 (3.9336)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:15:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][900/2502]	eta 0:17:10 lr 0.000028	 wd 0.0000	time 0.5350 (0.6434)	loss 1.3284 (1.2413)	grad_norm 3.9873 (3.9956)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:16:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1000/2502]	eta 0:15:51 lr 0.000028	 wd 0.0000	time 0.5351 (0.6335)	loss 1.3200 (1.2414)	grad_norm 3.6560 (4.0432)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:17:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1100/2502]	eta 0:14:36 lr 0.000028	 wd 0.0000	time 0.5139 (0.6254)	loss 1.4014 (1.2424)	grad_norm 2.8094 (4.0252)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:18:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1200/2502]	eta 0:13:25 lr 0.000028	 wd 0.0000	time 0.5348 (0.6186)	loss 0.9516 (1.2454)	grad_norm 2.6000 (4.0019)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:19:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1300/2502]	eta 0:12:16 lr 0.000027	 wd 0.0000	time 0.5095 (0.6129)	loss 1.3118 (1.2446)	grad_norm 3.4441 (4.0027)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:20:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1400/2502]	eta 0:11:09 lr 0.000027	 wd 0.0000	time 0.5226 (0.6079)	loss 1.2666 (1.2419)	grad_norm 4.1053 (4.0079)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:21:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1500/2502]	eta 0:10:10 lr 0.000027	 wd 0.0000	time 0.5362 (0.6094)	loss 1.4150 (1.2395)	grad_norm 2.7602 (4.0787)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:22:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1600/2502]	eta 0:09:19 lr 0.000027	 wd 0.0000	time 0.5931 (0.6207)	loss 1.2424 (1.2422)	grad_norm 5.9297 (4.1018)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:23:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1700/2502]	eta 0:08:16 lr 0.000027	 wd 0.0000	time 0.5982 (0.6191)	loss 1.5167 (1.2427)	grad_norm 3.7024 (4.1169)	loss_scale 2048.0000 (1063.7319)	mem 20118MB
[2024-05-28 16:24:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1800/2502]	eta 0:07:17 lr 0.000027	 wd 0.0000	time 0.5619 (0.6226)	loss 1.0423 (1.2428)	grad_norm 3.7900 (4.1218)	loss_scale 2048.0000 (1118.3831)	mem 20118MB
[2024-05-28 16:25:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][1900/2502]	eta 0:06:15 lr 0.000027	 wd 0.0000	time 0.5330 (0.6235)	loss 1.0205 (1.2425)	grad_norm 3.8285 (4.1708)	loss_scale 2048.0000 (1167.2846)	mem 20118MB
[2024-05-28 16:26:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][2000/2502]	eta 0:05:12 lr 0.000027	 wd 0.0000	time 0.5183 (0.6215)	loss 1.3407 (1.2411)	grad_norm 3.5055 (nan)	loss_scale 1024.0000 (1204.1339)	mem 20118MB
[2024-05-28 16:27:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][2100/2502]	eta 0:04:09 lr 0.000027	 wd 0.0000	time 0.5140 (0.6212)	loss 1.2365 (1.2412)	grad_norm 4.6689 (nan)	loss_scale 1024.0000 (1195.5602)	mem 20118MB
[2024-05-28 16:28:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][2200/2502]	eta 0:03:07 lr 0.000027	 wd 0.0000	time 0.7081 (0.6208)	loss 0.9615 (1.2403)	grad_norm 2.7780 (nan)	loss_scale 1024.0000 (1187.7656)	mem 20118MB
[2024-05-28 16:29:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][2300/2502]	eta 0:02:05 lr 0.000027	 wd 0.0000	time 0.5465 (0.6207)	loss 1.3519 (1.2404)	grad_norm 7.3222 (nan)	loss_scale 1024.0000 (1180.6484)	mem 20118MB
[2024-05-28 16:30:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][2400/2502]	eta 0:01:03 lr 0.000026	 wd 0.0000	time 0.5121 (0.6216)	loss 1.3632 (1.2393)	grad_norm 3.7344 (nan)	loss_scale 1024.0000 (1174.1241)	mem 20118MB
[2024-05-28 16:31:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000026	 wd 0.0000	time 0.5136 (0.6196)	loss 1.3741 (1.2396)	grad_norm 4.9105 (nan)	loss_scale 1024.0000 (1168.1216)	mem 20118MB
[2024-05-28 16:31:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 14 training takes 0:26:00
[2024-05-28 16:32:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 65.073 (65.073)	Loss 0.3726 (0.3726)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 16:33:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.074 Acc@5 97.868
[2024-05-28 16:33:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 16:33:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.07%
[2024-05-28 16:33:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][0/2502]	eta 1 day, 7:51:42 lr 0.000026	 wd 0.0000	time 45.8443 (45.8443)	loss 1.1678 (1.1678)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:34:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][100/2502]	eta 0:39:42 lr 0.000026	 wd 0.0000	time 0.5352 (0.9917)	loss 1.2767 (1.2257)	grad_norm 2.7009 (4.7830)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:35:46 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][200/2502]	eta 0:29:32 lr 0.000026	 wd 0.0000	time 0.5282 (0.7701)	loss 1.3596 (1.2241)	grad_norm 3.2458 (4.3903)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:36:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][300/2502]	eta 0:25:30 lr 0.000026	 wd 0.0000	time 0.5209 (0.6953)	loss 1.4579 (1.2270)	grad_norm 2.2472 (4.3482)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:37:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][400/2502]	eta 0:23:02 lr 0.000026	 wd 0.0000	time 0.5406 (0.6576)	loss 1.2288 (1.2372)	grad_norm 4.1912 (4.3439)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:38:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][500/2502]	eta 0:21:11 lr 0.000026	 wd 0.0000	time 0.5146 (0.6350)	loss 1.3335 (1.2408)	grad_norm 3.2574 (4.4639)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:39:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][600/2502]	eta 0:19:39 lr 0.000026	 wd 0.0000	time 0.5186 (0.6199)	loss 0.9855 (1.2370)	grad_norm 5.0136 (4.3741)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:40:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][700/2502]	eta 0:18:23 lr 0.000026	 wd 0.0000	time 1.5882 (0.6125)	loss 0.9213 (1.2400)	grad_norm 5.5190 (4.3225)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:41:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][800/2502]	eta 0:17:23 lr 0.000026	 wd 0.0000	time 0.5367 (0.6134)	loss 1.2474 (1.2392)	grad_norm 3.0055 (4.2500)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:42:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][900/2502]	eta 0:16:40 lr 0.000025	 wd 0.0000	time 0.5329 (0.6244)	loss 1.1980 (1.2411)	grad_norm 5.5418 (4.2296)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:43:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1000/2502]	eta 0:15:38 lr 0.000025	 wd 0.0000	time 0.6271 (0.6249)	loss 0.8907 (1.2435)	grad_norm 2.2268 (4.2353)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:44:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1100/2502]	eta 0:14:34 lr 0.000025	 wd 0.0000	time 0.5198 (0.6236)	loss 1.1795 (1.2413)	grad_norm 3.8045 (4.2191)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:45:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1200/2502]	eta 0:13:32 lr 0.000025	 wd 0.0000	time 0.5551 (0.6240)	loss 1.2828 (1.2409)	grad_norm 2.6795 (4.2428)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:46:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1300/2502]	eta 0:12:26 lr 0.000025	 wd 0.0000	time 0.5293 (0.6212)	loss 1.4686 (1.2396)	grad_norm 4.7991 (4.2945)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:47:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1400/2502]	eta 0:11:27 lr 0.000025	 wd 0.0000	time 0.5149 (0.6237)	loss 0.9110 (1.2372)	grad_norm 3.9342 (4.2814)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:48:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1500/2502]	eta 0:10:28 lr 0.000025	 wd 0.0000	time 0.6322 (0.6269)	loss 1.3436 (1.2359)	grad_norm 3.2738 (4.2777)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:49:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1600/2502]	eta 0:09:22 lr 0.000025	 wd 0.0000	time 0.5604 (0.6241)	loss 0.8274 (1.2380)	grad_norm 3.5344 (4.2817)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:50:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1700/2502]	eta 0:08:22 lr 0.000025	 wd 0.0000	time 0.6363 (0.6271)	loss 1.4145 (1.2383)	grad_norm 4.1309 (4.2540)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:52:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1800/2502]	eta 0:07:21 lr 0.000025	 wd 0.0000	time 2.1603 (0.6290)	loss 1.3396 (1.2380)	grad_norm 3.5571 (4.2235)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:53:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][1900/2502]	eta 0:06:20 lr 0.000024	 wd 0.0000	time 0.5414 (0.6313)	loss 1.1382 (1.2368)	grad_norm 4.2513 (4.2169)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:54:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][2000/2502]	eta 0:05:18 lr 0.000024	 wd 0.0000	time 0.6221 (0.6338)	loss 1.2432 (1.2363)	grad_norm 4.9410 (4.2011)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:55:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][2100/2502]	eta 0:04:15 lr 0.000024	 wd 0.0000	time 0.5983 (0.6359)	loss 1.4269 (1.2359)	grad_norm 5.0808 (4.1966)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:56:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][2200/2502]	eta 0:03:11 lr 0.000024	 wd 0.0000	time 0.5120 (0.6339)	loss 1.2980 (1.2349)	grad_norm 4.9465 (4.1879)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:57:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][2300/2502]	eta 0:02:07 lr 0.000024	 wd 0.0000	time 0.5192 (0.6334)	loss 1.3757 (1.2355)	grad_norm 2.4052 (4.2077)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:58:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][2400/2502]	eta 0:01:04 lr 0.000024	 wd 0.0000	time 0.5042 (0.6321)	loss 1.3353 (1.2354)	grad_norm 3.1053 (4.2035)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:59:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000024	 wd 0.0000	time 0.5250 (0.6354)	loss 0.8698 (1.2370)	grad_norm 3.8190 (4.1859)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 16:59:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 15 training takes 0:26:37
[2024-05-28 17:01:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 78.255 (78.255)	Loss 0.3674 (0.3674)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 17:01:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.098 Acc@5 97.862
[2024-05-28 17:01:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 17:01:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.10%
[2024-05-28 17:02:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][0/2502]	eta 1 day, 7:17:01 lr 0.000024	 wd 0.0000	time 45.0125 (45.0125)	loss 1.2322 (1.2322)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:03:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][100/2502]	eta 0:39:24 lr 0.000024	 wd 0.0000	time 0.5243 (0.9844)	loss 1.2393 (1.2235)	grad_norm 4.0635 (3.7530)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:03:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][200/2502]	eta 0:29:24 lr 0.000024	 wd 0.0000	time 0.5216 (0.7663)	loss 1.3382 (1.2237)	grad_norm 2.3743 (3.9539)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:04:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][300/2502]	eta 0:25:25 lr 0.000024	 wd 0.0000	time 0.5324 (0.6926)	loss 1.4171 (1.2239)	grad_norm 3.3382 (3.9512)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:05:46 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][400/2502]	eta 0:22:58 lr 0.000024	 wd 0.0000	time 0.5430 (0.6559)	loss 0.8044 (1.2262)	grad_norm 3.8608 (3.9755)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:06:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][500/2502]	eta 0:21:08 lr 0.000023	 wd 0.0000	time 0.5425 (0.6336)	loss 1.0656 (1.2245)	grad_norm 7.8889 (4.0123)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:07:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][600/2502]	eta 0:19:37 lr 0.000023	 wd 0.0000	time 0.5248 (0.6191)	loss 1.3486 (1.2268)	grad_norm 3.8624 (4.0262)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:08:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][700/2502]	eta 0:18:17 lr 0.000023	 wd 0.0000	time 0.5199 (0.6089)	loss 1.4758 (1.2264)	grad_norm 2.9592 (4.0122)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:09:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][800/2502]	eta 0:17:03 lr 0.000023	 wd 0.0000	time 0.5193 (0.6012)	loss 1.2657 (1.2232)	grad_norm 2.3385 (4.0296)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:10:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][900/2502]	eta 0:16:00 lr 0.000023	 wd 0.0000	time 0.5688 (0.5993)	loss 0.7314 (1.2255)	grad_norm 10.0040 (4.1000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:11:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1000/2502]	eta 0:14:59 lr 0.000023	 wd 0.0000	time 0.5347 (0.5991)	loss 1.3635 (1.2307)	grad_norm 3.5715 (inf)	loss_scale 1024.0000 (1032.1838)	mem 20118MB
[2024-05-28 17:12:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1100/2502]	eta 0:14:15 lr 0.000023	 wd 0.0000	time 0.5861 (0.6104)	loss 1.1220 (1.2303)	grad_norm 4.2696 (inf)	loss_scale 1024.0000 (1031.4405)	mem 20118MB
[2024-05-28 17:13:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1200/2502]	eta 0:13:29 lr 0.000023	 wd 0.0000	time 0.5442 (0.6215)	loss 0.8065 (1.2328)	grad_norm 13.0066 (inf)	loss_scale 1024.0000 (1030.8210)	mem 20118MB
[2024-05-28 17:14:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1300/2502]	eta 0:12:26 lr 0.000023	 wd 0.0000	time 0.5218 (0.6212)	loss 1.3759 (1.2296)	grad_norm 3.0453 (inf)	loss_scale 1024.0000 (1030.2967)	mem 20118MB
[2024-05-28 17:15:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1400/2502]	eta 0:11:26 lr 0.000023	 wd 0.0000	time 0.5089 (0.6233)	loss 1.3576 (1.2283)	grad_norm 3.1244 (inf)	loss_scale 1024.0000 (1029.8473)	mem 20118MB
[2024-05-28 17:17:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1500/2502]	eta 0:10:34 lr 0.000022	 wd 0.0000	time 0.5300 (0.6332)	loss 1.2077 (1.2311)	grad_norm 4.1947 (inf)	loss_scale 1024.0000 (1029.4577)	mem 20118MB
[2024-05-28 17:18:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1600/2502]	eta 0:09:29 lr 0.000022	 wd 0.0000	time 0.5866 (0.6309)	loss 0.9872 (1.2298)	grad_norm 5.3615 (inf)	loss_scale 1024.0000 (1029.1168)	mem 20118MB
[2024-05-28 17:19:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1700/2502]	eta 0:08:33 lr 0.000022	 wd 0.0000	time 0.5251 (0.6404)	loss 1.3551 (1.2277)	grad_norm 3.4914 (inf)	loss_scale 1024.0000 (1028.8160)	mem 20118MB
[2024-05-28 17:20:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1800/2502]	eta 0:07:30 lr 0.000022	 wd 0.0000	time 0.5727 (0.6415)	loss 1.2978 (1.2280)	grad_norm 3.0854 (inf)	loss_scale 1024.0000 (1028.5486)	mem 20118MB
[2024-05-28 17:21:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][1900/2502]	eta 0:06:24 lr 0.000022	 wd 0.0000	time 0.6220 (0.6385)	loss 1.2360 (1.2276)	grad_norm 6.1118 (inf)	loss_scale 1024.0000 (1028.3093)	mem 20118MB
[2024-05-28 17:22:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][2000/2502]	eta 0:05:22 lr 0.000022	 wd 0.0000	time 0.5837 (0.6417)	loss 1.1060 (1.2288)	grad_norm 2.9851 (inf)	loss_scale 1024.0000 (1028.0940)	mem 20118MB
[2024-05-28 17:23:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][2100/2502]	eta 0:04:18 lr 0.000022	 wd 0.0000	time 0.5260 (0.6424)	loss 1.3116 (1.2297)	grad_norm 3.5754 (inf)	loss_scale 1024.0000 (1027.8991)	mem 20118MB
[2024-05-28 17:24:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][2200/2502]	eta 0:03:13 lr 0.000022	 wd 0.0000	time 0.5589 (0.6401)	loss 1.1836 (1.2296)	grad_norm 3.3149 (inf)	loss_scale 1024.0000 (1027.7219)	mem 20118MB
[2024-05-28 17:26:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][2300/2502]	eta 0:02:10 lr 0.000022	 wd 0.0000	time 0.5679 (0.6456)	loss 1.2297 (1.2291)	grad_norm 5.4624 (inf)	loss_scale 1024.0000 (1027.5602)	mem 20118MB
[2024-05-28 17:27:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][2400/2502]	eta 0:01:05 lr 0.000022	 wd 0.0000	time 0.5584 (0.6452)	loss 1.2127 (1.2290)	grad_norm 4.1038 (inf)	loss_scale 1024.0000 (1027.4119)	mem 20118MB
[2024-05-28 17:28:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000021	 wd 0.0000	time 0.5203 (0.6417)	loss 1.0851 (1.2289)	grad_norm 2.6450 (inf)	loss_scale 1024.0000 (1027.2755)	mem 20118MB
[2024-05-28 17:28:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 16 training takes 0:26:54
[2024-05-28 17:29:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 56.534 (56.534)	Loss 0.3540 (0.3540)	Acc@1 93.750 (93.750)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 17:29:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.062 Acc@5 97.864
[2024-05-28 17:29:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 17:29:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.10%
[2024-05-28 17:30:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][0/2502]	eta 1 day, 0:44:20 lr 0.000021	 wd 0.0000	time 35.5956 (35.5956)	loss 1.0312 (1.0312)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:31:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][100/2502]	eta 0:35:47 lr 0.000021	 wd 0.0000	time 0.5355 (0.8942)	loss 1.3240 (1.2030)	grad_norm 3.7439 (3.7965)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:31:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][200/2502]	eta 0:27:38 lr 0.000021	 wd 0.0000	time 0.5246 (0.7203)	loss 1.4735 (1.2186)	grad_norm 3.7252 (3.8712)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:32:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][300/2502]	eta 0:24:17 lr 0.000021	 wd 0.0000	time 0.5386 (0.6619)	loss 1.2151 (1.2248)	grad_norm 5.5513 (3.9126)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:33:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][400/2502]	eta 0:22:09 lr 0.000021	 wd 0.0000	time 0.5184 (0.6327)	loss 1.0079 (1.2182)	grad_norm 3.3471 (3.9331)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:34:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][500/2502]	eta 0:20:31 lr 0.000021	 wd 0.0000	time 0.5131 (0.6153)	loss 1.2695 (1.2235)	grad_norm 11.6918 (4.0176)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:35:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][600/2502]	eta 0:19:38 lr 0.000021	 wd 0.0000	time 0.5848 (0.6195)	loss 1.5283 (1.2230)	grad_norm 3.2725 (4.0767)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:36:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][700/2502]	eta 0:18:45 lr 0.000021	 wd 0.0000	time 0.5585 (0.6246)	loss 1.3027 (1.2230)	grad_norm 3.1957 (4.0304)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:37:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][800/2502]	eta 0:17:56 lr 0.000021	 wd 0.0000	time 0.5694 (0.6323)	loss 1.2113 (1.2285)	grad_norm 2.4426 (4.0729)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:39:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][900/2502]	eta 0:16:57 lr 0.000021	 wd 0.0000	time 0.5675 (0.6353)	loss 1.0808 (1.2256)	grad_norm 2.6474 (4.0920)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:40:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1000/2502]	eta 0:16:02 lr 0.000020	 wd 0.0000	time 0.6123 (0.6410)	loss 0.9322 (1.2260)	grad_norm 3.5292 (4.0812)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:41:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1100/2502]	eta 0:14:53 lr 0.000020	 wd 0.0000	time 0.5203 (0.6371)	loss 1.2727 (1.2272)	grad_norm 3.3991 (4.0495)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:42:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1200/2502]	eta 0:13:59 lr 0.000020	 wd 0.0000	time 0.5462 (0.6447)	loss 1.4389 (1.2281)	grad_norm 2.6990 (4.0289)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:43:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1300/2502]	eta 0:12:55 lr 0.000020	 wd 0.0000	time 0.6525 (0.6449)	loss 1.4178 (1.2307)	grad_norm 4.0475 (4.0604)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:44:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1400/2502]	eta 0:11:46 lr 0.000020	 wd 0.0000	time 0.5287 (0.6407)	loss 1.4942 (1.2284)	grad_norm 5.9793 (4.0701)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:45:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1500/2502]	eta 0:10:44 lr 0.000020	 wd 0.0000	time 0.6150 (0.6436)	loss 1.2798 (1.2288)	grad_norm 2.8005 (4.0725)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:46:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1600/2502]	eta 0:09:42 lr 0.000020	 wd 0.0000	time 0.5478 (0.6461)	loss 1.2770 (1.2299)	grad_norm 3.7583 (4.0685)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:47:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1700/2502]	eta 0:08:36 lr 0.000020	 wd 0.0000	time 0.5232 (0.6440)	loss 1.4421 (1.2313)	grad_norm 5.2579 (4.0711)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:48:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1800/2502]	eta 0:07:31 lr 0.000020	 wd 0.0000	time 0.5169 (0.6434)	loss 1.0854 (1.2325)	grad_norm 3.1987 (4.0730)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:49:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][1900/2502]	eta 0:06:25 lr 0.000020	 wd 0.0000	time 0.6312 (0.6402)	loss 1.2934 (1.2310)	grad_norm 4.0171 (4.0745)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:50:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][2000/2502]	eta 0:05:21 lr 0.000019	 wd 0.0000	time 0.5790 (0.6412)	loss 1.2011 (1.2303)	grad_norm 3.6110 (4.0681)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:52:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][2100/2502]	eta 0:04:18 lr 0.000019	 wd 0.0000	time 0.5350 (0.6422)	loss 1.3464 (1.2301)	grad_norm 2.5382 (4.0780)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:52:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][2200/2502]	eta 0:03:13 lr 0.000019	 wd 0.0000	time 0.5156 (0.6393)	loss 1.3455 (1.2307)	grad_norm 2.7689 (4.0881)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:54:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][2300/2502]	eta 0:02:09 lr 0.000019	 wd 0.0000	time 0.5345 (0.6388)	loss 1.5684 (1.2301)	grad_norm 16.4690 (4.0955)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:55:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][2400/2502]	eta 0:01:05 lr 0.000019	 wd 0.0000	time 1.1447 (0.6411)	loss 1.3566 (1.2288)	grad_norm 2.7268 (4.1147)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:56:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0000	time 0.5050 (0.6407)	loss 1.2872 (1.2294)	grad_norm 3.5816 (inf)	loss_scale 1024.0000 (1025.6377)	mem 20118MB
[2024-05-28 17:56:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 17 training takes 0:26:50
[2024-05-28 17:57:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 61.395 (61.395)	Loss 0.3574 (0.3574)	Acc@1 93.555 (93.555)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 17:57:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.096 Acc@5 97.834
[2024-05-28 17:57:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 17:57:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.10%
[2024-05-28 17:58:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][0/2502]	eta 1 day, 5:51:40 lr 0.000019	 wd 0.0000	time 42.9657 (42.9657)	loss 1.0564 (1.0564)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 17:59:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][100/2502]	eta 0:38:37 lr 0.000019	 wd 0.0000	time 0.5095 (0.9648)	loss 1.3077 (1.2461)	grad_norm 3.8361 (3.9955)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:00:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][200/2502]	eta 0:29:01 lr 0.000019	 wd 0.0000	time 0.5311 (0.7563)	loss 1.4644 (1.2488)	grad_norm 3.7015 (3.8750)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:01:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][300/2502]	eta 0:25:10 lr 0.000019	 wd 0.0000	time 0.5482 (0.6861)	loss 1.0795 (1.2390)	grad_norm 4.5637 (3.8937)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:02:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][400/2502]	eta 0:22:47 lr 0.000019	 wd 0.0000	time 0.5124 (0.6508)	loss 1.3881 (1.2358)	grad_norm 5.1959 (4.0614)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:02:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][500/2502]	eta 0:21:00 lr 0.000018	 wd 0.0000	time 0.5335 (0.6297)	loss 1.2383 (1.2283)	grad_norm 4.2702 (4.3754)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:04:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][600/2502]	eta 0:19:55 lr 0.000018	 wd 0.0000	time 0.5909 (0.6288)	loss 0.8230 (1.2253)	grad_norm 2.9149 (4.3787)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:05:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][700/2502]	eta 0:19:24 lr 0.000018	 wd 0.0000	time 0.5275 (0.6463)	loss 0.9767 (1.2260)	grad_norm 5.0476 (4.3258)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:06:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][800/2502]	eta 0:18:06 lr 0.000018	 wd 0.0000	time 0.5289 (0.6386)	loss 1.0287 (1.2259)	grad_norm 3.6896 (nan)	loss_scale 512.0000 (1009.9376)	mem 20118MB
[2024-05-28 18:07:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][900/2502]	eta 0:17:17 lr 0.000018	 wd 0.0000	time 0.5642 (0.6478)	loss 1.5591 (1.2238)	grad_norm 5.9389 (nan)	loss_scale 512.0000 (954.6726)	mem 20118MB
[2024-05-28 18:08:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1000/2502]	eta 0:16:21 lr 0.000018	 wd 0.0000	time 0.5472 (0.6536)	loss 1.0601 (1.2247)	grad_norm 3.4199 (nan)	loss_scale 512.0000 (910.4496)	mem 20118MB
[2024-05-28 18:09:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1100/2502]	eta 0:15:08 lr 0.000018	 wd 0.0000	time 0.5929 (0.6480)	loss 1.0168 (1.2232)	grad_norm 2.7515 (nan)	loss_scale 512.0000 (874.2598)	mem 20118MB
[2024-05-28 18:10:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1200/2502]	eta 0:14:18 lr 0.000018	 wd 0.0000	time 0.5457 (0.6591)	loss 1.2388 (1.2242)	grad_norm 3.4822 (nan)	loss_scale 512.0000 (844.0966)	mem 20118MB
[2024-05-28 18:11:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1300/2502]	eta 0:13:12 lr 0.000018	 wd 0.0000	time 0.5199 (0.6590)	loss 0.9038 (1.2238)	grad_norm 3.3236 (nan)	loss_scale 512.0000 (818.5703)	mem 20118MB
[2024-05-28 18:13:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1400/2502]	eta 0:12:06 lr 0.000018	 wd 0.0000	time 0.5446 (0.6592)	loss 1.2147 (1.2229)	grad_norm 5.6355 (nan)	loss_scale 512.0000 (796.6881)	mem 20118MB
[2024-05-28 18:14:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1500/2502]	eta 0:10:57 lr 0.000017	 wd 0.0000	time 0.5110 (0.6557)	loss 1.4476 (1.2220)	grad_norm 4.4009 (nan)	loss_scale 512.0000 (777.7215)	mem 20118MB
[2024-05-28 18:15:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1600/2502]	eta 0:09:51 lr 0.000017	 wd 0.0000	time 0.5254 (0.6562)	loss 1.3048 (1.2217)	grad_norm 3.0924 (nan)	loss_scale 512.0000 (761.1243)	mem 20118MB
[2024-05-28 18:16:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1700/2502]	eta 0:08:42 lr 0.000017	 wd 0.0000	time 0.5597 (0.6518)	loss 1.3924 (1.2212)	grad_norm 3.8772 (nan)	loss_scale 512.0000 (746.4785)	mem 20118MB
[2024-05-28 18:17:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1800/2502]	eta 0:07:37 lr 0.000017	 wd 0.0000	time 0.5773 (0.6522)	loss 0.9228 (1.2237)	grad_norm 4.6359 (nan)	loss_scale 512.0000 (733.4592)	mem 20118MB
[2024-05-28 18:18:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][1900/2502]	eta 0:06:33 lr 0.000017	 wd 0.0000	time 0.5182 (0.6534)	loss 1.1178 (1.2238)	grad_norm 4.0487 (nan)	loss_scale 512.0000 (721.8096)	mem 20118MB
[2024-05-28 18:19:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][2000/2502]	eta 0:05:26 lr 0.000017	 wd 0.0000	time 0.5168 (0.6496)	loss 0.8825 (1.2241)	grad_norm 5.5994 (nan)	loss_scale 512.0000 (711.3243)	mem 20118MB
[2024-05-28 18:20:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][2100/2502]	eta 0:04:21 lr 0.000017	 wd 0.0000	time 0.6873 (0.6508)	loss 0.7902 (1.2251)	grad_norm 2.5730 (nan)	loss_scale 512.0000 (701.8372)	mem 20118MB
[2024-05-28 18:21:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][2200/2502]	eta 0:03:18 lr 0.000017	 wd 0.0000	time 0.5172 (0.6556)	loss 1.4026 (1.2248)	grad_norm 2.5775 (nan)	loss_scale 512.0000 (693.2122)	mem 20118MB
[2024-05-28 18:22:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][2300/2502]	eta 0:02:11 lr 0.000017	 wd 0.0000	time 0.5268 (0.6523)	loss 1.3472 (1.2242)	grad_norm 2.9877 (nan)	loss_scale 512.0000 (685.3368)	mem 20118MB
[2024-05-28 18:23:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][2400/2502]	eta 0:01:06 lr 0.000017	 wd 0.0000	time 0.7040 (0.6511)	loss 1.1213 (1.2244)	grad_norm 2.9553 (nan)	loss_scale 512.0000 (678.1175)	mem 20118MB
[2024-05-28 18:24:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0000	time 0.5122 (0.6475)	loss 1.2346 (1.2235)	grad_norm 2.6017 (nan)	loss_scale 512.0000 (671.4754)	mem 20118MB
[2024-05-28 18:25:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 18 training takes 0:27:18
[2024-05-28 18:26:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 65.126 (65.126)	Loss 0.3518 (0.3518)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 18:26:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.028 Acc@5 97.856
[2024-05-28 18:26:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-05-28 18:26:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.10%
[2024-05-28 18:26:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][0/2502]	eta 12:25:33 lr 0.000016	 wd 0.0000	time 17.8789 (17.8789)	loss 1.1977 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:27:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][100/2502]	eta 0:28:43 lr 0.000016	 wd 0.0000	time 0.5496 (0.7176)	loss 1.1127 (1.1797)	grad_norm 3.8097 (4.3698)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:28:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][200/2502]	eta 0:24:11 lr 0.000016	 wd 0.0000	time 0.5135 (0.6303)	loss 1.3946 (1.2195)	grad_norm 3.1268 (4.1401)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:29:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][300/2502]	eta 0:22:05 lr 0.000016	 wd 0.0000	time 0.5234 (0.6019)	loss 0.8812 (1.2299)	grad_norm 7.9942 (4.3883)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:30:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][400/2502]	eta 0:20:34 lr 0.000016	 wd 0.0000	time 0.5108 (0.5873)	loss 1.0764 (1.2283)	grad_norm 2.9996 (4.4837)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:31:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][500/2502]	eta 0:19:25 lr 0.000016	 wd 0.0000	time 0.5251 (0.5823)	loss 1.2383 (1.2250)	grad_norm 6.5918 (4.4224)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:32:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][600/2502]	eta 0:18:31 lr 0.000016	 wd 0.0000	time 0.5454 (0.5845)	loss 1.4377 (1.2262)	grad_norm 3.7855 (4.3970)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:33:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][700/2502]	eta 0:18:17 lr 0.000016	 wd 0.0000	time 0.5495 (0.6089)	loss 1.5718 (1.2250)	grad_norm 2.9749 (4.4264)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:34:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][800/2502]	eta 0:17:37 lr 0.000016	 wd 0.0000	time 0.5066 (0.6211)	loss 0.8771 (1.2223)	grad_norm 3.9577 (4.4132)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:35:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][900/2502]	eta 0:16:27 lr 0.000016	 wd 0.0000	time 0.5270 (0.6166)	loss 1.2057 (1.2252)	grad_norm 3.1293 (4.4347)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:37:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1000/2502]	eta 0:15:40 lr 0.000016	 wd 0.0000	time 0.5074 (0.6261)	loss 1.0218 (1.2226)	grad_norm 3.9854 (4.4046)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:38:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1100/2502]	eta 0:14:48 lr 0.000015	 wd 0.0000	time 0.5957 (0.6340)	loss 1.2771 (1.2229)	grad_norm 3.7360 (4.3985)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:39:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1200/2502]	eta 0:13:39 lr 0.000015	 wd 0.0000	time 0.5221 (0.6295)	loss 0.9586 (1.2213)	grad_norm 4.7058 (4.3607)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:40:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1300/2502]	eta 0:12:49 lr 0.000015	 wd 0.0000	time 0.6018 (0.6404)	loss 1.1466 (1.2213)	grad_norm 4.0108 (4.3244)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:41:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1400/2502]	eta 0:11:46 lr 0.000015	 wd 0.0000	time 0.5146 (0.6407)	loss 1.2920 (1.2208)	grad_norm 7.2739 (4.3588)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:42:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1500/2502]	eta 0:10:38 lr 0.000015	 wd 0.0000	time 0.5576 (0.6369)	loss 1.1356 (1.2215)	grad_norm 2.6750 (4.3445)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:43:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1600/2502]	eta 0:09:35 lr 0.000015	 wd 0.0000	time 0.5366 (0.6385)	loss 1.3120 (1.2196)	grad_norm 3.1295 (4.3381)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:44:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1700/2502]	eta 0:08:31 lr 0.000015	 wd 0.0000	time 0.5736 (0.6376)	loss 0.8063 (1.2192)	grad_norm 5.8792 (4.3132)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:45:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1800/2502]	eta 0:07:25 lr 0.000015	 wd 0.0000	time 0.5456 (0.6352)	loss 1.1375 (1.2175)	grad_norm 4.2858 (4.3018)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:46:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][1900/2502]	eta 0:06:22 lr 0.000015	 wd 0.0000	time 0.5069 (0.6354)	loss 1.2537 (1.2180)	grad_norm 3.4072 (4.3050)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:47:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][2000/2502]	eta 0:05:17 lr 0.000015	 wd 0.0000	time 0.5853 (0.6333)	loss 1.3445 (1.2187)	grad_norm 4.5043 (4.2825)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:49:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][2100/2502]	eta 0:04:17 lr 0.000014	 wd 0.0000	time 0.6571 (0.6411)	loss 1.0781 (1.2190)	grad_norm 4.4332 (4.2640)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:50:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][2200/2502]	eta 0:03:13 lr 0.000014	 wd 0.0000	time 0.5185 (0.6401)	loss 1.1074 (1.2186)	grad_norm 5.0179 (4.2692)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 18:51:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][2300/2502]	eta 0:02:09 lr 0.000014	 wd 0.0000	time 0.5831 (0.6409)	loss 0.8569 (1.2178)	grad_norm 3.4218 (4.2821)	loss_scale 1024.0000 (517.3403)	mem 20118MB
[2024-05-28 18:52:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][2400/2502]	eta 0:01:06 lr 0.000014	 wd 0.0000	time 0.5341 (0.6556)	loss 1.4358 (1.2193)	grad_norm 3.4903 (4.2792)	loss_scale 1024.0000 (538.4423)	mem 20118MB
[2024-05-28 18:53:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0000	time 0.5088 (0.6510)	loss 1.3580 (1.2196)	grad_norm 6.1994 (4.2764)	loss_scale 1024.0000 (557.8569)	mem 20118MB
[2024-05-28 18:53:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 19 training takes 0:27:16
[2024-05-28 18:54:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 41.797 (41.797)	Loss 0.3547 (0.3547)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 18:54:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.098 Acc@5 97.844
[2024-05-28 18:54:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 18:54:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.10%
[2024-05-28 18:55:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][0/2502]	eta 15:19:41 lr 0.000014	 wd 0.0000	time 22.0549 (22.0549)	loss 1.4275 (1.4275)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:56:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][100/2502]	eta 0:30:24 lr 0.000014	 wd 0.0000	time 0.5244 (0.7595)	loss 0.8993 (1.2366)	grad_norm 3.6131 (5.4434)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:57:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][200/2502]	eta 0:24:59 lr 0.000014	 wd 0.0000	time 0.5344 (0.6514)	loss 1.3197 (1.2145)	grad_norm 4.2853 (4.8498)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:57:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][300/2502]	eta 0:22:36 lr 0.000014	 wd 0.0000	time 0.5534 (0.6159)	loss 1.0419 (1.2081)	grad_norm 2.7185 (4.5128)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:58:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][400/2502]	eta 0:20:57 lr 0.000014	 wd 0.0000	time 0.5364 (0.5980)	loss 1.1450 (1.2150)	grad_norm 2.9954 (4.4072)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 18:59:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][500/2502]	eta 0:19:58 lr 0.000014	 wd 0.0000	time 0.5336 (0.5987)	loss 1.1342 (1.2212)	grad_norm 3.8471 (4.5108)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:00:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][600/2502]	eta 0:18:57 lr 0.000014	 wd 0.0000	time 0.5340 (0.5978)	loss 1.3824 (1.2216)	grad_norm 4.7180 (4.4969)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:02:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][700/2502]	eta 0:18:21 lr 0.000013	 wd 0.0000	time 0.6115 (0.6114)	loss 0.9296 (1.2218)	grad_norm 4.0252 (inf)	loss_scale 512.0000 (987.4807)	mem 20118MB
[2024-05-28 19:03:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][800/2502]	eta 0:17:29 lr 0.000013	 wd 0.0000	time 0.5297 (0.6164)	loss 1.2432 (1.2162)	grad_norm 2.7061 (inf)	loss_scale 512.0000 (928.1199)	mem 20118MB
[2024-05-28 19:04:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][900/2502]	eta 0:16:24 lr 0.000013	 wd 0.0000	time 0.5289 (0.6145)	loss 1.3004 (1.2179)	grad_norm 2.8385 (inf)	loss_scale 512.0000 (881.9356)	mem 20118MB
[2024-05-28 19:05:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1000/2502]	eta 0:15:44 lr 0.000013	 wd 0.0000	time 0.5546 (0.6286)	loss 1.3639 (1.2169)	grad_norm 5.1281 (inf)	loss_scale 512.0000 (844.9790)	mem 20118MB
[2024-05-28 19:06:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1100/2502]	eta 0:14:54 lr 0.000013	 wd 0.0000	time 0.5431 (0.6377)	loss 0.9096 (1.2161)	grad_norm 2.7838 (inf)	loss_scale 512.0000 (814.7357)	mem 20118MB
[2024-05-28 19:07:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1200/2502]	eta 0:13:45 lr 0.000013	 wd 0.0000	time 0.5704 (0.6339)	loss 0.9146 (1.2148)	grad_norm 2.4373 (inf)	loss_scale 512.0000 (789.5287)	mem 20118MB
[2024-05-28 19:08:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1300/2502]	eta 0:12:48 lr 0.000013	 wd 0.0000	time 0.5354 (0.6391)	loss 1.2844 (1.2146)	grad_norm 6.1580 (inf)	loss_scale 512.0000 (768.1968)	mem 20118MB
[2024-05-28 19:10:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1400/2502]	eta 0:11:53 lr 0.000013	 wd 0.0000	time 0.6305 (0.6473)	loss 0.8739 (1.2163)	grad_norm 4.3037 (inf)	loss_scale 512.0000 (749.9101)	mem 20118MB
[2024-05-28 19:11:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1500/2502]	eta 0:10:48 lr 0.000013	 wd 0.0000	time 0.6378 (0.6477)	loss 1.1934 (1.2156)	grad_norm 7.7290 (inf)	loss_scale 512.0000 (734.0600)	mem 20118MB
[2024-05-28 19:12:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1600/2502]	eta 0:09:42 lr 0.000013	 wd 0.0000	time 0.5453 (0.6457)	loss 1.4291 (1.2151)	grad_norm 3.7668 (inf)	loss_scale 512.0000 (720.1899)	mem 20118MB
[2024-05-28 19:13:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1700/2502]	eta 0:08:37 lr 0.000012	 wd 0.0000	time 0.5143 (0.6455)	loss 1.0872 (1.2146)	grad_norm 4.6926 (inf)	loss_scale 512.0000 (707.9506)	mem 20118MB
[2024-05-28 19:14:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1800/2502]	eta 0:07:30 lr 0.000012	 wd 0.0000	time 0.6164 (0.6423)	loss 1.4062 (1.2140)	grad_norm 4.2142 (inf)	loss_scale 512.0000 (697.0705)	mem 20118MB
[2024-05-28 19:15:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][1900/2502]	eta 0:06:28 lr 0.000012	 wd 0.0000	time 0.5841 (0.6454)	loss 1.3073 (1.2138)	grad_norm 3.0605 (inf)	loss_scale 512.0000 (687.3351)	mem 20118MB
[2024-05-28 19:16:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][2000/2502]	eta 0:05:23 lr 0.000012	 wd 0.0000	time 0.5609 (0.6451)	loss 0.9660 (1.2158)	grad_norm 3.6263 (inf)	loss_scale 512.0000 (678.5727)	mem 20118MB
[2024-05-28 19:17:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][2100/2502]	eta 0:04:18 lr 0.000012	 wd 0.0000	time 0.5369 (0.6428)	loss 1.0148 (1.2151)	grad_norm 4.4235 (inf)	loss_scale 512.0000 (670.6445)	mem 20118MB
[2024-05-28 19:18:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][2200/2502]	eta 0:03:16 lr 0.000012	 wd 0.0000	time 0.5511 (0.6490)	loss 1.3138 (1.2168)	grad_norm 6.1731 (inf)	loss_scale 512.0000 (663.4366)	mem 20118MB
[2024-05-28 19:19:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][2300/2502]	eta 0:02:11 lr 0.000012	 wd 0.0000	time 0.5185 (0.6512)	loss 1.1623 (1.2170)	grad_norm 2.8242 (inf)	loss_scale 512.0000 (656.8553)	mem 20118MB
[2024-05-28 19:20:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][2400/2502]	eta 0:01:06 lr 0.000012	 wd 0.0000	time 0.5969 (0.6499)	loss 1.2656 (1.2180)	grad_norm 5.0068 (inf)	loss_scale 512.0000 (650.8222)	mem 20118MB
[2024-05-28 19:21:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0000	time 0.5255 (0.6473)	loss 1.3301 (1.2183)	grad_norm 3.3208 (inf)	loss_scale 512.0000 (645.2715)	mem 20118MB
[2024-05-28 19:22:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 20 training takes 0:27:13
[2024-05-28 19:23:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 88.111 (88.111)	Loss 0.3523 (0.3523)	Acc@1 93.750 (93.750)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 19:23:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.112 Acc@5 97.852
[2024-05-28 19:23:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 19:23:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.11%
[2024-05-28 19:24:46 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][0/2502]	eta 1 day, 8:57:33 lr 0.000012	 wd 0.0000	time 47.4233 (47.4233)	loss 1.3877 (1.3877)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:25:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][100/2502]	eta 0:40:26 lr 0.000012	 wd 0.0000	time 0.5456 (1.0101)	loss 0.9974 (1.2082)	grad_norm 2.4433 (4.3378)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:26:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][200/2502]	eta 0:29:52 lr 0.000012	 wd 0.0000	time 0.5237 (0.7786)	loss 1.2932 (1.2151)	grad_norm 5.8900 (4.4240)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:27:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][300/2502]	eta 0:25:43 lr 0.000012	 wd 0.0000	time 0.5221 (0.7012)	loss 1.1753 (1.2257)	grad_norm 3.6134 (4.5405)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:28:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][400/2502]	eta 0:23:12 lr 0.000011	 wd 0.0000	time 0.5451 (0.6623)	loss 1.1530 (1.2204)	grad_norm 3.8170 (4.5737)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:29:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][500/2502]	eta 0:21:19 lr 0.000011	 wd 0.0000	time 0.5140 (0.6389)	loss 1.1255 (1.2191)	grad_norm 3.5279 (4.4607)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:30:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][600/2502]	eta 0:19:45 lr 0.000011	 wd 0.0000	time 0.5195 (0.6233)	loss 1.2912 (1.2179)	grad_norm 2.8395 (4.3769)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:31:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][700/2502]	eta 0:18:40 lr 0.000011	 wd 0.0000	time 0.5528 (0.6215)	loss 1.1373 (1.2181)	grad_norm 4.0805 (4.3965)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:32:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][800/2502]	eta 0:17:34 lr 0.000011	 wd 0.0000	time 0.5269 (0.6198)	loss 1.4809 (1.2205)	grad_norm 3.1418 (4.3485)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:34:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][900/2502]	eta 0:18:21 lr 0.000011	 wd 0.0000	time 0.5522 (0.6876)	loss 0.8644 (1.2220)	grad_norm 3.2336 (4.3271)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:35:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1000/2502]	eta 0:16:51 lr 0.000011	 wd 0.0000	time 0.5236 (0.6734)	loss 0.9568 (1.2193)	grad_norm 5.5916 (4.2990)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:36:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1100/2502]	eta 0:15:27 lr 0.000011	 wd 0.0000	time 0.5245 (0.6617)	loss 1.2587 (1.2182)	grad_norm 3.6193 (4.2969)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:37:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1200/2502]	eta 0:14:08 lr 0.000011	 wd 0.0000	time 0.5340 (0.6518)	loss 1.4257 (1.2178)	grad_norm 3.4894 (4.2790)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:37:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1300/2502]	eta 0:12:53 lr 0.000011	 wd 0.0000	time 0.5180 (0.6437)	loss 1.3559 (1.2156)	grad_norm 3.6822 (4.2829)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:38:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1400/2502]	eta 0:11:41 lr 0.000011	 wd 0.0000	time 0.5094 (0.6366)	loss 0.9141 (1.2170)	grad_norm 9.0680 (4.2897)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:40:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1500/2502]	eta 0:10:47 lr 0.000010	 wd 0.0000	time 0.5663 (0.6464)	loss 1.4126 (1.2149)	grad_norm 3.3074 (4.3144)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:41:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1600/2502]	eta 0:09:45 lr 0.000010	 wd 0.0000	time 0.5485 (0.6496)	loss 0.9051 (1.2160)	grad_norm 12.5039 (4.3095)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:42:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1700/2502]	eta 0:08:38 lr 0.000010	 wd 0.0000	time 0.5092 (0.6463)	loss 1.2285 (1.2167)	grad_norm 3.2938 (4.2890)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:43:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1800/2502]	eta 0:07:37 lr 0.000010	 wd 0.0000	time 0.5893 (0.6520)	loss 1.4897 (1.2175)	grad_norm 4.3106 (4.2704)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:44:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][1900/2502]	eta 0:06:35 lr 0.000010	 wd 0.0000	time 0.5241 (0.6575)	loss 1.3038 (1.2174)	grad_norm 4.8478 (4.2928)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:46:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][2000/2502]	eta 0:05:31 lr 0.000010	 wd 0.0000	time 0.5276 (0.6604)	loss 1.2660 (1.2181)	grad_norm 3.6336 (4.2856)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:47:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][2100/2502]	eta 0:04:24 lr 0.000010	 wd 0.0000	time 0.5130 (0.6581)	loss 1.3270 (1.2179)	grad_norm 4.9034 (4.3063)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 19:48:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][2200/2502]	eta 0:03:18 lr 0.000010	 wd 0.0000	time 0.5263 (0.6580)	loss 0.9948 (1.2167)	grad_norm 2.8484 (4.3068)	loss_scale 1024.0000 (524.0963)	mem 20118MB
[2024-05-28 19:49:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][2300/2502]	eta 0:02:12 lr 0.000010	 wd 0.0000	time 0.5732 (0.6565)	loss 1.5463 (1.2166)	grad_norm 2.9780 (4.2973)	loss_scale 1024.0000 (545.8218)	mem 20118MB
[2024-05-28 19:50:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][2400/2502]	eta 0:01:06 lr 0.000010	 wd 0.0000	time 0.5755 (0.6542)	loss 0.8732 (1.2172)	grad_norm 2.5642 (4.2897)	loss_scale 1024.0000 (565.7376)	mem 20118MB
[2024-05-28 19:51:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000010	 wd 0.0000	time 0.5259 (0.6501)	loss 1.3392 (1.2172)	grad_norm 5.0262 (4.2771)	loss_scale 1024.0000 (584.0608)	mem 20118MB
[2024-05-28 19:51:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 21 training takes 0:27:27
[2024-05-28 19:52:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 67.257 (67.257)	Loss 0.3503 (0.3503)	Acc@1 93.555 (93.555)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 19:52:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.226 Acc@5 97.878
[2024-05-28 19:52:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 19:52:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.23%
[2024-05-28 19:53:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][0/2502]	eta 21:30:31 lr 0.000010	 wd 0.0000	time 30.9478 (30.9478)	loss 1.2697 (1.2697)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:54:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][100/2502]	eta 0:34:18 lr 0.000010	 wd 0.0000	time 0.5111 (0.8570)	loss 1.1542 (1.2273)	grad_norm 4.8500 (4.4902)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:55:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][200/2502]	eta 0:26:54 lr 0.000009	 wd 0.0000	time 0.5275 (0.7014)	loss 1.3334 (1.2207)	grad_norm 2.9798 (4.4046)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:56:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][300/2502]	eta 0:23:51 lr 0.000009	 wd 0.0000	time 0.5392 (0.6503)	loss 1.3518 (1.2159)	grad_norm 3.2847 (4.2067)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:57:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][400/2502]	eta 0:21:52 lr 0.000009	 wd 0.0000	time 0.5099 (0.6243)	loss 1.3287 (1.2191)	grad_norm 3.9021 (4.2016)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:58:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][500/2502]	eta 0:20:18 lr 0.000009	 wd 0.0000	time 0.5196 (0.6085)	loss 1.2822 (1.2221)	grad_norm 4.2879 (4.2336)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 19:58:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][600/2502]	eta 0:18:57 lr 0.000009	 wd 0.0000	time 0.5423 (0.5981)	loss 1.2103 (1.2235)	grad_norm 4.0567 (4.1891)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:00:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][700/2502]	eta 0:20:16 lr 0.000009	 wd 0.0000	time 0.5323 (0.6751)	loss 1.0660 (1.2215)	grad_norm 8.0819 (4.2525)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:01:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][800/2502]	eta 0:18:41 lr 0.000009	 wd 0.0000	time 0.5360 (0.6587)	loss 1.2560 (1.2197)	grad_norm 3.8229 (4.2221)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:02:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][900/2502]	eta 0:17:15 lr 0.000009	 wd 0.0000	time 0.5455 (0.6461)	loss 1.3789 (1.2197)	grad_norm 4.2832 (4.2149)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:03:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1000/2502]	eta 0:15:55 lr 0.000009	 wd 0.0000	time 0.5475 (0.6361)	loss 1.1226 (1.2208)	grad_norm 2.8664 (4.1941)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:04:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1100/2502]	eta 0:14:40 lr 0.000009	 wd 0.0000	time 0.5126 (0.6278)	loss 1.3817 (1.2210)	grad_norm 3.8400 (4.1821)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:05:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1200/2502]	eta 0:13:28 lr 0.000009	 wd 0.0000	time 0.5195 (0.6209)	loss 1.1304 (1.2162)	grad_norm 4.0493 (4.2045)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:06:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1300/2502]	eta 0:12:27 lr 0.000009	 wd 0.0000	time 0.6200 (0.6218)	loss 1.4077 (1.2173)	grad_norm 4.3056 (4.2090)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:07:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1400/2502]	eta 0:11:33 lr 0.000008	 wd 0.0000	time 1.6274 (0.6294)	loss 1.3885 (1.2179)	grad_norm 4.3433 (4.2128)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:08:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1500/2502]	eta 0:10:36 lr 0.000008	 wd 0.0000	time 0.5619 (0.6351)	loss 1.2757 (1.2168)	grad_norm 4.7890 (4.2177)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:10:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1600/2502]	eta 0:09:38 lr 0.000008	 wd 0.0000	time 0.5243 (0.6414)	loss 1.4149 (1.2161)	grad_norm 3.4944 (4.2229)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:11:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1700/2502]	eta 0:08:37 lr 0.000008	 wd 0.0000	time 0.5247 (0.6450)	loss 1.5234 (1.2171)	grad_norm 3.4203 (4.2758)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:12:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1800/2502]	eta 0:07:33 lr 0.000008	 wd 0.0000	time 0.5781 (0.6456)	loss 1.1285 (1.2188)	grad_norm 5.0610 (4.2691)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:13:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][1900/2502]	eta 0:06:29 lr 0.000008	 wd 0.0000	time 0.5102 (0.6466)	loss 1.2103 (1.2176)	grad_norm 4.4698 (4.2563)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:14:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][2000/2502]	eta 0:05:25 lr 0.000008	 wd 0.0000	time 0.5404 (0.6477)	loss 1.2534 (1.2190)	grad_norm 6.9264 (4.2593)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:15:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][2100/2502]	eta 0:04:20 lr 0.000008	 wd 0.0000	time 0.6816 (0.6488)	loss 0.8995 (1.2195)	grad_norm 4.2181 (4.2716)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:16:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][2200/2502]	eta 0:03:15 lr 0.000008	 wd 0.0000	time 0.5846 (0.6490)	loss 1.2358 (1.2188)	grad_norm 3.1904 (4.2691)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:18:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][2300/2502]	eta 0:02:11 lr 0.000008	 wd 0.0000	time 0.6383 (0.6528)	loss 0.8358 (1.2182)	grad_norm 5.8475 (4.2704)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:19:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][2400/2502]	eta 0:01:06 lr 0.000008	 wd 0.0000	time 0.5829 (0.6541)	loss 0.8095 (1.2198)	grad_norm 3.0904 (4.3067)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:20:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0000	time 0.5199 (0.6503)	loss 0.9386 (1.2194)	grad_norm 6.0129 (4.3322)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:20:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 22 training takes 0:27:21
[2024-05-28 20:21:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 81.084 (81.084)	Loss 0.3516 (0.3516)	Acc@1 93.555 (93.555)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 20:22:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.176 Acc@5 97.868
[2024-05-28 20:22:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 20:22:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.23%
[2024-05-28 20:22:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][0/2502]	eta 1 day, 8:24:41 lr 0.000008	 wd 0.0000	time 46.6353 (46.6353)	loss 1.0147 (1.0147)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:23:45 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][100/2502]	eta 0:40:02 lr 0.000008	 wd 0.0000	time 0.5186 (1.0004)	loss 1.3709 (1.2180)	grad_norm 4.8463 (4.2704)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:24:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][200/2502]	eta 0:29:41 lr 0.000007	 wd 0.0000	time 0.5267 (0.7737)	loss 1.3800 (1.2136)	grad_norm 2.9187 (4.1090)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:25:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][300/2502]	eta 0:25:35 lr 0.000007	 wd 0.0000	time 0.5503 (0.6975)	loss 1.2931 (1.2289)	grad_norm 4.5960 (4.1432)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:26:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][400/2502]	eta 0:23:05 lr 0.000007	 wd 0.0000	time 0.5218 (0.6593)	loss 0.9071 (1.2218)	grad_norm 3.7659 (4.1427)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:27:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][500/2502]	eta 0:21:13 lr 0.000007	 wd 0.0000	time 0.5091 (0.6363)	loss 1.3685 (1.2189)	grad_norm 5.1032 (4.1982)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:28:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][600/2502]	eta 0:19:40 lr 0.000007	 wd 0.0000	time 0.5275 (0.6208)	loss 1.2157 (1.2191)	grad_norm 3.0250 (4.2179)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:29:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][700/2502]	eta 0:18:49 lr 0.000007	 wd 0.0000	time 0.5373 (0.6266)	loss 1.0147 (1.2207)	grad_norm 3.7103 (4.2242)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:30:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][800/2502]	eta 0:17:43 lr 0.000007	 wd 0.0000	time 0.6456 (0.6249)	loss 1.3069 (1.2176)	grad_norm 2.7115 (4.2333)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:31:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][900/2502]	eta 0:16:44 lr 0.000007	 wd 0.0000	time 0.6436 (0.6273)	loss 0.8635 (1.2096)	grad_norm 3.6036 (4.2364)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:32:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1000/2502]	eta 0:16:22 lr 0.000007	 wd 0.0000	time 0.5973 (0.6543)	loss 1.2691 (1.2115)	grad_norm 3.2692 (4.2753)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:34:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1100/2502]	eta 0:15:32 lr 0.000007	 wd 0.0000	time 0.5176 (0.6649)	loss 1.0789 (1.2116)	grad_norm 31.0321 (4.3631)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:35:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1200/2502]	eta 0:14:12 lr 0.000007	 wd 0.0000	time 0.5216 (0.6549)	loss 1.1750 (1.2140)	grad_norm 4.3464 (4.4734)	loss_scale 2048.0000 (1071.7469)	mem 20118MB
[2024-05-28 20:36:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1300/2502]	eta 0:12:57 lr 0.000007	 wd 0.0000	time 0.5350 (0.6465)	loss 1.2854 (1.2170)	grad_norm 2.7548 (4.4302)	loss_scale 2048.0000 (1146.7855)	mem 20118MB
[2024-05-28 20:37:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1400/2502]	eta 0:11:44 lr 0.000007	 wd 0.0000	time 0.5256 (0.6392)	loss 1.3726 (1.2176)	grad_norm 3.2111 (4.4169)	loss_scale 2048.0000 (1211.1121)	mem 20118MB
[2024-05-28 20:37:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1500/2502]	eta 0:10:34 lr 0.000006	 wd 0.0000	time 0.5180 (0.6329)	loss 1.2677 (1.2182)	grad_norm 4.9643 (inf)	loss_scale 1024.0000 (1238.2145)	mem 20118MB
[2024-05-28 20:38:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1600/2502]	eta 0:09:26 lr 0.000006	 wd 0.0000	time 0.8006 (0.6278)	loss 0.8826 (1.2177)	grad_norm 4.5783 (inf)	loss_scale 1024.0000 (1224.8345)	mem 20118MB
[2024-05-28 20:40:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1700/2502]	eta 0:08:31 lr 0.000006	 wd 0.0000	time 0.6342 (0.6379)	loss 1.2731 (1.2162)	grad_norm 3.9443 (inf)	loss_scale 1024.0000 (1213.0276)	mem 20118MB
[2024-05-28 20:41:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1800/2502]	eta 0:07:33 lr 0.000006	 wd 0.0000	time 0.5802 (0.6466)	loss 0.8315 (1.2173)	grad_norm 3.1797 (inf)	loss_scale 1024.0000 (1202.5319)	mem 20118MB
[2024-05-28 20:42:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][1900/2502]	eta 0:06:28 lr 0.000006	 wd 0.0000	time 0.5097 (0.6457)	loss 0.8996 (1.2175)	grad_norm 5.7822 (inf)	loss_scale 1024.0000 (1193.1405)	mem 20118MB
[2024-05-28 20:43:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][2000/2502]	eta 0:05:22 lr 0.000006	 wd 0.0000	time 0.5282 (0.6423)	loss 1.0584 (1.2166)	grad_norm 4.2764 (inf)	loss_scale 1024.0000 (1184.6877)	mem 20118MB
[2024-05-28 20:44:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][2100/2502]	eta 0:04:18 lr 0.000006	 wd 0.0000	time 0.6403 (0.6434)	loss 1.1365 (1.2158)	grad_norm 4.5002 (inf)	loss_scale 1024.0000 (1177.0395)	mem 20118MB
[2024-05-28 20:45:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][2200/2502]	eta 0:03:14 lr 0.000006	 wd 0.0000	time 0.6711 (0.6425)	loss 1.4543 (1.2160)	grad_norm 3.0571 (inf)	loss_scale 1024.0000 (1170.0863)	mem 20118MB
[2024-05-28 20:46:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][2300/2502]	eta 0:02:09 lr 0.000006	 wd 0.0000	time 0.5252 (0.6407)	loss 1.0614 (1.2147)	grad_norm 3.1426 (inf)	loss_scale 1024.0000 (1163.7375)	mem 20118MB
[2024-05-28 20:47:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][2400/2502]	eta 0:01:05 lr 0.000006	 wd 0.0000	time 0.8453 (0.6433)	loss 1.2938 (1.2132)	grad_norm 2.5126 (inf)	loss_scale 1024.0000 (1157.9175)	mem 20118MB
[2024-05-28 20:48:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0000	time 0.5098 (0.6407)	loss 1.3300 (1.2126)	grad_norm 3.0414 (inf)	loss_scale 1024.0000 (1152.5630)	mem 20118MB
[2024-05-28 20:48:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 23 training takes 0:26:54
[2024-05-28 20:49:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 53.541 (53.541)	Loss 0.3501 (0.3501)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 20:50:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.144 Acc@5 97.876
[2024-05-28 20:50:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 20:50:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.23%
[2024-05-28 20:50:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][0/2502]	eta 1 day, 5:31:31 lr 0.000006	 wd 0.0000	time 42.4826 (42.4826)	loss 1.1259 (1.1259)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:51:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][100/2502]	eta 0:38:28 lr 0.000006	 wd 0.0000	time 0.5402 (0.9612)	loss 1.1545 (1.2257)	grad_norm 4.7732 (4.9291)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:52:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][200/2502]	eta 0:29:01 lr 0.000006	 wd 0.0000	time 0.5126 (0.7563)	loss 1.1188 (1.2105)	grad_norm 3.0381 (4.7453)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:53:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][300/2502]	eta 0:25:11 lr 0.000006	 wd 0.0000	time 0.5312 (0.6863)	loss 0.9839 (1.2066)	grad_norm 3.9983 (4.6368)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:54:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][400/2502]	eta 0:22:48 lr 0.000005	 wd 0.0000	time 0.5239 (0.6509)	loss 1.3857 (1.2130)	grad_norm 21.3243 (4.7308)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:55:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][500/2502]	eta 0:20:59 lr 0.000005	 wd 0.0000	time 0.5170 (0.6293)	loss 1.3552 (1.2176)	grad_norm 3.6092 (4.6420)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:56:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][600/2502]	eta 0:19:30 lr 0.000005	 wd 0.0000	time 0.5198 (0.6152)	loss 1.1026 (1.2191)	grad_norm 4.1358 (4.5967)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:57:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][700/2502]	eta 0:18:32 lr 0.000005	 wd 0.0000	time 0.5265 (0.6172)	loss 1.3655 (1.2154)	grad_norm 3.0406 (4.5840)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:58:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][800/2502]	eta 0:17:24 lr 0.000005	 wd 0.0000	time 0.5606 (0.6138)	loss 1.4732 (1.2167)	grad_norm 4.4635 (4.5739)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 20:59:29 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][900/2502]	eta 0:16:24 lr 0.000005	 wd 0.0000	time 0.6432 (0.6143)	loss 1.3590 (1.2137)	grad_norm 2.5304 (4.5114)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:00:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1000/2502]	eta 0:15:43 lr 0.000005	 wd 0.0000	time 10.8824 (0.6284)	loss 1.4539 (1.2125)	grad_norm 5.1893 (4.4609)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:01:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1100/2502]	eta 0:14:43 lr 0.000005	 wd 0.0000	time 0.5206 (0.6302)	loss 1.1752 (1.2144)	grad_norm 4.8539 (4.4340)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:02:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1200/2502]	eta 0:13:43 lr 0.000005	 wd 0.0000	time 0.5445 (0.6324)	loss 0.9771 (1.2105)	grad_norm 3.9343 (4.4282)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:04:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1300/2502]	eta 0:12:43 lr 0.000005	 wd 0.0000	time 0.5332 (0.6350)	loss 0.8165 (1.2106)	grad_norm 5.2742 (4.3815)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:05:01 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1400/2502]	eta 0:11:36 lr 0.000005	 wd 0.0000	time 0.5627 (0.6322)	loss 1.3585 (1.2104)	grad_norm 3.8790 (4.3574)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:06:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1500/2502]	eta 0:10:44 lr 0.000005	 wd 0.0000	time 0.5539 (0.6430)	loss 1.1431 (1.2114)	grad_norm 4.4349 (4.3373)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:07:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1600/2502]	eta 0:09:40 lr 0.000005	 wd 0.0000	time 0.5563 (0.6434)	loss 0.8714 (1.2112)	grad_norm 4.2643 (4.3569)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:08:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1700/2502]	eta 0:08:33 lr 0.000005	 wd 0.0000	time 0.5360 (0.6397)	loss 1.0434 (1.2129)	grad_norm 2.9534 (4.3522)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:09:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1800/2502]	eta 0:07:37 lr 0.000005	 wd 0.0000	time 0.5260 (0.6518)	loss 0.8618 (1.2138)	grad_norm 4.0667 (4.3441)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:10:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][1900/2502]	eta 0:06:31 lr 0.000005	 wd 0.0000	time 0.5275 (0.6506)	loss 1.2785 (1.2140)	grad_norm 2.4450 (4.3363)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:11:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][2000/2502]	eta 0:05:24 lr 0.000004	 wd 0.0000	time 0.6254 (0.6472)	loss 1.2834 (1.2148)	grad_norm 4.3033 (4.3112)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:12:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][2100/2502]	eta 0:04:20 lr 0.000004	 wd 0.0000	time 0.5276 (0.6474)	loss 0.8938 (1.2157)	grad_norm 3.5324 (4.2979)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:14:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][2200/2502]	eta 0:03:15 lr 0.000004	 wd 0.0000	time 0.5192 (0.6475)	loss 1.0932 (1.2148)	grad_norm 4.7786 (4.2894)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:14:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][2300/2502]	eta 0:02:10 lr 0.000004	 wd 0.0000	time 0.5121 (0.6445)	loss 0.9339 (1.2161)	grad_norm 3.1422 (4.3131)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:16:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][2400/2502]	eta 0:01:06 lr 0.000004	 wd 0.0000	time 0.5331 (0.6471)	loss 1.4489 (1.2161)	grad_norm 4.2938 (4.3107)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:17:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0000	time 0.5161 (0.6436)	loss 1.1526 (1.2168)	grad_norm 2.9587 (4.3093)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:17:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 24 training takes 0:27:04
[2024-05-28 21:18:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 72.513 (72.513)	Loss 0.3538 (0.3538)	Acc@1 93.555 (93.555)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 21:19:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.232 Acc@5 97.890
[2024-05-28 21:19:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 21:19:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.23%
[2024-05-28 21:19:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][0/2502]	eta 16:46:27 lr 0.000004	 wd 0.0000	time 24.1356 (24.1356)	loss 0.7629 (0.7629)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:20:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][100/2502]	eta 0:31:18 lr 0.000004	 wd 0.0000	time 0.5325 (0.7822)	loss 1.2979 (1.1940)	grad_norm 3.8051 (3.8639)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:21:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][200/2502]	eta 0:25:26 lr 0.000004	 wd 0.0000	time 0.5180 (0.6633)	loss 0.9491 (1.1999)	grad_norm 4.5431 (3.9317)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:22:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][300/2502]	eta 0:22:52 lr 0.000004	 wd 0.0000	time 0.5318 (0.6234)	loss 1.3599 (1.1997)	grad_norm 3.0093 (4.1052)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:23:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][400/2502]	eta 0:21:08 lr 0.000004	 wd 0.0000	time 0.5162 (0.6037)	loss 1.0784 (1.1971)	grad_norm 6.5225 (4.2311)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 21:24:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][500/2502]	eta 0:19:45 lr 0.000004	 wd 0.0000	time 0.5209 (0.5920)	loss 1.2308 (1.1995)	grad_norm 4.4306 (4.3575)	loss_scale 2048.0000 (1118.0200)	mem 20118MB
[2024-05-28 21:24:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][600/2502]	eta 0:18:30 lr 0.000004	 wd 0.0000	time 0.5173 (0.5841)	loss 1.2860 (1.2041)	grad_norm 2.7937 (4.3215)	loss_scale 2048.0000 (1272.7587)	mem 20118MB
[2024-05-28 21:25:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][700/2502]	eta 0:17:38 lr 0.000004	 wd 0.0000	time 0.5247 (0.5876)	loss 1.4083 (1.2058)	grad_norm 4.3077 (4.3950)	loss_scale 2048.0000 (1383.3495)	mem 20118MB
[2024-05-28 21:27:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][800/2502]	eta 0:16:53 lr 0.000004	 wd 0.0000	time 0.5156 (0.5955)	loss 1.4271 (1.2041)	grad_norm 3.4506 (4.3580)	loss_scale 2048.0000 (1466.3271)	mem 20118MB
[2024-05-28 21:28:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][900/2502]	eta 0:15:54 lr 0.000004	 wd 0.0000	time 0.5514 (0.5955)	loss 1.4525 (1.2076)	grad_norm 4.0515 (4.3102)	loss_scale 2048.0000 (1530.8857)	mem 20118MB
[2024-05-28 21:29:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1000/2502]	eta 0:15:11 lr 0.000004	 wd 0.0000	time 0.5438 (0.6069)	loss 1.0713 (1.2085)	grad_norm 2.8922 (4.3877)	loss_scale 2048.0000 (1582.5455)	mem 20118MB
[2024-05-28 21:30:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1100/2502]	eta 0:14:34 lr 0.000004	 wd 0.0000	time 0.6119 (0.6237)	loss 1.3874 (1.2065)	grad_norm 3.4250 (4.3709)	loss_scale 2048.0000 (1624.8211)	mem 20118MB
[2024-05-28 21:31:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1200/2502]	eta 0:13:29 lr 0.000004	 wd 0.0000	time 0.5471 (0.6218)	loss 1.4201 (1.2078)	grad_norm 3.8355 (4.3458)	loss_scale 2048.0000 (1660.0566)	mem 20118MB
[2024-05-28 21:32:51 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1300/2502]	eta 0:12:44 lr 0.000003	 wd 0.0000	time 0.5584 (0.6363)	loss 1.1501 (1.2063)	grad_norm 4.0661 (4.3144)	loss_scale 2048.0000 (1689.8755)	mem 20118MB
[2024-05-28 21:33:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1400/2502]	eta 0:11:42 lr 0.000003	 wd 0.0000	time 0.5582 (0.6370)	loss 1.3091 (1.2076)	grad_norm 3.5546 (4.3417)	loss_scale 2048.0000 (1715.4375)	mem 20118MB
[2024-05-28 21:34:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1500/2502]	eta 0:10:35 lr 0.000003	 wd 0.0000	time 0.5463 (0.6341)	loss 1.2151 (1.2073)	grad_norm 6.6947 (4.3647)	loss_scale 2048.0000 (1737.5936)	mem 20118MB
[2024-05-28 21:36:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1600/2502]	eta 0:09:44 lr 0.000003	 wd 0.0000	time 0.5402 (0.6475)	loss 0.9782 (1.2071)	grad_norm 3.6995 (4.3810)	loss_scale 2048.0000 (1756.9819)	mem 20118MB
[2024-05-28 21:37:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1700/2502]	eta 0:08:39 lr 0.000003	 wd 0.0000	time 0.5338 (0.6483)	loss 0.8365 (1.2073)	grad_norm 4.0568 (4.3616)	loss_scale 2048.0000 (1774.0905)	mem 20118MB
[2024-05-28 21:38:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1800/2502]	eta 0:07:41 lr 0.000003	 wd 0.0000	time 0.5508 (0.6571)	loss 1.2551 (1.2070)	grad_norm 4.2333 (4.3583)	loss_scale 2048.0000 (1789.2993)	mem 20118MB
[2024-05-28 21:39:56 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][1900/2502]	eta 0:06:36 lr 0.000003	 wd 0.0000	time 0.6513 (0.6587)	loss 1.3453 (1.2071)	grad_norm 3.1015 (4.3539)	loss_scale 2048.0000 (1802.9079)	mem 20118MB
[2024-05-28 21:41:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][2000/2502]	eta 0:05:31 lr 0.000003	 wd 0.0000	time 0.5330 (0.6598)	loss 1.3189 (1.2064)	grad_norm 3.6457 (4.3602)	loss_scale 2048.0000 (1815.1564)	mem 20118MB
[2024-05-28 21:42:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][2100/2502]	eta 0:04:26 lr 0.000003	 wd 0.0000	time 0.5187 (0.6621)	loss 1.1338 (1.2063)	grad_norm 3.4555 (4.3467)	loss_scale 2048.0000 (1826.2389)	mem 20118MB
[2024-05-28 21:43:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][2200/2502]	eta 0:03:19 lr 0.000003	 wd 0.0000	time 0.5546 (0.6591)	loss 0.9718 (1.2068)	grad_norm 20.5872 (4.3608)	loss_scale 2048.0000 (1836.3144)	mem 20118MB
[2024-05-28 21:44:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][2300/2502]	eta 0:02:12 lr 0.000003	 wd 0.0000	time 0.5764 (0.6581)	loss 1.4375 (1.2073)	grad_norm 4.4444 (4.3801)	loss_scale 2048.0000 (1845.5141)	mem 20118MB
[2024-05-28 21:45:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][2400/2502]	eta 0:01:07 lr 0.000003	 wd 0.0000	time 0.6087 (0.6653)	loss 1.1483 (1.2083)	grad_norm 2.9149 (4.3822)	loss_scale 2048.0000 (1853.9475)	mem 20118MB
[2024-05-28 21:46:38 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0000	time 0.5131 (0.6614)	loss 1.1207 (1.2078)	grad_norm 3.2485 (4.3700)	loss_scale 2048.0000 (1861.7065)	mem 20118MB
[2024-05-28 21:46:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 25 training takes 0:27:45
[2024-05-28 21:47:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 67.166 (67.166)	Loss 0.3574 (0.3574)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 20118MB
[2024-05-28 21:48:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.236 Acc@5 97.876
[2024-05-28 21:48:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 21:48:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.24%
[2024-05-28 21:48:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][0/2502]	eta 1 day, 0:03:05 lr 0.000003	 wd 0.0000	time 34.6066 (34.6066)	loss 1.3384 (1.3384)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 21:49:48 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][100/2502]	eta 0:35:21 lr 0.000003	 wd 0.0000	time 0.5222 (0.8834)	loss 1.1641 (1.2377)	grad_norm 2.6261 (4.0800)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 21:50:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][200/2502]	eta 0:27:26 lr 0.000003	 wd 0.0000	time 0.5280 (0.7153)	loss 1.3145 (1.2228)	grad_norm 7.2481 (4.1653)	loss_scale 2048.0000 (2048.0000)	mem 20118MB
[2024-05-28 21:51:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][300/2502]	eta 0:24:10 lr 0.000003	 wd 0.0000	time 0.5431 (0.6585)	loss 1.3781 (1.2171)	grad_norm 3.7940 (inf)	loss_scale 1024.0000 (2013.9801)	mem 20118MB
[2024-05-28 21:52:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][400/2502]	eta 0:22:05 lr 0.000003	 wd 0.0000	time 0.5333 (0.6307)	loss 1.6209 (1.2076)	grad_norm 3.6132 (inf)	loss_scale 1024.0000 (1767.1022)	mem 20118MB
[2024-05-28 21:53:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][500/2502]	eta 0:20:28 lr 0.000003	 wd 0.0000	time 0.5134 (0.6138)	loss 1.2112 (1.2058)	grad_norm 5.4005 (inf)	loss_scale 512.0000 (1579.9441)	mem 20118MB
[2024-05-28 21:54:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][600/2502]	eta 0:19:06 lr 0.000003	 wd 0.0000	time 0.5177 (0.6026)	loss 1.3891 (1.2096)	grad_norm 9.1080 (inf)	loss_scale 512.0000 (1402.2496)	mem 20118MB
[2024-05-28 21:55:40 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][700/2502]	eta 0:18:52 lr 0.000003	 wd 0.0000	time 0.5262 (0.6284)	loss 1.1402 (1.2125)	grad_norm 4.3010 (inf)	loss_scale 512.0000 (1275.2525)	mem 20118MB
[2024-05-28 21:56:59 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][800/2502]	eta 0:18:24 lr 0.000002	 wd 0.0000	time 0.5478 (0.6488)	loss 1.5011 (1.2134)	grad_norm 4.1403 (inf)	loss_scale 512.0000 (1179.9650)	mem 20118MB
[2024-05-28 21:58:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][900/2502]	eta 0:18:12 lr 0.000002	 wd 0.0000	time 0.5075 (0.6819)	loss 1.3427 (1.2122)	grad_norm 3.6728 (inf)	loss_scale 512.0000 (1105.8291)	mem 20118MB
[2024-05-28 21:59:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1000/2502]	eta 0:16:43 lr 0.000002	 wd 0.0000	time 0.5469 (0.6682)	loss 1.2186 (1.2079)	grad_norm 3.4715 (inf)	loss_scale 512.0000 (1046.5055)	mem 20118MB
[2024-05-28 22:00:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1100/2502]	eta 0:15:20 lr 0.000002	 wd 0.0000	time 0.5270 (0.6569)	loss 1.1324 (1.2084)	grad_norm 7.9946 (inf)	loss_scale 512.0000 (997.9582)	mem 20118MB
[2024-05-28 22:01:17 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1200/2502]	eta 0:14:03 lr 0.000002	 wd 0.0000	time 0.5106 (0.6475)	loss 0.7289 (1.2090)	grad_norm 5.4751 (inf)	loss_scale 512.0000 (957.4954)	mem 20118MB
[2024-05-28 22:02:11 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1300/2502]	eta 0:12:48 lr 0.000002	 wd 0.0000	time 0.5377 (0.6396)	loss 1.3616 (1.2085)	grad_norm 3.5394 (inf)	loss_scale 512.0000 (923.2529)	mem 20118MB
[2024-05-28 22:03:06 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1400/2502]	eta 0:11:37 lr 0.000002	 wd 0.0000	time 0.5273 (0.6329)	loss 1.2442 (1.2095)	grad_norm 6.5593 (inf)	loss_scale 512.0000 (893.8986)	mem 20118MB
[2024-05-28 22:04:08 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1500/2502]	eta 0:10:33 lr 0.000002	 wd 0.0000	time 0.5615 (0.6324)	loss 0.9356 (1.2078)	grad_norm 4.1356 (inf)	loss_scale 512.0000 (868.4557)	mem 20118MB
[2024-05-28 22:05:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1600/2502]	eta 0:09:35 lr 0.000002	 wd 0.0000	time 0.5359 (0.6375)	loss 0.9080 (1.2059)	grad_norm 5.8692 (inf)	loss_scale 512.0000 (846.1911)	mem 20118MB
[2024-05-28 22:06:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1700/2502]	eta 0:08:29 lr 0.000002	 wd 0.0000	time 0.5294 (0.6352)	loss 1.0764 (1.2062)	grad_norm 3.7262 (inf)	loss_scale 512.0000 (826.5444)	mem 20118MB
[2024-05-28 22:07:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1800/2502]	eta 0:07:29 lr 0.000002	 wd 0.0000	time 0.6072 (0.6400)	loss 1.1701 (1.2081)	grad_norm 5.0566 (inf)	loss_scale 512.0000 (809.0794)	mem 20118MB
[2024-05-28 22:08:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][1900/2502]	eta 0:06:25 lr 0.000002	 wd 0.0000	time 0.5237 (0.6406)	loss 0.8763 (1.2083)	grad_norm 2.8168 (inf)	loss_scale 512.0000 (793.4519)	mem 20118MB
[2024-05-28 22:09:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][2000/2502]	eta 0:05:20 lr 0.000002	 wd 0.0000	time 0.5583 (0.6381)	loss 1.1017 (1.2084)	grad_norm 3.4532 (inf)	loss_scale 512.0000 (779.3863)	mem 20118MB
[2024-05-28 22:10:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][2100/2502]	eta 0:04:16 lr 0.000002	 wd 0.0000	time 0.5513 (0.6387)	loss 1.3301 (1.2084)	grad_norm 4.8072 (inf)	loss_scale 512.0000 (766.6597)	mem 20118MB
[2024-05-28 22:11:55 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][2200/2502]	eta 0:03:14 lr 0.000002	 wd 0.0000	time 0.5911 (0.6431)	loss 0.8738 (1.2083)	grad_norm 3.3470 (inf)	loss_scale 512.0000 (755.0895)	mem 20118MB
[2024-05-28 22:12:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][2300/2502]	eta 0:02:09 lr 0.000002	 wd 0.0000	time 0.5294 (0.6410)	loss 1.0376 (1.2083)	grad_norm 4.0117 (inf)	loss_scale 512.0000 (744.5250)	mem 20118MB
[2024-05-28 22:13:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][2400/2502]	eta 0:01:05 lr 0.000002	 wd 0.0000	time 0.5176 (0.6404)	loss 1.1531 (1.2084)	grad_norm 2.9160 (inf)	loss_scale 512.0000 (734.8405)	mem 20118MB
[2024-05-28 22:14:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0000	time 0.5319 (0.6375)	loss 1.4028 (1.2089)	grad_norm 3.7829 (inf)	loss_scale 512.0000 (725.9304)	mem 20118MB
[2024-05-28 22:15:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 26 training takes 0:26:56
[2024-05-28 22:16:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 58.963 (58.963)	Loss 0.3552 (0.3552)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 22:16:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.216 Acc@5 97.874
[2024-05-28 22:16:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 22:16:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.24%
[2024-05-28 22:17:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][0/2502]	eta 18:07:20 lr 0.000002	 wd 0.0000	time 26.0754 (26.0754)	loss 1.1903 (1.1903)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:18:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][100/2502]	eta 0:31:55 lr 0.000002	 wd 0.0000	time 0.5254 (0.7975)	loss 0.9776 (1.1703)	grad_norm 3.6251 (4.6951)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:18:54 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][200/2502]	eta 0:25:48 lr 0.000002	 wd 0.0000	time 0.5217 (0.6726)	loss 1.3687 (1.1807)	grad_norm 3.8501 (4.4531)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:19:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][300/2502]	eta 0:23:07 lr 0.000002	 wd 0.0000	time 0.5197 (0.6299)	loss 1.2941 (1.1957)	grad_norm 3.6100 (4.3882)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:20:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][400/2502]	eta 0:21:19 lr 0.000002	 wd 0.0000	time 0.5207 (0.6086)	loss 1.4359 (1.1973)	grad_norm 4.7572 (4.4303)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:21:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][500/2502]	eta 0:19:59 lr 0.000002	 wd 0.0000	time 0.5657 (0.5989)	loss 1.2140 (1.1967)	grad_norm 3.5557 (4.2829)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:22:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][600/2502]	eta 0:19:41 lr 0.000002	 wd 0.0000	time 0.5204 (0.6212)	loss 1.3005 (1.1972)	grad_norm 2.7326 (4.3312)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:24:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][700/2502]	eta 0:19:25 lr 0.000002	 wd 0.0000	time 0.5366 (0.6468)	loss 0.8430 (1.1976)	grad_norm 4.3553 (4.3008)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:25:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][800/2502]	eta 0:19:01 lr 0.000002	 wd 0.0000	time 0.5781 (0.6704)	loss 1.5781 (1.1973)	grad_norm 8.3652 (4.3086)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:26:37 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][900/2502]	eta 0:17:43 lr 0.000001	 wd 0.0000	time 0.5176 (0.6637)	loss 1.2567 (1.1958)	grad_norm 7.4941 (4.3356)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:27:32 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1000/2502]	eta 0:16:18 lr 0.000001	 wd 0.0000	time 0.5306 (0.6517)	loss 1.3151 (1.1960)	grad_norm 3.0294 (4.3532)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:28:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1100/2502]	eta 0:15:00 lr 0.000001	 wd 0.0000	time 0.5389 (0.6420)	loss 1.0663 (1.1951)	grad_norm 4.9684 (4.3896)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:29:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1200/2502]	eta 0:13:45 lr 0.000001	 wd 0.0000	time 0.5256 (0.6339)	loss 1.3388 (1.1925)	grad_norm 3.1426 (4.3741)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:30:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1300/2502]	eta 0:12:33 lr 0.000001	 wd 0.0000	time 0.5349 (0.6272)	loss 1.1600 (1.1954)	grad_norm 3.8572 (4.3987)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:31:10 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1400/2502]	eta 0:11:24 lr 0.000001	 wd 0.0000	time 0.5150 (0.6213)	loss 1.1909 (1.1962)	grad_norm 2.5648 (4.3918)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:32:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1500/2502]	eta 0:10:23 lr 0.000001	 wd 0.0000	time 0.6520 (0.6219)	loss 1.3155 (1.1980)	grad_norm 3.9552 (4.3713)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:33:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1600/2502]	eta 0:09:32 lr 0.000001	 wd 0.0000	time 0.5512 (0.6343)	loss 0.9570 (1.1967)	grad_norm 4.1517 (4.3760)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:34:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1700/2502]	eta 0:08:27 lr 0.000001	 wd 0.0000	time 0.5373 (0.6329)	loss 1.3382 (1.1976)	grad_norm 6.5472 (4.3791)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:35:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1800/2502]	eta 0:07:26 lr 0.000001	 wd 0.0000	time 0.5472 (0.6357)	loss 1.3358 (1.1978)	grad_norm 3.1096 (4.3794)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:37:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][1900/2502]	eta 0:06:28 lr 0.000001	 wd 0.0000	time 0.5182 (0.6458)	loss 1.2083 (1.1982)	grad_norm 4.6728 (4.3525)	loss_scale 512.0000 (512.0000)	mem 20118MB
[2024-05-28 22:38:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][2000/2502]	eta 0:05:24 lr 0.000001	 wd 0.0000	time 0.5411 (0.6473)	loss 1.4398 (1.1991)	grad_norm 4.3839 (4.3475)	loss_scale 1024.0000 (522.2349)	mem 20118MB
[2024-05-28 22:39:16 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][2100/2502]	eta 0:04:19 lr 0.000001	 wd 0.0000	time 0.5359 (0.6457)	loss 0.9952 (1.2006)	grad_norm 3.6169 (4.3380)	loss_scale 1024.0000 (546.1171)	mem 20118MB
[2024-05-28 22:40:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][2200/2502]	eta 0:03:15 lr 0.000001	 wd 0.0000	time 0.5373 (0.6462)	loss 0.9927 (1.2006)	grad_norm 4.6144 (4.3290)	loss_scale 1024.0000 (567.8292)	mem 20118MB
[2024-05-28 22:41:21 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][2300/2502]	eta 0:02:10 lr 0.000001	 wd 0.0000	time 0.6144 (0.6441)	loss 1.0577 (1.2009)	grad_norm 3.9030 (4.3201)	loss_scale 1024.0000 (587.6541)	mem 20118MB
[2024-05-28 22:42:27 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][2400/2502]	eta 0:01:05 lr 0.000001	 wd 0.0000	time 0.5497 (0.6445)	loss 1.3799 (1.2020)	grad_norm 4.0932 (4.3421)	loss_scale 1024.0000 (605.8276)	mem 20118MB
[2024-05-28 22:43:22 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0000	time 0.5141 (0.6409)	loss 1.4611 (1.2021)	grad_norm 4.6149 (4.3426)	loss_scale 1024.0000 (622.5478)	mem 20118MB
[2024-05-28 22:43:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 27 training takes 0:27:03
[2024-05-28 22:45:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 77.102 (77.102)	Loss 0.3560 (0.3560)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 22:45:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.214 Acc@5 97.890
[2024-05-28 22:45:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 22:45:24 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.24%
[2024-05-28 22:45:57 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][0/2502]	eta 22:26:18 lr 0.000001	 wd 0.0000	time 32.2857 (32.2857)	loss 1.2457 (1.2457)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:46:53 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][100/2502]	eta 0:34:58 lr 0.000001	 wd 0.0000	time 0.5174 (0.8735)	loss 1.3518 (1.2292)	grad_norm 10.8150 (4.6121)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:47:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][200/2502]	eta 0:27:13 lr 0.000001	 wd 0.0000	time 0.5207 (0.7095)	loss 1.2853 (1.2165)	grad_norm 2.5966 (4.3007)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:48:41 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][300/2502]	eta 0:24:00 lr 0.000001	 wd 0.0000	time 0.5139 (0.6542)	loss 0.8932 (1.2211)	grad_norm 2.9659 (4.3276)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:49:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][400/2502]	eta 0:21:56 lr 0.000001	 wd 0.0000	time 0.5439 (0.6263)	loss 0.9224 (1.2201)	grad_norm 5.2404 (4.2470)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:50:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][500/2502]	eta 0:20:20 lr 0.000001	 wd 0.0000	time 0.5113 (0.6098)	loss 1.3573 (1.2153)	grad_norm 3.5980 (4.2848)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:51:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][600/2502]	eta 0:19:10 lr 0.000001	 wd 0.0000	time 0.6660 (0.6050)	loss 1.1769 (1.2116)	grad_norm 3.0046 (4.2672)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:52:30 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][700/2502]	eta 0:18:14 lr 0.000001	 wd 0.0000	time 0.5272 (0.6073)	loss 0.8076 (1.2153)	grad_norm 5.3443 (4.3067)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:54:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][800/2502]	eta 0:18:19 lr 0.000001	 wd 0.0000	time 0.5799 (0.6463)	loss 1.1528 (1.2137)	grad_norm 3.2913 (4.2546)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:55:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][900/2502]	eta 0:17:19 lr 0.000001	 wd 0.0000	time 0.5089 (0.6490)	loss 1.0249 (1.2108)	grad_norm 5.3294 (4.3186)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:56:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1000/2502]	eta 0:15:59 lr 0.000001	 wd 0.0000	time 0.5097 (0.6385)	loss 1.1292 (1.2091)	grad_norm 3.6145 (4.3096)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:56:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1100/2502]	eta 0:14:43 lr 0.000001	 wd 0.0000	time 0.5272 (0.6299)	loss 1.3519 (1.2101)	grad_norm 3.6125 (4.2812)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:57:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1200/2502]	eta 0:13:30 lr 0.000001	 wd 0.0000	time 0.5237 (0.6227)	loss 1.1755 (1.2100)	grad_norm 5.5261 (4.2445)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:58:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1300/2502]	eta 0:12:21 lr 0.000001	 wd 0.0000	time 0.5241 (0.6166)	loss 0.9264 (1.2094)	grad_norm 4.2133 (4.2260)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 22:59:43 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1400/2502]	eta 0:11:15 lr 0.000001	 wd 0.0000	time 0.7193 (0.6131)	loss 1.2947 (1.2101)	grad_norm 3.2548 (4.2173)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:00:49 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1500/2502]	eta 0:10:16 lr 0.000001	 wd 0.0000	time 0.5583 (0.6157)	loss 0.7723 (1.2098)	grad_norm 4.4986 (4.1936)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:02:02 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1600/2502]	eta 0:09:22 lr 0.000001	 wd 0.0000	time 0.7557 (0.6231)	loss 1.4534 (1.2096)	grad_norm 4.9370 (4.2017)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:03:12 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1700/2502]	eta 0:08:23 lr 0.000001	 wd 0.0000	time 0.6129 (0.6275)	loss 1.3547 (1.2075)	grad_norm 4.6983 (4.2054)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:04:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1800/2502]	eta 0:07:20 lr 0.000001	 wd 0.0000	time 0.5333 (0.6273)	loss 1.5598 (1.2068)	grad_norm 4.8515 (4.2343)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:05:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][1900/2502]	eta 0:06:18 lr 0.000001	 wd 0.0000	time 0.5441 (0.6289)	loss 1.1252 (1.2067)	grad_norm 4.4014 (4.2157)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:06:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][2000/2502]	eta 0:05:18 lr 0.000001	 wd 0.0000	time 0.6448 (0.6340)	loss 1.1245 (1.2060)	grad_norm 4.6412 (4.2098)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:07:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][2100/2502]	eta 0:04:14 lr 0.000001	 wd 0.0000	time 0.5460 (0.6326)	loss 1.0921 (1.2061)	grad_norm 2.8928 (4.2363)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:08:42 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][2200/2502]	eta 0:03:11 lr 0.000001	 wd 0.0000	time 0.5502 (0.6348)	loss 1.3157 (1.2054)	grad_norm 5.6683 (4.2614)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:09:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][2300/2502]	eta 0:02:08 lr 0.000001	 wd 0.0000	time 0.6224 (0.6342)	loss 1.3288 (1.2051)	grad_norm 4.8059 (4.2558)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:10:44 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][2400/2502]	eta 0:01:04 lr 0.000001	 wd 0.0000	time 0.6194 (0.6327)	loss 0.9489 (1.2046)	grad_norm 2.4690 (4.2644)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:11:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0000	time 0.5358 (0.6295)	loss 1.2664 (1.2040)	grad_norm 2.8323 (4.2856)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:12:04 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 28 training takes 0:26:39
[2024-05-28 23:13:28 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 84.380 (84.380)	Loss 0.3562 (0.3562)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 23:13:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.226 Acc@5 97.884
[2024-05-28 23:13:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 23:13:50 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.24%
[2024-05-28 23:14:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][0/2502]	eta 1 day, 7:37:06 lr 0.000001	 wd 0.0000	time 45.4941 (45.4941)	loss 0.8474 (0.8474)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:15:31 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][100/2502]	eta 0:40:01 lr 0.000001	 wd 0.0000	time 0.5161 (1.0000)	loss 1.3869 (1.1885)	grad_norm 2.3708 (3.9691)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:16:26 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][200/2502]	eta 0:29:41 lr 0.000001	 wd 0.0000	time 0.5182 (0.7740)	loss 1.2402 (1.2017)	grad_norm 2.7718 (4.1816)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:17:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][300/2502]	eta 0:25:35 lr 0.000001	 wd 0.0000	time 0.5097 (0.6975)	loss 1.3985 (1.2036)	grad_norm 2.8508 (4.0451)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:18:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][400/2502]	eta 0:23:06 lr 0.000001	 wd 0.0000	time 0.5083 (0.6594)	loss 1.2450 (1.2041)	grad_norm 3.7480 (4.0742)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:19:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][500/2502]	eta 0:21:14 lr 0.000001	 wd 0.0000	time 0.5368 (0.6364)	loss 1.2218 (1.2049)	grad_norm 3.8983 (4.0905)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:20:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][600/2502]	eta 0:20:27 lr 0.000000	 wd 0.0000	time 0.5993 (0.6451)	loss 0.8506 (1.2056)	grad_norm 5.7352 (4.1634)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:21:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][700/2502]	eta 0:19:24 lr 0.000000	 wd 0.0000	time 0.6361 (0.6461)	loss 1.4268 (1.2084)	grad_norm 3.1440 (4.1999)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:22:23 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][800/2502]	eta 0:18:09 lr 0.000000	 wd 0.0000	time 0.5328 (0.6404)	loss 0.9786 (1.2094)	grad_norm 3.4170 (4.2239)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:23:33 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][900/2502]	eta 0:17:14 lr 0.000000	 wd 0.0000	time 0.5510 (0.6461)	loss 1.3724 (1.2088)	grad_norm 25.0161 (4.2736)	loss_scale 1024.0000 (1024.0000)	mem 20118MB
[2024-05-28 23:24:52 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1000/2502]	eta 0:16:32 lr 0.000000	 wd 0.0000	time 0.7656 (0.6609)	loss 1.4074 (1.2066)	grad_norm 3.8303 (4.2983)	loss_scale 2048.0000 (1069.0110)	mem 20118MB
[2024-05-28 23:26:25 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1100/2502]	eta 0:16:01 lr 0.000000	 wd 0.0000	time 0.5134 (0.6857)	loss 1.2266 (1.2071)	grad_norm 2.7105 (4.2559)	loss_scale 2048.0000 (1157.9292)	mem 20118MB
[2024-05-28 23:27:20 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1200/2502]	eta 0:14:37 lr 0.000000	 wd 0.0000	time 0.5460 (0.6739)	loss 1.1527 (1.2064)	grad_norm 3.1743 (4.2867)	loss_scale 2048.0000 (1232.0400)	mem 20118MB
[2024-05-28 23:28:14 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1300/2502]	eta 0:13:18 lr 0.000000	 wd 0.0000	time 0.5412 (0.6640)	loss 1.1509 (1.2072)	grad_norm 2.9968 (4.3436)	loss_scale 2048.0000 (1294.7579)	mem 20118MB
[2024-05-28 23:29:09 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1400/2502]	eta 0:12:02 lr 0.000000	 wd 0.0000	time 0.5150 (0.6554)	loss 0.8843 (1.2074)	grad_norm 3.0862 (4.3428)	loss_scale 2048.0000 (1348.5225)	mem 20118MB
[2024-05-28 23:30:03 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1500/2502]	eta 0:10:49 lr 0.000000	 wd 0.0000	time 0.5129 (0.6481)	loss 1.3857 (1.2065)	grad_norm 8.3726 (4.3631)	loss_scale 2048.0000 (1395.1233)	mem 20118MB
[2024-05-28 23:30:58 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1600/2502]	eta 0:09:38 lr 0.000000	 wd 0.0000	time 0.5407 (0.6416)	loss 1.4656 (1.2080)	grad_norm 3.2546 (4.3560)	loss_scale 2048.0000 (1435.9026)	mem 20118MB
[2024-05-28 23:32:00 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1700/2502]	eta 0:08:33 lr 0.000000	 wd 0.0000	time 0.5150 (0.6406)	loss 1.0477 (1.2081)	grad_norm 5.1655 (4.3928)	loss_scale 2048.0000 (1471.8871)	mem 20118MB
[2024-05-28 23:33:07 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1800/2502]	eta 0:07:30 lr 0.000000	 wd 0.0000	time 0.6179 (0.6422)	loss 1.4230 (1.2083)	grad_norm 4.2619 (4.3791)	loss_scale 2048.0000 (1503.8756)	mem 20118MB
[2024-05-28 23:34:05 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][1900/2502]	eta 0:06:24 lr 0.000000	 wd 0.0000	time 0.5439 (0.6390)	loss 1.1924 (1.2084)	grad_norm 4.5405 (4.3668)	loss_scale 2048.0000 (1532.4987)	mem 20118MB
[2024-05-28 23:35:15 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][2000/2502]	eta 0:05:22 lr 0.000000	 wd 0.0000	time 0.8340 (0.6419)	loss 0.8991 (1.2086)	grad_norm 8.0180 (inf)	loss_scale 1024.0000 (1527.5562)	mem 20118MB
[2024-05-28 23:36:19 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][2100/2502]	eta 0:04:18 lr 0.000000	 wd 0.0000	time 0.5760 (0.6419)	loss 1.1245 (1.2090)	grad_norm 4.1524 (inf)	loss_scale 1024.0000 (1503.5888)	mem 20118MB
[2024-05-28 23:37:18 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][2200/2502]	eta 0:03:13 lr 0.000000	 wd 0.0000	time 0.5416 (0.6397)	loss 1.3640 (1.2084)	grad_norm 5.4020 (inf)	loss_scale 1024.0000 (1481.7992)	mem 20118MB
[2024-05-28 23:38:36 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][2300/2502]	eta 0:02:10 lr 0.000000	 wd 0.0000	time 0.5531 (0.6454)	loss 1.1298 (1.2087)	grad_norm 3.0374 (inf)	loss_scale 1024.0000 (1461.9035)	mem 20118MB
[2024-05-28 23:39:39 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][2400/2502]	eta 0:01:05 lr 0.000000	 wd 0.0000	time 0.6074 (0.6449)	loss 1.2784 (1.2090)	grad_norm 8.3673 (inf)	loss_scale 1024.0000 (1443.6651)	mem 20118MB
[2024-05-28 23:40:34 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 226): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0000	time 0.5292 (0.6410)	loss 1.4218 (1.2102)	grad_norm 2.6027 (inf)	loss_scale 1024.0000 (1426.8852)	mem 20118MB
[2024-05-28 23:40:47 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 235): INFO EPOCH 29 training takes 0:26:56
[2024-05-28 23:42:13 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 275): INFO Test: [0/98]	Time 85.954 (85.954)	Loss 0.3562 (0.3562)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 20118MB
[2024-05-28 23:42:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 282): INFO  * Acc@1 86.234 Acc@5 97.888
[2024-05-28 23:42:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 23:42:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 168): INFO Max accuracy: 86.24%
[2024-05-28 23:42:35 swin_mam_v2_large_patch4_window7_224_22kto1k_finetune_8x] (main.py 175): INFO Training time 14:12:32
