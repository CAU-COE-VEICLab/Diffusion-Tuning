[2024-07-11 15:38:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/config.json
[2024-07-11 15:38:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: convnext_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-11 15:38:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/convnext/diffusion_ft_convnext_base_224_22kto1k_sequence_crosslayer_process1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-11 15:38:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 108): INFO Creating model:convnext_diffusion_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune
[2024-07-11 15:38:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 110): INFO ConvNeXt_Diffusion_Finetune(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-11 15:38:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 113): INFO number of params: 88591464
[2024-07-11 15:38:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft, ignoring auto resume
[2024-07-11 15:38:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-11 15:38:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-11 15:38:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth'
[2024-07-11 15:39:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 61.045 (61.045)	Loss 0.3623 (0.3623)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 3484MB
[2024-07-11 15:40:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.608 Acc@5 97.592
[2024-07-11 15:40:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-11 15:40:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 168): INFO Start training
[2024-07-11 15:40:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 13:28:38 lr 0.000100	 wd 0.0000	time 19.3919 (19.3919)	loss 0.7739 (0.7739)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 11634MB
[2024-07-11 15:54:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/config.json
[2024-07-11 15:54:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: convnext_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-11 15:54:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/convnext/diffusion_ft_convnext_base_224_22kto1k_sequence_crosslayer_process1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-11 15:54:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 108): INFO Creating model:convnext_diffusion_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune
[2024-07-11 15:54:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 110): INFO ConvNeXt_Diffusion_Finetune(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-11 15:54:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 113): INFO number of params: 88591464
[2024-07-11 15:54:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft, ignoring auto resume
[2024-07-11 15:54:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-11 15:54:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-11 15:54:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth'
[2024-07-11 15:55:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 57.255 (57.255)	Loss 0.3623 (0.3623)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 3484MB
[2024-07-11 15:55:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.610 Acc@5 97.592
[2024-07-11 15:55:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-11 15:55:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 168): INFO Start training
[2024-07-11 15:56:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 13:57:05 lr 0.000100	 wd 0.0000	time 20.0740 (20.0740)	loss 0.7739 (0.7739)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 11634MB
[2024-07-11 15:57:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:32:40 lr 0.000100	 wd 0.0000	time 0.3047 (0.8163)	loss 0.8340 (0.8111)	grad_norm 3.5384 (nan)	loss_scale 16384.0000 (18168.3960)	mem 11634MB
[2024-07-11 15:57:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:21:58 lr 0.000100	 wd 0.0000	time 0.2963 (0.5729)	loss 0.8003 (0.8200)	grad_norm 3.3641 (nan)	loss_scale 16384.0000 (17280.6368)	mem 11634MB
[2024-07-11 15:58:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:18:02 lr 0.000100	 wd 0.0000	time 0.3173 (0.4917)	loss 0.7852 (0.8242)	grad_norm 2.8516 (nan)	loss_scale 16384.0000 (16982.7508)	mem 11634MB
[2024-07-11 15:58:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:15:46 lr 0.000100	 wd 0.0000	time 0.2733 (0.4501)	loss 1.1064 (0.8270)	grad_norm 2.5570 (nan)	loss_scale 16384.0000 (16833.4364)	mem 11634MB
[2024-07-11 15:59:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:14:11 lr 0.000100	 wd 0.0000	time 0.3052 (0.4254)	loss 0.7075 (0.8275)	grad_norm 3.1444 (nan)	loss_scale 16384.0000 (16743.7285)	mem 11634MB
[2024-07-11 15:59:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:12:58 lr 0.000100	 wd 0.0000	time 0.2858 (0.4093)	loss 0.7241 (0.8270)	grad_norm 2.5034 (nan)	loss_scale 16384.0000 (16683.8735)	mem 11634MB
[2024-07-11 16:00:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:11:55 lr 0.000100	 wd 0.0000	time 0.2986 (0.3973)	loss 0.8677 (0.8275)	grad_norm 2.7897 (nan)	loss_scale 16384.0000 (16641.0956)	mem 11634MB
[2024-07-11 16:00:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:11:00 lr 0.000100	 wd 0.0000	time 0.3234 (0.3883)	loss 0.8721 (0.8271)	grad_norm 2.0611 (nan)	loss_scale 16384.0000 (16608.9988)	mem 11634MB
[2024-07-11 16:01:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:10:11 lr 0.000100	 wd 0.0000	time 0.2852 (0.3817)	loss 0.8037 (0.8268)	grad_norm 2.0702 (nan)	loss_scale 16384.0000 (16584.0266)	mem 11634MB
[2024-07-11 16:01:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:09:25 lr 0.000100	 wd 0.0000	time 0.3108 (0.3762)	loss 0.9102 (0.8269)	grad_norm 2.1446 (nan)	loss_scale 16384.0000 (16564.0440)	mem 11634MB
[2024-07-11 16:02:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:08:40 lr 0.000100	 wd 0.0000	time 0.3257 (0.3716)	loss 0.7124 (0.8268)	grad_norm 2.9543 (nan)	loss_scale 16384.0000 (16547.6912)	mem 11634MB
[2024-07-11 16:03:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:08:00 lr 0.000100	 wd 0.0000	time 0.2989 (0.3691)	loss 0.8022 (0.8270)	grad_norm 2.1780 (nan)	loss_scale 16384.0000 (16534.0616)	mem 11634MB
[2024-07-11 16:03:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:07:19 lr 0.000100	 wd 0.0000	time 0.2886 (0.3657)	loss 0.7285 (0.8281)	grad_norm 2.3755 (nan)	loss_scale 16384.0000 (16522.5273)	mem 11634MB
[2024-07-11 16:04:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:39 lr 0.000100	 wd 0.0000	time 0.3267 (0.3628)	loss 0.8457 (0.8279)	grad_norm 3.0129 (nan)	loss_scale 16384.0000 (16512.6395)	mem 11634MB
[2024-07-11 16:05:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:06:18 lr 0.000100	 wd 0.0000	time 0.3089 (0.3776)	loss 0.8916 (0.8279)	grad_norm 2.2929 (nan)	loss_scale 16384.0000 (16504.0693)	mem 11634MB
[2024-07-11 16:06:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:06:05 lr 0.000100	 wd 0.0000	time 0.2798 (0.4056)	loss 0.8994 (0.8275)	grad_norm 2.4318 (nan)	loss_scale 16384.0000 (16496.5696)	mem 11634MB
[2024-07-11 16:07:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:05:21 lr 0.000100	 wd 0.0000	time 0.3102 (0.4010)	loss 0.7168 (0.8272)	grad_norm 2.0471 (nan)	loss_scale 16384.0000 (16489.9518)	mem 11634MB
[2024-07-11 16:07:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:04:38 lr 0.000100	 wd 0.0000	time 0.3081 (0.3968)	loss 0.7930 (0.8270)	grad_norm 2.0775 (nan)	loss_scale 16384.0000 (16484.0689)	mem 11634MB
[2024-07-11 16:08:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:56 lr 0.000100	 wd 0.0000	time 0.3080 (0.3932)	loss 0.7681 (0.8266)	grad_norm 2.0653 (nan)	loss_scale 16384.0000 (16478.8048)	mem 11634MB
[2024-07-11 16:08:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:03:15 lr 0.000100	 wd 0.0000	time 0.3103 (0.3900)	loss 0.8892 (0.8270)	grad_norm 2.2945 (nan)	loss_scale 16384.0000 (16474.0670)	mem 11634MB
[2024-07-11 16:09:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:35 lr 0.000100	 wd 0.0000	time 0.2946 (0.3870)	loss 0.7334 (0.8273)	grad_norm 2.2468 (nan)	loss_scale 16384.0000 (16469.7801)	mem 11634MB
[2024-07-11 16:09:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:56 lr 0.000100	 wd 0.0000	time 0.3094 (0.3846)	loss 0.8413 (0.8270)	grad_norm 2.4166 (nan)	loss_scale 16384.0000 (16465.8828)	mem 11634MB
[2024-07-11 16:10:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:17 lr 0.000100	 wd 0.0000	time 0.3095 (0.3822)	loss 0.9297 (0.8267)	grad_norm 2.3895 (nan)	loss_scale 16384.0000 (16462.3242)	mem 11634MB
[2024-07-11 16:10:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:38 lr 0.000100	 wd 0.0000	time 0.2692 (0.3799)	loss 0.7476 (0.8266)	grad_norm 3.1314 (nan)	loss_scale 16384.0000 (16459.0621)	mem 11634MB
[2024-07-11 16:11:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000100	 wd 0.0000	time 0.3095 (0.3779)	loss 0.9126 (0.8264)	grad_norm 2.4879 (nan)	loss_scale 16384.0000 (16456.0608)	mem 11634MB
[2024-07-11 16:11:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 0 training takes 0:16:04
[2024-07-11 16:11:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_0.pth saving......
[2024-07-11 16:11:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_0.pth saved !!!
[2024-07-11 16:12:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 69.804 (69.804)	Loss 0.3765 (0.3765)	Acc@1 91.211 (91.211)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 16:13:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.102 Acc@5 97.520
[2024-07-11 16:13:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 16:13:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.10%
[2024-07-11 16:13:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 16:13:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 16:13:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][0/2502]	eta 11:16:38 lr 0.000100	 wd 0.0000	time 16.2266 (16.2266)	loss 0.8301 (0.8301)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:14:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:19:28 lr 0.000100	 wd 0.0000	time 0.3013 (0.4866)	loss 0.7500 (0.8184)	grad_norm 2.7306 (2.1732)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:14:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:15:40 lr 0.000100	 wd 0.0000	time 0.3259 (0.4083)	loss 0.9321 (0.8236)	grad_norm 1.8383 (2.1689)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:15:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:13:59 lr 0.000100	 wd 0.0000	time 0.3196 (0.3811)	loss 0.9023 (0.8199)	grad_norm 2.0757 (2.1826)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:15:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:12:51 lr 0.000100	 wd 0.0000	time 0.3013 (0.3671)	loss 0.6997 (0.8220)	grad_norm 2.3165 (2.1940)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:16:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:11:58 lr 0.000100	 wd 0.0000	time 0.3082 (0.3591)	loss 0.8452 (0.8204)	grad_norm 2.0034 (2.2173)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:16:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:11:12 lr 0.000100	 wd 0.0000	time 0.3009 (0.3534)	loss 0.8833 (0.8216)	grad_norm 1.9794 (2.2342)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:17:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:10:29 lr 0.000100	 wd 0.0000	time 0.2970 (0.3494)	loss 0.7363 (0.8216)	grad_norm 2.2898 (2.2411)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:18:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:50 lr 0.000100	 wd 0.0000	time 0.3197 (0.3468)	loss 0.7183 (0.8218)	grad_norm 2.0066 (2.2338)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:18:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:09:11 lr 0.000099	 wd 0.0000	time 0.3062 (0.3445)	loss 0.8872 (0.8214)	grad_norm 2.0138 (inf)	loss_scale 8192.0000 (16038.4994)	mem 11634MB
[2024-07-11 16:19:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:08:36 lr 0.000099	 wd 0.0000	time 0.2678 (0.3436)	loss 0.8130 (0.8206)	grad_norm 2.1109 (inf)	loss_scale 8192.0000 (15254.6334)	mem 11634MB
[2024-07-11 16:19:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:08:00 lr 0.000099	 wd 0.0000	time 0.3020 (0.3425)	loss 0.9634 (0.8206)	grad_norm 2.1997 (inf)	loss_scale 8192.0000 (14613.1589)	mem 11634MB
[2024-07-11 16:20:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:24 lr 0.000099	 wd 0.0000	time 0.3281 (0.3413)	loss 0.8408 (0.8203)	grad_norm 2.3216 (inf)	loss_scale 8192.0000 (14078.5079)	mem 11634MB
[2024-07-11 16:20:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:48 lr 0.000099	 wd 0.0000	time 0.3073 (0.3402)	loss 0.8950 (0.8210)	grad_norm 1.7489 (inf)	loss_scale 8192.0000 (13626.0477)	mem 11634MB
[2024-07-11 16:21:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:06:29 lr 0.000099	 wd 0.0000	time 0.2935 (0.3538)	loss 0.8369 (0.8215)	grad_norm 1.9910 (inf)	loss_scale 8192.0000 (13238.1784)	mem 11634MB
[2024-07-11 16:22:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:55 lr 0.000099	 wd 0.0000	time 0.3139 (0.3544)	loss 0.8501 (0.8218)	grad_norm 2.3432 (inf)	loss_scale 8192.0000 (12901.9907)	mem 11634MB
[2024-07-11 16:23:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:05:43 lr 0.000099	 wd 0.0000	time 0.6362 (0.3811)	loss 0.9355 (0.8223)	grad_norm 1.9777 (inf)	loss_scale 8192.0000 (12607.8001)	mem 11634MB
[2024-07-11 16:24:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:05:11 lr 0.000099	 wd 0.0000	time 0.3031 (0.3880)	loss 0.7285 (0.8222)	grad_norm 2.4033 (inf)	loss_scale 8192.0000 (12348.1999)	mem 11634MB
[2024-07-11 16:24:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:04:29 lr 0.000099	 wd 0.0000	time 0.2997 (0.3845)	loss 0.9927 (0.8229)	grad_norm 1.7684 (inf)	loss_scale 8192.0000 (12117.4281)	mem 11634MB
[2024-07-11 16:25:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:49 lr 0.000099	 wd 0.0000	time 0.3072 (0.3816)	loss 0.8218 (0.8236)	grad_norm 2.3122 (inf)	loss_scale 8192.0000 (11910.9353)	mem 11634MB
[2024-07-11 16:26:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:03:10 lr 0.000099	 wd 0.0000	time 0.3109 (0.3792)	loss 0.6982 (0.8230)	grad_norm 1.9360 (inf)	loss_scale 8192.0000 (11725.0815)	mem 11634MB
[2024-07-11 16:26:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:31 lr 0.000099	 wd 0.0000	time 0.3041 (0.3768)	loss 0.8066 (0.8232)	grad_norm 2.1736 (inf)	loss_scale 8192.0000 (11556.9196)	mem 11634MB
[2024-07-11 16:27:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:53 lr 0.000099	 wd 0.0000	time 0.3176 (0.3745)	loss 0.7100 (0.8230)	grad_norm 2.4961 (inf)	loss_scale 8192.0000 (11404.0382)	mem 11634MB
[2024-07-11 16:27:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:15 lr 0.000099	 wd 0.0000	time 0.2945 (0.3727)	loss 0.9746 (0.8233)	grad_norm 2.1448 (inf)	loss_scale 8192.0000 (11264.4450)	mem 11634MB
[2024-07-11 16:28:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:37 lr 0.000099	 wd 0.0000	time 0.2965 (0.3708)	loss 0.8462 (0.8230)	grad_norm 1.8411 (inf)	loss_scale 8192.0000 (11136.4798)	mem 11634MB
[2024-07-11 16:28:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000099	 wd 0.0000	time 0.3254 (0.3691)	loss 1.0000 (0.8233)	grad_norm 1.7515 (inf)	loss_scale 8192.0000 (11018.7477)	mem 11634MB
[2024-07-11 16:28:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 1 training takes 0:15:27
[2024-07-11 16:30:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 76.071 (76.071)	Loss 0.3735 (0.3735)	Acc@1 91.406 (91.406)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 16:30:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.108 Acc@5 97.494
[2024-07-11 16:30:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 16:30:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.11%
[2024-07-11 16:30:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 16:30:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 16:31:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][0/2502]	eta 1 day, 1:30:52 lr 0.000099	 wd 0.0000	time 36.7118 (36.7118)	loss 0.7676 (0.7676)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:31:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:27:36 lr 0.000099	 wd 0.0000	time 0.3054 (0.6895)	loss 0.7959 (0.8141)	grad_norm 3.3860 (2.2808)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:32:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:19:29 lr 0.000099	 wd 0.0000	time 0.3018 (0.5080)	loss 0.8120 (0.8089)	grad_norm 1.8119 (2.2061)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:32:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:16:26 lr 0.000099	 wd 0.0000	time 0.3027 (0.4479)	loss 0.8662 (0.8081)	grad_norm 2.3185 (2.2034)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:33:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:14:37 lr 0.000099	 wd 0.0000	time 0.3081 (0.4174)	loss 0.7295 (0.8093)	grad_norm 2.8982 (2.1921)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:33:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:13:18 lr 0.000099	 wd 0.0000	time 0.2843 (0.3990)	loss 0.9927 (0.8074)	grad_norm 2.1665 (2.1793)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:34:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:12:16 lr 0.000099	 wd 0.0000	time 0.2938 (0.3872)	loss 0.8652 (0.8085)	grad_norm 2.3411 (2.1816)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:34:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:11:21 lr 0.000099	 wd 0.0000	time 0.2855 (0.3784)	loss 0.9604 (0.8090)	grad_norm 2.1589 (2.2138)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:35:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:10:32 lr 0.000099	 wd 0.0000	time 0.2863 (0.3718)	loss 0.6968 (0.8082)	grad_norm 2.0993 (2.2051)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:35:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:09:47 lr 0.000098	 wd 0.0000	time 0.2840 (0.3670)	loss 0.7568 (0.8094)	grad_norm 2.4021 (2.2100)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:36:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:09:05 lr 0.000098	 wd 0.0000	time 0.3195 (0.3631)	loss 0.7822 (0.8101)	grad_norm 2.2674 (2.2015)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:37:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:08:24 lr 0.000098	 wd 0.0000	time 0.3050 (0.3597)	loss 0.7856 (0.8102)	grad_norm 1.8059 (2.1974)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:37:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:08:06 lr 0.000098	 wd 0.0000	time 0.2938 (0.3736)	loss 0.9897 (0.8097)	grad_norm 2.4243 (2.1916)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:38:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:07:36 lr 0.000098	 wd 0.0000	time 0.3356 (0.3802)	loss 0.8770 (0.8105)	grad_norm 2.9062 (2.2008)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:39:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:07:22 lr 0.000098	 wd 0.0000	time 0.4643 (0.4018)	loss 0.8115 (0.8112)	grad_norm 1.8726 (2.2052)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:40:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:06:45 lr 0.000098	 wd 0.0000	time 0.3622 (0.4052)	loss 0.7485 (0.8115)	grad_norm 1.7044 (2.2035)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:41:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:06:03 lr 0.000098	 wd 0.0000	time 0.3311 (0.4032)	loss 0.8350 (0.8117)	grad_norm 2.7287 (2.1971)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:41:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:05:19 lr 0.000098	 wd 0.0000	time 0.2945 (0.3987)	loss 0.8809 (0.8120)	grad_norm 2.2444 (2.1895)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:42:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:04:37 lr 0.000098	 wd 0.0000	time 0.3384 (0.3948)	loss 0.6963 (0.8115)	grad_norm 2.4611 (2.1875)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:42:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:55 lr 0.000098	 wd 0.0000	time 0.3000 (0.3915)	loss 0.7412 (0.8121)	grad_norm 2.3527 (2.1928)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:43:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:03:14 lr 0.000098	 wd 0.0000	time 0.2897 (0.3883)	loss 0.8252 (0.8119)	grad_norm 1.9064 (2.1963)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:43:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:34 lr 0.000098	 wd 0.0000	time 0.3372 (0.3855)	loss 0.9277 (0.8122)	grad_norm 1.9248 (2.1939)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:44:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:55 lr 0.000098	 wd 0.0000	time 0.3063 (0.3831)	loss 0.7817 (0.8123)	grad_norm 2.3080 (2.1931)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:45:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:16 lr 0.000098	 wd 0.0000	time 0.2975 (0.3807)	loss 0.8950 (0.8122)	grad_norm 2.3079 (2.1904)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 16:45:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:38 lr 0.000098	 wd 0.0000	time 0.3372 (0.3786)	loss 1.0635 (0.8125)	grad_norm 2.3306 (2.1887)	loss_scale 16384.0000 (8328.4765)	mem 11634MB
[2024-07-11 16:46:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000098	 wd 0.0000	time 0.2919 (0.3861)	loss 0.7793 (0.8131)	grad_norm 2.0847 (2.1893)	loss_scale 16384.0000 (8650.5686)	mem 11634MB
[2024-07-11 16:46:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 2 training takes 0:16:11
[2024-07-11 16:47:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 56.670 (56.670)	Loss 0.3665 (0.3665)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 16:47:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.986 Acc@5 97.466
[2024-07-11 16:47:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-11 16:47:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.11%
[2024-07-11 16:48:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][0/2502]	eta 11:25:55 lr 0.000098	 wd 0.0000	time 16.4492 (16.4492)	loss 0.7271 (0.7271)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:48:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:19:33 lr 0.000098	 wd 0.0000	time 0.3025 (0.4886)	loss 0.7681 (0.8011)	grad_norm 2.0525 (2.0679)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:49:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:15:40 lr 0.000097	 wd 0.0000	time 0.3201 (0.4084)	loss 0.7739 (0.8037)	grad_norm 1.9421 (2.0954)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:49:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:13:58 lr 0.000097	 wd 0.0000	time 0.3128 (0.3807)	loss 0.9233 (0.8065)	grad_norm 2.0480 (2.1172)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:50:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:12:51 lr 0.000097	 wd 0.0000	time 0.2938 (0.3672)	loss 0.8438 (0.8061)	grad_norm 2.2447 (2.1533)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:50:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:11:59 lr 0.000097	 wd 0.0000	time 0.3093 (0.3594)	loss 0.7808 (0.8058)	grad_norm 2.0720 (2.1424)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:51:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:11:12 lr 0.000097	 wd 0.0000	time 0.2988 (0.3536)	loss 0.7612 (0.8051)	grad_norm 2.3801 (2.1458)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:51:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:10:30 lr 0.000097	 wd 0.0000	time 0.3272 (0.3499)	loss 0.9277 (0.8054)	grad_norm 2.2326 (2.1420)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:52:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:50 lr 0.000097	 wd 0.0000	time 0.2975 (0.3468)	loss 0.8926 (0.8064)	grad_norm 2.0290 (2.1443)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:52:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:09:11 lr 0.000097	 wd 0.0000	time 0.3008 (0.3445)	loss 0.8979 (0.8067)	grad_norm 2.5382 (2.1530)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:53:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:08:34 lr 0.000097	 wd 0.0000	time 0.3284 (0.3426)	loss 0.9185 (0.8077)	grad_norm 2.3819 (2.1550)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:54:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:08:40 lr 0.000097	 wd 0.0000	time 0.5664 (0.3711)	loss 0.7617 (0.8093)	grad_norm 2.6201 (2.1661)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:55:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:08:22 lr 0.000097	 wd 0.0000	time 0.3224 (0.3858)	loss 0.9111 (0.8097)	grad_norm 1.9968 (2.1649)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:56:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:07:38 lr 0.000097	 wd 0.0000	time 0.3254 (0.3813)	loss 0.9336 (0.8097)	grad_norm 2.2303 (2.1607)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:56:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:06:55 lr 0.000097	 wd 0.0000	time 0.3039 (0.3773)	loss 0.7754 (0.8105)	grad_norm 2.7740 (2.1619)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:57:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:06:14 lr 0.000097	 wd 0.0000	time 0.3158 (0.3740)	loss 0.7100 (0.8112)	grad_norm 2.2439 (2.1650)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:57:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:05:34 lr 0.000096	 wd 0.0000	time 0.2972 (0.3711)	loss 0.7651 (0.8110)	grad_norm 2.3193 (2.1640)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:58:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:55 lr 0.000096	 wd 0.0000	time 0.3007 (0.3685)	loss 0.8276 (0.8112)	grad_norm 2.2572 (2.1665)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:58:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:04:17 lr 0.000096	 wd 0.0000	time 0.2932 (0.3667)	loss 0.8774 (0.8108)	grad_norm 2.2993 (2.1679)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:59:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:39 lr 0.000096	 wd 0.0000	time 0.2974 (0.3648)	loss 0.8008 (0.8109)	grad_norm 2.3252 (2.1695)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 16:59:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:03:02 lr 0.000096	 wd 0.0000	time 0.2990 (0.3629)	loss 0.8149 (0.8110)	grad_norm 2.1637 (2.1639)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 17:00:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:28 lr 0.000096	 wd 0.0000	time 0.2864 (0.3685)	loss 0.7847 (0.8112)	grad_norm 1.7469 (2.1561)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 17:01:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:51 lr 0.000096	 wd 0.0000	time 0.3316 (0.3691)	loss 0.8594 (0.8111)	grad_norm 2.3500 (2.1494)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 17:02:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:16 lr 0.000096	 wd 0.0000	time 0.3710 (0.3810)	loss 0.8369 (0.8111)	grad_norm 2.3756 (2.1495)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 17:03:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:39 lr 0.000096	 wd 0.0000	time 0.3274 (0.3878)	loss 0.7949 (0.8109)	grad_norm 2.2635 (inf)	loss_scale 8192.0000 (16233.8759)	mem 11634MB
[2024-07-11 17:03:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000096	 wd 0.0000	time 0.3033 (0.3871)	loss 0.9087 (0.8110)	grad_norm 2.1270 (inf)	loss_scale 8192.0000 (15912.3295)	mem 11634MB
[2024-07-11 17:04:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 3 training takes 0:16:14
[2024-07-11 17:04:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 23.763 (23.763)	Loss 0.3889 (0.3889)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 17:04:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.046 Acc@5 97.464
[2024-07-11 17:04:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-11 17:04:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.11%
[2024-07-11 17:04:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][0/2502]	eta 14:33:22 lr 0.000096	 wd 0.0000	time 20.9444 (20.9444)	loss 0.8330 (0.8330)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:05:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:21:21 lr 0.000096	 wd 0.0000	time 0.3057 (0.5335)	loss 0.7334 (0.8012)	grad_norm 2.4062 (2.2392)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:06:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:16:30 lr 0.000096	 wd 0.0000	time 0.3087 (0.4303)	loss 0.9058 (0.8066)	grad_norm 1.9043 (2.1860)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:06:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:14:31 lr 0.000095	 wd 0.0000	time 0.3043 (0.3956)	loss 0.7930 (0.8053)	grad_norm 2.4329 (2.1637)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:07:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:13:15 lr 0.000095	 wd 0.0000	time 0.2873 (0.3784)	loss 0.8813 (0.8028)	grad_norm 1.8660 (2.1361)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:07:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:12:17 lr 0.000095	 wd 0.0000	time 0.3315 (0.3682)	loss 0.8979 (0.8030)	grad_norm 3.4140 (2.1396)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:08:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:11:27 lr 0.000095	 wd 0.0000	time 0.2986 (0.3613)	loss 0.7690 (0.8048)	grad_norm 2.5189 (2.1455)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:08:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:10:41 lr 0.000095	 wd 0.0000	time 0.3125 (0.3561)	loss 0.9233 (0.8037)	grad_norm 2.0568 (2.1542)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:09:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:59 lr 0.000095	 wd 0.0000	time 0.2835 (0.3522)	loss 0.6973 (0.8031)	grad_norm 1.8981 (2.1485)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:09:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:09:19 lr 0.000095	 wd 0.0000	time 0.3092 (0.3492)	loss 0.8281 (0.8037)	grad_norm 2.1876 (2.1495)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:10:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:09:08 lr 0.000095	 wd 0.0000	time 0.3206 (0.3649)	loss 0.7759 (0.8038)	grad_norm 2.0073 (2.1562)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:11:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:08:41 lr 0.000095	 wd 0.0000	time 0.5148 (0.3721)	loss 0.8511 (0.8044)	grad_norm 2.0962 (2.1535)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:12:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:08:16 lr 0.000095	 wd 0.0000	time 0.2892 (0.3811)	loss 0.7300 (0.8047)	grad_norm 1.8867 (2.1462)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:12:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:07:33 lr 0.000095	 wd 0.0000	time 0.3149 (0.3769)	loss 0.6992 (0.8041)	grad_norm 2.1481 (2.1522)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:13:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:06:51 lr 0.000094	 wd 0.0000	time 0.3019 (0.3733)	loss 0.7666 (0.8038)	grad_norm 2.8848 (2.1547)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:13:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:06:10 lr 0.000094	 wd 0.0000	time 0.2798 (0.3702)	loss 0.7236 (0.8040)	grad_norm 1.7549 (2.1488)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:14:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:05:31 lr 0.000094	 wd 0.0000	time 0.2991 (0.3675)	loss 0.8462 (0.8044)	grad_norm 2.0548 (2.1432)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:14:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:52 lr 0.000094	 wd 0.0000	time 0.2939 (0.3651)	loss 0.8364 (0.8048)	grad_norm 1.8777 (2.1476)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:15:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:04:15 lr 0.000094	 wd 0.0000	time 0.3263 (0.3634)	loss 0.8257 (0.8056)	grad_norm 2.4480 (2.1499)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:16:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:37 lr 0.000094	 wd 0.0000	time 0.3237 (0.3615)	loss 0.7632 (0.8053)	grad_norm 1.8511 (2.1478)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:16:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:03:00 lr 0.000094	 wd 0.0000	time 0.3123 (0.3600)	loss 0.7065 (0.8049)	grad_norm 1.9920 (2.1458)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:17:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:27 lr 0.000094	 wd 0.0000	time 0.5668 (0.3674)	loss 0.7705 (0.8052)	grad_norm 2.2256 (2.1490)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:18:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:50 lr 0.000094	 wd 0.0000	time 0.2993 (0.3669)	loss 0.8135 (0.8051)	grad_norm 1.8549 (2.1462)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:19:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:16 lr 0.000094	 wd 0.0000	time 0.2705 (0.3797)	loss 0.7329 (0.8057)	grad_norm 2.5388 (2.1477)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:19:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:38 lr 0.000093	 wd 0.0000	time 0.3370 (0.3784)	loss 0.7285 (0.8059)	grad_norm 1.6971 (2.1505)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:20:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000093	 wd 0.0000	time 0.3183 (0.3853)	loss 0.7061 (0.8059)	grad_norm 2.2426 (2.1475)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:20:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 4 training takes 0:16:09
[2024-07-11 17:21:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 54.133 (54.133)	Loss 0.3655 (0.3655)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 17:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.122 Acc@5 97.472
[2024-07-11 17:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 17:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 17:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 17:21:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 17:22:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][0/2502]	eta 9:48:35 lr 0.000093	 wd 0.0000	time 14.1150 (14.1150)	loss 0.9355 (0.9355)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:22:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:18:34 lr 0.000093	 wd 0.0000	time 0.3257 (0.4642)	loss 0.7534 (0.7995)	grad_norm 1.8823 (2.1135)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:23:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:15:13 lr 0.000093	 wd 0.0000	time 0.2911 (0.3968)	loss 0.7578 (0.8037)	grad_norm 2.1143 (2.1262)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:23:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:13:41 lr 0.000093	 wd 0.0000	time 0.2856 (0.3730)	loss 0.7900 (0.8011)	grad_norm 1.9364 (2.1360)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:24:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:12:38 lr 0.000093	 wd 0.0000	time 0.2891 (0.3609)	loss 0.7266 (0.7996)	grad_norm 2.1300 (2.1393)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:24:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:11:48 lr 0.000093	 wd 0.0000	time 0.2997 (0.3541)	loss 0.8379 (0.7996)	grad_norm 2.6608 (2.1370)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:25:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:11:04 lr 0.000093	 wd 0.0000	time 0.3065 (0.3493)	loss 1.0244 (0.8018)	grad_norm 1.7350 (2.1425)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:25:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:10:23 lr 0.000093	 wd 0.0000	time 0.3000 (0.3460)	loss 0.8276 (0.8016)	grad_norm 2.0169 (2.1482)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:26:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:45 lr 0.000093	 wd 0.0000	time 0.3014 (0.3440)	loss 0.8706 (0.8027)	grad_norm 2.6832 (2.1396)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:27:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:09:07 lr 0.000092	 wd 0.0000	time 0.3033 (0.3418)	loss 0.7617 (0.8029)	grad_norm 2.3026 (2.1514)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:27:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:08:30 lr 0.000092	 wd 0.0000	time 0.2730 (0.3402)	loss 0.7363 (0.8030)	grad_norm 2.2190 (2.1570)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:28:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:08:19 lr 0.000092	 wd 0.0000	time 0.3683 (0.3566)	loss 0.7144 (0.8035)	grad_norm 1.7365 (2.1686)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:29:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:07:57 lr 0.000092	 wd 0.0000	time 0.3306 (0.3664)	loss 0.9033 (0.8035)	grad_norm 2.1189 (2.1717)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:29:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:07:25 lr 0.000092	 wd 0.0000	time 0.2761 (0.3706)	loss 0.7681 (0.8034)	grad_norm 1.7631 (2.1685)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:30:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:06:44 lr 0.000092	 wd 0.0000	time 0.3106 (0.3673)	loss 0.8638 (0.8031)	grad_norm 2.5088 (2.1652)	loss_scale 16384.0000 (8472.6681)	mem 11634MB
[2024-07-11 17:31:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:06:05 lr 0.000092	 wd 0.0000	time 0.3347 (0.3646)	loss 0.8613 (0.8030)	grad_norm 2.0944 (2.1560)	loss_scale 16384.0000 (8999.7388)	mem 11634MB
[2024-07-11 17:31:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:05:26 lr 0.000092	 wd 0.0000	time 0.3031 (0.3625)	loss 0.7085 (0.8025)	grad_norm 2.2531 (2.1590)	loss_scale 16384.0000 (9460.9669)	mem 11634MB
[2024-07-11 17:32:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:49 lr 0.000092	 wd 0.0000	time 0.3038 (0.3604)	loss 0.7876 (0.8025)	grad_norm 1.8227 (2.1595)	loss_scale 16384.0000 (9867.9647)	mem 11634MB
[2024-07-11 17:32:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:04:11 lr 0.000091	 wd 0.0000	time 0.3490 (0.3586)	loss 0.7959 (0.8026)	grad_norm 1.7094 (2.1612)	loss_scale 16384.0000 (10229.7657)	mem 11634MB
[2024-07-11 17:33:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:35 lr 0.000091	 wd 0.0000	time 0.3100 (0.3574)	loss 0.9058 (0.8028)	grad_norm 1.9957 (inf)	loss_scale 8192.0000 (10312.1810)	mem 11634MB
[2024-07-11 17:33:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:58 lr 0.000091	 wd 0.0000	time 0.3141 (0.3560)	loss 0.9233 (0.8028)	grad_norm 1.9841 (inf)	loss_scale 8192.0000 (10206.2249)	mem 11634MB
[2024-07-11 17:34:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:22 lr 0.000091	 wd 0.0000	time 0.2905 (0.3547)	loss 0.8154 (0.8020)	grad_norm 2.0176 (inf)	loss_scale 8192.0000 (10110.3551)	mem 11634MB
[2024-07-11 17:35:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:50 lr 0.000091	 wd 0.0000	time 0.3426 (0.3665)	loss 0.8135 (0.8021)	grad_norm 1.5027 (inf)	loss_scale 8192.0000 (10023.1967)	mem 11634MB
[2024-07-11 17:36:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:14 lr 0.000091	 wd 0.0000	time 0.3236 (0.3672)	loss 0.6831 (0.8018)	grad_norm 2.2053 (inf)	loss_scale 8192.0000 (9943.6141)	mem 11634MB
[2024-07-11 17:37:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:39 lr 0.000091	 wd 0.0000	time 0.3385 (0.3854)	loss 0.8657 (0.8020)	grad_norm 2.5664 (inf)	loss_scale 8192.0000 (9870.6606)	mem 11634MB
[2024-07-11 17:37:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000091	 wd 0.0000	time 0.3254 (0.3835)	loss 0.8052 (0.8023)	grad_norm 2.5581 (inf)	loss_scale 8192.0000 (9803.5410)	mem 11634MB
[2024-07-11 17:38:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 5 training takes 0:16:08
[2024-07-11 17:38:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 32.972 (32.972)	Loss 0.3684 (0.3684)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 17:38:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.984 Acc@5 97.460
[2024-07-11 17:38:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-11 17:38:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 17:39:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][0/2502]	eta 11:50:31 lr 0.000091	 wd 0.0000	time 17.0391 (17.0391)	loss 0.8306 (0.8306)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:39:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:19:52 lr 0.000090	 wd 0.0000	time 0.2965 (0.4965)	loss 0.7432 (0.7982)	grad_norm 2.0337 (2.2322)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:40:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:15:46 lr 0.000090	 wd 0.0000	time 0.2878 (0.4110)	loss 0.7188 (0.7975)	grad_norm 1.8305 (2.2017)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:40:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:14:02 lr 0.000090	 wd 0.0000	time 0.3266 (0.3826)	loss 0.8398 (0.7985)	grad_norm 1.8682 (2.1724)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:41:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:12:55 lr 0.000090	 wd 0.0000	time 0.2973 (0.3687)	loss 0.7607 (0.7976)	grad_norm 1.8293 (2.1484)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:41:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:12:01 lr 0.000090	 wd 0.0000	time 0.3189 (0.3602)	loss 0.8579 (0.7954)	grad_norm 1.9981 (2.1434)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:42:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:11:13 lr 0.000090	 wd 0.0000	time 0.3028 (0.3542)	loss 0.8345 (0.7955)	grad_norm 2.3125 (2.1392)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:42:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:10:31 lr 0.000090	 wd 0.0000	time 0.3294 (0.3502)	loss 0.8350 (0.7979)	grad_norm 1.8360 (2.1191)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:43:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:50 lr 0.000090	 wd 0.0000	time 0.3004 (0.3470)	loss 0.7588 (0.7995)	grad_norm 2.2571 (2.1167)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:44:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:09:12 lr 0.000089	 wd 0.0000	time 0.2935 (0.3446)	loss 0.7988 (0.7993)	grad_norm 2.3585 (2.1271)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:44:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:09:02 lr 0.000089	 wd 0.0000	time 0.5372 (0.3611)	loss 0.7788 (0.7999)	grad_norm 2.3654 (2.1212)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:45:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:08:42 lr 0.000089	 wd 0.0000	time 0.3005 (0.3724)	loss 0.7310 (0.7996)	grad_norm 2.1654 (2.1254)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:46:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:08:09 lr 0.000089	 wd 0.0000	time 0.3102 (0.3759)	loss 0.6431 (0.8001)	grad_norm 2.1168 (2.1304)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:46:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:07:27 lr 0.000089	 wd 0.0000	time 0.3030 (0.3720)	loss 0.7358 (0.8010)	grad_norm 2.0645 (2.1359)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:47:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:06:46 lr 0.000089	 wd 0.0000	time 0.3048 (0.3688)	loss 1.0205 (0.7996)	grad_norm 2.3051 (2.1407)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:47:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:06:07 lr 0.000089	 wd 0.0000	time 0.3193 (0.3663)	loss 0.6724 (0.7995)	grad_norm 2.1903 (2.1435)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:48:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:05:28 lr 0.000089	 wd 0.0000	time 0.3250 (0.3638)	loss 0.8594 (0.7992)	grad_norm 1.7771 (2.1400)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:49:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:50 lr 0.000088	 wd 0.0000	time 0.2987 (0.3616)	loss 0.9292 (0.7981)	grad_norm 2.2137 (2.1375)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:49:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:04:13 lr 0.000088	 wd 0.0000	time 0.2760 (0.3604)	loss 0.6982 (0.7976)	grad_norm 2.2584 (2.1424)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:50:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:35 lr 0.000088	 wd 0.0000	time 0.3224 (0.3586)	loss 0.8613 (0.7977)	grad_norm 2.6466 (2.1430)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:50:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:59 lr 0.000088	 wd 0.0000	time 0.3039 (0.3571)	loss 0.9785 (0.7988)	grad_norm 2.3310 (2.1394)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:51:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:25 lr 0.000088	 wd 0.0000	time 0.3130 (0.3630)	loss 0.8359 (0.7989)	grad_norm 2.3064 (2.1461)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:52:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:49 lr 0.000088	 wd 0.0000	time 0.3395 (0.3635)	loss 0.8135 (0.7992)	grad_norm 2.3550 (2.1484)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:53:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:16 lr 0.000088	 wd 0.0000	time 0.3128 (0.3775)	loss 0.8169 (0.7990)	grad_norm 1.9185 (2.1493)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:53:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:38 lr 0.000087	 wd 0.0000	time 0.3435 (0.3775)	loss 0.8076 (0.7987)	grad_norm 2.1676 (2.1474)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:54:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000087	 wd 0.0000	time 0.3110 (0.3777)	loss 0.7178 (0.7985)	grad_norm 1.5696 (2.1487)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:54:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 6 training takes 0:15:57
[2024-07-11 17:55:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 60.747 (60.747)	Loss 0.3789 (0.3789)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 17:56:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.100 Acc@5 97.386
[2024-07-11 17:56:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 17:56:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 17:56:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][0/2502]	eta 12:10:51 lr 0.000087	 wd 0.0000	time 17.5266 (17.5266)	loss 0.8594 (0.8594)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:57:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:19:47 lr 0.000087	 wd 0.0000	time 0.2888 (0.4945)	loss 0.9766 (0.8044)	grad_norm 2.2278 (2.1348)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:57:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:15:47 lr 0.000087	 wd 0.0000	time 0.3031 (0.4115)	loss 0.7896 (0.7939)	grad_norm 2.0026 (2.1407)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:58:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:14:01 lr 0.000087	 wd 0.0000	time 0.3111 (0.3822)	loss 0.7944 (0.7922)	grad_norm 2.5407 (2.1380)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:58:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:54 lr 0.000087	 wd 0.0000	time 0.3112 (0.3683)	loss 0.8345 (0.7913)	grad_norm 1.9932 (2.1492)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:59:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:12:00 lr 0.000087	 wd 0.0000	time 0.2828 (0.3599)	loss 0.7344 (0.7920)	grad_norm 1.9249 (2.1382)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 17:59:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:11:13 lr 0.000086	 wd 0.0000	time 0.2933 (0.3539)	loss 0.7734 (0.7925)	grad_norm 1.7755 (2.1254)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:00:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:10:29 lr 0.000086	 wd 0.0000	time 0.3008 (0.3496)	loss 0.7085 (0.7917)	grad_norm 1.8560 (2.1311)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:00:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:50 lr 0.000086	 wd 0.0000	time 0.3069 (0.3467)	loss 0.7690 (0.7919)	grad_norm 2.0259 (2.1365)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:01:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:09:11 lr 0.000086	 wd 0.0000	time 0.3243 (0.3444)	loss 0.6519 (0.7926)	grad_norm 1.7089 (2.1380)	loss_scale 16384.0000 (8737.5272)	mem 11634MB
[2024-07-11 18:01:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:34 lr 0.000086	 wd 0.0000	time 0.3244 (0.3425)	loss 0.7578 (0.7935)	grad_norm 1.9722 (2.1291)	loss_scale 16384.0000 (9501.4106)	mem 11634MB
[2024-07-11 18:02:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:08:10 lr 0.000086	 wd 0.0000	time 0.4410 (0.3501)	loss 0.7993 (0.7928)	grad_norm 1.8936 (2.1388)	loss_scale 16384.0000 (10126.5322)	mem 11634MB
[2024-07-11 18:03:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:08:14 lr 0.000086	 wd 0.0000	time 0.2968 (0.3799)	loss 0.6743 (0.7935)	grad_norm 2.6768 (2.1446)	loss_scale 16384.0000 (10647.5537)	mem 11634MB
[2024-07-11 18:04:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:07:39 lr 0.000085	 wd 0.0000	time 0.2965 (0.3824)	loss 0.8525 (0.7935)	grad_norm 2.2191 (2.1408)	loss_scale 16384.0000 (11088.4796)	mem 11634MB
[2024-07-11 18:05:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:06:57 lr 0.000085	 wd 0.0000	time 0.2868 (0.3784)	loss 0.8330 (0.7942)	grad_norm 2.4017 (2.1385)	loss_scale 16384.0000 (11466.4611)	mem 11634MB
[2024-07-11 18:05:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:06:15 lr 0.000085	 wd 0.0000	time 0.2768 (0.3750)	loss 0.7139 (0.7936)	grad_norm 1.9489 (2.1326)	loss_scale 16384.0000 (11794.0786)	mem 11634MB
[2024-07-11 18:06:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:05:35 lr 0.000085	 wd 0.0000	time 0.3348 (0.3721)	loss 0.8545 (0.7943)	grad_norm 2.3688 (2.1340)	loss_scale 16384.0000 (12080.7695)	mem 11634MB
[2024-07-11 18:06:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:56 lr 0.000085	 wd 0.0000	time 0.3019 (0.3695)	loss 0.7432 (0.7942)	grad_norm 1.6855 (2.1354)	loss_scale 16384.0000 (12333.7519)	mem 11634MB
[2024-07-11 18:07:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:04:17 lr 0.000085	 wd 0.0000	time 0.3074 (0.3671)	loss 0.7422 (0.7934)	grad_norm 2.4014 (2.1380)	loss_scale 16384.0000 (12558.6408)	mem 11634MB
[2024-07-11 18:07:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:39 lr 0.000085	 wd 0.0000	time 0.2818 (0.3654)	loss 0.8931 (0.7944)	grad_norm 3.0223 (2.1376)	loss_scale 16384.0000 (12759.8695)	mem 11634MB
[2024-07-11 18:08:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:03:02 lr 0.000084	 wd 0.0000	time 0.2999 (0.3635)	loss 0.7280 (0.7941)	grad_norm 3.6332 (2.1395)	loss_scale 16384.0000 (12940.9855)	mem 11634MB
[2024-07-11 18:08:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:25 lr 0.000084	 wd 0.0000	time 0.3164 (0.3619)	loss 0.7681 (0.7943)	grad_norm 2.4272 (inf)	loss_scale 8192.0000 (12909.9058)	mem 11634MB
[2024-07-11 18:09:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:51 lr 0.000084	 wd 0.0000	time 0.4505 (0.3698)	loss 0.7637 (0.7940)	grad_norm 2.2729 (inf)	loss_scale 8192.0000 (12695.5529)	mem 11634MB
[2024-07-11 18:10:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:14 lr 0.000084	 wd 0.0000	time 0.3326 (0.3693)	loss 0.8115 (0.7939)	grad_norm 1.9609 (inf)	loss_scale 8192.0000 (12499.8314)	mem 11634MB
[2024-07-11 18:11:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:39 lr 0.000084	 wd 0.0000	time 0.3012 (0.3830)	loss 0.7412 (0.7945)	grad_norm 1.8435 (inf)	loss_scale 8192.0000 (12320.4132)	mem 11634MB
[2024-07-11 18:12:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000084	 wd 0.0000	time 0.3123 (0.3810)	loss 0.7388 (0.7951)	grad_norm 2.0092 (inf)	loss_scale 8192.0000 (12155.3427)	mem 11634MB
[2024-07-11 18:12:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 7 training takes 0:16:11
[2024-07-11 18:13:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 74.782 (74.782)	Loss 0.3513 (0.3513)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 18:14:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.974 Acc@5 97.436
[2024-07-11 18:14:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-11 18:14:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 18:14:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][0/2502]	eta 11:55:46 lr 0.000084	 wd 0.0000	time 17.1648 (17.1648)	loss 0.8804 (0.8804)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:15:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:19:51 lr 0.000083	 wd 0.0000	time 0.2890 (0.4961)	loss 0.7866 (0.7958)	grad_norm 2.2261 (2.2027)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:15:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:15:52 lr 0.000083	 wd 0.0000	time 0.3005 (0.4139)	loss 0.7510 (0.7900)	grad_norm 2.2153 (2.1683)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:16:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:14:06 lr 0.000083	 wd 0.0000	time 0.2921 (0.3846)	loss 0.8423 (0.7871)	grad_norm 2.0806 (2.1464)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:16:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:12:56 lr 0.000083	 wd 0.0000	time 0.2890 (0.3695)	loss 0.8745 (0.7876)	grad_norm 2.2886 (2.1551)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:17:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:12:02 lr 0.000083	 wd 0.0000	time 0.3135 (0.3611)	loss 0.8491 (0.7901)	grad_norm 1.8710 (2.1561)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:17:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:11:15 lr 0.000083	 wd 0.0000	time 0.3197 (0.3552)	loss 0.6768 (0.7899)	grad_norm 2.1149 (2.1597)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:18:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:10:32 lr 0.000083	 wd 0.0000	time 0.3062 (0.3510)	loss 0.9453 (0.7883)	grad_norm 1.9079 (2.1508)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:18:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:52 lr 0.000082	 wd 0.0000	time 0.2861 (0.3482)	loss 0.9253 (0.7897)	grad_norm 1.8592 (2.1485)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:19:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:09:13 lr 0.000082	 wd 0.0000	time 0.3194 (0.3458)	loss 0.8682 (0.7892)	grad_norm 1.8664 (2.1427)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:19:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:36 lr 0.000082	 wd 0.0000	time 0.3080 (0.3437)	loss 0.8276 (0.7905)	grad_norm 1.9774 (2.1543)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:20:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:59 lr 0.000082	 wd 0.0000	time 0.2839 (0.3421)	loss 0.7856 (0.7907)	grad_norm 2.2176 (2.1499)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:21:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:23 lr 0.000082	 wd 0.0000	time 0.2932 (0.3409)	loss 0.8481 (0.7911)	grad_norm 3.3167 (2.1496)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:21:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:48 lr 0.000082	 wd 0.0000	time 0.2758 (0.3397)	loss 0.7568 (0.7918)	grad_norm 1.9653 (2.1503)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:22:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:06:20 lr 0.000081	 wd 0.0000	time 0.2994 (0.3450)	loss 0.7866 (0.7926)	grad_norm 2.6808 (2.1537)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:23:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:06:00 lr 0.000081	 wd 0.0000	time 0.3104 (0.3598)	loss 0.7051 (0.7929)	grad_norm 2.0773 (2.1530)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:24:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:05:44 lr 0.000081	 wd 0.0000	time 0.5625 (0.3818)	loss 0.8057 (0.7930)	grad_norm 2.0700 (2.1534)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:25:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:05:10 lr 0.000081	 wd 0.0000	time 0.2803 (0.3871)	loss 0.7319 (0.7935)	grad_norm 1.9289 (2.1524)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:25:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:04:31 lr 0.000081	 wd 0.0000	time 0.3146 (0.3863)	loss 0.7407 (0.7941)	grad_norm 2.3817 (2.1555)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:26:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:50 lr 0.000081	 wd 0.0000	time 0.3094 (0.3832)	loss 0.7422 (0.7934)	grad_norm 2.1328 (2.1524)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:26:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:03:11 lr 0.000080	 wd 0.0000	time 0.3212 (0.3806)	loss 0.8306 (0.7931)	grad_norm 2.2227 (2.1523)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:27:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:32 lr 0.000080	 wd 0.0000	time 0.2982 (0.3784)	loss 0.8813 (0.7934)	grad_norm 2.1227 (2.1493)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:27:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:53 lr 0.000080	 wd 0.0000	time 0.2993 (0.3761)	loss 0.8633 (0.7937)	grad_norm 2.4997 (2.1531)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:28:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:15 lr 0.000080	 wd 0.0000	time 0.3033 (0.3742)	loss 0.8496 (0.7934)	grad_norm 2.4292 (2.1516)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:29:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:38 lr 0.000080	 wd 0.0000	time 0.3050 (0.3726)	loss 0.8364 (0.7932)	grad_norm 2.4126 (2.1518)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:29:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000080	 wd 0.0000	time 0.3145 (0.3706)	loss 0.8564 (0.7934)	grad_norm 2.4118 (2.1506)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:29:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 8 training takes 0:15:31
[2024-07-11 18:30:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 34.213 (34.213)	Loss 0.3914 (0.3914)	Acc@1 91.797 (91.797)	Acc@5 98.242 (98.242)	Mem 11634MB
[2024-07-11 18:30:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.012 Acc@5 97.386
[2024-07-11 18:30:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-11 18:30:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 18:30:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][0/2502]	eta 11:39:44 lr 0.000080	 wd 0.0000	time 16.7805 (16.7805)	loss 0.6606 (0.6606)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:31:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:19:32 lr 0.000079	 wd 0.0000	time 0.2984 (0.4882)	loss 0.8188 (0.7828)	grad_norm 1.9074 (2.1295)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:31:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:15:40 lr 0.000079	 wd 0.0000	time 0.2821 (0.4086)	loss 0.7329 (0.7817)	grad_norm 2.8956 (2.1885)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:32:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:13:59 lr 0.000079	 wd 0.0000	time 0.3016 (0.3812)	loss 0.8511 (0.7883)	grad_norm 1.9766 (2.1646)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:32:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:12:52 lr 0.000079	 wd 0.0000	time 0.3247 (0.3675)	loss 0.7246 (0.7873)	grad_norm 1.7987 (2.1418)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:33:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:11:59 lr 0.000079	 wd 0.0000	time 0.3052 (0.3594)	loss 0.7002 (0.7860)	grad_norm 1.9539 (2.1326)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:34:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:11:13 lr 0.000079	 wd 0.0000	time 0.2998 (0.3539)	loss 0.7671 (0.7845)	grad_norm 2.3635 (2.1436)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:34:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:10:30 lr 0.000078	 wd 0.0000	time 0.3083 (0.3499)	loss 0.8374 (0.7856)	grad_norm 2.3076 (2.1434)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:35:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:50 lr 0.000078	 wd 0.0000	time 0.3070 (0.3469)	loss 0.8975 (0.7861)	grad_norm 1.9257 (2.1455)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:36:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:11:21 lr 0.000078	 wd 0.0000	time 0.3174 (0.4251)	loss 0.9053 (0.7874)	grad_norm 1.8172 (2.1478)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:37:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:10:23 lr 0.000078	 wd 0.0000	time 0.3028 (0.4153)	loss 0.7793 (0.7876)	grad_norm 1.9575 (2.1520)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 18:37:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:09:30 lr 0.000078	 wd 0.0000	time 0.2919 (0.4071)	loss 0.8223 (0.7873)	grad_norm 2.0891 (2.1493)	loss_scale 16384.0000 (8593.7875)	mem 11634MB
[2024-07-11 18:38:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:08:41 lr 0.000078	 wd 0.0000	time 0.3047 (0.4006)	loss 0.7808 (0.7884)	grad_norm 1.8588 (2.1431)	loss_scale 16384.0000 (9242.4313)	mem 11634MB
[2024-07-11 18:39:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:07:54 lr 0.000077	 wd 0.0000	time 0.3050 (0.3948)	loss 0.8066 (0.7888)	grad_norm 2.1300 (2.1398)	loss_scale 16384.0000 (9791.3605)	mem 11634MB
[2024-07-11 18:39:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:07:09 lr 0.000077	 wd 0.0000	time 0.3328 (0.3898)	loss 0.8521 (0.7890)	grad_norm 2.1651 (2.1341)	loss_scale 16384.0000 (10261.9272)	mem 11634MB
[2024-07-11 18:40:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:06:26 lr 0.000077	 wd 0.0000	time 0.2998 (0.3860)	loss 0.7266 (0.7901)	grad_norm 1.8340 (2.1329)	loss_scale 16384.0000 (10669.7935)	mem 11634MB
[2024-07-11 18:40:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:05:44 lr 0.000077	 wd 0.0000	time 0.2976 (0.3823)	loss 0.7705 (0.7889)	grad_norm 2.1038 (2.1363)	loss_scale 16384.0000 (11026.7083)	mem 11634MB
[2024-07-11 18:41:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:05:03 lr 0.000077	 wd 0.0000	time 0.3143 (0.3790)	loss 0.7607 (0.7891)	grad_norm 2.3631 (2.1384)	loss_scale 16384.0000 (11341.6578)	mem 11634MB
[2024-07-11 18:41:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:04:27 lr 0.000077	 wd 0.0000	time 0.3022 (0.3817)	loss 0.7637 (0.7893)	grad_norm 1.9198 (2.1378)	loss_scale 16384.0000 (11621.6324)	mem 11634MB
[2024-07-11 18:42:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:52 lr 0.000076	 wd 0.0000	time 0.3152 (0.3858)	loss 0.7402 (0.7897)	grad_norm 2.0614 (2.1415)	loss_scale 16384.0000 (11872.1515)	mem 11634MB
[2024-07-11 18:43:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:03:14 lr 0.000076	 wd 0.0000	time 0.6184 (0.3870)	loss 0.7466 (0.7903)	grad_norm 2.1297 (2.1439)	loss_scale 16384.0000 (12097.6312)	mem 11634MB
[2024-07-11 18:44:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:43 lr 0.000076	 wd 0.0000	time 0.3215 (0.4077)	loss 0.7949 (0.7903)	grad_norm 1.6750 (2.1437)	loss_scale 16384.0000 (12301.6468)	mem 11634MB
[2024-07-11 18:45:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:02 lr 0.000076	 wd 0.0000	time 0.3030 (0.4066)	loss 0.6147 (0.7901)	grad_norm 2.8186 (2.1439)	loss_scale 16384.0000 (12487.1240)	mem 11634MB
[2024-07-11 18:45:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:21 lr 0.000076	 wd 0.0000	time 0.2816 (0.4032)	loss 0.7456 (0.7899)	grad_norm 1.9367 (2.1432)	loss_scale 16384.0000 (12656.4798)	mem 11634MB
[2024-07-11 18:46:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:40 lr 0.000075	 wd 0.0000	time 0.2970 (0.4001)	loss 0.8838 (0.7904)	grad_norm 2.0027 (2.1478)	loss_scale 16384.0000 (12811.7284)	mem 11634MB
[2024-07-11 18:47:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000075	 wd 0.0000	time 0.2981 (0.3973)	loss 0.8286 (0.7907)	grad_norm 2.0955 (2.1456)	loss_scale 16384.0000 (12954.5622)	mem 11634MB
[2024-07-11 18:47:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 9 training takes 0:16:40
[2024-07-11 18:47:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 20.949 (20.949)	Loss 0.3875 (0.3875)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 11634MB
[2024-07-11 18:47:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.006 Acc@5 97.306
[2024-07-11 18:47:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-11 18:47:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 18:48:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][0/2502]	eta 16:33:56 lr 0.000075	 wd 0.0000	time 23.8354 (23.8354)	loss 0.8413 (0.8413)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:48:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:22:25 lr 0.000075	 wd 0.0000	time 0.3233 (0.5602)	loss 0.8828 (0.7961)	grad_norm 2.3183 (2.3179)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:49:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:17:00 lr 0.000075	 wd 0.0000	time 0.3099 (0.4431)	loss 0.7690 (0.7918)	grad_norm 1.8454 (2.2600)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:49:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:14:49 lr 0.000075	 wd 0.0000	time 0.2962 (0.4040)	loss 0.7739 (0.7907)	grad_norm 1.8799 (2.2233)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:50:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:13:28 lr 0.000075	 wd 0.0000	time 0.2886 (0.3846)	loss 0.7568 (0.7917)	grad_norm 2.3047 (2.2156)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:50:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:12:25 lr 0.000074	 wd 0.0000	time 0.2978 (0.3724)	loss 0.7437 (0.7933)	grad_norm 2.0842 (2.2069)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:51:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:11:33 lr 0.000074	 wd 0.0000	time 0.2725 (0.3644)	loss 0.7393 (0.7925)	grad_norm 1.9530 (2.2034)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:51:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:10:46 lr 0.000074	 wd 0.0000	time 0.2999 (0.3589)	loss 0.7173 (0.7898)	grad_norm 2.0931 (2.2023)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:52:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:10:03 lr 0.000074	 wd 0.0000	time 0.3087 (0.3546)	loss 0.7383 (0.7909)	grad_norm 1.9694 (2.1929)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:52:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:09:23 lr 0.000074	 wd 0.0000	time 0.3188 (0.3515)	loss 0.6973 (0.7900)	grad_norm 1.6748 (2.1973)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:54:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:09:27 lr 0.000073	 wd 0.0000	time 0.3021 (0.3776)	loss 0.9385 (0.7899)	grad_norm 1.9843 (2.1911)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:54:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:08:47 lr 0.000073	 wd 0.0000	time 0.3663 (0.3762)	loss 0.8159 (0.7891)	grad_norm 1.6841 (2.1883)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 18:55:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:08:13 lr 0.000073	 wd 0.0000	time 0.2974 (0.3788)	loss 0.8848 (0.7889)	grad_norm 1.8646 (inf)	loss_scale 8192.0000 (16042.9509)	mem 11634MB
[2024-07-11 18:55:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:07:30 lr 0.000073	 wd 0.0000	time 0.2944 (0.3748)	loss 0.7368 (0.7888)	grad_norm 2.1133 (inf)	loss_scale 8192.0000 (15439.4958)	mem 11634MB
[2024-07-11 18:56:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:06:49 lr 0.000073	 wd 0.0000	time 0.2971 (0.3714)	loss 0.7783 (0.7888)	grad_norm 3.0768 (inf)	loss_scale 8192.0000 (14922.1870)	mem 11634MB
[2024-07-11 18:56:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:06:09 lr 0.000073	 wd 0.0000	time 0.2905 (0.3685)	loss 0.7866 (0.7886)	grad_norm 2.5922 (inf)	loss_scale 8192.0000 (14473.8068)	mem 11634MB
[2024-07-11 18:57:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:05:29 lr 0.000072	 wd 0.0000	time 0.2928 (0.3658)	loss 0.6538 (0.7881)	grad_norm 2.0300 (inf)	loss_scale 8192.0000 (14081.4391)	mem 11634MB
[2024-07-11 18:58:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:51 lr 0.000072	 wd 0.0000	time 0.3073 (0.3636)	loss 0.7471 (0.7881)	grad_norm 2.9583 (inf)	loss_scale 8192.0000 (13735.2052)	mem 11634MB
[2024-07-11 18:58:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:04:14 lr 0.000072	 wd 0.0000	time 0.3222 (0.3618)	loss 0.8525 (0.7883)	grad_norm 2.3045 (inf)	loss_scale 8192.0000 (13427.4203)	mem 11634MB
[2024-07-11 18:59:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:36 lr 0.000072	 wd 0.0000	time 0.2979 (0.3601)	loss 0.8838 (0.7897)	grad_norm 2.1636 (inf)	loss_scale 8192.0000 (13152.0168)	mem 11634MB
[2024-07-11 18:59:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:03:03 lr 0.000072	 wd 0.0000	time 14.6823 (0.3661)	loss 0.9995 (0.7902)	grad_norm 2.2245 (inf)	loss_scale 8192.0000 (12904.1399)	mem 11634MB
[2024-07-11 19:01:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:33 lr 0.000071	 wd 0.0000	time 0.2715 (0.3816)	loss 0.7793 (0.7905)	grad_norm 2.1696 (inf)	loss_scale 8192.0000 (12679.8591)	mem 11634MB
[2024-07-11 19:02:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:58 lr 0.000071	 wd 0.0000	time 0.3580 (0.3916)	loss 0.8330 (0.7909)	grad_norm 1.8994 (inf)	loss_scale 8192.0000 (12475.9582)	mem 11634MB
[2024-07-11 19:02:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:18 lr 0.000071	 wd 0.0000	time 0.2985 (0.3898)	loss 0.8462 (0.7910)	grad_norm 1.7612 (inf)	loss_scale 8192.0000 (12289.7801)	mem 11634MB
[2024-07-11 19:03:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:39 lr 0.000071	 wd 0.0000	time 0.2992 (0.3900)	loss 0.8740 (0.7910)	grad_norm 2.1550 (inf)	loss_scale 8192.0000 (12119.1104)	mem 11634MB
[2024-07-11 19:03:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000071	 wd 0.0000	time 0.3032 (0.3875)	loss 0.6870 (0.7907)	grad_norm 1.9703 (inf)	loss_scale 8192.0000 (11962.0888)	mem 11634MB
[2024-07-11 19:03:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 10 training takes 0:16:13
[2024-07-11 19:04:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 32.024 (32.024)	Loss 0.4055 (0.4055)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 19:04:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.034 Acc@5 97.334
[2024-07-11 19:04:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-11 19:04:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 19:05:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][0/2502]	eta 11:00:38 lr 0.000071	 wd 0.0000	time 15.8426 (15.8426)	loss 0.7114 (0.7114)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:05:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:19:27 lr 0.000070	 wd 0.0000	time 0.2978 (0.4861)	loss 0.7358 (0.7842)	grad_norm 2.1931 (2.1617)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:06:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:15:36 lr 0.000070	 wd 0.0000	time 0.3061 (0.4070)	loss 0.7998 (0.7853)	grad_norm 1.8057 (2.1665)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:06:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:13:57 lr 0.000070	 wd 0.0000	time 0.3031 (0.3803)	loss 0.7539 (0.7834)	grad_norm 2.2005 (2.1515)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:07:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:12:51 lr 0.000070	 wd 0.0000	time 0.2996 (0.3669)	loss 0.7388 (0.7807)	grad_norm 1.9442 (2.1570)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:07:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:11:58 lr 0.000070	 wd 0.0000	time 0.3095 (0.3589)	loss 0.7793 (0.7795)	grad_norm 2.4368 (2.1621)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:08:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:11:11 lr 0.000069	 wd 0.0000	time 0.2985 (0.3532)	loss 0.8037 (0.7800)	grad_norm 2.3275 (2.1499)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:08:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:10:29 lr 0.000069	 wd 0.0000	time 0.2950 (0.3492)	loss 0.7646 (0.7807)	grad_norm 2.2032 (2.1572)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:09:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:49 lr 0.000069	 wd 0.0000	time 0.3010 (0.3463)	loss 0.8477 (0.7820)	grad_norm 1.7851 (2.1528)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:10:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:09:59 lr 0.000069	 wd 0.0000	time 0.4079 (0.3743)	loss 0.7944 (0.7844)	grad_norm 2.3791 (2.1508)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:10:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:09:19 lr 0.000069	 wd 0.0000	time 0.3176 (0.3723)	loss 0.7744 (0.7848)	grad_norm 1.8211 (2.1447)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:12:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:09:44 lr 0.000069	 wd 0.0000	time 0.2890 (0.4166)	loss 0.8013 (0.7842)	grad_norm 1.8120 (2.1385)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:13:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:09:00 lr 0.000068	 wd 0.0000	time 0.3021 (0.4148)	loss 0.8203 (0.7841)	grad_norm 2.5225 (2.1373)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:13:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:08:10 lr 0.000068	 wd 0.0000	time 0.3252 (0.4079)	loss 0.7363 (0.7846)	grad_norm 2.2227 (2.1411)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:14:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:07:23 lr 0.000068	 wd 0.0000	time 0.2939 (0.4021)	loss 0.7783 (0.7847)	grad_norm 2.1611 (2.1389)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:14:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:06:37 lr 0.000068	 wd 0.0000	time 0.3194 (0.3972)	loss 0.8003 (0.7844)	grad_norm 1.9847 (2.1369)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:15:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:05:54 lr 0.000068	 wd 0.0000	time 0.3067 (0.3927)	loss 0.7109 (0.7844)	grad_norm 2.7555 (2.1439)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:15:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:05:11 lr 0.000067	 wd 0.0000	time 0.3268 (0.3889)	loss 0.7720 (0.7842)	grad_norm 1.6629 (2.1449)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:16:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:04:30 lr 0.000067	 wd 0.0000	time 0.3229 (0.3859)	loss 0.7617 (0.7849)	grad_norm 1.9596 (2.1423)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:16:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:50 lr 0.000067	 wd 0.0000	time 0.3001 (0.3828)	loss 0.7358 (0.7857)	grad_norm 2.1065 (2.1421)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:17:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:03:10 lr 0.000067	 wd 0.0000	time 0.2758 (0.3801)	loss 0.6167 (0.7856)	grad_norm 2.2089 (2.1451)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:18:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:39 lr 0.000067	 wd 0.0000	time 0.2948 (0.3976)	loss 0.8306 (0.7853)	grad_norm 2.2610 (2.1500)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:19:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:00 lr 0.000066	 wd 0.0000	time 0.2973 (0.3989)	loss 0.8677 (0.7857)	grad_norm 3.1895 (2.1551)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:19:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:19 lr 0.000066	 wd 0.0000	time 0.2815 (0.3958)	loss 0.7695 (0.7858)	grad_norm 2.4949 (2.1562)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:20:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:40 lr 0.000066	 wd 0.0000	time 0.3130 (0.3930)	loss 0.7944 (0.7856)	grad_norm 2.5667 (2.1581)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:21:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000066	 wd 0.0000	time 0.3044 (0.3904)	loss 0.7510 (0.7857)	grad_norm 2.3543 (2.1574)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:21:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 11 training takes 0:16:26
[2024-07-11 19:21:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 26.330 (26.330)	Loss 0.3855 (0.3855)	Acc@1 91.406 (91.406)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 19:21:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.924 Acc@5 97.328
[2024-07-11 19:21:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-11 19:21:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 19:22:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][0/2502]	eta 10:41:41 lr 0.000066	 wd 0.0000	time 15.3885 (15.3885)	loss 0.7065 (0.7065)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:22:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:19:21 lr 0.000066	 wd 0.0000	time 0.3030 (0.4836)	loss 0.7520 (0.7826)	grad_norm 2.1468 (2.2041)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:23:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:15:32 lr 0.000065	 wd 0.0000	time 0.3498 (0.4053)	loss 0.7378 (0.7811)	grad_norm 1.8962 (2.1837)	loss_scale 16384.0000 (10392.8358)	mem 11634MB
[2024-07-11 19:23:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:13:54 lr 0.000065	 wd 0.0000	time 0.3076 (0.3789)	loss 0.7217 (0.7879)	grad_norm 2.2566 (2.1673)	loss_scale 16384.0000 (12383.2558)	mem 11634MB
[2024-07-11 19:24:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:12:49 lr 0.000065	 wd 0.0000	time 0.2953 (0.3661)	loss 0.7451 (0.7860)	grad_norm 1.8163 (2.1743)	loss_scale 16384.0000 (13380.9476)	mem 11634MB
[2024-07-11 19:24:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:56 lr 0.000065	 wd 0.0000	time 0.3072 (0.3580)	loss 0.8433 (0.7880)	grad_norm 1.9031 (2.1716)	loss_scale 16384.0000 (13980.3593)	mem 11634MB
[2024-07-11 19:25:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:11:10 lr 0.000065	 wd 0.0000	time 0.3026 (0.3528)	loss 0.8350 (0.7854)	grad_norm 1.9483 (2.1639)	loss_scale 16384.0000 (14380.2995)	mem 11634MB
[2024-07-11 19:25:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:10:29 lr 0.000064	 wd 0.0000	time 0.2890 (0.3493)	loss 0.7695 (0.7869)	grad_norm 3.4274 (2.1632)	loss_scale 16384.0000 (14666.1341)	mem 11634MB
[2024-07-11 19:26:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:49 lr 0.000064	 wd 0.0000	time 0.2998 (0.3462)	loss 1.0137 (0.7864)	grad_norm 2.0244 (2.1502)	loss_scale 16384.0000 (14880.5993)	mem 11634MB
[2024-07-11 19:27:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:09:10 lr 0.000064	 wd 0.0000	time 0.3093 (0.3437)	loss 0.8955 (0.7865)	grad_norm 1.8394 (inf)	loss_scale 8192.0000 (15011.0899)	mem 11634MB
[2024-07-11 19:27:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:08:46 lr 0.000064	 wd 0.0000	time 0.3627 (0.3503)	loss 0.7031 (0.7866)	grad_norm 2.5144 (inf)	loss_scale 8192.0000 (14329.8621)	mem 11634MB
[2024-07-11 19:28:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:08:38 lr 0.000064	 wd 0.0000	time 0.3344 (0.3697)	loss 0.7666 (0.7881)	grad_norm 1.7664 (inf)	loss_scale 8192.0000 (13772.3815)	mem 11634MB
[2024-07-11 19:29:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:08:26 lr 0.000063	 wd 0.0000	time 0.9426 (0.3888)	loss 0.8472 (0.7884)	grad_norm 2.4826 (inf)	loss_scale 8192.0000 (13307.7369)	mem 11634MB
[2024-07-11 19:30:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:07:45 lr 0.000063	 wd 0.0000	time 0.3190 (0.3875)	loss 0.7827 (0.7887)	grad_norm 2.5464 (inf)	loss_scale 8192.0000 (12914.5211)	mem 11634MB
[2024-07-11 19:30:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:07:07 lr 0.000063	 wd 0.0000	time 0.2994 (0.3880)	loss 0.9194 (0.7893)	grad_norm 3.6059 (inf)	loss_scale 8192.0000 (12577.4390)	mem 11634MB
[2024-07-11 19:31:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:06:24 lr 0.000063	 wd 0.0000	time 0.2942 (0.3838)	loss 0.8237 (0.7898)	grad_norm 1.8606 (inf)	loss_scale 8192.0000 (12285.2712)	mem 11634MB
[2024-07-11 19:31:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:05:42 lr 0.000063	 wd 0.0000	time 0.3215 (0.3803)	loss 0.8071 (0.7890)	grad_norm 2.4802 (inf)	loss_scale 8192.0000 (12029.6015)	mem 11634MB
[2024-07-11 19:32:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:05:02 lr 0.000062	 wd 0.0000	time 0.3114 (0.3775)	loss 0.8589 (0.7888)	grad_norm 1.9193 (inf)	loss_scale 8192.0000 (11803.9929)	mem 11634MB
[2024-07-11 19:33:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:04:22 lr 0.000062	 wd 0.0000	time 0.3006 (0.3746)	loss 0.6562 (0.7887)	grad_norm 2.9069 (inf)	loss_scale 8192.0000 (11603.4381)	mem 11634MB
[2024-07-11 19:33:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:44 lr 0.000062	 wd 0.0000	time 0.2879 (0.3722)	loss 0.7515 (0.7888)	grad_norm 1.7544 (inf)	loss_scale 8192.0000 (11423.9832)	mem 11634MB
[2024-07-11 19:34:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:03:06 lr 0.000062	 wd 0.0000	time 0.3123 (0.3706)	loss 0.7246 (0.7886)	grad_norm 1.7883 (inf)	loss_scale 8192.0000 (11262.4648)	mem 11634MB
[2024-07-11 19:34:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:28 lr 0.000062	 wd 0.0000	time 0.2892 (0.3687)	loss 0.8394 (0.7889)	grad_norm 2.1095 (inf)	loss_scale 8192.0000 (11116.3218)	mem 11634MB
[2024-07-11 19:35:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:50 lr 0.000061	 wd 0.0000	time 0.3105 (0.3668)	loss 0.8721 (0.7891)	grad_norm 2.1369 (inf)	loss_scale 8192.0000 (10983.4584)	mem 11634MB
[2024-07-11 19:36:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:15 lr 0.000061	 wd 0.0000	time 0.2941 (0.3717)	loss 0.7358 (0.7891)	grad_norm 2.4030 (inf)	loss_scale 8192.0000 (10862.1434)	mem 11634MB
[2024-07-11 19:36:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:37 lr 0.000061	 wd 0.0000	time 0.2920 (0.3721)	loss 0.7231 (0.7889)	grad_norm 2.0527 (inf)	loss_scale 8192.0000 (10750.9338)	mem 11634MB
[2024-07-11 19:37:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000061	 wd 0.0000	time 0.2975 (0.3708)	loss 0.7637 (0.7891)	grad_norm 2.1631 (inf)	loss_scale 8192.0000 (10648.6174)	mem 11634MB
[2024-07-11 19:37:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 12 training takes 0:15:46
[2024-07-11 19:38:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 57.329 (57.329)	Loss 0.4050 (0.4050)	Acc@1 90.820 (90.820)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 19:38:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.104 Acc@5 97.356
[2024-07-11 19:38:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 19:38:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 19:39:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][0/2502]	eta 16:22:24 lr 0.000061	 wd 0.0000	time 23.5589 (23.5589)	loss 0.6650 (0.6650)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:39:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:22:14 lr 0.000061	 wd 0.0000	time 0.2876 (0.5556)	loss 0.7129 (0.7838)	grad_norm 2.4401 (2.1557)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:40:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:16:58 lr 0.000060	 wd 0.0000	time 0.3081 (0.4425)	loss 0.6680 (0.7775)	grad_norm 2.2549 (2.1231)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:40:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:14:49 lr 0.000060	 wd 0.0000	time 0.2859 (0.4041)	loss 0.7690 (0.7751)	grad_norm 2.5281 (2.1413)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:41:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:13:28 lr 0.000060	 wd 0.0000	time 0.2923 (0.3846)	loss 0.8428 (0.7786)	grad_norm 2.2457 (2.1410)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:41:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:12:26 lr 0.000060	 wd 0.0000	time 0.3448 (0.3729)	loss 0.8682 (0.7783)	grad_norm 2.0738 (2.1706)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:42:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:11:35 lr 0.000060	 wd 0.0000	time 0.3004 (0.3656)	loss 0.8105 (0.7777)	grad_norm 2.3791 (2.1682)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:43:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:10:48 lr 0.000059	 wd 0.0000	time 0.3252 (0.3599)	loss 0.7222 (0.7783)	grad_norm 2.6607 (2.1740)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:43:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:10:05 lr 0.000059	 wd 0.0000	time 0.3047 (0.3557)	loss 0.7769 (0.7785)	grad_norm 1.7694 (2.1602)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:44:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:09:24 lr 0.000059	 wd 0.0000	time 0.2911 (0.3526)	loss 0.7280 (0.7802)	grad_norm 2.5588 (2.1498)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:44:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:08:45 lr 0.000059	 wd 0.0000	time 0.2951 (0.3498)	loss 0.7827 (0.7823)	grad_norm 2.3826 (2.1498)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:45:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:08:07 lr 0.000059	 wd 0.0000	time 0.3237 (0.3476)	loss 0.7915 (0.7824)	grad_norm 2.0006 (2.1476)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:46:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:49 lr 0.000058	 wd 0.0000	time 0.2919 (0.3604)	loss 0.8330 (0.7822)	grad_norm 1.9159 (2.1475)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:46:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:07:13 lr 0.000058	 wd 0.0000	time 0.2923 (0.3603)	loss 0.8193 (0.7826)	grad_norm 2.0609 (2.1500)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:47:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:06:55 lr 0.000058	 wd 0.0000	time 0.3664 (0.3768)	loss 0.8364 (0.7819)	grad_norm 1.8498 (2.1510)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:48:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:06:16 lr 0.000058	 wd 0.0000	time 0.2870 (0.3756)	loss 0.6968 (0.7829)	grad_norm 2.0579 (2.1482)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:49:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:05:49 lr 0.000058	 wd 0.0000	time 0.3387 (0.3872)	loss 0.6523 (0.7832)	grad_norm 1.8547 (2.1434)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:49:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:05:09 lr 0.000057	 wd 0.0000	time 0.3405 (0.3859)	loss 0.7812 (0.7841)	grad_norm 1.9299 (2.1405)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:50:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:04:36 lr 0.000057	 wd 0.0000	time 0.4076 (0.3933)	loss 0.7842 (0.7846)	grad_norm 2.0260 (2.1442)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:51:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:55 lr 0.000057	 wd 0.0000	time 0.2939 (0.3920)	loss 0.6782 (0.7843)	grad_norm 2.2216 (2.1419)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:51:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:03:15 lr 0.000057	 wd 0.0000	time 0.3709 (0.3898)	loss 0.8145 (0.7842)	grad_norm 2.1075 (2.1477)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:52:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:40 lr 0.000057	 wd 0.0000	time 0.2777 (0.3997)	loss 0.7837 (0.7843)	grad_norm 2.0186 (2.1440)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:53:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:00 lr 0.000056	 wd 0.0000	time 0.3102 (0.3976)	loss 0.7124 (0.7839)	grad_norm 2.3426 (2.1485)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:54:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:22 lr 0.000056	 wd 0.0000	time 0.3378 (0.4068)	loss 0.8052 (0.7841)	grad_norm 2.0930 (2.1535)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 19:55:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:41 lr 0.000056	 wd 0.0000	time 0.2991 (0.4044)	loss 0.7607 (0.7843)	grad_norm 1.8657 (2.1517)	loss_scale 16384.0000 (8212.4715)	mem 11634MB
[2024-07-11 19:56:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000056	 wd 0.0000	time 0.3045 (0.4141)	loss 0.7617 (0.7842)	grad_norm 2.0394 (2.1490)	loss_scale 16384.0000 (8539.2019)	mem 11634MB
[2024-07-11 19:56:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 13 training takes 0:17:21
[2024-07-11 19:57:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 59.140 (59.140)	Loss 0.4050 (0.4050)	Acc@1 91.211 (91.211)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 19:57:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.080 Acc@5 97.358
[2024-07-11 19:57:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 19:57:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 19:57:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][0/2502]	eta 11:11:12 lr 0.000056	 wd 0.0000	time 16.0963 (16.0963)	loss 0.7422 (0.7422)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 19:58:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:19:32 lr 0.000055	 wd 0.0000	time 0.2819 (0.4879)	loss 0.7534 (0.7832)	grad_norm 2.3902 (2.0645)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 19:58:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:15:39 lr 0.000055	 wd 0.0000	time 0.3082 (0.4082)	loss 0.7690 (0.7789)	grad_norm 1.8813 (2.0942)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 19:59:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:13:58 lr 0.000055	 wd 0.0000	time 0.3147 (0.3807)	loss 0.8491 (0.7807)	grad_norm 2.3164 (2.1011)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 19:59:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:12:52 lr 0.000055	 wd 0.0000	time 0.3127 (0.3673)	loss 0.9019 (0.7814)	grad_norm 2.1471 (2.0990)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:00:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:11:59 lr 0.000055	 wd 0.0000	time 0.3063 (0.3593)	loss 0.8174 (0.7802)	grad_norm 2.0975 (2.0995)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:00:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:11:12 lr 0.000054	 wd 0.0000	time 0.2806 (0.3535)	loss 0.7397 (0.7818)	grad_norm 2.0122 (2.1271)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:01:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:10:29 lr 0.000054	 wd 0.0000	time 0.3103 (0.3496)	loss 0.6929 (0.7834)	grad_norm 1.7873 (2.1330)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:02:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:50 lr 0.000054	 wd 0.0000	time 0.3102 (0.3469)	loss 0.7905 (0.7822)	grad_norm 2.0328 (2.1291)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:02:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:09:11 lr 0.000054	 wd 0.0000	time 0.3242 (0.3445)	loss 0.8398 (0.7821)	grad_norm 1.8362 (2.1247)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:03:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:08:34 lr 0.000054	 wd 0.0000	time 0.2925 (0.3426)	loss 0.8789 (0.7818)	grad_norm 2.4179 (2.1253)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:04:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:08:25 lr 0.000053	 wd 0.0000	time 0.3153 (0.3604)	loss 0.7139 (0.7823)	grad_norm 2.1574 (2.1240)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:04:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:07:54 lr 0.000053	 wd 0.0000	time 0.2956 (0.3641)	loss 0.8228 (0.7825)	grad_norm 2.6955 (2.1265)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:05:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:07:37 lr 0.000053	 wd 0.0000	time 0.3133 (0.3810)	loss 0.8052 (0.7823)	grad_norm 1.9059 (2.1254)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:06:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:06:58 lr 0.000053	 wd 0.0000	time 0.3236 (0.3795)	loss 0.8076 (0.7816)	grad_norm 2.3358 (2.1244)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:07:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:06:32 lr 0.000053	 wd 0.0000	time 0.3246 (0.3920)	loss 0.8359 (0.7819)	grad_norm 2.2609 (2.1328)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:07:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:05:51 lr 0.000052	 wd 0.0000	time 0.3181 (0.3899)	loss 0.8545 (0.7823)	grad_norm 2.1905 (2.1374)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:08:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:05:19 lr 0.000052	 wd 0.0000	time 0.3315 (0.3980)	loss 0.8613 (0.7824)	grad_norm 2.1352 (2.1426)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:09:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:04:37 lr 0.000052	 wd 0.0000	time 0.2824 (0.3958)	loss 0.8975 (0.7824)	grad_norm 2.0959 (2.1499)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:10:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:58 lr 0.000052	 wd 0.0000	time 3.8772 (0.3958)	loss 0.7754 (0.7825)	grad_norm 2.2699 (2.1452)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:10:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:03:22 lr 0.000052	 wd 0.0000	time 0.3104 (0.4026)	loss 0.8057 (0.7826)	grad_norm 2.7311 (2.1496)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:11:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:40 lr 0.000051	 wd 0.0000	time 0.3706 (0.4000)	loss 0.7471 (0.7830)	grad_norm 2.0757 (2.1501)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:12:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:03 lr 0.000051	 wd 0.0000	time 0.3736 (0.4084)	loss 0.6987 (0.7836)	grad_norm 2.7392 (2.1550)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:12:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:22 lr 0.000051	 wd 0.0000	time 0.2981 (0.4061)	loss 0.8560 (0.7839)	grad_norm 2.2402 (2.1570)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:14:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:42 lr 0.000051	 wd 0.0000	time 0.2939 (0.4156)	loss 0.7500 (0.7840)	grad_norm 2.0725 (2.1570)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:14:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000051	 wd 0.0000	time 0.3030 (0.4125)	loss 0.8130 (0.7844)	grad_norm 2.0110 (2.1568)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:14:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 14 training takes 0:17:18
[2024-07-11 20:15:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 74.043 (74.043)	Loss 0.3816 (0.3816)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 20:16:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.094 Acc@5 97.384
[2024-07-11 20:16:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 20:16:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.12%
[2024-07-11 20:16:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][0/2502]	eta 1 day, 1:21:52 lr 0.000051	 wd 0.0000	time 36.4956 (36.4956)	loss 0.6812 (0.6812)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:17:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:27:53 lr 0.000050	 wd 0.0000	time 0.2792 (0.6965)	loss 0.8140 (0.7998)	grad_norm 2.3178 (2.2885)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:17:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:19:42 lr 0.000050	 wd 0.0000	time 0.2813 (0.5136)	loss 1.0469 (0.8094)	grad_norm 2.2997 (2.2433)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:18:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:16:36 lr 0.000050	 wd 0.0000	time 0.3227 (0.4527)	loss 0.7109 (0.8090)	grad_norm 1.7956 (2.2325)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:19:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:14:44 lr 0.000050	 wd 0.0000	time 0.2999 (0.4208)	loss 0.6455 (0.8093)	grad_norm 1.9491 (2.2173)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:19:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:13:23 lr 0.000049	 wd 0.0000	time 0.3063 (0.4014)	loss 0.7510 (0.8091)	grad_norm 2.5091 (2.1859)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:20:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:12:20 lr 0.000049	 wd 0.0000	time 0.3293 (0.3892)	loss 0.6724 (0.8091)	grad_norm 1.9013 (2.1777)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:20:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:11:24 lr 0.000049	 wd 0.0000	time 0.2977 (0.3800)	loss 0.7993 (0.8082)	grad_norm 1.9759 (2.1677)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:21:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:10:35 lr 0.000049	 wd 0.0000	time 0.2993 (0.3733)	loss 0.7676 (0.8074)	grad_norm 2.0185 (2.1598)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:21:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:09:50 lr 0.000049	 wd 0.0000	time 0.3142 (0.3685)	loss 0.7891 (0.8069)	grad_norm 2.0653 (2.1590)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:22:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:09:07 lr 0.000048	 wd 0.0000	time 0.3329 (0.3644)	loss 0.7881 (0.8075)	grad_norm 2.0440 (2.1592)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:22:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:08:26 lr 0.000048	 wd 0.0000	time 0.3305 (0.3610)	loss 0.8975 (0.8065)	grad_norm 2.5969 (2.1656)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:23:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:08:12 lr 0.000048	 wd 0.0000	time 0.3110 (0.3784)	loss 0.6958 (0.8059)	grad_norm 1.7608 (2.1660)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:24:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:07:35 lr 0.000048	 wd 0.0000	time 0.3384 (0.3790)	loss 0.8818 (0.8061)	grad_norm 2.1411 (2.1639)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 20:25:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:07:16 lr 0.000048	 wd 0.0000	time 0.3233 (0.3961)	loss 0.8188 (0.8060)	grad_norm 1.8657 (2.1672)	loss_scale 32768.0000 (16500.9450)	mem 11634MB
[2024-07-11 20:26:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:06:33 lr 0.000047	 wd 0.0000	time 0.3197 (0.3925)	loss 0.8091 (0.8056)	grad_norm 2.5611 (inf)	loss_scale 16384.0000 (17213.5696)	mem 11634MB
[2024-07-11 20:27:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:06:05 lr 0.000047	 wd 0.0000	time 0.3646 (0.4048)	loss 0.8613 (0.8056)	grad_norm 2.0864 (inf)	loss_scale 16384.0000 (17161.7539)	mem 11634MB
[2024-07-11 20:27:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:05:22 lr 0.000047	 wd 0.0000	time 0.3026 (0.4015)	loss 0.8750 (0.8051)	grad_norm 1.7838 (inf)	loss_scale 16384.0000 (17116.0306)	mem 11634MB
[2024-07-11 20:28:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:04:47 lr 0.000047	 wd 0.0000	time 0.3205 (0.4096)	loss 0.8208 (0.8063)	grad_norm 1.9807 (inf)	loss_scale 16384.0000 (17075.3848)	mem 11634MB
[2024-07-11 20:29:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:04:05 lr 0.000047	 wd 0.0000	time 0.2929 (0.4079)	loss 0.8257 (0.8063)	grad_norm 2.2620 (inf)	loss_scale 16384.0000 (17039.0153)	mem 11634MB
[2024-07-11 20:30:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:03:28 lr 0.000046	 wd 0.0000	time 0.3078 (0.4161)	loss 0.8579 (0.8063)	grad_norm 2.4401 (inf)	loss_scale 16384.0000 (17006.2809)	mem 11634MB
[2024-07-11 20:30:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:46 lr 0.000046	 wd 0.0000	time 0.2982 (0.4134)	loss 0.8555 (0.8063)	grad_norm 2.1114 (inf)	loss_scale 16384.0000 (16976.6625)	mem 11634MB
[2024-07-11 20:31:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:06 lr 0.000046	 wd 0.0000	time 0.3558 (0.4196)	loss 0.7900 (0.8065)	grad_norm 1.9683 (inf)	loss_scale 16384.0000 (16949.7356)	mem 11634MB
[2024-07-11 20:32:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:24 lr 0.000046	 wd 0.0000	time 0.3324 (0.4168)	loss 0.9131 (0.8061)	grad_norm 2.3585 (inf)	loss_scale 16384.0000 (16925.1491)	mem 11634MB
[2024-07-11 20:33:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:43 lr 0.000046	 wd 0.0000	time 0.4717 (0.4247)	loss 0.7798 (0.8060)	grad_norm 1.9269 (inf)	loss_scale 8192.0000 (16670.6006)	mem 11634MB
[2024-07-11 20:33:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000045	 wd 0.0000	time 0.2986 (0.4234)	loss 1.0430 (0.8063)	grad_norm 1.9760 (inf)	loss_scale 8192.0000 (16331.5922)	mem 11634MB
[2024-07-11 20:33:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 15 training takes 0:17:47
[2024-07-11 20:33:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_15.pth saving......
[2024-07-11 20:34:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_15.pth saved !!!
[2024-07-11 20:34:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 52.926 (52.926)	Loss 0.3967 (0.3967)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 20:35:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.132 Acc@5 97.404
[2024-07-11 20:35:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-11 20:35:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.13%
[2024-07-11 20:35:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 20:35:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 20:35:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][0/2502]	eta 11:23:49 lr 0.000045	 wd 0.0000	time 16.3985 (16.3985)	loss 0.7646 (0.7646)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:36:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:19:29 lr 0.000045	 wd 0.0000	time 0.2971 (0.4871)	loss 0.9209 (0.8062)	grad_norm 2.2690 (2.0878)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:36:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:15:36 lr 0.000045	 wd 0.0000	time 0.3037 (0.4066)	loss 0.8228 (0.8077)	grad_norm 2.0785 (2.1314)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:37:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:13:56 lr 0.000045	 wd 0.0000	time 0.2958 (0.3797)	loss 0.8174 (0.8064)	grad_norm 1.8184 (2.1183)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:37:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:12:51 lr 0.000045	 wd 0.0000	time 0.3215 (0.3668)	loss 0.8799 (0.8047)	grad_norm 1.8154 (2.1220)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:38:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:11:58 lr 0.000044	 wd 0.0000	time 0.3059 (0.3587)	loss 0.7148 (0.8028)	grad_norm 2.2346 (2.1530)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:38:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:11:11 lr 0.000044	 wd 0.0000	time 0.3167 (0.3531)	loss 0.7681 (0.8041)	grad_norm 2.5735 (2.1581)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:39:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:10:29 lr 0.000044	 wd 0.0000	time 0.3010 (0.3493)	loss 0.8296 (0.8039)	grad_norm 2.0057 (2.1475)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:39:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:49 lr 0.000044	 wd 0.0000	time 0.2964 (0.3464)	loss 0.9307 (0.8036)	grad_norm 2.7090 (2.1527)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:40:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:09:11 lr 0.000043	 wd 0.0000	time 0.3003 (0.3441)	loss 0.8105 (0.8056)	grad_norm 2.4362 (2.1544)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:40:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:08:34 lr 0.000043	 wd 0.0000	time 0.2909 (0.3425)	loss 0.7578 (0.8066)	grad_norm 2.0750 (2.1472)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:41:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:58 lr 0.000043	 wd 0.0000	time 0.2822 (0.3412)	loss 0.7104 (0.8054)	grad_norm 2.1030 (2.1554)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:42:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:22 lr 0.000043	 wd 0.0000	time 0.3217 (0.3400)	loss 0.8682 (0.8059)	grad_norm 1.9398 (2.1543)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:42:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:47 lr 0.000043	 wd 0.0000	time 0.3507 (0.3390)	loss 0.8174 (0.8042)	grad_norm 2.1254 (2.1511)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:43:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:06:24 lr 0.000042	 wd 0.0000	time 0.3113 (0.3493)	loss 0.8003 (0.8038)	grad_norm 2.2547 (2.1522)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:43:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:50 lr 0.000042	 wd 0.0000	time 0.2794 (0.3501)	loss 0.8306 (0.8035)	grad_norm 1.9538 (2.1499)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:44:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:05:25 lr 0.000042	 wd 0.0000	time 0.3013 (0.3614)	loss 0.7900 (0.8035)	grad_norm 2.1543 (2.1520)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:45:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:50 lr 0.000042	 wd 0.0000	time 0.3055 (0.3619)	loss 0.6914 (0.8044)	grad_norm 1.8237 (2.1498)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:46:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:04:21 lr 0.000042	 wd 0.0000	time 0.3397 (0.3723)	loss 0.8330 (0.8048)	grad_norm 2.2373 (2.1517)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:47:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:43 lr 0.000041	 wd 0.0000	time 0.3108 (0.3717)	loss 0.9224 (0.8053)	grad_norm 2.8075 (2.1526)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:47:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:03:09 lr 0.000041	 wd 0.0000	time 0.3414 (0.3772)	loss 0.7192 (0.8049)	grad_norm 2.0133 (2.1492)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:48:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:31 lr 0.000041	 wd 0.0000	time 0.3255 (0.3765)	loss 0.7197 (0.8048)	grad_norm 1.5765 (2.1461)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:49:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:53 lr 0.000041	 wd 0.0000	time 0.2911 (0.3754)	loss 0.7520 (0.8050)	grad_norm 1.9152 (2.1478)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:50:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:17 lr 0.000041	 wd 0.0000	time 0.2981 (0.3853)	loss 0.8330 (0.8050)	grad_norm 2.1970 (2.1451)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:50:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:39 lr 0.000040	 wd 0.0000	time 0.2990 (0.3838)	loss 0.7603 (0.8045)	grad_norm 2.0216 (2.1441)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:51:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.3057 (0.3931)	loss 0.8081 (0.8041)	grad_norm 2.1055 (2.1470)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:51:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 16 training takes 0:16:29
[2024-07-11 20:52:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 54.864 (54.864)	Loss 0.3823 (0.3823)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 20:52:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.236 Acc@5 97.400
[2024-07-11 20:52:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-11 20:52:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-11 20:52:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 20:52:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 20:53:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][0/2502]	eta 11:20:04 lr 0.000040	 wd 0.0000	time 16.3087 (16.3087)	loss 0.7339 (0.7339)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:53:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:19:18 lr 0.000040	 wd 0.0000	time 0.3132 (0.4823)	loss 0.8687 (0.7979)	grad_norm 1.8717 (2.0533)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:54:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:15:34 lr 0.000040	 wd 0.0000	time 0.2979 (0.4058)	loss 0.9424 (0.8055)	grad_norm 2.1291 (2.0562)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:54:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:13:53 lr 0.000040	 wd 0.0000	time 0.3330 (0.3785)	loss 0.8394 (0.8059)	grad_norm 2.8250 (2.0932)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:55:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:12:48 lr 0.000039	 wd 0.0000	time 0.3001 (0.3656)	loss 0.7300 (0.7996)	grad_norm 2.3874 (2.1050)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:55:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:55 lr 0.000039	 wd 0.0000	time 0.3070 (0.3576)	loss 0.7314 (0.8008)	grad_norm 2.1808 (2.1128)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:56:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:11:09 lr 0.000039	 wd 0.0000	time 0.2909 (0.3521)	loss 0.8433 (0.8017)	grad_norm 2.6808 (2.1110)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:57:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:10:27 lr 0.000039	 wd 0.0000	time 0.3005 (0.3483)	loss 0.7549 (0.8010)	grad_norm 1.9192 (2.1166)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:57:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:48 lr 0.000039	 wd 0.0000	time 0.2662 (0.3455)	loss 0.7285 (0.8011)	grad_norm 1.7371 (2.1246)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:58:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:09:09 lr 0.000038	 wd 0.0000	time 0.3027 (0.3432)	loss 0.7891 (0.8012)	grad_norm 2.2363 (2.1294)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:58:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:32 lr 0.000038	 wd 0.0000	time 0.3111 (0.3415)	loss 0.8501 (0.8012)	grad_norm 2.1064 (2.1323)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 20:59:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:08:21 lr 0.000038	 wd 0.0000	time 0.3009 (0.3579)	loss 0.8599 (0.8022)	grad_norm 1.9525 (2.1354)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:00:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:48 lr 0.000038	 wd 0.0000	time 0.3135 (0.3599)	loss 0.8662 (0.8025)	grad_norm 2.4851 (2.1315)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:01:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:07:30 lr 0.000038	 wd 0.0000	time 0.3092 (0.3745)	loss 0.9292 (0.8016)	grad_norm 2.0123 (2.1391)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:01:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:06:53 lr 0.000037	 wd 0.0000	time 0.2942 (0.3750)	loss 0.7266 (0.8021)	grad_norm 1.8892 (2.1421)	loss_scale 16384.0000 (8613.0021)	mem 11634MB
[2024-07-11 21:02:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:06:27 lr 0.000037	 wd 0.0000	time 0.3258 (0.3867)	loss 0.7866 (0.8024)	grad_norm 2.2228 (2.1410)	loss_scale 16384.0000 (9130.7235)	mem 11634MB
[2024-07-11 21:03:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:05:47 lr 0.000037	 wd 0.0000	time 0.3216 (0.3847)	loss 0.7031 (0.8019)	grad_norm 2.3606 (2.1437)	loss_scale 16384.0000 (9583.7701)	mem 11634MB
[2024-07-11 21:04:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:05:14 lr 0.000037	 wd 0.0000	time 0.6093 (0.3926)	loss 0.9351 (0.8017)	grad_norm 2.4063 (2.1451)	loss_scale 16384.0000 (9983.5485)	mem 11634MB
[2024-07-11 21:04:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:04:34 lr 0.000037	 wd 0.0000	time 0.2699 (0.3906)	loss 0.7285 (0.8017)	grad_norm 1.9622 (2.1425)	loss_scale 16384.0000 (10338.9317)	mem 11634MB
[2024-07-11 21:05:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:53 lr 0.000036	 wd 0.0000	time 0.3710 (0.3885)	loss 0.7446 (0.8019)	grad_norm 2.2077 (2.1383)	loss_scale 16384.0000 (10656.9258)	mem 11634MB
[2024-07-11 21:06:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:03:21 lr 0.000036	 wd 0.0000	time 0.3685 (0.4010)	loss 0.7539 (0.8014)	grad_norm 1.7671 (2.1390)	loss_scale 16384.0000 (10943.1364)	mem 11634MB
[2024-07-11 21:06:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:40 lr 0.000036	 wd 0.0000	time 0.3058 (0.3986)	loss 0.7495 (0.8011)	grad_norm 2.0961 (2.1413)	loss_scale 16384.0000 (11202.1019)	mem 11634MB
[2024-07-11 21:08:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:04 lr 0.000036	 wd 0.0000	time 0.3434 (0.4116)	loss 0.8159 (0.8011)	grad_norm 2.3688 (2.1405)	loss_scale 16384.0000 (11437.5357)	mem 11634MB
[2024-07-11 21:09:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:24 lr 0.000036	 wd 0.0000	time 0.3151 (0.4205)	loss 0.7603 (0.8008)	grad_norm 1.9140 (2.1382)	loss_scale 16384.0000 (11652.5059)	mem 11634MB
[2024-07-11 21:10:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:43 lr 0.000035	 wd 0.0000	time 0.5357 (0.4264)	loss 0.6846 (0.8015)	grad_norm 2.0644 (2.1359)	loss_scale 16384.0000 (11849.5693)	mem 11634MB
[2024-07-11 21:10:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.3075 (0.4241)	loss 0.9585 (0.8018)	grad_norm 2.1851 (2.1359)	loss_scale 16384.0000 (12030.8741)	mem 11634MB
[2024-07-11 21:10:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 17 training takes 0:17:52
[2024-07-11 21:11:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 28.519 (28.519)	Loss 0.3787 (0.3787)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 21:11:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.174 Acc@5 97.392
[2024-07-11 21:11:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-11 21:11:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-11 21:12:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][0/2502]	eta 1 day, 1:24:54 lr 0.000035	 wd 0.0000	time 36.5685 (36.5685)	loss 0.8423 (0.8423)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:12:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:27:31 lr 0.000035	 wd 0.0000	time 0.3042 (0.6875)	loss 0.7515 (0.7935)	grad_norm 2.1592 (2.2135)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:13:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:19:29 lr 0.000035	 wd 0.0000	time 0.2739 (0.5079)	loss 0.8857 (0.7923)	grad_norm 2.2590 (2.1830)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:13:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:16:27 lr 0.000035	 wd 0.0000	time 0.3161 (0.4483)	loss 0.8828 (0.7960)	grad_norm 2.2998 (2.1731)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:14:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:14:38 lr 0.000034	 wd 0.0000	time 0.3161 (0.4179)	loss 0.8306 (0.7980)	grad_norm 1.8951 (2.1715)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:14:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:13:19 lr 0.000034	 wd 0.0000	time 0.2931 (0.3994)	loss 0.7705 (0.7988)	grad_norm 2.1199 (2.1566)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:15:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:12:16 lr 0.000034	 wd 0.0000	time 0.2955 (0.3872)	loss 0.7285 (0.7979)	grad_norm 3.0012 (2.1528)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:16:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:11:21 lr 0.000034	 wd 0.0000	time 0.3053 (0.3783)	loss 0.8491 (0.7963)	grad_norm 1.9097 (2.1407)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:16:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:10:32 lr 0.000034	 wd 0.0000	time 0.3010 (0.3715)	loss 0.8569 (0.7942)	grad_norm 2.1850 (2.1369)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:17:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:09:46 lr 0.000033	 wd 0.0000	time 0.3024 (0.3664)	loss 0.8320 (0.7944)	grad_norm 2.3069 (2.1453)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:17:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:09:04 lr 0.000033	 wd 0.0000	time 0.3032 (0.3622)	loss 0.7520 (0.7945)	grad_norm 1.8290 (2.1462)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:18:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:08:23 lr 0.000033	 wd 0.0000	time 0.3056 (0.3589)	loss 0.7646 (0.7954)	grad_norm 1.8027 (2.1445)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:19:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:08:17 lr 0.000033	 wd 0.0000	time 0.3453 (0.3820)	loss 1.0742 (0.7949)	grad_norm 1.8523 (2.1447)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:20:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:07:43 lr 0.000033	 wd 0.0000	time 0.2855 (0.3860)	loss 0.6758 (0.7953)	grad_norm 1.9329 (2.1359)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:20:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:07:06 lr 0.000032	 wd 0.0000	time 0.2970 (0.3868)	loss 0.7559 (0.7944)	grad_norm 2.5012 (2.1356)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:21:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:06:23 lr 0.000032	 wd 0.0000	time 0.3144 (0.3827)	loss 0.7212 (0.7946)	grad_norm 3.3584 (2.1327)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:21:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:05:42 lr 0.000032	 wd 0.0000	time 0.3813 (0.3794)	loss 0.7900 (0.7945)	grad_norm 2.0623 (2.1306)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:22:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:05:02 lr 0.000032	 wd 0.0000	time 0.2992 (0.3766)	loss 0.9268 (0.7945)	grad_norm 2.0565 (2.1346)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:22:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:04:22 lr 0.000032	 wd 0.0000	time 0.3068 (0.3738)	loss 0.8237 (0.7948)	grad_norm 1.8837 (2.1338)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:23:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:43 lr 0.000032	 wd 0.0000	time 0.3097 (0.3717)	loss 0.9438 (0.7949)	grad_norm 1.8410 (2.1282)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:23:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:03:05 lr 0.000031	 wd 0.0000	time 0.2978 (0.3699)	loss 0.9072 (0.7956)	grad_norm 2.1177 (2.1282)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:24:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:27 lr 0.000031	 wd 0.0000	time 0.3183 (0.3679)	loss 0.7129 (0.7956)	grad_norm 2.1543 (2.1340)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:25:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:50 lr 0.000031	 wd 0.0000	time 0.4964 (0.3666)	loss 0.7056 (0.7960)	grad_norm 1.8566 (2.1331)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:26:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:16 lr 0.000031	 wd 0.0000	time 0.3014 (0.3798)	loss 0.7700 (0.7962)	grad_norm 2.0512 (2.1348)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:27:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:40 lr 0.000031	 wd 0.0000	time 2.3394 (0.3982)	loss 0.8262 (0.7960)	grad_norm 2.3927 (2.1351)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:28:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000030	 wd 0.0000	time 0.3040 (0.4024)	loss 0.8179 (0.7963)	grad_norm 1.9237 (2.1357)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:28:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 18 training takes 0:17:02
[2024-07-11 21:29:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 30.583 (30.583)	Loss 0.3909 (0.3909)	Acc@1 91.016 (91.016)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 21:29:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.164 Acc@5 97.430
[2024-07-11 21:29:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-11 21:29:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-11 21:30:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][0/2502]	eta 1 day, 1:20:18 lr 0.000030	 wd 0.0000	time 36.4582 (36.4582)	loss 0.7461 (0.7461)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:30:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:27:17 lr 0.000030	 wd 0.0000	time 0.3023 (0.6816)	loss 0.9282 (0.8033)	grad_norm 2.3119 (2.1875)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 21:31:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:19:19 lr 0.000030	 wd 0.0000	time 0.3012 (0.5039)	loss 0.6650 (0.8041)	grad_norm 2.1967 (inf)	loss_scale 8192.0000 (15242.8259)	mem 11634MB
[2024-07-11 21:31:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:16:22 lr 0.000030	 wd 0.0000	time 0.2893 (0.4461)	loss 0.7563 (0.8021)	grad_norm 1.8849 (inf)	loss_scale 8192.0000 (12900.3588)	mem 11634MB
[2024-07-11 21:32:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:14:34 lr 0.000030	 wd 0.0000	time 0.2885 (0.4162)	loss 0.8364 (0.7992)	grad_norm 1.7779 (inf)	loss_scale 8192.0000 (11726.2045)	mem 11634MB
[2024-07-11 21:32:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:13:17 lr 0.000029	 wd 0.0000	time 0.3182 (0.3982)	loss 0.8398 (0.7968)	grad_norm 1.6712 (inf)	loss_scale 8192.0000 (11020.7745)	mem 11634MB
[2024-07-11 21:33:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:12:15 lr 0.000029	 wd 0.0000	time 0.3231 (0.3867)	loss 0.7368 (0.7967)	grad_norm 1.9329 (inf)	loss_scale 8192.0000 (10550.0965)	mem 11634MB
[2024-07-11 21:33:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:11:20 lr 0.000029	 wd 0.0000	time 0.3131 (0.3779)	loss 0.8315 (0.7968)	grad_norm 2.8656 (inf)	loss_scale 8192.0000 (10213.7061)	mem 11634MB
[2024-07-11 21:34:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:10:31 lr 0.000029	 wd 0.0000	time 0.3400 (0.3712)	loss 0.6807 (0.7959)	grad_norm 2.1801 (inf)	loss_scale 8192.0000 (9961.3084)	mem 11634MB
[2024-07-11 21:34:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:09:47 lr 0.000029	 wd 0.0000	time 0.2980 (0.3665)	loss 0.7051 (0.7944)	grad_norm 2.6065 (inf)	loss_scale 8192.0000 (9764.9367)	mem 11634MB
[2024-07-11 21:35:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:09:04 lr 0.000028	 wd 0.0000	time 0.2889 (0.3623)	loss 0.7163 (0.7945)	grad_norm 2.6543 (inf)	loss_scale 8192.0000 (9607.8002)	mem 11634MB
[2024-07-11 21:36:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:08:23 lr 0.000028	 wd 0.0000	time 0.3085 (0.3590)	loss 0.9106 (0.7947)	grad_norm 2.6854 (inf)	loss_scale 8192.0000 (9479.2080)	mem 11634MB
[2024-07-11 21:37:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:08:20 lr 0.000028	 wd 0.0000	time 0.3330 (0.3843)	loss 0.7939 (0.7944)	grad_norm 1.9434 (inf)	loss_scale 8192.0000 (9372.0300)	mem 11634MB
[2024-07-11 21:38:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:08:12 lr 0.000028	 wd 0.0000	time 0.3162 (0.4097)	loss 0.6724 (0.7951)	grad_norm 1.9621 (inf)	loss_scale 8192.0000 (9281.3282)	mem 11634MB
[2024-07-11 21:38:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:07:24 lr 0.000028	 wd 0.0000	time 0.3140 (0.4037)	loss 0.7070 (0.7956)	grad_norm 2.2496 (inf)	loss_scale 8192.0000 (9203.5746)	mem 11634MB
[2024-07-11 21:39:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:06:39 lr 0.000028	 wd 0.0000	time 0.3108 (0.3987)	loss 0.8535 (0.7962)	grad_norm 1.9961 (inf)	loss_scale 8192.0000 (9136.1812)	mem 11634MB
[2024-07-11 21:40:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:05:55 lr 0.000027	 wd 0.0000	time 0.3026 (0.3943)	loss 0.7466 (0.7962)	grad_norm 2.1571 (inf)	loss_scale 8192.0000 (9077.2067)	mem 11634MB
[2024-07-11 21:40:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:05:13 lr 0.000027	 wd 0.0000	time 0.2777 (0.3904)	loss 0.6870 (0.7959)	grad_norm 2.4615 (inf)	loss_scale 8192.0000 (9025.1664)	mem 11634MB
[2024-07-11 21:41:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:04:31 lr 0.000027	 wd 0.0000	time 0.2730 (0.3869)	loss 0.6938 (0.7963)	grad_norm 1.9709 (inf)	loss_scale 8192.0000 (8978.9051)	mem 11634MB
[2024-07-11 21:41:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:51 lr 0.000027	 wd 0.0000	time 0.3025 (0.3842)	loss 0.7827 (0.7960)	grad_norm 2.3824 (inf)	loss_scale 8192.0000 (8937.5108)	mem 11634MB
[2024-07-11 21:42:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:03:11 lr 0.000027	 wd 0.0000	time 0.3101 (0.3814)	loss 0.8564 (0.7952)	grad_norm 2.0199 (inf)	loss_scale 8192.0000 (8900.2539)	mem 11634MB
[2024-07-11 21:42:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:32 lr 0.000026	 wd 0.0000	time 0.3220 (0.3789)	loss 0.9019 (0.7947)	grad_norm 2.0694 (inf)	loss_scale 8192.0000 (8866.5436)	mem 11634MB
[2024-07-11 21:43:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:55 lr 0.000026	 wd 0.0000	time 0.3127 (0.3838)	loss 0.8198 (0.7948)	grad_norm 2.8911 (inf)	loss_scale 8192.0000 (8835.8964)	mem 11634MB
[2024-07-11 21:44:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:17 lr 0.000026	 wd 0.0000	time 0.3692 (0.3834)	loss 0.8438 (0.7950)	grad_norm 1.6151 (inf)	loss_scale 8192.0000 (8807.9131)	mem 11634MB
[2024-07-11 21:45:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:40 lr 0.000026	 wd 0.0000	time 4.4099 (0.4004)	loss 0.9878 (0.7948)	grad_norm 2.6990 (inf)	loss_scale 8192.0000 (8782.2607)	mem 11634MB
[2024-07-11 21:46:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.3079 (0.4057)	loss 0.6948 (0.7945)	grad_norm 1.8993 (inf)	loss_scale 8192.0000 (8758.6597)	mem 11634MB
[2024-07-11 21:46:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 19 training takes 0:17:09
[2024-07-11 21:47:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 30.149 (30.149)	Loss 0.3884 (0.3884)	Acc@1 91.211 (91.211)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 21:47:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.226 Acc@5 97.424
[2024-07-11 21:47:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-11 21:47:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-11 21:48:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][0/2502]	eta 23:42:29 lr 0.000026	 wd 0.0000	time 34.1125 (34.1125)	loss 0.9033 (0.9033)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:48:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:28:14 lr 0.000026	 wd 0.0000	time 0.3013 (0.7055)	loss 0.8228 (0.7998)	grad_norm 2.2274 (2.2253)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:49:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:19:48 lr 0.000025	 wd 0.0000	time 0.3228 (0.5162)	loss 0.8516 (0.7949)	grad_norm 1.7087 (2.1668)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:49:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:16:38 lr 0.000025	 wd 0.0000	time 0.3090 (0.4536)	loss 0.7310 (0.7939)	grad_norm 2.4171 (2.1890)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:50:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:14:45 lr 0.000025	 wd 0.0000	time 0.2736 (0.4214)	loss 0.6797 (0.7905)	grad_norm 2.1303 (2.1823)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:50:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:13:25 lr 0.000025	 wd 0.0000	time 0.3109 (0.4021)	loss 0.8408 (0.7910)	grad_norm 2.0216 (2.1825)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:51:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:12:21 lr 0.000025	 wd 0.0000	time 0.2957 (0.3896)	loss 0.7407 (0.7910)	grad_norm 1.9757 (2.1879)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:51:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:11:25 lr 0.000025	 wd 0.0000	time 0.2744 (0.3804)	loss 0.7124 (0.7915)	grad_norm 1.8473 (2.1867)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:52:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:10:36 lr 0.000024	 wd 0.0000	time 0.3278 (0.3739)	loss 0.7974 (0.7920)	grad_norm 1.8818 (2.1704)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:53:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:09:51 lr 0.000024	 wd 0.0000	time 0.2653 (0.3690)	loss 0.9292 (0.7916)	grad_norm 2.6922 (2.1625)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:53:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:09:07 lr 0.000024	 wd 0.0000	time 0.3062 (0.3646)	loss 0.7300 (0.7931)	grad_norm 1.8602 (2.1613)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:54:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:08:26 lr 0.000024	 wd 0.0000	time 0.2845 (0.3611)	loss 0.7324 (0.7931)	grad_norm 2.7402 (2.1612)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:55:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:08:41 lr 0.000024	 wd 0.0000	time 5.2945 (0.4002)	loss 0.7085 (0.7926)	grad_norm 2.2376 (2.1535)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:56:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:08:21 lr 0.000023	 wd 0.0000	time 0.3044 (0.4170)	loss 0.7666 (0.7919)	grad_norm 2.5142 (2.1547)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:57:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:07:32 lr 0.000023	 wd 0.0000	time 0.2678 (0.4106)	loss 0.7793 (0.7920)	grad_norm 2.6025 (2.1568)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:57:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:06:45 lr 0.000023	 wd 0.0000	time 0.3178 (0.4051)	loss 0.8784 (0.7917)	grad_norm 2.4143 (2.1577)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:58:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:06:00 lr 0.000023	 wd 0.0000	time 0.3182 (0.4002)	loss 0.8335 (0.7906)	grad_norm 1.9939 (2.1600)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 21:58:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:05:17 lr 0.000023	 wd 0.0000	time 0.3051 (0.3959)	loss 0.9668 (0.7903)	grad_norm 1.7021 (2.1605)	loss_scale 16384.0000 (8336.4797)	mem 11634MB
[2024-07-11 21:59:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:04:35 lr 0.000023	 wd 0.0000	time 0.3502 (0.3921)	loss 0.8286 (0.7903)	grad_norm 2.1928 (2.1604)	loss_scale 16384.0000 (8783.3159)	mem 11634MB
[2024-07-11 21:59:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:54 lr 0.000022	 wd 0.0000	time 0.2728 (0.3890)	loss 0.8296 (0.7904)	grad_norm 2.5822 (2.1595)	loss_scale 16384.0000 (9183.1415)	mem 11634MB
[2024-07-11 22:00:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:03:13 lr 0.000022	 wd 0.0000	time 0.3337 (0.3859)	loss 0.8164 (0.7903)	grad_norm 2.0482 (2.1583)	loss_scale 16384.0000 (9543.0045)	mem 11634MB
[2024-07-11 22:00:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:34 lr 0.000022	 wd 0.0000	time 0.2945 (0.3832)	loss 0.8418 (0.7905)	grad_norm 2.2167 (2.1578)	loss_scale 16384.0000 (9868.6111)	mem 11634MB
[2024-07-11 22:01:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:59 lr 0.000022	 wd 0.0000	time 0.3084 (0.3941)	loss 0.7627 (0.7903)	grad_norm 1.9825 (2.1592)	loss_scale 16384.0000 (10164.6306)	mem 11634MB
[2024-07-11 22:02:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:19 lr 0.000022	 wd 0.0000	time 0.4189 (0.3929)	loss 0.7671 (0.7906)	grad_norm 1.7650 (2.1609)	loss_scale 16384.0000 (10434.9205)	mem 11634MB
[2024-07-11 22:03:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:41 lr 0.000022	 wd 0.0000	time 0.3177 (0.4051)	loss 0.7612 (0.7909)	grad_norm 1.7734 (2.1594)	loss_scale 16384.0000 (10682.6955)	mem 11634MB
[2024-07-11 22:04:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.3117 (0.4024)	loss 0.7622 (0.7908)	grad_norm 1.7722 (2.1589)	loss_scale 16384.0000 (10910.6565)	mem 11634MB
[2024-07-11 22:04:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 20 training takes 0:17:09
[2024-07-11 22:05:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 75.222 (75.222)	Loss 0.3904 (0.3904)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 22:06:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.244 Acc@5 97.384
[2024-07-11 22:06:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-11 22:06:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-11 22:06:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 22:06:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 22:06:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][0/2502]	eta 10:21:53 lr 0.000021	 wd 0.0000	time 14.9133 (14.9133)	loss 0.8457 (0.8457)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:07:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:18:48 lr 0.000021	 wd 0.0000	time 0.2993 (0.4698)	loss 0.7744 (0.7906)	grad_norm 2.3667 (2.2443)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:07:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:15:19 lr 0.000021	 wd 0.0000	time 0.3205 (0.3995)	loss 0.8140 (0.7937)	grad_norm 2.1208 (2.1962)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:08:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:13:46 lr 0.000021	 wd 0.0000	time 0.3369 (0.3752)	loss 0.7505 (0.7867)	grad_norm 2.0721 (2.1781)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:08:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:12:42 lr 0.000021	 wd 0.0000	time 0.3040 (0.3627)	loss 0.6885 (0.7848)	grad_norm 2.2556 (2.1861)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:09:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:51 lr 0.000021	 wd 0.0000	time 0.3142 (0.3554)	loss 0.9404 (0.7873)	grad_norm 2.1355 (2.1832)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:09:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:11:06 lr 0.000020	 wd 0.0000	time 0.3001 (0.3505)	loss 0.8452 (0.7870)	grad_norm 2.0391 (2.1763)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:10:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:10:25 lr 0.000020	 wd 0.0000	time 0.3032 (0.3473)	loss 0.7725 (0.7866)	grad_norm 2.4132 (2.1643)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:10:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:47 lr 0.000020	 wd 0.0000	time 0.2938 (0.3451)	loss 0.8003 (0.7863)	grad_norm 1.8643 (2.1606)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:11:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:09:09 lr 0.000020	 wd 0.0000	time 0.3335 (0.3430)	loss 0.8188 (0.7859)	grad_norm 1.8395 (2.1575)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:12:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:08:32 lr 0.000020	 wd 0.0000	time 0.3002 (0.3414)	loss 0.7271 (0.7852)	grad_norm 2.3575 (2.1534)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:13:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:08:39 lr 0.000020	 wd 0.0000	time 0.3373 (0.3703)	loss 0.7490 (0.7861)	grad_norm 1.7245 (2.1579)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:14:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:08:35 lr 0.000019	 wd 0.0000	time 0.2930 (0.3958)	loss 0.8506 (0.7858)	grad_norm 3.0648 (2.1617)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:14:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:07:49 lr 0.000019	 wd 0.0000	time 0.3212 (0.3905)	loss 0.8970 (0.7857)	grad_norm 1.7837 (2.1657)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-11 22:15:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:07:05 lr 0.000019	 wd 0.0000	time 0.2930 (0.3860)	loss 0.7173 (0.7857)	grad_norm 2.0246 (inf)	loss_scale 8192.0000 (16278.7495)	mem 11634MB
[2024-07-11 22:15:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:06:22 lr 0.000019	 wd 0.0000	time 0.3012 (0.3821)	loss 0.7793 (0.7858)	grad_norm 2.1383 (inf)	loss_scale 8192.0000 (15739.9920)	mem 11634MB
[2024-07-11 22:16:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:05:41 lr 0.000019	 wd 0.0000	time 0.3273 (0.3786)	loss 0.7632 (0.7850)	grad_norm 2.6866 (inf)	loss_scale 8192.0000 (15268.5372)	mem 11634MB
[2024-07-11 22:17:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:05:01 lr 0.000019	 wd 0.0000	time 0.3253 (0.3756)	loss 0.7930 (0.7847)	grad_norm 2.1595 (inf)	loss_scale 8192.0000 (14852.5150)	mem 11634MB
[2024-07-11 22:17:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:04:21 lr 0.000018	 wd 0.0000	time 0.2867 (0.3731)	loss 0.8120 (0.7843)	grad_norm 2.3061 (inf)	loss_scale 8192.0000 (14482.6918)	mem 11634MB
[2024-07-11 22:18:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:43 lr 0.000018	 wd 0.0000	time 0.3207 (0.3707)	loss 0.6831 (0.7839)	grad_norm 2.4995 (inf)	loss_scale 8192.0000 (14151.7770)	mem 11634MB
[2024-07-11 22:18:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:03:05 lr 0.000018	 wd 0.0000	time 0.3107 (0.3685)	loss 0.7524 (0.7842)	grad_norm 1.9322 (inf)	loss_scale 8192.0000 (13853.9370)	mem 11634MB
[2024-07-11 22:19:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:30 lr 0.000018	 wd 0.0000	time 0.2956 (0.3741)	loss 0.7939 (0.7844)	grad_norm 2.2085 (inf)	loss_scale 8192.0000 (13584.4493)	mem 11634MB
[2024-07-11 22:20:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:52 lr 0.000018	 wd 0.0000	time 0.2683 (0.3741)	loss 0.6611 (0.7843)	grad_norm 1.8664 (inf)	loss_scale 8192.0000 (13339.4493)	mem 11634MB
[2024-07-11 22:21:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:19 lr 0.000018	 wd 0.0000	time 0.6300 (0.3918)	loss 0.8564 (0.7850)	grad_norm 2.4419 (inf)	loss_scale 8192.0000 (13115.7445)	mem 11634MB
[2024-07-11 22:22:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:41 lr 0.000018	 wd 0.0000	time 0.3144 (0.4043)	loss 0.7896 (0.7852)	grad_norm 1.9179 (inf)	loss_scale 8192.0000 (12910.6739)	mem 11634MB
[2024-07-11 22:23:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0000	time 0.3107 (0.4011)	loss 0.8286 (0.7857)	grad_norm 1.8058 (inf)	loss_scale 8192.0000 (12722.0024)	mem 11634MB
[2024-07-11 22:23:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 21 training takes 0:16:50
[2024-07-11 22:23:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 29.613 (29.613)	Loss 0.3877 (0.3877)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 22:24:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.194 Acc@5 97.422
[2024-07-11 22:24:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-11 22:24:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-11 22:24:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][0/2502]	eta 10:58:35 lr 0.000017	 wd 0.0000	time 15.7937 (15.7937)	loss 0.8247 (0.8247)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:24:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:19:25 lr 0.000017	 wd 0.0000	time 0.2923 (0.4853)	loss 0.8647 (0.7952)	grad_norm 1.8680 (2.2588)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:25:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:15:36 lr 0.000017	 wd 0.0000	time 0.3042 (0.4069)	loss 0.6758 (0.7933)	grad_norm 2.0120 (2.1799)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:26:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:13:55 lr 0.000017	 wd 0.0000	time 0.3017 (0.3796)	loss 0.8438 (0.7896)	grad_norm 2.4003 (2.1959)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:26:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:12:49 lr 0.000017	 wd 0.0000	time 0.3278 (0.3662)	loss 0.8628 (0.7909)	grad_norm 1.8186 (2.1766)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:27:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:57 lr 0.000017	 wd 0.0000	time 0.2946 (0.3581)	loss 0.9014 (0.7928)	grad_norm 2.0596 (2.1803)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:27:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:11:10 lr 0.000016	 wd 0.0000	time 0.2894 (0.3527)	loss 0.9307 (0.7926)	grad_norm 2.6630 (2.1783)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:28:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:10:28 lr 0.000016	 wd 0.0000	time 0.3007 (0.3486)	loss 0.7744 (0.7932)	grad_norm 2.2673 (2.1786)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:28:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:48 lr 0.000016	 wd 0.0000	time 0.3056 (0.3458)	loss 0.7275 (0.7936)	grad_norm 2.2274 (2.1777)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:29:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:09:56 lr 0.000016	 wd 0.0000	time 0.5000 (0.3722)	loss 0.7891 (0.7924)	grad_norm 1.8039 (2.1809)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:30:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:09:18 lr 0.000016	 wd 0.0000	time 0.3998 (0.3717)	loss 0.6680 (0.7931)	grad_norm 2.4684 (2.1777)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:31:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:09:54 lr 0.000016	 wd 0.0000	time 0.4484 (0.4239)	loss 0.7222 (0.7932)	grad_norm 2.1961 (2.1866)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:32:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:09:08 lr 0.000016	 wd 0.0000	time 0.3098 (0.4214)	loss 0.7900 (0.7932)	grad_norm 2.0904 (2.1858)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:33:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:08:17 lr 0.000015	 wd 0.0000	time 0.3093 (0.4140)	loss 0.7598 (0.7926)	grad_norm 2.7145 (2.1753)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:33:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:07:29 lr 0.000015	 wd 0.0000	time 0.3091 (0.4078)	loss 0.7915 (0.7924)	grad_norm 2.2649 (2.1763)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:34:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:06:43 lr 0.000015	 wd 0.0000	time 0.3081 (0.4024)	loss 0.6128 (0.7926)	grad_norm 2.0826 (2.1746)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:34:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:05:58 lr 0.000015	 wd 0.0000	time 0.2908 (0.3976)	loss 0.8804 (0.7926)	grad_norm 1.8903 (2.1775)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:35:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:05:15 lr 0.000015	 wd 0.0000	time 0.3209 (0.3936)	loss 0.6509 (0.7914)	grad_norm 1.8639 (2.1765)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:35:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:04:33 lr 0.000015	 wd 0.0000	time 0.3237 (0.3901)	loss 0.9321 (0.7907)	grad_norm 1.9513 (2.1764)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:36:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:52 lr 0.000015	 wd 0.0000	time 0.3035 (0.3867)	loss 0.8296 (0.7911)	grad_norm 2.2652 (2.1799)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:37:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:03:15 lr 0.000014	 wd 0.0000	time 0.3120 (0.3885)	loss 0.7690 (0.7911)	grad_norm 2.3693 (2.1780)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:37:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:38 lr 0.000014	 wd 0.0000	time 0.2921 (0.3938)	loss 0.7573 (0.7910)	grad_norm 1.9831 (2.1797)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:39:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:03 lr 0.000014	 wd 0.0000	time 0.9730 (0.4090)	loss 0.6426 (0.7902)	grad_norm 2.0242 (2.1827)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:40:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:25 lr 0.000014	 wd 0.0000	time 0.3075 (0.4232)	loss 0.7568 (0.7897)	grad_norm 2.4566 (2.1843)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:40:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:42 lr 0.000014	 wd 0.0000	time 0.2892 (0.4193)	loss 0.7520 (0.7901)	grad_norm 2.3491 (2.1805)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:41:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.3029 (0.4156)	loss 0.7368 (0.7899)	grad_norm 2.0768 (2.1794)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:41:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 22 training takes 0:17:29
[2024-07-11 22:42:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 38.775 (38.775)	Loss 0.3877 (0.3877)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 11634MB
[2024-07-11 22:42:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.306 Acc@5 97.418
[2024-07-11 22:42:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-11 22:42:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.31%
[2024-07-11 22:42:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 22:42:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 22:42:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][0/2502]	eta 9:51:16 lr 0.000014	 wd 0.0000	time 14.1794 (14.1794)	loss 0.8105 (0.8105)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:43:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:19:03 lr 0.000014	 wd 0.0000	time 0.3265 (0.4760)	loss 0.8418 (0.7836)	grad_norm 2.4444 (2.2524)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:43:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:15:23 lr 0.000013	 wd 0.0000	time 0.3169 (0.4013)	loss 0.7075 (0.7813)	grad_norm 2.2412 (2.2196)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:44:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:13:47 lr 0.000013	 wd 0.0000	time 0.3029 (0.3756)	loss 0.7612 (0.7837)	grad_norm 2.3905 (2.2088)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 22:45:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:12:42 lr 0.000013	 wd 0.0000	time 0.2693 (0.3629)	loss 0.8750 (0.7860)	grad_norm 2.0298 (2.2156)	loss_scale 16384.0000 (8641.4364)	mem 11634MB
[2024-07-11 22:45:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:51 lr 0.000013	 wd 0.0000	time 0.3149 (0.3556)	loss 0.8960 (0.7884)	grad_norm 3.1699 (2.2242)	loss_scale 16384.0000 (10186.8583)	mem 11634MB
[2024-07-11 22:46:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:11:06 lr 0.000013	 wd 0.0000	time 0.3007 (0.3503)	loss 0.9521 (0.7883)	grad_norm 4.2517 (2.2309)	loss_scale 16384.0000 (11217.9967)	mem 11634MB
[2024-07-11 22:46:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:10:24 lr 0.000013	 wd 0.0000	time 0.2789 (0.3468)	loss 0.8467 (0.7894)	grad_norm 2.1502 (2.2264)	loss_scale 16384.0000 (11954.9444)	mem 11634MB
[2024-07-11 22:47:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:45 lr 0.000013	 wd 0.0000	time 0.3136 (0.3443)	loss 0.7124 (0.7875)	grad_norm 1.9959 (2.2151)	loss_scale 16384.0000 (12507.8851)	mem 11634MB
[2024-07-11 22:47:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:09:08 lr 0.000012	 wd 0.0000	time 0.3053 (0.3421)	loss 1.0020 (0.7868)	grad_norm 2.2950 (2.2143)	loss_scale 16384.0000 (12938.0866)	mem 11634MB
[2024-07-11 22:48:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:31 lr 0.000012	 wd 0.0000	time 0.2926 (0.3404)	loss 0.7104 (0.7877)	grad_norm 2.2754 (2.2101)	loss_scale 16384.0000 (13282.3337)	mem 11634MB
[2024-07-11 22:48:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:55 lr 0.000012	 wd 0.0000	time 0.3282 (0.3394)	loss 0.7427 (0.7865)	grad_norm 2.0906 (2.2034)	loss_scale 16384.0000 (13564.0472)	mem 11634MB
[2024-07-11 22:49:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:20 lr 0.000012	 wd 0.0000	time 0.2756 (0.3382)	loss 0.7607 (0.7865)	grad_norm 2.6000 (2.2006)	loss_scale 16384.0000 (13798.8476)	mem 11634MB
[2024-07-11 22:49:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:45 lr 0.000012	 wd 0.0000	time 0.3215 (0.3374)	loss 0.8403 (0.7853)	grad_norm 1.9933 (2.1952)	loss_scale 16384.0000 (13997.5527)	mem 11634MB
[2024-07-11 22:51:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:06:38 lr 0.000012	 wd 0.0000	time 0.3316 (0.3617)	loss 0.8145 (0.7851)	grad_norm 2.6164 (2.1994)	loss_scale 16384.0000 (14167.8915)	mem 11634MB
[2024-07-11 22:51:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:06:06 lr 0.000012	 wd 0.0000	time 0.2955 (0.3656)	loss 0.8774 (0.7845)	grad_norm 3.0962 (2.1961)	loss_scale 16384.0000 (14315.5336)	mem 11634MB
[2024-07-11 22:52:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:05:27 lr 0.000012	 wd 0.0000	time 0.2686 (0.3632)	loss 0.7593 (0.7846)	grad_norm 2.3283 (2.1890)	loss_scale 16384.0000 (14444.7320)	mem 11634MB
[2024-07-11 22:52:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:49 lr 0.000011	 wd 0.0000	time 0.2973 (0.3611)	loss 0.6865 (0.7848)	grad_norm 1.9395 (2.1921)	loss_scale 16384.0000 (14558.7396)	mem 11634MB
[2024-07-11 22:53:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:04:12 lr 0.000011	 wd 0.0000	time 0.3028 (0.3596)	loss 0.8398 (0.7851)	grad_norm 2.0693 (2.1937)	loss_scale 16384.0000 (14660.0866)	mem 11634MB
[2024-07-11 22:53:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:35 lr 0.000011	 wd 0.0000	time 0.2933 (0.3580)	loss 0.7241 (0.7850)	grad_norm 1.7938 (2.1957)	loss_scale 16384.0000 (14750.7712)	mem 11634MB
[2024-07-11 22:54:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:58 lr 0.000011	 wd 0.0000	time 0.3100 (0.3564)	loss 0.8628 (0.7844)	grad_norm 2.2350 (2.1948)	loss_scale 16384.0000 (14832.3918)	mem 11634MB
[2024-07-11 22:55:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:24 lr 0.000011	 wd 0.0000	time 0.4540 (0.3591)	loss 0.8169 (0.7849)	grad_norm 3.2015 (2.1959)	loss_scale 16384.0000 (14906.2427)	mem 11634MB
[2024-07-11 22:55:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:48 lr 0.000011	 wd 0.0000	time 0.2749 (0.3600)	loss 0.8130 (0.7847)	grad_norm 1.7325 (2.1976)	loss_scale 16384.0000 (14973.3830)	mem 11634MB
[2024-07-11 22:56:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:12 lr 0.000011	 wd 0.0000	time 0.3354 (0.3594)	loss 0.8354 (0.7849)	grad_norm 2.4164 (inf)	loss_scale 8192.0000 (14771.2334)	mem 11634MB
[2024-07-11 22:57:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:37 lr 0.000011	 wd 0.0000	time 0.3321 (0.3721)	loss 0.7812 (0.7849)	grad_norm 2.0754 (inf)	loss_scale 8192.0000 (14497.2128)	mem 11634MB
[2024-07-11 22:58:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.3038 (0.3708)	loss 0.8228 (0.7845)	grad_norm 1.8278 (inf)	loss_scale 8192.0000 (14245.1052)	mem 11634MB
[2024-07-11 22:58:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 23 training takes 0:15:52
[2024-07-11 22:59:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 77.308 (77.308)	Loss 0.3916 (0.3916)	Acc@1 91.602 (91.602)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 23:00:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.294 Acc@5 97.436
[2024-07-11 23:00:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-11 23:00:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.31%
[2024-07-11 23:00:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][0/2502]	eta 12:11:31 lr 0.000010	 wd 0.0000	time 17.5427 (17.5427)	loss 0.7275 (0.7275)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:01:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:19:51 lr 0.000010	 wd 0.0000	time 0.3108 (0.4961)	loss 0.8184 (0.7743)	grad_norm 2.1428 (2.2439)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:01:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:15:52 lr 0.000010	 wd 0.0000	time 0.3117 (0.4137)	loss 0.8560 (0.7815)	grad_norm 1.8731 (2.1957)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:02:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:14:05 lr 0.000010	 wd 0.0000	time 0.2854 (0.3840)	loss 0.7212 (0.7799)	grad_norm 2.0994 (2.1871)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:02:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:12:56 lr 0.000010	 wd 0.0000	time 0.3001 (0.3692)	loss 0.7935 (0.7804)	grad_norm 2.0153 (2.1660)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:03:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:12:02 lr 0.000010	 wd 0.0000	time 0.2957 (0.3608)	loss 0.7583 (0.7809)	grad_norm 2.7015 (2.1802)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:03:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:11:14 lr 0.000010	 wd 0.0000	time 0.2926 (0.3549)	loss 0.7212 (0.7812)	grad_norm 2.5723 (2.1839)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:04:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:10:31 lr 0.000010	 wd 0.0000	time 0.3211 (0.3507)	loss 0.7383 (0.7812)	grad_norm 2.0270 (2.1801)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:04:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:52 lr 0.000010	 wd 0.0000	time 0.2884 (0.3480)	loss 0.6997 (0.7820)	grad_norm 2.6556 (2.1769)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:05:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:09:13 lr 0.000009	 wd 0.0000	time 0.2853 (0.3455)	loss 0.8242 (0.7822)	grad_norm 2.0550 (2.1743)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:05:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:08:36 lr 0.000009	 wd 0.0000	time 0.3238 (0.3436)	loss 0.8569 (0.7838)	grad_norm 1.9008 (2.1771)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:06:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:08:23 lr 0.000009	 wd 0.0000	time 0.3035 (0.3594)	loss 0.7236 (0.7819)	grad_norm 2.0791 (2.1715)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:07:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:51 lr 0.000009	 wd 0.0000	time 0.3257 (0.3623)	loss 0.6973 (0.7817)	grad_norm 2.1166 (2.1733)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:08:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:07:54 lr 0.000009	 wd 0.0000	time 0.5036 (0.3951)	loss 0.8594 (0.7827)	grad_norm 2.0812 (2.1728)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:09:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:07:32 lr 0.000009	 wd 0.0000	time 0.2941 (0.4104)	loss 0.8369 (0.7830)	grad_norm 2.2005 (2.1698)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:10:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:06:45 lr 0.000009	 wd 0.0000	time 0.2888 (0.4047)	loss 0.6562 (0.7835)	grad_norm 2.1961 (2.1643)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:10:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:06:00 lr 0.000009	 wd 0.0000	time 0.2977 (0.3998)	loss 0.7715 (0.7835)	grad_norm 2.4365 (2.1601)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:11:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:05:17 lr 0.000008	 wd 0.0000	time 0.2930 (0.3957)	loss 0.8013 (0.7840)	grad_norm 1.9113 (2.1606)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:12:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:04:35 lr 0.000008	 wd 0.0000	time 0.3017 (0.3919)	loss 0.7407 (0.7835)	grad_norm 2.7074 (inf)	loss_scale 4096.0000 (8114.6741)	mem 11634MB
[2024-07-11 23:12:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:53 lr 0.000008	 wd 0.0000	time 0.2982 (0.3884)	loss 0.5859 (0.7839)	grad_norm 2.1078 (inf)	loss_scale 4096.0000 (7903.2762)	mem 11634MB
[2024-07-11 23:13:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:03:13 lr 0.000008	 wd 0.0000	time 0.3043 (0.3858)	loss 0.8755 (0.7833)	grad_norm 2.3496 (inf)	loss_scale 4096.0000 (7713.0075)	mem 11634MB
[2024-07-11 23:13:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:33 lr 0.000008	 wd 0.0000	time 0.2698 (0.3829)	loss 0.7222 (0.7829)	grad_norm 2.6340 (inf)	loss_scale 4096.0000 (7540.8510)	mem 11634MB
[2024-07-11 23:14:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:54 lr 0.000008	 wd 0.0000	time 0.2989 (0.3804)	loss 0.8101 (0.7825)	grad_norm 2.0677 (inf)	loss_scale 4096.0000 (7384.3380)	mem 11634MB
[2024-07-11 23:15:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:17 lr 0.000008	 wd 0.0000	time 0.3267 (0.3852)	loss 0.8628 (0.7828)	grad_norm 2.5677 (inf)	loss_scale 4096.0000 (7241.4289)	mem 11634MB
[2024-07-11 23:15:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:39 lr 0.000008	 wd 0.0000	time 0.2902 (0.3846)	loss 0.8843 (0.7828)	grad_norm 1.8742 (inf)	loss_scale 4096.0000 (7110.4240)	mem 11634MB
[2024-07-11 23:16:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.3033 (0.3826)	loss 0.8701 (0.7830)	grad_norm 2.0515 (inf)	loss_scale 4096.0000 (6989.8952)	mem 11634MB
[2024-07-11 23:16:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 24 training takes 0:16:25
[2024-07-11 23:17:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 74.272 (74.272)	Loss 0.3923 (0.3923)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 23:18:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.308 Acc@5 97.432
[2024-07-11 23:18:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-11 23:18:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.31%
[2024-07-11 23:18:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 23:18:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 23:18:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][0/2502]	eta 12:05:24 lr 0.000008	 wd 0.0000	time 17.3958 (17.3958)	loss 0.7520 (0.7520)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:19:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:19:49 lr 0.000008	 wd 0.0000	time 0.3370 (0.4952)	loss 0.8247 (0.7779)	grad_norm 2.0871 (2.1248)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:19:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:15:48 lr 0.000007	 wd 0.0000	time 0.2985 (0.4119)	loss 0.7368 (0.7788)	grad_norm 2.0147 (2.1894)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:20:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:14:03 lr 0.000007	 wd 0.0000	time 0.3116 (0.3830)	loss 0.8057 (0.7811)	grad_norm 2.1076 (2.2081)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:20:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:12:54 lr 0.000007	 wd 0.0000	time 0.2932 (0.3686)	loss 0.8608 (0.7796)	grad_norm 1.9368 (2.1977)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:21:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:12:01 lr 0.000007	 wd 0.0000	time 0.3257 (0.3604)	loss 0.7612 (0.7804)	grad_norm 1.9107 (2.1861)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:21:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:11:13 lr 0.000007	 wd 0.0000	time 0.2954 (0.3544)	loss 0.7793 (0.7814)	grad_norm 1.7741 (2.1827)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:22:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:10:31 lr 0.000007	 wd 0.0000	time 0.2990 (0.3503)	loss 0.7881 (0.7807)	grad_norm 2.4215 (2.1848)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:22:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:51 lr 0.000007	 wd 0.0000	time 0.2961 (0.3476)	loss 0.8413 (0.7804)	grad_norm 1.8597 (2.1900)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:23:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:09:13 lr 0.000007	 wd 0.0000	time 0.2989 (0.3452)	loss 0.7505 (0.7801)	grad_norm 2.0587 (2.1842)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:23:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:08:35 lr 0.000007	 wd 0.0000	time 0.2855 (0.3433)	loss 0.8691 (0.7799)	grad_norm 2.6410 (2.1871)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:25:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:08:38 lr 0.000007	 wd 0.0000	time 0.3367 (0.3698)	loss 0.7334 (0.7796)	grad_norm 2.0923 (2.1878)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:25:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:08:04 lr 0.000006	 wd 0.0000	time 0.2789 (0.3718)	loss 0.9810 (0.7807)	grad_norm 2.1916 (2.1797)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:26:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:07:28 lr 0.000006	 wd 0.0000	time 0.3080 (0.3735)	loss 0.7754 (0.7807)	grad_norm 1.7194 (2.1767)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:26:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:06:47 lr 0.000006	 wd 0.0000	time 0.3064 (0.3700)	loss 0.7500 (0.7804)	grad_norm 1.7764 (2.1750)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:27:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:06:07 lr 0.000006	 wd 0.0000	time 0.3083 (0.3671)	loss 0.8369 (0.7805)	grad_norm 1.9840 (2.1767)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:28:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:05:29 lr 0.000006	 wd 0.0000	time 0.2921 (0.3650)	loss 0.8442 (0.7803)	grad_norm 2.1646 (2.1722)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:28:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:50 lr 0.000006	 wd 0.0000	time 0.3077 (0.3628)	loss 0.7598 (0.7799)	grad_norm 2.0314 (2.1711)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:29:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:04:13 lr 0.000006	 wd 0.0000	time 0.2849 (0.3610)	loss 0.9839 (0.7807)	grad_norm 3.0130 (2.1675)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:29:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:36 lr 0.000006	 wd 0.0000	time 0.2911 (0.3597)	loss 1.0635 (0.7811)	grad_norm 1.8377 (2.1716)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:30:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:59 lr 0.000006	 wd 0.0000	time 0.3054 (0.3581)	loss 0.8130 (0.7810)	grad_norm 2.2800 (2.1735)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:30:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:23 lr 0.000006	 wd 0.0000	time 0.3078 (0.3567)	loss 0.8174 (0.7812)	grad_norm 1.7803 (2.1729)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:31:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:51 lr 0.000006	 wd 0.0000	time 0.3431 (0.3702)	loss 0.6836 (0.7805)	grad_norm 2.1454 (2.1709)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:32:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:16 lr 0.000005	 wd 0.0000	time 25.2865 (0.3809)	loss 0.7358 (0.7804)	grad_norm 1.8135 (2.1736)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:33:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:39 lr 0.000005	 wd 0.0000	time 0.3422 (0.3904)	loss 0.7622 (0.7802)	grad_norm 2.0670 (2.1716)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:34:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0000	time 0.3030 (0.3896)	loss 0.7612 (0.7799)	grad_norm 1.9570 (2.1735)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:34:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 25 training takes 0:16:26
[2024-07-11 23:35:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 26.350 (26.350)	Loss 0.3926 (0.3926)	Acc@1 91.211 (91.211)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 23:35:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.318 Acc@5 97.424
[2024-07-11 23:35:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-11 23:35:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-11 23:35:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-11 23:35:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-11 23:35:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][0/2502]	eta 20:48:25 lr 0.000005	 wd 0.0000	time 29.9384 (29.9384)	loss 0.7324 (0.7324)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:36:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:25:06 lr 0.000005	 wd 0.0000	time 0.3101 (0.6273)	loss 0.8677 (0.7823)	grad_norm 2.1886 (2.1252)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:37:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:18:18 lr 0.000005	 wd 0.0000	time 0.2910 (0.4770)	loss 0.8047 (0.7813)	grad_norm 2.6375 (2.1608)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:37:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:15:42 lr 0.000005	 wd 0.0000	time 0.2984 (0.4278)	loss 0.7388 (0.7814)	grad_norm 2.1480 (2.1548)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:38:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:14:05 lr 0.000005	 wd 0.0000	time 0.3009 (0.4021)	loss 0.8057 (0.7796)	grad_norm 2.1564 (2.1735)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:38:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:12:54 lr 0.000005	 wd 0.0000	time 0.3038 (0.3867)	loss 0.8936 (0.7806)	grad_norm 2.2439 (2.1714)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:39:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:11:56 lr 0.000005	 wd 0.0000	time 0.3098 (0.3767)	loss 0.7910 (0.7810)	grad_norm 2.2341 (2.1669)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:39:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:11:05 lr 0.000005	 wd 0.0000	time 0.3291 (0.3694)	loss 0.9272 (0.7796)	grad_norm 2.0425 (2.1581)	loss_scale 4096.0000 (4096.0000)	mem 11634MB
[2024-07-11 23:40:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:10:21 lr 0.000005	 wd 0.0000	time 0.2990 (0.3653)	loss 0.7632 (0.7792)	grad_norm 1.7023 (2.1605)	loss_scale 8192.0000 (4290.3171)	mem 11634MB
[2024-07-11 23:40:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:09:38 lr 0.000005	 wd 0.0000	time 0.3090 (0.3612)	loss 0.8242 (0.7787)	grad_norm 2.1141 (2.1578)	loss_scale 8192.0000 (4723.3563)	mem 11634MB
[2024-07-11 23:41:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:56 lr 0.000004	 wd 0.0000	time 0.2973 (0.3575)	loss 0.9067 (0.7789)	grad_norm 2.0952 (2.1594)	loss_scale 8192.0000 (5069.8741)	mem 11634MB
[2024-07-11 23:41:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:08:17 lr 0.000004	 wd 0.0000	time 0.3082 (0.3545)	loss 0.7812 (0.7787)	grad_norm 2.1340 (2.1638)	loss_scale 8192.0000 (5353.4460)	mem 11634MB
[2024-07-11 23:43:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:08:11 lr 0.000004	 wd 0.0000	time 0.3511 (0.3778)	loss 0.7026 (0.7794)	grad_norm 2.0020 (2.1587)	loss_scale 8192.0000 (5589.7952)	mem 11634MB
[2024-07-11 23:44:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:07:58 lr 0.000004	 wd 0.0000	time 0.3478 (0.3979)	loss 0.7383 (0.7792)	grad_norm 2.2597 (2.1589)	loss_scale 8192.0000 (5789.8109)	mem 11634MB
[2024-07-11 23:44:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:07:13 lr 0.000004	 wd 0.0000	time 0.3116 (0.3930)	loss 0.8296 (0.7796)	grad_norm 2.1337 (2.1634)	loss_scale 8192.0000 (5961.2734)	mem 11634MB
[2024-07-11 23:45:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:06:29 lr 0.000004	 wd 0.0000	time 0.2925 (0.3885)	loss 0.7202 (0.7799)	grad_norm 2.1277 (2.1619)	loss_scale 8192.0000 (6109.8894)	mem 11634MB
[2024-07-11 23:45:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:05:47 lr 0.000004	 wd 0.0000	time 0.3091 (0.3847)	loss 0.7109 (0.7790)	grad_norm 2.2142 (2.1653)	loss_scale 8192.0000 (6239.9400)	mem 11634MB
[2024-07-11 23:46:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:05:05 lr 0.000004	 wd 0.0000	time 0.3334 (0.3814)	loss 0.6919 (0.7784)	grad_norm 2.4595 (2.1645)	loss_scale 8192.0000 (6354.6996)	mem 11634MB
[2024-07-11 23:46:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:04:25 lr 0.000004	 wd 0.0000	time 0.3130 (0.3784)	loss 0.7271 (0.7784)	grad_norm 1.9227 (2.1646)	loss_scale 8192.0000 (6456.7152)	mem 11634MB
[2024-07-11 23:47:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:46 lr 0.000004	 wd 0.0000	time 0.2982 (0.3762)	loss 0.7041 (0.7785)	grad_norm 2.4959 (2.1649)	loss_scale 8192.0000 (6547.9979)	mem 11634MB
[2024-07-11 23:47:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:03:07 lr 0.000004	 wd 0.0000	time 0.3057 (0.3738)	loss 0.7705 (0.7785)	grad_norm 2.1109 (2.1637)	loss_scale 8192.0000 (6630.1569)	mem 11634MB
[2024-07-11 23:48:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:29 lr 0.000004	 wd 0.0000	time 0.2867 (0.3715)	loss 0.8174 (0.7786)	grad_norm 2.0911 (2.1640)	loss_scale 8192.0000 (6704.4950)	mem 11634MB
[2024-07-11 23:49:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:53 lr 0.000004	 wd 0.0000	time 0.3266 (0.3747)	loss 0.7798 (0.7785)	grad_norm 1.9907 (2.1651)	loss_scale 8192.0000 (6772.0781)	mem 11634MB
[2024-07-11 23:49:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:15 lr 0.000004	 wd 0.0000	time 0.3276 (0.3743)	loss 0.7466 (0.7789)	grad_norm 2.2943 (2.1647)	loss_scale 8192.0000 (6833.7870)	mem 11634MB
[2024-07-11 23:50:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:38 lr 0.000003	 wd 0.0000	time 10.1759 (0.3806)	loss 0.7588 (0.7791)	grad_norm 2.2038 (2.1598)	loss_scale 8192.0000 (6890.3557)	mem 11634MB
[2024-07-11 23:51:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.2882 (0.3844)	loss 0.7041 (0.7797)	grad_norm 3.0673 (2.1624)	loss_scale 8192.0000 (6942.4006)	mem 11634MB
[2024-07-11 23:51:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 26 training takes 0:16:11
[2024-07-11 23:52:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 52.855 (52.855)	Loss 0.3928 (0.3928)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-11 23:52:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.310 Acc@5 97.426
[2024-07-11 23:52:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-11 23:52:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-11 23:53:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][0/2502]	eta 10:23:27 lr 0.000003	 wd 0.0000	time 14.9509 (14.9509)	loss 0.7002 (0.7002)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:53:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:19:22 lr 0.000003	 wd 0.0000	time 0.3234 (0.4842)	loss 0.7671 (0.7769)	grad_norm 3.4521 (2.1919)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:54:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:15:32 lr 0.000003	 wd 0.0000	time 0.3108 (0.4052)	loss 0.7500 (0.7772)	grad_norm 2.4122 (2.1895)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:54:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:13:53 lr 0.000003	 wd 0.0000	time 0.3083 (0.3786)	loss 0.7900 (0.7739)	grad_norm 2.2178 (2.1795)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:55:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:12:47 lr 0.000003	 wd 0.0000	time 0.2967 (0.3651)	loss 0.7827 (0.7750)	grad_norm 2.0876 (2.1546)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:55:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:11:54 lr 0.000003	 wd 0.0000	time 0.3218 (0.3571)	loss 0.7144 (0.7729)	grad_norm 2.0393 (2.1525)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:56:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:11:09 lr 0.000003	 wd 0.0000	time 0.3388 (0.3519)	loss 0.7119 (0.7760)	grad_norm 2.2313 (2.1523)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:56:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:10:27 lr 0.000003	 wd 0.0000	time 0.2896 (0.3483)	loss 0.7832 (0.7781)	grad_norm 2.6110 (2.1539)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:57:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:48 lr 0.000003	 wd 0.0000	time 0.3110 (0.3458)	loss 0.8267 (0.7767)	grad_norm 1.8936 (2.1575)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:58:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:09:10 lr 0.000003	 wd 0.0000	time 0.3029 (0.3437)	loss 0.7671 (0.7789)	grad_norm 2.0664 (2.1569)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:58:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:08:33 lr 0.000003	 wd 0.0000	time 0.3056 (0.3419)	loss 0.9165 (0.7780)	grad_norm 2.0939 (2.1633)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-11 23:59:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:08:26 lr 0.000003	 wd 0.0000	time 0.3524 (0.3611)	loss 0.7002 (0.7787)	grad_norm 2.8369 (2.1649)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:00:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:49 lr 0.000003	 wd 0.0000	time 0.3313 (0.3606)	loss 0.7285 (0.7782)	grad_norm 2.3044 (2.1677)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:01:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:08:09 lr 0.000003	 wd 0.0000	time 0.3306 (0.4070)	loss 0.8125 (0.7783)	grad_norm 2.4216 (2.1660)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:02:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:07:29 lr 0.000003	 wd 0.0000	time 0.2924 (0.4077)	loss 0.6812 (0.7785)	grad_norm 2.3880 (2.1622)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:02:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:06:43 lr 0.000003	 wd 0.0000	time 0.3125 (0.4024)	loss 0.8037 (0.7791)	grad_norm 2.3274 (2.1589)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:03:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:05:58 lr 0.000003	 wd 0.0000	time 0.3321 (0.3980)	loss 0.7583 (0.7796)	grad_norm 2.0344 (2.1571)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:04:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:05:15 lr 0.000002	 wd 0.0000	time 0.2840 (0.3939)	loss 0.8691 (0.7791)	grad_norm 1.7525 (2.1567)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:04:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:04:33 lr 0.000002	 wd 0.0000	time 0.2975 (0.3902)	loss 0.7388 (0.7790)	grad_norm 2.0693 (2.1532)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:05:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:52 lr 0.000002	 wd 0.0000	time 0.2921 (0.3870)	loss 0.8398 (0.7788)	grad_norm 2.1445 (2.1523)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:05:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:03:13 lr 0.000002	 wd 0.0000	time 0.2872 (0.3846)	loss 0.6602 (0.7785)	grad_norm 1.7991 (2.1513)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:06:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:33 lr 0.000002	 wd 0.0000	time 0.2953 (0.3819)	loss 0.9023 (0.7795)	grad_norm 2.4998 (2.1528)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:06:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:54 lr 0.000002	 wd 0.0000	time 0.2884 (0.3795)	loss 0.7002 (0.7794)	grad_norm 1.8238 (2.1499)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:07:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:18 lr 0.000002	 wd 0.0000	time 0.2894 (0.3874)	loss 0.8188 (0.7796)	grad_norm 2.0611 (2.1537)	loss_scale 16384.0000 (8334.4076)	mem 11634MB
[2024-07-12 00:08:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:39 lr 0.000002	 wd 0.0000	time 0.3296 (0.3869)	loss 0.8032 (0.7796)	grad_norm 1.8861 (2.1559)	loss_scale 16384.0000 (8669.6676)	mem 11634MB
[2024-07-12 00:09:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.3083 (0.4059)	loss 0.7192 (0.7800)	grad_norm 1.8353 (2.1571)	loss_scale 16384.0000 (8978.1176)	mem 11634MB
[2024-07-12 00:09:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 27 training takes 0:17:07
[2024-07-12 00:10:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 38.598 (38.598)	Loss 0.3904 (0.3904)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-12 00:10:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.314 Acc@5 97.418
[2024-07-12 00:10:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-12 00:10:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-12 00:11:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][0/2502]	eta 10:11:00 lr 0.000002	 wd 0.0000	time 14.6525 (14.6525)	loss 0.7837 (0.7837)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-12 00:11:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:19:21 lr 0.000002	 wd 0.0000	time 0.2773 (0.4835)	loss 0.8062 (0.7885)	grad_norm 1.9505 (2.1448)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-12 00:12:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:15:32 lr 0.000002	 wd 0.0000	time 0.2996 (0.4050)	loss 0.8618 (0.7829)	grad_norm 3.4565 (2.1675)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-12 00:12:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:13:52 lr 0.000002	 wd 0.0000	time 0.2819 (0.3783)	loss 0.7925 (0.7826)	grad_norm 1.6387 (2.1578)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-12 00:13:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:12:47 lr 0.000002	 wd 0.0000	time 0.3232 (0.3651)	loss 0.6899 (0.7812)	grad_norm 2.6722 (2.1656)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-12 00:13:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:54 lr 0.000002	 wd 0.0000	time 0.3011 (0.3571)	loss 0.9111 (0.7823)	grad_norm 2.1632 (2.1649)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-12 00:14:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:11:09 lr 0.000002	 wd 0.0000	time 0.2959 (0.3519)	loss 0.7715 (0.7819)	grad_norm 1.9983 (2.1622)	loss_scale 16384.0000 (16384.0000)	mem 11634MB
[2024-07-12 00:14:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:10:27 lr 0.000002	 wd 0.0000	time 0.3010 (0.3482)	loss 0.7383 (0.7802)	grad_norm 2.2122 (inf)	loss_scale 8192.0000 (15449.1070)	mem 11634MB
[2024-07-12 00:15:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:48 lr 0.000002	 wd 0.0000	time 0.2890 (0.3455)	loss 0.7598 (0.7793)	grad_norm 2.0530 (inf)	loss_scale 8192.0000 (14543.1011)	mem 11634MB
[2024-07-12 00:16:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:09:09 lr 0.000002	 wd 0.0000	time 0.3114 (0.3433)	loss 0.8354 (0.7781)	grad_norm 2.1490 (inf)	loss_scale 8192.0000 (13838.2064)	mem 11634MB
[2024-07-12 00:16:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:33 lr 0.000002	 wd 0.0000	time 0.2937 (0.3416)	loss 0.8384 (0.7776)	grad_norm 2.4127 (inf)	loss_scale 8192.0000 (13274.1499)	mem 11634MB
[2024-07-12 00:17:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:08:46 lr 0.000002	 wd 0.0000	time 0.3327 (0.3758)	loss 0.7446 (0.7774)	grad_norm 2.7030 (inf)	loss_scale 8192.0000 (12812.5559)	mem 11634MB
[2024-07-12 00:18:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:08:12 lr 0.000002	 wd 0.0000	time 0.2989 (0.3780)	loss 0.8696 (0.7767)	grad_norm 2.2234 (inf)	loss_scale 8192.0000 (12427.8301)	mem 11634MB
[2024-07-12 00:19:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:07:29 lr 0.000002	 wd 0.0000	time 0.3051 (0.3740)	loss 0.7495 (0.7765)	grad_norm 2.3548 (inf)	loss_scale 8192.0000 (12102.2475)	mem 11634MB
[2024-07-12 00:19:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:06:48 lr 0.000002	 wd 0.0000	time 0.3029 (0.3707)	loss 0.7412 (0.7765)	grad_norm 2.4847 (inf)	loss_scale 8192.0000 (11823.1435)	mem 11634MB
[2024-07-12 00:20:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:06:09 lr 0.000002	 wd 0.0000	time 0.3057 (0.3684)	loss 0.7549 (0.7766)	grad_norm 1.8271 (inf)	loss_scale 8192.0000 (11581.2285)	mem 11634MB
[2024-07-12 00:20:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:05:29 lr 0.000002	 wd 0.0000	time 0.2952 (0.3658)	loss 0.6665 (0.7770)	grad_norm 1.7689 (inf)	loss_scale 8192.0000 (11369.5340)	mem 11634MB
[2024-07-12 00:21:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:51 lr 0.000001	 wd 0.0000	time 0.3280 (0.3638)	loss 0.6807 (0.7772)	grad_norm 1.8432 (inf)	loss_scale 8192.0000 (11182.7302)	mem 11634MB
[2024-07-12 00:21:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:04:14 lr 0.000001	 wd 0.0000	time 0.2843 (0.3620)	loss 0.6763 (0.7772)	grad_norm 2.9766 (inf)	loss_scale 8192.0000 (11016.6707)	mem 11634MB
[2024-07-12 00:22:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:36 lr 0.000001	 wd 0.0000	time 0.2978 (0.3603)	loss 0.8379 (0.7772)	grad_norm 1.7823 (inf)	loss_scale 8192.0000 (10868.0821)	mem 11634MB
[2024-07-12 00:22:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:03:00 lr 0.000001	 wd 0.0000	time 0.2963 (0.3587)	loss 0.7607 (0.7772)	grad_norm 2.1262 (inf)	loss_scale 8192.0000 (10734.3448)	mem 11634MB
[2024-07-12 00:23:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:26 lr 0.000001	 wd 0.0000	time 0.2798 (0.3633)	loss 0.8286 (0.7776)	grad_norm 2.6143 (inf)	loss_scale 8192.0000 (10613.3384)	mem 11634MB
[2024-07-12 00:24:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:49 lr 0.000001	 wd 0.0000	time 0.3614 (0.3638)	loss 0.8779 (0.7776)	grad_norm 2.1992 (inf)	loss_scale 8192.0000 (10503.3276)	mem 11634MB
[2024-07-12 00:25:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:15 lr 0.000001	 wd 0.0000	time 0.3762 (0.3719)	loss 0.7539 (0.7779)	grad_norm 2.5958 (inf)	loss_scale 8192.0000 (10402.8787)	mem 11634MB
[2024-07-12 00:25:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:37 lr 0.000001	 wd 0.0000	time 0.3008 (0.3717)	loss 0.8467 (0.7782)	grad_norm 2.4484 (inf)	loss_scale 8192.0000 (10310.7972)	mem 11634MB
[2024-07-12 00:26:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.3052 (0.3703)	loss 0.7241 (0.7787)	grad_norm 2.3232 (inf)	loss_scale 8192.0000 (10226.0792)	mem 11634MB
[2024-07-12 00:26:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 28 training takes 0:15:36
[2024-07-12 00:27:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 72.364 (72.364)	Loss 0.3911 (0.3911)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-12 00:28:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.322 Acc@5 97.416
[2024-07-12 00:28:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-12 00:28:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-12 00:28:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-12 00:28:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-12 00:28:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][0/2502]	eta 16:19:12 lr 0.000001	 wd 0.0000	time 23.4821 (23.4821)	loss 0.7422 (0.7422)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:29:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:22:08 lr 0.000001	 wd 0.0000	time 0.2985 (0.5532)	loss 0.8623 (0.7782)	grad_norm 2.4845 (2.0884)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:29:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:16:56 lr 0.000001	 wd 0.0000	time 0.3281 (0.4414)	loss 0.7412 (0.7842)	grad_norm 2.2252 (2.0981)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:30:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:14:47 lr 0.000001	 wd 0.0000	time 0.3020 (0.4031)	loss 0.7734 (0.7808)	grad_norm 2.9784 (2.1271)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:30:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:13:26 lr 0.000001	 wd 0.0000	time 0.3073 (0.3836)	loss 0.6987 (0.7820)	grad_norm 2.3588 (2.1423)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:31:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:12:25 lr 0.000001	 wd 0.0000	time 0.3084 (0.3722)	loss 0.7573 (0.7810)	grad_norm 1.9080 (2.1519)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:31:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:11:33 lr 0.000001	 wd 0.0000	time 0.2759 (0.3644)	loss 0.7583 (0.7789)	grad_norm 1.8655 (2.1444)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:32:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:10:46 lr 0.000001	 wd 0.0000	time 0.3004 (0.3590)	loss 0.8887 (0.7779)	grad_norm 1.8728 (2.1296)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:32:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:10:03 lr 0.000001	 wd 0.0000	time 0.3229 (0.3548)	loss 0.7036 (0.7798)	grad_norm 2.1406 (2.1355)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:33:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:09:23 lr 0.000001	 wd 0.0000	time 0.3027 (0.3519)	loss 0.8262 (0.7794)	grad_norm 1.9692 (2.1366)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:33:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:08:44 lr 0.000001	 wd 0.0000	time 0.2942 (0.3492)	loss 0.6841 (0.7787)	grad_norm 1.9354 (2.1337)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:34:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:08:06 lr 0.000001	 wd 0.0000	time 0.3196 (0.3473)	loss 0.7944 (0.7792)	grad_norm 1.8351 (2.1337)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:35:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:08:29 lr 0.000001	 wd 0.0000	time 0.3170 (0.3914)	loss 0.7588 (0.7781)	grad_norm 2.5859 (2.1358)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:36:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:07:52 lr 0.000001	 wd 0.0000	time 0.3102 (0.3935)	loss 0.6924 (0.7791)	grad_norm 2.0052 (2.1326)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:37:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:07:08 lr 0.000001	 wd 0.0000	time 0.2989 (0.3887)	loss 0.8721 (0.7796)	grad_norm 1.8113 (2.1321)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:37:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:06:25 lr 0.000001	 wd 0.0000	time 0.2983 (0.3847)	loss 0.7231 (0.7794)	grad_norm 1.8882 (2.1299)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:38:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:05:43 lr 0.000001	 wd 0.0000	time 0.3139 (0.3812)	loss 0.7705 (0.7792)	grad_norm 2.2032 (2.1364)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:38:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:05:03 lr 0.000001	 wd 0.0000	time 0.2996 (0.3781)	loss 0.9722 (0.7797)	grad_norm 2.1419 (2.1391)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:39:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:04:23 lr 0.000001	 wd 0.0000	time 0.3131 (0.3753)	loss 0.6597 (0.7801)	grad_norm 2.0295 (2.1412)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:39:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:44 lr 0.000001	 wd 0.0000	time 0.3050 (0.3732)	loss 0.7422 (0.7808)	grad_norm 2.0596 (2.1389)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:40:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:03:06 lr 0.000001	 wd 0.0000	time 0.3037 (0.3709)	loss 0.8071 (0.7806)	grad_norm 2.2987 (2.1371)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:41:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:28 lr 0.000001	 wd 0.0000	time 0.3158 (0.3690)	loss 0.8174 (0.7806)	grad_norm 2.7091 (2.1376)	loss_scale 8192.0000 (8192.0000)	mem 11634MB
[2024-07-12 00:41:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:53 lr 0.000001	 wd 0.0000	time 0.3144 (0.3750)	loss 0.9429 (0.7803)	grad_norm 1.8818 (2.1366)	loss_scale 16384.0000 (8497.1995)	mem 11634MB
[2024-07-12 00:42:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:15 lr 0.000001	 wd 0.0000	time 0.3185 (0.3743)	loss 0.7798 (0.7801)	grad_norm 2.1752 (2.1387)	loss_scale 16384.0000 (8839.9548)	mem 11634MB
[2024-07-12 00:44:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:40 lr 0.000001	 wd 0.0000	time 0.3261 (0.3993)	loss 0.7661 (0.7804)	grad_norm 2.2079 (2.1359)	loss_scale 16384.0000 (9154.1591)	mem 11634MB
[2024-07-12 00:44:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.3231 (0.4006)	loss 0.9570 (0.7803)	grad_norm 1.6007 (2.1376)	loss_scale 16384.0000 (9443.2371)	mem 11634MB
[2024-07-12 00:45:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 29 training takes 0:16:51
[2024-07-12 00:45:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_29.pth saving......
[2024-07-12 00:45:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_29.pth saved !!!
[2024-07-12 00:45:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 19.622 (19.622)	Loss 0.3906 (0.3906)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 11634MB
[2024-07-12 00:45:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.326 Acc@5 97.410
[2024-07-12 00:45:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-12 00:45:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.33%
[2024-07-12 00:45:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saving......
[2024-07-12 00:45:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer1-full-ft/ckpt_epoch_best.pth saved !!!
[2024-07-12 00:45:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 189): INFO Training time 8:50:01
