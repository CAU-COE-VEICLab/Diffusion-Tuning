[2024-07-10 16:18:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/config.json
[2024-07-10 16:18:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: convnext_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-10 16:18:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/convnext/diffusion_ft_convnext_base_224_22kto1k_sequence_crosslayer_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-10 16:18:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 108): INFO Creating model:convnext_diffusion_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0
[2024-07-10 16:18:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 110): INFO ConvNeXt_Diffusion_Finetune(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-10 16:18:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 113): INFO number of params: 51816808
[2024-07-10 16:18:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0, ignoring auto resume
[2024-07-10 16:18:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth for fine-tuning......
[2024-07-10 16:18:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-10 16:18:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-10 16:18:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth'
[2024-07-10 16:19:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 64.537 (64.537)	Loss 0.3726 (0.3726)	Acc@1 91.406 (91.406)	Acc@5 98.828 (98.828)	Mem 3290MB
[2024-07-10 16:19:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 65.032 Acc@5 77.620
[2024-07-10 16:19:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 65.0%
[2024-07-10 16:19:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 168): INFO Start training
[2024-07-10 16:19:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][0/2502]	eta 13:37:10 lr 0.000100	 wd 0.0000	time 19.5966 (19.5966)	loss 1.6660 (1.6660)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 10059MB
[2024-07-10 16:20:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:22:52 lr 0.000100	 wd 0.0000	time 0.2588 (0.5713)	loss 1.3770 (1.4641)	grad_norm 4.1281 (nan)	loss_scale 16384.0000 (18168.3960)	mem 10059MB
[2024-07-10 16:21:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:16:09 lr 0.000100	 wd 0.0000	time 0.2230 (0.4210)	loss 1.1182 (1.3943)	grad_norm 4.0573 (nan)	loss_scale 16384.0000 (17280.6368)	mem 10059MB
[2024-07-10 16:21:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:30 lr 0.000100	 wd 0.0000	time 0.2364 (0.3679)	loss 1.2051 (1.3343)	grad_norm 2.6765 (nan)	loss_scale 8192.0000 (14478.8837)	mem 10059MB
[2024-07-10 16:21:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:11:59 lr 0.000100	 wd 0.0000	time 0.2679 (0.3425)	loss 1.3057 (1.2817)	grad_norm 3.7794 (nan)	loss_scale 8192.0000 (12911.0823)	mem 10059MB
[2024-07-10 16:22:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:04 lr 0.000100	 wd 0.0000	time 0.2626 (0.3319)	loss 0.9385 (1.2422)	grad_norm 5.6504 (nan)	loss_scale 8192.0000 (11969.1497)	mem 10059MB
[2024-07-10 16:22:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:10:08 lr 0.000100	 wd 0.0000	time 0.2306 (0.3200)	loss 0.9971 (1.2106)	grad_norm 2.5573 (nan)	loss_scale 8192.0000 (11340.6722)	mem 10059MB
[2024-07-10 16:23:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:09:21 lr 0.000100	 wd 0.0000	time 0.2302 (0.3116)	loss 1.1182 (1.1871)	grad_norm 5.7732 (nan)	loss_scale 8192.0000 (10891.5036)	mem 10059MB
[2024-07-10 16:23:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:08:41 lr 0.000100	 wd 0.0000	time 0.2434 (0.3063)	loss 1.0186 (1.1664)	grad_norm 3.4351 (nan)	loss_scale 8192.0000 (10554.4869)	mem 10059MB
[2024-07-10 16:24:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:08:05 lr 0.000100	 wd 0.0000	time 0.2509 (0.3033)	loss 0.9917 (1.1489)	grad_norm 3.2670 (nan)	loss_scale 8192.0000 (10292.2797)	mem 10059MB
[2024-07-10 16:24:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:07:29 lr 0.000100	 wd 0.0000	time 0.2295 (0.2992)	loss 1.0625 (1.1344)	grad_norm 4.5876 (nan)	loss_scale 8192.0000 (10082.4615)	mem 10059MB
[2024-07-10 16:25:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:06:54 lr 0.000100	 wd 0.0000	time 0.2514 (0.2960)	loss 0.9102 (1.1224)	grad_norm 5.5338 (nan)	loss_scale 8192.0000 (9910.7575)	mem 10059MB
[2024-07-10 16:25:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:06:23 lr 0.000100	 wd 0.0000	time 0.2537 (0.2948)	loss 0.9463 (1.1121)	grad_norm 4.3155 (nan)	loss_scale 8192.0000 (9767.6470)	mem 10059MB
[2024-07-10 16:26:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:05:51 lr 0.000100	 wd 0.0000	time 0.2624 (0.2926)	loss 0.8057 (1.1034)	grad_norm 2.9554 (nan)	loss_scale 8192.0000 (9646.5365)	mem 10059MB
[2024-07-10 16:26:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:05:20 lr 0.000100	 wd 0.0000	time 0.2553 (0.2906)	loss 0.9585 (1.0949)	grad_norm 4.1639 (nan)	loss_scale 8192.0000 (9542.7152)	mem 10059MB
[2024-07-10 16:26:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:04:49 lr 0.000100	 wd 0.0000	time 0.2382 (0.2890)	loss 1.0967 (1.0876)	grad_norm 3.6079 (nan)	loss_scale 8192.0000 (9452.7275)	mem 10059MB
[2024-07-10 16:27:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:04:20 lr 0.000100	 wd 0.0000	time 0.2709 (0.2883)	loss 1.0947 (1.0807)	grad_norm 3.8031 (nan)	loss_scale 8192.0000 (9373.9813)	mem 10059MB
[2024-07-10 16:27:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:03:50 lr 0.000100	 wd 0.0000	time 0.2455 (0.2870)	loss 0.8555 (1.0743)	grad_norm 3.0854 (nan)	loss_scale 8192.0000 (9304.4938)	mem 10059MB
[2024-07-10 16:28:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:20 lr 0.000100	 wd 0.0000	time 0.2479 (0.2857)	loss 0.8950 (1.0686)	grad_norm 3.5203 (nan)	loss_scale 8192.0000 (9242.7229)	mem 10059MB
[2024-07-10 16:28:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:52 lr 0.000100	 wd 0.0000	time 0.4611 (0.2860)	loss 0.9341 (1.0631)	grad_norm 2.6042 (nan)	loss_scale 8192.0000 (9187.4508)	mem 10059MB
[2024-07-10 16:30:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:36 lr 0.000100	 wd 0.0000	time 0.2540 (0.3124)	loss 1.0371 (1.0591)	grad_norm 3.2145 (nan)	loss_scale 8192.0000 (9137.7031)	mem 10059MB
[2024-07-10 16:30:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:06 lr 0.000100	 wd 0.0000	time 0.2612 (0.3138)	loss 0.9302 (1.0552)	grad_norm 3.0062 (nan)	loss_scale 8192.0000 (9092.6911)	mem 10059MB
[2024-07-10 16:31:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:34 lr 0.000100	 wd 0.0000	time 0.2584 (0.3115)	loss 0.9629 (1.0509)	grad_norm 3.3312 (nan)	loss_scale 8192.0000 (9051.7692)	mem 10059MB
[2024-07-10 16:31:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:02 lr 0.000100	 wd 0.0000	time 0.2466 (0.3095)	loss 1.0605 (1.0467)	grad_norm 2.5080 (nan)	loss_scale 8192.0000 (9014.4042)	mem 10059MB
[2024-07-10 16:32:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:31 lr 0.000100	 wd 0.0000	time 0.2466 (0.3082)	loss 0.8921 (1.0431)	grad_norm 3.2357 (nan)	loss_scale 8192.0000 (8980.1516)	mem 10059MB
[2024-07-10 16:32:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000100	 wd 0.0000	time 0.2535 (0.3065)	loss 1.1055 (1.0395)	grad_norm 2.3243 (nan)	loss_scale 8192.0000 (8948.6381)	mem 10059MB
[2024-07-10 16:32:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 0 training takes 0:12:52
[2024-07-10 16:32:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_0.pth saving......
[2024-07-10 16:32:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_0.pth saved !!!
[2024-07-10 16:33:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 39.586 (39.586)	Loss 0.3977 (0.3977)	Acc@1 91.016 (91.016)	Acc@5 98.828 (98.828)	Mem 10059MB
[2024-07-10 16:33:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.792 Acc@5 97.086
[2024-07-10 16:33:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-10 16:33:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.79%
[2024-07-10 16:33:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 16:33:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 16:34:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][0/2502]	eta 12:00:36 lr 0.000100	 wd 0.0000	time 17.2809 (17.2809)	loss 0.9316 (0.9316)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:34:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:17:21 lr 0.000100	 wd 0.0000	time 0.2480 (0.4338)	loss 0.8638 (0.9464)	grad_norm 3.2840 (3.4733)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:34:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:13:23 lr 0.000100	 wd 0.0000	time 0.2780 (0.3491)	loss 1.0820 (0.9541)	grad_norm 3.0810 (3.2888)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:35:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:11:47 lr 0.000100	 wd 0.0000	time 0.2576 (0.3213)	loss 1.0391 (0.9504)	grad_norm 3.1182 (3.2438)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:35:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:10:43 lr 0.000100	 wd 0.0000	time 0.2387 (0.3061)	loss 0.7974 (0.9514)	grad_norm 2.7291 (3.2055)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:36:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:09:55 lr 0.000100	 wd 0.0000	time 0.2301 (0.2973)	loss 0.9624 (0.9483)	grad_norm 2.9602 (3.1416)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:36:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:09:15 lr 0.000100	 wd 0.0000	time 0.2647 (0.2920)	loss 1.0312 (0.9494)	grad_norm 2.7075 (3.1604)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:37:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:08:38 lr 0.000100	 wd 0.0000	time 0.2706 (0.2878)	loss 0.8428 (0.9484)	grad_norm 3.2959 (3.1849)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:37:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:08:04 lr 0.000100	 wd 0.0000	time 0.2465 (0.2847)	loss 0.8521 (0.9490)	grad_norm 3.3969 (3.1553)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:38:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:07:31 lr 0.000099	 wd 0.0000	time 0.2546 (0.2821)	loss 1.0391 (0.9479)	grad_norm 6.0044 (3.1503)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:38:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:07:04 lr 0.000099	 wd 0.0000	time 0.2461 (0.2829)	loss 0.9404 (0.9471)	grad_norm 3.1060 (3.1788)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:38:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:06:34 lr 0.000099	 wd 0.0000	time 0.2328 (0.2811)	loss 1.0625 (0.9467)	grad_norm 4.2198 (3.1762)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:39:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:06:04 lr 0.000099	 wd 0.0000	time 0.2355 (0.2796)	loss 0.9927 (0.9461)	grad_norm 2.9153 (3.1709)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:39:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:05:34 lr 0.000099	 wd 0.0000	time 0.2518 (0.2785)	loss 1.0098 (0.9465)	grad_norm 2.1292 (3.1501)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:40:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:05:06 lr 0.000099	 wd 0.0000	time 0.2536 (0.2782)	loss 0.9551 (0.9463)	grad_norm 2.8345 (3.1314)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:40:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:04:37 lr 0.000099	 wd 0.0000	time 0.2537 (0.2774)	loss 0.9971 (0.9463)	grad_norm 2.1603 (3.1184)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:41:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:09 lr 0.000099	 wd 0.0000	time 0.2395 (0.2766)	loss 1.0127 (0.9464)	grad_norm 2.4540 (3.1068)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:41:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:03:41 lr 0.000099	 wd 0.0000	time 0.2659 (0.2760)	loss 0.7773 (0.9461)	grad_norm 2.9869 (3.1064)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 16:42:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:14 lr 0.000099	 wd 0.0000	time 0.2867 (0.2766)	loss 1.0879 (0.9464)	grad_norm 2.4919 (3.0969)	loss_scale 16384.0000 (8619.5669)	mem 10059MB
[2024-07-10 16:42:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:46 lr 0.000099	 wd 0.0000	time 0.2589 (0.2760)	loss 0.9585 (0.9466)	grad_norm 2.8655 (3.0953)	loss_scale 16384.0000 (9028.0063)	mem 10059MB
[2024-07-10 16:42:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:18 lr 0.000099	 wd 0.0000	time 0.2309 (0.2754)	loss 0.8350 (0.9460)	grad_norm 2.6363 (3.0984)	loss_scale 16384.0000 (9395.6222)	mem 10059MB
[2024-07-10 16:43:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:50 lr 0.000099	 wd 0.0000	time 0.2495 (0.2759)	loss 0.8687 (0.9458)	grad_norm 2.4767 (3.1057)	loss_scale 16384.0000 (9728.2437)	mem 10059MB
[2024-07-10 16:43:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:23 lr 0.000099	 wd 0.0000	time 0.2566 (0.2759)	loss 0.7349 (0.9452)	grad_norm 3.2373 (3.0979)	loss_scale 16384.0000 (10030.6406)	mem 10059MB
[2024-07-10 16:44:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:55 lr 0.000099	 wd 0.0000	time 0.2540 (0.2755)	loss 1.1650 (0.9454)	grad_norm 2.6999 (3.0929)	loss_scale 16384.0000 (10306.7536)	mem 10059MB
[2024-07-10 16:44:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:28 lr 0.000099	 wd 0.0000	time 0.2606 (0.2751)	loss 0.9590 (0.9448)	grad_norm 4.2589 (3.0881)	loss_scale 16384.0000 (10559.8667)	mem 10059MB
[2024-07-10 16:45:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000099	 wd 0.0000	time 0.2335 (0.2801)	loss 1.1289 (0.9447)	grad_norm 2.9023 (3.0887)	loss_scale 16384.0000 (10792.7389)	mem 10059MB
[2024-07-10 16:45:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 1 training takes 0:11:49
[2024-07-10 16:46:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 65.745 (65.745)	Loss 0.3855 (0.3855)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 16:46:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.296 Acc@5 97.266
[2024-07-10 16:46:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.3%
[2024-07-10 16:46:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.30%
[2024-07-10 16:46:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 16:46:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 16:47:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][0/2502]	eta 11:16:25 lr 0.000099	 wd 0.0000	time 16.2211 (16.2211)	loss 0.8691 (0.8691)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:47:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:16:49 lr 0.000099	 wd 0.0000	time 0.2615 (0.4202)	loss 0.9268 (0.9319)	grad_norm 2.5763 (2.9405)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:48:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:50 lr 0.000099	 wd 0.0000	time 0.2697 (0.3867)	loss 0.9126 (0.9228)	grad_norm 2.2992 (2.9677)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:48:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:12:40 lr 0.000099	 wd 0.0000	time 0.2381 (0.3452)	loss 1.0068 (0.9214)	grad_norm 2.6028 (2.9273)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:49:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:11:21 lr 0.000099	 wd 0.0000	time 0.2550 (0.3243)	loss 0.8174 (0.9213)	grad_norm 3.4981 (2.9240)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:49:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:10:25 lr 0.000099	 wd 0.0000	time 0.2509 (0.3123)	loss 1.1172 (0.9192)	grad_norm 3.3222 (2.9604)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:49:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:09:39 lr 0.000099	 wd 0.0000	time 0.2799 (0.3046)	loss 0.9712 (0.9206)	grad_norm 3.4278 (2.9613)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:50:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:08:58 lr 0.000099	 wd 0.0000	time 0.2483 (0.2989)	loss 1.0215 (0.9209)	grad_norm 2.7626 (2.9910)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:50:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:08:20 lr 0.000099	 wd 0.0000	time 0.2328 (0.2943)	loss 0.8398 (0.9194)	grad_norm 2.8503 (2.9536)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:51:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:07:46 lr 0.000098	 wd 0.0000	time 0.2681 (0.2913)	loss 0.9014 (0.9204)	grad_norm 2.1736 (2.9613)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:51:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:07:15 lr 0.000098	 wd 0.0000	time 0.2581 (0.2898)	loss 0.8140 (0.9206)	grad_norm 2.6764 (2.9698)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:52:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:06:42 lr 0.000098	 wd 0.0000	time 0.2456 (0.2874)	loss 0.9526 (0.9201)	grad_norm 2.5163 (2.9642)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:52:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:06:11 lr 0.000098	 wd 0.0000	time 0.2511 (0.2854)	loss 1.0986 (0.9193)	grad_norm 2.6594 (2.9607)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:53:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:05:42 lr 0.000098	 wd 0.0000	time 0.2498 (0.2852)	loss 1.0137 (0.9203)	grad_norm 2.2383 (2.9573)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:53:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:05:12 lr 0.000098	 wd 0.0000	time 0.2632 (0.2838)	loss 0.9443 (0.9207)	grad_norm 2.9804 (2.9497)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:54:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:04:43 lr 0.000098	 wd 0.0000	time 0.2549 (0.2825)	loss 0.8662 (0.9204)	grad_norm 2.4448 (2.9356)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:54:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:04:13 lr 0.000098	 wd 0.0000	time 0.2702 (0.2815)	loss 0.9131 (0.9203)	grad_norm 2.7913 (2.9258)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:54:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:03:45 lr 0.000098	 wd 0.0000	time 0.2452 (0.2814)	loss 0.9800 (0.9207)	grad_norm 3.8482 (2.9092)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:55:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:16 lr 0.000098	 wd 0.0000	time 0.2611 (0.2806)	loss 0.8125 (0.9203)	grad_norm 2.7347 (2.9122)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:55:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:02:48 lr 0.000098	 wd 0.0000	time 0.2363 (0.2798)	loss 0.8447 (0.9207)	grad_norm 2.8493 (2.9189)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:56:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:20 lr 0.000098	 wd 0.0000	time 0.2623 (0.2792)	loss 0.8892 (0.9202)	grad_norm 3.0698 (2.9214)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:57:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:00 lr 0.000098	 wd 0.0000	time 0.3515 (0.3002)	loss 1.0020 (0.9207)	grad_norm 3.4312 (2.9205)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:58:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:33 lr 0.000098	 wd 0.0000	time 0.2383 (0.3095)	loss 0.8638 (0.9203)	grad_norm 2.7459 (2.9158)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:58:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:02 lr 0.000098	 wd 0.0000	time 0.2236 (0.3075)	loss 0.9937 (0.9199)	grad_norm 2.8734 (2.9090)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:59:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:31 lr 0.000098	 wd 0.0000	time 0.2440 (0.3057)	loss 1.1572 (0.9199)	grad_norm 2.5202 (2.9141)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:59:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000098	 wd 0.0000	time 0.2478 (0.3040)	loss 0.8818 (0.9201)	grad_norm 3.4820 (2.9081)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 16:59:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 2 training takes 0:12:49
[2024-07-10 17:00:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 28.522 (28.522)	Loss 0.3606 (0.3606)	Acc@1 91.992 (91.992)	Acc@5 98.828 (98.828)	Mem 10059MB
[2024-07-10 17:00:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.498 Acc@5 97.426
[2024-07-10 17:00:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-10 17:00:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.50%
[2024-07-10 17:00:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 17:00:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 17:00:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][0/2502]	eta 10:01:49 lr 0.000098	 wd 0.0000	time 14.4324 (14.4324)	loss 0.7837 (0.7837)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:01:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:17:00 lr 0.000098	 wd 0.0000	time 0.2690 (0.4250)	loss 0.8730 (0.9051)	grad_norm 4.6732 (2.9068)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:01:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:13:11 lr 0.000097	 wd 0.0000	time 0.2680 (0.3437)	loss 0.8857 (0.9069)	grad_norm 2.5650 (2.9034)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:02:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:11:36 lr 0.000097	 wd 0.0000	time 0.2350 (0.3162)	loss 1.0322 (0.9096)	grad_norm 2.3676 (2.7995)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:02:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:10:36 lr 0.000097	 wd 0.0000	time 0.2416 (0.3030)	loss 0.9365 (0.9092)	grad_norm 1.8128 (2.7708)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:02:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:09:50 lr 0.000097	 wd 0.0000	time 0.2471 (0.2950)	loss 0.8804 (0.9084)	grad_norm 2.5599 (2.7415)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:03:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:09:10 lr 0.000097	 wd 0.0000	time 0.2683 (0.2895)	loss 0.8516 (0.9087)	grad_norm 2.9987 (2.7351)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:03:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:08:34 lr 0.000097	 wd 0.0000	time 0.2536 (0.2857)	loss 1.1240 (0.9088)	grad_norm 3.2869 (2.7616)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:04:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:08:02 lr 0.000097	 wd 0.0000	time 0.2472 (0.2835)	loss 0.9883 (0.9095)	grad_norm 2.8589 (2.8098)	loss_scale 32768.0000 (18388.5343)	mem 10059MB
[2024-07-10 17:04:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:07:31 lr 0.000097	 wd 0.0000	time 0.2236 (0.2821)	loss 0.9897 (0.9099)	grad_norm 2.3742 (2.8055)	loss_scale 32768.0000 (19984.4795)	mem 10059MB
[2024-07-10 17:05:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:07:00 lr 0.000097	 wd 0.0000	time 0.2794 (0.2802)	loss 0.9854 (0.9116)	grad_norm 3.3690 (2.8186)	loss_scale 32768.0000 (21261.5544)	mem 10059MB
[2024-07-10 17:05:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:06:30 lr 0.000097	 wd 0.0000	time 0.2590 (0.2785)	loss 0.8501 (0.9123)	grad_norm 3.5805 (2.8156)	loss_scale 32768.0000 (22306.6449)	mem 10059MB
[2024-07-10 17:06:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:06:01 lr 0.000097	 wd 0.0000	time 0.2476 (0.2777)	loss 1.0039 (0.9125)	grad_norm 2.9041 (2.8082)	loss_scale 32768.0000 (23177.6986)	mem 10059MB
[2024-07-10 17:06:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:05:33 lr 0.000097	 wd 0.0000	time 0.2612 (0.2772)	loss 1.0176 (0.9127)	grad_norm 4.0621 (2.8024)	loss_scale 32768.0000 (23914.8470)	mem 10059MB
[2024-07-10 17:06:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:05:04 lr 0.000097	 wd 0.0000	time 0.2431 (0.2763)	loss 0.8750 (0.9134)	grad_norm 2.7879 (2.8100)	loss_scale 32768.0000 (24546.7637)	mem 10059MB
[2024-07-10 17:07:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:04:36 lr 0.000097	 wd 0.0000	time 0.2524 (0.2755)	loss 0.7974 (0.9140)	grad_norm 2.5384 (2.8126)	loss_scale 32768.0000 (25094.4810)	mem 10059MB
[2024-07-10 17:07:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:04:08 lr 0.000096	 wd 0.0000	time 0.2393 (0.2755)	loss 0.8589 (0.9133)	grad_norm 1.8330 (2.8144)	loss_scale 32768.0000 (25573.7764)	mem 10059MB
[2024-07-10 17:08:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:03:40 lr 0.000096	 wd 0.0000	time 0.2374 (0.2751)	loss 0.8628 (0.9131)	grad_norm 2.4775 (2.8132)	loss_scale 32768.0000 (25996.7172)	mem 10059MB
[2024-07-10 17:08:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:12 lr 0.000096	 wd 0.0000	time 0.2203 (0.2745)	loss 1.0391 (0.9127)	grad_norm 2.1892 (2.8122)	loss_scale 32768.0000 (26372.6907)	mem 10059MB
[2024-07-10 17:09:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:02:45 lr 0.000096	 wd 0.0000	time 0.2253 (0.2741)	loss 0.9092 (0.9124)	grad_norm 2.9885 (2.8107)	loss_scale 32768.0000 (26709.1089)	mem 10059MB
[2024-07-10 17:10:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:31 lr 0.000096	 wd 0.0000	time 0.2687 (0.3008)	loss 0.9272 (0.9125)	grad_norm 2.6614 (2.8015)	loss_scale 32768.0000 (27011.9020)	mem 10059MB
[2024-07-10 17:11:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:01 lr 0.000096	 wd 0.0000	time 0.2313 (0.3031)	loss 0.8291 (0.9123)	grad_norm 2.1475 (2.7909)	loss_scale 32768.0000 (27285.8715)	mem 10059MB
[2024-07-10 17:11:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:30 lr 0.000096	 wd 0.0000	time 0.2245 (0.3013)	loss 0.9233 (0.9119)	grad_norm 1.9714 (2.7819)	loss_scale 32768.0000 (27534.9459)	mem 10059MB
[2024-07-10 17:11:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:00 lr 0.000096	 wd 0.0000	time 0.2586 (0.2997)	loss 0.9502 (0.9118)	grad_norm 4.1314 (2.7859)	loss_scale 32768.0000 (27762.3711)	mem 10059MB
[2024-07-10 17:12:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:30 lr 0.000096	 wd 0.0000	time 0.2332 (0.2984)	loss 0.9336 (0.9115)	grad_norm inf (inf)	loss_scale 16384.0000 (27957.2045)	mem 10059MB
[2024-07-10 17:12:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000096	 wd 0.0000	time 0.2396 (0.2973)	loss 0.9785 (0.9115)	grad_norm 2.0890 (inf)	loss_scale 16384.0000 (27494.4614)	mem 10059MB
[2024-07-10 17:12:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 3 training takes 0:12:30
[2024-07-10 17:13:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 22.946 (22.946)	Loss 0.3789 (0.3789)	Acc@1 91.406 (91.406)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 17:13:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.790 Acc@5 97.496
[2024-07-10 17:13:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-10 17:13:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.79%
[2024-07-10 17:13:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 17:13:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 17:13:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][0/2502]	eta 12:24:17 lr 0.000096	 wd 0.0000	time 17.8489 (17.8489)	loss 0.9482 (0.9482)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:14:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:17:56 lr 0.000096	 wd 0.0000	time 0.2469 (0.4482)	loss 0.8867 (0.8950)	grad_norm 3.7251 (2.8518)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:14:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:13:34 lr 0.000096	 wd 0.0000	time 0.2382 (0.3538)	loss 1.0264 (0.9034)	grad_norm 2.4103 (2.7778)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:15:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:11:51 lr 0.000095	 wd 0.0000	time 0.2317 (0.3230)	loss 0.8457 (0.9026)	grad_norm 3.9255 (2.7663)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:15:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:10:47 lr 0.000095	 wd 0.0000	time 0.2666 (0.3082)	loss 0.9307 (0.8986)	grad_norm 3.1160 (2.7849)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:16:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:09:58 lr 0.000095	 wd 0.0000	time 0.2389 (0.2991)	loss 0.9741 (0.8990)	grad_norm 2.5369 (2.7993)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:16:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:09:17 lr 0.000095	 wd 0.0000	time 0.2445 (0.2930)	loss 0.8755 (0.9012)	grad_norm 3.6605 (2.8202)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:16:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:08:40 lr 0.000095	 wd 0.0000	time 0.2413 (0.2886)	loss 1.0820 (0.8992)	grad_norm 3.4248 (2.8344)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:17:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:08:06 lr 0.000095	 wd 0.0000	time 0.2562 (0.2860)	loss 0.7998 (0.8982)	grad_norm 1.8025 (2.8123)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:17:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:07:34 lr 0.000095	 wd 0.0000	time 0.2421 (0.2840)	loss 0.9482 (0.8992)	grad_norm 2.2537 (2.7957)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:18:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:07:03 lr 0.000095	 wd 0.0000	time 0.2368 (0.2818)	loss 0.8750 (0.8989)	grad_norm 2.1325 (2.7890)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:18:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:06:32 lr 0.000095	 wd 0.0000	time 0.2478 (0.2801)	loss 1.0020 (0.8996)	grad_norm 2.6874 (2.7787)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:19:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:06:03 lr 0.000095	 wd 0.0000	time 0.2828 (0.2795)	loss 0.8193 (0.8999)	grad_norm 2.6245 (2.7753)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:19:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:05:34 lr 0.000095	 wd 0.0000	time 0.2619 (0.2786)	loss 0.7568 (0.8993)	grad_norm 3.0265 (2.7794)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:20:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:05:05 lr 0.000094	 wd 0.0000	time 0.2406 (0.2775)	loss 0.8877 (0.8986)	grad_norm 2.4174 (2.7914)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:20:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:04:37 lr 0.000094	 wd 0.0000	time 0.2519 (0.2768)	loss 0.8242 (0.8990)	grad_norm 2.1674 (2.7876)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:20:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:04:10 lr 0.000094	 wd 0.0000	time 0.2673 (0.2773)	loss 0.9424 (0.8993)	grad_norm 3.1899 (2.7826)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:21:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:03:41 lr 0.000094	 wd 0.0000	time 0.2392 (0.2767)	loss 0.9302 (0.8997)	grad_norm 1.8842 (2.7705)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:21:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:13 lr 0.000094	 wd 0.0000	time 0.2643 (0.2760)	loss 0.9287 (0.9006)	grad_norm 5.5189 (2.7698)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:22:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:46 lr 0.000094	 wd 0.0000	time 0.4550 (0.2767)	loss 0.8667 (0.9003)	grad_norm 2.4894 (2.7691)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:23:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:25 lr 0.000094	 wd 0.0000	time 0.2526 (0.2904)	loss 0.7559 (0.8997)	grad_norm 2.2623 (2.7582)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:23:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:56 lr 0.000094	 wd 0.0000	time 0.2262 (0.2899)	loss 0.8057 (0.8998)	grad_norm 2.6063 (2.7615)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:25:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:35 lr 0.000094	 wd 0.0000	time 0.2736 (0.3160)	loss 0.9102 (0.8995)	grad_norm 3.2274 (2.7601)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:25:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:04 lr 0.000094	 wd 0.0000	time 0.2290 (0.3170)	loss 0.8540 (0.8999)	grad_norm 3.1093 (2.7638)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:26:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:32 lr 0.000093	 wd 0.0000	time 0.2384 (0.3148)	loss 0.8350 (0.8998)	grad_norm 3.2202 (2.7720)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:26:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000093	 wd 0.0000	time 0.2407 (0.3125)	loss 0.7808 (0.8998)	grad_norm 2.7130 (2.7651)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:26:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 4 training takes 0:13:06
[2024-07-10 17:27:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 38.709 (38.709)	Loss 0.3623 (0.3623)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 17:27:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.972 Acc@5 97.548
[2024-07-10 17:27:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-10 17:27:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.97%
[2024-07-10 17:27:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 17:27:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 17:27:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][0/2502]	eta 10:20:12 lr 0.000093	 wd 0.0000	time 14.8730 (14.8730)	loss 1.0439 (1.0439)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:28:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:16:14 lr 0.000093	 wd 0.0000	time 0.2609 (0.4059)	loss 0.8198 (0.8908)	grad_norm 2.7756 (2.6561)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:28:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:12:58 lr 0.000093	 wd 0.0000	time 0.2471 (0.3381)	loss 0.8306 (0.8958)	grad_norm 3.1613 (2.7242)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:29:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:11:30 lr 0.000093	 wd 0.0000	time 0.2467 (0.3135)	loss 0.8418 (0.8931)	grad_norm 2.2584 (2.7677)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:29:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:10:31 lr 0.000093	 wd 0.0000	time 0.2507 (0.3004)	loss 0.7900 (0.8917)	grad_norm 2.3262 (2.7524)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:30:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:09:45 lr 0.000093	 wd 0.0000	time 0.2352 (0.2922)	loss 0.8984 (0.8915)	grad_norm 2.4935 (2.7559)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:30:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:09:08 lr 0.000093	 wd 0.0000	time 0.2494 (0.2883)	loss 1.1455 (0.8940)	grad_norm 2.0877 (2.7452)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:30:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:08:33 lr 0.000093	 wd 0.0000	time 0.2483 (0.2851)	loss 0.9194 (0.8936)	grad_norm 2.3989 (2.7442)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:31:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:08:00 lr 0.000093	 wd 0.0000	time 0.2360 (0.2823)	loss 0.8965 (0.8943)	grad_norm 2.4020 (2.7366)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:31:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:07:28 lr 0.000092	 wd 0.0000	time 0.2453 (0.2802)	loss 0.8799 (0.8942)	grad_norm 2.6083 (2.7332)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:32:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:06:59 lr 0.000092	 wd 0.0000	time 0.2608 (0.2794)	loss 0.8188 (0.8943)	grad_norm 3.6153 (2.7373)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:32:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:06:30 lr 0.000092	 wd 0.0000	time 0.2705 (0.2782)	loss 0.7891 (0.8951)	grad_norm 2.4744 (2.7552)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:33:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:06:00 lr 0.000092	 wd 0.0000	time 0.2392 (0.2769)	loss 1.0283 (0.8951)	grad_norm 2.7464 (2.7826)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:33:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:05:31 lr 0.000092	 wd 0.0000	time 0.2685 (0.2761)	loss 0.8613 (0.8948)	grad_norm 2.1246 (2.7631)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:34:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:05:04 lr 0.000092	 wd 0.0000	time 0.2748 (0.2759)	loss 0.9077 (0.8939)	grad_norm 3.5763 (2.7528)	loss_scale 32768.0000 (16454.1670)	mem 10059MB
[2024-07-10 17:34:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:04:35 lr 0.000092	 wd 0.0000	time 0.2566 (0.2751)	loss 0.9341 (0.8936)	grad_norm 2.7113 (2.7376)	loss_scale 32768.0000 (17541.0313)	mem 10059MB
[2024-07-10 17:34:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:04:07 lr 0.000092	 wd 0.0000	time 0.2315 (0.2744)	loss 0.8252 (0.8932)	grad_norm 2.3206 (2.7326)	loss_scale 32768.0000 (18492.1224)	mem 10059MB
[2024-07-10 17:35:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:03:39 lr 0.000092	 wd 0.0000	time 0.3857 (0.2742)	loss 0.9409 (0.8931)	grad_norm 2.0752 (2.7322)	loss_scale 32768.0000 (19331.3862)	mem 10059MB
[2024-07-10 17:35:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:12 lr 0.000091	 wd 0.0000	time 0.2495 (0.2742)	loss 0.8721 (0.8928)	grad_norm 2.1257 (inf)	loss_scale 16384.0000 (19568.0089)	mem 10059MB
[2024-07-10 17:36:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:44 lr 0.000091	 wd 0.0000	time 0.2517 (0.2737)	loss 1.0508 (0.8933)	grad_norm 3.2792 (inf)	loss_scale 16384.0000 (19400.5176)	mem 10059MB
[2024-07-10 17:36:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:17 lr 0.000091	 wd 0.0000	time 0.2219 (0.2733)	loss 1.0488 (0.8931)	grad_norm 2.2828 (inf)	loss_scale 16384.0000 (19249.7671)	mem 10059MB
[2024-07-10 17:37:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:54 lr 0.000091	 wd 0.0000	time 0.2697 (0.2858)	loss 0.9102 (0.8922)	grad_norm 2.6784 (inf)	loss_scale 16384.0000 (19113.3670)	mem 10059MB
[2024-07-10 17:38:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:26 lr 0.000091	 wd 0.0000	time 0.2705 (0.2862)	loss 0.9077 (0.8922)	grad_norm 2.8237 (inf)	loss_scale 16384.0000 (18989.3612)	mem 10059MB
[2024-07-10 17:39:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:00 lr 0.000091	 wd 0.0000	time 0.3666 (0.3000)	loss 0.7451 (0.8916)	grad_norm 2.4440 (inf)	loss_scale 16384.0000 (18876.1339)	mem 10059MB
[2024-07-10 17:39:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:31 lr 0.000091	 wd 0.0000	time 0.2415 (0.3097)	loss 0.9648 (0.8915)	grad_norm 3.0000 (inf)	loss_scale 16384.0000 (18772.3382)	mem 10059MB
[2024-07-10 17:40:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000091	 wd 0.0000	time 0.2469 (0.3102)	loss 0.8887 (0.8918)	grad_norm 2.8655 (inf)	loss_scale 16384.0000 (18676.8429)	mem 10059MB
[2024-07-10 17:40:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 5 training takes 0:13:01
[2024-07-10 17:41:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 26.799 (26.799)	Loss 0.3613 (0.3613)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 10059MB
[2024-07-10 17:41:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.998 Acc@5 97.576
[2024-07-10 17:41:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-10 17:41:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.00%
[2024-07-10 17:41:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 17:41:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 17:41:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][0/2502]	eta 12:33:52 lr 0.000091	 wd 0.0000	time 18.0786 (18.0786)	loss 0.9648 (0.9648)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:42:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:17:54 lr 0.000090	 wd 0.0000	time 0.2382 (0.4475)	loss 0.8276 (0.8862)	grad_norm 3.0232 (2.7019)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:42:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:13:37 lr 0.000090	 wd 0.0000	time 0.2323 (0.3552)	loss 0.7876 (0.8845)	grad_norm 2.8882 (2.7050)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:42:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:11:53 lr 0.000090	 wd 0.0000	time 0.2494 (0.3239)	loss 0.9160 (0.8854)	grad_norm 2.5308 (2.6112)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:43:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:10:51 lr 0.000090	 wd 0.0000	time 0.2657 (0.3098)	loss 0.8818 (0.8851)	grad_norm 2.9916 (2.6323)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:43:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:10:02 lr 0.000090	 wd 0.0000	time 0.2484 (0.3008)	loss 0.9277 (0.8842)	grad_norm 1.9109 (2.6556)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:44:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:09:19 lr 0.000090	 wd 0.0000	time 0.2560 (0.2942)	loss 0.9482 (0.8846)	grad_norm 2.2852 (2.6687)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:44:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:08:41 lr 0.000090	 wd 0.0000	time 0.2462 (0.2895)	loss 0.8892 (0.8865)	grad_norm 2.1447 (2.6547)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:45:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:08:07 lr 0.000090	 wd 0.0000	time 0.2386 (0.2864)	loss 0.7998 (0.8878)	grad_norm 2.1018 (2.6355)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 17:45:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:07:34 lr 0.000089	 wd 0.0000	time 0.2631 (0.2838)	loss 0.8618 (0.8878)	grad_norm 2.1614 (inf)	loss_scale 8192.0000 (16238.5261)	mem 10059MB
[2024-07-10 17:45:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:07:03 lr 0.000089	 wd 0.0000	time 0.2345 (0.2817)	loss 0.8486 (0.8884)	grad_norm 2.2135 (inf)	loss_scale 8192.0000 (15434.6773)	mem 10059MB
[2024-07-10 17:46:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:06:32 lr 0.000089	 wd 0.0000	time 0.2611 (0.2800)	loss 0.7773 (0.8873)	grad_norm 2.3392 (inf)	loss_scale 8192.0000 (14776.8501)	mem 10059MB
[2024-07-10 17:46:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:06:04 lr 0.000089	 wd 0.0000	time 0.2740 (0.2797)	loss 0.7002 (0.8875)	grad_norm 2.3160 (inf)	loss_scale 8192.0000 (14228.5695)	mem 10059MB
[2024-07-10 17:47:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:05:34 lr 0.000089	 wd 0.0000	time 0.2674 (0.2785)	loss 0.8706 (0.8884)	grad_norm 3.5256 (inf)	loss_scale 8192.0000 (13764.5749)	mem 10059MB
[2024-07-10 17:47:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:05:05 lr 0.000089	 wd 0.0000	time 0.2509 (0.2774)	loss 1.1377 (0.8868)	grad_norm 3.0649 (inf)	loss_scale 8192.0000 (13366.8180)	mem 10059MB
[2024-07-10 17:48:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:04:37 lr 0.000089	 wd 0.0000	time 0.2398 (0.2766)	loss 0.7114 (0.8867)	grad_norm 3.2210 (inf)	loss_scale 8192.0000 (13022.0600)	mem 10059MB
[2024-07-10 17:48:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:04:10 lr 0.000089	 wd 0.0000	time 0.2548 (0.2772)	loss 0.9077 (0.8865)	grad_norm 2.0628 (inf)	loss_scale 8192.0000 (12720.3698)	mem 10059MB
[2024-07-10 17:49:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:03:41 lr 0.000088	 wd 0.0000	time 0.2616 (0.2765)	loss 1.0156 (0.8853)	grad_norm 2.9935 (inf)	loss_scale 8192.0000 (12454.1517)	mem 10059MB
[2024-07-10 17:49:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:13 lr 0.000088	 wd 0.0000	time 0.2452 (0.2758)	loss 0.7412 (0.8846)	grad_norm 2.5910 (inf)	loss_scale 8192.0000 (12217.4969)	mem 10059MB
[2024-07-10 17:49:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:45 lr 0.000088	 wd 0.0000	time 0.2556 (0.2757)	loss 0.9966 (0.8844)	grad_norm 3.0348 (inf)	loss_scale 8192.0000 (12005.7401)	mem 10059MB
[2024-07-10 17:50:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:18 lr 0.000088	 wd 0.0000	time 0.2426 (0.2757)	loss 1.1055 (0.8854)	grad_norm 2.9165 (inf)	loss_scale 8192.0000 (11815.1484)	mem 10059MB
[2024-07-10 17:50:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:50 lr 0.000088	 wd 0.0000	time 0.2421 (0.2752)	loss 0.8921 (0.8855)	grad_norm 2.6734 (inf)	loss_scale 8192.0000 (11642.6997)	mem 10059MB
[2024-07-10 17:51:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:22 lr 0.000088	 wd 0.0000	time 0.2408 (0.2747)	loss 0.9355 (0.8857)	grad_norm 2.8401 (inf)	loss_scale 8192.0000 (11485.9209)	mem 10059MB
[2024-07-10 17:52:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:56 lr 0.000088	 wd 0.0000	time 0.2332 (0.2801)	loss 0.9487 (0.8855)	grad_norm 2.4126 (inf)	loss_scale 8192.0000 (11342.7692)	mem 10059MB
[2024-07-10 17:52:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:28 lr 0.000087	 wd 0.0000	time 0.2526 (0.2808)	loss 0.8853 (0.8850)	grad_norm 2.8594 (inf)	loss_scale 8192.0000 (11211.5419)	mem 10059MB
[2024-07-10 17:52:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000087	 wd 0.0000	time 0.2533 (0.2803)	loss 0.8047 (0.8845)	grad_norm 3.3011 (inf)	loss_scale 8192.0000 (11090.8085)	mem 10059MB
[2024-07-10 17:53:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 6 training takes 0:11:46
[2024-07-10 17:54:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 62.341 (62.341)	Loss 0.3555 (0.3555)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 10059MB
[2024-07-10 17:54:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.166 Acc@5 97.610
[2024-07-10 17:54:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-10 17:54:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.17%
[2024-07-10 17:54:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 17:54:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 17:54:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][0/2502]	eta 23:04:08 lr 0.000087	 wd 0.0000	time 33.1927 (33.1927)	loss 1.0078 (1.0078)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:55:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:24:22 lr 0.000087	 wd 0.0000	time 0.2490 (0.6087)	loss 1.0303 (0.8892)	grad_norm 2.3480 (2.8005)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:55:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:16:43 lr 0.000087	 wd 0.0000	time 0.2511 (0.4358)	loss 0.9072 (0.8756)	grad_norm 2.4268 (2.7056)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:56:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:13:53 lr 0.000087	 wd 0.0000	time 0.2582 (0.3784)	loss 0.8691 (0.8750)	grad_norm 2.4024 (2.6763)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:56:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:57 lr 0.000087	 wd 0.0000	time 0.2333 (0.3697)	loss 0.9263 (0.8754)	grad_norm 2.9710 (2.6530)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:57:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:36 lr 0.000087	 wd 0.0000	time 0.2546 (0.3480)	loss 0.8364 (0.8751)	grad_norm 2.8563 (2.6412)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:57:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:10:34 lr 0.000086	 wd 0.0000	time 0.2471 (0.3334)	loss 0.8105 (0.8759)	grad_norm 2.7748 (2.6575)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:58:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:09:48 lr 0.000086	 wd 0.0000	time 0.2309 (0.3264)	loss 0.8184 (0.8752)	grad_norm 2.3530 (2.6821)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:58:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:01 lr 0.000086	 wd 0.0000	time 0.2474 (0.3184)	loss 0.8687 (0.8756)	grad_norm 2.8783 (2.6807)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:59:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:08:20 lr 0.000086	 wd 0.0000	time 0.2682 (0.3122)	loss 0.7036 (0.8762)	grad_norm 2.8373 (2.6725)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:59:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:07:41 lr 0.000086	 wd 0.0000	time 0.2772 (0.3075)	loss 0.8384 (0.8771)	grad_norm 2.9867 (2.6674)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 17:59:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:06 lr 0.000086	 wd 0.0000	time 0.2277 (0.3042)	loss 0.8423 (0.8760)	grad_norm 2.5007 (2.6728)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:00:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:06:31 lr 0.000086	 wd 0.0000	time 0.2281 (0.3009)	loss 0.7573 (0.8766)	grad_norm 2.2277 (2.6740)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:00:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:05:58 lr 0.000085	 wd 0.0000	time 0.2186 (0.2980)	loss 0.9346 (0.8762)	grad_norm 2.4114 (2.6613)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:01:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:05:25 lr 0.000085	 wd 0.0000	time 0.2312 (0.2957)	loss 0.8936 (0.8772)	grad_norm 2.3011 (2.6527)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:01:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:04:54 lr 0.000085	 wd 0.0000	time 0.2643 (0.2943)	loss 0.8335 (0.8768)	grad_norm 3.0537 (2.6518)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:02:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:04:23 lr 0.000085	 wd 0.0000	time 0.2375 (0.2925)	loss 0.9302 (0.8775)	grad_norm 2.4456 (2.6477)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:02:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:03:53 lr 0.000085	 wd 0.0000	time 0.2415 (0.2908)	loss 0.8237 (0.8771)	grad_norm 2.3814 (2.6390)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:03:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:23 lr 0.000085	 wd 0.0000	time 0.2627 (0.2902)	loss 0.8018 (0.8763)	grad_norm 2.5254 (2.6407)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:03:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:54 lr 0.000085	 wd 0.0000	time 0.2192 (0.2892)	loss 1.0225 (0.8774)	grad_norm 2.8939 (2.6356)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:03:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:24 lr 0.000084	 wd 0.0000	time 0.2499 (0.2880)	loss 0.7827 (0.8771)	grad_norm 2.8649 (2.6335)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:04:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:55 lr 0.000084	 wd 0.0000	time 0.2514 (0.2870)	loss 0.9053 (0.8775)	grad_norm 2.7189 (2.6386)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:05:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:29 lr 0.000084	 wd 0.0000	time 0.2508 (0.2968)	loss 0.8247 (0.8772)	grad_norm 2.9709 (2.6398)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:05:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:59 lr 0.000084	 wd 0.0000	time 0.2718 (0.2970)	loss 0.9092 (0.8770)	grad_norm 2.2376 (2.6372)	loss_scale 8192.0000 (8192.0000)	mem 10059MB
[2024-07-10 18:06:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:31 lr 0.000084	 wd 0.0000	time 0.2489 (0.3106)	loss 0.8120 (0.8775)	grad_norm 2.2547 (2.6432)	loss_scale 16384.0000 (8253.4144)	mem 10059MB
[2024-07-10 18:07:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000084	 wd 0.0000	time 0.2426 (0.3091)	loss 0.8223 (0.8780)	grad_norm 2.3059 (2.6387)	loss_scale 16384.0000 (8578.5078)	mem 10059MB
[2024-07-10 18:07:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 7 training takes 0:13:00
[2024-07-10 18:08:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 53.262 (53.262)	Loss 0.3408 (0.3408)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 10059MB
[2024-07-10 18:08:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.162 Acc@5 97.592
[2024-07-10 18:08:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-10 18:08:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.17%
[2024-07-10 18:08:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][0/2502]	eta 10:49:38 lr 0.000084	 wd 0.0000	time 15.5791 (15.5791)	loss 0.9648 (0.9648)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:09:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:16:48 lr 0.000083	 wd 0.0000	time 0.2710 (0.4197)	loss 0.8696 (0.8816)	grad_norm 2.7724 (2.6840)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:09:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:13:22 lr 0.000083	 wd 0.0000	time 0.2381 (0.3485)	loss 0.8306 (0.8742)	grad_norm 2.3000 (2.5788)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:10:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:11:43 lr 0.000083	 wd 0.0000	time 0.2479 (0.3195)	loss 0.9390 (0.8701)	grad_norm 2.4991 (2.6024)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:10:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:10:40 lr 0.000083	 wd 0.0000	time 0.2398 (0.3049)	loss 0.9155 (0.8700)	grad_norm 2.3850 (2.6024)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:10:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:09:53 lr 0.000083	 wd 0.0000	time 0.2748 (0.2967)	loss 0.9048 (0.8715)	grad_norm 2.4703 (2.5893)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:11:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:09:13 lr 0.000083	 wd 0.0000	time 0.2467 (0.2912)	loss 0.7271 (0.8713)	grad_norm 2.8022 (2.6212)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:11:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:08:36 lr 0.000083	 wd 0.0000	time 0.2593 (0.2867)	loss 0.9648 (0.8694)	grad_norm 2.7117 (2.6257)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:12:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:08:02 lr 0.000082	 wd 0.0000	time 0.2378 (0.2836)	loss 0.9722 (0.8707)	grad_norm 2.4288 (2.6424)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:12:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:07:31 lr 0.000082	 wd 0.0000	time 0.2538 (0.2817)	loss 0.9741 (0.8702)	grad_norm 2.3111 (2.6323)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:13:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:07:01 lr 0.000082	 wd 0.0000	time 0.2531 (0.2807)	loss 0.9214 (0.8716)	grad_norm 2.4447 (2.6370)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:13:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:06:31 lr 0.000082	 wd 0.0000	time 0.2386 (0.2790)	loss 0.8525 (0.8719)	grad_norm 2.8984 (2.6357)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:13:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:06:01 lr 0.000082	 wd 0.0000	time 0.2477 (0.2776)	loss 0.8813 (0.8722)	grad_norm 2.9294 (2.6223)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:14:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:05:33 lr 0.000082	 wd 0.0000	time 0.2322 (0.2772)	loss 0.8325 (0.8729)	grad_norm 2.2246 (2.6233)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:14:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:05:04 lr 0.000081	 wd 0.0000	time 0.2499 (0.2764)	loss 0.8525 (0.8736)	grad_norm 3.3687 (2.6259)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:15:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:04:36 lr 0.000081	 wd 0.0000	time 0.2401 (0.2755)	loss 0.7847 (0.8738)	grad_norm 2.9169 (2.6291)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:15:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:04:08 lr 0.000081	 wd 0.0000	time 0.2513 (0.2749)	loss 0.9199 (0.8739)	grad_norm 2.5006 (2.6345)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:16:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:03:40 lr 0.000081	 wd 0.0000	time 0.2396 (0.2755)	loss 0.7715 (0.8746)	grad_norm 2.1340 (2.6383)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:16:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:13 lr 0.000081	 wd 0.0000	time 0.2406 (0.2749)	loss 0.7979 (0.8752)	grad_norm 2.6565 (2.6429)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:17:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:45 lr 0.000081	 wd 0.0000	time 0.2444 (0.2743)	loss 0.7524 (0.8741)	grad_norm 2.3341 (2.6308)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:17:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:17 lr 0.000080	 wd 0.0000	time 0.2364 (0.2740)	loss 0.9194 (0.8740)	grad_norm 2.4206 (2.6256)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:18:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:53 lr 0.000080	 wd 0.0000	time 0.2484 (0.2819)	loss 1.0127 (0.8743)	grad_norm 2.1408 (2.6224)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:18:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:25 lr 0.000080	 wd 0.0000	time 0.2372 (0.2825)	loss 0.9302 (0.8742)	grad_norm 2.2075 (2.6149)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:19:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:00 lr 0.000080	 wd 0.0000	time 0.3103 (0.2987)	loss 0.9053 (0.8739)	grad_norm 2.5245 (2.6066)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:20:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:31 lr 0.000080	 wd 0.0000	time 0.2620 (0.3054)	loss 0.9004 (0.8737)	grad_norm 3.3502 (2.6048)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:21:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000080	 wd 0.0000	time 0.2479 (0.3064)	loss 0.9805 (0.8738)	grad_norm 2.7011 (2.6074)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:21:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 8 training takes 0:12:51
[2024-07-10 18:21:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 22.808 (22.808)	Loss 0.3643 (0.3643)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 18:21:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.228 Acc@5 97.574
[2024-07-10 18:21:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-10 18:21:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.23%
[2024-07-10 18:21:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 18:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 18:22:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][0/2502]	eta 11:22:57 lr 0.000080	 wd 0.0000	time 16.3779 (16.3779)	loss 0.7432 (0.7432)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:22:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:17:09 lr 0.000079	 wd 0.0000	time 0.2337 (0.4287)	loss 0.8691 (0.8589)	grad_norm 2.4890 (2.6662)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:23:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:13:15 lr 0.000079	 wd 0.0000	time 0.2483 (0.3454)	loss 0.7935 (0.8596)	grad_norm 3.2519 (2.7147)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:23:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:11:39 lr 0.000079	 wd 0.0000	time 0.2507 (0.3178)	loss 0.9287 (0.8668)	grad_norm 2.1848 (2.6710)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:23:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:10:39 lr 0.000079	 wd 0.0000	time 0.2507 (0.3044)	loss 0.7852 (0.8650)	grad_norm 2.4832 (2.6456)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:24:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:09:53 lr 0.000079	 wd 0.0000	time 0.2379 (0.2964)	loss 0.7510 (0.8638)	grad_norm 2.1326 (2.5929)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:24:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:09:12 lr 0.000079	 wd 0.0000	time 0.2438 (0.2906)	loss 0.7983 (0.8622)	grad_norm 2.1886 (2.6091)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:25:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:08:36 lr 0.000078	 wd 0.0000	time 0.2536 (0.2865)	loss 0.9316 (0.8633)	grad_norm 2.6626 (2.5968)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:25:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:08:02 lr 0.000078	 wd 0.0000	time 0.2355 (0.2837)	loss 1.0078 (0.8639)	grad_norm 2.3465 (2.6013)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:26:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:07:32 lr 0.000078	 wd 0.0000	time 0.2447 (0.2822)	loss 0.9951 (0.8653)	grad_norm 2.0901 (2.5871)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:26:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:07:00 lr 0.000078	 wd 0.0000	time 0.2422 (0.2802)	loss 0.8779 (0.8657)	grad_norm 3.6708 (2.5926)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:27:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:06:30 lr 0.000078	 wd 0.0000	time 0.2415 (0.2786)	loss 0.9297 (0.8650)	grad_norm 2.0036 (2.5884)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:27:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:06:02 lr 0.000078	 wd 0.0000	time 0.2447 (0.2781)	loss 0.8818 (0.8662)	grad_norm 2.6467 (2.5750)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:27:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:05:33 lr 0.000077	 wd 0.0000	time 0.2407 (0.2773)	loss 0.8687 (0.8663)	grad_norm 2.0902 (2.5697)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:28:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:05:04 lr 0.000077	 wd 0.0000	time 0.2411 (0.2764)	loss 0.9219 (0.8665)	grad_norm 2.6447 (2.5678)	loss_scale 32768.0000 (16641.2791)	mem 10059MB
[2024-07-10 18:28:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:04:36 lr 0.000077	 wd 0.0000	time 0.2421 (0.2757)	loss 0.8247 (0.8673)	grad_norm 2.3119 (inf)	loss_scale 16384.0000 (17169.9081)	mem 10059MB
[2024-07-10 18:29:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:04:08 lr 0.000077	 wd 0.0000	time 0.2515 (0.2758)	loss 0.8525 (0.8661)	grad_norm 2.3215 (inf)	loss_scale 16384.0000 (17120.8195)	mem 10059MB
[2024-07-10 18:29:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:03:40 lr 0.000077	 wd 0.0000	time 0.2412 (0.2752)	loss 0.8232 (0.8660)	grad_norm 3.2077 (inf)	loss_scale 16384.0000 (17077.5026)	mem 10059MB
[2024-07-10 18:30:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:12 lr 0.000077	 wd 0.0000	time 0.2727 (0.2745)	loss 0.8628 (0.8663)	grad_norm 1.9028 (inf)	loss_scale 16384.0000 (17038.9961)	mem 10059MB
[2024-07-10 18:30:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:45 lr 0.000076	 wd 0.0000	time 0.2229 (0.2741)	loss 0.8647 (0.8667)	grad_norm 2.6866 (inf)	loss_scale 16384.0000 (17004.5408)	mem 10059MB
[2024-07-10 18:31:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:17 lr 0.000076	 wd 0.0000	time 0.2463 (0.2745)	loss 0.8003 (0.8675)	grad_norm 2.9254 (inf)	loss_scale 16384.0000 (16973.5292)	mem 10059MB
[2024-07-10 18:31:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:50 lr 0.000076	 wd 0.0000	time 0.2470 (0.2741)	loss 0.8589 (0.8675)	grad_norm 2.2947 (inf)	loss_scale 16384.0000 (16945.4698)	mem 10059MB
[2024-07-10 18:31:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:22 lr 0.000076	 wd 0.0000	time 0.2452 (0.2737)	loss 0.6729 (0.8671)	grad_norm 3.3524 (inf)	loss_scale 16384.0000 (16919.9600)	mem 10059MB
[2024-07-10 18:32:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:57 lr 0.000076	 wd 0.0000	time 0.2350 (0.2826)	loss 0.8325 (0.8669)	grad_norm 2.5444 (inf)	loss_scale 16384.0000 (16896.6675)	mem 10059MB
[2024-07-10 18:33:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:29 lr 0.000075	 wd 0.0000	time 0.2676 (0.2862)	loss 0.9341 (0.8671)	grad_norm 2.1546 (inf)	loss_scale 16384.0000 (16875.3153)	mem 10059MB
[2024-07-10 18:33:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000075	 wd 0.0000	time 0.2453 (0.2854)	loss 0.8467 (0.8674)	grad_norm 3.0388 (inf)	loss_scale 16384.0000 (16855.6705)	mem 10059MB
[2024-07-10 18:34:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 9 training takes 0:12:12
[2024-07-10 18:35:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 65.169 (65.169)	Loss 0.3682 (0.3682)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 10059MB
[2024-07-10 18:35:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.196 Acc@5 97.592
[2024-07-10 18:35:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-10 18:35:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.23%
[2024-07-10 18:35:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][0/2502]	eta 14:19:07 lr 0.000075	 wd 0.0000	time 20.6027 (20.6027)	loss 0.9346 (0.9346)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:36:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:18:28 lr 0.000075	 wd 0.0000	time 0.2465 (0.4614)	loss 0.9233 (0.8755)	grad_norm 2.3237 (2.6476)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:36:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:13:53 lr 0.000075	 wd 0.0000	time 0.2459 (0.3622)	loss 0.8516 (0.8704)	grad_norm 2.8132 (2.6647)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:37:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:12:52 lr 0.000075	 wd 0.0000	time 0.2464 (0.3508)	loss 0.8438 (0.8675)	grad_norm 2.5113 (2.6067)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:37:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:11:30 lr 0.000075	 wd 0.0000	time 0.2601 (0.3283)	loss 0.8374 (0.8670)	grad_norm 1.9167 (2.5602)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:38:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:10:30 lr 0.000074	 wd 0.0000	time 0.2277 (0.3149)	loss 0.8008 (0.8684)	grad_norm 2.9560 (2.5878)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:38:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:09:43 lr 0.000074	 wd 0.0000	time 0.2572 (0.3067)	loss 0.7847 (0.8680)	grad_norm 2.0957 (2.5960)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:39:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:09:02 lr 0.000074	 wd 0.0000	time 0.2567 (0.3011)	loss 0.7930 (0.8653)	grad_norm 2.2284 (2.6034)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:39:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:08:24 lr 0.000074	 wd 0.0000	time 0.2529 (0.2964)	loss 0.8218 (0.8660)	grad_norm 2.2807 (2.5813)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:39:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:07:48 lr 0.000074	 wd 0.0000	time 0.2408 (0.2925)	loss 0.7881 (0.8650)	grad_norm 2.1774 (2.5817)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:40:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:07:15 lr 0.000073	 wd 0.0000	time 0.2837 (0.2903)	loss 0.9653 (0.8645)	grad_norm 1.9308 (2.5843)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:40:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:06:44 lr 0.000073	 wd 0.0000	time 0.2558 (0.2882)	loss 0.8955 (0.8641)	grad_norm 3.1936 (2.6034)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:41:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:06:12 lr 0.000073	 wd 0.0000	time 0.2625 (0.2861)	loss 0.9580 (0.8638)	grad_norm 2.3757 (2.6044)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:41:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:05:41 lr 0.000073	 wd 0.0000	time 0.2595 (0.2845)	loss 0.8022 (0.8636)	grad_norm 2.3110 (2.5939)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:42:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:05:13 lr 0.000073	 wd 0.0000	time 0.2521 (0.2849)	loss 0.8198 (0.8638)	grad_norm 2.7567 (2.5817)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:42:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:04:44 lr 0.000073	 wd 0.0000	time 0.2243 (0.2835)	loss 0.8638 (0.8637)	grad_norm 2.9325 (2.5798)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:43:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:04:14 lr 0.000072	 wd 0.0000	time 0.2366 (0.2823)	loss 0.6792 (0.8632)	grad_norm 2.0245 (2.5731)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:43:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:03:45 lr 0.000072	 wd 0.0000	time 0.2698 (0.2815)	loss 0.7881 (0.8632)	grad_norm 2.9257 (2.5692)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:44:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:17 lr 0.000072	 wd 0.0000	time 0.2503 (0.2818)	loss 0.9658 (0.8631)	grad_norm 2.8659 (2.5737)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:44:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:02:49 lr 0.000072	 wd 0.0000	time 0.2443 (0.2809)	loss 0.9580 (0.8646)	grad_norm 3.9883 (2.5771)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:44:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:20 lr 0.000072	 wd 0.0000	time 0.2241 (0.2802)	loss 1.0586 (0.8650)	grad_norm 2.2386 (2.5793)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:45:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:55 lr 0.000071	 wd 0.0000	time 0.2482 (0.2864)	loss 0.8457 (0.8652)	grad_norm 3.0266 (2.5815)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:46:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:26 lr 0.000071	 wd 0.0000	time 0.2547 (0.2866)	loss 0.8857 (0.8654)	grad_norm 2.0945 (2.5789)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:46:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:57 lr 0.000071	 wd 0.0000	time 0.2278 (0.2863)	loss 0.9336 (0.8653)	grad_norm 2.4815 (2.5764)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:47:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:31 lr 0.000071	 wd 0.0000	time 0.4653 (0.3062)	loss 0.9395 (0.8653)	grad_norm 2.6757 (2.5787)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:48:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000071	 wd 0.0000	time 0.2266 (0.3090)	loss 0.7490 (0.8648)	grad_norm 2.4953 (2.5763)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:48:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 10 training takes 0:13:04
[2024-07-10 18:49:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 37.810 (37.810)	Loss 0.3687 (0.3687)	Acc@1 91.406 (91.406)	Acc@5 98.828 (98.828)	Mem 10059MB
[2024-07-10 18:49:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.364 Acc@5 97.590
[2024-07-10 18:49:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-10 18:49:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.36%
[2024-07-10 18:49:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 18:49:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 18:49:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][0/2502]	eta 11:05:02 lr 0.000071	 wd 0.0000	time 15.9481 (15.9481)	loss 0.8330 (0.8330)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:50:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:17:07 lr 0.000070	 wd 0.0000	time 0.2342 (0.4279)	loss 0.8042 (0.8608)	grad_norm 2.1882 (2.6524)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:50:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:13:15 lr 0.000070	 wd 0.0000	time 0.2450 (0.3454)	loss 0.8574 (0.8583)	grad_norm 2.4653 (2.5576)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:51:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:11:39 lr 0.000070	 wd 0.0000	time 0.2547 (0.3177)	loss 0.8594 (0.8556)	grad_norm 2.8741 (2.5715)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:51:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:10:39 lr 0.000070	 wd 0.0000	time 0.2625 (0.3044)	loss 0.8188 (0.8526)	grad_norm 3.9758 (2.5611)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 18:51:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:09:55 lr 0.000070	 wd 0.0000	time 0.2495 (0.2973)	loss 0.8604 (0.8507)	grad_norm 2.4254 (inf)	loss_scale 16384.0000 (17888.3194)	mem 10059MB
[2024-07-10 18:52:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:09:13 lr 0.000069	 wd 0.0000	time 0.2454 (0.2912)	loss 0.9175 (0.8517)	grad_norm 2.5843 (inf)	loss_scale 16384.0000 (17638.0166)	mem 10059MB
[2024-07-10 18:52:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:08:37 lr 0.000069	 wd 0.0000	time 0.2281 (0.2870)	loss 0.8477 (0.8527)	grad_norm 2.4234 (inf)	loss_scale 16384.0000 (17459.1270)	mem 10059MB
[2024-07-10 18:53:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:08:03 lr 0.000069	 wd 0.0000	time 0.2347 (0.2840)	loss 0.9531 (0.8538)	grad_norm 3.4318 (inf)	loss_scale 16384.0000 (17324.9039)	mem 10059MB
[2024-07-10 18:53:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:07:31 lr 0.000069	 wd 0.0000	time 0.2542 (0.2816)	loss 0.8784 (0.8568)	grad_norm 2.7115 (inf)	loss_scale 16384.0000 (17220.4750)	mem 10059MB
[2024-07-10 18:54:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:07:00 lr 0.000069	 wd 0.0000	time 0.2360 (0.2797)	loss 0.8657 (0.8569)	grad_norm 2.5021 (inf)	loss_scale 16384.0000 (17136.9111)	mem 10059MB
[2024-07-10 18:54:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:06:30 lr 0.000069	 wd 0.0000	time 0.2578 (0.2783)	loss 0.8911 (0.8563)	grad_norm 2.2874 (inf)	loss_scale 16384.0000 (17068.5268)	mem 10059MB
[2024-07-10 18:55:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:06:01 lr 0.000068	 wd 0.0000	time 0.3571 (0.2777)	loss 0.9243 (0.8561)	grad_norm 2.9400 (inf)	loss_scale 16384.0000 (17011.5304)	mem 10059MB
[2024-07-10 18:55:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:05:32 lr 0.000068	 wd 0.0000	time 0.2397 (0.2769)	loss 0.8188 (0.8566)	grad_norm 2.1870 (inf)	loss_scale 16384.0000 (16963.2959)	mem 10059MB
[2024-07-10 18:55:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:05:04 lr 0.000068	 wd 0.0000	time 0.2508 (0.2760)	loss 0.8555 (0.8567)	grad_norm 2.6963 (inf)	loss_scale 16384.0000 (16921.9472)	mem 10059MB
[2024-07-10 18:56:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:04:35 lr 0.000068	 wd 0.0000	time 0.2462 (0.2752)	loss 0.8413 (0.8562)	grad_norm 2.3665 (inf)	loss_scale 16384.0000 (16886.1079)	mem 10059MB
[2024-07-10 18:56:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:04:08 lr 0.000068	 wd 0.0000	time 0.2536 (0.2752)	loss 0.7358 (0.8562)	grad_norm 2.7789 (inf)	loss_scale 16384.0000 (16854.7458)	mem 10059MB
[2024-07-10 18:57:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:03:40 lr 0.000067	 wd 0.0000	time 0.2257 (0.2747)	loss 0.8545 (0.8555)	grad_norm 2.2417 (inf)	loss_scale 16384.0000 (16827.0711)	mem 10059MB
[2024-07-10 18:57:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:12 lr 0.000067	 wd 0.0000	time 0.2444 (0.2741)	loss 0.8301 (0.8563)	grad_norm 2.7290 (inf)	loss_scale 16384.0000 (16802.4697)	mem 10059MB
[2024-07-10 18:58:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:44 lr 0.000067	 wd 0.0000	time 0.2474 (0.2736)	loss 0.8179 (0.8569)	grad_norm 2.4496 (inf)	loss_scale 16384.0000 (16780.4566)	mem 10059MB
[2024-07-10 18:58:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:17 lr 0.000067	 wd 0.0000	time 0.2482 (0.2749)	loss 0.6606 (0.8569)	grad_norm 3.0550 (inf)	loss_scale 16384.0000 (16760.6437)	mem 10059MB
[2024-07-10 18:59:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:50 lr 0.000067	 wd 0.0000	time 0.2453 (0.2744)	loss 0.9775 (0.8565)	grad_norm 3.0803 (inf)	loss_scale 16384.0000 (16742.7168)	mem 10059MB
[2024-07-10 18:59:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:22 lr 0.000066	 wd 0.0000	time 0.2525 (0.2740)	loss 0.9307 (0.8567)	grad_norm 2.2524 (inf)	loss_scale 16384.0000 (16726.4189)	mem 10059MB
[2024-07-10 19:00:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:56 lr 0.000066	 wd 0.0000	time 0.2249 (0.2811)	loss 0.8643 (0.8568)	grad_norm 2.6656 (inf)	loss_scale 16384.0000 (16711.5376)	mem 10059MB
[2024-07-10 19:00:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:28 lr 0.000066	 wd 0.0000	time 0.2771 (0.2821)	loss 0.8193 (0.8566)	grad_norm 2.6315 (inf)	loss_scale 16384.0000 (16697.8959)	mem 10059MB
[2024-07-10 19:01:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000066	 wd 0.0000	time 0.2526 (0.2817)	loss 0.8525 (0.8565)	grad_norm 2.6510 (inf)	loss_scale 16384.0000 (16685.3451)	mem 10059MB
[2024-07-10 19:01:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 11 training takes 0:11:50
[2024-07-10 19:02:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 70.594 (70.594)	Loss 0.3535 (0.3535)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 19:02:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.336 Acc@5 97.596
[2024-07-10 19:02:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-10 19:02:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.36%
[2024-07-10 19:03:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][0/2502]	eta 1 day, 2:19:13 lr 0.000066	 wd 0.0000	time 37.8711 (37.8711)	loss 0.8149 (0.8149)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:03:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:27:06 lr 0.000066	 wd 0.0000	time 0.2347 (0.6772)	loss 0.8145 (0.8482)	grad_norm 1.9661 (2.4782)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:04:18 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:18:01 lr 0.000065	 wd 0.0000	time 0.2528 (0.4697)	loss 0.8145 (0.8486)	grad_norm 1.9405 (2.5868)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:04:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:14:46 lr 0.000065	 wd 0.0000	time 0.2462 (0.4024)	loss 0.7700 (0.8570)	grad_norm 3.3087 (2.5351)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:05:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:12:52 lr 0.000065	 wd 0.0000	time 0.2595 (0.3673)	loss 0.8281 (0.8566)	grad_norm 2.0797 (2.5413)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:05:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:32 lr 0.000065	 wd 0.0000	time 0.2435 (0.3460)	loss 0.8896 (0.8587)	grad_norm 1.8706 (2.5243)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:06:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:10:31 lr 0.000065	 wd 0.0000	time 0.2554 (0.3319)	loss 0.8906 (0.8558)	grad_norm 2.8247 (2.5146)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:06:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:09:42 lr 0.000064	 wd 0.0000	time 0.2499 (0.3232)	loss 0.8149 (0.8570)	grad_norm 2.3906 (2.5326)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:06:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:08:57 lr 0.000064	 wd 0.0000	time 0.2475 (0.3159)	loss 1.0713 (0.8562)	grad_norm 3.1764 (2.5121)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:07:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:08:16 lr 0.000064	 wd 0.0000	time 0.2366 (0.3099)	loss 0.9624 (0.8559)	grad_norm 1.8013 (2.4924)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:07:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:07:38 lr 0.000064	 wd 0.0000	time 0.2489 (0.3054)	loss 0.7964 (0.8561)	grad_norm 2.8560 (2.4974)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:08:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:03 lr 0.000064	 wd 0.0000	time 0.2372 (0.3023)	loss 0.8286 (0.8576)	grad_norm 2.7486 (2.4964)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:08:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:06:29 lr 0.000063	 wd 0.0000	time 0.2423 (0.2990)	loss 0.9229 (0.8574)	grad_norm 2.6786 (2.5161)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:09:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:05:56 lr 0.000063	 wd 0.0000	time 0.2242 (0.2962)	loss 0.8530 (0.8577)	grad_norm 2.8751 (2.5256)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:09:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:05:24 lr 0.000063	 wd 0.0000	time 0.2538 (0.2941)	loss 1.0264 (0.8581)	grad_norm 2.3787 (2.5350)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:10:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:04:53 lr 0.000063	 wd 0.0000	time 0.2445 (0.2928)	loss 0.8774 (0.8583)	grad_norm 2.4169 (2.5352)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:10:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:04:22 lr 0.000063	 wd 0.0000	time 0.2522 (0.2910)	loss 0.8896 (0.8574)	grad_norm 2.7772 (2.5412)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:10:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:03:52 lr 0.000062	 wd 0.0000	time 0.2451 (0.2894)	loss 0.9321 (0.8572)	grad_norm 1.9699 (2.5395)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:11:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:22 lr 0.000062	 wd 0.0000	time 0.2750 (0.2890)	loss 0.7437 (0.8572)	grad_norm 3.0993 (2.5416)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:11:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:53 lr 0.000062	 wd 0.0000	time 0.2395 (0.2880)	loss 0.8062 (0.8575)	grad_norm 2.0167 (2.5480)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:12:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:24 lr 0.000062	 wd 0.0000	time 0.2569 (0.2869)	loss 0.8140 (0.8575)	grad_norm 2.0211 (2.5358)	loss_scale 32768.0000 (16465.8791)	mem 10059MB
[2024-07-10 19:12:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:54 lr 0.000062	 wd 0.0000	time 0.2611 (0.2859)	loss 0.9297 (0.8578)	grad_norm 3.2206 (2.5340)	loss_scale 32768.0000 (17241.8010)	mem 10059MB
[2024-07-10 19:13:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:29 lr 0.000061	 wd 0.0000	time 0.2444 (0.2947)	loss 0.9951 (0.8578)	grad_norm 2.8727 (2.5318)	loss_scale 32768.0000 (17947.2167)	mem 10059MB
[2024-07-10 19:14:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:59 lr 0.000061	 wd 0.0000	time 0.2393 (0.2943)	loss 0.7852 (0.8577)	grad_norm 1.9925 (2.5360)	loss_scale 32768.0000 (18591.3186)	mem 10059MB
[2024-07-10 19:15:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:31 lr 0.000061	 wd 0.0000	time 0.6466 (0.3079)	loss 0.8145 (0.8575)	grad_norm 2.4217 (2.5322)	loss_scale 32768.0000 (19181.7676)	mem 10059MB
[2024-07-10 19:15:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000061	 wd 0.0000	time 0.2405 (0.3177)	loss 0.8169 (0.8575)	grad_norm 2.0268 (2.5352)	loss_scale 32768.0000 (19724.9996)	mem 10059MB
[2024-07-10 19:16:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 12 training takes 0:13:20
[2024-07-10 19:16:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 37.869 (37.869)	Loss 0.3638 (0.3638)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 19:16:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.380 Acc@5 97.618
[2024-07-10 19:16:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-10 19:16:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.38%
[2024-07-10 19:16:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 19:16:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 19:17:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][0/2502]	eta 10:46:48 lr 0.000061	 wd 0.0000	time 15.5109 (15.5109)	loss 0.7227 (0.7227)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:17:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:17:34 lr 0.000061	 wd 0.0000	time 0.4569 (0.4389)	loss 0.7725 (0.8485)	grad_norm 2.4665 (2.6201)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:18:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:14:38 lr 0.000060	 wd 0.0000	time 0.2276 (0.3815)	loss 0.7441 (0.8450)	grad_norm 2.3025 (2.5326)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:18:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:12:31 lr 0.000060	 wd 0.0000	time 0.2419 (0.3415)	loss 0.8354 (0.8417)	grad_norm 2.8203 (2.5286)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:19:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:11:15 lr 0.000060	 wd 0.0000	time 0.2554 (0.3215)	loss 0.9043 (0.8456)	grad_norm 2.7623 (2.5343)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:19:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:10:21 lr 0.000060	 wd 0.0000	time 0.2536 (0.3106)	loss 0.8462 (0.8447)	grad_norm 2.4093 (2.5372)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:19:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:09:35 lr 0.000060	 wd 0.0000	time 0.2577 (0.3023)	loss 0.8398 (0.8434)	grad_norm 4.5009 (inf)	loss_scale 16384.0000 (31895.6406)	mem 10059MB
[2024-07-10 19:20:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:08:54 lr 0.000059	 wd 0.0000	time 0.2733 (0.2964)	loss 0.8218 (0.8441)	grad_norm 2.8246 (inf)	loss_scale 16384.0000 (29682.8531)	mem 10059MB
[2024-07-10 19:20:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:08:17 lr 0.000059	 wd 0.0000	time 0.2383 (0.2922)	loss 0.8486 (0.8446)	grad_norm 2.5258 (inf)	loss_scale 16384.0000 (28022.5718)	mem 10059MB
[2024-07-10 19:21:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:07:43 lr 0.000059	 wd 0.0000	time 0.2430 (0.2890)	loss 0.8242 (0.8460)	grad_norm 2.8115 (inf)	loss_scale 16384.0000 (26730.8324)	mem 10059MB
[2024-07-10 19:21:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:07:09 lr 0.000059	 wd 0.0000	time 0.2575 (0.2861)	loss 0.8662 (0.8486)	grad_norm 4.3570 (inf)	loss_scale 16384.0000 (25697.1828)	mem 10059MB
[2024-07-10 19:22:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:06:38 lr 0.000059	 wd 0.0000	time 0.2403 (0.2840)	loss 0.8530 (0.8485)	grad_norm 3.0430 (inf)	loss_scale 16384.0000 (24851.2988)	mem 10059MB
[2024-07-10 19:22:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:06:07 lr 0.000058	 wd 0.0000	time 0.2364 (0.2825)	loss 0.9336 (0.8481)	grad_norm 1.8485 (inf)	loss_scale 16384.0000 (24146.2781)	mem 10059MB
[2024-07-10 19:23:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:05:39 lr 0.000058	 wd 0.0000	time 0.2392 (0.2821)	loss 0.8931 (0.8485)	grad_norm 2.0916 (inf)	loss_scale 16384.0000 (23549.6387)	mem 10059MB
[2024-07-10 19:23:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:05:09 lr 0.000058	 wd 0.0000	time 0.2537 (0.2807)	loss 0.8970 (0.8477)	grad_norm 2.2753 (inf)	loss_scale 16384.0000 (23038.1727)	mem 10059MB
[2024-07-10 19:23:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:04:40 lr 0.000058	 wd 0.0000	time 0.2231 (0.2795)	loss 0.7988 (0.8488)	grad_norm 2.4829 (inf)	loss_scale 16384.0000 (22594.8568)	mem 10059MB
[2024-07-10 19:24:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:04:11 lr 0.000058	 wd 0.0000	time 0.2505 (0.2792)	loss 0.7021 (0.8493)	grad_norm 2.0235 (inf)	loss_scale 16384.0000 (22206.9207)	mem 10059MB
[2024-07-10 19:24:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:03:43 lr 0.000057	 wd 0.0000	time 0.2460 (0.2785)	loss 0.8438 (0.8503)	grad_norm 2.6018 (inf)	loss_scale 16384.0000 (21864.5973)	mem 10059MB
[2024-07-10 19:25:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:14 lr 0.000057	 wd 0.0000	time 0.2282 (0.2777)	loss 0.8521 (0.8509)	grad_norm 2.1595 (inf)	loss_scale 16384.0000 (21560.2887)	mem 10059MB
[2024-07-10 19:25:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:46 lr 0.000057	 wd 0.0000	time 0.2516 (0.2772)	loss 0.7339 (0.8503)	grad_norm 2.2959 (inf)	loss_scale 16384.0000 (21287.9958)	mem 10059MB
[2024-07-10 19:26:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:24 lr 0.000057	 wd 0.0000	time 0.2528 (0.2869)	loss 0.8765 (0.8499)	grad_norm 2.0328 (inf)	loss_scale 16384.0000 (21042.9185)	mem 10059MB
[2024-07-10 19:26:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:55 lr 0.000057	 wd 0.0000	time 0.2674 (0.2876)	loss 0.8340 (0.8501)	grad_norm 2.1336 (inf)	loss_scale 16384.0000 (20821.1709)	mem 10059MB
[2024-07-10 19:28:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:31 lr 0.000056	 wd 0.0000	time 0.2783 (0.3028)	loss 0.8042 (0.8499)	grad_norm 2.5167 (inf)	loss_scale 16384.0000 (20619.5729)	mem 10059MB
[2024-07-10 19:28:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:01 lr 0.000056	 wd 0.0000	time 0.2519 (0.3024)	loss 0.8892 (0.8499)	grad_norm 2.1391 (inf)	loss_scale 16384.0000 (20435.4976)	mem 10059MB
[2024-07-10 19:29:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:30 lr 0.000056	 wd 0.0000	time 0.3423 (0.3016)	loss 0.8281 (0.8500)	grad_norm 1.8620 (inf)	loss_scale 16384.0000 (20266.7555)	mem 10059MB
[2024-07-10 19:30:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000056	 wd 0.0000	time 0.2461 (0.3154)	loss 0.7729 (0.8499)	grad_norm 2.1429 (inf)	loss_scale 16384.0000 (20111.5074)	mem 10059MB
[2024-07-10 19:30:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 13 training takes 0:13:14
[2024-07-10 19:31:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 61.235 (61.235)	Loss 0.3718 (0.3718)	Acc@1 91.406 (91.406)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 19:31:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.454 Acc@5 97.612
[2024-07-10 19:31:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-10 19:31:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.45%
[2024-07-10 19:31:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 19:31:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 19:31:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][0/2502]	eta 11:03:25 lr 0.000056	 wd 0.0000	time 15.9094 (15.9094)	loss 0.8013 (0.8013)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:32:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:17:07 lr 0.000055	 wd 0.0000	time 0.2410 (0.4278)	loss 0.8232 (0.8479)	grad_norm 2.2470 (2.5244)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:32:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:14:20 lr 0.000055	 wd 0.0000	time 0.2179 (0.3736)	loss 0.8242 (0.8434)	grad_norm 2.7060 (2.5872)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:33:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:12:21 lr 0.000055	 wd 0.0000	time 0.2631 (0.3368)	loss 0.9126 (0.8451)	grad_norm 2.0700 (2.5569)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:33:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:11:07 lr 0.000055	 wd 0.0000	time 0.2467 (0.3178)	loss 0.9966 (0.8466)	grad_norm 1.8792 (2.5372)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:34:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:10:15 lr 0.000055	 wd 0.0000	time 0.2394 (0.3077)	loss 0.8345 (0.8452)	grad_norm 2.4327 (2.5104)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:34:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:09:32 lr 0.000054	 wd 0.0000	time 0.2461 (0.3011)	loss 0.7905 (0.8468)	grad_norm 2.2509 (2.5155)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:34:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:08:51 lr 0.000054	 wd 0.0000	time 0.2654 (0.2952)	loss 0.7583 (0.8486)	grad_norm 2.0471 (2.5090)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:35:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:08:15 lr 0.000054	 wd 0.0000	time 0.2358 (0.2908)	loss 0.8564 (0.8467)	grad_norm 2.0519 (2.4939)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:35:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:07:42 lr 0.000054	 wd 0.0000	time 0.2614 (0.2889)	loss 0.8882 (0.8461)	grad_norm 2.2305 (2.4930)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:36:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:07:10 lr 0.000054	 wd 0.0000	time 0.2449 (0.2867)	loss 0.9849 (0.8456)	grad_norm 2.5950 (2.4971)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:36:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:06:38 lr 0.000053	 wd 0.0000	time 0.2247 (0.2844)	loss 0.7612 (0.8460)	grad_norm 2.3398 (2.5044)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:37:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:06:08 lr 0.000053	 wd 0.0000	time 0.2255 (0.2829)	loss 0.8916 (0.8463)	grad_norm 2.0917 (2.5000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:37:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:05:39 lr 0.000053	 wd 0.0000	time 0.2630 (0.2821)	loss 0.8828 (0.8459)	grad_norm 1.8253 (2.4933)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:38:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:05:09 lr 0.000053	 wd 0.0000	time 0.2446 (0.2808)	loss 0.8594 (0.8452)	grad_norm 3.1231 (2.5051)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:38:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:04:40 lr 0.000053	 wd 0.0000	time 0.2238 (0.2796)	loss 0.9146 (0.8453)	grad_norm 3.2457 (2.4994)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:38:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:04:11 lr 0.000052	 wd 0.0000	time 0.2600 (0.2789)	loss 0.8989 (0.8458)	grad_norm 2.1372 (2.4929)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:39:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:03:43 lr 0.000052	 wd 0.0000	time 0.2418 (0.2791)	loss 0.9443 (0.8459)	grad_norm 2.0858 (2.5073)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:39:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:15 lr 0.000052	 wd 0.0000	time 0.2386 (0.2784)	loss 0.9551 (0.8459)	grad_norm 2.4846 (2.5092)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:40:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:47 lr 0.000052	 wd 0.0000	time 0.2569 (0.2777)	loss 0.8452 (0.8458)	grad_norm 2.0329 (2.5046)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:41:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:23 lr 0.000052	 wd 0.0000	time 0.2793 (0.2865)	loss 0.8755 (0.8459)	grad_norm 2.0844 (2.4964)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:41:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:55 lr 0.000051	 wd 0.0000	time 0.2835 (0.2869)	loss 0.8198 (0.8465)	grad_norm 2.0438 (2.4966)	loss_scale 32768.0000 (16649.1385)	mem 10059MB
[2024-07-10 19:41:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:26 lr 0.000051	 wd 0.0000	time 0.2482 (0.2865)	loss 0.7832 (0.8472)	grad_norm 2.1629 (2.5032)	loss_scale 32768.0000 (17381.4811)	mem 10059MB
[2024-07-10 19:43:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:03 lr 0.000051	 wd 0.0000	time 0.2719 (0.3137)	loss 0.9370 (0.8475)	grad_norm 2.3978 (2.5006)	loss_scale 32768.0000 (18050.1695)	mem 10059MB
[2024-07-10 19:44:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:32 lr 0.000051	 wd 0.0000	time 0.2232 (0.3151)	loss 0.8242 (0.8477)	grad_norm 2.1095 (2.4988)	loss_scale 32768.0000 (18663.1570)	mem 10059MB
[2024-07-10 19:44:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000051	 wd 0.0000	time 0.2439 (0.3128)	loss 0.8232 (0.8477)	grad_norm 2.4886 (2.4980)	loss_scale 32768.0000 (19227.1251)	mem 10059MB
[2024-07-10 19:44:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 14 training takes 0:13:06
[2024-07-10 19:44:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 18.666 (18.666)	Loss 0.3623 (0.3623)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 19:45:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.604 Acc@5 97.592
[2024-07-10 19:45:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-10 19:45:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 19:45:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 19:45:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 19:45:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][0/2502]	eta 20:00:25 lr 0.000051	 wd 0.0000	time 28.7873 (28.7873)	loss 0.7280 (0.7280)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:46:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:22:24 lr 0.000050	 wd 0.0000	time 0.2532 (0.5597)	loss 0.8545 (0.8395)	grad_norm 2.0802 (2.5531)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:46:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:15:45 lr 0.000050	 wd 0.0000	time 0.2509 (0.4109)	loss 1.0732 (0.8454)	grad_norm 2.4248 (2.5380)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:46:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:13:19 lr 0.000050	 wd 0.0000	time 0.2315 (0.3629)	loss 0.7485 (0.8453)	grad_norm 2.1619 (2.5159)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:47:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:11:52 lr 0.000050	 wd 0.0000	time 0.2546 (0.3390)	loss 0.6597 (0.8453)	grad_norm 2.2867 (2.4929)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:47:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:10:47 lr 0.000049	 wd 0.0000	time 0.2404 (0.3234)	loss 0.7847 (0.8453)	grad_norm 2.1603 (2.5068)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:48:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:09:54 lr 0.000049	 wd 0.0000	time 0.2474 (0.3128)	loss 0.7529 (0.8451)	grad_norm 2.4528 (2.5079)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:48:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:09:11 lr 0.000049	 wd 0.0000	time 0.2730 (0.3061)	loss 0.8511 (0.8443)	grad_norm 2.1871 (2.4956)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:49:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:08:31 lr 0.000049	 wd 0.0000	time 0.2381 (0.3007)	loss 0.8101 (0.8435)	grad_norm 2.1110 (2.4779)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:49:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:07:55 lr 0.000049	 wd 0.0000	time 0.2543 (0.2965)	loss 0.8125 (0.8435)	grad_norm 3.1394 (2.4841)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:49:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:07:20 lr 0.000048	 wd 0.0000	time 0.2359 (0.2931)	loss 0.8047 (0.8440)	grad_norm 2.4759 (2.4916)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:50:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:06:48 lr 0.000048	 wd 0.0000	time 0.2390 (0.2911)	loss 0.9312 (0.8426)	grad_norm 3.0266 (2.4919)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:50:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:06:16 lr 0.000048	 wd 0.0000	time 0.2538 (0.2889)	loss 0.7285 (0.8424)	grad_norm 1.9456 (2.4906)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:51:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:05:44 lr 0.000048	 wd 0.0000	time 0.2493 (0.2869)	loss 0.9038 (0.8428)	grad_norm 2.6149 (2.4835)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:51:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:05:14 lr 0.000048	 wd 0.0000	time 0.2772 (0.2854)	loss 0.8193 (0.8425)	grad_norm 2.4053 (2.4783)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 19:52:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:04:46 lr 0.000047	 wd 0.0000	time 0.2781 (0.2859)	loss 0.8340 (0.8422)	grad_norm 3.4848 (inf)	loss_scale 16384.0000 (32025.7535)	mem 10059MB
[2024-07-10 19:52:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:04:16 lr 0.000047	 wd 0.0000	time 0.2524 (0.2846)	loss 0.9019 (0.8420)	grad_norm 2.3858 (inf)	loss_scale 16384.0000 (31048.7545)	mem 10059MB
[2024-07-10 19:53:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:03:47 lr 0.000047	 wd 0.0000	time 0.2321 (0.2833)	loss 0.8857 (0.8417)	grad_norm 1.6812 (inf)	loss_scale 16384.0000 (30186.6290)	mem 10059MB
[2024-07-10 19:53:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:18 lr 0.000047	 wd 0.0000	time 0.2430 (0.2834)	loss 0.8691 (0.8429)	grad_norm 2.2594 (inf)	loss_scale 16384.0000 (29420.2421)	mem 10059MB
[2024-07-10 19:54:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:50 lr 0.000047	 wd 0.0000	time 0.2308 (0.2827)	loss 0.8950 (0.8428)	grad_norm 2.0092 (inf)	loss_scale 16384.0000 (28734.4850)	mem 10059MB
[2024-07-10 19:54:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:21 lr 0.000046	 wd 0.0000	time 0.2542 (0.2819)	loss 0.9507 (0.8428)	grad_norm 2.4189 (inf)	loss_scale 16384.0000 (28117.2694)	mem 10059MB
[2024-07-10 19:54:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:53 lr 0.000046	 wd 0.0000	time 0.2334 (0.2811)	loss 0.8770 (0.8426)	grad_norm 2.2603 (inf)	loss_scale 16384.0000 (27558.8082)	mem 10059MB
[2024-07-10 19:55:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:28 lr 0.000046	 wd 0.0000	time 0.2437 (0.2922)	loss 0.8140 (0.8429)	grad_norm 2.1984 (inf)	loss_scale 16384.0000 (27051.0931)	mem 10059MB
[2024-07-10 19:56:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:58 lr 0.000046	 wd 0.0000	time 0.2434 (0.2919)	loss 0.9482 (0.8425)	grad_norm 4.2411 (inf)	loss_scale 16384.0000 (26587.5080)	mem 10059MB
[2024-07-10 19:57:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:30 lr 0.000046	 wd 0.0000	time 0.4643 (0.3036)	loss 0.8105 (0.8425)	grad_norm 2.0798 (inf)	loss_scale 16384.0000 (26162.5389)	mem 10059MB
[2024-07-10 19:58:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000045	 wd 0.0000	time 0.2263 (0.3109)	loss 1.0605 (0.8429)	grad_norm 2.1794 (inf)	loss_scale 16384.0000 (25771.5538)	mem 10059MB
[2024-07-10 19:58:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 15 training takes 0:13:02
[2024-07-10 19:58:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 145): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_15.pth saving......
[2024-07-10 19:58:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 147): INFO pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_15.pth saved !!!
[2024-07-10 19:58:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 41.510 (41.510)	Loss 0.3687 (0.3687)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 19:59:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.448 Acc@5 97.614
[2024-07-10 19:59:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-10 19:59:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 19:59:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][0/2502]	eta 11:40:15 lr 0.000045	 wd 0.0000	time 16.7927 (16.7927)	loss 0.7827 (0.7827)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 19:59:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:17:06 lr 0.000045	 wd 0.0000	time 0.2831 (0.4272)	loss 1.0010 (0.8476)	grad_norm 2.2654 (2.3909)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:00:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:14:32 lr 0.000045	 wd 0.0000	time 0.2564 (0.3791)	loss 0.8384 (0.8468)	grad_norm 2.3021 (2.4967)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:00:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:12:29 lr 0.000045	 wd 0.0000	time 0.2277 (0.3404)	loss 0.8540 (0.8457)	grad_norm 1.8885 (2.4992)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:01:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:11:13 lr 0.000045	 wd 0.0000	time 0.2482 (0.3204)	loss 0.9175 (0.8435)	grad_norm 2.4246 (2.4905)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:01:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:10:19 lr 0.000044	 wd 0.0000	time 0.2757 (0.3097)	loss 0.7744 (0.8411)	grad_norm 2.6454 (2.4942)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:02:05 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:09:34 lr 0.000044	 wd 0.0000	time 0.2555 (0.3019)	loss 0.8286 (0.8423)	grad_norm 2.4547 (2.4941)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:02:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:08:53 lr 0.000044	 wd 0.0000	time 0.2231 (0.2961)	loss 0.8584 (0.8418)	grad_norm 2.5922 (2.4724)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:02:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:08:16 lr 0.000044	 wd 0.0000	time 0.2473 (0.2917)	loss 0.9561 (0.8413)	grad_norm 2.7228 (2.4733)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:03:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:07:43 lr 0.000043	 wd 0.0000	time 0.2695 (0.2891)	loss 0.8096 (0.8431)	grad_norm 2.0711 (2.4534)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:03:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:07:10 lr 0.000043	 wd 0.0000	time 0.2578 (0.2868)	loss 0.7979 (0.8446)	grad_norm 2.8108 (2.4526)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:04:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:06:38 lr 0.000043	 wd 0.0000	time 0.2423 (0.2846)	loss 0.7598 (0.8433)	grad_norm 2.0429 (2.4590)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:04:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:06:08 lr 0.000043	 wd 0.0000	time 0.2426 (0.2828)	loss 0.9209 (0.8440)	grad_norm 2.2885 (2.4592)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:05:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:05:39 lr 0.000043	 wd 0.0000	time 0.2430 (0.2822)	loss 0.8330 (0.8422)	grad_norm 2.5919 (2.4636)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:05:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:05:09 lr 0.000042	 wd 0.0000	time 0.2506 (0.2809)	loss 0.8198 (0.8415)	grad_norm 2.9865 (2.4653)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:06:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:04:40 lr 0.000042	 wd 0.0000	time 0.2552 (0.2797)	loss 0.8599 (0.8412)	grad_norm 3.1883 (2.4793)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:06:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:04:11 lr 0.000042	 wd 0.0000	time 0.2696 (0.2789)	loss 0.8203 (0.8412)	grad_norm 2.6314 (2.4772)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:06:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:03:43 lr 0.000042	 wd 0.0000	time 0.2723 (0.2786)	loss 0.7090 (0.8420)	grad_norm 1.9041 (2.4746)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:07:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:15 lr 0.000042	 wd 0.0000	time 0.2231 (0.2779)	loss 0.8823 (0.8423)	grad_norm 2.3465 (2.4766)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:07:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:46 lr 0.000041	 wd 0.0000	time 0.2266 (0.2772)	loss 0.9541 (0.8426)	grad_norm 2.2409 (2.4830)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:08:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:19 lr 0.000041	 wd 0.0000	time 0.2301 (0.2777)	loss 0.7510 (0.8422)	grad_norm 2.3450 (2.4838)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:08:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:51 lr 0.000041	 wd 0.0000	time 0.2496 (0.2773)	loss 0.7471 (0.8420)	grad_norm 2.5489 (2.4836)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:09:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:23 lr 0.000041	 wd 0.0000	time 0.2260 (0.2767)	loss 0.7861 (0.8421)	grad_norm 2.0791 (2.4891)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:09:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:55 lr 0.000041	 wd 0.0000	time 0.2290 (0.2762)	loss 0.8740 (0.8420)	grad_norm 2.2844 (2.4885)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:10:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:28 lr 0.000040	 wd 0.0000	time 0.2616 (0.2838)	loss 0.7515 (0.8416)	grad_norm 2.2733 (2.4944)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:10:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.2408 (0.2834)	loss 0.8462 (0.8413)	grad_norm 2.0168 (2.5014)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:10:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 16 training takes 0:11:54
[2024-07-10 20:12:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 89.118 (89.118)	Loss 0.3572 (0.3572)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 20:12:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.554 Acc@5 97.616
[2024-07-10 20:12:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-10 20:12:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 20:13:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][0/2502]	eta 1 day, 4:46:12 lr 0.000040	 wd 0.0000	time 41.3957 (41.3957)	loss 0.7681 (0.7681)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:13:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:27:02 lr 0.000040	 wd 0.0000	time 0.2390 (0.6756)	loss 0.9062 (0.8347)	grad_norm 2.3184 (2.4395)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:14:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:17:58 lr 0.000040	 wd 0.0000	time 0.2376 (0.4687)	loss 0.9883 (0.8425)	grad_norm 2.3991 (2.4877)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:14:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:14:47 lr 0.000040	 wd 0.0000	time 0.2893 (0.4030)	loss 0.8896 (0.8425)	grad_norm 2.4600 (2.4865)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:15:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:12:52 lr 0.000039	 wd 0.0000	time 0.2606 (0.3675)	loss 0.7896 (0.8356)	grad_norm 2.6472 (2.4482)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:15:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:33 lr 0.000039	 wd 0.0000	time 0.2419 (0.3462)	loss 0.7710 (0.8377)	grad_norm 3.0293 (inf)	loss_scale 16384.0000 (17103.4571)	mem 10059MB
[2024-07-10 20:16:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:10:31 lr 0.000039	 wd 0.0000	time 0.2246 (0.3322)	loss 0.8774 (0.8394)	grad_norm 2.8563 (inf)	loss_scale 16384.0000 (16983.7471)	mem 10059MB
[2024-07-10 20:16:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:09:41 lr 0.000039	 wd 0.0000	time 0.2668 (0.3228)	loss 0.7690 (0.8386)	grad_norm 2.5839 (inf)	loss_scale 16384.0000 (16898.1912)	mem 10059MB
[2024-07-10 20:16:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:08:56 lr 0.000039	 wd 0.0000	time 0.2613 (0.3152)	loss 0.7627 (0.8385)	grad_norm 2.2649 (inf)	loss_scale 16384.0000 (16833.9975)	mem 10059MB
[2024-07-10 20:17:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:08:15 lr 0.000038	 wd 0.0000	time 0.2443 (0.3094)	loss 0.8442 (0.8388)	grad_norm 2.0149 (inf)	loss_scale 16384.0000 (16784.0533)	mem 10059MB
[2024-07-10 20:17:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:07:38 lr 0.000038	 wd 0.0000	time 0.2268 (0.3051)	loss 0.8853 (0.8387)	grad_norm 2.5698 (inf)	loss_scale 16384.0000 (16744.0879)	mem 10059MB
[2024-07-10 20:18:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:03 lr 0.000038	 wd 0.0000	time 0.2469 (0.3023)	loss 0.9102 (0.8400)	grad_norm 2.2783 (inf)	loss_scale 16384.0000 (16711.3824)	mem 10059MB
[2024-07-10 20:18:42 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:06:29 lr 0.000038	 wd 0.0000	time 0.2396 (0.2990)	loss 0.8936 (0.8406)	grad_norm 2.6477 (inf)	loss_scale 16384.0000 (16684.1232)	mem 10059MB
[2024-07-10 20:19:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:05:56 lr 0.000038	 wd 0.0000	time 0.2299 (0.2962)	loss 0.9785 (0.8394)	grad_norm 2.0471 (inf)	loss_scale 16384.0000 (16661.0546)	mem 10059MB
[2024-07-10 20:19:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:05:24 lr 0.000037	 wd 0.0000	time 0.2426 (0.2945)	loss 0.7817 (0.8400)	grad_norm 1.9170 (inf)	loss_scale 16384.0000 (16641.2791)	mem 10059MB
[2024-07-10 20:20:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:04:53 lr 0.000037	 wd 0.0000	time 0.2441 (0.2926)	loss 0.8125 (0.8402)	grad_norm 2.4482 (inf)	loss_scale 16384.0000 (16624.1386)	mem 10059MB
[2024-07-10 20:20:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:22 lr 0.000037	 wd 0.0000	time 0.2515 (0.2909)	loss 0.7681 (0.8398)	grad_norm 2.5732 (inf)	loss_scale 16384.0000 (16609.1393)	mem 10059MB
[2024-07-10 20:20:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:52 lr 0.000037	 wd 0.0000	time 0.2215 (0.2893)	loss 1.0059 (0.8397)	grad_norm 2.5328 (inf)	loss_scale 16384.0000 (16595.9036)	mem 10059MB
[2024-07-10 20:21:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:22 lr 0.000037	 wd 0.0000	time 0.2288 (0.2888)	loss 0.7964 (0.8397)	grad_norm 2.3673 (inf)	loss_scale 16384.0000 (16584.1377)	mem 10059MB
[2024-07-10 20:21:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:53 lr 0.000036	 wd 0.0000	time 0.2538 (0.2876)	loss 0.7832 (0.8398)	grad_norm 2.7160 (inf)	loss_scale 16384.0000 (16573.6097)	mem 10059MB
[2024-07-10 20:22:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:23 lr 0.000036	 wd 0.0000	time 0.2577 (0.2864)	loss 0.8335 (0.8393)	grad_norm 3.3640 (inf)	loss_scale 16384.0000 (16564.1339)	mem 10059MB
[2024-07-10 20:22:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:54 lr 0.000036	 wd 0.0000	time 0.2478 (0.2855)	loss 0.8018 (0.8393)	grad_norm 3.2875 (inf)	loss_scale 16384.0000 (16555.5602)	mem 10059MB
[2024-07-10 20:23:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:29 lr 0.000036	 wd 0.0000	time 0.2810 (0.2949)	loss 0.8413 (0.8393)	grad_norm 2.8662 (inf)	loss_scale 16384.0000 (16547.7656)	mem 10059MB
[2024-07-10 20:24:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:59 lr 0.000036	 wd 0.0000	time 0.2706 (0.2943)	loss 0.7886 (0.8391)	grad_norm 2.1937 (inf)	loss_scale 16384.0000 (16540.6484)	mem 10059MB
[2024-07-10 20:24:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:30 lr 0.000035	 wd 0.0000	time 0.3696 (0.3038)	loss 0.7266 (0.8397)	grad_norm 2.1250 (inf)	loss_scale 16384.0000 (16534.1241)	mem 10059MB
[2024-07-10 20:25:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.2447 (0.3027)	loss 0.9927 (0.8401)	grad_norm 2.7515 (inf)	loss_scale 16384.0000 (16528.1216)	mem 10059MB
[2024-07-10 20:25:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 17 training takes 0:12:42
[2024-07-10 20:26:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 55.238 (55.238)	Loss 0.3579 (0.3579)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 20:26:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.556 Acc@5 97.632
[2024-07-10 20:26:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-10 20:26:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 20:26:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][0/2502]	eta 11:44:36 lr 0.000035	 wd 0.0000	time 16.8973 (16.8973)	loss 0.8770 (0.8770)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:27:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:17:04 lr 0.000035	 wd 0.0000	time 0.2549 (0.4266)	loss 0.8179 (0.8290)	grad_norm 1.9927 (2.4926)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:27:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:09 lr 0.000035	 wd 0.0000	time 0.2356 (0.3690)	loss 0.9219 (0.8286)	grad_norm 2.0962 (2.4645)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:28:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:12:13 lr 0.000035	 wd 0.0000	time 0.2531 (0.3331)	loss 0.9282 (0.8334)	grad_norm 2.1158 (2.4673)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:28:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:11:02 lr 0.000034	 wd 0.0000	time 0.2210 (0.3150)	loss 0.8975 (0.8362)	grad_norm 2.0227 (2.4627)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:29:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:10:10 lr 0.000034	 wd 0.0000	time 0.2530 (0.3048)	loss 0.7944 (0.8368)	grad_norm 3.1951 (2.4613)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:29:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:09:28 lr 0.000034	 wd 0.0000	time 0.2287 (0.2986)	loss 0.7754 (0.8360)	grad_norm 2.2936 (2.4562)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:29:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:08:48 lr 0.000034	 wd 0.0000	time 0.2698 (0.2933)	loss 0.9541 (0.8350)	grad_norm 2.3022 (2.4540)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:30:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:08:12 lr 0.000034	 wd 0.0000	time 0.2459 (0.2893)	loss 0.8828 (0.8326)	grad_norm 2.2877 (2.4420)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:30:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:07:39 lr 0.000033	 wd 0.0000	time 0.2400 (0.2869)	loss 0.8750 (0.8328)	grad_norm 2.8124 (2.4507)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:31:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:07:08 lr 0.000033	 wd 0.0000	time 0.2281 (0.2855)	loss 0.7661 (0.8327)	grad_norm 2.0034 (2.4493)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:31:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:06:37 lr 0.000033	 wd 0.0000	time 0.2376 (0.2835)	loss 0.8252 (0.8338)	grad_norm 2.1004 (2.4470)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:32:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:06:07 lr 0.000033	 wd 0.0000	time 0.2540 (0.2820)	loss 1.1074 (0.8333)	grad_norm 2.7959 (2.4449)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:32:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:05:38 lr 0.000033	 wd 0.0000	time 0.2646 (0.2814)	loss 0.7466 (0.8341)	grad_norm 2.1999 (2.4440)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:33:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:05:08 lr 0.000032	 wd 0.0000	time 0.2455 (0.2803)	loss 0.8003 (0.8333)	grad_norm 4.2409 (2.4439)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:33:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:04:39 lr 0.000032	 wd 0.0000	time 0.2692 (0.2792)	loss 0.7666 (0.8336)	grad_norm 2.6668 (2.4433)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:34:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:04:11 lr 0.000032	 wd 0.0000	time 0.2568 (0.2784)	loss 0.8296 (0.8335)	grad_norm 2.4412 (2.4386)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:34:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:03:43 lr 0.000032	 wd 0.0000	time 0.2454 (0.2788)	loss 0.9565 (0.8334)	grad_norm 2.2897 (2.4431)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:34:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:15 lr 0.000032	 wd 0.0000	time 0.2432 (0.2780)	loss 0.8477 (0.8336)	grad_norm 2.1945 (2.4419)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:35:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:46 lr 0.000032	 wd 0.0000	time 0.2485 (0.2773)	loss 1.0068 (0.8336)	grad_norm 2.3683 (2.4367)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:35:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:20 lr 0.000031	 wd 0.0000	time 0.2560 (0.2803)	loss 0.9419 (0.8345)	grad_norm 1.9743 (2.4333)	loss_scale 32768.0000 (16809.7711)	mem 10059MB
[2024-07-10 20:36:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:53 lr 0.000031	 wd 0.0000	time 0.2605 (0.2836)	loss 0.7290 (0.8344)	grad_norm 2.3123 (2.4356)	loss_scale 32768.0000 (17569.3251)	mem 10059MB
[2024-07-10 20:36:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:25 lr 0.000031	 wd 0.0000	time 0.2303 (0.2841)	loss 0.7446 (0.8349)	grad_norm 2.5392 (2.4315)	loss_scale 32768.0000 (18259.8601)	mem 10059MB
[2024-07-10 20:37:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:59 lr 0.000031	 wd 0.0000	time 0.2456 (0.2961)	loss 0.7603 (0.8351)	grad_norm 2.3215 (2.4326)	loss_scale 32768.0000 (18890.3746)	mem 10059MB
[2024-07-10 20:38:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:30 lr 0.000031	 wd 0.0000	time 0.2318 (0.2957)	loss 0.8467 (0.8350)	grad_norm 2.4887 (2.4419)	loss_scale 32768.0000 (19468.3682)	mem 10059MB
[2024-07-10 20:38:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000030	 wd 0.0000	time 0.2447 (0.2945)	loss 0.8564 (0.8353)	grad_norm 2.4964 (2.4425)	loss_scale 32768.0000 (20000.1407)	mem 10059MB
[2024-07-10 20:38:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 18 training takes 0:12:22
[2024-07-10 20:40:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 65.943 (65.943)	Loss 0.3586 (0.3586)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 20:40:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.552 Acc@5 97.618
[2024-07-10 20:40:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-10 20:40:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 20:40:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][0/2502]	eta 1 day, 5:10:51 lr 0.000030	 wd 0.0000	time 41.9871 (41.9871)	loss 0.7866 (0.7866)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:41:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:27:05 lr 0.000030	 wd 0.0000	time 0.2637 (0.6768)	loss 0.9863 (0.8451)	grad_norm 2.7905 (2.4884)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:41:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:18:00 lr 0.000030	 wd 0.0000	time 0.2538 (0.4694)	loss 0.7002 (0.8437)	grad_norm 3.3213 (2.4989)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:42:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:14:48 lr 0.000030	 wd 0.0000	time 0.2671 (0.4034)	loss 0.8003 (0.8411)	grad_norm 2.5305 (2.4917)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:42:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:12:53 lr 0.000030	 wd 0.0000	time 0.2581 (0.3679)	loss 0.9141 (0.8390)	grad_norm 2.2859 (2.4804)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:43:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:33 lr 0.000029	 wd 0.0000	time 0.2603 (0.3466)	loss 0.8696 (0.8369)	grad_norm 1.8607 (2.4670)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:43:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:10:31 lr 0.000029	 wd 0.0000	time 0.2580 (0.3321)	loss 0.8091 (0.8366)	grad_norm 2.1127 (2.4673)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:44:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:09:42 lr 0.000029	 wd 0.0000	time 0.2486 (0.3234)	loss 0.8735 (0.8366)	grad_norm 2.4259 (2.4596)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 20:44:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:08:57 lr 0.000029	 wd 0.0000	time 0.2502 (0.3160)	loss 0.7393 (0.8357)	grad_norm 2.4490 (inf)	loss_scale 16384.0000 (30804.3745)	mem 10059MB
[2024-07-10 20:44:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:08:16 lr 0.000029	 wd 0.0000	time 0.2380 (0.3100)	loss 0.7607 (0.8343)	grad_norm 2.5997 (inf)	loss_scale 16384.0000 (29203.8890)	mem 10059MB
[2024-07-10 20:45:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:07:38 lr 0.000028	 wd 0.0000	time 0.2346 (0.3055)	loss 0.7666 (0.8341)	grad_norm 3.2282 (inf)	loss_scale 16384.0000 (27923.1808)	mem 10059MB
[2024-07-10 20:45:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:04 lr 0.000028	 wd 0.0000	time 0.2221 (0.3026)	loss 0.9189 (0.8342)	grad_norm 2.4338 (inf)	loss_scale 16384.0000 (26875.1172)	mem 10059MB
[2024-07-10 20:46:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:06:29 lr 0.000028	 wd 0.0000	time 0.2502 (0.2994)	loss 0.8604 (0.8341)	grad_norm 2.4525 (inf)	loss_scale 16384.0000 (26001.5853)	mem 10059MB
[2024-07-10 20:46:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:05:56 lr 0.000028	 wd 0.0000	time 0.2682 (0.2965)	loss 0.6938 (0.8346)	grad_norm 3.4493 (inf)	loss_scale 16384.0000 (25262.3397)	mem 10059MB
[2024-07-10 20:47:08 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:05:24 lr 0.000028	 wd 0.0000	time 0.2725 (0.2945)	loss 0.7437 (0.8351)	grad_norm 3.5879 (inf)	loss_scale 16384.0000 (24628.6253)	mem 10059MB
[2024-07-10 20:47:35 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:04:53 lr 0.000028	 wd 0.0000	time 0.2611 (0.2931)	loss 0.8755 (0.8357)	grad_norm 2.3795 (inf)	loss_scale 16384.0000 (24079.3498)	mem 10059MB
[2024-07-10 20:48:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:22 lr 0.000027	 wd 0.0000	time 0.2554 (0.2913)	loss 0.7749 (0.8357)	grad_norm 2.1357 (inf)	loss_scale 16384.0000 (23598.6908)	mem 10059MB
[2024-07-10 20:48:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:03:52 lr 0.000027	 wd 0.0000	time 0.2715 (0.2897)	loss 0.7393 (0.8354)	grad_norm 2.5312 (inf)	loss_scale 16384.0000 (23174.5467)	mem 10059MB
[2024-07-10 20:48:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:23 lr 0.000027	 wd 0.0000	time 0.2419 (0.2893)	loss 0.6973 (0.8358)	grad_norm 2.3344 (inf)	loss_scale 16384.0000 (22797.5036)	mem 10059MB
[2024-07-10 20:49:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:53 lr 0.000027	 wd 0.0000	time 0.2714 (0.2883)	loss 0.8408 (0.8354)	grad_norm 2.9029 (inf)	loss_scale 16384.0000 (22460.1284)	mem 10059MB
[2024-07-10 20:49:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:24 lr 0.000027	 wd 0.0000	time 0.2434 (0.2871)	loss 0.9004 (0.8346)	grad_norm 3.6020 (inf)	loss_scale 16384.0000 (22156.4738)	mem 10059MB
[2024-07-10 20:50:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:55 lr 0.000026	 wd 0.0000	time 0.2619 (0.2861)	loss 0.9707 (0.8341)	grad_norm 2.7148 (inf)	loss_scale 16384.0000 (21881.7249)	mem 10059MB
[2024-07-10 20:51:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:28 lr 0.000026	 wd 0.0000	time 0.2584 (0.2932)	loss 0.8364 (0.8339)	grad_norm 3.8369 (inf)	loss_scale 16384.0000 (21631.9418)	mem 10059MB
[2024-07-10 20:51:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:59 lr 0.000026	 wd 0.0000	time 0.2466 (0.2937)	loss 0.8691 (0.8343)	grad_norm 2.0681 (inf)	loss_scale 16384.0000 (21403.8696)	mem 10059MB
[2024-07-10 20:52:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:30 lr 0.000026	 wd 0.0000	time 0.4562 (0.3027)	loss 1.0361 (0.8339)	grad_norm 3.8098 (inf)	loss_scale 16384.0000 (21194.7955)	mem 10059MB
[2024-07-10 20:52:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.2451 (0.3041)	loss 0.7480 (0.8336)	grad_norm 1.8837 (inf)	loss_scale 16384.0000 (21002.4406)	mem 10059MB
[2024-07-10 20:53:02 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 19 training takes 0:12:46
[2024-07-10 20:53:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 56.849 (56.849)	Loss 0.3582 (0.3582)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 20:54:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.596 Acc@5 97.620
[2024-07-10 20:54:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-10 20:54:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 20:54:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][0/2502]	eta 11:08:53 lr 0.000026	 wd 0.0000	time 16.0407 (16.0407)	loss 0.9414 (0.9414)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:54:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:16:55 lr 0.000026	 wd 0.0000	time 0.2643 (0.4229)	loss 0.8799 (0.8398)	grad_norm 2.6101 (2.5532)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:55:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:14:26 lr 0.000025	 wd 0.0000	time 0.2497 (0.3764)	loss 0.9443 (0.8354)	grad_norm 2.2902 (2.4649)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:55:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:12:24 lr 0.000025	 wd 0.0000	time 0.2399 (0.3380)	loss 0.7622 (0.8335)	grad_norm 3.3311 (2.4786)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:56:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:11:10 lr 0.000025	 wd 0.0000	time 0.2196 (0.3191)	loss 0.7119 (0.8298)	grad_norm 2.1069 (2.4447)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:56:47 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:10:22 lr 0.000025	 wd 0.0000	time 0.2970 (0.3108)	loss 0.8682 (0.8310)	grad_norm 1.9290 (2.4460)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:57:20 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:09:58 lr 0.000025	 wd 0.0000	time 0.2185 (0.3149)	loss 0.7856 (0.8308)	grad_norm 2.0449 (2.4459)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:57:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:09:13 lr 0.000025	 wd 0.0000	time 0.2493 (0.3071)	loss 0.7378 (0.8307)	grad_norm 2.0153 (2.4393)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:58:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:08:33 lr 0.000024	 wd 0.0000	time 0.2384 (0.3015)	loss 0.8452 (0.8315)	grad_norm 2.2393 (2.4224)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:58:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:07:57 lr 0.000024	 wd 0.0000	time 0.2455 (0.2983)	loss 0.9458 (0.8311)	grad_norm 2.7331 (2.4113)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:59:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:07:22 lr 0.000024	 wd 0.0000	time 0.2306 (0.2948)	loss 0.8081 (0.8326)	grad_norm 2.9537 (2.4121)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:59:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:06:49 lr 0.000024	 wd 0.0000	time 0.2525 (0.2918)	loss 0.7471 (0.8325)	grad_norm 2.4842 (2.4089)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 20:59:59 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:06:17 lr 0.000024	 wd 0.0000	time 0.2494 (0.2898)	loss 0.7329 (0.8322)	grad_norm 2.2086 (2.4063)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:00:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:05:46 lr 0.000023	 wd 0.0000	time 0.2230 (0.2885)	loss 0.7754 (0.8311)	grad_norm 2.5997 (2.3969)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:00:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:05:15 lr 0.000023	 wd 0.0000	time 0.2561 (0.2867)	loss 0.8120 (0.8311)	grad_norm 3.1817 (2.3992)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:01:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:04:45 lr 0.000023	 wd 0.0000	time 0.2532 (0.2851)	loss 0.9141 (0.8310)	grad_norm 2.4633 (2.3954)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:01:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:04:16 lr 0.000023	 wd 0.0000	time 0.2714 (0.2840)	loss 0.8652 (0.8299)	grad_norm 1.9780 (2.3916)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:02:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:03:47 lr 0.000023	 wd 0.0000	time 0.2402 (0.2837)	loss 1.0156 (0.8299)	grad_norm 2.1169 (2.3881)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:02:40 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:18 lr 0.000023	 wd 0.0000	time 0.2267 (0.2826)	loss 0.8662 (0.8300)	grad_norm 2.5647 (2.3866)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:03:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:49 lr 0.000022	 wd 0.0000	time 0.2556 (0.2817)	loss 0.8389 (0.8301)	grad_norm 2.3038 (2.3872)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:03:34 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:21 lr 0.000022	 wd 0.0000	time 0.2654 (0.2817)	loss 0.8765 (0.8300)	grad_norm 2.5407 (2.3883)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:04:01 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:52 lr 0.000022	 wd 0.0000	time 0.2398 (0.2810)	loss 0.8672 (0.8302)	grad_norm 2.7031 (2.3955)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:04:28 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:24 lr 0.000022	 wd 0.0000	time 0.2325 (0.2802)	loss 0.8262 (0.8300)	grad_norm 2.5775 (2.3979)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:04:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:56 lr 0.000022	 wd 0.0000	time 0.2452 (0.2796)	loss 0.8174 (0.8305)	grad_norm 1.9764 (inf)	loss_scale 16384.0000 (16896.6675)	mem 10059MB
[2024-07-10 21:05:55 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:29 lr 0.000022	 wd 0.0000	time 0.2943 (0.2931)	loss 0.8110 (0.8309)	grad_norm 2.4430 (inf)	loss_scale 16384.0000 (16875.3153)	mem 10059MB
[2024-07-10 21:06:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.2406 (0.2924)	loss 0.7944 (0.8308)	grad_norm 2.2541 (inf)	loss_scale 16384.0000 (16855.6705)	mem 10059MB
[2024-07-10 21:06:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 20 training takes 0:12:18
[2024-07-10 21:07:31 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 61.486 (61.486)	Loss 0.3569 (0.3569)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 21:07:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.544 Acc@5 97.624
[2024-07-10 21:07:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-10 21:07:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 21:08:25 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][0/2502]	eta 1 day, 1:01:35 lr 0.000021	 wd 0.0000	time 36.0093 (36.0093)	loss 0.9165 (0.9165)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:08:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:24:46 lr 0.000021	 wd 0.0000	time 0.2496 (0.6188)	loss 0.8340 (0.8324)	grad_norm 2.0744 (2.4878)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:09:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:16:55 lr 0.000021	 wd 0.0000	time 0.2435 (0.4409)	loss 0.8213 (0.8354)	grad_norm 2.4324 (2.4913)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:09:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:14:03 lr 0.000021	 wd 0.0000	time 0.2269 (0.3832)	loss 0.7754 (0.8286)	grad_norm 2.5396 (2.4507)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:10:11 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:12:23 lr 0.000021	 wd 0.0000	time 0.2586 (0.3539)	loss 0.7153 (0.8262)	grad_norm 2.3062 (2.4116)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:10:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:10 lr 0.000021	 wd 0.0000	time 0.2579 (0.3351)	loss 0.9639 (0.8279)	grad_norm 2.2323 (2.3980)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:11:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:14 lr 0.000020	 wd 0.0000	time 0.2498 (0.3230)	loss 0.8521 (0.8279)	grad_norm 1.8909 (2.4105)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:11:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:09:27 lr 0.000020	 wd 0.0000	time 0.2542 (0.3150)	loss 0.7817 (0.8279)	grad_norm 2.5879 (2.4179)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:11:57 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:08:46 lr 0.000020	 wd 0.0000	time 0.2571 (0.3094)	loss 0.8374 (0.8276)	grad_norm 2.4035 (2.4203)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:12:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:08:07 lr 0.000020	 wd 0.0000	time 0.2456 (0.3042)	loss 0.8301 (0.8273)	grad_norm 2.2357 (2.4192)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:12:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:07:30 lr 0.000020	 wd 0.0000	time 0.2423 (0.3001)	loss 0.7559 (0.8263)	grad_norm 3.3465 (2.4101)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:13:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:06:57 lr 0.000020	 wd 0.0000	time 0.2618 (0.2977)	loss 0.8057 (0.8273)	grad_norm 2.4474 (2.4039)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:13:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:06:24 lr 0.000019	 wd 0.0000	time 0.2296 (0.2951)	loss 0.8975 (0.8271)	grad_norm 2.3650 (2.4110)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:14:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:05:51 lr 0.000019	 wd 0.0000	time 0.2335 (0.2926)	loss 0.9272 (0.8269)	grad_norm 1.9329 (2.4138)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:14:36 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:05:20 lr 0.000019	 wd 0.0000	time 0.2504 (0.2906)	loss 0.7651 (0.8270)	grad_norm 1.9863 (2.4145)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:15:03 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:04:50 lr 0.000019	 wd 0.0000	time 0.2426 (0.2896)	loss 0.7998 (0.8269)	grad_norm 2.5016 (2.4161)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:15:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:04:19 lr 0.000019	 wd 0.0000	time 0.2348 (0.2880)	loss 0.7925 (0.8262)	grad_norm 2.4385 (2.4187)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:15:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:03:49 lr 0.000019	 wd 0.0000	time 0.2600 (0.2867)	loss 0.8066 (0.8258)	grad_norm 2.3960 (2.4187)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:16:23 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:20 lr 0.000018	 wd 0.0000	time 0.3872 (0.2857)	loss 0.8296 (0.8253)	grad_norm 2.5047 (2.4218)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:16:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:52 lr 0.000018	 wd 0.0000	time 0.2472 (0.2860)	loss 0.7197 (0.8250)	grad_norm 2.8894 (2.4280)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:17:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:23 lr 0.000018	 wd 0.0000	time 0.2593 (0.2850)	loss 0.7773 (0.8253)	grad_norm 2.1427 (2.4303)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:17:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:54 lr 0.000018	 wd 0.0000	time 0.2473 (0.2841)	loss 0.7988 (0.8254)	grad_norm 2.5010 (2.4324)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:18:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:29 lr 0.000018	 wd 0.0000	time 0.2763 (0.2948)	loss 0.6895 (0.8253)	grad_norm 2.8103 (2.4354)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:19:06 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:59 lr 0.000018	 wd 0.0000	time 0.2391 (0.2943)	loss 0.9097 (0.8259)	grad_norm 2.8481 (2.4400)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:19:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:30 lr 0.000018	 wd 0.0000	time 0.8431 (0.2954)	loss 0.8223 (0.8261)	grad_norm 2.6841 (2.4408)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:20:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0000	time 0.2455 (0.3053)	loss 0.8691 (0.8264)	grad_norm 1.6667 (2.4428)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:20:39 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 21 training takes 0:12:50
[2024-07-10 21:21:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 58.480 (58.480)	Loss 0.3562 (0.3562)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 21:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.550 Acc@5 97.582
[2024-07-10 21:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-10 21:21:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 21:22:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][0/2502]	eta 11:25:15 lr 0.000017	 wd 0.0000	time 16.4330 (16.4330)	loss 0.8809 (0.8809)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:22:37 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:17:07 lr 0.000017	 wd 0.0000	time 0.2338 (0.4277)	loss 0.9043 (0.8373)	grad_norm 2.3013 (2.4876)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:23:07 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:14:05 lr 0.000017	 wd 0.0000	time 0.2190 (0.3675)	loss 0.7100 (0.8347)	grad_norm 2.2146 (2.4499)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:23:33 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:12:12 lr 0.000017	 wd 0.0000	time 0.2321 (0.3327)	loss 0.9136 (0.8308)	grad_norm 2.7503 (2.4539)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:24:00 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:11:02 lr 0.000017	 wd 0.0000	time 0.2467 (0.3151)	loss 0.8804 (0.8327)	grad_norm 1.8750 (2.4170)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:24:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:10:10 lr 0.000017	 wd 0.0000	time 0.2688 (0.3051)	loss 0.9248 (0.8335)	grad_norm 2.2986 (2.4203)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:24:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:09:27 lr 0.000016	 wd 0.0000	time 0.2409 (0.2983)	loss 0.9854 (0.8344)	grad_norm 2.4238 (2.4281)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:25:19 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:08:48 lr 0.000016	 wd 0.0000	time 0.2482 (0.2933)	loss 0.8325 (0.8353)	grad_norm 2.0554 (2.4213)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:25:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:08:12 lr 0.000016	 wd 0.0000	time 0.2416 (0.2893)	loss 0.7861 (0.8357)	grad_norm 2.3184 (2.4241)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:26:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:07:39 lr 0.000016	 wd 0.0000	time 0.2263 (0.2867)	loss 0.8374 (0.8345)	grad_norm 2.2878 (2.4135)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:26:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:07:06 lr 0.000016	 wd 0.0000	time 0.2264 (0.2842)	loss 0.7139 (0.8350)	grad_norm 2.7610 (2.4139)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:27:04 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:06:35 lr 0.000016	 wd 0.0000	time 0.2302 (0.2823)	loss 0.7700 (0.8350)	grad_norm 2.4204 (2.4255)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:27:30 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:06:05 lr 0.000016	 wd 0.0000	time 0.2489 (0.2806)	loss 0.8062 (0.8347)	grad_norm 2.2318 (2.4173)	loss_scale 16384.0000 (16384.0000)	mem 10059MB
[2024-07-10 21:27:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:05:36 lr 0.000015	 wd 0.0000	time 0.2504 (0.2802)	loss 0.8154 (0.8339)	grad_norm 2.3687 (2.3995)	loss_scale 32768.0000 (16761.8017)	mem 10059MB
[2024-07-10 21:28:24 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:05:07 lr 0.000015	 wd 0.0000	time 0.2366 (0.2790)	loss 0.8379 (0.8338)	grad_norm 2.5216 (2.4073)	loss_scale 32768.0000 (17904.2855)	mem 10059MB
[2024-07-10 21:28:51 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:04:38 lr 0.000015	 wd 0.0000	time 0.2490 (0.2780)	loss 0.6187 (0.8340)	grad_norm 2.3599 (2.3938)	loss_scale 32768.0000 (18894.5396)	mem 10059MB
[2024-07-10 21:29:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:04:10 lr 0.000015	 wd 0.0000	time 0.2676 (0.2773)	loss 0.9326 (0.8338)	grad_norm 2.1632 (2.3921)	loss_scale 32768.0000 (19761.0893)	mem 10059MB
[2024-07-10 21:29:45 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:03:42 lr 0.000015	 wd 0.0000	time 0.2483 (0.2775)	loss 0.6860 (0.8328)	grad_norm 1.9996 (2.3905)	loss_scale 32768.0000 (20525.7519)	mem 10059MB
[2024-07-10 21:30:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:14 lr 0.000015	 wd 0.0000	time 0.2483 (0.2768)	loss 1.0000 (0.8319)	grad_norm 2.0157 (2.3910)	loss_scale 32768.0000 (21205.4992)	mem 10059MB
[2024-07-10 21:30:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:46 lr 0.000015	 wd 0.0000	time 0.2626 (0.2761)	loss 0.8779 (0.8324)	grad_norm 2.0425 (2.3943)	loss_scale 32768.0000 (21813.7317)	mem 10059MB
[2024-07-10 21:31:32 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:25 lr 0.000014	 wd 0.0000	time 0.6352 (0.2894)	loss 0.8208 (0.8322)	grad_norm 2.1162 (2.3944)	loss_scale 32768.0000 (22361.1714)	mem 10059MB
[2024-07-10 21:32:14 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:58 lr 0.000014	 wd 0.0000	time 0.2566 (0.2956)	loss 0.7568 (0.8322)	grad_norm 2.3153 (2.4004)	loss_scale 32768.0000 (22856.4988)	mem 10059MB
[2024-07-10 21:32:50 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:30 lr 0.000014	 wd 0.0000	time 0.2438 (0.2983)	loss 0.6787 (0.8315)	grad_norm 1.8735 (2.3990)	loss_scale 32768.0000 (23306.8169)	mem 10059MB
[2024-07-10 21:33:16 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:59 lr 0.000014	 wd 0.0000	time 0.2245 (0.2968)	loss 0.8457 (0.8310)	grad_norm 2.7407 (2.4012)	loss_scale 32768.0000 (23717.9939)	mem 10059MB
[2024-07-10 21:33:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:30 lr 0.000014	 wd 0.0000	time 0.2593 (0.2955)	loss 0.7798 (0.8315)	grad_norm 2.3328 (2.3967)	loss_scale 32768.0000 (24094.9204)	mem 10059MB
[2024-07-10 21:34:09 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.2404 (0.2940)	loss 0.7920 (0.8314)	grad_norm 2.2747 (2.3931)	loss_scale 32768.0000 (24441.7049)	mem 10059MB
[2024-07-10 21:34:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 22 training takes 0:12:23
[2024-07-10 21:34:58 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 41.097 (41.097)	Loss 0.3564 (0.3564)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 10059MB
[2024-07-10 21:35:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.572 Acc@5 97.578
[2024-07-10 21:35:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-10 21:35:13 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 85.60%
[2024-07-10 21:35:29 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][0/2502]	eta 11:08:11 lr 0.000014	 wd 0.0000	time 16.0237 (16.0237)	loss 0.8613 (0.8613)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 21:35:56 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:17:11 lr 0.000014	 wd 0.0000	time 0.2519 (0.4295)	loss 0.8799 (0.8247)	grad_norm 2.3891 (2.3095)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 21:36:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:13:15 lr 0.000013	 wd 0.0000	time 0.2491 (0.3455)	loss 0.7427 (0.8218)	grad_norm 2.2646 (2.3470)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 21:36:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:11:38 lr 0.000013	 wd 0.0000	time 0.2724 (0.3170)	loss 0.7759 (0.8234)	grad_norm 2.2285 (2.3548)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 21:37:15 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:10:38 lr 0.000013	 wd 0.0000	time 0.2645 (0.3035)	loss 0.8979 (0.8271)	grad_norm 2.6189 (2.3793)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-10 21:37:41 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:09:51 lr 0.000013	 wd 0.0000	time 0.2432 (0.2956)	loss 0.9370 (0.8299)	grad_norm 2.8688 (2.4026)	loss_scale 32768.0000 (32768.0000)	mem 10059MB
[2024-07-11 09:23:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/config.json
[2024-07-11 09:23:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: convnext_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-11 09:23:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/convnext/diffusion_ft_convnext_base_224_22kto1k_sequence_crosslayer_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-11 09:23:21 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 108): INFO Creating model:convnext_diffusion_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0
[2024-07-11 09:23:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 110): INFO ConvNeXt_Diffusion_Finetune(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-11 09:23:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 113): INFO number of params: 51816808
[2024-07-11 09:23:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 148): INFO auto resuming from pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_15.pth
[2024-07-11 09:23:22 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 19): INFO ==============> Resuming form pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_15.pth....................
[2024-07-11 09:23:26 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 26): INFO <All keys matched successfully>
[2024-07-11 09:23:27 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 36): INFO => loaded successfully 'pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_15.pth' (epoch 15)
[2024-07-11 09:24:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/config.json
[2024-07-11 09:24:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: convnext_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 27
    - 3
    DIMS:
    - 128
    - 256
    - 512
    - 1024
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 0.8
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-06
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 0
  WARMUP_LR: 0.0
  WEIGHT_DECAY: 1.0e-08

[2024-07-11 09:24:44 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/convnext/diffusion_ft_convnext_base_224_22kto1k_sequence_crosslayer_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/convnext-b/convnext_base_22k_224.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-11 09:24:48 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 108): INFO Creating model:convnext_diffusion_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0
[2024-07-11 09:24:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 110): INFO ConvNeXt_Diffusion_Finetune(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=512, out_features=128, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (9): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (10): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (11): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (12): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (13): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (14): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (15): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (16): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (17): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (18): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (19): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (20): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (21): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (22): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (23): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (24): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (25): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
      (26): Block(
        (dwconv): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=2048, out_features=512, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=1024, out_features=4096, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=4096, out_features=1024, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-11 09:24:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 113): INFO number of params: 51816808
[2024-07-11 09:24:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 148): INFO auto resuming from pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth
[2024-07-11 09:24:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 19): INFO ==============> Resuming form pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth....................
[2024-07-11 09:24:52 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 26): INFO <All keys matched successfully>
[2024-07-11 09:24:53 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 36): INFO => loaded successfully 'pretrain/vcnu_finetune/convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_convnext_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth' (epoch 14)
[2024-07-11 09:25:54 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 61.625 (61.625)	Loss 0.3623 (0.3623)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 3686MB
[2024-07-11 09:26:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 85.602 Acc@5 97.592
[2024-07-11 09:26:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 155): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-11 09:26:10 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 168): INFO Start training
[2024-07-11 09:26:49 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][0/2502]	eta 1 day, 3:04:39 lr 0.000051	 wd 0.0000	time 38.9605 (38.9605)	loss 0.9224 (0.9224)	grad_norm 0.0000 (0.0000)	loss_scale 32768.0000 (32768.0000)	mem 10451MB
[2024-07-11 09:27:17 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:26:38 lr 0.000050	 wd 0.0000	time 0.2356 (0.6657)	loss 0.8467 (0.8354)	grad_norm 2.4641 (2.5340)	loss_scale 32768.0000 (32768.0000)	mem 10451MB
[2024-07-11 09:27:43 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:17:46 lr 0.000050	 wd 0.0000	time 0.2457 (0.4634)	loss 0.9004 (0.8355)	grad_norm 2.7626 (inf)	loss_scale 16384.0000 (30974.7264)	mem 10451MB
[2024-07-11 09:28:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:14:49 lr 0.000050	 wd 0.0000	time 0.4532 (0.4038)	loss 0.8560 (0.8348)	grad_norm 2.2956 (inf)	loss_scale 16384.0000 (26127.3090)	mem 10451MB
[2024-07-11 09:28:46 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:13:34 lr 0.000050	 wd 0.0000	time 0.2430 (0.3877)	loss 0.7383 (0.8354)	grad_norm 3.3492 (inf)	loss_scale 16384.0000 (23697.5561)	mem 10451MB
[2024-07-11 09:29:12 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:12:05 lr 0.000049	 wd 0.0000	time 0.2612 (0.3625)	loss 0.8877 (0.8367)	grad_norm 2.1775 (inf)	loss_scale 16384.0000 (22237.7645)	mem 10451MB
[2024-07-11 09:29:38 convnext_diffusion_finetune_base_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:10:58 lr 0.000049	 wd 0.0000	time 0.2673 (0.3461)	loss 0.7368 (0.8380)	grad_norm 2.6486 (inf)	loss_scale 16384.0000 (21263.7604)	mem 10451MB
