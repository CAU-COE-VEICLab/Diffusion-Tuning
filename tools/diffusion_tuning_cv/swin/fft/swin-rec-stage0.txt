[2024-07-01 22:18:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/config.json
[2024-07-01 22:18:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_swin_base_patch4_22kto1k_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-01 22:18:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility/configs/swin/diffusion_ft_swin_base_patch4_window7_224_22kto1k_finetune_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_swin_base_patch4_22kto1k_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-01 22:18:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 108): INFO Creating model:swin_diffusion_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0
[2024-07-01 22:18:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 110): INFO SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-01 22:18:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 113): INFO number of params: 1027048
[2024-07-01 22:18:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-01 22:18:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune, ignoring auto resume
[2024-07-01 22:18:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-07-01 22:18:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-01 22:18:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=[])
[2024-07-01 22:18:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth'
[2024-07-01 22:18:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 12.444 (12.444)	Loss 0.3547 (0.3547)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 1394MB
[2024-07-01 22:19:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 82.584 Acc@5 96.558
[2024-07-01 22:19:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 82.6%
[2024-07-01 22:19:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 168): INFO Start training
[2024-07-01 22:34:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/config.json
[2024-07-01 22:34:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_swin_base_patch4_22kto1k_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-01 22:34:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility/configs/swin/diffusion_ft_swin_base_patch4_window7_224_22kto1k_finetune_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_swin_base_patch4_22kto1k_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-01 22:35:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 108): INFO Creating model:swin_diffusion_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 110): INFO SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 113): INFO number of params: 1563568
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune, ignoring auto resume
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=[])
[2024-07-01 22:35:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility/pretrain/pretrain_weights/swin-b/swin_base_patch4_window7_224_22k.pth'
[2024-07-01 22:35:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 12.405 (12.405)	Loss 0.3547 (0.3547)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 1398MB
[2024-07-01 22:35:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 82.584 Acc@5 96.558
[2024-07-01 22:35:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 82.6%
[2024-07-01 22:35:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 168): INFO Start training
[2024-07-01 22:35:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][0/2502]	eta 7:41:31 lr 0.000000	 wd 0.0000	time 11.0679 (11.0679)	loss 1.8071 (1.8071)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 8821MB
[2024-07-01 22:36:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:13:39 lr 0.000000	 wd 0.0000	time 0.2270 (0.3410)	loss 1.6301 (1.5312)	grad_norm 3.3019 (nan)	loss_scale 4096.0000 (7299.8020)	mem 8828MB
[2024-07-01 22:36:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:10:59 lr 0.000001	 wd 0.0000	time 0.2298 (0.2865)	loss 1.4506 (1.5148)	grad_norm 5.6223 (nan)	loss_scale 4096.0000 (5705.8706)	mem 8828MB
[2024-07-01 22:36:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:09:50 lr 0.000001	 wd 0.0000	time 0.2316 (0.2683)	loss 1.3951 (1.4943)	grad_norm 4.5580 (nan)	loss_scale 2048.0000 (4599.4950)	mem 8828MB
[2024-07-01 22:37:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:09:04 lr 0.000001	 wd 0.0000	time 0.2294 (0.2592)	loss 2.0307 (1.4967)	grad_norm 4.6483 (nan)	loss_scale 2048.0000 (3963.2120)	mem 8828MB
[2024-07-01 22:37:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:08:28 lr 0.000002	 wd 0.0000	time 0.2302 (0.2538)	loss 1.7085 (1.4975)	grad_norm 4.7249 (nan)	loss_scale 2048.0000 (3580.9341)	mem 8828MB
[2024-07-01 22:37:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:07:55 lr 0.000002	 wd 0.0000	time 0.2296 (0.2502)	loss 1.0524 (1.5019)	grad_norm 4.8446 (nan)	loss_scale 2048.0000 (3325.8702)	mem 8828MB
[2024-07-01 22:38:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:07:26 lr 0.000002	 wd 0.0000	time 0.2288 (0.2476)	loss 1.6000 (1.4995)	grad_norm 5.6846 (nan)	loss_scale 1024.0000 (3096.8331)	mem 8828MB
[2024-07-01 22:38:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:06:58 lr 0.000003	 wd 0.0000	time 0.2326 (0.2458)	loss 1.7205 (1.4995)	grad_norm 6.6985 (nan)	loss_scale 1024.0000 (2838.0524)	mem 8828MB
[2024-07-01 22:39:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:06:31 lr 0.000003	 wd 0.0000	time 0.2289 (0.2443)	loss 1.7388 (1.4935)	grad_norm 7.0697 (nan)	loss_scale 1024.0000 (2636.7148)	mem 8828MB
[2024-07-01 22:39:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:06:05 lr 0.000003	 wd 0.0000	time 0.2291 (0.2432)	loss 1.4829 (1.4928)	grad_norm 5.7684 (nan)	loss_scale 1024.0000 (2475.6044)	mem 8828MB
[2024-07-01 22:39:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:05:39 lr 0.000004	 wd 0.0000	time 0.2284 (0.2422)	loss 1.6193 (1.4929)	grad_norm 6.4248 (nan)	loss_scale 1024.0000 (2343.7602)	mem 8828MB
[2024-07-01 22:40:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:05:14 lr 0.000004	 wd 0.0000	time 0.2302 (0.2414)	loss 1.4274 (1.4966)	grad_norm 9.8294 (nan)	loss_scale 1024.0000 (2233.8718)	mem 8828MB
[2024-07-01 22:40:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:04:49 lr 0.000004	 wd 0.0000	time 0.2296 (0.2407)	loss 1.6621 (1.4979)	grad_norm 3.9727 (nan)	loss_scale 1024.0000 (2140.8762)	mem 8828MB
[2024-07-01 22:41:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:04:24 lr 0.000005	 wd 0.0000	time 0.2299 (0.2402)	loss 1.6751 (1.4992)	grad_norm 7.4610 (nan)	loss_scale 1024.0000 (2061.1563)	mem 8828MB
[2024-07-01 22:41:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:04:00 lr 0.000005	 wd 0.0000	time 0.2282 (0.2397)	loss 1.5141 (1.4981)	grad_norm 4.0074 (nan)	loss_scale 1024.0000 (1992.0586)	mem 8828MB
[2024-07-01 22:41:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:03:35 lr 0.000005	 wd 0.0000	time 0.2289 (0.2393)	loss 1.9091 (1.4989)	grad_norm 5.8833 (nan)	loss_scale 1024.0000 (1931.5928)	mem 8828MB
[2024-07-01 22:42:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:03:11 lr 0.000005	 wd 0.0000	time 0.2303 (0.2390)	loss 1.6975 (1.4978)	grad_norm 8.3038 (nan)	loss_scale 1024.0000 (1878.2363)	mem 8828MB
[2024-07-01 22:42:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:02:47 lr 0.000006	 wd 0.0000	time 0.2305 (0.2387)	loss 1.2953 (1.4975)	grad_norm 7.5717 (nan)	loss_scale 1024.0000 (1830.8051)	mem 8828MB
[2024-07-01 22:43:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:23 lr 0.000006	 wd 0.0000	time 0.2350 (0.2384)	loss 1.5584 (1.4961)	grad_norm 6.5551 (nan)	loss_scale 1024.0000 (1788.3640)	mem 8828MB
[2024-07-01 22:43:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:01:59 lr 0.000006	 wd 0.0000	time 0.2307 (0.2382)	loss 1.6844 (1.4932)	grad_norm 4.2305 (nan)	loss_scale 1024.0000 (1750.1649)	mem 8828MB
[2024-07-01 22:43:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:35 lr 0.000007	 wd 0.0000	time 0.2335 (0.2380)	loss 1.5343 (1.4943)	grad_norm 5.6854 (nan)	loss_scale 1024.0000 (1715.6021)	mem 8828MB
[2024-07-01 22:44:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:11 lr 0.000007	 wd 0.0000	time 0.2362 (0.2378)	loss 1.6374 (1.4939)	grad_norm 20.1223 (nan)	loss_scale 1024.0000 (1684.1799)	mem 8828MB
[2024-07-01 22:44:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:48 lr 0.000007	 wd 0.0000	time 0.2312 (0.2376)	loss 1.5012 (1.4922)	grad_norm 5.9231 (nan)	loss_scale 1024.0000 (1655.4889)	mem 8828MB
[2024-07-01 22:44:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:24 lr 0.000008	 wd 0.0000	time 0.2302 (0.2375)	loss 1.6039 (1.4924)	grad_norm 6.8121 (nan)	loss_scale 1024.0000 (1629.1878)	mem 8828MB
[2024-07-01 22:45:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.2310 (0.2373)	loss 1.7321 (1.4924)	grad_norm 7.4414 (nan)	loss_scale 1024.0000 (1604.9900)	mem 8828MB
[2024-07-01 22:45:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 0 training takes 0:09:56
[2024-07-01 22:45:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_0.pth saving......
[2024-07-01 22:45:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_0.pth saved !!!
[2024-07-01 22:45:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.141 (10.141)	Loss 0.3638 (0.3638)	Acc@1 91.602 (91.602)	Acc@5 98.242 (98.242)	Mem 8828MB
[2024-07-01 22:45:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 82.754 Acc@5 96.578
[2024-07-01 22:45:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 82.8%
[2024-07-01 22:45:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 82.75%
[2024-07-01 22:45:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:45:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:45:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:09:40 lr 0.000008	 wd 0.0000	time 10.3038 (10.3038)	loss 1.3353 (1.3353)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 22:46:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:13:24 lr 0.000008	 wd 0.0000	time 0.2279 (0.3350)	loss 1.2385 (1.5144)	grad_norm 5.6466 (7.2589)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 22:46:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:10:53 lr 0.000009	 wd 0.0000	time 0.2294 (0.2837)	loss 1.5220 (1.5175)	grad_norm 4.4406 (7.3145)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 22:47:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:09:46 lr 0.000009	 wd 0.0000	time 0.2286 (0.2665)	loss 1.8954 (1.5014)	grad_norm 5.6279 (6.8715)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 22:47:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:09:02 lr 0.000009	 wd 0.0000	time 0.2289 (0.2579)	loss 0.9555 (1.4854)	grad_norm 4.6746 (6.7581)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 22:47:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:08:25 lr 0.000010	 wd 0.0000	time 0.2302 (0.2527)	loss 1.8028 (1.4835)	grad_norm 6.6746 (6.8517)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 22:48:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:07:54 lr 0.000010	 wd 0.0000	time 0.2329 (0.2493)	loss 1.4493 (1.4786)	grad_norm 4.2786 (6.8456)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 22:48:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:07:24 lr 0.000010	 wd 0.0000	time 0.2309 (0.2469)	loss 1.6083 (1.4757)	grad_norm 7.5074 (nan)	loss_scale 512.0000 (974.3338)	mem 8828MB
[2024-07-01 22:49:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:06:57 lr 0.000011	 wd 0.0000	time 0.2296 (0.2452)	loss 1.6780 (1.4794)	grad_norm 4.5289 (nan)	loss_scale 512.0000 (916.6142)	mem 8828MB
[2024-07-01 22:49:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:06:30 lr 0.000011	 wd 0.0000	time 0.2320 (0.2438)	loss 1.6642 (1.4771)	grad_norm 6.6623 (nan)	loss_scale 512.0000 (871.7070)	mem 8828MB
[2024-07-01 22:49:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:06:04 lr 0.000011	 wd 0.0000	time 0.2300 (0.2427)	loss 1.5356 (1.4749)	grad_norm 5.3692 (nan)	loss_scale 512.0000 (835.7722)	mem 8828MB
[2024-07-01 22:50:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:05:38 lr 0.000012	 wd 0.0000	time 0.2293 (0.2418)	loss 1.1824 (1.4747)	grad_norm 4.2441 (nan)	loss_scale 512.0000 (806.3651)	mem 8828MB
[2024-07-01 22:50:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:05:13 lr 0.000012	 wd 0.0000	time 0.2298 (0.2411)	loss 1.5824 (1.4773)	grad_norm 8.7995 (nan)	loss_scale 512.0000 (781.8551)	mem 8828MB
[2024-07-01 22:51:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:04:49 lr 0.000012	 wd 0.0000	time 0.2306 (0.2405)	loss 1.6298 (1.4786)	grad_norm 5.8936 (nan)	loss_scale 512.0000 (761.1130)	mem 8828MB
[2024-07-01 22:51:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:04:24 lr 0.000012	 wd 0.0000	time 0.2299 (0.2400)	loss 1.5424 (1.4759)	grad_norm 6.7710 (nan)	loss_scale 512.0000 (743.3319)	mem 8828MB
[2024-07-01 22:51:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:04:00 lr 0.000013	 wd 0.0000	time 0.2301 (0.2396)	loss 1.0107 (1.4735)	grad_norm 4.7571 (nan)	loss_scale 512.0000 (727.9201)	mem 8828MB
[2024-07-01 22:52:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:03:35 lr 0.000013	 wd 0.0000	time 0.2305 (0.2393)	loss 1.0351 (1.4719)	grad_norm 3.5691 (nan)	loss_scale 512.0000 (714.4335)	mem 8828MB
[2024-07-01 22:52:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:03:11 lr 0.000013	 wd 0.0000	time 0.2300 (0.2390)	loss 1.5384 (1.4714)	grad_norm 8.7862 (nan)	loss_scale 512.0000 (702.5326)	mem 8828MB
[2024-07-01 22:52:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:02:47 lr 0.000014	 wd 0.0000	time 0.2322 (0.2387)	loss 1.4350 (1.4705)	grad_norm 8.6591 (nan)	loss_scale 512.0000 (691.9534)	mem 8828MB
[2024-07-01 22:53:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:23 lr 0.000014	 wd 0.0000	time 0.2326 (0.2385)	loss 1.5014 (1.4710)	grad_norm 5.1925 (nan)	loss_scale 512.0000 (682.4871)	mem 8828MB
[2024-07-01 22:53:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:01:59 lr 0.000014	 wd 0.0000	time 0.2309 (0.2383)	loss 1.5868 (1.4702)	grad_norm 4.1914 (nan)	loss_scale 512.0000 (673.9670)	mem 8828MB
[2024-07-01 22:54:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:35 lr 0.000015	 wd 0.0000	time 0.2316 (0.2381)	loss 1.6804 (1.4709)	grad_norm 4.2251 (nan)	loss_scale 512.0000 (666.2580)	mem 8828MB
[2024-07-01 22:54:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:11 lr 0.000015	 wd 0.0000	time 0.2321 (0.2379)	loss 1.1468 (1.4712)	grad_norm 9.2409 (nan)	loss_scale 512.0000 (659.2494)	mem 8828MB
[2024-07-01 22:54:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:48 lr 0.000015	 wd 0.0000	time 0.2313 (0.2377)	loss 1.4959 (1.4710)	grad_norm 5.2377 (nan)	loss_scale 512.0000 (652.8501)	mem 8828MB
[2024-07-01 22:55:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:24 lr 0.000016	 wd 0.0000	time 0.2297 (0.2376)	loss 1.6683 (1.4687)	grad_norm 4.4886 (nan)	loss_scale 512.0000 (646.9838)	mem 8828MB
[2024-07-01 22:55:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.2298 (0.2374)	loss 1.1628 (1.4698)	grad_norm 4.1376 (nan)	loss_scale 512.0000 (641.5866)	mem 8828MB
[2024-07-01 22:55:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 1 training takes 0:09:56
[2024-07-01 22:55:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.135 (11.135)	Loss 0.3840 (0.3840)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 8828MB
[2024-07-01 22:56:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.010 Acc@5 96.668
[2024-07-01 22:56:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.0%
[2024-07-01 22:56:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.01%
[2024-07-01 22:56:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 22:56:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 22:56:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][0/2502]	eta 6:55:49 lr 0.000016	 wd 0.0000	time 9.9720 (9.9720)	loss 1.5442 (1.5442)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:56:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:13:17 lr 0.000016	 wd 0.0000	time 0.2287 (0.3320)	loss 1.6670 (1.4338)	grad_norm 9.0554 (6.5463)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:57:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:10:49 lr 0.000017	 wd 0.0000	time 0.2335 (0.2822)	loss 1.6060 (1.4520)	grad_norm 5.4697 (6.2701)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:57:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:09:44 lr 0.000017	 wd 0.0000	time 0.2306 (0.2655)	loss 1.2149 (1.4565)	grad_norm 7.1099 (6.1889)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:57:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:09:00 lr 0.000017	 wd 0.0000	time 0.2277 (0.2571)	loss 1.6539 (1.4515)	grad_norm 4.8088 (5.9619)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:58:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:08:24 lr 0.000018	 wd 0.0000	time 0.2280 (0.2521)	loss 1.5164 (1.4527)	grad_norm 3.7075 (5.8633)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:58:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:07:53 lr 0.000018	 wd 0.0000	time 0.2306 (0.2488)	loss 1.3506 (1.4458)	grad_norm 5.9941 (5.8584)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:59:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:07:24 lr 0.000018	 wd 0.0000	time 0.2297 (0.2465)	loss 1.3004 (1.4488)	grad_norm 6.8312 (5.9261)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:59:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:06:56 lr 0.000019	 wd 0.0000	time 0.2320 (0.2448)	loss 1.6349 (1.4466)	grad_norm 5.1777 (5.9986)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 22:59:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:06:30 lr 0.000019	 wd 0.0000	time 0.2295 (0.2435)	loss 1.5654 (1.4520)	grad_norm 4.8513 (5.9174)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:00:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:06:04 lr 0.000019	 wd 0.0000	time 0.2295 (0.2424)	loss 1.5363 (1.4536)	grad_norm 5.6008 (5.8485)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:00:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:05:38 lr 0.000020	 wd 0.0000	time 0.2300 (0.2416)	loss 1.2584 (1.4537)	grad_norm 5.6255 (5.8766)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:00:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:05:13 lr 0.000020	 wd 0.0000	time 0.2305 (0.2409)	loss 1.3793 (1.4520)	grad_norm 9.9997 (5.9207)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:01:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:04:48 lr 0.000020	 wd 0.0000	time 0.2301 (0.2403)	loss 1.7653 (1.4543)	grad_norm 4.1993 (5.9255)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:01:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:04:24 lr 0.000020	 wd 0.0000	time 0.2296 (0.2399)	loss 1.4519 (1.4543)	grad_norm 4.2477 (5.9324)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:02:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:03:59 lr 0.000021	 wd 0.0000	time 0.2299 (0.2395)	loss 1.6129 (1.4518)	grad_norm 5.2400 (5.9077)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:02:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:03:35 lr 0.000021	 wd 0.0000	time 0.2335 (0.2391)	loss 1.3536 (1.4516)	grad_norm 6.8253 (5.8981)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:02:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:03:11 lr 0.000021	 wd 0.0000	time 0.2313 (0.2388)	loss 1.3838 (1.4498)	grad_norm 14.7799 (5.8764)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:03:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:02:47 lr 0.000022	 wd 0.0000	time 0.2298 (0.2386)	loss 1.4134 (1.4494)	grad_norm 4.8593 (5.9328)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:03:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:02:23 lr 0.000022	 wd 0.0000	time 0.2370 (0.2384)	loss 1.0137 (1.4471)	grad_norm 5.7650 (5.9095)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:04:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:01:59 lr 0.000022	 wd 0.0000	time 0.2329 (0.2382)	loss 1.2405 (1.4445)	grad_norm 7.9569 (5.9264)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:04:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:35 lr 0.000023	 wd 0.0000	time 0.2355 (0.2380)	loss 1.5017 (1.4443)	grad_norm 4.7379 (5.9036)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:04:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:11 lr 0.000023	 wd 0.0000	time 0.2308 (0.2378)	loss 1.6620 (1.4435)	grad_norm 4.7160 (5.9307)	loss_scale 1024.0000 (528.2835)	mem 8828MB
[2024-07-01 23:05:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:48 lr 0.000023	 wd 0.0000	time 0.2264 (0.2376)	loss 1.7674 (1.4435)	grad_norm 4.9244 (5.9335)	loss_scale 1024.0000 (549.8270)	mem 8828MB
[2024-07-01 23:05:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:24 lr 0.000024	 wd 0.0000	time 0.2315 (0.2375)	loss 1.5726 (1.4421)	grad_norm 2.9723 (5.9336)	loss_scale 1024.0000 (569.5760)	mem 8828MB
[2024-07-01 23:06:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.2310 (0.2373)	loss 1.6413 (1.4419)	grad_norm 3.7940 (5.9040)	loss_scale 1024.0000 (587.7457)	mem 8828MB
[2024-07-01 23:06:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 2 training takes 0:09:56
[2024-07-01 23:06:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.226 (10.226)	Loss 0.4006 (0.4006)	Acc@1 91.992 (91.992)	Acc@5 98.242 (98.242)	Mem 8828MB
[2024-07-01 23:06:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.088 Acc@5 96.782
[2024-07-01 23:06:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.1%
[2024-07-01 23:06:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.09%
[2024-07-01 23:06:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:06:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:06:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:12:28 lr 0.000024	 wd 0.0000	time 10.3710 (10.3710)	loss 1.0167 (1.0167)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:07:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:13:19 lr 0.000024	 wd 0.0000	time 0.2326 (0.3329)	loss 1.5339 (1.4047)	grad_norm 7.7304 (5.5528)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:07:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:10:50 lr 0.000025	 wd 0.0000	time 0.2287 (0.2827)	loss 1.4824 (1.4220)	grad_norm 6.6985 (5.4670)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:07:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:09:45 lr 0.000025	 wd 0.0000	time 0.2287 (0.2659)	loss 1.6472 (1.4149)	grad_norm 5.5764 (5.7253)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:08:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:09:01 lr 0.000025	 wd 0.0000	time 0.2303 (0.2575)	loss 1.6250 (1.4177)	grad_norm 5.8489 (5.7997)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:08:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:08:25 lr 0.000026	 wd 0.0000	time 0.2320 (0.2524)	loss 1.5331 (1.4154)	grad_norm 6.1486 (6.0418)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:08:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:07:53 lr 0.000026	 wd 0.0000	time 0.2296 (0.2491)	loss 1.4405 (1.4158)	grad_norm 4.6195 (5.9239)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:09:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:07:24 lr 0.000026	 wd 0.0000	time 0.2303 (0.2468)	loss 1.7640 (1.4171)	grad_norm 5.4112 (5.9312)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:09:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:06:56 lr 0.000027	 wd 0.0000	time 0.2308 (0.2450)	loss 1.1268 (1.4176)	grad_norm 3.8984 (5.9417)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:10:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:06:30 lr 0.000027	 wd 0.0000	time 0.2323 (0.2437)	loss 1.5639 (1.4186)	grad_norm 6.0149 (5.9001)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:10:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:06:04 lr 0.000027	 wd 0.0000	time 0.2303 (0.2427)	loss 1.5421 (1.4216)	grad_norm 4.8762 (5.8490)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:10:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:05:39 lr 0.000028	 wd 0.0000	time 0.2307 (0.2418)	loss 1.4061 (1.4237)	grad_norm 3.5856 (5.8645)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:11:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:05:13 lr 0.000028	 wd 0.0000	time 0.2311 (0.2411)	loss 1.2848 (1.4215)	grad_norm 4.1129 (5.9219)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:11:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:04:49 lr 0.000028	 wd 0.0000	time 0.2306 (0.2405)	loss 1.3476 (1.4242)	grad_norm 5.7056 (5.9357)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:12:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:04:24 lr 0.000028	 wd 0.0000	time 0.2293 (0.2400)	loss 1.2978 (1.4246)	grad_norm 6.8815 (5.9135)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:12:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:04:00 lr 0.000029	 wd 0.0000	time 0.2304 (0.2396)	loss 1.5415 (1.4261)	grad_norm 4.3609 (5.8540)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:12:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:03:35 lr 0.000029	 wd 0.0000	time 0.2303 (0.2392)	loss 1.5767 (1.4248)	grad_norm 2.8554 (5.8395)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:13:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:03:11 lr 0.000029	 wd 0.0000	time 0.2377 (0.2389)	loss 1.1279 (1.4234)	grad_norm 4.1554 (5.9166)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:13:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:02:47 lr 0.000030	 wd 0.0000	time 0.2398 (0.2386)	loss 1.5612 (1.4235)	grad_norm 4.7843 (5.8916)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:14:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:02:23 lr 0.000030	 wd 0.0000	time 0.2315 (0.2384)	loss 1.5653 (1.4225)	grad_norm 5.4111 (5.8813)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:14:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:01:59 lr 0.000030	 wd 0.0000	time 0.2321 (0.2381)	loss 1.1809 (1.4230)	grad_norm 13.8586 (5.9037)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:14:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:35 lr 0.000031	 wd 0.0000	time 0.2343 (0.2380)	loss 1.2175 (1.4215)	grad_norm 4.2919 (5.9106)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:15:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:11 lr 0.000031	 wd 0.0000	time 0.2308 (0.2378)	loss 1.4337 (1.4221)	grad_norm 6.2255 (5.9035)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:15:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:00:47 lr 0.000031	 wd 0.0000	time 0.2300 (0.2376)	loss 1.5515 (1.4229)	grad_norm 6.8873 (5.8905)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:15:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:24 lr 0.000032	 wd 0.0000	time 0.2310 (0.2375)	loss 1.0514 (1.4219)	grad_norm 6.0025 (5.8805)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:16:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000032	 wd 0.0000	time 0.2314 (0.2373)	loss 1.4857 (1.4221)	grad_norm 4.4988 (5.8697)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:16:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 3 training takes 0:09:56
[2024-07-01 23:16:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.644 (11.644)	Loss 0.4194 (0.4194)	Acc@1 91.211 (91.211)	Acc@5 98.242 (98.242)	Mem 8828MB
[2024-07-01 23:16:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.172 Acc@5 96.844
[2024-07-01 23:16:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.2%
[2024-07-01 23:16:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.17%
[2024-07-01 23:16:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:16:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:16:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][0/2502]	eta 6:05:57 lr 0.000032	 wd 0.0000	time 8.7761 (8.7761)	loss 1.4423 (1.4423)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:17:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:13:14 lr 0.000032	 wd 0.0000	time 0.2301 (0.3310)	loss 1.3071 (1.4500)	grad_norm 4.2261 (6.4717)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:17:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:10:48 lr 0.000033	 wd 0.0000	time 0.2299 (0.2816)	loss 1.1693 (1.4368)	grad_norm 4.6719 (5.8437)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:18:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:09:43 lr 0.000033	 wd 0.0000	time 0.2305 (0.2651)	loss 1.1306 (1.4364)	grad_norm 6.9003 (5.7364)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:18:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:08:59 lr 0.000033	 wd 0.0000	time 0.2301 (0.2569)	loss 1.4701 (1.4256)	grad_norm 4.7583 (5.7083)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:18:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:08:24 lr 0.000034	 wd 0.0000	time 0.2277 (0.2519)	loss 1.4341 (1.4205)	grad_norm 4.1696 (5.5619)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:19:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:07:52 lr 0.000034	 wd 0.0000	time 0.2298 (0.2487)	loss 1.4692 (1.4153)	grad_norm 4.3961 (5.5781)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:19:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:07:23 lr 0.000034	 wd 0.0000	time 0.2303 (0.2464)	loss 1.3273 (1.4153)	grad_norm 5.0358 (5.6714)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:20:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:06:56 lr 0.000035	 wd 0.0000	time 0.2303 (0.2447)	loss 1.4330 (1.4159)	grad_norm 4.0338 (5.5968)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:20:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:06:29 lr 0.000035	 wd 0.0000	time 0.2326 (0.2434)	loss 1.5344 (1.4169)	grad_norm 10.6088 (5.5358)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:20:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:06:04 lr 0.000035	 wd 0.0000	time 0.2300 (0.2424)	loss 1.6123 (1.4176)	grad_norm 3.6016 (5.5082)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:21:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:05:38 lr 0.000036	 wd 0.0000	time 0.2296 (0.2415)	loss 1.6127 (1.4176)	grad_norm 7.9600 (5.5029)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:21:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:05:13 lr 0.000036	 wd 0.0000	time 0.2310 (0.2408)	loss 1.0939 (1.4166)	grad_norm 4.4913 (5.4976)	loss_scale 2048.0000 (1087.0941)	mem 8828MB
[2024-07-01 23:22:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:04:48 lr 0.000036	 wd 0.0000	time 0.2294 (0.2403)	loss 1.6014 (1.4153)	grad_norm 6.8253 (5.5380)	loss_scale 2048.0000 (1160.9531)	mem 8828MB
[2024-07-01 23:22:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:04:24 lr 0.000036	 wd 0.0000	time 0.2303 (0.2398)	loss 1.5130 (1.4153)	grad_norm 9.8700 (5.5211)	loss_scale 2048.0000 (1224.2684)	mem 8828MB
[2024-07-01 23:22:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:03:59 lr 0.000037	 wd 0.0000	time 0.2320 (0.2394)	loss 1.3021 (1.4183)	grad_norm 5.2571 (5.4658)	loss_scale 2048.0000 (1279.1472)	mem 8828MB
[2024-07-01 23:23:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:03:35 lr 0.000037	 wd 0.0000	time 0.2305 (0.2391)	loss 1.2352 (1.4175)	grad_norm 4.8319 (nan)	loss_scale 1024.0000 (1302.8657)	mem 8828MB
[2024-07-01 23:23:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:03:11 lr 0.000037	 wd 0.0000	time 0.2309 (0.2387)	loss 1.6034 (1.4169)	grad_norm 4.2412 (nan)	loss_scale 1024.0000 (1286.4715)	mem 8828MB
[2024-07-01 23:23:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:02:47 lr 0.000038	 wd 0.0000	time 0.2301 (0.2385)	loss 1.6079 (1.4165)	grad_norm 4.6452 (nan)	loss_scale 1024.0000 (1271.8978)	mem 8828MB
[2024-07-01 23:24:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:23 lr 0.000038	 wd 0.0000	time 0.2358 (0.2383)	loss 1.6017 (1.4163)	grad_norm 4.6964 (nan)	loss_scale 1024.0000 (1258.8574)	mem 8828MB
[2024-07-01 23:24:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:01:59 lr 0.000038	 wd 0.0000	time 0.2307 (0.2381)	loss 1.5901 (1.4163)	grad_norm 5.2797 (nan)	loss_scale 1024.0000 (1247.1204)	mem 8828MB
[2024-07-01 23:25:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:35 lr 0.000039	 wd 0.0000	time 0.2313 (0.2379)	loss 1.4101 (1.4162)	grad_norm 4.2448 (nan)	loss_scale 1024.0000 (1236.5007)	mem 8828MB
[2024-07-01 23:25:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:11 lr 0.000039	 wd 0.0000	time 0.2338 (0.2377)	loss 1.0363 (1.4132)	grad_norm 5.8726 (nan)	loss_scale 1024.0000 (1226.8460)	mem 8828MB
[2024-07-01 23:25:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:47 lr 0.000039	 wd 0.0000	time 0.2341 (0.2376)	loss 1.3478 (1.4140)	grad_norm 3.9551 (nan)	loss_scale 1024.0000 (1218.0304)	mem 8828MB
[2024-07-01 23:26:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:24 lr 0.000040	 wd 0.0000	time 0.2304 (0.2375)	loss 1.5055 (1.4133)	grad_norm 4.8263 (nan)	loss_scale 1024.0000 (1209.9492)	mem 8828MB
[2024-07-01 23:26:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.2296 (0.2373)	loss 0.9214 (1.4130)	grad_norm 4.4890 (nan)	loss_scale 1024.0000 (1202.5142)	mem 8828MB
[2024-07-01 23:26:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 4 training takes 0:09:56
[2024-07-01 23:26:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.667 (11.667)	Loss 0.4390 (0.4390)	Acc@1 91.602 (91.602)	Acc@5 98.242 (98.242)	Mem 8828MB
[2024-07-01 23:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.254 Acc@5 96.888
[2024-07-01 23:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.3%
[2024-07-01 23:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.25%
[2024-07-01 23:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:27:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:27:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:09:18 lr 0.000040	 wd 0.0000	time 10.2952 (10.2952)	loss 1.6175 (1.6175)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:27:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:13:19 lr 0.000040	 wd 0.0000	time 0.2287 (0.3330)	loss 1.3401 (1.4264)	grad_norm 5.0043 (5.0276)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:28:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:10:50 lr 0.000040	 wd 0.0000	time 0.2311 (0.2826)	loss 1.4874 (1.4401)	grad_norm 3.4112 (5.3904)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:28:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:09:45 lr 0.000040	 wd 0.0000	time 0.2300 (0.2657)	loss 1.7065 (1.4264)	grad_norm 4.2296 (5.3177)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:28:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:09:00 lr 0.000040	 wd 0.0000	time 0.2307 (0.2573)	loss 1.1782 (1.4275)	grad_norm 4.1401 (5.1985)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:29:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:08:25 lr 0.000040	 wd 0.0000	time 0.2292 (0.2523)	loss 1.6774 (1.4222)	grad_norm 4.4217 (5.1788)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:29:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:07:53 lr 0.000040	 wd 0.0000	time 0.2301 (0.2491)	loss 1.5888 (1.4252)	grad_norm 8.1364 (5.1698)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:30:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:07:24 lr 0.000040	 wd 0.0000	time 0.2299 (0.2467)	loss 1.5313 (1.4221)	grad_norm 6.9407 (5.1647)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:30:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:06:56 lr 0.000040	 wd 0.0000	time 0.2303 (0.2450)	loss 1.1274 (1.4157)	grad_norm 5.5542 (5.1075)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:30:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:06:30 lr 0.000040	 wd 0.0000	time 0.2302 (0.2436)	loss 0.8593 (1.4141)	grad_norm 4.2179 (5.2063)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:31:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:06:04 lr 0.000040	 wd 0.0000	time 0.2295 (0.2426)	loss 1.4205 (1.4169)	grad_norm 5.5051 (5.1832)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:31:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:05:38 lr 0.000040	 wd 0.0000	time 0.2291 (0.2417)	loss 1.6028 (1.4124)	grad_norm 4.3345 (5.2335)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:31:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:05:13 lr 0.000040	 wd 0.0000	time 0.2327 (0.2410)	loss 1.5496 (1.4117)	grad_norm 3.4080 (5.2624)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:32:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:04:49 lr 0.000040	 wd 0.0000	time 0.2317 (0.2405)	loss 1.5839 (1.4117)	grad_norm 4.5062 (5.2361)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:32:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:04:24 lr 0.000040	 wd 0.0000	time 0.2296 (0.2400)	loss 1.5533 (1.4109)	grad_norm 3.2262 (inf)	loss_scale 512.0000 (993.3019)	mem 8828MB
[2024-07-01 23:33:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:04:00 lr 0.000040	 wd 0.0000	time 0.2305 (0.2395)	loss 1.3341 (1.4109)	grad_norm 3.8787 (inf)	loss_scale 512.0000 (961.2365)	mem 8828MB
[2024-07-01 23:33:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:03:35 lr 0.000040	 wd 0.0000	time 0.2328 (0.2391)	loss 1.4923 (1.4118)	grad_norm 3.7976 (inf)	loss_scale 512.0000 (933.1768)	mem 8828MB
[2024-07-01 23:33:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:03:11 lr 0.000040	 wd 0.0000	time 0.2314 (0.2388)	loss 1.5318 (1.4115)	grad_norm 5.0322 (inf)	loss_scale 512.0000 (908.4162)	mem 8828MB
[2024-07-01 23:34:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:02:47 lr 0.000040	 wd 0.0000	time 0.2307 (0.2386)	loss 1.1103 (1.4102)	grad_norm 6.5172 (inf)	loss_scale 512.0000 (886.4053)	mem 8828MB
[2024-07-01 23:34:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:23 lr 0.000040	 wd 0.0000	time 0.2352 (0.2383)	loss 1.5926 (1.4124)	grad_norm 5.1248 (inf)	loss_scale 512.0000 (866.7102)	mem 8828MB
[2024-07-01 23:35:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:01:59 lr 0.000040	 wd 0.0000	time 0.2327 (0.2381)	loss 1.4547 (1.4138)	grad_norm 4.0469 (inf)	loss_scale 512.0000 (848.9835)	mem 8828MB
[2024-07-01 23:35:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:35 lr 0.000040	 wd 0.0000	time 0.2295 (0.2379)	loss 1.6488 (1.4146)	grad_norm 2.8256 (inf)	loss_scale 512.0000 (832.9443)	mem 8828MB
[2024-07-01 23:35:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:11 lr 0.000040	 wd 0.0000	time 0.2301 (0.2378)	loss 1.4225 (1.4124)	grad_norm 4.2294 (inf)	loss_scale 512.0000 (818.3626)	mem 8828MB
[2024-07-01 23:36:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:47 lr 0.000040	 wd 0.0000	time 0.2299 (0.2376)	loss 0.9927 (1.4119)	grad_norm 2.6944 (inf)	loss_scale 512.0000 (805.0482)	mem 8828MB
[2024-07-01 23:36:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:24 lr 0.000040	 wd 0.0000	time 0.2325 (0.2375)	loss 1.6282 (1.4119)	grad_norm 5.4905 (inf)	loss_scale 512.0000 (792.8430)	mem 8828MB
[2024-07-01 23:37:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.2294 (0.2373)	loss 1.5374 (1.4117)	grad_norm 4.2681 (inf)	loss_scale 512.0000 (781.6138)	mem 8828MB
[2024-07-01 23:37:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 5 training takes 0:09:56
[2024-07-01 23:37:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.164 (11.164)	Loss 0.4414 (0.4414)	Acc@1 91.602 (91.602)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-01 23:37:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.352 Acc@5 96.942
[2024-07-01 23:37:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.4%
[2024-07-01 23:37:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.35%
[2024-07-01 23:37:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:37:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:37:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][0/2502]	eta 6:58:59 lr 0.000040	 wd 0.0000	time 10.0476 (10.0476)	loss 1.7735 (1.7735)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:38:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:13:15 lr 0.000040	 wd 0.0000	time 0.2302 (0.3313)	loss 1.2768 (1.3942)	grad_norm 3.4785 (5.0083)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:38:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:10:49 lr 0.000040	 wd 0.0000	time 0.2290 (0.2819)	loss 1.5013 (1.4114)	grad_norm 6.8834 (4.9527)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:38:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:09:44 lr 0.000040	 wd 0.0000	time 0.2286 (0.2653)	loss 1.6014 (1.4172)	grad_norm 4.3424 (5.1133)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:39:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:09:00 lr 0.000040	 wd 0.0000	time 0.2288 (0.2570)	loss 1.4414 (1.4101)	grad_norm 3.8508 (5.1356)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:39:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:08:24 lr 0.000040	 wd 0.0000	time 0.2311 (0.2521)	loss 1.5825 (1.4079)	grad_norm 32.0673 (5.2270)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:39:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:07:53 lr 0.000040	 wd 0.0000	time 0.2296 (0.2488)	loss 1.5435 (1.4091)	grad_norm 6.1005 (5.1587)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:40:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:07:24 lr 0.000040	 wd 0.0000	time 0.2314 (0.2466)	loss 1.4367 (1.4095)	grad_norm 4.7338 (5.3640)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:40:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:06:56 lr 0.000040	 wd 0.0000	time 0.2345 (0.2448)	loss 1.1792 (1.4093)	grad_norm 5.3321 (5.3139)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:41:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:06:30 lr 0.000040	 wd 0.0000	time 0.2290 (0.2435)	loss 1.5983 (1.4111)	grad_norm 5.9531 (5.2664)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:41:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:06:04 lr 0.000040	 wd 0.0000	time 0.2293 (0.2425)	loss 1.3324 (1.4075)	grad_norm 4.2484 (5.2361)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:41:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:05:38 lr 0.000040	 wd 0.0000	time 0.2292 (0.2417)	loss 1.3496 (1.4050)	grad_norm 5.9764 (5.2037)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:42:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:05:13 lr 0.000040	 wd 0.0000	time 0.2305 (0.2410)	loss 1.1420 (1.4061)	grad_norm 3.5698 (5.1834)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:42:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:04:48 lr 0.000040	 wd 0.0000	time 0.2348 (0.2404)	loss 1.5713 (1.4048)	grad_norm 3.3704 (5.1770)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:43:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:04:24 lr 0.000040	 wd 0.0000	time 0.2371 (0.2399)	loss 1.8245 (1.4029)	grad_norm 4.2747 (5.1684)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:43:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:03:59 lr 0.000040	 wd 0.0000	time 0.2297 (0.2395)	loss 1.5148 (1.4050)	grad_norm 2.5823 (5.1256)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:43:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:03:35 lr 0.000040	 wd 0.0000	time 0.2290 (0.2392)	loss 1.4704 (1.4031)	grad_norm 4.4917 (5.1169)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:44:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:03:11 lr 0.000040	 wd 0.0000	time 0.2318 (0.2388)	loss 1.5750 (1.4030)	grad_norm 5.9180 (5.1027)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:44:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:02:47 lr 0.000040	 wd 0.0000	time 0.2274 (0.2386)	loss 1.1522 (1.4026)	grad_norm 71.4492 (5.1772)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:45:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:23 lr 0.000040	 wd 0.0000	time 0.2318 (0.2384)	loss 1.2361 (1.4025)	grad_norm 9.8702 (5.1833)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:45:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:01:59 lr 0.000039	 wd 0.0000	time 0.2301 (0.2382)	loss 1.1074 (1.4029)	grad_norm 3.6634 (5.1803)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:45:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:35 lr 0.000039	 wd 0.0000	time 0.2323 (0.2380)	loss 1.5509 (1.4020)	grad_norm 4.8595 (5.1597)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:46:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:11 lr 0.000039	 wd 0.0000	time 0.2335 (0.2378)	loss 1.2919 (1.4022)	grad_norm 2.6592 (5.1315)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:46:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:47 lr 0.000039	 wd 0.0000	time 0.2304 (0.2376)	loss 1.5384 (1.4028)	grad_norm 3.1238 (5.1136)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:46:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:24 lr 0.000039	 wd 0.0000	time 0.2321 (0.2375)	loss 1.1626 (1.4021)	grad_norm 3.7258 (5.1040)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:47:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.2279 (0.2373)	loss 1.0442 (1.4026)	grad_norm 5.3407 (5.0922)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:47:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 6 training takes 0:09:56
[2024-07-01 23:47:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.756 (10.756)	Loss 0.4343 (0.4343)	Acc@1 91.797 (91.797)	Acc@5 98.242 (98.242)	Mem 8828MB
[2024-07-01 23:47:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.418 Acc@5 96.940
[2024-07-01 23:47:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.4%
[2024-07-01 23:47:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.42%
[2024-07-01 23:47:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:47:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:48:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:41:34 lr 0.000039	 wd 0.0000	time 11.0690 (11.0690)	loss 1.7309 (1.7309)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:48:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:13:35 lr 0.000039	 wd 0.0000	time 0.2276 (0.3397)	loss 1.3726 (1.4158)	grad_norm 4.5317 (5.3011)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:48:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:10:58 lr 0.000039	 wd 0.0000	time 0.2285 (0.2862)	loss 1.6085 (1.3944)	grad_norm 4.0281 (5.0607)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:49:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:09:50 lr 0.000039	 wd 0.0000	time 0.2309 (0.2682)	loss 1.4043 (1.3957)	grad_norm 4.2085 (4.8309)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-01 23:49:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:09:04 lr 0.000039	 wd 0.0000	time 0.2342 (0.2592)	loss 1.4522 (1.3995)	grad_norm 3.5587 (4.8325)	loss_scale 1024.0000 (624.3591)	mem 8828MB
[2024-07-01 23:49:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:08:28 lr 0.000039	 wd 0.0000	time 0.2304 (0.2538)	loss 1.5710 (1.3955)	grad_norm 3.9214 (4.8625)	loss_scale 1024.0000 (704.1277)	mem 8828MB
[2024-07-01 23:50:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:07:56 lr 0.000039	 wd 0.0000	time 0.2294 (0.2503)	loss 1.4107 (1.3987)	grad_norm 3.0515 (4.8560)	loss_scale 1024.0000 (757.3511)	mem 8828MB
[2024-07-01 23:50:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:07:26 lr 0.000039	 wd 0.0000	time 0.2311 (0.2478)	loss 1.3952 (1.4005)	grad_norm 3.0946 (4.9645)	loss_scale 1024.0000 (795.3894)	mem 8828MB
[2024-07-01 23:51:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:06:58 lr 0.000039	 wd 0.0000	time 0.2298 (0.2459)	loss 1.0481 (1.4023)	grad_norm 7.0663 (4.9797)	loss_scale 1024.0000 (823.9301)	mem 8828MB
[2024-07-01 23:51:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:06:31 lr 0.000039	 wd 0.0000	time 0.2299 (0.2445)	loss 1.2168 (1.4048)	grad_norm 3.8691 (4.9695)	loss_scale 1024.0000 (846.1354)	mem 8828MB
[2024-07-01 23:51:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:06:05 lr 0.000039	 wd 0.0000	time 0.2301 (0.2433)	loss 1.2770 (1.4015)	grad_norm 3.8947 (5.1033)	loss_scale 1024.0000 (863.9041)	mem 8828MB
[2024-07-01 23:52:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:05:39 lr 0.000039	 wd 0.0000	time 0.2296 (0.2424)	loss 1.3468 (1.3998)	grad_norm 4.0374 (5.0765)	loss_scale 1024.0000 (878.4450)	mem 8828MB
[2024-07-01 23:52:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:05:14 lr 0.000039	 wd 0.0000	time 0.2320 (0.2417)	loss 1.3803 (1.3967)	grad_norm 3.9005 (5.0874)	loss_scale 1024.0000 (890.5645)	mem 8828MB
[2024-07-01 23:53:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:04:49 lr 0.000039	 wd 0.0000	time 0.2296 (0.2411)	loss 1.4753 (1.3976)	grad_norm 3.1823 (5.0431)	loss_scale 1024.0000 (900.8209)	mem 8828MB
[2024-07-01 23:53:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:04:25 lr 0.000039	 wd 0.0000	time 0.2306 (0.2405)	loss 1.5831 (1.3980)	grad_norm 2.6101 (5.0295)	loss_scale 1024.0000 (909.6131)	mem 8828MB
[2024-07-01 23:53:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:04:00 lr 0.000039	 wd 0.0000	time 0.2298 (0.2401)	loss 1.4893 (1.3970)	grad_norm 5.3373 (5.0042)	loss_scale 1024.0000 (917.2338)	mem 8828MB
[2024-07-01 23:54:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:03:36 lr 0.000039	 wd 0.0000	time 0.2302 (0.2397)	loss 1.5049 (1.3982)	grad_norm 3.8624 (5.0384)	loss_scale 1024.0000 (923.9026)	mem 8828MB
[2024-07-01 23:54:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:03:11 lr 0.000039	 wd 0.0000	time 0.2325 (0.2394)	loss 1.5627 (1.3982)	grad_norm 6.6424 (5.0246)	loss_scale 1024.0000 (929.7872)	mem 8828MB
[2024-07-01 23:54:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:02:47 lr 0.000039	 wd 0.0000	time 0.2324 (0.2391)	loss 1.3701 (1.3977)	grad_norm 5.9102 (5.0274)	loss_scale 1024.0000 (935.0183)	mem 8828MB
[2024-07-01 23:55:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:23 lr 0.000039	 wd 0.0000	time 0.2305 (0.2388)	loss 1.3949 (1.3969)	grad_norm 4.2670 (5.0326)	loss_scale 1024.0000 (939.6991)	mem 8828MB
[2024-07-01 23:55:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:01:59 lr 0.000039	 wd 0.0000	time 0.2329 (0.2386)	loss 1.5081 (1.3977)	grad_norm 6.2281 (5.0506)	loss_scale 1024.0000 (943.9120)	mem 8828MB
[2024-07-01 23:56:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:35 lr 0.000039	 wd 0.0000	time 0.2296 (0.2384)	loss 1.6263 (1.3995)	grad_norm 11.3802 (5.1072)	loss_scale 1024.0000 (947.7239)	mem 8828MB
[2024-07-01 23:56:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:11 lr 0.000039	 wd 0.0000	time 0.2321 (0.2382)	loss 1.2915 (1.3979)	grad_norm 4.1737 (5.0983)	loss_scale 1024.0000 (951.1895)	mem 8828MB
[2024-07-01 23:56:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:48 lr 0.000039	 wd 0.0000	time 0.2341 (0.2380)	loss 1.5734 (1.3984)	grad_norm 3.6327 (5.0949)	loss_scale 1024.0000 (954.3538)	mem 8828MB
[2024-07-01 23:57:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:24 lr 0.000039	 wd 0.0000	time 0.2300 (0.2379)	loss 1.3993 (1.3980)	grad_norm 3.8713 (5.0937)	loss_scale 1024.0000 (957.2545)	mem 8828MB
[2024-07-01 23:57:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.2298 (0.2377)	loss 1.3319 (1.3980)	grad_norm 7.5002 (5.0893)	loss_scale 1024.0000 (959.9232)	mem 8828MB
[2024-07-01 23:57:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 7 training takes 0:09:57
[2024-07-01 23:57:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.831 (10.831)	Loss 0.4424 (0.4424)	Acc@1 91.602 (91.602)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-01 23:58:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.434 Acc@5 96.998
[2024-07-01 23:58:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.4%
[2024-07-01 23:58:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.43%
[2024-07-01 23:58:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-01 23:58:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-01 23:58:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:03:21 lr 0.000039	 wd 0.0000	time 10.1526 (10.1526)	loss 1.5861 (1.5861)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:58:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:13:19 lr 0.000039	 wd 0.0000	time 0.2290 (0.3330)	loss 1.4983 (1.4285)	grad_norm 23.8492 (5.2519)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:59:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:10:50 lr 0.000039	 wd 0.0000	time 0.2325 (0.2828)	loss 1.4904 (1.4019)	grad_norm 4.1609 (4.8993)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:59:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:09:45 lr 0.000038	 wd 0.0000	time 0.2295 (0.2659)	loss 1.4020 (1.3968)	grad_norm 4.0579 (4.9027)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-01 23:59:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:09:01 lr 0.000038	 wd 0.0000	time 0.2298 (0.2575)	loss 1.5153 (1.3941)	grad_norm 5.2089 (4.9710)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:00:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:08:25 lr 0.000038	 wd 0.0000	time 0.2296 (0.2524)	loss 1.5301 (1.4006)	grad_norm 5.2552 (4.9883)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:00:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:07:53 lr 0.000038	 wd 0.0000	time 0.2301 (0.2491)	loss 1.6129 (1.4049)	grad_norm 2.8024 (4.9216)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:01:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:07:24 lr 0.000038	 wd 0.0000	time 0.2300 (0.2468)	loss 1.5651 (1.4051)	grad_norm 5.2714 (4.9441)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:01:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:06:57 lr 0.000038	 wd 0.0000	time 0.2298 (0.2450)	loss 1.5054 (1.4042)	grad_norm 5.9882 (4.9080)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:01:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:06:30 lr 0.000038	 wd 0.0000	time 0.2264 (0.2437)	loss 1.5834 (1.4050)	grad_norm 3.2434 (4.9395)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:02:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:06:04 lr 0.000038	 wd 0.0000	time 0.2320 (0.2426)	loss 1.4690 (1.4027)	grad_norm 3.1724 (4.9134)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:02:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:05:38 lr 0.000038	 wd 0.0000	time 0.2304 (0.2418)	loss 1.5094 (1.3989)	grad_norm 4.3875 (4.9284)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:02:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:05:13 lr 0.000038	 wd 0.0000	time 0.2344 (0.2411)	loss 1.4601 (1.3979)	grad_norm 5.0657 (4.8979)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:03:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:04:49 lr 0.000038	 wd 0.0000	time 0.2280 (0.2405)	loss 1.1084 (1.3957)	grad_norm 3.4657 (4.8708)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:03:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:04:24 lr 0.000038	 wd 0.0000	time 0.2297 (0.2400)	loss 0.9874 (1.3967)	grad_norm 3.7373 (4.8521)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:04:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:04:00 lr 0.000038	 wd 0.0000	time 0.2321 (0.2396)	loss 0.9702 (1.3930)	grad_norm 4.1665 (4.8297)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:04:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:03:35 lr 0.000038	 wd 0.0000	time 0.2309 (0.2393)	loss 1.3017 (1.3903)	grad_norm 7.1296 (4.9671)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:04:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:03:11 lr 0.000038	 wd 0.0000	time 0.2352 (0.2390)	loss 1.2400 (1.3916)	grad_norm 4.2219 (inf)	loss_scale 512.0000 (1007.7460)	mem 8828MB
[2024-07-02 00:05:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:02:47 lr 0.000038	 wd 0.0000	time 0.2308 (0.2387)	loss 1.4937 (1.3934)	grad_norm 3.3466 (inf)	loss_scale 512.0000 (980.2199)	mem 8828MB
[2024-07-02 00:05:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:23 lr 0.000038	 wd 0.0000	time 0.2307 (0.2384)	loss 1.3430 (1.3939)	grad_norm 10.6368 (inf)	loss_scale 512.0000 (955.5897)	mem 8828MB
[2024-07-02 00:06:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:01:59 lr 0.000038	 wd 0.0000	time 0.2381 (0.2382)	loss 1.7724 (1.3958)	grad_norm 3.2921 (inf)	loss_scale 512.0000 (933.4213)	mem 8828MB
[2024-07-02 00:06:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:35 lr 0.000038	 wd 0.0000	time 0.2298 (0.2381)	loss 1.4151 (1.3957)	grad_norm 3.6010 (inf)	loss_scale 512.0000 (913.3632)	mem 8828MB
[2024-07-02 00:06:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:11 lr 0.000038	 wd 0.0000	time 0.2303 (0.2379)	loss 1.5352 (1.3946)	grad_norm 3.9378 (inf)	loss_scale 512.0000 (895.1277)	mem 8828MB
[2024-07-02 00:07:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:48 lr 0.000038	 wd 0.0000	time 0.2309 (0.2378)	loss 1.4129 (1.3952)	grad_norm 3.9015 (inf)	loss_scale 512.0000 (878.4772)	mem 8828MB
[2024-07-02 00:07:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:24 lr 0.000038	 wd 0.0000	time 0.2340 (0.2376)	loss 1.4499 (1.3969)	grad_norm 3.8069 (inf)	loss_scale 512.0000 (863.2137)	mem 8828MB
[2024-07-02 00:08:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000038	 wd 0.0000	time 0.2298 (0.2374)	loss 1.6467 (1.3981)	grad_norm 3.9247 (inf)	loss_scale 512.0000 (849.1707)	mem 8828MB
[2024-07-02 00:08:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 8 training takes 0:09:56
[2024-07-02 00:08:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.994 (10.994)	Loss 0.4285 (0.4285)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 00:08:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.510 Acc@5 97.010
[2024-07-02 00:08:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.5%
[2024-07-02 00:08:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.51%
[2024-07-02 00:08:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 00:08:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 00:08:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][0/2502]	eta 6:46:05 lr 0.000038	 wd 0.0000	time 9.7384 (9.7384)	loss 1.4219 (1.4219)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:09:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:13:14 lr 0.000038	 wd 0.0000	time 0.2286 (0.3308)	loss 1.3260 (1.3712)	grad_norm 6.2998 (4.6917)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:09:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:10:48 lr 0.000037	 wd 0.0000	time 0.2280 (0.2818)	loss 1.5564 (1.3772)	grad_norm 4.3076 (4.9125)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:09:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:09:44 lr 0.000037	 wd 0.0000	time 0.2299 (0.2653)	loss 1.4488 (1.3879)	grad_norm 3.3928 (5.0103)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:10:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:09:00 lr 0.000037	 wd 0.0000	time 0.2303 (0.2571)	loss 1.5907 (1.3828)	grad_norm 4.4991 (4.9395)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:10:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:08:24 lr 0.000037	 wd 0.0000	time 0.2303 (0.2522)	loss 1.3763 (1.3875)	grad_norm 3.7490 (4.9702)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:11:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:07:53 lr 0.000037	 wd 0.0000	time 0.2351 (0.2490)	loss 1.4666 (1.3814)	grad_norm 4.2880 (4.8540)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:11:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:07:24 lr 0.000037	 wd 0.0000	time 0.2305 (0.2466)	loss 1.5665 (1.3815)	grad_norm 2.5345 (4.7942)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:11:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:06:56 lr 0.000037	 wd 0.0000	time 0.2305 (0.2449)	loss 1.4483 (1.3869)	grad_norm 4.2820 (4.7811)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:12:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:06:30 lr 0.000037	 wd 0.0000	time 0.2299 (0.2436)	loss 1.5851 (1.3838)	grad_norm 6.5383 (4.8084)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:12:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:06:04 lr 0.000037	 wd 0.0000	time 0.2357 (0.2426)	loss 1.5759 (1.3860)	grad_norm 3.0981 (4.7944)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:12:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:05:38 lr 0.000037	 wd 0.0000	time 0.2312 (0.2417)	loss 1.3745 (1.3897)	grad_norm 5.0094 (4.7993)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:13:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:05:13 lr 0.000037	 wd 0.0000	time 0.2289 (0.2411)	loss 1.6839 (1.3921)	grad_norm 6.6427 (4.8385)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:13:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:04:49 lr 0.000037	 wd 0.0000	time 0.2342 (0.2405)	loss 1.0103 (1.3896)	grad_norm 4.0488 (4.8203)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:14:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:04:24 lr 0.000037	 wd 0.0000	time 0.2289 (0.2400)	loss 1.4137 (1.3918)	grad_norm 4.1577 (4.8335)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:14:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:04:00 lr 0.000037	 wd 0.0000	time 0.2306 (0.2396)	loss 1.5849 (1.3942)	grad_norm 6.1033 (4.8194)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:14:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:03:35 lr 0.000037	 wd 0.0000	time 0.2364 (0.2392)	loss 1.0404 (1.3939)	grad_norm 4.6237 (4.8269)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:15:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:03:11 lr 0.000037	 wd 0.0000	time 0.2312 (0.2389)	loss 1.0726 (1.3952)	grad_norm 3.4297 (4.8477)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:15:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:02:47 lr 0.000037	 wd 0.0000	time 0.2327 (0.2386)	loss 1.5355 (1.3958)	grad_norm 5.2114 (4.9206)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:16:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:23 lr 0.000037	 wd 0.0000	time 0.2341 (0.2384)	loss 1.4101 (1.3949)	grad_norm 14.8043 (4.9321)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:16:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:01:59 lr 0.000037	 wd 0.0000	time 0.2330 (0.2382)	loss 1.5763 (1.3940)	grad_norm 4.2876 (4.9260)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:16:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:35 lr 0.000036	 wd 0.0000	time 0.2323 (0.2380)	loss 1.2193 (1.3947)	grad_norm 3.4185 (4.9407)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:17:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:11 lr 0.000036	 wd 0.0000	time 0.2262 (0.2379)	loss 1.4993 (1.3960)	grad_norm 50.7183 (4.9736)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:17:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:48 lr 0.000036	 wd 0.0000	time 0.2321 (0.2377)	loss 1.4505 (1.3965)	grad_norm 4.1374 (4.9594)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:18:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:24 lr 0.000036	 wd 0.0000	time 0.2318 (0.2376)	loss 1.2887 (1.3964)	grad_norm 5.0896 (4.9512)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:18:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000036	 wd 0.0000	time 0.2297 (0.2374)	loss 0.9553 (1.3972)	grad_norm 2.9519 (4.9504)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:18:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 9 training takes 0:09:56
[2024-07-02 00:18:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.453 (11.453)	Loss 0.4268 (0.4268)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 00:18:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.520 Acc@5 97.040
[2024-07-02 00:18:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.5%
[2024-07-02 00:18:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.52%
[2024-07-02 00:18:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 00:18:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 00:19:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:21:23 lr 0.000036	 wd 0.0000	time 10.5848 (10.5848)	loss 1.3265 (1.3265)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:19:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:13:25 lr 0.000036	 wd 0.0000	time 0.2299 (0.3353)	loss 1.2461 (1.3617)	grad_norm 3.0515 (4.9221)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:19:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:10:53 lr 0.000036	 wd 0.0000	time 0.2286 (0.2839)	loss 1.4131 (1.3888)	grad_norm 5.8112 (4.9381)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:20:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:09:47 lr 0.000036	 wd 0.0000	time 0.2286 (0.2667)	loss 0.9261 (1.3912)	grad_norm 4.0788 (4.9122)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:20:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:09:02 lr 0.000036	 wd 0.0000	time 0.2288 (0.2581)	loss 1.6283 (1.3960)	grad_norm 3.8571 (5.2433)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:20:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:08:26 lr 0.000036	 wd 0.0000	time 0.2290 (0.2529)	loss 1.5263 (1.3925)	grad_norm 4.6445 (5.2620)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:21:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:07:54 lr 0.000036	 wd 0.0000	time 0.2293 (0.2496)	loss 1.6297 (1.3915)	grad_norm 5.3423 (5.2010)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:21:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:07:25 lr 0.000036	 wd 0.0000	time 0.2289 (0.2472)	loss 1.4797 (1.3888)	grad_norm 2.8735 (5.1468)	loss_scale 1024.0000 (554.3623)	mem 8828MB
[2024-07-02 00:22:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:06:57 lr 0.000036	 wd 0.0000	time 0.2300 (0.2454)	loss 1.3316 (1.3899)	grad_norm 3.3121 (5.0973)	loss_scale 1024.0000 (612.9938)	mem 8828MB
[2024-07-02 00:22:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:06:30 lr 0.000036	 wd 0.0000	time 0.2299 (0.2441)	loss 1.5472 (1.3891)	grad_norm 9.9580 (5.0613)	loss_scale 1024.0000 (658.6104)	mem 8828MB
[2024-07-02 00:22:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:06:04 lr 0.000036	 wd 0.0000	time 0.2315 (0.2430)	loss 1.6160 (1.3872)	grad_norm 6.6062 (5.0381)	loss_scale 1024.0000 (695.1129)	mem 8828MB
[2024-07-02 00:23:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:05:39 lr 0.000036	 wd 0.0000	time 0.2277 (0.2421)	loss 1.3911 (1.3847)	grad_norm 6.3908 (5.0548)	loss_scale 1024.0000 (724.9846)	mem 8828MB
[2024-07-02 00:23:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:05:14 lr 0.000035	 wd 0.0000	time 0.2314 (0.2414)	loss 1.2801 (1.3854)	grad_norm 3.4846 (5.0090)	loss_scale 1024.0000 (749.8818)	mem 8828MB
[2024-07-02 00:24:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:04:49 lr 0.000035	 wd 0.0000	time 0.2306 (0.2408)	loss 1.5955 (1.3879)	grad_norm 4.0453 (5.0161)	loss_scale 1024.0000 (770.9516)	mem 8828MB
[2024-07-02 00:24:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:04:24 lr 0.000035	 wd 0.0000	time 0.2312 (0.2403)	loss 1.5001 (1.3898)	grad_norm 4.1232 (5.0318)	loss_scale 1024.0000 (789.0136)	mem 8828MB
[2024-07-02 00:24:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:04:00 lr 0.000035	 wd 0.0000	time 0.2360 (0.2399)	loss 1.3192 (1.3901)	grad_norm 8.2502 (5.0181)	loss_scale 1024.0000 (804.6689)	mem 8828MB
[2024-07-02 00:25:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:03:36 lr 0.000035	 wd 0.0000	time 0.2316 (0.2395)	loss 1.2275 (1.3913)	grad_norm 5.7108 (4.9777)	loss_scale 1024.0000 (818.3685)	mem 8828MB
[2024-07-02 00:25:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:03:11 lr 0.000035	 wd 0.0000	time 0.2323 (0.2392)	loss 1.4588 (1.3921)	grad_norm 4.1472 (4.9795)	loss_scale 1024.0000 (830.4574)	mem 8828MB
[2024-07-02 00:26:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:02:47 lr 0.000035	 wd 0.0000	time 0.2299 (0.2389)	loss 1.3497 (1.3920)	grad_norm 6.4306 (4.9709)	loss_scale 1024.0000 (841.2038)	mem 8828MB
[2024-07-02 00:26:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:02:23 lr 0.000035	 wd 0.0000	time 0.2321 (0.2387)	loss 1.4940 (1.3927)	grad_norm 4.9750 (4.9684)	loss_scale 1024.0000 (850.8196)	mem 8828MB
[2024-07-02 00:26:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:01:59 lr 0.000035	 wd 0.0000	time 0.2304 (0.2385)	loss 1.3461 (1.3928)	grad_norm 4.7698 (4.9661)	loss_scale 1024.0000 (859.4743)	mem 8828MB
[2024-07-02 00:27:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:35 lr 0.000035	 wd 0.0000	time 0.2296 (0.2383)	loss 1.5494 (1.3943)	grad_norm 6.4258 (4.9383)	loss_scale 1024.0000 (867.3051)	mem 8828MB
[2024-07-02 00:27:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:11 lr 0.000035	 wd 0.0000	time 0.2348 (0.2381)	loss 1.0205 (1.3948)	grad_norm 5.8478 (4.9445)	loss_scale 1024.0000 (874.4244)	mem 8828MB
[2024-07-02 00:27:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:48 lr 0.000035	 wd 0.0000	time 0.2305 (0.2379)	loss 1.4819 (1.3942)	grad_norm 4.8675 (4.9499)	loss_scale 1024.0000 (880.9248)	mem 8828MB
[2024-07-02 00:28:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:24 lr 0.000035	 wd 0.0000	time 0.2309 (0.2378)	loss 1.4822 (1.3942)	grad_norm 5.4548 (4.9493)	loss_scale 1024.0000 (886.8838)	mem 8828MB
[2024-07-02 00:28:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.2298 (0.2376)	loss 1.5505 (1.3938)	grad_norm 6.6322 (4.9321)	loss_scale 1024.0000 (892.3663)	mem 8828MB
[2024-07-02 00:28:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 10 training takes 0:09:56
[2024-07-02 00:28:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.503 (11.503)	Loss 0.4248 (0.4248)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 00:29:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.506 Acc@5 97.036
[2024-07-02 00:29:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.5%
[2024-07-02 00:29:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.52%
[2024-07-02 00:29:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:49:16 lr 0.000035	 wd 0.0000	time 11.2537 (11.2537)	loss 1.6166 (1.6166)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:29:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:13:40 lr 0.000035	 wd 0.0000	time 0.2313 (0.3417)	loss 1.3756 (1.3911)	grad_norm 5.5632 (4.7261)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:30:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:11:00 lr 0.000034	 wd 0.0000	time 0.2281 (0.2871)	loss 1.5890 (1.3948)	grad_norm 3.2666 (5.2359)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:30:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:09:51 lr 0.000034	 wd 0.0000	time 0.2293 (0.2688)	loss 1.2953 (1.3943)	grad_norm 4.4858 (5.1772)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:30:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:09:05 lr 0.000034	 wd 0.0000	time 0.2310 (0.2598)	loss 1.5376 (1.3992)	grad_norm 4.4849 (5.1663)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:31:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:08:28 lr 0.000034	 wd 0.0000	time 0.2321 (0.2542)	loss 1.3484 (1.3922)	grad_norm 3.3968 (5.1224)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:31:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:07:56 lr 0.000034	 wd 0.0000	time 0.2297 (0.2506)	loss 1.4572 (1.3897)	grad_norm 3.4298 (5.0129)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:32:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:07:27 lr 0.000034	 wd 0.0000	time 0.2308 (0.2481)	loss 1.4084 (1.3896)	grad_norm 3.6716 (4.9768)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:32:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:06:58 lr 0.000034	 wd 0.0000	time 0.2293 (0.2462)	loss 1.7011 (1.3926)	grad_norm 3.2443 (4.9258)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:32:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:06:31 lr 0.000034	 wd 0.0000	time 0.2316 (0.2447)	loss 1.5406 (1.3931)	grad_norm 7.2100 (4.9283)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:33:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:06:05 lr 0.000034	 wd 0.0000	time 0.2308 (0.2435)	loss 1.5201 (1.3931)	grad_norm 4.4582 (4.9043)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:33:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:05:40 lr 0.000034	 wd 0.0000	time 0.2285 (0.2426)	loss 1.4634 (1.3950)	grad_norm 4.2872 (4.9489)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:34:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:05:14 lr 0.000034	 wd 0.0000	time 0.2309 (0.2418)	loss 1.6602 (1.3956)	grad_norm 3.6674 (4.9510)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:34:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:04:49 lr 0.000034	 wd 0.0000	time 0.2313 (0.2412)	loss 1.0725 (1.3939)	grad_norm 2.9808 (4.9333)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:34:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:04:25 lr 0.000034	 wd 0.0000	time 0.2316 (0.2406)	loss 0.9886 (1.3939)	grad_norm 6.7795 (4.9055)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:35:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:04:00 lr 0.000034	 wd 0.0000	time 0.2329 (0.2402)	loss 1.4789 (1.3931)	grad_norm 4.0648 (4.9221)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:35:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:03:36 lr 0.000034	 wd 0.0000	time 0.2308 (0.2398)	loss 1.2935 (1.3942)	grad_norm 3.6982 (4.9152)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:35:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:03:12 lr 0.000033	 wd 0.0000	time 0.2325 (0.2394)	loss 1.3956 (1.3927)	grad_norm 3.2018 (4.9353)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 00:36:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:02:47 lr 0.000033	 wd 0.0000	time 0.2307 (0.2391)	loss 1.3680 (1.3944)	grad_norm 5.5124 (nan)	loss_scale 512.0000 (1006.9428)	mem 8828MB
[2024-07-02 00:36:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:23 lr 0.000033	 wd 0.0000	time 0.2306 (0.2389)	loss 1.5899 (1.3943)	grad_norm 5.5329 (nan)	loss_scale 512.0000 (980.9069)	mem 8828MB
[2024-07-02 00:37:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:01:59 lr 0.000033	 wd 0.0000	time 0.2311 (0.2386)	loss 1.2293 (1.3940)	grad_norm 2.9496 (nan)	loss_scale 512.0000 (957.4733)	mem 8828MB
[2024-07-02 00:37:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:35 lr 0.000033	 wd 0.0000	time 0.2294 (0.2384)	loss 1.4287 (1.3932)	grad_norm 4.2817 (nan)	loss_scale 512.0000 (936.2703)	mem 8828MB
[2024-07-02 00:37:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:11 lr 0.000033	 wd 0.0000	time 0.2295 (0.2382)	loss 1.4138 (1.3947)	grad_norm 3.9698 (nan)	loss_scale 512.0000 (916.9941)	mem 8828MB
[2024-07-02 00:38:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:48 lr 0.000033	 wd 0.0000	time 0.2304 (0.2380)	loss 1.5860 (1.3941)	grad_norm 4.6071 (nan)	loss_scale 512.0000 (899.3933)	mem 8828MB
[2024-07-02 00:38:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:24 lr 0.000033	 wd 0.0000	time 0.2315 (0.2379)	loss 1.3860 (1.3934)	grad_norm 4.8121 (nan)	loss_scale 512.0000 (883.2586)	mem 8828MB
[2024-07-02 00:39:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000033	 wd 0.0000	time 0.2307 (0.2377)	loss 1.4648 (1.3933)	grad_norm 3.8980 (nan)	loss_scale 512.0000 (868.4142)	mem 8828MB
[2024-07-02 00:39:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 11 training takes 0:09:57
[2024-07-02 00:39:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.923 (10.923)	Loss 0.4238 (0.4238)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 00:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.572 Acc@5 97.046
[2024-07-02 00:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-07-02 00:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.57%
[2024-07-02 00:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 00:39:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 00:39:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][0/2502]	eta 6:47:11 lr 0.000033	 wd 0.0000	time 9.7648 (9.7648)	loss 1.6593 (1.6593)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:40:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:13:32 lr 0.000033	 wd 0.0000	time 0.2318 (0.3382)	loss 1.5604 (1.3925)	grad_norm 4.1791 (4.6919)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:40:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:10:56 lr 0.000033	 wd 0.0000	time 0.2350 (0.2854)	loss 0.9187 (1.3964)	grad_norm 3.9146 (4.7580)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:40:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:09:49 lr 0.000033	 wd 0.0000	time 0.2291 (0.2676)	loss 1.3657 (1.3974)	grad_norm 3.9392 (4.7079)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:41:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:09:03 lr 0.000033	 wd 0.0000	time 0.2288 (0.2587)	loss 1.3568 (1.3902)	grad_norm 4.4608 (4.8026)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:41:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:08:27 lr 0.000032	 wd 0.0000	time 0.2298 (0.2535)	loss 1.1300 (1.3916)	grad_norm 3.2152 (4.8718)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:42:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:07:55 lr 0.000032	 wd 0.0000	time 0.2298 (0.2500)	loss 1.4084 (1.3888)	grad_norm 3.4047 (4.8044)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:42:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:07:26 lr 0.000032	 wd 0.0000	time 0.2294 (0.2476)	loss 1.4662 (1.3882)	grad_norm 2.7778 (4.8664)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:42:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:06:58 lr 0.000032	 wd 0.0000	time 0.2286 (0.2457)	loss 1.7138 (1.3976)	grad_norm 3.7734 (4.9083)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 00:43:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:06:31 lr 0.000032	 wd 0.0000	time 0.2309 (0.2443)	loss 1.0982 (1.3993)	grad_norm 3.2316 (nan)	loss_scale 256.0000 (489.2697)	mem 8828MB
[2024-07-02 00:43:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:06:05 lr 0.000032	 wd 0.0000	time 0.2323 (0.2432)	loss 1.4720 (1.3976)	grad_norm 3.8968 (nan)	loss_scale 256.0000 (465.9660)	mem 8828MB
[2024-07-02 00:43:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:05:39 lr 0.000032	 wd 0.0000	time 0.2298 (0.2423)	loss 1.5598 (1.4028)	grad_norm 3.5581 (nan)	loss_scale 256.0000 (446.8955)	mem 8828MB
[2024-07-02 00:44:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:05:14 lr 0.000032	 wd 0.0000	time 0.2313 (0.2415)	loss 1.6677 (1.4011)	grad_norm 3.6299 (nan)	loss_scale 256.0000 (431.0008)	mem 8828MB
[2024-07-02 00:44:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:04:49 lr 0.000032	 wd 0.0000	time 0.2305 (0.2409)	loss 1.5938 (1.4010)	grad_norm 6.5284 (nan)	loss_scale 256.0000 (417.5496)	mem 8828MB
[2024-07-02 00:45:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:04:24 lr 0.000032	 wd 0.0000	time 0.2299 (0.2404)	loss 1.6159 (1.4009)	grad_norm 5.2930 (nan)	loss_scale 256.0000 (406.0186)	mem 8828MB
[2024-07-02 00:45:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:04:00 lr 0.000032	 wd 0.0000	time 0.2321 (0.2400)	loss 1.5346 (1.4018)	grad_norm 3.9683 (nan)	loss_scale 256.0000 (396.0240)	mem 8828MB
[2024-07-02 00:45:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:03:36 lr 0.000032	 wd 0.0000	time 0.2332 (0.2396)	loss 1.5415 (1.4001)	grad_norm 4.3845 (nan)	loss_scale 256.0000 (387.2780)	mem 8828MB
[2024-07-02 00:46:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:03:11 lr 0.000031	 wd 0.0000	time 0.2308 (0.2393)	loss 1.4617 (1.4012)	grad_norm 3.8787 (nan)	loss_scale 256.0000 (379.5603)	mem 8828MB
[2024-07-02 00:46:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:02:47 lr 0.000031	 wd 0.0000	time 0.2312 (0.2390)	loss 1.3992 (1.4021)	grad_norm 4.2389 (nan)	loss_scale 256.0000 (372.6996)	mem 8828MB
[2024-07-02 00:47:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:23 lr 0.000031	 wd 0.0000	time 0.2341 (0.2388)	loss 1.3135 (1.4019)	grad_norm 3.4832 (nan)	loss_scale 256.0000 (366.5608)	mem 8828MB
[2024-07-02 00:47:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:01:59 lr 0.000031	 wd 0.0000	time 0.2308 (0.2386)	loss 1.4207 (1.4013)	grad_norm 4.6574 (nan)	loss_scale 256.0000 (361.0355)	mem 8828MB
[2024-07-02 00:47:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:35 lr 0.000031	 wd 0.0000	time 0.2347 (0.2384)	loss 1.5482 (1.4017)	grad_norm 5.2572 (nan)	loss_scale 256.0000 (356.0362)	mem 8828MB
[2024-07-02 00:48:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:11 lr 0.000031	 wd 0.0000	time 0.2318 (0.2382)	loss 1.5372 (1.4025)	grad_norm 3.6178 (nan)	loss_scale 256.0000 (351.4911)	mem 8828MB
[2024-07-02 00:48:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:48 lr 0.000031	 wd 0.0000	time 0.2315 (0.2380)	loss 1.4133 (1.4022)	grad_norm 6.2957 (nan)	loss_scale 256.0000 (347.3412)	mem 8828MB
[2024-07-02 00:49:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:24 lr 0.000031	 wd 0.0000	time 0.2366 (0.2379)	loss 1.5836 (1.4018)	grad_norm 3.9945 (nan)	loss_scale 256.0000 (343.5369)	mem 8828MB
[2024-07-02 00:49:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000031	 wd 0.0000	time 0.2299 (0.2377)	loss 1.1458 (1.4004)	grad_norm 3.8841 (nan)	loss_scale 256.0000 (340.0368)	mem 8828MB
[2024-07-02 00:49:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 12 training takes 0:09:57
[2024-07-02 00:49:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.655 (11.655)	Loss 0.4258 (0.4258)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 00:49:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.600 Acc@5 97.056
[2024-07-02 00:49:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-07-02 00:49:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.60%
[2024-07-02 00:49:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 00:49:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 00:50:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:19:59 lr 0.000031	 wd 0.0000	time 10.5512 (10.5512)	loss 1.4036 (1.4036)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:50:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:13:23 lr 0.000031	 wd 0.0000	time 0.2307 (0.3347)	loss 1.5186 (1.3951)	grad_norm 4.4717 (4.9895)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:50:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:10:52 lr 0.000031	 wd 0.0000	time 0.2292 (0.2837)	loss 1.6720 (1.3955)	grad_norm 5.1106 (4.7356)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:51:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:09:46 lr 0.000031	 wd 0.0000	time 0.2310 (0.2666)	loss 1.5489 (1.4014)	grad_norm 3.0280 (4.7860)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:51:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:09:02 lr 0.000030	 wd 0.0000	time 0.2289 (0.2579)	loss 1.4764 (1.3978)	grad_norm 5.2339 (4.7639)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:52:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:08:26 lr 0.000030	 wd 0.0000	time 0.2292 (0.2528)	loss 1.5205 (1.3925)	grad_norm 3.9443 (4.6488)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:52:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:07:54 lr 0.000030	 wd 0.0000	time 0.2309 (0.2494)	loss 1.5383 (1.3972)	grad_norm 4.9376 (4.6717)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:52:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:07:25 lr 0.000030	 wd 0.0000	time 0.2319 (0.2470)	loss 1.7315 (1.3926)	grad_norm 4.4308 (4.6405)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:53:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:06:57 lr 0.000030	 wd 0.0000	time 0.2282 (0.2453)	loss 1.5463 (1.3960)	grad_norm 6.6277 (4.7350)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:53:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:06:30 lr 0.000030	 wd 0.0000	time 0.2298 (0.2439)	loss 1.6671 (1.3916)	grad_norm 8.9344 (4.8012)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:53:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:06:04 lr 0.000030	 wd 0.0000	time 0.2289 (0.2428)	loss 1.5639 (1.3946)	grad_norm 3.5891 (4.7989)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:54:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:05:39 lr 0.000030	 wd 0.0000	time 0.2344 (0.2419)	loss 1.4960 (1.3967)	grad_norm 4.0983 (4.8126)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:54:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:05:14 lr 0.000030	 wd 0.0000	time 0.2321 (0.2412)	loss 1.6816 (1.3939)	grad_norm 3.6047 (4.8977)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:55:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:04:49 lr 0.000030	 wd 0.0000	time 0.2337 (0.2406)	loss 1.5951 (1.3931)	grad_norm 3.6769 (4.8721)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:55:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:04:24 lr 0.000030	 wd 0.0000	time 0.2302 (0.2401)	loss 1.6727 (1.3936)	grad_norm 5.1426 (4.9229)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:55:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:04:00 lr 0.000030	 wd 0.0000	time 0.2300 (0.2397)	loss 1.5877 (1.3930)	grad_norm 4.3185 (4.8965)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:56:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:03:35 lr 0.000029	 wd 0.0000	time 0.2343 (0.2393)	loss 1.5208 (1.3929)	grad_norm 5.5087 (4.9097)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:56:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:03:11 lr 0.000029	 wd 0.0000	time 0.2309 (0.2390)	loss 0.9062 (1.3902)	grad_norm 2.8828 (4.9053)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:57:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:02:47 lr 0.000029	 wd 0.0000	time 0.2301 (0.2387)	loss 1.4564 (1.3917)	grad_norm 3.2046 (5.0478)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:57:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:23 lr 0.000029	 wd 0.0000	time 0.2352 (0.2385)	loss 1.1671 (1.3919)	grad_norm 3.1308 (5.0191)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:57:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:01:59 lr 0.000029	 wd 0.0000	time 0.2312 (0.2383)	loss 1.4298 (1.3923)	grad_norm 3.5124 (4.9853)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:58:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:35 lr 0.000029	 wd 0.0000	time 0.2295 (0.2381)	loss 1.1769 (1.3919)	grad_norm 4.6691 (4.9586)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:58:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:11 lr 0.000029	 wd 0.0000	time 0.2344 (0.2379)	loss 1.5749 (1.3937)	grad_norm 4.9502 (4.9995)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:59:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:48 lr 0.000029	 wd 0.0000	time 0.2336 (0.2377)	loss 1.4047 (1.3928)	grad_norm 3.8675 (4.9717)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 00:59:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:24 lr 0.000029	 wd 0.0000	time 0.2344 (0.2376)	loss 1.5160 (1.3935)	grad_norm 5.5231 (4.9532)	loss_scale 512.0000 (264.7430)	mem 8828MB
[2024-07-02 00:59:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000029	 wd 0.0000	time 0.2297 (0.2374)	loss 1.5099 (1.3930)	grad_norm 3.2387 (4.9413)	loss_scale 512.0000 (274.6293)	mem 8828MB
[2024-07-02 00:59:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 13 training takes 0:09:56
[2024-07-02 01:00:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.162 (11.162)	Loss 0.4221 (0.4221)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 01:00:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.664 Acc@5 97.088
[2024-07-02 01:00:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 01:00:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.66%
[2024-07-02 01:00:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 01:00:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 01:00:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][0/2502]	eta 6:50:10 lr 0.000029	 wd 0.0000	time 9.8365 (9.8365)	loss 1.5628 (1.5628)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:00:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:13:25 lr 0.000029	 wd 0.0000	time 0.2305 (0.3355)	loss 1.5570 (1.3931)	grad_norm 3.1633 (4.4964)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:01:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:10:53 lr 0.000028	 wd 0.0000	time 0.2288 (0.2840)	loss 1.2670 (1.4164)	grad_norm 4.1930 (4.4958)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:01:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:09:47 lr 0.000028	 wd 0.0000	time 0.2285 (0.2667)	loss 1.2201 (1.4059)	grad_norm 3.6383 (4.5260)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:01:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:09:02 lr 0.000028	 wd 0.0000	time 0.2288 (0.2580)	loss 1.2689 (1.3926)	grad_norm 4.7523 (4.6476)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:02:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:08:26 lr 0.000028	 wd 0.0000	time 0.2293 (0.2529)	loss 1.4450 (1.3935)	grad_norm 5.5137 (4.8077)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:02:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:07:54 lr 0.000028	 wd 0.0000	time 0.2301 (0.2495)	loss 1.6427 (1.3974)	grad_norm 4.8015 (4.7559)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:03:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:07:25 lr 0.000028	 wd 0.0000	time 0.2318 (0.2471)	loss 1.3856 (1.3939)	grad_norm 6.6919 (4.7861)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:03:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:06:57 lr 0.000028	 wd 0.0000	time 0.2294 (0.2453)	loss 1.3507 (1.3903)	grad_norm 5.2648 (4.8122)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:03:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:06:30 lr 0.000028	 wd 0.0000	time 0.2306 (0.2439)	loss 1.6052 (1.3884)	grad_norm 5.5487 (4.9185)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:04:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:06:04 lr 0.000028	 wd 0.0000	time 0.2303 (0.2429)	loss 1.1652 (1.3864)	grad_norm 5.2274 (4.9566)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:04:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:05:39 lr 0.000028	 wd 0.0000	time 0.2331 (0.2420)	loss 1.4344 (1.3840)	grad_norm 4.8492 (4.9342)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:05:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:05:14 lr 0.000028	 wd 0.0000	time 0.2315 (0.2413)	loss 1.5166 (1.3842)	grad_norm 3.3260 (4.9487)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:05:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:04:49 lr 0.000027	 wd 0.0000	time 0.2302 (0.2407)	loss 1.2724 (1.3855)	grad_norm 4.0719 (4.9151)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:05:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:04:24 lr 0.000027	 wd 0.0000	time 0.2295 (0.2402)	loss 1.5358 (1.3847)	grad_norm 3.1612 (4.8817)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:06:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:04:00 lr 0.000027	 wd 0.0000	time 0.2345 (0.2398)	loss 1.3884 (1.3853)	grad_norm 6.2830 (4.8988)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:06:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:03:35 lr 0.000027	 wd 0.0000	time 0.2302 (0.2394)	loss 1.3553 (1.3861)	grad_norm 5.6117 (4.8833)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:07:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:03:11 lr 0.000027	 wd 0.0000	time 0.2312 (0.2391)	loss 1.2589 (1.3864)	grad_norm 4.2344 (4.8504)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:07:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:02:47 lr 0.000027	 wd 0.0000	time 0.2297 (0.2388)	loss 1.3289 (1.3857)	grad_norm 3.3303 (4.8395)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:07:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:23 lr 0.000027	 wd 0.0000	time 0.2301 (0.2385)	loss 1.4600 (1.3857)	grad_norm 7.4686 (4.8305)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:08:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:01:59 lr 0.000027	 wd 0.0000	time 0.2351 (0.2383)	loss 1.0906 (1.3871)	grad_norm 2.9865 (4.8333)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:08:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:35 lr 0.000027	 wd 0.0000	time 0.2294 (0.2381)	loss 1.7033 (1.3861)	grad_norm 9.9547 (4.8349)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:08:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:11 lr 0.000027	 wd 0.0000	time 0.2311 (0.2379)	loss 1.2842 (1.3864)	grad_norm 8.8259 (4.8277)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:09:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:48 lr 0.000027	 wd 0.0000	time 0.2330 (0.2378)	loss 1.3986 (1.3873)	grad_norm 5.3270 (4.8286)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:09:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:24 lr 0.000026	 wd 0.0000	time 0.2315 (0.2376)	loss 1.2904 (1.3882)	grad_norm 4.3648 (4.8326)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:10:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.2289 (0.2374)	loss 1.3993 (1.3874)	grad_norm 4.2413 (4.8464)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:10:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 14 training takes 0:09:56
[2024-07-02 01:10:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.678 (11.678)	Loss 0.4229 (0.4229)	Acc@1 91.992 (91.992)	Acc@5 98.242 (98.242)	Mem 8828MB
[2024-07-02 01:10:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.588 Acc@5 97.082
[2024-07-02 01:10:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-07-02 01:10:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.66%
[2024-07-02 01:10:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][0/2502]	eta 6:49:20 lr 0.000026	 wd 0.0000	time 9.8162 (9.8162)	loss 1.4293 (1.4293)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:11:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:13:29 lr 0.000026	 wd 0.0000	time 0.2295 (0.3371)	loss 1.0951 (1.3761)	grad_norm 2.3703 (4.8435)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:11:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:10:55 lr 0.000026	 wd 0.0000	time 0.2280 (0.2847)	loss 1.5471 (1.3881)	grad_norm 3.6969 (4.6028)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:11:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:09:48 lr 0.000026	 wd 0.0000	time 0.2346 (0.2673)	loss 1.3923 (1.3748)	grad_norm 9.2068 (4.9627)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:12:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:09:03 lr 0.000026	 wd 0.0000	time 0.2317 (0.2585)	loss 1.4091 (1.3783)	grad_norm 7.4955 (5.2224)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:12:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:08:26 lr 0.000026	 wd 0.0000	time 0.2296 (0.2532)	loss 0.9468 (1.3739)	grad_norm 3.0928 (5.1410)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:13:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:07:55 lr 0.000026	 wd 0.0000	time 0.2283 (0.2498)	loss 0.9108 (1.3768)	grad_norm 3.7811 (4.9945)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:13:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:07:25 lr 0.000026	 wd 0.0000	time 0.2309 (0.2473)	loss 1.1616 (1.3805)	grad_norm 3.8564 (4.9467)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:13:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:06:57 lr 0.000026	 wd 0.0000	time 0.2298 (0.2455)	loss 1.5658 (1.3815)	grad_norm 3.8748 (4.9683)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:14:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:06:31 lr 0.000025	 wd 0.0000	time 0.2305 (0.2441)	loss 1.2027 (1.3810)	grad_norm 3.5964 (4.9211)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:14:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:06:05 lr 0.000025	 wd 0.0000	time 0.2306 (0.2430)	loss 1.3848 (1.3812)	grad_norm 2.5628 (4.9243)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:15:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:05:39 lr 0.000025	 wd 0.0000	time 0.2317 (0.2421)	loss 1.3792 (1.3781)	grad_norm 6.1698 (4.9416)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:15:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:05:14 lr 0.000025	 wd 0.0000	time 0.2290 (0.2414)	loss 1.6157 (1.3783)	grad_norm 3.7069 (4.9446)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:15:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:04:49 lr 0.000025	 wd 0.0000	time 0.2302 (0.2408)	loss 1.3893 (1.3771)	grad_norm 3.5710 (4.9279)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:16:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:04:24 lr 0.000025	 wd 0.0000	time 0.2300 (0.2403)	loss 1.3688 (1.3781)	grad_norm 3.9407 (4.8947)	loss_scale 1024.0000 (543.4290)	mem 8828MB
[2024-07-02 01:16:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:04:00 lr 0.000025	 wd 0.0000	time 0.2376 (0.2398)	loss 1.6619 (1.3775)	grad_norm 3.9763 (4.8910)	loss_scale 1024.0000 (575.4457)	mem 8828MB
[2024-07-02 01:16:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:03:35 lr 0.000025	 wd 0.0000	time 0.2295 (0.2395)	loss 1.2547 (1.3799)	grad_norm 3.8583 (4.8805)	loss_scale 1024.0000 (603.4628)	mem 8828MB
[2024-07-02 01:17:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:03:11 lr 0.000025	 wd 0.0000	time 0.2340 (0.2392)	loss 1.5742 (1.3804)	grad_norm 4.2580 (4.8875)	loss_scale 1024.0000 (628.1858)	mem 8828MB
[2024-07-02 01:17:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:02:47 lr 0.000025	 wd 0.0000	time 0.2301 (0.2389)	loss 1.1152 (1.3801)	grad_norm 3.3306 (4.8556)	loss_scale 1024.0000 (650.1632)	mem 8828MB
[2024-07-02 01:18:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:23 lr 0.000024	 wd 0.0000	time 0.2279 (0.2386)	loss 1.6665 (1.3813)	grad_norm 3.6047 (4.8634)	loss_scale 1024.0000 (669.8285)	mem 8828MB
[2024-07-02 01:18:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:01:59 lr 0.000024	 wd 0.0000	time 0.2341 (0.2384)	loss 1.7974 (1.3842)	grad_norm 3.4331 (4.8708)	loss_scale 1024.0000 (687.5282)	mem 8828MB
[2024-07-02 01:18:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:35 lr 0.000024	 wd 0.0000	time 0.2306 (0.2382)	loss 1.6366 (1.3849)	grad_norm 6.1053 (4.8506)	loss_scale 1024.0000 (703.5431)	mem 8828MB
[2024-07-02 01:19:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:11 lr 0.000024	 wd 0.0000	time 0.2308 (0.2380)	loss 0.9650 (1.3859)	grad_norm 5.0858 (4.8294)	loss_scale 1024.0000 (718.1027)	mem 8828MB
[2024-07-02 01:19:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:48 lr 0.000024	 wd 0.0000	time 0.2308 (0.2379)	loss 1.5825 (1.3862)	grad_norm 4.8248 (4.8552)	loss_scale 1024.0000 (731.3968)	mem 8828MB
[2024-07-02 01:20:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:24 lr 0.000024	 wd 0.0000	time 0.2311 (0.2377)	loss 1.2383 (1.3852)	grad_norm 5.1667 (4.8963)	loss_scale 1024.0000 (743.5835)	mem 8828MB
[2024-07-02 01:20:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.2305 (0.2375)	loss 1.4384 (1.3855)	grad_norm 2.7366 (4.9008)	loss_scale 1024.0000 (754.7957)	mem 8828MB
[2024-07-02 01:20:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 15 training takes 0:09:56
[2024-07-02 01:20:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_15.pth saving......
[2024-07-02 01:20:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_15.pth saved !!!
[2024-07-02 01:20:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.266 (11.266)	Loss 0.4211 (0.4211)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 01:20:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.628 Acc@5 97.094
[2024-07-02 01:20:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-07-02 01:20:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.66%
[2024-07-02 01:21:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:18:29 lr 0.000024	 wd 0.0000	time 10.5155 (10.5155)	loss 1.2252 (1.2252)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:21:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:13:33 lr 0.000024	 wd 0.0000	time 0.2288 (0.3386)	loss 1.5514 (1.3951)	grad_norm 2.8744 (4.5980)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:21:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:10:57 lr 0.000024	 wd 0.0000	time 0.2298 (0.2857)	loss 1.3669 (1.4002)	grad_norm 6.7219 (4.5229)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:22:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:09:49 lr 0.000024	 wd 0.0000	time 0.2297 (0.2679)	loss 1.3747 (1.4081)	grad_norm 3.7474 (4.6405)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:22:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:09:04 lr 0.000024	 wd 0.0000	time 0.2314 (0.2589)	loss 1.2542 (1.4028)	grad_norm 3.1507 (4.7742)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:23:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:08:27 lr 0.000023	 wd 0.0000	time 0.2306 (0.2536)	loss 1.4488 (1.4107)	grad_norm 3.7669 (4.7321)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:23:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:07:55 lr 0.000023	 wd 0.0000	time 0.2288 (0.2502)	loss 1.0966 (1.4015)	grad_norm 3.3758 (4.6447)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:23:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:07:26 lr 0.000023	 wd 0.0000	time 0.2311 (0.2477)	loss 1.3365 (1.4023)	grad_norm 4.4278 (4.6409)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:24:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:06:58 lr 0.000023	 wd 0.0000	time 0.2294 (0.2458)	loss 1.3842 (1.3960)	grad_norm 3.2311 (4.6392)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:24:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:06:31 lr 0.000023	 wd 0.0000	time 0.2370 (0.2444)	loss 1.3469 (1.3948)	grad_norm 3.0305 (4.6245)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:24:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:06:05 lr 0.000023	 wd 0.0000	time 0.2295 (0.2433)	loss 1.4830 (1.3947)	grad_norm 4.2169 (4.6697)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:25:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:05:39 lr 0.000023	 wd 0.0000	time 0.2296 (0.2424)	loss 1.3518 (1.3970)	grad_norm 4.3204 (4.7169)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:25:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:05:14 lr 0.000023	 wd 0.0000	time 0.2350 (0.2416)	loss 1.5900 (1.3964)	grad_norm 6.0094 (4.7061)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:26:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:04:49 lr 0.000023	 wd 0.0000	time 0.2313 (0.2410)	loss 1.0857 (1.3966)	grad_norm 3.8456 (4.7512)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:26:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:04:25 lr 0.000023	 wd 0.0000	time 0.2347 (0.2405)	loss 1.3420 (1.3942)	grad_norm 3.7392 (4.7804)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:26:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:04:00 lr 0.000022	 wd 0.0000	time 0.2305 (0.2401)	loss 1.4259 (1.3914)	grad_norm 6.4505 (nan)	loss_scale 512.0000 (998.0759)	mem 8828MB
[2024-07-02 01:27:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:03:36 lr 0.000022	 wd 0.0000	time 0.2324 (0.2397)	loss 1.4891 (1.3908)	grad_norm 8.7649 (nan)	loss_scale 512.0000 (967.7152)	mem 8828MB
[2024-07-02 01:27:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:03:11 lr 0.000022	 wd 0.0000	time 0.2343 (0.2394)	loss 1.1264 (1.3890)	grad_norm 5.3643 (nan)	loss_scale 512.0000 (940.9242)	mem 8828MB
[2024-07-02 01:28:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:02:47 lr 0.000022	 wd 0.0000	time 0.2308 (0.2391)	loss 1.1479 (1.3879)	grad_norm 8.2825 (nan)	loss_scale 512.0000 (917.1083)	mem 8828MB
[2024-07-02 01:28:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:23 lr 0.000022	 wd 0.0000	time 0.2309 (0.2388)	loss 1.3105 (1.3872)	grad_norm 2.6279 (nan)	loss_scale 512.0000 (895.7980)	mem 8828MB
[2024-07-02 01:28:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:01:59 lr 0.000022	 wd 0.0000	time 0.2310 (0.2386)	loss 1.6146 (1.3872)	grad_norm 6.7208 (nan)	loss_scale 512.0000 (876.6177)	mem 8828MB
[2024-07-02 01:29:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:35 lr 0.000022	 wd 0.0000	time 0.2305 (0.2384)	loss 1.4844 (1.3867)	grad_norm 3.7689 (nan)	loss_scale 512.0000 (859.2632)	mem 8828MB
[2024-07-02 01:29:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:11 lr 0.000022	 wd 0.0000	time 0.2310 (0.2382)	loss 1.3632 (1.3874)	grad_norm 3.9817 (nan)	loss_scale 512.0000 (843.4857)	mem 8828MB
[2024-07-02 01:30:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:48 lr 0.000022	 wd 0.0000	time 0.2320 (0.2380)	loss 1.3060 (1.3877)	grad_norm 4.6067 (nan)	loss_scale 512.0000 (829.0795)	mem 8828MB
[2024-07-02 01:30:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:24 lr 0.000022	 wd 0.0000	time 0.2316 (0.2378)	loss 1.3959 (1.3876)	grad_norm 5.2048 (nan)	loss_scale 512.0000 (815.8734)	mem 8828MB
[2024-07-02 01:30:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.2299 (0.2376)	loss 1.4412 (1.3872)	grad_norm 4.0390 (nan)	loss_scale 512.0000 (803.7233)	mem 8828MB
[2024-07-02 01:30:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 16 training takes 0:09:57
[2024-07-02 01:31:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.865 (11.865)	Loss 0.4236 (0.4236)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 01:31:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.614 Acc@5 97.086
[2024-07-02 01:31:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-07-02 01:31:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.66%
[2024-07-02 01:31:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:14:44 lr 0.000021	 wd 0.0000	time 10.4256 (10.4256)	loss 1.4919 (1.4919)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:31:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:13:33 lr 0.000021	 wd 0.0000	time 0.2295 (0.3385)	loss 1.5927 (1.3653)	grad_norm 3.4065 (4.8200)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:32:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:10:57 lr 0.000021	 wd 0.0000	time 0.2325 (0.2855)	loss 1.5574 (1.3805)	grad_norm 3.7859 (4.5661)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:32:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:09:49 lr 0.000021	 wd 0.0000	time 0.2301 (0.2678)	loss 1.4161 (1.3777)	grad_norm 5.3898 (4.5243)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:32:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:09:04 lr 0.000021	 wd 0.0000	time 0.2297 (0.2590)	loss 1.1485 (1.3767)	grad_norm 2.8810 (4.5058)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:33:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:08:27 lr 0.000021	 wd 0.0000	time 0.2297 (0.2537)	loss 1.4015 (1.3741)	grad_norm 8.4411 (4.7153)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:33:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:07:55 lr 0.000021	 wd 0.0000	time 0.2294 (0.2502)	loss 1.1615 (1.3794)	grad_norm 5.6014 (4.6635)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:34:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:07:26 lr 0.000021	 wd 0.0000	time 0.2330 (0.2477)	loss 1.3667 (1.3841)	grad_norm 42.4624 (4.7719)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:34:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:06:58 lr 0.000021	 wd 0.0000	time 0.2308 (0.2458)	loss 1.3793 (1.3875)	grad_norm 3.0712 (4.7108)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:34:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:06:31 lr 0.000021	 wd 0.0000	time 0.2295 (0.2444)	loss 1.4959 (1.3900)	grad_norm 3.3128 (4.6192)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:35:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:06:05 lr 0.000020	 wd 0.0000	time 0.2313 (0.2433)	loss 1.5621 (1.3877)	grad_norm 4.1295 (4.6088)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:35:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:05:39 lr 0.000020	 wd 0.0000	time 0.2295 (0.2423)	loss 1.1913 (1.3861)	grad_norm 3.6803 (4.6104)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:36:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:05:14 lr 0.000020	 wd 0.0000	time 0.2301 (0.2416)	loss 1.4507 (1.3885)	grad_norm 2.6992 (4.5736)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:36:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:04:49 lr 0.000020	 wd 0.0000	time 0.2305 (0.2410)	loss 1.2824 (1.3884)	grad_norm 3.0722 (4.5758)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:36:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:04:25 lr 0.000020	 wd 0.0000	time 0.2322 (0.2405)	loss 1.5595 (1.3888)	grad_norm 3.9916 (4.5754)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:37:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:04:00 lr 0.000020	 wd 0.0000	time 0.2340 (0.2400)	loss 1.5410 (1.3895)	grad_norm 3.0572 (4.5592)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:37:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:03:36 lr 0.000020	 wd 0.0000	time 0.2295 (0.2396)	loss 1.4665 (1.3908)	grad_norm 5.3939 (4.5615)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:38:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:11 lr 0.000020	 wd 0.0000	time 0.2318 (0.2393)	loss 1.6438 (1.3925)	grad_norm 3.2954 (4.5403)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:38:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:02:47 lr 0.000020	 wd 0.0000	time 0.2312 (0.2390)	loss 1.4371 (1.3928)	grad_norm 3.8034 (4.5586)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:38:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:23 lr 0.000020	 wd 0.0000	time 0.2301 (0.2388)	loss 1.2603 (1.3934)	grad_norm 4.3967 (4.6290)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:39:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:01:59 lr 0.000019	 wd 0.0000	time 0.2295 (0.2385)	loss 1.4541 (1.3910)	grad_norm 3.2106 (4.6392)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:39:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:35 lr 0.000019	 wd 0.0000	time 0.2346 (0.2383)	loss 1.5552 (1.3909)	grad_norm 3.7134 (4.6504)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:39:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:11 lr 0.000019	 wd 0.0000	time 0.2303 (0.2382)	loss 1.3205 (1.3912)	grad_norm 4.0462 (4.6394)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:40:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:48 lr 0.000019	 wd 0.0000	time 0.2322 (0.2380)	loss 1.6256 (1.3901)	grad_norm 4.4018 (4.6394)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:40:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:24 lr 0.000019	 wd 0.0000	time 0.2318 (0.2379)	loss 1.4714 (1.3882)	grad_norm 5.1011 (4.6893)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:41:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0000	time 0.2288 (0.2377)	loss 1.4050 (1.3889)	grad_norm 3.5770 (4.7118)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:41:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 17 training takes 0:09:57
[2024-07-02 01:41:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.577 (11.577)	Loss 0.4224 (0.4224)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 01:41:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.686 Acc@5 97.114
[2024-07-02 01:41:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 01:41:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.69%
[2024-07-02 01:41:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 01:41:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 01:41:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:16:41 lr 0.000019	 wd 0.0000	time 10.4722 (10.4722)	loss 1.6641 (1.6641)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:42:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:13:25 lr 0.000019	 wd 0.0000	time 0.2286 (0.3352)	loss 1.6030 (1.4319)	grad_norm 5.2099 (4.5169)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:42:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:10:53 lr 0.000019	 wd 0.0000	time 0.2286 (0.2839)	loss 1.5953 (1.4048)	grad_norm 3.3840 (4.4866)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:42:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:09:47 lr 0.000019	 wd 0.0000	time 0.2324 (0.2667)	loss 1.6134 (1.4013)	grad_norm 3.8636 (4.6170)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:43:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:09:02 lr 0.000019	 wd 0.0000	time 0.2294 (0.2581)	loss 1.4388 (1.3975)	grad_norm 3.9649 (4.6675)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 01:43:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:08:26 lr 0.000018	 wd 0.0000	time 0.2294 (0.2530)	loss 1.2724 (1.3907)	grad_norm 3.7778 (4.6718)	loss_scale 1024.0000 (593.7565)	mem 8828MB
[2024-07-02 01:44:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:07:54 lr 0.000018	 wd 0.0000	time 0.2350 (0.2496)	loss 1.5856 (1.3892)	grad_norm 2.5692 (4.6646)	loss_scale 1024.0000 (665.3444)	mem 8828MB
[2024-07-02 01:44:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:07:25 lr 0.000018	 wd 0.0000	time 0.2315 (0.2473)	loss 1.6228 (1.3903)	grad_norm 3.7233 (4.6406)	loss_scale 1024.0000 (716.5078)	mem 8828MB
[2024-07-02 01:44:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:06:57 lr 0.000018	 wd 0.0000	time 0.2300 (0.2455)	loss 1.1565 (1.3871)	grad_norm 6.1936 (4.6393)	loss_scale 1024.0000 (754.8964)	mem 8828MB
[2024-07-02 01:45:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:06:31 lr 0.000018	 wd 0.0000	time 0.2296 (0.2442)	loss 1.4140 (1.3837)	grad_norm 4.0161 (4.6121)	loss_scale 1024.0000 (784.7636)	mem 8828MB
[2024-07-02 01:45:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:06:05 lr 0.000018	 wd 0.0000	time 0.2296 (0.2431)	loss 1.1525 (1.3878)	grad_norm 4.7737 (4.6376)	loss_scale 1024.0000 (808.6633)	mem 8828MB
[2024-07-02 01:46:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:05:39 lr 0.000018	 wd 0.0000	time 0.2325 (0.2423)	loss 1.3221 (1.3870)	grad_norm 3.4071 (4.6645)	loss_scale 1024.0000 (828.2216)	mem 8828MB
[2024-07-02 01:46:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:05:14 lr 0.000018	 wd 0.0000	time 0.2321 (0.2416)	loss 1.5602 (1.3872)	grad_norm 7.0584 (4.6978)	loss_scale 1024.0000 (844.5229)	mem 8828MB
[2024-07-02 01:46:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:04:49 lr 0.000018	 wd 0.0000	time 0.2341 (0.2410)	loss 1.5994 (1.3882)	grad_norm 6.5456 (4.6997)	loss_scale 1024.0000 (858.3182)	mem 8828MB
[2024-07-02 01:47:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:04:25 lr 0.000018	 wd 0.0000	time 0.2354 (0.2405)	loss 1.2394 (1.3877)	grad_norm 4.3793 (4.7243)	loss_scale 1024.0000 (870.1442)	mem 8828MB
[2024-07-02 01:47:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:04:00 lr 0.000017	 wd 0.0000	time 0.2309 (0.2400)	loss 1.5770 (1.3860)	grad_norm 12.5640 (4.7862)	loss_scale 1024.0000 (880.3944)	mem 8828MB
[2024-07-02 01:48:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:03:36 lr 0.000017	 wd 0.0000	time 0.2310 (0.2397)	loss 1.6063 (1.3857)	grad_norm 3.5862 (4.7777)	loss_scale 1024.0000 (889.3641)	mem 8828MB
[2024-07-02 01:48:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:03:11 lr 0.000017	 wd 0.0000	time 0.2341 (0.2394)	loss 1.5183 (1.3847)	grad_norm 4.6814 (4.7844)	loss_scale 1024.0000 (897.2792)	mem 8828MB
[2024-07-02 01:48:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:02:47 lr 0.000017	 wd 0.0000	time 0.2302 (0.2391)	loss 1.3802 (1.3861)	grad_norm 3.3387 (4.7571)	loss_scale 1024.0000 (904.3154)	mem 8828MB
[2024-07-02 01:49:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:23 lr 0.000017	 wd 0.0000	time 0.2300 (0.2389)	loss 1.6099 (1.3861)	grad_norm 4.7767 (4.7675)	loss_scale 1024.0000 (910.6113)	mem 8828MB
[2024-07-02 01:49:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:01:59 lr 0.000017	 wd 0.0000	time 0.2300 (0.2386)	loss 1.4965 (1.3882)	grad_norm 3.1163 (4.7421)	loss_scale 1024.0000 (916.2779)	mem 8828MB
[2024-07-02 01:49:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:35 lr 0.000017	 wd 0.0000	time 0.2323 (0.2384)	loss 1.4055 (1.3876)	grad_norm 3.1503 (4.7485)	loss_scale 1024.0000 (921.4050)	mem 8828MB
[2024-07-02 01:50:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:11 lr 0.000017	 wd 0.0000	time 0.2301 (0.2383)	loss 1.4656 (1.3875)	grad_norm 2.8591 (4.7378)	loss_scale 1024.0000 (926.0663)	mem 8828MB
[2024-07-02 01:50:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:48 lr 0.000017	 wd 0.0000	time 0.2306 (0.2381)	loss 1.4656 (1.3875)	grad_norm 7.0536 (4.7537)	loss_scale 1024.0000 (930.3225)	mem 8828MB
[2024-07-02 01:51:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:24 lr 0.000017	 wd 0.0000	time 0.2316 (0.2379)	loss 1.5908 (1.3890)	grad_norm 3.7210 (4.7590)	loss_scale 1024.0000 (934.2241)	mem 8828MB
[2024-07-02 01:51:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.2302 (0.2377)	loss 1.5498 (1.3886)	grad_norm 4.1303 (4.7608)	loss_scale 1024.0000 (937.8137)	mem 8828MB
[2024-07-02 01:51:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 18 training takes 0:09:57
[2024-07-02 01:51:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.384 (11.384)	Loss 0.4177 (0.4177)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 01:51:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.710 Acc@5 97.116
[2024-07-02 01:51:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 01:51:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.71%
[2024-07-02 01:51:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 01:51:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 01:52:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:11:00 lr 0.000016	 wd 0.0000	time 10.3359 (10.3359)	loss 1.5434 (1.5434)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:52:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:13:20 lr 0.000016	 wd 0.0000	time 0.2350 (0.3333)	loss 1.1436 (1.3941)	grad_norm 8.3851 (4.7610)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:52:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:10:51 lr 0.000016	 wd 0.0000	time 0.2294 (0.2828)	loss 1.5601 (1.3924)	grad_norm 7.4899 (4.7524)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:53:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:09:45 lr 0.000016	 wd 0.0000	time 0.2291 (0.2661)	loss 1.3848 (1.4038)	grad_norm 4.5229 (4.6238)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:53:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:09:01 lr 0.000016	 wd 0.0000	time 0.2324 (0.2576)	loss 1.6072 (1.4029)	grad_norm 4.9813 (4.7109)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:54:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:08:25 lr 0.000016	 wd 0.0000	time 0.2297 (0.2526)	loss 1.6799 (1.3999)	grad_norm 3.6722 (4.7638)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:54:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:07:54 lr 0.000016	 wd 0.0000	time 0.2290 (0.2493)	loss 1.5813 (1.3955)	grad_norm 8.3352 (4.8295)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:54:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:07:25 lr 0.000016	 wd 0.0000	time 0.2283 (0.2470)	loss 1.6848 (1.3975)	grad_norm 5.8581 (4.7784)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:55:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:06:57 lr 0.000016	 wd 0.0000	time 0.2300 (0.2453)	loss 1.3564 (1.3942)	grad_norm 4.4131 (4.7517)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:55:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:06:30 lr 0.000016	 wd 0.0000	time 0.2335 (0.2439)	loss 1.4628 (1.3958)	grad_norm 3.1814 (4.7976)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:56:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:06:04 lr 0.000016	 wd 0.0000	time 0.2294 (0.2429)	loss 1.5347 (1.3983)	grad_norm 3.2133 (4.8344)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:56:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:05:39 lr 0.000015	 wd 0.0000	time 0.2312 (0.2420)	loss 0.9153 (1.3964)	grad_norm 3.0868 (4.8246)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:56:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:05:14 lr 0.000015	 wd 0.0000	time 0.2301 (0.2413)	loss 1.1670 (1.3938)	grad_norm 4.4848 (4.8053)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:57:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:04:49 lr 0.000015	 wd 0.0000	time 0.2319 (0.2407)	loss 1.5072 (1.3940)	grad_norm 3.5073 (4.7813)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 01:57:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:04:24 lr 0.000015	 wd 0.0000	time 0.2277 (0.2402)	loss 1.4710 (1.3956)	grad_norm 3.7858 (nan)	loss_scale 512.0000 (991.1092)	mem 8828MB
[2024-07-02 01:57:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:04:00 lr 0.000015	 wd 0.0000	time 0.2315 (0.2398)	loss 1.2608 (1.3953)	grad_norm 3.5285 (nan)	loss_scale 512.0000 (959.1899)	mem 8828MB
[2024-07-02 01:58:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:03:36 lr 0.000015	 wd 0.0000	time 0.2283 (0.2395)	loss 1.4356 (1.3959)	grad_norm 4.4654 (nan)	loss_scale 512.0000 (931.2580)	mem 8828MB
[2024-07-02 01:58:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:03:11 lr 0.000015	 wd 0.0000	time 0.2337 (0.2392)	loss 1.5505 (1.3965)	grad_norm 8.1806 (nan)	loss_scale 512.0000 (906.6102)	mem 8828MB
[2024-07-02 01:59:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:02:47 lr 0.000015	 wd 0.0000	time 0.2319 (0.2389)	loss 1.3680 (1.3965)	grad_norm 4.8921 (nan)	loss_scale 512.0000 (884.6996)	mem 8828MB
[2024-07-02 01:59:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:23 lr 0.000015	 wd 0.0000	time 0.2327 (0.2387)	loss 1.3516 (1.3970)	grad_norm 6.0793 (nan)	loss_scale 512.0000 (865.0942)	mem 8828MB
[2024-07-02 01:59:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:01:59 lr 0.000015	 wd 0.0000	time 0.2295 (0.2385)	loss 1.0484 (1.3963)	grad_norm 6.0086 (nan)	loss_scale 512.0000 (847.4483)	mem 8828MB
[2024-07-02 02:00:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:35 lr 0.000014	 wd 0.0000	time 0.2302 (0.2383)	loss 1.2152 (1.3956)	grad_norm 3.3085 (nan)	loss_scale 512.0000 (831.4822)	mem 8828MB
[2024-07-02 02:00:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:11 lr 0.000014	 wd 0.0000	time 0.2307 (0.2381)	loss 1.5401 (1.3938)	grad_norm 7.8529 (nan)	loss_scale 512.0000 (816.9668)	mem 8828MB
[2024-07-02 02:01:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:48 lr 0.000014	 wd 0.0000	time 0.2307 (0.2379)	loss 1.4281 (1.3931)	grad_norm 4.3366 (nan)	loss_scale 512.0000 (803.7132)	mem 8828MB
[2024-07-02 02:01:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:24 lr 0.000014	 wd 0.0000	time 0.2321 (0.2378)	loss 1.6084 (1.3940)	grad_norm 3.2486 (nan)	loss_scale 512.0000 (791.5635)	mem 8828MB
[2024-07-02 02:01:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.2308 (0.2376)	loss 1.2953 (1.3940)	grad_norm 6.3333 (nan)	loss_scale 512.0000 (780.3854)	mem 8828MB
[2024-07-02 02:01:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 19 training takes 0:09:57
[2024-07-02 02:02:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.751 (10.751)	Loss 0.4185 (0.4185)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 02:02:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.692 Acc@5 97.106
[2024-07-02 02:02:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 02:02:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.71%
[2024-07-02 02:02:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][0/2502]	eta 6:30:17 lr 0.000014	 wd 0.0000	time 9.3594 (9.3594)	loss 1.1039 (1.1039)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:02:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:13:34 lr 0.000014	 wd 0.0000	time 0.2293 (0.3391)	loss 1.7462 (1.3889)	grad_norm 4.1700 (5.7019)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:03:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:10:58 lr 0.000014	 wd 0.0000	time 0.2305 (0.2860)	loss 1.4576 (1.3956)	grad_norm 3.7952 (5.3458)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:03:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:09:50 lr 0.000014	 wd 0.0000	time 0.2255 (0.2680)	loss 1.4754 (1.3933)	grad_norm 2.8474 (5.0181)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:04:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:09:04 lr 0.000014	 wd 0.0000	time 0.2296 (0.2591)	loss 1.5742 (1.3997)	grad_norm 7.5988 (4.9941)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:04:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:08:27 lr 0.000014	 wd 0.0000	time 0.2307 (0.2537)	loss 1.4038 (1.3991)	grad_norm 3.0185 (4.9174)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:04:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:07:55 lr 0.000014	 wd 0.0000	time 0.2285 (0.2502)	loss 1.5052 (1.3949)	grad_norm 4.9000 (4.8849)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:05:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:07:26 lr 0.000013	 wd 0.0000	time 0.2259 (0.2477)	loss 1.4818 (1.4000)	grad_norm 5.1495 (4.8537)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:05:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:06:58 lr 0.000013	 wd 0.0000	time 0.2306 (0.2459)	loss 1.2930 (1.3971)	grad_norm 5.0866 (4.8471)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:05:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:06:31 lr 0.000013	 wd 0.0000	time 0.2355 (0.2445)	loss 1.6703 (1.3999)	grad_norm 6.8297 (4.8492)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:06:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:06:05 lr 0.000013	 wd 0.0000	time 0.2296 (0.2434)	loss 1.3944 (1.3995)	grad_norm 6.9720 (4.8454)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:06:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:05:40 lr 0.000013	 wd 0.0000	time 0.2328 (0.2425)	loss 1.4606 (1.3967)	grad_norm 4.7597 (4.8613)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:07:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:05:14 lr 0.000013	 wd 0.0000	time 0.2303 (0.2418)	loss 1.4754 (1.3938)	grad_norm 3.8736 (4.8324)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:07:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:04:49 lr 0.000013	 wd 0.0000	time 0.2302 (0.2412)	loss 1.2230 (1.3901)	grad_norm 9.7498 (4.7851)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:07:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:04:25 lr 0.000013	 wd 0.0000	time 0.2311 (0.2406)	loss 1.3759 (1.3917)	grad_norm 5.1597 (4.8000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:08:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:04:00 lr 0.000013	 wd 0.0000	time 0.2316 (0.2401)	loss 1.2952 (1.3932)	grad_norm 6.8967 (4.7770)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:08:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:03:36 lr 0.000013	 wd 0.0000	time 0.2308 (0.2397)	loss 1.6107 (1.3939)	grad_norm 4.0255 (4.7727)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:09:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:03:12 lr 0.000012	 wd 0.0000	time 0.2347 (0.2394)	loss 1.5705 (1.3934)	grad_norm 5.7704 (4.7527)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:09:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:02:47 lr 0.000012	 wd 0.0000	time 0.2309 (0.2392)	loss 1.3517 (1.3922)	grad_norm 4.2695 (4.7850)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:09:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:23 lr 0.000012	 wd 0.0000	time 0.2310 (0.2389)	loss 1.6883 (1.3932)	grad_norm 4.7293 (4.7921)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:10:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:01:59 lr 0.000012	 wd 0.0000	time 0.2319 (0.2386)	loss 1.5285 (1.3924)	grad_norm 4.1900 (4.7747)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:10:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:35 lr 0.000012	 wd 0.0000	time 0.2332 (0.2385)	loss 1.4089 (1.3930)	grad_norm 3.8623 (4.7656)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:11:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:11 lr 0.000012	 wd 0.0000	time 0.2306 (0.2383)	loss 1.3623 (1.3943)	grad_norm 6.7259 (4.7775)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:11:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:48 lr 0.000012	 wd 0.0000	time 0.2327 (0.2381)	loss 1.3124 (1.3946)	grad_norm 2.9143 (4.7933)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:11:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:24 lr 0.000012	 wd 0.0000	time 0.2363 (0.2379)	loss 1.3048 (1.3951)	grad_norm 4.4973 (4.8093)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:12:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0000	time 0.2315 (0.2377)	loss 1.5278 (1.3958)	grad_norm 3.4763 (4.8075)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:12:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 20 training takes 0:09:57
[2024-07-02 02:12:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.726 (11.726)	Loss 0.4219 (0.4219)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 02:12:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.708 Acc@5 97.108
[2024-07-02 02:12:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 02:12:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.71%
[2024-07-02 02:12:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:46:10 lr 0.000012	 wd 0.0000	time 11.1794 (11.1794)	loss 1.6249 (1.6249)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:13:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:13:38 lr 0.000012	 wd 0.0000	time 0.2324 (0.3409)	loss 1.6105 (1.4303)	grad_norm 4.4744 (5.2989)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:13:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:11:00 lr 0.000012	 wd 0.0000	time 0.2309 (0.2867)	loss 0.9927 (1.4006)	grad_norm 3.4345 (5.0379)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:13:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:09:51 lr 0.000012	 wd 0.0000	time 0.2331 (0.2686)	loss 1.4186 (1.3902)	grad_norm 3.5277 (4.9954)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:14:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:09:05 lr 0.000011	 wd 0.0000	time 0.2306 (0.2595)	loss 1.3871 (1.3924)	grad_norm 3.5629 (5.1121)	loss_scale 1024.0000 (632.0200)	mem 8828MB
[2024-07-02 02:14:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:08:28 lr 0.000011	 wd 0.0000	time 0.2293 (0.2540)	loss 1.3493 (1.3916)	grad_norm 5.0618 (4.9911)	loss_scale 1024.0000 (710.2595)	mem 8828MB
[2024-07-02 02:15:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:07:56 lr 0.000011	 wd 0.0000	time 0.2308 (0.2505)	loss 1.2804 (1.3932)	grad_norm 4.8275 (4.9358)	loss_scale 1024.0000 (762.4626)	mem 8828MB
[2024-07-02 02:15:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:07:26 lr 0.000011	 wd 0.0000	time 0.2305 (0.2480)	loss 1.3160 (1.3875)	grad_norm 3.7212 (4.9534)	loss_scale 1024.0000 (799.7718)	mem 8828MB
[2024-07-02 02:15:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:06:58 lr 0.000011	 wd 0.0000	time 0.2339 (0.2461)	loss 1.3061 (1.3858)	grad_norm 5.3116 (4.8717)	loss_scale 1024.0000 (827.7653)	mem 8828MB
[2024-07-02 02:16:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:06:32 lr 0.000011	 wd 0.0000	time 0.2297 (0.2447)	loss 1.6151 (1.3849)	grad_norm 6.4293 (4.8721)	loss_scale 1024.0000 (849.5450)	mem 8828MB
[2024-07-02 02:16:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:06:05 lr 0.000011	 wd 0.0000	time 0.2418 (0.2436)	loss 1.1228 (1.3838)	grad_norm 5.5573 (4.8511)	loss_scale 1024.0000 (866.9730)	mem 8828MB
[2024-07-02 02:17:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:05:40 lr 0.000011	 wd 0.0000	time 0.2361 (0.2426)	loss 1.4495 (1.3840)	grad_norm 3.4035 (4.8402)	loss_scale 1024.0000 (881.2352)	mem 8828MB
[2024-07-02 02:17:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:05:14 lr 0.000011	 wd 0.0000	time 0.2316 (0.2419)	loss 1.4576 (1.3843)	grad_norm 3.5634 (4.8011)	loss_scale 1024.0000 (893.1224)	mem 8828MB
[2024-07-02 02:17:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:04:49 lr 0.000011	 wd 0.0000	time 0.2309 (0.2413)	loss 1.5652 (1.3838)	grad_norm 4.4037 (4.7667)	loss_scale 1024.0000 (903.1822)	mem 8828MB
[2024-07-02 02:18:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:04:25 lr 0.000011	 wd 0.0000	time 0.2333 (0.2407)	loss 1.3864 (1.3844)	grad_norm 4.2318 (4.7469)	loss_scale 1024.0000 (911.8059)	mem 8828MB
[2024-07-02 02:18:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:04:00 lr 0.000010	 wd 0.0000	time 0.2363 (0.2403)	loss 1.7183 (1.3840)	grad_norm 4.8897 (4.8464)	loss_scale 1024.0000 (919.2805)	mem 8828MB
[2024-07-02 02:19:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:03:36 lr 0.000010	 wd 0.0000	time 0.2332 (0.2399)	loss 1.3141 (1.3834)	grad_norm 4.8385 (4.8273)	loss_scale 1024.0000 (925.8214)	mem 8828MB
[2024-07-02 02:19:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:03:12 lr 0.000010	 wd 0.0000	time 0.2319 (0.2396)	loss 1.5212 (1.3840)	grad_norm 4.7928 (4.8249)	loss_scale 1024.0000 (931.5932)	mem 8828MB
[2024-07-02 02:19:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:02:47 lr 0.000010	 wd 0.0000	time 0.2354 (0.2392)	loss 1.1149 (1.3849)	grad_norm 10.2907 (4.8283)	loss_scale 1024.0000 (936.7240)	mem 8828MB
[2024-07-02 02:20:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:23 lr 0.000010	 wd 0.0000	time 0.2303 (0.2390)	loss 1.0728 (1.3833)	grad_norm 4.0374 (4.8110)	loss_scale 1024.0000 (941.3151)	mem 8828MB
[2024-07-02 02:20:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:01:59 lr 0.000010	 wd 0.0000	time 0.2316 (0.2387)	loss 1.4355 (1.3828)	grad_norm 3.6047 (4.8056)	loss_scale 1024.0000 (945.4473)	mem 8828MB
[2024-07-02 02:21:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:35 lr 0.000010	 wd 0.0000	time 0.2355 (0.2385)	loss 1.2192 (1.3846)	grad_norm 5.0373 (4.8034)	loss_scale 1024.0000 (949.1861)	mem 8828MB
[2024-07-02 02:21:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:11 lr 0.000010	 wd 0.0000	time 0.2329 (0.2383)	loss 1.6674 (1.3829)	grad_norm 2.7179 (4.7922)	loss_scale 1024.0000 (952.5852)	mem 8828MB
[2024-07-02 02:21:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:48 lr 0.000010	 wd 0.0000	time 0.2329 (0.2382)	loss 1.6486 (1.3820)	grad_norm 4.9602 (4.7713)	loss_scale 1024.0000 (955.6888)	mem 8828MB
[2024-07-02 02:22:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:24 lr 0.000010	 wd 0.0000	time 0.2223 (0.2380)	loss 1.3810 (1.3833)	grad_norm 3.0955 (4.7661)	loss_scale 1024.0000 (958.5339)	mem 8828MB
[2024-07-02 02:22:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.2292 (0.2378)	loss 1.4851 (1.3827)	grad_norm 3.6477 (4.7483)	loss_scale 1024.0000 (961.1515)	mem 8828MB
[2024-07-02 02:22:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 21 training takes 0:09:57
[2024-07-02 02:22:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.829 (10.829)	Loss 0.4209 (0.4209)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 02:23:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.720 Acc@5 97.116
[2024-07-02 02:23:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 02:23:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.72%
[2024-07-02 02:23:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 02:23:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 02:23:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][0/2502]	eta 6:43:52 lr 0.000010	 wd 0.0000	time 9.6851 (9.6851)	loss 1.2756 (1.2756)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:23:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:13:15 lr 0.000010	 wd 0.0000	time 0.2299 (0.3313)	loss 1.3906 (1.4041)	grad_norm 4.3007 (4.2902)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:23:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:10:49 lr 0.000009	 wd 0.0000	time 0.2279 (0.2820)	loss 1.0500 (1.3934)	grad_norm 3.1460 (4.3425)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:24:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:09:44 lr 0.000009	 wd 0.0000	time 0.2300 (0.2654)	loss 1.5431 (1.3971)	grad_norm 4.3798 (4.4558)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:24:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:09:00 lr 0.000009	 wd 0.0000	time 0.2298 (0.2571)	loss 1.3750 (1.4013)	grad_norm 4.3637 (4.4876)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:25:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:08:24 lr 0.000009	 wd 0.0000	time 0.2328 (0.2522)	loss 1.0406 (1.3996)	grad_norm 3.4267 (4.5623)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:25:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:07:53 lr 0.000009	 wd 0.0000	time 0.2311 (0.2489)	loss 1.4802 (1.3995)	grad_norm 4.5779 (4.5372)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:25:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:07:24 lr 0.000009	 wd 0.0000	time 0.2301 (0.2467)	loss 0.9303 (1.3930)	grad_norm 5.0316 (4.5918)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:26:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:06:56 lr 0.000009	 wd 0.0000	time 0.2312 (0.2449)	loss 1.3366 (1.3883)	grad_norm 4.1612 (4.5688)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:26:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:06:30 lr 0.000009	 wd 0.0000	time 0.2296 (0.2436)	loss 1.3598 (1.3860)	grad_norm 3.3494 (4.5969)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:27:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:06:04 lr 0.000009	 wd 0.0000	time 0.2323 (0.2426)	loss 1.5545 (1.3880)	grad_norm 7.0662 (4.6808)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:27:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:05:38 lr 0.000009	 wd 0.0000	time 0.2298 (0.2417)	loss 1.3897 (1.3862)	grad_norm 5.6110 (4.6458)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 02:27:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:05:13 lr 0.000009	 wd 0.0000	time 0.2312 (0.2410)	loss 1.3479 (1.3853)	grad_norm 4.7279 (inf)	loss_scale 512.0000 (1010.3580)	mem 8828MB
[2024-07-02 02:28:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:04:49 lr 0.000009	 wd 0.0000	time 0.2308 (0.2405)	loss 1.0395 (1.3863)	grad_norm 6.3288 (inf)	loss_scale 512.0000 (972.0523)	mem 8828MB
[2024-07-02 02:28:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:04:24 lr 0.000008	 wd 0.0000	time 0.2339 (0.2400)	loss 1.5632 (1.3881)	grad_norm 3.2444 (inf)	loss_scale 512.0000 (939.2148)	mem 8828MB
[2024-07-02 02:29:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:04:00 lr 0.000008	 wd 0.0000	time 0.2313 (0.2396)	loss 1.2388 (1.3887)	grad_norm 3.6882 (inf)	loss_scale 512.0000 (910.7528)	mem 8828MB
[2024-07-02 02:29:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:03:35 lr 0.000008	 wd 0.0000	time 0.2301 (0.2392)	loss 1.6139 (1.3887)	grad_norm 4.7474 (inf)	loss_scale 512.0000 (885.8463)	mem 8828MB
[2024-07-02 02:29:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:03:11 lr 0.000008	 wd 0.0000	time 0.2313 (0.2389)	loss 1.3218 (1.3879)	grad_norm 4.5173 (inf)	loss_scale 512.0000 (863.8683)	mem 8828MB
[2024-07-02 02:30:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:02:47 lr 0.000008	 wd 0.0000	time 0.2292 (0.2386)	loss 1.6868 (1.3877)	grad_norm 3.5813 (inf)	loss_scale 512.0000 (844.3309)	mem 8828MB
[2024-07-02 02:30:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:23 lr 0.000008	 wd 0.0000	time 0.2366 (0.2384)	loss 1.5618 (1.3868)	grad_norm 3.6385 (inf)	loss_scale 512.0000 (826.8490)	mem 8828MB
[2024-07-02 02:30:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:01:59 lr 0.000008	 wd 0.0000	time 0.2295 (0.2382)	loss 1.5177 (1.3856)	grad_norm 4.0446 (nan)	loss_scale 256.0000 (804.7176)	mem 8828MB
[2024-07-02 02:31:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:35 lr 0.000008	 wd 0.0000	time 0.2311 (0.2380)	loss 1.5142 (1.3862)	grad_norm 7.9197 (nan)	loss_scale 256.0000 (778.6007)	mem 8828MB
[2024-07-02 02:31:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:11 lr 0.000008	 wd 0.0000	time 0.2301 (0.2378)	loss 1.5009 (1.3850)	grad_norm 5.1835 (nan)	loss_scale 256.0000 (754.8569)	mem 8828MB
[2024-07-02 02:32:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:48 lr 0.000008	 wd 0.0000	time 0.2335 (0.2377)	loss 1.6181 (1.3852)	grad_norm 2.6842 (nan)	loss_scale 256.0000 (733.1769)	mem 8828MB
[2024-07-02 02:32:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:24 lr 0.000008	 wd 0.0000	time 0.2301 (0.2375)	loss 1.1362 (1.3847)	grad_norm 3.5485 (nan)	loss_scale 256.0000 (713.3028)	mem 8828MB
[2024-07-02 02:32:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.2295 (0.2374)	loss 1.0719 (1.3836)	grad_norm 4.5127 (nan)	loss_scale 256.0000 (695.0180)	mem 8828MB
[2024-07-02 02:33:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 22 training takes 0:09:58
[2024-07-02 02:33:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.710 (10.710)	Loss 0.4180 (0.4180)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 02:33:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.726 Acc@5 97.140
[2024-07-02 02:33:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 02:33:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.73%
[2024-07-02 02:33:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 02:33:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 02:33:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:00:38 lr 0.000008	 wd 0.0000	time 10.0872 (10.0872)	loss 1.1480 (1.1480)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:34:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:13:26 lr 0.000008	 wd 0.0000	time 0.2290 (0.3359)	loss 1.4890 (1.4270)	grad_norm 3.5706 (4.4321)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:34:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:10:54 lr 0.000007	 wd 0.0000	time 0.2286 (0.2841)	loss 1.5920 (1.4038)	grad_norm 3.1460 (4.8314)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:34:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:09:47 lr 0.000007	 wd 0.0000	time 0.2300 (0.2668)	loss 1.4040 (1.3973)	grad_norm 5.0093 (4.7828)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:35:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:09:02 lr 0.000007	 wd 0.0000	time 0.2323 (0.2582)	loss 1.7221 (1.3921)	grad_norm 5.1766 (4.7127)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:35:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:08:26 lr 0.000007	 wd 0.0000	time 0.2314 (0.2530)	loss 1.6083 (1.3963)	grad_norm 3.5622 (4.7340)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:35:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:07:54 lr 0.000007	 wd 0.0000	time 0.2292 (0.2497)	loss 1.2033 (1.3969)	grad_norm 2.7476 (4.8303)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:36:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:07:25 lr 0.000007	 wd 0.0000	time 0.2301 (0.2473)	loss 1.5352 (1.3934)	grad_norm 4.3362 (4.7998)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:36:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:06:57 lr 0.000007	 wd 0.0000	time 0.2294 (0.2455)	loss 1.7445 (1.3908)	grad_norm 3.3040 (4.8818)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:37:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:06:31 lr 0.000007	 wd 0.0000	time 0.2336 (0.2441)	loss 1.6680 (1.3878)	grad_norm 4.1706 (4.8354)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:37:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:06:04 lr 0.000007	 wd 0.0000	time 0.2300 (0.2430)	loss 1.1203 (1.3870)	grad_norm 4.0370 (4.7910)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:37:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:05:39 lr 0.000007	 wd 0.0000	time 0.2331 (0.2421)	loss 0.8659 (1.3818)	grad_norm 4.3159 (4.8272)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:38:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:05:14 lr 0.000007	 wd 0.0000	time 0.2303 (0.2414)	loss 1.5911 (1.3838)	grad_norm 3.3425 (4.8515)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:38:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:04:49 lr 0.000007	 wd 0.0000	time 0.2344 (0.2407)	loss 1.5724 (1.3851)	grad_norm 4.8468 (4.8386)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:39:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:04:24 lr 0.000007	 wd 0.0000	time 0.2286 (0.2402)	loss 1.3871 (1.3848)	grad_norm 3.3807 (4.9231)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:39:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:04:00 lr 0.000006	 wd 0.0000	time 0.2333 (0.2398)	loss 1.2700 (1.3864)	grad_norm 4.0069 (4.9087)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:39:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:03:35 lr 0.000006	 wd 0.0000	time 0.2331 (0.2394)	loss 1.2543 (1.3861)	grad_norm 4.7156 (4.9002)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:40:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:03:11 lr 0.000006	 wd 0.0000	time 0.2313 (0.2391)	loss 1.4125 (1.3863)	grad_norm 5.8128 (4.8889)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:40:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:02:47 lr 0.000006	 wd 0.0000	time 0.2307 (0.2388)	loss 1.3924 (1.3856)	grad_norm 3.5749 (4.8629)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:41:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:02:23 lr 0.000006	 wd 0.0000	time 0.2310 (0.2386)	loss 1.2252 (1.3881)	grad_norm 5.0014 (4.8436)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:41:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:01:59 lr 0.000006	 wd 0.0000	time 0.2272 (0.2384)	loss 1.4061 (1.3869)	grad_norm 5.1140 (4.8367)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:41:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:35 lr 0.000006	 wd 0.0000	time 0.2304 (0.2382)	loss 1.7002 (1.3870)	grad_norm 3.3463 (4.8456)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:42:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:11 lr 0.000006	 wd 0.0000	time 0.2350 (0.2380)	loss 1.5058 (1.3844)	grad_norm 3.2100 (4.8454)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:42:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:48 lr 0.000006	 wd 0.0000	time 0.2376 (0.2379)	loss 1.5165 (1.3840)	grad_norm 4.2271 (4.8342)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:42:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:24 lr 0.000006	 wd 0.0000	time 0.2308 (0.2377)	loss 1.5682 (1.3847)	grad_norm 3.3892 (4.8287)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:43:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0000	time 0.2297 (0.2375)	loss 1.5162 (1.3856)	grad_norm 3.1745 (4.8464)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:43:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 23 training takes 0:10:01
[2024-07-02 02:43:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.358 (11.358)	Loss 0.4189 (0.4189)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 02:43:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.766 Acc@5 97.144
[2024-07-02 02:43:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-02 02:43:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.77%
[2024-07-02 02:43:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 02:43:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 02:44:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:35:00 lr 0.000006	 wd 0.0000	time 10.9115 (10.9115)	loss 1.6312 (1.6312)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:44:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:13:30 lr 0.000006	 wd 0.0000	time 0.2309 (0.3372)	loss 1.2348 (1.4237)	grad_norm 4.2371 (5.3691)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:44:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:10:55 lr 0.000006	 wd 0.0000	time 0.2289 (0.2849)	loss 1.6624 (1.4103)	grad_norm 4.0968 (5.2894)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:45:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:09:48 lr 0.000006	 wd 0.0000	time 0.2290 (0.2673)	loss 1.6559 (1.3979)	grad_norm 3.4500 (5.0290)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:45:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:09:03 lr 0.000005	 wd 0.0000	time 0.2275 (0.2585)	loss 1.5051 (1.3930)	grad_norm 3.8364 (5.3710)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:46:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:08:26 lr 0.000005	 wd 0.0000	time 0.2293 (0.2532)	loss 1.6249 (1.3947)	grad_norm 3.5656 (5.2538)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:46:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:07:55 lr 0.000005	 wd 0.0000	time 0.2341 (0.2498)	loss 1.4086 (1.3894)	grad_norm 3.7153 (5.0806)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:46:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:07:25 lr 0.000005	 wd 0.0000	time 0.2272 (0.2474)	loss 1.3276 (1.3909)	grad_norm 6.0986 (5.0180)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:47:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:06:57 lr 0.000005	 wd 0.0000	time 0.2300 (0.2455)	loss 1.4405 (1.3921)	grad_norm 4.3615 (4.9740)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:47:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:06:31 lr 0.000005	 wd 0.0000	time 0.2290 (0.2441)	loss 1.5069 (1.3898)	grad_norm 3.6699 (4.9672)	loss_scale 256.0000 (256.0000)	mem 8828MB
[2024-07-02 02:48:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:06:05 lr 0.000005	 wd 0.0000	time 0.2303 (0.2430)	loss 1.3311 (1.3897)	grad_norm 3.4776 (4.9419)	loss_scale 512.0000 (269.8102)	mem 8828MB
[2024-07-02 02:48:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:05:39 lr 0.000005	 wd 0.0000	time 0.2340 (0.2421)	loss 1.5208 (1.3879)	grad_norm 3.9047 (4.8944)	loss_scale 512.0000 (291.8074)	mem 8828MB
[2024-07-02 02:48:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:05:14 lr 0.000005	 wd 0.0000	time 0.2305 (0.2414)	loss 1.1703 (1.3864)	grad_norm 4.5878 (4.8790)	loss_scale 512.0000 (310.1415)	mem 8828MB
[2024-07-02 02:49:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:04:49 lr 0.000005	 wd 0.0000	time 0.2304 (0.2408)	loss 1.2943 (1.3873)	grad_norm 36.7893 (4.8855)	loss_scale 512.0000 (325.6572)	mem 8828MB
[2024-07-02 02:49:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:04:24 lr 0.000005	 wd 0.0000	time 0.2325 (0.2403)	loss 1.2807 (1.3857)	grad_norm 24.1265 (4.8914)	loss_scale 512.0000 (338.9579)	mem 8828MB
[2024-07-02 02:49:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:04:00 lr 0.000005	 wd 0.0000	time 0.2301 (0.2398)	loss 1.0181 (1.3844)	grad_norm 6.1943 (4.8688)	loss_scale 512.0000 (350.4863)	mem 8828MB
[2024-07-02 02:50:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:03:35 lr 0.000005	 wd 0.0000	time 0.2305 (0.2395)	loss 1.4478 (1.3838)	grad_norm 3.3810 (4.8362)	loss_scale 512.0000 (360.5746)	mem 8828MB
[2024-07-02 02:50:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:03:11 lr 0.000005	 wd 0.0000	time 0.2297 (0.2391)	loss 1.7429 (1.3859)	grad_norm 7.1479 (4.8223)	loss_scale 512.0000 (369.4768)	mem 8828MB
[2024-07-02 02:51:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:02:47 lr 0.000005	 wd 0.0000	time 0.2303 (0.2388)	loss 1.2445 (1.3851)	grad_norm 5.8322 (4.8144)	loss_scale 512.0000 (377.3903)	mem 8828MB
[2024-07-02 02:51:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:02:23 lr 0.000005	 wd 0.0000	time 0.2297 (0.2386)	loss 1.5170 (1.3864)	grad_norm 3.1599 (4.8056)	loss_scale 512.0000 (384.4713)	mem 8828MB
[2024-07-02 02:51:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:01:59 lr 0.000004	 wd 0.0000	time 0.2310 (0.2384)	loss 1.5756 (1.3864)	grad_norm 5.1775 (4.7807)	loss_scale 512.0000 (390.8446)	mem 8828MB
[2024-07-02 02:52:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:35 lr 0.000004	 wd 0.0000	time 0.2265 (0.2382)	loss 1.5851 (1.3848)	grad_norm 3.6645 (4.7597)	loss_scale 512.0000 (396.6111)	mem 8828MB
[2024-07-02 02:52:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:11 lr 0.000004	 wd 0.0000	time 0.2326 (0.2380)	loss 1.2996 (1.3843)	grad_norm 3.8817 (4.7459)	loss_scale 512.0000 (401.8537)	mem 8828MB
[2024-07-02 02:53:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:48 lr 0.000004	 wd 0.0000	time 0.2369 (0.2378)	loss 1.7338 (1.3839)	grad_norm 3.5480 (4.7541)	loss_scale 512.0000 (406.6406)	mem 8828MB
[2024-07-02 02:53:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:24 lr 0.000004	 wd 0.0000	time 0.2309 (0.2377)	loss 1.6301 (1.3840)	grad_norm 4.2614 (4.7341)	loss_scale 512.0000 (411.0287)	mem 8828MB
[2024-07-02 02:53:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0000	time 0.2306 (0.2375)	loss 1.2667 (1.3845)	grad_norm 6.5696 (4.7212)	loss_scale 512.0000 (415.0660)	mem 8828MB
[2024-07-02 02:53:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 24 training takes 0:10:01
[2024-07-02 02:54:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.776 (11.776)	Loss 0.4204 (0.4204)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 02:54:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.748 Acc@5 97.108
[2024-07-02 02:54:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-02 02:54:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.77%
[2024-07-02 02:54:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:14:24 lr 0.000004	 wd 0.0000	time 10.4174 (10.4174)	loss 1.1169 (1.1169)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:55:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:13:36 lr 0.000004	 wd 0.0000	time 0.2292 (0.3399)	loss 1.3598 (1.3533)	grad_norm 3.9471 (4.1596)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:55:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:10:58 lr 0.000004	 wd 0.0000	time 0.2327 (0.2863)	loss 1.4776 (1.3754)	grad_norm 3.2735 (4.4407)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:55:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:09:50 lr 0.000004	 wd 0.0000	time 0.2287 (0.2682)	loss 1.1006 (1.3697)	grad_norm 5.3445 (4.6405)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:56:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:09:04 lr 0.000004	 wd 0.0000	time 0.2287 (0.2592)	loss 1.3155 (1.3837)	grad_norm 3.4452 (4.6888)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:56:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:08:28 lr 0.000004	 wd 0.0000	time 0.2327 (0.2538)	loss 1.4812 (1.3845)	grad_norm 3.4095 (4.6979)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:56:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:07:56 lr 0.000004	 wd 0.0000	time 0.2287 (0.2503)	loss 1.2404 (1.3867)	grad_norm 4.9727 (4.6907)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:57:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:07:26 lr 0.000004	 wd 0.0000	time 0.2312 (0.2477)	loss 1.2491 (1.3866)	grad_norm 6.1068 (4.6968)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:57:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:06:58 lr 0.000004	 wd 0.0000	time 0.2295 (0.2458)	loss 1.1248 (1.3869)	grad_norm 3.4921 (4.7047)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:58:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:06:31 lr 0.000004	 wd 0.0000	time 0.2305 (0.2444)	loss 1.4470 (1.3892)	grad_norm 3.7176 (4.6861)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:58:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:06:05 lr 0.000004	 wd 0.0000	time 0.2267 (0.2433)	loss 1.3660 (1.3903)	grad_norm 4.3037 (4.7521)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:58:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:05:39 lr 0.000004	 wd 0.0000	time 0.2300 (0.2424)	loss 1.4176 (1.3881)	grad_norm 7.2421 (4.7748)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:59:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:05:14 lr 0.000004	 wd 0.0000	time 0.2322 (0.2416)	loss 1.3825 (1.3895)	grad_norm 2.8953 (4.7543)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 02:59:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:04:49 lr 0.000003	 wd 0.0000	time 0.2316 (0.2410)	loss 1.6008 (1.3899)	grad_norm 4.7303 (4.7445)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:00:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:04:24 lr 0.000003	 wd 0.0000	time 0.2277 (0.2405)	loss 1.2387 (1.3909)	grad_norm 5.5151 (4.7052)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:00:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:04:00 lr 0.000003	 wd 0.0000	time 0.2328 (0.2400)	loss 1.2816 (1.3901)	grad_norm 5.0253 (4.7019)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:00:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:03:36 lr 0.000003	 wd 0.0000	time 0.2357 (0.2396)	loss 1.5640 (1.3901)	grad_norm 4.0647 (4.6709)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:01:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:03:11 lr 0.000003	 wd 0.0000	time 0.2313 (0.2393)	loss 1.3560 (1.3899)	grad_norm 5.2641 (4.7058)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:01:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:02:47 lr 0.000003	 wd 0.0000	time 0.2305 (0.2390)	loss 1.4848 (1.3889)	grad_norm 4.9181 (4.7364)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:02:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:02:23 lr 0.000003	 wd 0.0000	time 0.2302 (0.2388)	loss 1.6585 (1.3893)	grad_norm 4.4116 (4.7433)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:02:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:01:59 lr 0.000003	 wd 0.0000	time 0.2338 (0.2386)	loss 1.3257 (1.3888)	grad_norm 3.6841 (4.7376)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:02:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:35 lr 0.000003	 wd 0.0000	time 0.2318 (0.2384)	loss 0.9301 (1.3893)	grad_norm 4.2409 (4.7265)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:03:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:11 lr 0.000003	 wd 0.0000	time 0.2319 (0.2382)	loss 1.4397 (1.3881)	grad_norm 3.6916 (4.6968)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:03:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:48 lr 0.000003	 wd 0.0000	time 0.2307 (0.2380)	loss 1.4728 (1.3866)	grad_norm 5.0851 (4.7196)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:03:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:24 lr 0.000003	 wd 0.0000	time 0.2314 (0.2378)	loss 1.4746 (1.3881)	grad_norm 9.3230 (4.7307)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:04:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.2295 (0.2376)	loss 1.5541 (1.3882)	grad_norm 3.5119 (4.7305)	loss_scale 1024.0000 (523.4642)	mem 8828MB
[2024-07-02 03:04:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 25 training takes 0:10:00
[2024-07-02 03:04:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.438 (11.438)	Loss 0.4207 (0.4207)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 03:04:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.784 Acc@5 97.110
[2024-07-02 03:04:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-02 03:04:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.78%
[2024-07-02 03:04:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saving......
[2024-07-02 03:04:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_best.pth saved !!!
[2024-07-02 03:05:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:04:57 lr 0.000003	 wd 0.0000	time 10.1908 (10.1908)	loss 1.2302 (1.2302)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:05:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:13:19 lr 0.000003	 wd 0.0000	time 0.2274 (0.3330)	loss 1.5234 (1.3800)	grad_norm 3.2043 (4.4477)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:05:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:10:51 lr 0.000003	 wd 0.0000	time 0.2283 (0.2828)	loss 1.4853 (1.4080)	grad_norm 3.8823 (4.4466)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:06:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:09:45 lr 0.000003	 wd 0.0000	time 0.2296 (0.2661)	loss 1.3758 (1.4001)	grad_norm 4.2738 (4.6260)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:06:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:09:01 lr 0.000003	 wd 0.0000	time 0.2291 (0.2576)	loss 1.7175 (1.4024)	grad_norm 8.7423 (4.7105)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:07:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:08:25 lr 0.000003	 wd 0.0000	time 0.2323 (0.2526)	loss 1.7055 (1.4093)	grad_norm 3.1513 (4.7498)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:07:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:07:54 lr 0.000003	 wd 0.0000	time 0.2297 (0.2492)	loss 1.2839 (1.4037)	grad_norm 3.4221 (4.7470)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:07:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:07:24 lr 0.000003	 wd 0.0000	time 0.2326 (0.2469)	loss 1.4618 (1.3980)	grad_norm 3.0107 (4.6748)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:08:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:06:57 lr 0.000002	 wd 0.0000	time 0.2349 (0.2452)	loss 1.4730 (1.3945)	grad_norm 6.1255 (4.6423)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:08:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:06:30 lr 0.000002	 wd 0.0000	time 0.2294 (0.2438)	loss 1.3914 (1.3962)	grad_norm 4.7662 (4.7062)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:09:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:06:04 lr 0.000002	 wd 0.0000	time 0.2309 (0.2428)	loss 1.5003 (1.3957)	grad_norm 3.1948 (4.7018)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:09:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:05:39 lr 0.000002	 wd 0.0000	time 0.2300 (0.2419)	loss 1.5518 (1.3976)	grad_norm 3.5771 (4.7569)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:09:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:05:14 lr 0.000002	 wd 0.0000	time 0.2339 (0.2412)	loss 1.1181 (1.3979)	grad_norm 5.3337 (4.7572)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:10:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:04:49 lr 0.000002	 wd 0.0000	time 0.2342 (0.2406)	loss 0.9983 (1.3955)	grad_norm 12.1081 (4.7776)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:10:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:04:24 lr 0.000002	 wd 0.0000	time 0.2308 (0.2401)	loss 1.8182 (1.3956)	grad_norm 5.0841 (4.8082)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:10:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:04:00 lr 0.000002	 wd 0.0000	time 0.2301 (0.2397)	loss 1.5720 (1.3941)	grad_norm 6.4135 (4.8293)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:11:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:03:35 lr 0.000002	 wd 0.0000	time 0.2304 (0.2394)	loss 1.4819 (1.3927)	grad_norm 3.3130 (4.8206)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:11:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:03:11 lr 0.000002	 wd 0.0000	time 0.2301 (0.2391)	loss 1.3778 (1.3931)	grad_norm 4.2590 (4.7969)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:12:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:02:47 lr 0.000002	 wd 0.0000	time 0.2325 (0.2388)	loss 1.5256 (1.3948)	grad_norm 4.4482 (4.7962)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:12:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:02:23 lr 0.000002	 wd 0.0000	time 0.2308 (0.2385)	loss 1.1235 (1.3931)	grad_norm 3.1355 (4.7896)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:12:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:01:59 lr 0.000002	 wd 0.0000	time 0.2300 (0.2383)	loss 1.2154 (1.3927)	grad_norm 11.9350 (4.7955)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:13:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:35 lr 0.000002	 wd 0.0000	time 0.2316 (0.2381)	loss 1.0181 (1.3911)	grad_norm 3.6805 (4.7834)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:13:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:11 lr 0.000002	 wd 0.0000	time 0.2374 (0.2380)	loss 0.9315 (1.3906)	grad_norm 3.5471 (4.8104)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:14:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:48 lr 0.000002	 wd 0.0000	time 0.2300 (0.2378)	loss 1.1096 (1.3895)	grad_norm 6.3242 (4.8218)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:14:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:24 lr 0.000002	 wd 0.0000	time 0.2317 (0.2377)	loss 1.1570 (1.3891)	grad_norm 4.5949 (4.7998)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:14:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.2298 (0.2375)	loss 1.0696 (1.3873)	grad_norm 3.7959 (4.8001)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:14:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 26 training takes 0:09:59
[2024-07-02 03:15:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 11.207 (11.207)	Loss 0.4204 (0.4204)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 8828MB
[2024-07-02 03:15:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.754 Acc@5 97.124
[2024-07-02 03:15:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-02 03:15:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.78%
[2024-07-02 03:15:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:14:40 lr 0.000002	 wd 0.0000	time 10.4237 (10.4237)	loss 0.9620 (0.9620)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:16:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:13:28 lr 0.000002	 wd 0.0000	time 0.2301 (0.3368)	loss 1.5097 (1.4046)	grad_norm 6.3615 (inf)	loss_scale 512.0000 (841.5050)	mem 8828MB
[2024-07-02 03:16:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:10:55 lr 0.000002	 wd 0.0000	time 0.2331 (0.2847)	loss 1.5485 (1.3939)	grad_norm 4.9377 (inf)	loss_scale 512.0000 (677.5721)	mem 8828MB
[2024-07-02 03:16:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:09:48 lr 0.000002	 wd 0.0000	time 0.2304 (0.2672)	loss 1.3378 (1.3873)	grad_norm 4.0798 (inf)	loss_scale 512.0000 (622.5648)	mem 8828MB
[2024-07-02 03:17:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:09:03 lr 0.000002	 wd 0.0000	time 0.2293 (0.2585)	loss 1.3794 (1.3865)	grad_norm 2.7664 (inf)	loss_scale 512.0000 (594.9925)	mem 8828MB
[2024-07-02 03:17:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:08:26 lr 0.000002	 wd 0.0000	time 0.2380 (0.2532)	loss 0.8682 (1.3800)	grad_norm 4.6267 (inf)	loss_scale 512.0000 (578.4271)	mem 8828MB
[2024-07-02 03:17:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:07:54 lr 0.000002	 wd 0.0000	time 0.2332 (0.2497)	loss 1.4359 (1.3802)	grad_norm 7.3424 (inf)	loss_scale 512.0000 (567.3744)	mem 8828MB
[2024-07-02 03:18:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:07:25 lr 0.000002	 wd 0.0000	time 0.2329 (0.2473)	loss 1.4591 (1.3853)	grad_norm 2.9709 (inf)	loss_scale 512.0000 (559.4750)	mem 8828MB
[2024-07-02 03:18:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:06:57 lr 0.000002	 wd 0.0000	time 0.2301 (0.2455)	loss 1.6097 (1.3883)	grad_norm 8.3727 (inf)	loss_scale 512.0000 (553.5481)	mem 8828MB
[2024-07-02 03:19:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:06:31 lr 0.000001	 wd 0.0000	time 0.2296 (0.2441)	loss 1.4665 (1.3930)	grad_norm 4.6948 (inf)	loss_scale 512.0000 (548.9367)	mem 8828MB
[2024-07-02 03:19:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:06:05 lr 0.000001	 wd 0.0000	time 0.2300 (0.2430)	loss 1.4582 (1.3918)	grad_norm 3.6696 (inf)	loss_scale 512.0000 (545.2468)	mem 8828MB
[2024-07-02 03:19:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:05:39 lr 0.000001	 wd 0.0000	time 0.2328 (0.2422)	loss 1.4171 (1.3924)	grad_norm 4.8983 (inf)	loss_scale 512.0000 (542.2271)	mem 8828MB
[2024-07-02 03:20:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:05:14 lr 0.000001	 wd 0.0000	time 0.2305 (0.2414)	loss 1.5509 (1.3927)	grad_norm 4.7321 (inf)	loss_scale 512.0000 (539.7102)	mem 8828MB
[2024-07-02 03:20:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:04:49 lr 0.000001	 wd 0.0000	time 0.2362 (0.2409)	loss 1.4711 (1.3910)	grad_norm 2.7646 (inf)	loss_scale 512.0000 (537.5803)	mem 8828MB
[2024-07-02 03:21:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:04:24 lr 0.000001	 wd 0.0000	time 0.2298 (0.2403)	loss 1.3881 (1.3924)	grad_norm 3.5470 (inf)	loss_scale 512.0000 (535.7545)	mem 8828MB
[2024-07-02 03:21:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:04:00 lr 0.000001	 wd 0.0000	time 0.2333 (0.2399)	loss 1.5546 (1.3922)	grad_norm 5.1873 (inf)	loss_scale 512.0000 (534.1719)	mem 8828MB
[2024-07-02 03:21:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:03:36 lr 0.000001	 wd 0.0000	time 0.2311 (0.2395)	loss 1.5362 (1.3906)	grad_norm 8.2783 (inf)	loss_scale 512.0000 (532.7870)	mem 8828MB
[2024-07-02 03:22:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:03:11 lr 0.000001	 wd 0.0000	time 0.2307 (0.2392)	loss 1.4231 (1.3888)	grad_norm 3.4232 (inf)	loss_scale 512.0000 (531.5650)	mem 8828MB
[2024-07-02 03:22:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:02:47 lr 0.000001	 wd 0.0000	time 0.2334 (0.2389)	loss 1.4708 (1.3896)	grad_norm 3.9049 (inf)	loss_scale 512.0000 (530.4786)	mem 8828MB
[2024-07-02 03:23:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:02:23 lr 0.000001	 wd 0.0000	time 0.2296 (0.2387)	loss 0.9702 (1.3906)	grad_norm 4.9864 (inf)	loss_scale 512.0000 (529.5066)	mem 8828MB
[2024-07-02 03:23:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:01:59 lr 0.000001	 wd 0.0000	time 0.2300 (0.2384)	loss 0.9074 (1.3891)	grad_norm 4.4245 (inf)	loss_scale 512.0000 (528.6317)	mem 8828MB
[2024-07-02 03:23:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:35 lr 0.000001	 wd 0.0000	time 0.2297 (0.2382)	loss 1.8152 (1.3880)	grad_norm 6.4922 (inf)	loss_scale 512.0000 (527.8401)	mem 8828MB
[2024-07-02 03:24:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:11 lr 0.000001	 wd 0.0000	time 0.2317 (0.2381)	loss 1.0140 (1.3891)	grad_norm 8.1493 (inf)	loss_scale 512.0000 (527.1204)	mem 8828MB
[2024-07-02 03:24:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:48 lr 0.000001	 wd 0.0000	time 0.2307 (0.2379)	loss 1.5476 (1.3894)	grad_norm 3.4321 (inf)	loss_scale 512.0000 (526.4633)	mem 8828MB
[2024-07-02 03:24:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:24 lr 0.000001	 wd 0.0000	time 0.2347 (0.2377)	loss 1.3087 (1.3892)	grad_norm 5.6133 (inf)	loss_scale 512.0000 (525.8609)	mem 8828MB
[2024-07-02 03:25:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.2286 (0.2376)	loss 1.5256 (1.3876)	grad_norm 4.9966 (inf)	loss_scale 512.0000 (525.3067)	mem 8828MB
[2024-07-02 03:25:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 27 training takes 0:10:01
[2024-07-02 03:25:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.205 (10.205)	Loss 0.4204 (0.4204)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 03:25:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.768 Acc@5 97.122
[2024-07-02 03:25:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-02 03:25:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.78%
[2024-07-02 03:26:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:40:41 lr 0.000001	 wd 0.0000	time 11.0476 (11.0476)	loss 1.3071 (1.3071)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:26:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:13:36 lr 0.000001	 wd 0.0000	time 0.2299 (0.3400)	loss 1.0160 (1.3688)	grad_norm 7.5901 (4.7016)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:26:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:10:58 lr 0.000001	 wd 0.0000	time 0.2316 (0.2862)	loss 1.1166 (1.3729)	grad_norm 4.9253 (4.7443)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:27:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:09:50 lr 0.000001	 wd 0.0000	time 0.2269 (0.2683)	loss 1.6590 (1.3876)	grad_norm 4.0641 (4.6001)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:27:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:09:05 lr 0.000001	 wd 0.0000	time 0.2298 (0.2593)	loss 1.5572 (1.3930)	grad_norm 2.4910 (4.5379)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:28:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:08:28 lr 0.000001	 wd 0.0000	time 0.2301 (0.2539)	loss 1.6278 (1.3931)	grad_norm 4.6011 (4.5421)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:28:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:07:56 lr 0.000001	 wd 0.0000	time 0.2290 (0.2504)	loss 1.4424 (1.3937)	grad_norm 4.7136 (4.5344)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:28:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:07:26 lr 0.000001	 wd 0.0000	time 0.2305 (0.2478)	loss 1.0752 (1.3964)	grad_norm 3.1935 (4.5134)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:29:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:06:58 lr 0.000001	 wd 0.0000	time 0.2345 (0.2459)	loss 1.6215 (1.3936)	grad_norm 3.0624 (4.5379)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:29:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:06:31 lr 0.000001	 wd 0.0000	time 0.2297 (0.2445)	loss 1.5261 (1.3938)	grad_norm 3.0734 (4.4899)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:30:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:06:05 lr 0.000001	 wd 0.0000	time 0.2321 (0.2434)	loss 1.5002 (1.3948)	grad_norm 7.9463 (4.5647)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:30:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:05:39 lr 0.000001	 wd 0.0000	time 0.2313 (0.2425)	loss 1.0802 (1.3934)	grad_norm 5.2365 (4.5976)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:30:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:05:14 lr 0.000001	 wd 0.0000	time 0.2315 (0.2417)	loss 1.1571 (1.3921)	grad_norm 3.9579 (4.5920)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:31:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:04:49 lr 0.000001	 wd 0.0000	time 0.2302 (0.2411)	loss 1.4968 (1.3892)	grad_norm 3.7621 (4.5688)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:31:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:04:25 lr 0.000001	 wd 0.0000	time 0.2295 (0.2405)	loss 1.0092 (1.3900)	grad_norm 4.4033 (4.5476)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:31:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:04:00 lr 0.000001	 wd 0.0000	time 0.2300 (0.2401)	loss 1.5199 (1.3869)	grad_norm 5.2130 (4.5516)	loss_scale 512.0000 (512.0000)	mem 8828MB
[2024-07-02 03:32:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:03:36 lr 0.000001	 wd 0.0000	time 0.2311 (0.2397)	loss 1.2475 (1.3867)	grad_norm 5.4061 (4.5548)	loss_scale 1024.0000 (524.1524)	mem 8828MB
[2024-07-02 03:32:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:03:11 lr 0.000001	 wd 0.0000	time 0.2279 (0.2394)	loss 1.5085 (1.3880)	grad_norm 4.9993 (4.5687)	loss_scale 1024.0000 (553.5379)	mem 8828MB
[2024-07-02 03:33:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:02:47 lr 0.000001	 wd 0.0000	time 0.2304 (0.2391)	loss 1.4050 (1.3871)	grad_norm 4.4398 (4.5535)	loss_scale 1024.0000 (579.6602)	mem 8828MB
[2024-07-02 03:33:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:02:23 lr 0.000001	 wd 0.0000	time 0.2308 (0.2388)	loss 1.6563 (1.3859)	grad_norm 3.3063 (4.5460)	loss_scale 1024.0000 (603.0342)	mem 8828MB
[2024-07-02 03:33:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:01:59 lr 0.000001	 wd 0.0000	time 0.2295 (0.2386)	loss 0.9173 (1.3851)	grad_norm 5.4065 (4.5760)	loss_scale 1024.0000 (624.0720)	mem 8828MB
[2024-07-02 03:34:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:35 lr 0.000001	 wd 0.0000	time 0.2301 (0.2384)	loss 1.1800 (1.3852)	grad_norm 4.2728 (4.5860)	loss_scale 1024.0000 (643.1071)	mem 8828MB
[2024-07-02 03:34:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:11 lr 0.000001	 wd 0.0000	time 0.2314 (0.2382)	loss 1.5299 (1.3841)	grad_norm 6.7823 (4.5707)	loss_scale 1024.0000 (660.4125)	mem 8828MB
[2024-07-02 03:35:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:48 lr 0.000001	 wd 0.0000	time 0.2303 (0.2381)	loss 1.2613 (1.3844)	grad_norm 7.2378 (4.6370)	loss_scale 1024.0000 (676.2138)	mem 8828MB
[2024-07-02 03:35:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:24 lr 0.000001	 wd 0.0000	time 0.2313 (0.2379)	loss 1.6432 (1.3833)	grad_norm 3.4722 (4.6495)	loss_scale 1024.0000 (690.6989)	mem 8828MB
[2024-07-02 03:35:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.2299 (0.2377)	loss 1.2395 (1.3818)	grad_norm 5.5333 (4.6682)	loss_scale 1024.0000 (704.0256)	mem 8828MB
[2024-07-02 03:35:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 28 training takes 0:10:01
[2024-07-02 03:36:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.907 (10.907)	Loss 0.4207 (0.4207)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 03:36:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.764 Acc@5 97.122
[2024-07-02 03:36:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-02 03:36:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.78%
[2024-07-02 03:36:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:51:57 lr 0.000001	 wd 0.0000	time 11.3179 (11.3179)	loss 1.4756 (1.4756)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:37:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:13:41 lr 0.000001	 wd 0.0000	time 0.2287 (0.3419)	loss 1.5477 (1.3601)	grad_norm 5.3997 (4.3786)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:37:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:11:01 lr 0.000001	 wd 0.0000	time 0.2286 (0.2872)	loss 1.1924 (1.3763)	grad_norm 8.9738 (4.5420)	loss_scale 1024.0000 (1024.0000)	mem 8828MB
[2024-07-02 03:37:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:09:51 lr 0.000001	 wd 0.0000	time 0.2342 (0.2688)	loss 1.5718 (1.3754)	grad_norm 8.5158 (inf)	loss_scale 512.0000 (928.7442)	mem 8828MB
[2024-07-02 03:38:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:09:05 lr 0.000001	 wd 0.0000	time 0.2289 (0.2596)	loss 1.2554 (1.3807)	grad_norm 3.7195 (inf)	loss_scale 512.0000 (824.8180)	mem 8828MB
[2024-07-02 03:38:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:08:28 lr 0.000001	 wd 0.0000	time 0.2288 (0.2542)	loss 0.9502 (1.3856)	grad_norm 4.3330 (inf)	loss_scale 512.0000 (762.3792)	mem 8828MB
[2024-07-02 03:38:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:07:56 lr 0.000000	 wd 0.0000	time 0.2343 (0.2506)	loss 1.4058 (1.3905)	grad_norm 3.7356 (inf)	loss_scale 512.0000 (720.7188)	mem 8828MB
[2024-07-02 03:39:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:07:26 lr 0.000000	 wd 0.0000	time 0.2296 (0.2480)	loss 1.6993 (1.3898)	grad_norm 3.8596 (inf)	loss_scale 512.0000 (690.9444)	mem 8828MB
[2024-07-02 03:39:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:06:58 lr 0.000000	 wd 0.0000	time 0.2313 (0.2461)	loss 0.8931 (1.3891)	grad_norm 4.0187 (inf)	loss_scale 512.0000 (668.6042)	mem 8828MB
[2024-07-02 03:40:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:06:31 lr 0.000000	 wd 0.0000	time 0.2292 (0.2446)	loss 1.3844 (1.3896)	grad_norm 5.3417 (inf)	loss_scale 512.0000 (651.2231)	mem 8828MB
[2024-07-02 03:40:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:06:05 lr 0.000000	 wd 0.0000	time 0.2324 (0.2434)	loss 1.3764 (1.3880)	grad_norm 3.7546 (inf)	loss_scale 512.0000 (637.3147)	mem 8828MB
[2024-07-02 03:40:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:05:39 lr 0.000000	 wd 0.0000	time 0.2291 (0.2425)	loss 1.3652 (1.3882)	grad_norm 5.0331 (inf)	loss_scale 512.0000 (625.9328)	mem 8828MB
[2024-07-02 03:41:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:05:14 lr 0.000000	 wd 0.0000	time 0.2314 (0.2417)	loss 1.4076 (1.3852)	grad_norm 5.7926 (inf)	loss_scale 512.0000 (616.4463)	mem 8828MB
[2024-07-02 03:41:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:04:49 lr 0.000000	 wd 0.0000	time 0.2306 (0.2411)	loss 1.5492 (1.3856)	grad_norm 4.1155 (inf)	loss_scale 512.0000 (608.4181)	mem 8828MB
[2024-07-02 03:42:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:04:25 lr 0.000000	 wd 0.0000	time 0.2293 (0.2406)	loss 1.6269 (1.3829)	grad_norm 8.3613 (inf)	loss_scale 512.0000 (601.5360)	mem 8828MB
[2024-07-02 03:42:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:04:00 lr 0.000000	 wd 0.0000	time 0.2308 (0.2401)	loss 1.3849 (1.3839)	grad_norm 6.5839 (inf)	loss_scale 512.0000 (595.5710)	mem 8828MB
[2024-07-02 03:42:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:03:36 lr 0.000000	 wd 0.0000	time 0.2304 (0.2397)	loss 1.3742 (1.3857)	grad_norm 6.9677 (inf)	loss_scale 512.0000 (590.3510)	mem 8828MB
[2024-07-02 03:43:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:03:11 lr 0.000000	 wd 0.0000	time 0.2299 (0.2394)	loss 1.1948 (1.3834)	grad_norm 6.2394 (inf)	loss_scale 512.0000 (585.7449)	mem 8828MB
[2024-07-02 03:43:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:02:47 lr 0.000000	 wd 0.0000	time 0.2318 (0.2391)	loss 1.1873 (1.3853)	grad_norm 20.0527 (inf)	loss_scale 512.0000 (581.6502)	mem 8828MB
[2024-07-02 03:44:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:02:23 lr 0.000000	 wd 0.0000	time 0.2315 (0.2388)	loss 1.3276 (1.3846)	grad_norm 6.4376 (inf)	loss_scale 512.0000 (577.9863)	mem 8828MB
[2024-07-02 03:44:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:01:59 lr 0.000000	 wd 0.0000	time 0.2248 (0.2386)	loss 1.6081 (1.3843)	grad_norm 3.9454 (inf)	loss_scale 512.0000 (574.6887)	mem 8828MB
[2024-07-02 03:44:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:35 lr 0.000000	 wd 0.0000	time 0.2307 (0.2384)	loss 1.4165 (1.3850)	grad_norm 3.1651 (inf)	loss_scale 512.0000 (571.7049)	mem 8828MB
[2024-07-02 03:45:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:11 lr 0.000000	 wd 0.0000	time 0.2293 (0.2382)	loss 1.5140 (1.3867)	grad_norm 4.1977 (inf)	loss_scale 512.0000 (568.9923)	mem 8828MB
[2024-07-02 03:45:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:48 lr 0.000000	 wd 0.0000	time 0.2310 (0.2380)	loss 1.5359 (1.3877)	grad_norm 3.5504 (inf)	loss_scale 512.0000 (566.5154)	mem 8828MB
[2024-07-02 03:45:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:24 lr 0.000000	 wd 0.0000	time 0.2312 (0.2379)	loss 1.5609 (1.3884)	grad_norm 4.6477 (inf)	loss_scale 512.0000 (564.2449)	mem 8828MB
[2024-07-02 03:46:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0000	time 0.2292 (0.2377)	loss 1.4781 (1.3879)	grad_norm 6.5577 (inf)	loss_scale 512.0000 (562.1559)	mem 8828MB
[2024-07-02 03:46:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 249): INFO EPOCH 29 training takes 0:10:00
[2024-07-02 03:46:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_29.pth saving......
[2024-07-02 03:46:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune/ckpt_epoch_29.pth saved !!!
[2024-07-02 03:46:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 289): INFO Test: [0/98]	Time 10.441 (10.441)	Loss 0.4202 (0.4202)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 8828MB
[2024-07-02 03:46:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 296): INFO  * Acc@1 83.750 Acc@5 97.116
[2024-07-02 03:46:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-02 03:46:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 182): INFO Max accuracy: 83.78%
[2024-07-02 03:46:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_process0] (main.py 189): INFO Training time 5:11:27
