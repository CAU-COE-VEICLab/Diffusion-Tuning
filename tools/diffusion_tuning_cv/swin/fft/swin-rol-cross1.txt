[2024-07-03 08:44:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/config.json
[2024-07-03 08:44:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_cross_layer_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune_crosslayer_process0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-03 08:44:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/swin/diffusion_ft_swin_base_patch4_window7_224_22kto1k_finetune_crosslayer_proces2-fullfinetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_cross_layer_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune_crosslayer_process0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-03 08:44:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 108): INFO Creating model:swin_diffusion_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune
[2024-07-03 08:44:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 110): INFO SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-03 08:44:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 113): INFO number of params: 87768224
[2024-07-03 08:44:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-03 08:44:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune, ignoring auto resume
[2024-07-03 08:44:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_cross_layer_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune_crosslayer_process0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-03 08:44:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=[])
[2024-07-03 08:44:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_cross_layer_process0/diffusion_ft_swin_base_patch4_22kto1k_finetune_crosslayer_process0/ckpt_epoch_best.pth'
[2024-07-03 08:44:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.823 (17.823)	Loss 0.4043 (0.4043)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 1731MB
[2024-07-03 08:44:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.760 Acc@5 97.500
[2024-07-03 08:44:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-03 08:44:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 168): INFO Start training
[2024-07-03 08:45:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 11:20:38 lr 0.000000	 wd 0.0000	time 16.3225 (16.3225)	loss 1.5222 (1.5222)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 11179MB
[2024-07-03 08:45:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:20:36 lr 0.000000	 wd 0.0000	time 0.3188 (0.5147)	loss 1.3553 (1.3515)	grad_norm 5.1409 (nan)	loss_scale 4096.0000 (5839.8416)	mem 12173MB
[2024-07-03 08:46:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:16:30 lr 0.000001	 wd 0.0000	time 0.3108 (0.4305)	loss 1.2587 (1.3344)	grad_norm 4.6406 (nan)	loss_scale 2048.0000 (4340.5373)	mem 12173MB
[2024-07-03 08:46:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:14:40 lr 0.000001	 wd 0.0000	time 0.3325 (0.3997)	loss 1.2667 (1.3169)	grad_norm 5.9819 (nan)	loss_scale 2048.0000 (3578.8970)	mem 12173MB
[2024-07-03 08:47:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:13:42 lr 0.000001	 wd 0.0000	time 0.3182 (0.3913)	loss 1.8079 (1.3190)	grad_norm 4.0650 (nan)	loss_scale 2048.0000 (3197.1272)	mem 12173MB
[2024-07-03 08:48:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:12:44 lr 0.000002	 wd 0.0000	time 0.3408 (0.3818)	loss 1.5174 (1.3194)	grad_norm 5.6622 (nan)	loss_scale 2048.0000 (2967.7605)	mem 12173MB
[2024-07-03 08:48:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:11:57 lr 0.000002	 wd 0.0000	time 0.3130 (0.3773)	loss 0.9391 (1.3235)	grad_norm 3.8319 (nan)	loss_scale 2048.0000 (2814.7221)	mem 12173MB
[2024-07-03 08:49:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:11:15 lr 0.000002	 wd 0.0000	time 0.2951 (0.3747)	loss 1.4045 (1.3219)	grad_norm 3.4333 (nan)	loss_scale 2048.0000 (2705.3466)	mem 12173MB
[2024-07-03 08:49:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:10:30 lr 0.000003	 wd 0.0000	time 0.3323 (0.3704)	loss 1.5134 (1.3220)	grad_norm 4.4409 (nan)	loss_scale 2048.0000 (2623.2809)	mem 12173MB
[2024-07-03 08:50:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:09:53 lr 0.000003	 wd 0.0000	time 0.3151 (0.3705)	loss 1.5159 (1.3174)	grad_norm 4.4765 (nan)	loss_scale 2048.0000 (2559.4317)	mem 12173MB
[2024-07-03 08:50:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:09:13 lr 0.000003	 wd 0.0000	time 0.3289 (0.3683)	loss 1.3226 (1.3167)	grad_norm 8.3830 (nan)	loss_scale 2048.0000 (2508.3397)	mem 12173MB
[2024-07-03 08:51:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:08:33 lr 0.000004	 wd 0.0000	time 0.3247 (0.3661)	loss 1.4715 (1.3176)	grad_norm 6.0145 (nan)	loss_scale 2048.0000 (2466.5286)	mem 12173MB
[2024-07-03 08:52:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:07:54 lr 0.000004	 wd 0.0000	time 0.3224 (0.3647)	loss 1.2695 (1.3207)	grad_norm 5.6027 (nan)	loss_scale 2048.0000 (2431.6803)	mem 12173MB
[2024-07-03 08:52:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:07:16 lr 0.000004	 wd 0.0000	time 0.3161 (0.3635)	loss 1.4711 (1.3225)	grad_norm 5.4518 (nan)	loss_scale 2048.0000 (2402.1891)	mem 12173MB
[2024-07-03 08:53:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:38 lr 0.000005	 wd 0.0000	time 0.3096 (0.3616)	loss 1.5108 (1.3232)	grad_norm 4.5392 (nan)	loss_scale 2048.0000 (2376.9079)	mem 12173MB
[2024-07-03 08:53:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:06:01 lr 0.000005	 wd 0.0000	time 0.3141 (0.3611)	loss 1.3036 (1.3225)	grad_norm 8.8091 (nan)	loss_scale 2048.0000 (2354.9953)	mem 12173MB
[2024-07-03 08:54:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:05:24 lr 0.000005	 wd 0.0000	time 0.3092 (0.3599)	loss 1.6679 (1.3234)	grad_norm 5.4542 (nan)	loss_scale 1024.0000 (2287.2105)	mem 12173MB
[2024-07-03 08:55:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:47 lr 0.000005	 wd 0.0000	time 0.3215 (0.3586)	loss 1.5319 (1.3228)	grad_norm 5.6715 (nan)	loss_scale 1024.0000 (2212.9477)	mem 12173MB
[2024-07-03 08:55:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:04:11 lr 0.000006	 wd 0.0000	time 0.3034 (0.3582)	loss 1.1555 (1.3229)	grad_norm 4.3769 (nan)	loss_scale 1024.0000 (2146.9317)	mem 12173MB
[2024-07-03 08:56:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:35 lr 0.000006	 wd 0.0000	time 0.3090 (0.3573)	loss 1.3719 (1.3219)	grad_norm 12.7419 (nan)	loss_scale 1024.0000 (2087.8611)	mem 12173MB
[2024-07-03 08:56:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:58 lr 0.000006	 wd 0.0000	time 0.3091 (0.3562)	loss 1.4916 (1.3196)	grad_norm 18.3690 (nan)	loss_scale 1024.0000 (2034.6947)	mem 12173MB
[2024-07-03 08:57:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:23 lr 0.000007	 wd 0.0000	time 0.3334 (0.3558)	loss 1.3464 (1.3207)	grad_norm 4.6217 (nan)	loss_scale 1024.0000 (1986.5892)	mem 12173MB
[2024-07-03 08:57:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:47 lr 0.000007	 wd 0.0000	time 0.3261 (0.3551)	loss 1.4447 (1.3208)	grad_norm 3.9941 (nan)	loss_scale 1024.0000 (1942.8551)	mem 12173MB
[2024-07-03 08:58:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:11 lr 0.000007	 wd 0.0000	time 0.3161 (0.3542)	loss 1.3285 (1.3195)	grad_norm 7.4990 (nan)	loss_scale 1024.0000 (1902.9222)	mem 12173MB
[2024-07-03 08:59:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:36 lr 0.000008	 wd 0.0000	time 0.3992 (0.3544)	loss 1.4308 (1.3198)	grad_norm 4.9231 (nan)	loss_scale 1024.0000 (1866.3157)	mem 12173MB
[2024-07-03 08:59:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.2985 (0.3534)	loss 1.5497 (1.3200)	grad_norm 4.3878 (nan)	loss_scale 1024.0000 (1832.6365)	mem 12173MB
[2024-07-03 08:59:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 0 training takes 0:14:47
[2024-07-03 08:59:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_0.pth saving......
[2024-07-03 08:59:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_0.pth saved !!!
[2024-07-03 08:59:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 15.095 (15.095)	Loss 0.4045 (0.4045)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 12173MB
[2024-07-03 09:00:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.772 Acc@5 97.486
[2024-07-03 09:00:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-03 09:00:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 09:00:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 09:00:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 09:00:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][0/2502]	eta 10:53:58 lr 0.000008	 wd 0.0000	time 15.6828 (15.6828)	loss 1.1965 (1.1965)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:01:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:20:00 lr 0.000008	 wd 0.0000	time 0.3341 (0.4997)	loss 1.1406 (1.3462)	grad_norm 4.7665 (5.4998)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:01:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:16:04 lr 0.000009	 wd 0.0000	time 0.3113 (0.4189)	loss 1.3342 (1.3521)	grad_norm 4.1308 (5.4741)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:02:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:14:29 lr 0.000009	 wd 0.0000	time 0.3200 (0.3948)	loss 1.6703 (1.3390)	grad_norm 4.3800 (5.3924)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:02:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:13:35 lr 0.000009	 wd 0.0000	time 0.3078 (0.3881)	loss 0.8709 (1.3247)	grad_norm nan (nan)	loss_scale 512.0000 (1021.4464)	mem 12173MB
[2024-07-03 09:03:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:12:37 lr 0.000010	 wd 0.0000	time 0.3129 (0.3786)	loss 1.5823 (1.3231)	grad_norm 6.5208 (nan)	loss_scale 512.0000 (919.7605)	mem 12173MB
[2024-07-03 09:03:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:11:49 lr 0.000010	 wd 0.0000	time 0.2905 (0.3728)	loss 1.3209 (1.3199)	grad_norm 9.3375 (nan)	loss_scale 512.0000 (851.9135)	mem 12173MB
[2024-07-03 09:04:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:11:03 lr 0.000010	 wd 0.0000	time 0.3274 (0.3683)	loss 1.4631 (1.3190)	grad_norm 5.3761 (nan)	loss_scale 512.0000 (803.4237)	mem 12173MB
[2024-07-03 09:05:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:10:22 lr 0.000011	 wd 0.0000	time 0.3293 (0.3655)	loss 1.4691 (1.3219)	grad_norm 3.4011 (nan)	loss_scale 512.0000 (767.0412)	mem 12173MB
[2024-07-03 09:05:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:09:42 lr 0.000011	 wd 0.0000	time 0.2892 (0.3637)	loss 1.5086 (1.3208)	grad_norm 4.3320 (nan)	loss_scale 512.0000 (738.7347)	mem 12173MB
[2024-07-03 09:06:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:09:03 lr 0.000011	 wd 0.0000	time 0.3467 (0.3617)	loss 1.4079 (1.3189)	grad_norm 5.8301 (nan)	loss_scale 512.0000 (716.0839)	mem 12173MB
[2024-07-03 09:06:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:08:26 lr 0.000012	 wd 0.0000	time 0.3238 (0.3612)	loss 1.0875 (1.3195)	grad_norm 2.9380 (nan)	loss_scale 512.0000 (697.5477)	mem 12173MB
[2024-07-03 09:07:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:48 lr 0.000012	 wd 0.0000	time 0.3371 (0.3595)	loss 1.3478 (1.3221)	grad_norm 3.7738 (nan)	loss_scale 512.0000 (682.0983)	mem 12173MB
[2024-07-03 09:08:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:07:10 lr 0.000012	 wd 0.0000	time 0.3160 (0.3579)	loss 1.4498 (1.3234)	grad_norm 6.3734 (nan)	loss_scale 512.0000 (669.0238)	mem 12173MB
[2024-07-03 09:08:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:06:36 lr 0.000012	 wd 0.0000	time 0.3139 (0.3598)	loss 1.4428 (1.3218)	grad_norm 4.6593 (nan)	loss_scale 512.0000 (657.8158)	mem 12173MB
[2024-07-03 09:09:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:59 lr 0.000013	 wd 0.0000	time 0.3215 (0.3588)	loss 0.8930 (1.3200)	grad_norm 7.7605 (nan)	loss_scale 512.0000 (648.1013)	mem 12173MB
[2024-07-03 09:09:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:05:22 lr 0.000013	 wd 0.0000	time 0.3135 (0.3576)	loss 0.9142 (1.3191)	grad_norm 3.6440 (nan)	loss_scale 512.0000 (639.6002)	mem 12173MB
[2024-07-03 09:10:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:46 lr 0.000013	 wd 0.0000	time 0.3030 (0.3577)	loss 1.3506 (1.3190)	grad_norm 5.6926 (nan)	loss_scale 512.0000 (632.0988)	mem 12173MB
[2024-07-03 09:10:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:04:10 lr 0.000014	 wd 0.0000	time 0.3153 (0.3566)	loss 1.2715 (1.3184)	grad_norm 3.3074 (nan)	loss_scale 512.0000 (625.4303)	mem 12173MB
[2024-07-03 09:11:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:34 lr 0.000014	 wd 0.0000	time 0.3343 (0.3556)	loss 1.3558 (1.3193)	grad_norm 3.9813 (nan)	loss_scale 512.0000 (619.4634)	mem 12173MB
[2024-07-03 09:12:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:59 lr 0.000014	 wd 0.0000	time 0.3123 (0.3566)	loss 1.4031 (1.3191)	grad_norm 5.3259 (nan)	loss_scale 512.0000 (614.0930)	mem 12173MB
[2024-07-03 09:12:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:23 lr 0.000015	 wd 0.0000	time 0.3376 (0.3559)	loss 1.4619 (1.3201)	grad_norm 4.3289 (nan)	loss_scale 512.0000 (609.2337)	mem 12173MB
[2024-07-03 09:13:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:47 lr 0.000015	 wd 0.0000	time 0.4545 (0.3554)	loss 1.0418 (1.3209)	grad_norm 5.9924 (nan)	loss_scale 512.0000 (604.8160)	mem 12173MB
[2024-07-03 09:13:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:11 lr 0.000015	 wd 0.0000	time 0.2989 (0.3549)	loss 1.3712 (1.3213)	grad_norm 5.9623 (nan)	loss_scale 512.0000 (600.7823)	mem 12173MB
[2024-07-03 09:14:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:36 lr 0.000016	 wd 0.0000	time 0.3196 (0.3542)	loss 1.4671 (1.3197)	grad_norm 4.5890 (nan)	loss_scale 512.0000 (597.0845)	mem 12173MB
[2024-07-03 09:14:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.3017 (0.3534)	loss 1.0657 (1.3211)	grad_norm 4.6606 (nan)	loss_scale 512.0000 (593.6825)	mem 12173MB
[2024-07-03 09:15:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 1 training takes 0:14:57
[2024-07-03 09:15:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.500 (16.500)	Loss 0.4141 (0.4141)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12173MB
[2024-07-03 09:15:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.650 Acc@5 97.470
[2024-07-03 09:15:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 09:15:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 09:15:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][0/2502]	eta 11:55:17 lr 0.000016	 wd 0.0000	time 17.1534 (17.1534)	loss 1.4111 (1.4111)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:16:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:20:15 lr 0.000016	 wd 0.0000	time 0.3120 (0.5061)	loss 1.3933 (1.2984)	grad_norm 5.1893 (5.5301)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:17:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:16:25 lr 0.000017	 wd 0.0000	time 0.3256 (0.4280)	loss 1.4336 (1.3153)	grad_norm 4.1090 (5.4313)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:17:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:14:34 lr 0.000017	 wd 0.0000	time 0.3206 (0.3973)	loss 1.1315 (1.3185)	grad_norm 3.7421 (5.3610)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:18:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:13:22 lr 0.000017	 wd 0.0000	time 0.2979 (0.3818)	loss 1.4544 (1.3146)	grad_norm 6.2399 (5.3694)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:18:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:12:28 lr 0.000018	 wd 0.0000	time 0.3620 (0.3737)	loss 1.3834 (1.3171)	grad_norm 11.4574 (5.4400)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:19:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:11:40 lr 0.000018	 wd 0.0000	time 0.3176 (0.3685)	loss 1.2260 (1.3113)	grad_norm 5.9396 (5.3068)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:19:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:10:57 lr 0.000018	 wd 0.0000	time 0.3193 (0.3646)	loss 1.1968 (1.3149)	grad_norm 4.4297 (5.3145)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:20:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:10:15 lr 0.000019	 wd 0.0000	time 0.3090 (0.3617)	loss 1.4796 (1.3130)	grad_norm 4.3094 (5.2745)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:21:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:09:40 lr 0.000019	 wd 0.0000	time 0.3325 (0.3626)	loss 1.4252 (1.3182)	grad_norm 5.4651 (5.3065)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:21:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:09:02 lr 0.000019	 wd 0.0000	time 0.3241 (0.3610)	loss 1.4037 (1.3197)	grad_norm 5.7371 (5.2810)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:22:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:08:23 lr 0.000020	 wd 0.0000	time 0.2958 (0.3592)	loss 1.1452 (1.3209)	grad_norm 4.4051 (5.2993)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:22:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:07:45 lr 0.000020	 wd 0.0000	time 0.3203 (0.3574)	loss 1.2763 (1.3202)	grad_norm 5.5187 (5.3314)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:23:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:07:08 lr 0.000020	 wd 0.0000	time 0.3226 (0.3563)	loss 1.6119 (1.3227)	grad_norm 4.0508 (5.3057)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:23:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:06:31 lr 0.000020	 wd 0.0000	time 0.3252 (0.3551)	loss 1.3658 (1.3233)	grad_norm 4.3021 (5.3765)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:24:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:55 lr 0.000021	 wd 0.0000	time 0.3095 (0.3548)	loss 1.4941 (1.3216)	grad_norm 3.8019 (5.3836)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:25:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:05:21 lr 0.000021	 wd 0.0000	time 0.3247 (0.3562)	loss 1.1991 (1.3217)	grad_norm 5.2215 (5.4328)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:25:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:45 lr 0.000021	 wd 0.0000	time 0.3172 (0.3554)	loss 1.2618 (1.3207)	grad_norm 4.8952 (5.4024)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:26:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:04:09 lr 0.000022	 wd 0.0000	time 0.3162 (0.3556)	loss 1.2667 (1.3207)	grad_norm 11.3277 (5.3984)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:26:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:34 lr 0.000022	 wd 0.0000	time 0.3258 (0.3560)	loss 0.9293 (1.3191)	grad_norm 5.0513 (5.4066)	loss_scale 1024.0000 (513.0773)	mem 12173MB
[2024-07-03 09:27:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:58 lr 0.000022	 wd 0.0000	time 0.3020 (0.3552)	loss 1.1223 (1.3170)	grad_norm 4.0759 (5.4237)	loss_scale 1024.0000 (538.6107)	mem 12173MB
[2024-07-03 09:28:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:23 lr 0.000023	 wd 0.0000	time 0.3099 (0.3558)	loss 1.4021 (1.3174)	grad_norm 4.4311 (5.4224)	loss_scale 1024.0000 (561.7135)	mem 12173MB
[2024-07-03 09:28:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:47 lr 0.000023	 wd 0.0000	time 0.3336 (0.3555)	loss 1.5552 (1.3170)	grad_norm 5.3061 (5.4293)	loss_scale 1024.0000 (582.7169)	mem 12173MB
[2024-07-03 09:29:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:11 lr 0.000023	 wd 0.0000	time 0.3075 (0.3548)	loss 1.6028 (1.3170)	grad_norm 11.4007 (5.4274)	loss_scale 1024.0000 (601.8948)	mem 12173MB
[2024-07-03 09:29:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:36 lr 0.000024	 wd 0.0000	time 0.3292 (0.3543)	loss 1.4311 (1.3163)	grad_norm 4.0308 (5.4057)	loss_scale 1024.0000 (619.4752)	mem 12173MB
[2024-07-03 09:30:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.3012 (0.3537)	loss 1.4765 (1.3164)	grad_norm 4.4784 (5.4065)	loss_scale 1024.0000 (635.6497)	mem 12173MB
[2024-07-03 09:30:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 2 training takes 0:14:50
[2024-07-03 09:30:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.624 (17.624)	Loss 0.4067 (0.4067)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 09:30:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.604 Acc@5 97.448
[2024-07-03 09:30:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-03 09:30:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 09:31:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][0/2502]	eta 11:11:53 lr 0.000024	 wd 0.0000	time 16.1127 (16.1127)	loss 0.9388 (0.9388)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:31:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:20:34 lr 0.000024	 wd 0.0000	time 0.3222 (0.5139)	loss 1.4271 (1.2878)	grad_norm 4.4250 (5.1748)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:32:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:16:26 lr 0.000025	 wd 0.0000	time 0.3163 (0.4284)	loss 1.3608 (1.3041)	grad_norm 4.3068 (5.1187)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:33:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:14:38 lr 0.000025	 wd 0.0000	time 0.3552 (0.3989)	loss 1.5216 (1.2993)	grad_norm 4.4859 (5.3211)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:33:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:13:32 lr 0.000025	 wd 0.0000	time 0.3032 (0.3865)	loss 1.4782 (1.3027)	grad_norm 4.1008 (5.3400)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:34:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:12:36 lr 0.000026	 wd 0.0000	time 0.3110 (0.3780)	loss 1.3679 (1.3016)	grad_norm 6.5475 (5.2984)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:34:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:11:49 lr 0.000026	 wd 0.0000	time 0.3356 (0.3733)	loss 1.3349 (1.3032)	grad_norm 4.7971 (5.3808)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:35:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:11:07 lr 0.000026	 wd 0.0000	time 0.3355 (0.3706)	loss 1.6293 (1.3051)	grad_norm 5.4209 (5.3972)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:35:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:10:24 lr 0.000027	 wd 0.0000	time 0.3103 (0.3671)	loss 1.0578 (1.3053)	grad_norm 4.4873 (5.3445)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:36:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:09:44 lr 0.000027	 wd 0.0000	time 0.2878 (0.3651)	loss 1.4212 (1.3063)	grad_norm 4.8017 (5.3362)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:37:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:09:05 lr 0.000027	 wd 0.0000	time 0.3167 (0.3630)	loss 1.4130 (1.3088)	grad_norm 3.2263 (5.3215)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:37:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:08:25 lr 0.000028	 wd 0.0000	time 0.3213 (0.3607)	loss 1.3065 (1.3115)	grad_norm 4.2466 (5.3446)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:38:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:07:47 lr 0.000028	 wd 0.0000	time 0.3172 (0.3593)	loss 1.1956 (1.3096)	grad_norm 4.5353 (5.4079)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:38:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:07:10 lr 0.000028	 wd 0.0000	time 0.3528 (0.3580)	loss 1.2518 (1.3122)	grad_norm 4.5868 (5.4204)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:39:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:06:32 lr 0.000028	 wd 0.0000	time 0.3184 (0.3566)	loss 1.2243 (1.3126)	grad_norm 5.1593 (5.4019)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:39:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:56 lr 0.000029	 wd 0.0000	time 0.3587 (0.3561)	loss 1.4357 (1.3144)	grad_norm 3.2146 (5.3934)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:40:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:05:20 lr 0.000029	 wd 0.0000	time 0.3163 (0.3553)	loss 1.4779 (1.3132)	grad_norm 4.4002 (5.3804)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:41:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:44 lr 0.000029	 wd 0.0000	time 0.2781 (0.3544)	loss 1.0825 (1.3124)	grad_norm 3.4752 (5.3664)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:41:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:04:09 lr 0.000030	 wd 0.0000	time 0.3281 (0.3558)	loss 1.4350 (1.3128)	grad_norm 5.0310 (5.3954)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:42:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:33 lr 0.000030	 wd 0.0000	time 0.2940 (0.3549)	loss 1.3898 (1.3122)	grad_norm 4.0918 (5.4214)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:42:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:58 lr 0.000030	 wd 0.0000	time 0.3096 (0.3548)	loss 1.1175 (1.3127)	grad_norm 51.9424 (5.4587)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:43:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:22 lr 0.000031	 wd 0.0000	time 0.3008 (0.3548)	loss 1.0981 (1.3118)	grad_norm 5.5312 (5.4585)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:43:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:46 lr 0.000031	 wd 0.0000	time 0.3346 (0.3540)	loss 1.3112 (1.3125)	grad_norm 8.9718 (5.4616)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 09:44:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:11 lr 0.000031	 wd 0.0000	time 0.3325 (0.3535)	loss 1.4283 (1.3135)	grad_norm 3.6727 (inf)	loss_scale 512.0000 (1020.8848)	mem 12173MB
[2024-07-03 09:45:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:36 lr 0.000032	 wd 0.0000	time 0.3208 (0.3532)	loss 0.9922 (1.3128)	grad_norm 4.3047 (inf)	loss_scale 512.0000 (999.6901)	mem 12173MB
[2024-07-03 09:45:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000032	 wd 0.0000	time 0.2950 (0.3523)	loss 1.3753 (1.3134)	grad_norm 4.3000 (inf)	loss_scale 512.0000 (980.1903)	mem 12173MB
[2024-07-03 09:45:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 3 training takes 0:14:44
[2024-07-03 09:46:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.932 (16.932)	Loss 0.4009 (0.4009)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 09:46:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.654 Acc@5 97.460
[2024-07-03 09:46:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 09:46:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 09:46:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][0/2502]	eta 11:23:38 lr 0.000032	 wd 0.0000	time 16.3945 (16.3945)	loss 1.3821 (1.3821)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:47:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:20:03 lr 0.000032	 wd 0.0000	time 0.3274 (0.5012)	loss 1.1750 (1.3439)	grad_norm 4.4142 (5.4227)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:47:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:16:35 lr 0.000033	 wd 0.0000	time 2.6058 (0.4326)	loss 1.1510 (1.3316)	grad_norm 6.8616 (5.1815)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:48:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:14:46 lr 0.000033	 wd 0.0000	time 0.3100 (0.4026)	loss 1.0713 (1.3311)	grad_norm 4.3193 (5.3490)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:48:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:13:33 lr 0.000033	 wd 0.0000	time 0.2978 (0.3871)	loss 1.3616 (1.3209)	grad_norm 3.2608 (5.3477)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:49:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:12:36 lr 0.000034	 wd 0.0000	time 0.3464 (0.3777)	loss 1.3764 (1.3160)	grad_norm 5.5140 (5.2885)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:50:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:11:53 lr 0.000034	 wd 0.0000	time 0.2770 (0.3750)	loss 1.3763 (1.3109)	grad_norm 5.9896 (5.3377)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:50:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:11:07 lr 0.000034	 wd 0.0000	time 0.3095 (0.3702)	loss 1.2099 (1.3112)	grad_norm 7.6544 (5.4742)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:51:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:10:30 lr 0.000035	 wd 0.0000	time 0.3132 (0.3702)	loss 1.2570 (1.3124)	grad_norm 5.3810 (5.3808)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:51:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:09:48 lr 0.000035	 wd 0.0000	time 0.3220 (0.3673)	loss 1.4364 (1.3128)	grad_norm 4.7083 (5.3256)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:52:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:09:07 lr 0.000035	 wd 0.0000	time 0.3176 (0.3644)	loss 1.4481 (1.3136)	grad_norm 6.6337 (5.3049)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:53:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:08:31 lr 0.000036	 wd 0.0000	time 0.3225 (0.3648)	loss 1.5156 (1.3140)	grad_norm 6.7687 (5.3352)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:53:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:07:52 lr 0.000036	 wd 0.0000	time 0.3108 (0.3631)	loss 0.9792 (1.3134)	grad_norm 9.5992 (5.3250)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:54:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:07:14 lr 0.000036	 wd 0.0000	time 0.3106 (0.3613)	loss 1.5112 (1.3124)	grad_norm 4.6927 (5.3399)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:54:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:06:37 lr 0.000036	 wd 0.0000	time 0.3127 (0.3608)	loss 1.4116 (1.3126)	grad_norm 3.9620 (5.3674)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:55:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:06:00 lr 0.000037	 wd 0.0000	time 0.3200 (0.3597)	loss 1.2532 (1.3156)	grad_norm 3.2037 (5.3561)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:55:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:05:23 lr 0.000037	 wd 0.0000	time 0.3326 (0.3583)	loss 1.1546 (1.3151)	grad_norm 4.2215 (5.3562)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:56:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:50 lr 0.000037	 wd 0.0000	time 0.2965 (0.3620)	loss 1.4801 (1.3147)	grad_norm 5.4017 (5.3736)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:57:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:04:13 lr 0.000038	 wd 0.0000	time 0.3142 (0.3610)	loss 1.5311 (1.3144)	grad_norm 6.0506 (5.3681)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:57:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:37 lr 0.000038	 wd 0.0000	time 0.3185 (0.3608)	loss 1.4810 (1.3143)	grad_norm 4.3432 (5.3594)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:58:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:03:00 lr 0.000038	 wd 0.0000	time 0.3207 (0.3602)	loss 1.5309 (1.3146)	grad_norm 7.0446 (5.3620)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:58:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:24 lr 0.000039	 wd 0.0000	time 0.3514 (0.3595)	loss 1.3496 (1.3146)	grad_norm 4.9614 (5.3394)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 09:59:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:48 lr 0.000039	 wd 0.0000	time 0.3151 (0.3588)	loss 0.9430 (1.3122)	grad_norm 3.5995 (5.3253)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:00:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:12 lr 0.000039	 wd 0.0000	time 0.3083 (0.3580)	loss 1.2149 (1.3131)	grad_norm 4.9562 (5.3275)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:00:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:36 lr 0.000040	 wd 0.0000	time 0.3085 (0.3571)	loss 1.3844 (1.3127)	grad_norm 4.2791 (5.3114)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:01:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.2950 (0.3565)	loss 0.8703 (1.3126)	grad_norm 5.4774 (5.3903)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:01:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 4 training takes 0:15:03
[2024-07-03 10:01:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.567 (16.567)	Loss 0.4126 (0.4126)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 10:01:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.558 Acc@5 97.384
[2024-07-03 10:01:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-03 10:01:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 10:02:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][0/2502]	eta 10:33:58 lr 0.000040	 wd 0.0000	time 15.2031 (15.2031)	loss 1.5151 (1.5151)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:02:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:21:08 lr 0.000040	 wd 0.0000	time 0.3264 (0.5280)	loss 1.2348 (1.3290)	grad_norm 4.7718 (5.2113)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:03:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:16:43 lr 0.000040	 wd 0.0000	time 0.3041 (0.4357)	loss 1.3935 (1.3418)	grad_norm 5.0389 (5.1006)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:03:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:14:46 lr 0.000040	 wd 0.0000	time 0.3075 (0.4028)	loss 1.5570 (1.3279)	grad_norm 4.4153 (5.0484)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:04:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:13:37 lr 0.000040	 wd 0.0000	time 0.3117 (0.3889)	loss 1.0883 (1.3290)	grad_norm 3.8484 (5.0664)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:05:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:12:40 lr 0.000040	 wd 0.0000	time 0.3144 (0.3797)	loss 1.6267 (1.3242)	grad_norm 3.5476 (5.0966)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:05:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:11:48 lr 0.000040	 wd 0.0000	time 0.2984 (0.3727)	loss 1.4413 (1.3275)	grad_norm 9.2073 (5.0678)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:06:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:11:09 lr 0.000040	 wd 0.0000	time 0.2895 (0.3717)	loss 1.4531 (1.3252)	grad_norm 6.3725 (5.0600)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:06:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:10:27 lr 0.000040	 wd 0.0000	time 0.3041 (0.3685)	loss 1.0679 (1.3192)	grad_norm 3.9028 (5.1152)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:07:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:09:49 lr 0.000040	 wd 0.0000	time 0.3431 (0.3677)	loss 0.7998 (1.3181)	grad_norm 4.1100 (5.1235)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:08:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:09:12 lr 0.000040	 wd 0.0000	time 0.3025 (0.3678)	loss 1.3266 (1.3206)	grad_norm 3.3611 (5.1167)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:08:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:08:33 lr 0.000040	 wd 0.0000	time 0.3120 (0.3660)	loss 1.4936 (1.3162)	grad_norm 4.6695 (5.0729)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:09:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:07:57 lr 0.000040	 wd 0.0000	time 0.3385 (0.3667)	loss 1.4206 (1.3156)	grad_norm 4.5350 (5.0660)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:09:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:07:19 lr 0.000040	 wd 0.0000	time 0.2952 (0.3655)	loss 1.4392 (1.3153)	grad_norm 4.8636 (5.0488)	loss_scale 1024.0000 (519.0838)	mem 12173MB
[2024-07-03 10:10:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:06:42 lr 0.000040	 wd 0.0000	time 0.3038 (0.3653)	loss 1.4126 (1.3150)	grad_norm 3.4914 (5.0889)	loss_scale 1024.0000 (555.1235)	mem 12173MB
[2024-07-03 10:11:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:06:05 lr 0.000040	 wd 0.0000	time 0.3497 (0.3646)	loss 1.2748 (1.3150)	grad_norm 4.3490 (5.0907)	loss_scale 1024.0000 (586.3611)	mem 12173MB
[2024-07-03 10:11:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:05:27 lr 0.000040	 wd 0.0000	time 0.2846 (0.3634)	loss 1.4252 (1.3163)	grad_norm 7.0932 (5.0846)	loss_scale 1024.0000 (613.6964)	mem 12173MB
[2024-07-03 10:12:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:50 lr 0.000040	 wd 0.0000	time 0.3151 (0.3619)	loss 1.3800 (1.3167)	grad_norm 5.2105 (5.0730)	loss_scale 1024.0000 (637.8178)	mem 12173MB
[2024-07-03 10:12:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:04:13 lr 0.000040	 wd 0.0000	time 0.3260 (0.3613)	loss 1.0050 (1.3155)	grad_norm 7.5969 (5.0890)	loss_scale 1024.0000 (659.2604)	mem 12173MB
[2024-07-03 10:13:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:36 lr 0.000040	 wd 0.0000	time 0.3213 (0.3603)	loss 1.4634 (1.3174)	grad_norm 5.1500 (5.1414)	loss_scale 1024.0000 (678.4471)	mem 12173MB
[2024-07-03 10:13:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:03:00 lr 0.000040	 wd 0.0000	time 0.3528 (0.3592)	loss 1.3577 (1.3188)	grad_norm 4.4725 (5.1203)	loss_scale 1024.0000 (695.7161)	mem 12173MB
[2024-07-03 10:14:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:24 lr 0.000040	 wd 0.0000	time 0.3005 (0.3592)	loss 1.5097 (1.3198)	grad_norm 4.4817 (5.1182)	loss_scale 1024.0000 (711.3413)	mem 12173MB
[2024-07-03 10:15:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:48 lr 0.000040	 wd 0.0000	time 0.3144 (0.3582)	loss 1.2756 (1.3178)	grad_norm 3.6166 (5.1159)	loss_scale 1024.0000 (725.5466)	mem 12173MB
[2024-07-03 10:15:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:12 lr 0.000040	 wd 0.0000	time 0.3313 (0.3573)	loss 0.8816 (1.3173)	grad_norm 3.3965 (5.1141)	loss_scale 1024.0000 (738.5172)	mem 12173MB
[2024-07-03 10:16:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:36 lr 0.000040	 wd 0.0000	time 0.3013 (0.3565)	loss 1.6024 (1.3175)	grad_norm 4.5291 (5.1330)	loss_scale 1024.0000 (750.4073)	mem 12173MB
[2024-07-03 10:16:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.3015 (0.3559)	loss 1.3845 (1.3174)	grad_norm 3.6469 (5.1489)	loss_scale 1024.0000 (761.3467)	mem 12173MB
[2024-07-03 10:16:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 5 training takes 0:14:54
[2024-07-03 10:17:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 18.549 (18.549)	Loss 0.4197 (0.4197)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 10:17:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.502 Acc@5 97.444
[2024-07-03 10:17:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-03 10:17:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 10:17:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][0/2502]	eta 11:33:45 lr 0.000040	 wd 0.0000	time 16.6367 (16.6367)	loss 1.6953 (1.6953)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:18:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:20:10 lr 0.000040	 wd 0.0000	time 0.3256 (0.5041)	loss 1.1838 (1.3025)	grad_norm 6.0161 (5.1056)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:18:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:16:10 lr 0.000040	 wd 0.0000	time 0.3433 (0.4214)	loss 1.3916 (1.3186)	grad_norm 4.0254 (5.0619)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:19:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:14:48 lr 0.000040	 wd 0.0000	time 0.2923 (0.4037)	loss 1.5178 (1.3239)	grad_norm 5.1257 (5.2473)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:20:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:13:36 lr 0.000040	 wd 0.0000	time 0.3129 (0.3886)	loss 1.3688 (1.3176)	grad_norm 4.5136 (5.1986)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:20:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:12:38 lr 0.000040	 wd 0.0000	time 0.3006 (0.3789)	loss 1.4215 (1.3144)	grad_norm 3.2196 (5.1104)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:21:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:11:52 lr 0.000040	 wd 0.0000	time 0.3249 (0.3748)	loss 1.4755 (1.3162)	grad_norm 3.4596 (5.1294)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:21:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:11:06 lr 0.000040	 wd 0.0000	time 0.3573 (0.3697)	loss 1.3233 (1.3167)	grad_norm 3.6899 (5.1388)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:22:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:10:24 lr 0.000040	 wd 0.0000	time 0.4190 (0.3669)	loss 1.0920 (1.3168)	grad_norm 4.6547 (5.1407)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:22:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:09:47 lr 0.000040	 wd 0.0000	time 0.3101 (0.3670)	loss 1.4758 (1.3186)	grad_norm 3.6480 (5.1260)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:23:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:09:07 lr 0.000040	 wd 0.0000	time 0.3202 (0.3645)	loss 1.2328 (1.3149)	grad_norm 5.0479 (5.1972)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:24:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:08:30 lr 0.000040	 wd 0.0000	time 0.3184 (0.3644)	loss 1.2476 (1.3128)	grad_norm 4.7114 (5.2149)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:24:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:07:52 lr 0.000040	 wd 0.0000	time 0.3253 (0.3631)	loss 1.0578 (1.3138)	grad_norm 8.3656 (5.2468)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:25:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:07:14 lr 0.000040	 wd 0.0000	time 0.3301 (0.3614)	loss 1.4730 (1.3128)	grad_norm 3.7330 (5.3106)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:25:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:06:38 lr 0.000040	 wd 0.0000	time 0.3283 (0.3619)	loss 1.7008 (1.3110)	grad_norm 7.4698 (5.3191)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:26:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:06:01 lr 0.000040	 wd 0.0000	time 0.3216 (0.3606)	loss 1.3969 (1.3133)	grad_norm 3.6526 (5.2885)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:27:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:05:24 lr 0.000040	 wd 0.0000	time 0.2956 (0.3598)	loss 1.3912 (1.3115)	grad_norm 4.7542 (5.2496)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:27:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:48 lr 0.000040	 wd 0.0000	time 0.3097 (0.3602)	loss 1.4924 (1.3114)	grad_norm 4.5105 (5.2268)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:28:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:04:12 lr 0.000040	 wd 0.0000	time 0.2755 (0.3592)	loss 1.0753 (1.3113)	grad_norm 4.4034 (5.2148)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 10:28:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:35 lr 0.000040	 wd 0.0000	time 0.3203 (0.3585)	loss 1.2138 (1.3114)	grad_norm 3.3236 (nan)	loss_scale 512.0000 (999.2215)	mem 12173MB
[2024-07-03 10:29:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:03:00 lr 0.000039	 wd 0.0000	time 0.2949 (0.3605)	loss 1.0534 (1.3119)	grad_norm 5.0456 (nan)	loss_scale 512.0000 (974.8726)	mem 12173MB
[2024-07-03 10:30:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:24 lr 0.000039	 wd 0.0000	time 0.3053 (0.3597)	loss 1.4527 (1.3112)	grad_norm 3.8375 (nan)	loss_scale 512.0000 (952.8415)	mem 12173MB
[2024-07-03 10:30:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:48 lr 0.000039	 wd 0.0000	time 0.3171 (0.3591)	loss 1.2166 (1.3112)	grad_norm 4.4742 (nan)	loss_scale 512.0000 (932.8124)	mem 12173MB
[2024-07-03 10:31:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:12 lr 0.000039	 wd 0.0000	time 0.3112 (0.3582)	loss 1.4399 (1.3118)	grad_norm 3.5314 (nan)	loss_scale 512.0000 (914.5241)	mem 12173MB
[2024-07-03 10:31:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:36 lr 0.000039	 wd 0.0000	time 0.3173 (0.3573)	loss 1.0891 (1.3112)	grad_norm 5.0519 (nan)	loss_scale 512.0000 (897.7593)	mem 12173MB
[2024-07-03 10:32:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.3126 (0.3563)	loss 0.9348 (1.3115)	grad_norm 3.8600 (nan)	loss_scale 512.0000 (882.3351)	mem 12173MB
[2024-07-03 10:32:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 6 training takes 0:15:04
[2024-07-03 10:32:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.848 (17.848)	Loss 0.4099 (0.4099)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 10:32:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.660 Acc@5 97.406
[2024-07-03 10:32:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 10:32:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 10:33:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][0/2502]	eta 11:24:14 lr 0.000039	 wd 0.0000	time 16.4088 (16.4088)	loss 1.6089 (1.6089)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:33:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:20:55 lr 0.000039	 wd 0.0000	time 0.3310 (0.5228)	loss 1.3149 (1.3251)	grad_norm 4.7583 (4.8625)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:34:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:16:46 lr 0.000039	 wd 0.0000	time 0.3212 (0.4371)	loss 1.5217 (1.3042)	grad_norm 8.1558 (5.2742)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:35:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:14:52 lr 0.000039	 wd 0.0000	time 0.2967 (0.4052)	loss 1.2830 (1.3039)	grad_norm 6.0241 (5.1270)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:35:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:13:38 lr 0.000039	 wd 0.0000	time 0.3212 (0.3892)	loss 1.3707 (1.3064)	grad_norm 3.8661 (5.3627)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:36:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:12:39 lr 0.000039	 wd 0.0000	time 0.3265 (0.3792)	loss 1.4524 (1.3028)	grad_norm 4.6706 (5.2919)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:36:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:11:48 lr 0.000039	 wd 0.0000	time 0.3495 (0.3726)	loss 1.3606 (1.3062)	grad_norm 6.7199 (5.2233)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:37:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:11:09 lr 0.000039	 wd 0.0000	time 0.3511 (0.3715)	loss 1.3839 (1.3086)	grad_norm 4.7762 (5.2090)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:37:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:10:26 lr 0.000039	 wd 0.0000	time 0.3056 (0.3683)	loss 1.0110 (1.3105)	grad_norm 4.5425 (5.1477)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:38:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:09:44 lr 0.000039	 wd 0.0000	time 0.3328 (0.3649)	loss 1.1416 (1.3131)	grad_norm 3.9581 (5.1181)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:39:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:09:05 lr 0.000039	 wd 0.0000	time 0.2813 (0.3630)	loss 1.2241 (1.3102)	grad_norm 5.0803 (5.0667)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:39:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:08:26 lr 0.000039	 wd 0.0000	time 0.3097 (0.3610)	loss 1.2908 (1.3087)	grad_norm 3.8506 (5.0936)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:40:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:07:47 lr 0.000039	 wd 0.0000	time 0.3053 (0.3593)	loss 1.3089 (1.3062)	grad_norm 5.2562 (5.1201)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:40:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:07:10 lr 0.000039	 wd 0.0000	time 0.3471 (0.3586)	loss 1.3668 (1.3072)	grad_norm 6.5534 (5.1336)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:41:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:06:33 lr 0.000039	 wd 0.0000	time 0.3174 (0.3573)	loss 1.5145 (1.3076)	grad_norm 6.3796 (5.1419)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:41:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:56 lr 0.000039	 wd 0.0000	time 0.3470 (0.3561)	loss 1.3996 (1.3067)	grad_norm 10.0562 (5.1735)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:42:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:05:21 lr 0.000039	 wd 0.0000	time 0.3300 (0.3559)	loss 1.4346 (1.3080)	grad_norm 5.3085 (5.1438)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:43:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:44 lr 0.000039	 wd 0.0000	time 0.2961 (0.3548)	loss 1.5174 (1.3082)	grad_norm 4.9085 (5.2281)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:43:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:04:08 lr 0.000039	 wd 0.0000	time 0.3600 (0.3541)	loss 1.2607 (1.3079)	grad_norm 9.0258 (5.2747)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:44:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:33 lr 0.000039	 wd 0.0000	time 0.3014 (0.3541)	loss 1.2833 (1.3071)	grad_norm 6.9647 (5.2568)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:44:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:57 lr 0.000039	 wd 0.0000	time 0.3255 (0.3534)	loss 1.4516 (1.3079)	grad_norm 4.7870 (5.2742)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:45:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:21 lr 0.000039	 wd 0.0000	time 0.3260 (0.3530)	loss 1.5706 (1.3095)	grad_norm 5.1425 (5.2677)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:45:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:46 lr 0.000039	 wd 0.0000	time 0.2892 (0.3524)	loss 1.2040 (1.3081)	grad_norm 4.2754 (5.2562)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:46:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:11 lr 0.000039	 wd 0.0000	time 0.3095 (0.3519)	loss 1.4469 (1.3086)	grad_norm 4.2090 (5.2392)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:47:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:35 lr 0.000039	 wd 0.0000	time 0.2999 (0.3516)	loss 1.3409 (1.3084)	grad_norm 3.4940 (5.2291)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:47:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.3011 (0.3524)	loss 1.2259 (1.3086)	grad_norm 4.1857 (5.2159)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:47:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 7 training takes 0:14:47
[2024-07-03 10:48:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.057 (17.057)	Loss 0.4050 (0.4050)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12173MB
[2024-07-03 10:48:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.604 Acc@5 97.396
[2024-07-03 10:48:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-03 10:48:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 10:48:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][0/2502]	eta 10:29:39 lr 0.000039	 wd 0.0000	time 15.0996 (15.0996)	loss 1.4606 (1.4606)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:49:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:21:20 lr 0.000039	 wd 0.0000	time 0.3183 (0.5329)	loss 1.4229 (1.3334)	grad_norm 10.7540 (5.8051)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:49:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:16:49 lr 0.000039	 wd 0.0000	time 0.3553 (0.4384)	loss 1.3478 (1.3082)	grad_norm 7.5068 (5.4287)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:50:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:14:57 lr 0.000038	 wd 0.0000	time 0.3166 (0.4074)	loss 1.2724 (1.3031)	grad_norm 4.6469 (5.2810)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:50:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:13:44 lr 0.000038	 wd 0.0000	time 0.3224 (0.3925)	loss 1.3841 (1.3007)	grad_norm 6.4744 (5.2020)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:51:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:12:43 lr 0.000038	 wd 0.0000	time 0.3431 (0.3816)	loss 1.4520 (1.3081)	grad_norm 3.9469 (5.3228)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:51:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:11:53 lr 0.000038	 wd 0.0000	time 0.3033 (0.3750)	loss 1.5098 (1.3124)	grad_norm 4.3160 (5.2502)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:52:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:11:07 lr 0.000038	 wd 0.0000	time 0.3257 (0.3704)	loss 1.5050 (1.3131)	grad_norm 4.0256 (5.1982)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:53:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:10:24 lr 0.000038	 wd 0.0000	time 0.3260 (0.3669)	loss 1.3644 (1.3127)	grad_norm 5.3857 (5.1512)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 10:53:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:09:44 lr 0.000038	 wd 0.0000	time 0.3535 (0.3646)	loss 1.4858 (1.3136)	grad_norm 5.1681 (5.1882)	loss_scale 1024.0000 (566.5527)	mem 12173MB
[2024-07-03 10:54:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:09:04 lr 0.000038	 wd 0.0000	time 0.3149 (0.3622)	loss 1.3633 (1.3115)	grad_norm 13.5406 (5.1647)	loss_scale 1024.0000 (612.2517)	mem 12173MB
[2024-07-03 10:54:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:08:25 lr 0.000038	 wd 0.0000	time 0.3116 (0.3604)	loss 1.4306 (1.3084)	grad_norm 5.0438 (5.1744)	loss_scale 1024.0000 (649.6494)	mem 12173MB
[2024-07-03 10:55:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:47 lr 0.000038	 wd 0.0000	time 0.3265 (0.3594)	loss 1.3884 (1.3078)	grad_norm 5.1043 (5.1968)	loss_scale 1024.0000 (680.8193)	mem 12173MB
[2024-07-03 10:55:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:07:11 lr 0.000038	 wd 0.0000	time 0.3248 (0.3589)	loss 1.0226 (1.3058)	grad_norm 4.2055 (5.1697)	loss_scale 1024.0000 (707.1975)	mem 12173MB
[2024-07-03 10:56:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:06:33 lr 0.000038	 wd 0.0000	time 0.3291 (0.3574)	loss 0.8981 (1.3068)	grad_norm 4.8167 (5.1714)	loss_scale 1024.0000 (729.8101)	mem 12173MB
[2024-07-03 10:57:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:58 lr 0.000038	 wd 0.0000	time 0.3253 (0.3579)	loss 0.8910 (1.3036)	grad_norm 4.0300 (5.1326)	loss_scale 1024.0000 (749.4097)	mem 12173MB
[2024-07-03 10:57:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:05:21 lr 0.000038	 wd 0.0000	time 0.3662 (0.3570)	loss 1.2260 (1.3007)	grad_norm 4.0018 (5.1157)	loss_scale 1024.0000 (766.5609)	mem 12173MB
[2024-07-03 10:58:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:45 lr 0.000038	 wd 0.0000	time 0.3237 (0.3559)	loss 1.1907 (1.3022)	grad_norm 3.7140 (5.0824)	loss_scale 1024.0000 (781.6955)	mem 12173MB
[2024-07-03 10:58:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:04:10 lr 0.000038	 wd 0.0000	time 0.2970 (0.3574)	loss 1.4274 (1.3041)	grad_norm 3.8117 (5.0562)	loss_scale 1024.0000 (795.1494)	mem 12173MB
[2024-07-03 10:59:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:34 lr 0.000038	 wd 0.0000	time 0.3018 (0.3564)	loss 1.2185 (1.3044)	grad_norm 4.1210 (5.0537)	loss_scale 1024.0000 (807.1878)	mem 12173MB
[2024-07-03 11:00:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:58 lr 0.000038	 wd 0.0000	time 0.3078 (0.3566)	loss 1.6908 (1.3061)	grad_norm 2.9170 (5.0154)	loss_scale 1024.0000 (818.0230)	mem 12173MB
[2024-07-03 11:00:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:23 lr 0.000038	 wd 0.0000	time 0.3030 (0.3571)	loss 1.3006 (1.3062)	grad_norm 3.5194 (5.0464)	loss_scale 1024.0000 (827.8267)	mem 12173MB
[2024-07-03 11:01:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:47 lr 0.000038	 wd 0.0000	time 0.3197 (0.3563)	loss 1.3921 (1.3050)	grad_norm 3.6904 (5.0199)	loss_scale 1024.0000 (836.7397)	mem 12173MB
[2024-07-03 11:01:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:11 lr 0.000038	 wd 0.0000	time 0.3165 (0.3561)	loss 1.3348 (1.3056)	grad_norm 6.8605 (5.0430)	loss_scale 1024.0000 (844.8779)	mem 12173MB
[2024-07-03 11:02:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:36 lr 0.000038	 wd 0.0000	time 0.3130 (0.3556)	loss 1.3848 (1.3074)	grad_norm 3.8255 (5.0437)	loss_scale 1024.0000 (852.3382)	mem 12173MB
[2024-07-03 11:02:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000038	 wd 0.0000	time 0.3008 (0.3546)	loss 1.5452 (1.3087)	grad_norm 5.4608 (5.0380)	loss_scale 1024.0000 (859.2019)	mem 12173MB
[2024-07-03 11:03:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 8 training takes 0:14:50
[2024-07-03 11:03:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 24.312 (24.312)	Loss 0.4028 (0.4028)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 11:03:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.658 Acc@5 97.430
[2024-07-03 11:03:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 11:03:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 11:03:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][0/2502]	eta 11:03:03 lr 0.000038	 wd 0.0000	time 15.9006 (15.9006)	loss 1.3020 (1.3020)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:04:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:20:07 lr 0.000038	 wd 0.0000	time 0.3205 (0.5028)	loss 1.2530 (1.2827)	grad_norm 7.6219 (5.3730)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:05:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:16:57 lr 0.000037	 wd 0.0000	time 0.2974 (0.4421)	loss 1.4346 (1.2865)	grad_norm 4.1133 (5.2948)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:05:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:15:00 lr 0.000037	 wd 0.0000	time 0.3272 (0.4089)	loss 1.3653 (1.2967)	grad_norm 4.4537 (5.3211)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:06:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:13:55 lr 0.000037	 wd 0.0000	time 0.3161 (0.3975)	loss 1.5542 (1.2923)	grad_norm 6.2611 (5.3529)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:06:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:12:52 lr 0.000037	 wd 0.0000	time 0.3115 (0.3859)	loss 1.2900 (1.2978)	grad_norm 4.6313 (5.2259)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:07:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:11:59 lr 0.000037	 wd 0.0000	time 0.3138 (0.3785)	loss 1.3471 (1.2923)	grad_norm 4.4411 (5.1752)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:08:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:11:12 lr 0.000037	 wd 0.0000	time 0.3338 (0.3731)	loss 1.4536 (1.2928)	grad_norm 4.2500 (5.2048)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:08:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:10:33 lr 0.000037	 wd 0.0000	time 0.3185 (0.3721)	loss 1.3381 (1.2978)	grad_norm 4.3890 (5.1252)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:09:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:09:51 lr 0.000037	 wd 0.0000	time 0.2979 (0.3694)	loss 1.4743 (1.2950)	grad_norm 5.1372 (5.0567)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:09:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:09:10 lr 0.000037	 wd 0.0000	time 0.3170 (0.3668)	loss 1.4743 (1.2973)	grad_norm 3.8584 (4.9900)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:10:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:08:32 lr 0.000037	 wd 0.0000	time 0.3261 (0.3658)	loss 1.2528 (1.3007)	grad_norm 7.9019 (5.0217)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:10:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:07:53 lr 0.000037	 wd 0.0000	time 0.3186 (0.3638)	loss 1.5654 (1.3032)	grad_norm 2.8413 (5.0086)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:11:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:07:15 lr 0.000037	 wd 0.0000	time 0.4209 (0.3627)	loss 0.9112 (1.3005)	grad_norm 6.6586 (4.9814)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:12:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:06:40 lr 0.000037	 wd 0.0000	time 0.3152 (0.3632)	loss 1.3335 (1.3027)	grad_norm 3.3808 (4.9509)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:12:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:06:02 lr 0.000037	 wd 0.0000	time 0.3297 (0.3618)	loss 1.4744 (1.3050)	grad_norm 5.6426 (4.9092)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:13:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:05:25 lr 0.000037	 wd 0.0000	time 0.3006 (0.3607)	loss 0.9496 (1.3046)	grad_norm 4.1321 (4.9006)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:13:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:49 lr 0.000037	 wd 0.0000	time 0.3267 (0.3605)	loss 1.0548 (1.3062)	grad_norm 3.9734 (4.8737)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:14:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:04:12 lr 0.000037	 wd 0.0000	time 0.2887 (0.3591)	loss 1.4282 (1.3070)	grad_norm 4.5924 (4.8883)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:15:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:36 lr 0.000037	 wd 0.0000	time 0.2985 (0.3602)	loss 1.3666 (1.3063)	grad_norm 4.3117 (4.9340)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:15:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:03:00 lr 0.000037	 wd 0.0000	time 0.3182 (0.3597)	loss 1.5045 (1.3054)	grad_norm 7.8858 (4.9199)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:16:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:24 lr 0.000036	 wd 0.0000	time 0.3665 (0.3588)	loss 1.1213 (1.3062)	grad_norm 5.3408 (4.9263)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:16:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:48 lr 0.000036	 wd 0.0000	time 0.3059 (0.3582)	loss 1.4001 (1.3076)	grad_norm 3.4931 (4.9234)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:17:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:12 lr 0.000036	 wd 0.0000	time 0.3097 (0.3576)	loss 1.3297 (1.3081)	grad_norm 3.5835 (4.9099)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:17:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:36 lr 0.000036	 wd 0.0000	time 0.3034 (0.3569)	loss 1.2102 (1.3079)	grad_norm 7.0213 (inf)	loss_scale 1024.0000 (1053.0012)	mem 12173MB
[2024-07-03 11:18:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000036	 wd 0.0000	time 0.2956 (0.3570)	loss 0.8433 (1.3088)	grad_norm 4.3132 (inf)	loss_scale 1024.0000 (1051.8417)	mem 12173MB
[2024-07-03 11:18:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 9 training takes 0:15:02
[2024-07-03 11:18:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.795 (16.795)	Loss 0.3838 (0.3838)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 11:19:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.666 Acc@5 97.376
[2024-07-03 11:19:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 11:19:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 11:19:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][0/2502]	eta 11:09:49 lr 0.000036	 wd 0.0000	time 16.0628 (16.0628)	loss 1.2220 (1.2220)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:20:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:20:26 lr 0.000036	 wd 0.0000	time 0.3422 (0.5105)	loss 1.1600 (1.2679)	grad_norm 7.2249 (5.2522)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:20:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:16:28 lr 0.000036	 wd 0.0000	time 0.3316 (0.4294)	loss 1.3438 (1.2949)	grad_norm 9.6782 (5.0964)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:21:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:14:40 lr 0.000036	 wd 0.0000	time 0.2862 (0.3996)	loss 0.8828 (1.2969)	grad_norm 3.3157 (5.0685)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:21:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:13:31 lr 0.000036	 wd 0.0000	time 0.3339 (0.3862)	loss 1.4880 (1.3022)	grad_norm 6.7362 (4.9723)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:22:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:12:34 lr 0.000036	 wd 0.0000	time 0.3246 (0.3767)	loss 1.4468 (1.3006)	grad_norm 4.3527 (5.0060)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:22:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:11:44 lr 0.000036	 wd 0.0000	time 0.3325 (0.3705)	loss 1.5554 (1.2992)	grad_norm 6.8054 (4.9783)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:23:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:11:03 lr 0.000036	 wd 0.0000	time 0.3449 (0.3682)	loss 1.3750 (1.2967)	grad_norm 4.7418 (5.0699)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:24:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:10:21 lr 0.000036	 wd 0.0000	time 0.3334 (0.3651)	loss 1.2507 (1.2982)	grad_norm 3.7826 (5.0121)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:24:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:09:43 lr 0.000036	 wd 0.0000	time 0.3252 (0.3645)	loss 1.4391 (1.2982)	grad_norm 4.3981 (4.9825)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:25:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:09:05 lr 0.000036	 wd 0.0000	time 0.3120 (0.3632)	loss 1.5530 (1.2966)	grad_norm 7.3299 (4.9469)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:25:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:08:27 lr 0.000036	 wd 0.0000	time 0.3369 (0.3621)	loss 1.3279 (1.2946)	grad_norm 4.2277 (4.9373)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:26:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:07:49 lr 0.000035	 wd 0.0000	time 0.3135 (0.3605)	loss 1.2061 (1.2949)	grad_norm 8.0353 (4.9670)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:26:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:07:12 lr 0.000035	 wd 0.0000	time 0.3226 (0.3600)	loss 1.5400 (1.2976)	grad_norm 4.4989 (4.9438)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:27:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:06:35 lr 0.000035	 wd 0.0000	time 0.3506 (0.3586)	loss 1.3847 (1.2998)	grad_norm 4.2430 (4.9285)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:28:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:59 lr 0.000035	 wd 0.0000	time 0.3171 (0.3591)	loss 1.1838 (1.3001)	grad_norm 3.5089 (4.9547)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:28:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:05:24 lr 0.000035	 wd 0.0000	time 0.3075 (0.3596)	loss 1.1411 (1.3015)	grad_norm 4.3334 (4.9462)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:29:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:48 lr 0.000035	 wd 0.0000	time 0.3662 (0.3591)	loss 1.3359 (1.3024)	grad_norm 3.8714 (4.9890)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:29:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:04:13 lr 0.000035	 wd 0.0000	time 0.3158 (0.3611)	loss 1.3030 (1.3023)	grad_norm 7.8479 (4.9633)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 11:30:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:36 lr 0.000035	 wd 0.0000	time 0.3150 (0.3601)	loss 1.3780 (1.3032)	grad_norm 5.1866 (nan)	loss_scale 512.0000 (1012.6881)	mem 12173MB
[2024-07-03 11:31:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:03:00 lr 0.000035	 wd 0.0000	time 0.3013 (0.3592)	loss 1.2531 (1.3034)	grad_norm 5.1223 (nan)	loss_scale 512.0000 (987.6662)	mem 12173MB
[2024-07-03 11:31:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:24 lr 0.000035	 wd 0.0000	time 0.3152 (0.3595)	loss 1.3980 (1.3049)	grad_norm 5.0832 (nan)	loss_scale 512.0000 (965.0262)	mem 12173MB
[2024-07-03 11:32:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:48 lr 0.000035	 wd 0.0000	time 0.3146 (0.3586)	loss 0.9622 (1.3053)	grad_norm 4.3338 (nan)	loss_scale 512.0000 (944.4434)	mem 12173MB
[2024-07-03 11:32:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:12 lr 0.000035	 wd 0.0000	time 0.3005 (0.3583)	loss 1.4159 (1.3047)	grad_norm 4.9972 (nan)	loss_scale 512.0000 (925.6497)	mem 12173MB
[2024-07-03 11:33:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:36 lr 0.000035	 wd 0.0000	time 0.3277 (0.3578)	loss 1.4009 (1.3049)	grad_norm 4.4020 (nan)	loss_scale 512.0000 (908.4215)	mem 12173MB
[2024-07-03 11:34:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.2952 (0.3569)	loss 1.4826 (1.3044)	grad_norm 4.4638 (nan)	loss_scale 512.0000 (892.5710)	mem 12173MB
[2024-07-03 11:34:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 10 training takes 0:14:56
[2024-07-03 11:34:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.748 (16.748)	Loss 0.4106 (0.4106)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 11:34:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.716 Acc@5 97.422
[2024-07-03 11:34:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 11:34:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 11:34:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][0/2502]	eta 10:22:36 lr 0.000035	 wd 0.0000	time 14.9305 (14.9305)	loss 1.4951 (1.4951)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:35:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:19:54 lr 0.000035	 wd 0.0000	time 0.3039 (0.4972)	loss 1.2568 (1.3027)	grad_norm 4.7376 (4.1729)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:36:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:16:08 lr 0.000034	 wd 0.0000	time 0.3135 (0.4206)	loss 1.5030 (1.3042)	grad_norm 3.8158 (4.3649)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:36:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:14:32 lr 0.000034	 wd 0.0000	time 0.3465 (0.3964)	loss 1.1973 (1.3038)	grad_norm 3.8201 (4.4649)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:37:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:13:25 lr 0.000034	 wd 0.0000	time 0.3217 (0.3834)	loss 1.3995 (1.3075)	grad_norm 4.0533 (4.4925)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:37:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:12:33 lr 0.000034	 wd 0.0000	time 0.3107 (0.3766)	loss 1.2849 (1.3001)	grad_norm 3.3431 (4.4879)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:38:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:11:47 lr 0.000034	 wd 0.0000	time 0.3987 (0.3718)	loss 1.3834 (1.2984)	grad_norm 4.2933 (4.5448)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:39:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:11:12 lr 0.000034	 wd 0.0000	time 0.3340 (0.3733)	loss 1.3435 (1.2984)	grad_norm 7.3896 (4.6080)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:39:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:10:29 lr 0.000034	 wd 0.0000	time 0.3355 (0.3701)	loss 1.6088 (1.3018)	grad_norm 4.7249 (4.6330)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:40:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:09:47 lr 0.000034	 wd 0.0000	time 0.3260 (0.3669)	loss 1.4645 (1.3019)	grad_norm 4.4872 (4.7362)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:40:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:09:06 lr 0.000034	 wd 0.0000	time 0.3275 (0.3641)	loss 1.4096 (1.3017)	grad_norm 5.5439 (4.7648)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:41:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:08:28 lr 0.000034	 wd 0.0000	time 0.2760 (0.3627)	loss 1.3362 (1.3034)	grad_norm 4.3990 (4.7614)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:41:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:07:49 lr 0.000034	 wd 0.0000	time 0.3824 (0.3608)	loss 1.4950 (1.3039)	grad_norm 3.8123 (4.7478)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:42:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:07:11 lr 0.000034	 wd 0.0000	time 0.3241 (0.3591)	loss 1.0532 (1.3028)	grad_norm 3.7405 (4.7650)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:43:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:06:35 lr 0.000034	 wd 0.0000	time 0.3256 (0.3586)	loss 0.9169 (1.3025)	grad_norm 4.3824 (4.7408)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:43:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:06:01 lr 0.000034	 wd 0.0000	time 0.3383 (0.3604)	loss 1.4063 (1.3017)	grad_norm 3.2655 (4.7188)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:44:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:05:25 lr 0.000034	 wd 0.0000	time 0.3648 (0.3613)	loss 1.2251 (1.3030)	grad_norm 3.8457 (4.7746)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:44:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:49 lr 0.000033	 wd 0.0000	time 0.3016 (0.3606)	loss 1.3502 (1.3020)	grad_norm 4.1987 (4.7851)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:45:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:04:12 lr 0.000033	 wd 0.0000	time 0.2938 (0.3595)	loss 1.2675 (1.3034)	grad_norm 4.1808 (4.7888)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:46:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:36 lr 0.000033	 wd 0.0000	time 0.2829 (0.3590)	loss 1.4733 (1.3031)	grad_norm 5.2454 (4.7799)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:46:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:03:00 lr 0.000033	 wd 0.0000	time 0.3319 (0.3597)	loss 1.1695 (1.3030)	grad_norm 3.9703 (4.8061)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:47:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:24 lr 0.000033	 wd 0.0000	time 0.3153 (0.3590)	loss 1.3796 (1.3023)	grad_norm 3.8343 (4.7949)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:47:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:48 lr 0.000033	 wd 0.0000	time 0.3379 (0.3585)	loss 1.3437 (1.3039)	grad_norm 4.0452 (4.8210)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:48:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:12 lr 0.000033	 wd 0.0000	time 0.3014 (0.3577)	loss 1.4193 (1.3034)	grad_norm 3.5229 (4.8149)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:48:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:36 lr 0.000033	 wd 0.0000	time 0.3033 (0.3570)	loss 1.3022 (1.3028)	grad_norm 4.6955 (4.8314)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:49:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000033	 wd 0.0000	time 0.3061 (0.3559)	loss 1.3592 (1.3029)	grad_norm 5.2823 (4.8537)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:49:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 11 training takes 0:15:02
[2024-07-03 11:49:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.811 (16.811)	Loss 0.4023 (0.4023)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 11:50:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.602 Acc@5 97.346
[2024-07-03 11:50:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-03 11:50:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 11:50:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][0/2502]	eta 11:24:01 lr 0.000033	 wd 0.0000	time 16.4034 (16.4034)	loss 1.5450 (1.5450)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:51:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:21:00 lr 0.000033	 wd 0.0000	time 0.3465 (0.5247)	loss 1.4560 (1.2984)	grad_norm 3.4794 (5.0125)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:51:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:16:36 lr 0.000033	 wd 0.0000	time 0.3138 (0.4327)	loss 0.8642 (1.3062)	grad_norm 4.1434 (5.1812)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:52:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:14:45 lr 0.000033	 wd 0.0000	time 0.3102 (0.4020)	loss 1.2605 (1.3054)	grad_norm 3.9887 (4.9778)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:52:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:13:35 lr 0.000033	 wd 0.0000	time 0.3034 (0.3879)	loss 1.2541 (1.2989)	grad_norm 2.7767 (4.9554)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:53:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:12:38 lr 0.000032	 wd 0.0000	time 0.3085 (0.3790)	loss 1.0221 (1.3004)	grad_norm 6.0692 (4.9316)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:53:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:11:56 lr 0.000032	 wd 0.0000	time 0.2964 (0.3765)	loss 1.2761 (1.2971)	grad_norm 3.8560 (4.9544)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:54:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:11:14 lr 0.000032	 wd 0.0000	time 0.3228 (0.3742)	loss 1.3994 (1.2970)	grad_norm 3.9488 (4.9135)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:55:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:10:30 lr 0.000032	 wd 0.0000	time 0.3110 (0.3702)	loss 1.5864 (1.3062)	grad_norm 5.1022 (4.8821)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 11:55:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:09:47 lr 0.000032	 wd 0.0000	time 0.3508 (0.3669)	loss 1.0152 (1.3073)	grad_norm 4.4030 (4.8882)	loss_scale 1024.0000 (538.1398)	mem 12173MB
[2024-07-03 11:56:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:09:13 lr 0.000032	 wd 0.0000	time 0.3272 (0.3682)	loss 1.3781 (1.3059)	grad_norm 6.2953 (4.8713)	loss_scale 1024.0000 (586.6773)	mem 12173MB
[2024-07-03 11:56:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:08:35 lr 0.000032	 wd 0.0000	time 0.3342 (0.3678)	loss 1.4344 (1.3108)	grad_norm 3.4978 (4.8987)	loss_scale 1024.0000 (626.3978)	mem 12173MB
[2024-07-03 11:57:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:07:56 lr 0.000032	 wd 0.0000	time 0.2900 (0.3658)	loss 1.5266 (1.3092)	grad_norm 4.9061 (4.9428)	loss_scale 1024.0000 (659.5037)	mem 12173MB
[2024-07-03 11:58:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:07:20 lr 0.000032	 wd 0.0000	time 0.3011 (0.3662)	loss 1.5309 (1.3095)	grad_norm 6.5036 (5.0043)	loss_scale 1024.0000 (687.5204)	mem 12173MB
[2024-07-03 11:58:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:06:41 lr 0.000032	 wd 0.0000	time 0.3163 (0.3643)	loss 1.4511 (1.3095)	grad_norm 3.9443 (4.9966)	loss_scale 1024.0000 (711.5375)	mem 12173MB
[2024-07-03 11:59:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:06:03 lr 0.000032	 wd 0.0000	time 0.3003 (0.3632)	loss 1.4026 (1.3101)	grad_norm 3.8360 (4.9860)	loss_scale 1024.0000 (732.3544)	mem 12173MB
[2024-07-03 11:59:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:05:26 lr 0.000032	 wd 0.0000	time 0.3251 (0.3620)	loss 1.4314 (1.3084)	grad_norm 3.7105 (4.9680)	loss_scale 1024.0000 (750.5709)	mem 12173MB
[2024-07-03 12:00:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:49 lr 0.000031	 wd 0.0000	time 0.3437 (0.3608)	loss 1.3824 (1.3095)	grad_norm 3.7655 (4.9887)	loss_scale 1024.0000 (766.6455)	mem 12173MB
[2024-07-03 12:01:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:04:13 lr 0.000031	 wd 0.0000	time 0.2862 (0.3606)	loss 1.3046 (1.3103)	grad_norm 5.1041 (4.9826)	loss_scale 1024.0000 (780.9350)	mem 12173MB
[2024-07-03 12:01:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:36 lr 0.000031	 wd 0.0000	time 0.3184 (0.3596)	loss 1.2627 (1.3101)	grad_norm 4.0043 (4.9805)	loss_scale 1024.0000 (793.7212)	mem 12173MB
[2024-07-03 12:02:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:59 lr 0.000031	 wd 0.0000	time 0.3207 (0.3584)	loss 1.3295 (1.3095)	grad_norm 3.4401 (4.9798)	loss_scale 1024.0000 (805.2294)	mem 12173MB
[2024-07-03 12:02:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:23 lr 0.000031	 wd 0.0000	time 0.2949 (0.3580)	loss 1.4950 (1.3104)	grad_norm 3.7652 (4.9845)	loss_scale 1024.0000 (815.6421)	mem 12173MB
[2024-07-03 12:03:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:47 lr 0.000031	 wd 0.0000	time 0.3480 (0.3574)	loss 1.4275 (1.3110)	grad_norm 3.5268 (4.9741)	loss_scale 1024.0000 (825.1086)	mem 12173MB
[2024-07-03 12:03:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:12 lr 0.000031	 wd 0.0000	time 0.3290 (0.3568)	loss 1.3120 (1.3105)	grad_norm 5.1504 (4.9667)	loss_scale 1024.0000 (833.7523)	mem 12173MB
[2024-07-03 12:04:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:36 lr 0.000031	 wd 0.0000	time 0.3120 (0.3564)	loss 1.4777 (1.3102)	grad_norm 22.0983 (5.0097)	loss_scale 1024.0000 (841.6760)	mem 12173MB
[2024-07-03 12:04:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000031	 wd 0.0000	time 0.3021 (0.3555)	loss 1.0501 (1.3086)	grad_norm 5.6272 (4.9857)	loss_scale 1024.0000 (848.9660)	mem 12173MB
[2024-07-03 12:05:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 12 training takes 0:14:53
[2024-07-03 12:05:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.568 (17.568)	Loss 0.4019 (0.4019)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 12:05:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.752 Acc@5 97.414
[2024-07-03 12:05:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-03 12:05:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 12:05:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][0/2502]	eta 13:14:42 lr 0.000031	 wd 0.0000	time 19.0579 (19.0579)	loss 1.2922 (1.2922)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:06:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:21:00 lr 0.000031	 wd 0.0000	time 0.2835 (0.5247)	loss 1.4429 (1.2998)	grad_norm 4.5764 (5.0724)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:06:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:16:37 lr 0.000031	 wd 0.0000	time 0.2925 (0.4335)	loss 1.5169 (1.3014)	grad_norm 4.4017 (4.9329)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:07:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:15:25 lr 0.000031	 wd 0.0000	time 0.3378 (0.4203)	loss 1.4882 (1.3079)	grad_norm 3.6221 (4.8088)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:08:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:14:04 lr 0.000030	 wd 0.0000	time 0.3399 (0.4017)	loss 1.3902 (1.3059)	grad_norm 6.4364 (4.7539)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:08:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:12:59 lr 0.000030	 wd 0.0000	time 0.3252 (0.3892)	loss 1.4005 (1.3007)	grad_norm 4.2907 (4.8339)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:09:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:12:10 lr 0.000030	 wd 0.0000	time 0.3142 (0.3840)	loss 1.4442 (1.3055)	grad_norm 3.5525 (4.8521)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:09:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:11:24 lr 0.000030	 wd 0.0000	time 0.3215 (0.3800)	loss 1.6043 (1.3021)	grad_norm 3.4490 (nan)	loss_scale 512.0000 (952.4223)	mem 12173MB
[2024-07-03 12:10:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:10:39 lr 0.000030	 wd 0.0000	time 0.3109 (0.3755)	loss 1.4538 (1.3052)	grad_norm 5.4838 (nan)	loss_scale 512.0000 (897.4382)	mem 12173MB
[2024-07-03 12:11:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:09:58 lr 0.000030	 wd 0.0000	time 0.3123 (0.3733)	loss 1.5331 (1.3013)	grad_norm 5.2595 (nan)	loss_scale 512.0000 (854.6593)	mem 12173MB
[2024-07-03 12:11:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:09:16 lr 0.000030	 wd 0.0000	time 0.3062 (0.3702)	loss 1.4575 (1.3039)	grad_norm 3.5821 (nan)	loss_scale 512.0000 (820.4276)	mem 12173MB
[2024-07-03 12:12:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:08:37 lr 0.000030	 wd 0.0000	time 0.2893 (0.3695)	loss 1.3984 (1.3056)	grad_norm 4.0652 (nan)	loss_scale 512.0000 (792.4142)	mem 12173MB
[2024-07-03 12:12:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:58 lr 0.000030	 wd 0.0000	time 0.2890 (0.3676)	loss 1.6066 (1.3030)	grad_norm 5.8600 (nan)	loss_scale 512.0000 (769.0658)	mem 12173MB
[2024-07-03 12:13:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:07:19 lr 0.000030	 wd 0.0000	time 0.3229 (0.3656)	loss 1.4814 (1.3022)	grad_norm 8.1467 (nan)	loss_scale 512.0000 (749.3067)	mem 12173MB
[2024-07-03 12:14:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:06:41 lr 0.000030	 wd 0.0000	time 0.3320 (0.3642)	loss 1.5461 (1.3028)	grad_norm 3.8698 (nan)	loss_scale 512.0000 (732.3683)	mem 12173MB
[2024-07-03 12:14:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:06:03 lr 0.000030	 wd 0.0000	time 0.3399 (0.3629)	loss 1.4631 (1.3021)	grad_norm 4.7637 (nan)	loss_scale 512.0000 (717.6869)	mem 12173MB
[2024-07-03 12:15:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:05:26 lr 0.000029	 wd 0.0000	time 0.3137 (0.3617)	loss 1.4208 (1.3017)	grad_norm 3.7038 (nan)	loss_scale 512.0000 (704.8395)	mem 12173MB
[2024-07-03 12:15:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:50 lr 0.000029	 wd 0.0000	time 0.3784 (0.3622)	loss 0.8309 (1.2992)	grad_norm 3.3007 (nan)	loss_scale 512.0000 (693.5026)	mem 12173MB
[2024-07-03 12:16:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:04:13 lr 0.000029	 wd 0.0000	time 0.3023 (0.3612)	loss 1.2901 (1.3005)	grad_norm 3.6810 (nan)	loss_scale 512.0000 (683.4248)	mem 12173MB
[2024-07-03 12:16:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:36 lr 0.000029	 wd 0.0000	time 0.3124 (0.3600)	loss 1.1306 (1.3006)	grad_norm 7.4799 (nan)	loss_scale 512.0000 (674.4072)	mem 12173MB
[2024-07-03 12:17:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:03:01 lr 0.000029	 wd 0.0000	time 0.3152 (0.3609)	loss 1.3448 (1.3012)	grad_norm 4.2715 (nan)	loss_scale 512.0000 (666.2909)	mem 12173MB
[2024-07-03 12:18:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:24 lr 0.000029	 wd 0.0000	time 0.3079 (0.3600)	loss 1.0696 (1.3009)	grad_norm 3.2776 (nan)	loss_scale 512.0000 (658.9472)	mem 12173MB
[2024-07-03 12:18:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:48 lr 0.000029	 wd 0.0000	time 0.3351 (0.3595)	loss 1.4898 (1.3027)	grad_norm 4.2376 (nan)	loss_scale 512.0000 (652.2708)	mem 12173MB
[2024-07-03 12:19:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:12 lr 0.000029	 wd 0.0000	time 0.3316 (0.3587)	loss 1.2991 (1.3017)	grad_norm 4.0301 (nan)	loss_scale 512.0000 (646.1747)	mem 12173MB
[2024-07-03 12:19:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:36 lr 0.000029	 wd 0.0000	time 0.3115 (0.3587)	loss 1.4337 (1.3025)	grad_norm 12.7703 (nan)	loss_scale 512.0000 (640.5864)	mem 12173MB
[2024-07-03 12:20:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000029	 wd 0.0000	time 0.2692 (0.3576)	loss 1.4248 (1.3020)	grad_norm 3.4876 (nan)	loss_scale 512.0000 (635.4450)	mem 12173MB
[2024-07-03 12:20:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 13 training takes 0:15:07
[2024-07-03 12:20:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.743 (16.743)	Loss 0.4055 (0.4055)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 12:21:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.750 Acc@5 97.416
[2024-07-03 12:21:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-03 12:21:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.77%
[2024-07-03 12:21:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][0/2502]	eta 10:48:25 lr 0.000029	 wd 0.0000	time 15.5497 (15.5497)	loss 1.4997 (1.4997)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:21:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:19:46 lr 0.000029	 wd 0.0000	time 0.3107 (0.4941)	loss 1.4550 (1.2947)	grad_norm 3.9222 (4.6746)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:22:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:16:05 lr 0.000028	 wd 0.0000	time 0.3076 (0.4194)	loss 1.1898 (1.3194)	grad_norm 2.7445 (4.4914)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:23:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:14:38 lr 0.000028	 wd 0.0000	time 0.3216 (0.3988)	loss 1.0797 (1.3074)	grad_norm 3.1964 (4.5266)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:23:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:13:26 lr 0.000028	 wd 0.0000	time 0.3031 (0.3835)	loss 1.1182 (1.2956)	grad_norm 3.6044 (4.5633)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:24:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:12:30 lr 0.000028	 wd 0.0000	time 0.3269 (0.3749)	loss 1.3465 (1.2968)	grad_norm 4.5300 (4.6780)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:24:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:11:48 lr 0.000028	 wd 0.0000	time 0.3623 (0.3727)	loss 1.5500 (1.3005)	grad_norm 4.9027 (4.9003)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:25:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:11:03 lr 0.000028	 wd 0.0000	time 0.3857 (0.3683)	loss 1.2729 (1.2970)	grad_norm 3.4622 (4.8728)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:26:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:10:22 lr 0.000028	 wd 0.0000	time 0.2969 (0.3659)	loss 1.2919 (1.2945)	grad_norm 13.9533 (5.0157)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:26:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:09:41 lr 0.000028	 wd 0.0000	time 0.3285 (0.3630)	loss 1.5021 (1.2929)	grad_norm 4.1810 (5.0072)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:27:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:09:03 lr 0.000028	 wd 0.0000	time 0.4445 (0.3616)	loss 1.0854 (1.2917)	grad_norm 3.9832 (4.9613)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:27:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:08:24 lr 0.000028	 wd 0.0000	time 0.3350 (0.3598)	loss 1.3249 (1.2892)	grad_norm 4.0589 (4.9664)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:28:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:07:46 lr 0.000028	 wd 0.0000	time 0.3184 (0.3580)	loss 1.4765 (1.2895)	grad_norm 3.5024 (4.9070)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:28:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:07:10 lr 0.000027	 wd 0.0000	time 0.3174 (0.3583)	loss 1.1641 (1.2909)	grad_norm 3.1423 (4.8882)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:29:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:06:34 lr 0.000027	 wd 0.0000	time 0.3231 (0.3577)	loss 1.3812 (1.2904)	grad_norm 3.8819 (4.8649)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:30:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:57 lr 0.000027	 wd 0.0000	time 0.3100 (0.3564)	loss 1.3515 (1.2913)	grad_norm 8.6947 (4.8604)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:30:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:05:21 lr 0.000027	 wd 0.0000	time 0.2962 (0.3559)	loss 1.3425 (1.2922)	grad_norm 6.7680 (4.8693)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:31:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:45 lr 0.000027	 wd 0.0000	time 0.3313 (0.3556)	loss 1.1473 (1.2926)	grad_norm 7.0999 (4.8821)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:31:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:04:08 lr 0.000027	 wd 0.0000	time 0.3105 (0.3545)	loss 1.2403 (1.2918)	grad_norm 4.9016 (4.8621)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:32:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:33 lr 0.000027	 wd 0.0000	time 0.3186 (0.3550)	loss 1.3348 (1.2920)	grad_norm 4.2389 (4.8493)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:32:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:57 lr 0.000027	 wd 0.0000	time 0.3051 (0.3542)	loss 0.9675 (1.2935)	grad_norm 5.0613 (4.8400)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:33:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:22 lr 0.000027	 wd 0.0000	time 0.3013 (0.3540)	loss 1.5747 (1.2927)	grad_norm 6.2643 (4.8287)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 12:34:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:46 lr 0.000027	 wd 0.0000	time 0.3566 (0.3536)	loss 1.2032 (1.2930)	grad_norm 4.2554 (4.8473)	loss_scale 1024.0000 (535.2622)	mem 12173MB
[2024-07-03 12:34:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:11 lr 0.000027	 wd 0.0000	time 0.2968 (0.3530)	loss 1.3146 (1.2941)	grad_norm 5.0393 (4.8529)	loss_scale 1024.0000 (556.5024)	mem 12173MB
[2024-07-03 12:35:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:36 lr 0.000026	 wd 0.0000	time 0.3263 (0.3531)	loss 1.2092 (1.2951)	grad_norm 4.5970 (4.8439)	loss_scale 1024.0000 (575.9733)	mem 12173MB
[2024-07-03 12:35:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.3167 (0.3523)	loss 1.2796 (1.2944)	grad_norm 6.0522 (4.8539)	loss_scale 1024.0000 (593.8872)	mem 12173MB
[2024-07-03 12:35:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 14 training takes 0:14:47
[2024-07-03 12:36:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.374 (17.374)	Loss 0.4011 (0.4011)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 12:36:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.832 Acc@5 97.472
[2024-07-03 12:36:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-03 12:36:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.83%
[2024-07-03 12:36:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 12:36:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 12:36:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][0/2502]	eta 10:08:52 lr 0.000026	 wd 0.0000	time 14.6014 (14.6014)	loss 1.3235 (1.3235)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:37:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:20:42 lr 0.000026	 wd 0.0000	time 0.2998 (0.5174)	loss 1.0411 (1.2831)	grad_norm 4.3804 (4.6259)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:37:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:16:27 lr 0.000026	 wd 0.0000	time 0.3364 (0.4291)	loss 1.3800 (1.2946)	grad_norm 4.3856 (4.8191)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:38:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:14:37 lr 0.000026	 wd 0.0000	time 0.3212 (0.3986)	loss 1.3435 (1.2819)	grad_norm 2.5793 (4.7248)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:39:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:13:44 lr 0.000026	 wd 0.0000	time 0.2968 (0.3924)	loss 1.3223 (1.2856)	grad_norm 4.6169 (4.7826)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:39:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:12:44 lr 0.000026	 wd 0.0000	time 0.3278 (0.3820)	loss 0.8562 (1.2800)	grad_norm 4.3409 (4.8332)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:40:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:11:55 lr 0.000026	 wd 0.0000	time 0.3481 (0.3760)	loss 0.8335 (1.2838)	grad_norm 6.2241 (4.7960)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:40:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:11:10 lr 0.000026	 wd 0.0000	time 0.3375 (0.3723)	loss 1.1317 (1.2875)	grad_norm 5.1630 (4.8178)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:41:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:10:27 lr 0.000026	 wd 0.0000	time 0.2964 (0.3686)	loss 1.4806 (1.2888)	grad_norm 5.3058 (4.8654)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:41:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:09:48 lr 0.000025	 wd 0.0000	time 0.2973 (0.3675)	loss 1.1199 (1.2882)	grad_norm 5.4148 (4.8558)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:42:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:09:08 lr 0.000025	 wd 0.0000	time 0.3541 (0.3654)	loss 1.2859 (1.2884)	grad_norm 4.1459 (4.8314)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:43:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:08:29 lr 0.000025	 wd 0.0000	time 0.3169 (0.3631)	loss 1.2520 (1.2855)	grad_norm 3.1172 (4.8702)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:43:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:07:52 lr 0.000025	 wd 0.0000	time 0.2886 (0.3631)	loss 1.5074 (1.2857)	grad_norm 3.7652 (4.8670)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:44:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:07:14 lr 0.000025	 wd 0.0000	time 0.3344 (0.3615)	loss 1.3372 (1.2843)	grad_norm 16.3424 (4.8835)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:44:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:06:37 lr 0.000025	 wd 0.0000	time 0.3131 (0.3608)	loss 1.2896 (1.2852)	grad_norm 7.0913 (4.8670)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:45:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:06:00 lr 0.000025	 wd 0.0000	time 0.3244 (0.3602)	loss 1.5652 (1.2846)	grad_norm 5.9785 (4.8550)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:46:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:05:23 lr 0.000025	 wd 0.0000	time 0.3254 (0.3591)	loss 1.1958 (1.2874)	grad_norm 3.8463 (4.8575)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:46:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:47 lr 0.000025	 wd 0.0000	time 0.3209 (0.3579)	loss 1.5056 (1.2877)	grad_norm 4.1158 (4.8715)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:47:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:04:14 lr 0.000025	 wd 0.0000	time 0.4105 (0.3619)	loss 1.0274 (1.2876)	grad_norm 4.6597 (4.9007)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:47:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:37 lr 0.000024	 wd 0.0000	time 0.2837 (0.3610)	loss 1.5490 (1.2886)	grad_norm 4.6352 (4.9484)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:48:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:03:01 lr 0.000024	 wd 0.0000	time 0.3074 (0.3613)	loss 1.6917 (1.2913)	grad_norm 4.7379 (4.9416)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:49:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:25 lr 0.000024	 wd 0.0000	time 0.3077 (0.3610)	loss 1.5461 (1.2919)	grad_norm 4.3444 (4.9317)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:49:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:48 lr 0.000024	 wd 0.0000	time 0.3033 (0.3600)	loss 0.8630 (1.2930)	grad_norm 3.4832 (4.9236)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:50:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:12 lr 0.000024	 wd 0.0000	time 0.3281 (0.3600)	loss 1.4364 (1.2934)	grad_norm 7.7611 (4.9389)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:50:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:36 lr 0.000024	 wd 0.0000	time 0.2957 (0.3600)	loss 1.1642 (1.2924)	grad_norm 3.7483 (4.9351)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:51:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.3012 (0.3592)	loss 1.3363 (1.2927)	grad_norm 3.4765 (4.9193)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:51:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 15 training takes 0:15:02
[2024-07-03 12:51:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_15.pth saving......
[2024-07-03 12:51:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_15.pth saved !!!
[2024-07-03 12:51:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 20.436 (20.436)	Loss 0.3894 (0.3894)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 12:52:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.828 Acc@5 97.502
[2024-07-03 12:52:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-03 12:52:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.83%
[2024-07-03 12:52:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][0/2502]	eta 11:11:23 lr 0.000024	 wd 0.0000	time 16.1004 (16.1004)	loss 1.1277 (1.1277)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:52:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:19:56 lr 0.000024	 wd 0.0000	time 0.3360 (0.4982)	loss 1.3896 (1.2954)	grad_norm 4.3957 (4.9182)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:53:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:16:28 lr 0.000024	 wd 0.0000	time 0.3367 (0.4296)	loss 1.2527 (1.3019)	grad_norm 4.9846 (4.9822)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:54:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:14:45 lr 0.000024	 wd 0.0000	time 0.3445 (0.4022)	loss 1.2675 (1.3100)	grad_norm 3.3984 (4.7873)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:54:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:13:33 lr 0.000024	 wd 0.0000	time 0.3131 (0.3868)	loss 1.1870 (1.3048)	grad_norm 3.5729 (4.9065)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:55:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:12:38 lr 0.000023	 wd 0.0000	time 0.3447 (0.3789)	loss 1.3067 (1.3124)	grad_norm 8.1926 (4.8804)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:55:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:11:49 lr 0.000023	 wd 0.0000	time 0.3319 (0.3730)	loss 1.0392 (1.3040)	grad_norm 3.9026 (4.8972)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:56:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:11:06 lr 0.000023	 wd 0.0000	time 0.3156 (0.3700)	loss 1.2711 (1.3067)	grad_norm 4.4099 (4.9410)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:56:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:10:27 lr 0.000023	 wd 0.0000	time 0.2899 (0.3684)	loss 1.2638 (1.3004)	grad_norm 2.7002 (4.8820)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:57:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:09:45 lr 0.000023	 wd 0.0000	time 0.3123 (0.3653)	loss 1.2808 (1.2998)	grad_norm 2.9818 (4.8706)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:58:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:09:05 lr 0.000023	 wd 0.0000	time 0.3232 (0.3629)	loss 1.4384 (1.2997)	grad_norm 4.4864 (4.8560)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 12:58:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:08:28 lr 0.000023	 wd 0.0000	time 0.3347 (0.3627)	loss 1.3031 (1.3017)	grad_norm 4.1143 (4.8779)	loss_scale 2048.0000 (1027.7203)	mem 12173MB
[2024-07-03 12:59:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:50 lr 0.000023	 wd 0.0000	time 0.3317 (0.3611)	loss 1.4416 (1.3007)	grad_norm 4.7445 (4.8780)	loss_scale 2048.0000 (1112.6728)	mem 12173MB
[2024-07-03 12:59:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:07:12 lr 0.000023	 wd 0.0000	time 0.3323 (0.3594)	loss 1.0299 (1.3010)	grad_norm 4.6157 (nan)	loss_scale 1024.0000 (1116.8762)	mem 12173MB
[2024-07-03 13:00:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:06:37 lr 0.000023	 wd 0.0000	time 0.3225 (0.3608)	loss 1.2254 (1.2986)	grad_norm 3.5848 (nan)	loss_scale 1024.0000 (1110.2470)	mem 12173MB
[2024-07-03 13:01:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:06:00 lr 0.000022	 wd 0.0000	time 0.3248 (0.3595)	loss 1.3473 (1.2961)	grad_norm 7.9520 (nan)	loss_scale 1024.0000 (1104.5010)	mem 12173MB
[2024-07-03 13:01:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:05:24 lr 0.000022	 wd 0.0000	time 0.3338 (0.3593)	loss 1.3612 (1.2954)	grad_norm 4.3831 (nan)	loss_scale 1024.0000 (1099.4728)	mem 12173MB
[2024-07-03 13:02:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:48 lr 0.000022	 wd 0.0000	time 0.3054 (0.3602)	loss 1.0151 (1.2937)	grad_norm 4.0312 (nan)	loss_scale 1024.0000 (1095.0359)	mem 12173MB
[2024-07-03 13:02:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:04:12 lr 0.000022	 wd 0.0000	time 0.3176 (0.3593)	loss 1.0546 (1.2928)	grad_norm 5.5290 (nan)	loss_scale 1024.0000 (1091.0916)	mem 12173MB
[2024-07-03 13:03:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:37 lr 0.000022	 wd 0.0000	time 0.3015 (0.3609)	loss 1.2241 (1.2921)	grad_norm 7.7795 (nan)	loss_scale 1024.0000 (1087.5623)	mem 12173MB
[2024-07-03 13:04:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:03:01 lr 0.000022	 wd 0.0000	time 0.3375 (0.3606)	loss 1.5081 (1.2921)	grad_norm 4.3985 (nan)	loss_scale 1024.0000 (1084.3858)	mem 12173MB
[2024-07-03 13:04:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:24 lr 0.000022	 wd 0.0000	time 0.3212 (0.3597)	loss 1.3878 (1.2917)	grad_norm 4.2061 (nan)	loss_scale 1024.0000 (1081.5117)	mem 12173MB
[2024-07-03 13:05:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:48 lr 0.000022	 wd 0.0000	time 0.5791 (0.3597)	loss 1.2895 (1.2924)	grad_norm 3.3247 (nan)	loss_scale 1024.0000 (1078.8987)	mem 12173MB
[2024-07-03 13:05:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:12 lr 0.000022	 wd 0.0000	time 0.3112 (0.3587)	loss 1.2624 (1.2927)	grad_norm 7.5871 (nan)	loss_scale 1024.0000 (1076.5128)	mem 12173MB
[2024-07-03 13:06:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:36 lr 0.000022	 wd 0.0000	time 0.3228 (0.3577)	loss 1.2426 (1.2926)	grad_norm 4.7872 (nan)	loss_scale 1024.0000 (1074.3257)	mem 12173MB
[2024-07-03 13:06:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.2959 (0.3568)	loss 1.3620 (1.2925)	grad_norm 3.8803 (nan)	loss_scale 1024.0000 (1072.3135)	mem 12173MB
[2024-07-03 13:07:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 16 training takes 0:15:00
[2024-07-03 13:07:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 15.909 (15.909)	Loss 0.4038 (0.4038)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 13:07:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.870 Acc@5 97.492
[2024-07-03 13:07:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-03 13:07:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.87%
[2024-07-03 13:07:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 13:07:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 13:07:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][0/2502]	eta 10:31:21 lr 0.000021	 wd 0.0000	time 15.1406 (15.1406)	loss 1.3674 (1.3674)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:08:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:20:11 lr 0.000021	 wd 0.0000	time 0.3058 (0.5042)	loss 1.5127 (1.2721)	grad_norm 4.1792 (4.6605)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:08:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:16:14 lr 0.000021	 wd 0.0000	time 0.3219 (0.4235)	loss 1.4571 (1.2859)	grad_norm 4.8958 (5.2806)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:09:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:14:30 lr 0.000021	 wd 0.0000	time 0.3185 (0.3955)	loss 1.3415 (1.2824)	grad_norm 6.8371 (5.0256)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:10:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:13:29 lr 0.000021	 wd 0.0000	time 0.3325 (0.3849)	loss 1.0793 (1.2823)	grad_norm 3.7746 (4.9327)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:10:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:12:34 lr 0.000021	 wd 0.0000	time 0.3038 (0.3768)	loss 1.2890 (1.2791)	grad_norm 3.6677 (4.9021)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:11:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:11:44 lr 0.000021	 wd 0.0000	time 0.2983 (0.3703)	loss 1.1256 (1.2844)	grad_norm 5.3413 (4.8921)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:11:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:11:04 lr 0.000021	 wd 0.0000	time 0.3213 (0.3688)	loss 1.2840 (1.2884)	grad_norm 5.4045 (4.8840)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:12:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:10:22 lr 0.000021	 wd 0.0000	time 0.3069 (0.3658)	loss 1.2810 (1.2918)	grad_norm 3.5257 (4.9328)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:13:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:09:41 lr 0.000021	 wd 0.0000	time 0.3099 (0.3631)	loss 1.4055 (1.2943)	grad_norm 11.9696 (4.9689)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:13:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:09:03 lr 0.000020	 wd 0.0000	time 0.3393 (0.3620)	loss 1.4592 (1.2923)	grad_norm 4.1257 (5.0033)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:14:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:08:24 lr 0.000020	 wd 0.0000	time 0.2983 (0.3601)	loss 1.0893 (1.2903)	grad_norm 4.3070 (4.9632)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:14:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:48 lr 0.000020	 wd 0.0000	time 0.3065 (0.3595)	loss 1.3514 (1.2925)	grad_norm 5.3822 (4.9408)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:15:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:07:11 lr 0.000020	 wd 0.0000	time 0.3318 (0.3590)	loss 1.1928 (1.2925)	grad_norm 4.3260 (4.9112)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:15:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:06:34 lr 0.000020	 wd 0.0000	time 0.3202 (0.3579)	loss 1.4169 (1.2930)	grad_norm 5.8263 (4.9194)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:16:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:58 lr 0.000020	 wd 0.0000	time 0.3063 (0.3575)	loss 1.4934 (1.2938)	grad_norm 4.2437 (4.9595)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:17:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:05:21 lr 0.000020	 wd 0.0000	time 0.3029 (0.3566)	loss 1.3705 (1.2949)	grad_norm 5.5180 (4.9403)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:17:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:45 lr 0.000020	 wd 0.0000	time 0.3073 (0.3555)	loss 1.4858 (1.2961)	grad_norm 3.9716 (4.9415)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:18:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:04:09 lr 0.000020	 wd 0.0000	time 0.3905 (0.3552)	loss 1.3519 (1.2968)	grad_norm 4.5012 (4.9694)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:18:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:33 lr 0.000020	 wd 0.0000	time 0.3430 (0.3546)	loss 1.1921 (1.2974)	grad_norm 3.9157 (4.9555)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:19:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:57 lr 0.000019	 wd 0.0000	time 0.3291 (0.3537)	loss 1.3738 (1.2952)	grad_norm 5.3740 (4.9506)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:19:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:22 lr 0.000019	 wd 0.0000	time 0.3283 (0.3541)	loss 1.4387 (1.2951)	grad_norm 3.4343 (4.9533)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:20:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:46 lr 0.000019	 wd 0.0000	time 0.3153 (0.3534)	loss 1.2877 (1.2956)	grad_norm 4.9112 (4.9477)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:21:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:11 lr 0.000019	 wd 0.0000	time 0.3157 (0.3535)	loss 1.5403 (1.2949)	grad_norm 3.9271 (4.9364)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:21:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:36 lr 0.000019	 wd 0.0000	time 0.4197 (0.3534)	loss 1.3402 (1.2932)	grad_norm 5.5739 (4.9272)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:22:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0000	time 0.3008 (0.3526)	loss 1.2696 (1.2939)	grad_norm 5.0317 (4.9116)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:22:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 17 training takes 0:14:46
[2024-07-03 13:22:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 18.211 (18.211)	Loss 0.3977 (0.3977)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 13:22:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.850 Acc@5 97.500
[2024-07-03 13:22:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-03 13:22:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.87%
[2024-07-03 13:23:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][0/2502]	eta 13:38:22 lr 0.000019	 wd 0.0000	time 19.6253 (19.6253)	loss 1.5126 (1.5126)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:23:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:21:10 lr 0.000019	 wd 0.0000	time 0.2969 (0.5290)	loss 1.4927 (1.3283)	grad_norm 5.8534 (4.8566)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:24:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:16:42 lr 0.000019	 wd 0.0000	time 0.3344 (0.4355)	loss 1.4859 (1.3058)	grad_norm 3.5338 (5.1406)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:24:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:14:58 lr 0.000019	 wd 0.0000	time 0.3479 (0.4081)	loss 1.4872 (1.3028)	grad_norm 3.9339 (5.4024)	loss_scale 2048.0000 (1330.1794)	mem 12173MB
[2024-07-03 13:25:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:13:42 lr 0.000019	 wd 0.0000	time 0.3141 (0.3911)	loss 1.3508 (1.3003)	grad_norm 4.0945 (5.1974)	loss_scale 2048.0000 (1509.1870)	mem 12173MB
[2024-07-03 13:25:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:12:43 lr 0.000018	 wd 0.0000	time 0.3330 (0.3813)	loss 1.1847 (1.2939)	grad_norm 3.7201 (5.1760)	loss_scale 2048.0000 (1616.7345)	mem 12173MB
[2024-07-03 13:26:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:11:57 lr 0.000018	 wd 0.0000	time 0.3123 (0.3774)	loss 1.5139 (1.2929)	grad_norm 4.8687 (5.1115)	loss_scale 2048.0000 (1688.4925)	mem 12173MB
[2024-07-03 13:27:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:11:11 lr 0.000018	 wd 0.0000	time 0.3359 (0.3727)	loss 1.5114 (1.2944)	grad_norm 3.2714 (5.1151)	loss_scale 2048.0000 (1739.7775)	mem 12173MB
[2024-07-03 13:27:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:10:26 lr 0.000018	 wd 0.0000	time 0.2889 (0.3684)	loss 1.0404 (1.2912)	grad_norm 6.1752 (5.0686)	loss_scale 2048.0000 (1778.2572)	mem 12173MB
[2024-07-03 13:28:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:09:46 lr 0.000018	 wd 0.0000	time 0.3197 (0.3660)	loss 1.3415 (1.2882)	grad_norm 5.2832 (5.0302)	loss_scale 2048.0000 (1808.1953)	mem 12173MB
[2024-07-03 13:28:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:09:06 lr 0.000018	 wd 0.0000	time 0.3565 (0.3636)	loss 1.0759 (1.2919)	grad_norm 4.5854 (nan)	loss_scale 1024.0000 (1793.2787)	mem 12173MB
[2024-07-03 13:29:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:08:26 lr 0.000018	 wd 0.0000	time 0.3565 (0.3613)	loss 1.2596 (1.2907)	grad_norm 21.5647 (nan)	loss_scale 1024.0000 (1723.4078)	mem 12173MB
[2024-07-03 13:30:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:07:49 lr 0.000018	 wd 0.0000	time 0.3125 (0.3605)	loss 1.4332 (1.2908)	grad_norm 3.8056 (nan)	loss_scale 1024.0000 (1665.1724)	mem 12173MB
[2024-07-03 13:30:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:07:11 lr 0.000018	 wd 0.0000	time 0.2896 (0.3589)	loss 1.4881 (1.2917)	grad_norm 5.2838 (nan)	loss_scale 1024.0000 (1615.8893)	mem 12173MB
[2024-07-03 13:31:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:06:35 lr 0.000018	 wd 0.0000	time 0.3435 (0.3593)	loss 1.1105 (1.2908)	grad_norm 5.4033 (nan)	loss_scale 1024.0000 (1573.6417)	mem 12173MB
[2024-07-03 13:31:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:06:02 lr 0.000017	 wd 0.0000	time 0.3177 (0.3618)	loss 1.4565 (1.2890)	grad_norm 5.9452 (nan)	loss_scale 1024.0000 (1537.0233)	mem 12173MB
[2024-07-03 13:32:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:05:25 lr 0.000017	 wd 0.0000	time 0.3448 (0.3605)	loss 1.5607 (1.2889)	grad_norm 4.9603 (nan)	loss_scale 1024.0000 (1504.9794)	mem 12173MB
[2024-07-03 13:33:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:51 lr 0.000017	 wd 0.0000	time 0.4538 (0.3630)	loss 1.4471 (1.2878)	grad_norm 4.6607 (nan)	loss_scale 1024.0000 (1476.7031)	mem 12173MB
[2024-07-03 13:33:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:04:13 lr 0.000017	 wd 0.0000	time 0.3003 (0.3616)	loss 1.2715 (1.2892)	grad_norm 4.7355 (nan)	loss_scale 1024.0000 (1451.5669)	mem 12173MB
[2024-07-03 13:34:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:36 lr 0.000017	 wd 0.0000	time 0.3227 (0.3604)	loss 1.4898 (1.2893)	grad_norm 2.5707 (nan)	loss_scale 1024.0000 (1429.0752)	mem 12173MB
[2024-07-03 13:34:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:03:00 lr 0.000017	 wd 0.0000	time 0.3422 (0.3603)	loss 1.3567 (1.2914)	grad_norm 4.5405 (nan)	loss_scale 1024.0000 (1408.8316)	mem 12173MB
[2024-07-03 13:35:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:24 lr 0.000017	 wd 0.0000	time 0.3334 (0.3596)	loss 1.3230 (1.2908)	grad_norm 5.5072 (nan)	loss_scale 1024.0000 (1390.5150)	mem 12173MB
[2024-07-03 13:35:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:48 lr 0.000017	 wd 0.0000	time 0.3181 (0.3585)	loss 1.3531 (1.2908)	grad_norm 4.3008 (nan)	loss_scale 1024.0000 (1373.8628)	mem 12173MB
[2024-07-03 13:36:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:12 lr 0.000017	 wd 0.0000	time 0.3258 (0.3582)	loss 1.4021 (1.2908)	grad_norm 6.6101 (nan)	loss_scale 1024.0000 (1358.6580)	mem 12173MB
[2024-07-03 13:37:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:36 lr 0.000017	 wd 0.0000	time 0.3459 (0.3579)	loss 1.4507 (1.2925)	grad_norm 4.8592 (nan)	loss_scale 1024.0000 (1344.7197)	mem 12173MB
[2024-07-03 13:37:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.3044 (0.3568)	loss 1.4717 (1.2923)	grad_norm 3.9553 (nan)	loss_scale 1024.0000 (1331.8960)	mem 12173MB
[2024-07-03 13:37:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 18 training takes 0:15:05
[2024-07-03 13:38:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.433 (17.433)	Loss 0.3936 (0.3936)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 13:38:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.870 Acc@5 97.490
[2024-07-03 13:38:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-03 13:38:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.87%
[2024-07-03 13:38:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 13:38:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 13:38:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][0/2502]	eta 10:45:47 lr 0.000016	 wd 0.0000	time 15.4866 (15.4866)	loss 1.4425 (1.4425)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:39:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:19:33 lr 0.000016	 wd 0.0000	time 0.3319 (0.4887)	loss 1.0701 (1.2989)	grad_norm 4.9653 (5.0222)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:39:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:16:04 lr 0.000016	 wd 0.0000	time 0.3041 (0.4190)	loss 1.4894 (1.2942)	grad_norm 6.2545 (4.8246)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:40:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:14:29 lr 0.000016	 wd 0.0000	time 0.3105 (0.3947)	loss 1.3369 (1.3047)	grad_norm 5.4252 (5.0067)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:40:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:13:21 lr 0.000016	 wd 0.0000	time 0.3351 (0.3814)	loss 1.4804 (1.3045)	grad_norm 3.3103 (5.0418)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:41:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:12:29 lr 0.000016	 wd 0.0000	time 0.3305 (0.3744)	loss 1.5543 (1.3026)	grad_norm 5.4751 (5.1639)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:42:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:11:41 lr 0.000016	 wd 0.0000	time 0.3217 (0.3687)	loss 1.5382 (1.2980)	grad_norm 3.3970 (5.2373)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:42:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:10:56 lr 0.000016	 wd 0.0000	time 0.3254 (0.3645)	loss 1.5614 (1.2995)	grad_norm 4.6176 (5.2208)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:43:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:10:17 lr 0.000016	 wd 0.0000	time 0.3184 (0.3631)	loss 1.2366 (1.2961)	grad_norm 3.6277 (5.1494)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:43:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:09:37 lr 0.000016	 wd 0.0000	time 0.3480 (0.3606)	loss 1.3709 (1.2975)	grad_norm 5.3833 (5.2297)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:44:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:59 lr 0.000016	 wd 0.0000	time 0.2989 (0.3592)	loss 1.3932 (1.3002)	grad_norm 6.0940 (5.1951)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:44:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:08:22 lr 0.000015	 wd 0.0000	time 0.3160 (0.3582)	loss 0.8425 (1.2985)	grad_norm 3.1018 (5.1559)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:45:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:44 lr 0.000015	 wd 0.0000	time 0.3266 (0.3568)	loss 1.0544 (1.2958)	grad_norm 6.3832 (5.1410)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:46:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:07:07 lr 0.000015	 wd 0.0000	time 0.3604 (0.3556)	loss 1.3715 (1.2956)	grad_norm 3.8035 (5.1548)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:46:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:06:31 lr 0.000015	 wd 0.0000	time 0.3171 (0.3550)	loss 1.3542 (1.2969)	grad_norm 6.8127 (5.1530)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:47:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:54 lr 0.000015	 wd 0.0000	time 0.3070 (0.3538)	loss 1.2025 (1.2966)	grad_norm 3.6600 (5.1663)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 13:47:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:05:19 lr 0.000015	 wd 0.0000	time 0.2931 (0.3542)	loss 1.3011 (1.2974)	grad_norm 7.3301 (nan)	loss_scale 512.0000 (1003.5328)	mem 12173MB
[2024-07-03 13:48:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:43 lr 0.000015	 wd 0.0000	time 0.3212 (0.3536)	loss 1.4655 (1.2981)	grad_norm 4.2869 (nan)	loss_scale 512.0000 (974.6361)	mem 12173MB
[2024-07-03 13:48:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:04:07 lr 0.000015	 wd 0.0000	time 0.2940 (0.3527)	loss 1.2791 (1.2980)	grad_norm 4.1437 (nan)	loss_scale 512.0000 (948.9484)	mem 12173MB
[2024-07-03 13:49:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:32 lr 0.000015	 wd 0.0000	time 0.3520 (0.3528)	loss 1.2847 (1.2983)	grad_norm 3.8732 (nan)	loss_scale 512.0000 (925.9632)	mem 12173MB
[2024-07-03 13:50:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:56 lr 0.000015	 wd 0.0000	time 0.3105 (0.3522)	loss 0.9204 (1.2976)	grad_norm 5.9467 (nan)	loss_scale 512.0000 (905.2754)	mem 12173MB
[2024-07-03 13:50:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:21 lr 0.000014	 wd 0.0000	time 0.3142 (0.3518)	loss 1.1378 (1.2970)	grad_norm 3.7859 (nan)	loss_scale 512.0000 (886.5569)	mem 12173MB
[2024-07-03 13:51:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:46 lr 0.000014	 wd 0.0000	time 0.3266 (0.3516)	loss 1.4132 (1.2953)	grad_norm 3.5789 (nan)	loss_scale 512.0000 (869.5393)	mem 12173MB
[2024-07-03 13:51:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:10 lr 0.000014	 wd 0.0000	time 0.3291 (0.3511)	loss 1.3436 (1.2947)	grad_norm 4.4659 (nan)	loss_scale 512.0000 (854.0009)	mem 12173MB
[2024-07-03 13:52:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:35 lr 0.000014	 wd 0.0000	time 0.3032 (0.3505)	loss 1.4497 (1.2958)	grad_norm 3.1365 (nan)	loss_scale 512.0000 (839.7568)	mem 12173MB
[2024-07-03 13:53:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.2942 (0.3515)	loss 1.2067 (1.2958)	grad_norm 5.0695 (nan)	loss_scale 512.0000 (826.6517)	mem 12173MB
[2024-07-03 13:53:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 19 training takes 0:14:49
[2024-07-03 13:53:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.673 (16.673)	Loss 0.3896 (0.3896)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 13:53:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.950 Acc@5 97.546
[2024-07-03 13:53:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-03 13:53:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 84.95%
[2024-07-03 13:53:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 13:53:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 13:53:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][0/2502]	eta 10:42:50 lr 0.000014	 wd 0.0000	time 15.4159 (15.4159)	loss 1.0216 (1.0216)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:54:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:21:03 lr 0.000014	 wd 0.0000	time 0.3180 (0.5262)	loss 1.6231 (1.2871)	grad_norm 4.5625 (5.0904)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:55:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:16:38 lr 0.000014	 wd 0.0000	time 0.3073 (0.4338)	loss 1.3456 (1.2935)	grad_norm 4.4151 (5.1549)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:55:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:14:45 lr 0.000014	 wd 0.0000	time 0.3422 (0.4023)	loss 1.3737 (1.2915)	grad_norm 6.9823 (5.0614)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:56:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:13:37 lr 0.000014	 wd 0.0000	time 0.3226 (0.3889)	loss 1.4723 (1.2987)	grad_norm 3.4152 (4.9798)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:56:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:12:38 lr 0.000014	 wd 0.0000	time 0.3044 (0.3791)	loss 1.3023 (1.2972)	grad_norm 27.7921 (5.1560)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:57:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:11:50 lr 0.000014	 wd 0.0000	time 0.3622 (0.3735)	loss 1.3904 (1.2931)	grad_norm 6.5164 (5.1579)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:58:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:11:09 lr 0.000013	 wd 0.0000	time 0.3364 (0.3718)	loss 1.4110 (1.2983)	grad_norm 4.0095 (5.2424)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:58:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:10:28 lr 0.000013	 wd 0.0000	time 0.3314 (0.3691)	loss 1.2329 (1.2956)	grad_norm 5.1977 (5.1716)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:59:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:09:48 lr 0.000013	 wd 0.0000	time 0.3882 (0.3674)	loss 1.5538 (1.2989)	grad_norm 11.1848 (5.1551)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 13:59:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:09:08 lr 0.000013	 wd 0.0000	time 0.3466 (0.3651)	loss 1.3068 (1.2987)	grad_norm 7.5220 (5.1328)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:00:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:08:29 lr 0.000013	 wd 0.0000	time 0.3256 (0.3632)	loss 1.3930 (1.2964)	grad_norm 5.0565 (5.2197)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:00:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:07:51 lr 0.000013	 wd 0.0000	time 0.2884 (0.3622)	loss 1.3414 (1.2936)	grad_norm 10.8294 (5.1862)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:01:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:07:13 lr 0.000013	 wd 0.0000	time 0.3411 (0.3610)	loss 1.1321 (1.2904)	grad_norm 5.8507 (5.1710)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:02:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:06:36 lr 0.000013	 wd 0.0000	time 0.3439 (0.3594)	loss 1.2781 (1.2918)	grad_norm 4.2928 (5.1792)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:02:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:59 lr 0.000013	 wd 0.0000	time 0.2995 (0.3588)	loss 1.1856 (1.2934)	grad_norm 3.5103 (5.1515)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:03:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:05:22 lr 0.000013	 wd 0.0000	time 0.3175 (0.3577)	loss 1.4466 (1.2940)	grad_norm 4.3056 (5.1528)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:03:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:45 lr 0.000012	 wd 0.0000	time 0.3183 (0.3565)	loss 1.4926 (1.2937)	grad_norm 7.8372 (5.1694)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:04:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:04:10 lr 0.000012	 wd 0.0000	time 0.3145 (0.3563)	loss 1.2560 (1.2930)	grad_norm 19.3966 (5.1744)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:04:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:33 lr 0.000012	 wd 0.0000	time 0.3097 (0.3553)	loss 1.5646 (1.2938)	grad_norm 2.7734 (5.1629)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:05:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:57 lr 0.000012	 wd 0.0000	time 0.3273 (0.3545)	loss 1.4032 (1.2933)	grad_norm 2.9486 (5.1708)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:06:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:22 lr 0.000012	 wd 0.0000	time 0.3131 (0.3543)	loss 1.3682 (1.2940)	grad_norm 2.9861 (5.1610)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:06:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:47 lr 0.000012	 wd 0.0000	time 0.3087 (0.3553)	loss 1.2042 (1.2953)	grad_norm 7.4720 (5.1418)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:07:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:11 lr 0.000012	 wd 0.0000	time 0.3448 (0.3548)	loss 1.2631 (1.2956)	grad_norm 5.7125 (5.1198)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:07:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:36 lr 0.000012	 wd 0.0000	time 0.3115 (0.3547)	loss 1.1892 (1.2960)	grad_norm 5.2736 (5.1295)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:08:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0000	time 0.3273 (0.3538)	loss 1.4330 (1.2966)	grad_norm 5.0582 (5.1053)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:08:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 20 training takes 0:14:49
[2024-07-03 14:08:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.767 (16.767)	Loss 0.4004 (0.4004)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12173MB
[2024-07-03 14:09:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.036 Acc@5 97.518
[2024-07-03 14:09:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-03 14:09:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.04%
[2024-07-03 14:09:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 14:09:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 14:09:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][0/2502]	eta 10:37:37 lr 0.000012	 wd 0.0000	time 15.2907 (15.2907)	loss 1.4706 (1.4706)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:10:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:19:31 lr 0.000012	 wd 0.0000	time 0.2877 (0.4877)	loss 1.5166 (1.3287)	grad_norm 9.7354 (5.6064)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:10:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:15:54 lr 0.000012	 wd 0.0000	time 0.3194 (0.4144)	loss 0.9370 (1.2979)	grad_norm 3.4162 (5.1897)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:11:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:14:32 lr 0.000012	 wd 0.0000	time 0.3210 (0.3961)	loss 1.3544 (1.2906)	grad_norm 15.3959 (5.1991)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:11:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:13:22 lr 0.000011	 wd 0.0000	time 0.3065 (0.3818)	loss 1.2784 (1.2930)	grad_norm 4.1462 (5.1787)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:12:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:12:34 lr 0.000011	 wd 0.0000	time 0.3266 (0.3769)	loss 1.2591 (1.2920)	grad_norm 4.4250 (5.1702)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:12:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:11:51 lr 0.000011	 wd 0.0000	time 0.2968 (0.3739)	loss 1.1566 (1.2940)	grad_norm 5.2276 (5.1618)	loss_scale 1024.0000 (569.9301)	mem 12173MB
[2024-07-03 14:13:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:11:05 lr 0.000011	 wd 0.0000	time 0.3678 (0.3693)	loss 1.2269 (1.2882)	grad_norm 3.3485 (5.1261)	loss_scale 1024.0000 (634.7047)	mem 12173MB
[2024-07-03 14:14:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:10:23 lr 0.000011	 wd 0.0000	time 0.3135 (0.3663)	loss 1.1942 (1.2865)	grad_norm 3.7883 (5.1218)	loss_scale 1024.0000 (683.3059)	mem 12173MB
[2024-07-03 14:14:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:09:42 lr 0.000011	 wd 0.0000	time 0.3323 (0.3635)	loss 1.4855 (1.2856)	grad_norm 4.8886 (5.1184)	loss_scale 1024.0000 (721.1188)	mem 12173MB
[2024-07-03 14:15:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:09:03 lr 0.000011	 wd 0.0000	time 0.3171 (0.3615)	loss 1.0468 (1.2849)	grad_norm 3.5599 (5.0698)	loss_scale 1024.0000 (751.3766)	mem 12173MB
[2024-07-03 14:15:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:08:24 lr 0.000011	 wd 0.0000	time 0.3154 (0.3598)	loss 1.3714 (1.2853)	grad_norm 3.3280 (5.1415)	loss_scale 1024.0000 (776.1381)	mem 12173MB
[2024-07-03 14:16:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:07:46 lr 0.000011	 wd 0.0000	time 0.3635 (0.3583)	loss 1.3603 (1.2855)	grad_norm 3.6385 (5.1247)	loss_scale 1024.0000 (796.7760)	mem 12173MB
[2024-07-03 14:16:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:07:09 lr 0.000011	 wd 0.0000	time 0.3365 (0.3571)	loss 1.4932 (1.2849)	grad_norm 3.2384 (5.0880)	loss_scale 1024.0000 (814.2414)	mem 12173MB
[2024-07-03 14:17:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:06:32 lr 0.000011	 wd 0.0000	time 0.3432 (0.3563)	loss 1.2459 (1.2855)	grad_norm 5.6896 (5.1128)	loss_scale 1024.0000 (829.2134)	mem 12173MB
[2024-07-03 14:18:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:56 lr 0.000010	 wd 0.0000	time 0.3166 (0.3556)	loss 1.5995 (1.2851)	grad_norm 4.2515 (5.1040)	loss_scale 1024.0000 (842.1905)	mem 12173MB
[2024-07-03 14:18:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:05:20 lr 0.000010	 wd 0.0000	time 0.3207 (0.3548)	loss 1.2313 (1.2846)	grad_norm 4.2860 (5.0842)	loss_scale 1024.0000 (853.5465)	mem 12173MB
[2024-07-03 14:19:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:44 lr 0.000010	 wd 0.0000	time 0.3258 (0.3550)	loss 1.4284 (1.2852)	grad_norm 19.8972 (5.0813)	loss_scale 1024.0000 (863.5673)	mem 12173MB
[2024-07-03 14:19:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:04:09 lr 0.000010	 wd 0.0000	time 0.3315 (0.3554)	loss 1.0228 (1.2862)	grad_norm 4.6842 (5.1195)	loss_scale 1024.0000 (872.4753)	mem 12173MB
[2024-07-03 14:20:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:33 lr 0.000010	 wd 0.0000	time 0.3650 (0.3548)	loss 1.0353 (1.2845)	grad_norm 5.9255 (5.0992)	loss_scale 1024.0000 (880.4461)	mem 12173MB
[2024-07-03 14:21:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:57 lr 0.000010	 wd 0.0000	time 0.3196 (0.3546)	loss 1.3219 (1.2841)	grad_norm 4.5011 (5.0843)	loss_scale 1024.0000 (887.6202)	mem 12173MB
[2024-07-03 14:21:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:22 lr 0.000010	 wd 0.0000	time 0.3082 (0.3539)	loss 1.1278 (1.2857)	grad_norm 4.8116 (5.0696)	loss_scale 1024.0000 (894.1114)	mem 12173MB
[2024-07-03 14:22:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:46 lr 0.000010	 wd 0.0000	time 0.3344 (0.3533)	loss 1.5395 (1.2841)	grad_norm 4.0406 (5.0741)	loss_scale 1024.0000 (900.0127)	mem 12173MB
[2024-07-03 14:22:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:11 lr 0.000010	 wd 0.0000	time 0.3227 (0.3529)	loss 1.5481 (1.2831)	grad_norm 3.9633 (5.0838)	loss_scale 1024.0000 (905.4011)	mem 12173MB
[2024-07-03 14:23:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:35 lr 0.000010	 wd 0.0000	time 0.3194 (0.3523)	loss 1.3026 (1.2844)	grad_norm 3.5033 (5.0878)	loss_scale 1024.0000 (910.3407)	mem 12173MB
[2024-07-03 14:23:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.3042 (0.3514)	loss 1.3617 (1.2838)	grad_norm 3.5845 (5.0915)	loss_scale 1024.0000 (914.8852)	mem 12173MB
[2024-07-03 14:24:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 21 training takes 0:14:54
[2024-07-03 14:24:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.418 (16.418)	Loss 0.3936 (0.3936)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 14:24:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.956 Acc@5 97.514
[2024-07-03 14:24:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-03 14:24:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.04%
[2024-07-03 14:24:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][0/2502]	eta 10:38:38 lr 0.000010	 wd 0.0000	time 15.3151 (15.3151)	loss 1.1888 (1.1888)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:25:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:22:39 lr 0.000010	 wd 0.0000	time 0.3240 (0.5660)	loss 1.2919 (1.3063)	grad_norm 6.6528 (4.9290)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:26:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:17:24 lr 0.000009	 wd 0.0000	time 0.3045 (0.4537)	loss 0.9553 (1.2925)	grad_norm 3.7796 (4.8755)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:26:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:15:20 lr 0.000009	 wd 0.0000	time 0.3138 (0.4179)	loss 1.4314 (1.2958)	grad_norm 4.3399 (4.8189)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:27:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:13:57 lr 0.000009	 wd 0.0000	time 0.3580 (0.3982)	loss 1.2732 (1.2993)	grad_norm 5.4695 (5.0479)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:27:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:12:54 lr 0.000009	 wd 0.0000	time 0.3291 (0.3867)	loss 0.9705 (1.2985)	grad_norm 3.0711 (5.1414)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:28:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:12:00 lr 0.000009	 wd 0.0000	time 0.3170 (0.3786)	loss 1.3859 (1.2991)	grad_norm 5.8915 (5.0889)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:28:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:11:15 lr 0.000009	 wd 0.0000	time 0.3277 (0.3748)	loss 0.8597 (1.2931)	grad_norm 4.8770 (5.1359)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:29:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:10:31 lr 0.000009	 wd 0.0000	time 0.3318 (0.3708)	loss 1.2461 (1.2886)	grad_norm 3.4030 (5.1227)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:30:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:09:48 lr 0.000009	 wd 0.0000	time 0.3072 (0.3674)	loss 1.2554 (1.2867)	grad_norm 4.8720 (5.1136)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:30:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:09:10 lr 0.000009	 wd 0.0000	time 0.3235 (0.3663)	loss 1.4289 (1.2887)	grad_norm 4.1778 (5.1138)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:31:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:08:31 lr 0.000009	 wd 0.0000	time 0.3223 (0.3648)	loss 1.2972 (1.2871)	grad_norm 3.9917 (5.1450)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:31:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:07:52 lr 0.000009	 wd 0.0000	time 0.3116 (0.3625)	loss 1.2607 (1.2860)	grad_norm 4.6224 (5.1694)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:32:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:07:18 lr 0.000009	 wd 0.0000	time 0.3173 (0.3651)	loss 0.9432 (1.2867)	grad_norm 3.7026 (5.1443)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:33:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:06:40 lr 0.000008	 wd 0.0000	time 0.3316 (0.3635)	loss 1.4604 (1.2884)	grad_norm 11.9894 (5.1241)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:33:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:06:03 lr 0.000008	 wd 0.0000	time 0.3274 (0.3626)	loss 1.1142 (1.2887)	grad_norm 3.7359 (5.1247)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:34:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:05:26 lr 0.000008	 wd 0.0000	time 0.3140 (0.3619)	loss 1.4801 (1.2886)	grad_norm 6.7845 (5.0833)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:34:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:50 lr 0.000008	 wd 0.0000	time 0.3144 (0.3618)	loss 1.2594 (1.2879)	grad_norm 6.0997 (5.0963)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:35:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:04:14 lr 0.000008	 wd 0.0000	time 0.3551 (0.3625)	loss 1.5446 (1.2878)	grad_norm 7.5011 (5.0885)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:36:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:37 lr 0.000008	 wd 0.0000	time 0.3076 (0.3617)	loss 1.4521 (1.2871)	grad_norm 5.8969 (5.1198)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:36:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:03:01 lr 0.000008	 wd 0.0000	time 0.3563 (0.3611)	loss 1.3542 (1.2862)	grad_norm 3.2890 (5.1553)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 14:37:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:25 lr 0.000008	 wd 0.0000	time 0.3505 (0.3611)	loss 1.4124 (1.2866)	grad_norm 4.1017 (nan)	loss_scale 512.0000 (1004.5045)	mem 12173MB
[2024-07-03 14:37:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:48 lr 0.000008	 wd 0.0000	time 0.3078 (0.3602)	loss 1.4029 (1.2856)	grad_norm 4.2624 (nan)	loss_scale 512.0000 (982.1281)	mem 12173MB
[2024-07-03 14:38:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:12 lr 0.000008	 wd 0.0000	time 0.3350 (0.3592)	loss 1.5083 (1.2858)	grad_norm 3.2560 (nan)	loss_scale 512.0000 (961.6967)	mem 12173MB
[2024-07-03 14:38:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:36 lr 0.000008	 wd 0.0000	time 0.3179 (0.3592)	loss 1.0241 (1.2853)	grad_norm 4.5750 (nan)	loss_scale 512.0000 (942.9671)	mem 12173MB
[2024-07-03 14:39:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.2982 (0.3582)	loss 1.0083 (1.2845)	grad_norm 4.1372 (nan)	loss_scale 512.0000 (925.7353)	mem 12173MB
[2024-07-03 14:39:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 22 training takes 0:15:02
[2024-07-03 14:39:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.246 (16.246)	Loss 0.3945 (0.3945)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 14:40:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.974 Acc@5 97.536
[2024-07-03 14:40:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-03 14:40:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.04%
[2024-07-03 14:40:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][0/2502]	eta 12:47:48 lr 0.000008	 wd 0.0000	time 18.4128 (18.4128)	loss 1.0449 (1.0449)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:41:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:20:56 lr 0.000008	 wd 0.0000	time 0.3162 (0.5231)	loss 1.4129 (1.3245)	grad_norm 3.0074 (4.9901)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:41:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:16:34 lr 0.000007	 wd 0.0000	time 0.2968 (0.4320)	loss 1.4605 (1.3025)	grad_norm 4.5814 (4.9026)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:42:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:14:58 lr 0.000007	 wd 0.0000	time 0.3407 (0.4081)	loss 1.3005 (1.2991)	grad_norm 4.9951 (5.0241)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:42:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:13:41 lr 0.000007	 wd 0.0000	time 0.3430 (0.3908)	loss 1.6366 (1.2931)	grad_norm 8.0876 (4.9911)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:43:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:12:40 lr 0.000007	 wd 0.0000	time 0.3309 (0.3801)	loss 1.5361 (1.2971)	grad_norm 5.5217 (5.0096)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:43:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:11:55 lr 0.000007	 wd 0.0000	time 0.3317 (0.3760)	loss 1.1287 (1.2969)	grad_norm 6.9822 (5.0985)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:44:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:11:08 lr 0.000007	 wd 0.0000	time 0.3465 (0.3712)	loss 1.4054 (1.2936)	grad_norm 3.8458 (5.0437)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:45:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:10:26 lr 0.000007	 wd 0.0000	time 0.3381 (0.3682)	loss 1.6358 (1.2914)	grad_norm 3.9399 (5.0174)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:45:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:09:46 lr 0.000007	 wd 0.0000	time 0.3241 (0.3663)	loss 1.5978 (1.2884)	grad_norm 4.2127 (5.1492)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:46:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:09:06 lr 0.000007	 wd 0.0000	time 0.3181 (0.3638)	loss 1.0427 (1.2878)	grad_norm 3.9080 (5.1352)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:46:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:08:28 lr 0.000007	 wd 0.0000	time 0.2991 (0.3626)	loss 0.8034 (1.2829)	grad_norm 4.9648 (5.1008)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:47:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:50 lr 0.000007	 wd 0.0000	time 0.3178 (0.3613)	loss 1.5054 (1.2846)	grad_norm 5.5018 (5.1569)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:47:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:07:13 lr 0.000007	 wd 0.0000	time 0.2870 (0.3606)	loss 1.4582 (1.2859)	grad_norm 4.5856 (5.1544)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:48:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:06:36 lr 0.000007	 wd 0.0000	time 0.3099 (0.3597)	loss 1.2750 (1.2852)	grad_norm 4.0750 (5.1287)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:49:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:59 lr 0.000006	 wd 0.0000	time 0.3532 (0.3585)	loss 1.1524 (1.2868)	grad_norm 4.3414 (5.1632)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:49:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:05:22 lr 0.000006	 wd 0.0000	time 0.3035 (0.3573)	loss 1.1640 (1.2867)	grad_norm 5.1919 (5.1430)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:50:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:47 lr 0.000006	 wd 0.0000	time 0.3249 (0.3584)	loss 1.2761 (1.2871)	grad_norm 5.1604 (5.1393)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:50:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:04:10 lr 0.000006	 wd 0.0000	time 0.3300 (0.3573)	loss 1.2737 (1.2866)	grad_norm 8.3552 (5.1340)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:51:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:34 lr 0.000006	 wd 0.0000	time 0.3471 (0.3563)	loss 1.1714 (1.2888)	grad_norm 3.8113 (5.1308)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:52:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:59 lr 0.000006	 wd 0.0000	time 0.3183 (0.3577)	loss 1.3231 (1.2877)	grad_norm 4.2600 (5.1326)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:52:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:23 lr 0.000006	 wd 0.0000	time 0.3050 (0.3568)	loss 1.5997 (1.2878)	grad_norm 6.7309 (5.1274)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:53:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:47 lr 0.000006	 wd 0.0000	time 0.3263 (0.3566)	loss 1.4381 (1.2852)	grad_norm 9.8594 (5.1106)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:53:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:11 lr 0.000006	 wd 0.0000	time 0.3149 (0.3564)	loss 1.4534 (1.2849)	grad_norm 6.9618 (5.0898)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:54:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:36 lr 0.000006	 wd 0.0000	time 0.3278 (0.3559)	loss 1.5133 (1.2855)	grad_norm 4.7665 (5.0981)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:54:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0000	time 0.2979 (0.3549)	loss 1.3915 (1.2863)	grad_norm 10.8664 (5.1044)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:55:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 23 training takes 0:15:03
[2024-07-03 14:55:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.873 (16.873)	Loss 0.3928 (0.3928)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 12173MB
[2024-07-03 14:55:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 84.948 Acc@5 97.534
[2024-07-03 14:55:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-03 14:55:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.04%
[2024-07-03 14:56:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][0/2502]	eta 11:13:44 lr 0.000006	 wd 0.0000	time 16.1568 (16.1568)	loss 1.5023 (1.5023)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:56:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:20:54 lr 0.000006	 wd 0.0000	time 0.3522 (0.5225)	loss 1.1517 (1.3139)	grad_norm 4.9366 (5.0941)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:57:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:16:35 lr 0.000006	 wd 0.0000	time 0.3069 (0.4325)	loss 1.5677 (1.3032)	grad_norm 4.8984 (4.9270)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:57:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:14:42 lr 0.000006	 wd 0.0000	time 0.3069 (0.4009)	loss 1.5677 (1.2938)	grad_norm 4.8308 (5.1127)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:58:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:13:31 lr 0.000005	 wd 0.0000	time 0.2834 (0.3861)	loss 1.3733 (1.2888)	grad_norm 5.4513 (5.1398)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:58:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:12:35 lr 0.000005	 wd 0.0000	time 0.2862 (0.3774)	loss 1.4812 (1.2911)	grad_norm 3.9832 (5.2983)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 14:59:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:11:49 lr 0.000005	 wd 0.0000	time 0.3260 (0.3730)	loss 1.2842 (1.2867)	grad_norm 5.2312 (5.2130)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:00:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:11:04 lr 0.000005	 wd 0.0000	time 0.3183 (0.3688)	loss 1.2478 (1.2886)	grad_norm 6.6180 (5.1931)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:00:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:10:22 lr 0.000005	 wd 0.0000	time 0.3061 (0.3657)	loss 1.3472 (1.2897)	grad_norm 3.5826 (5.3117)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:01:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:09:41 lr 0.000005	 wd 0.0000	time 0.3078 (0.3628)	loss 1.3789 (1.2876)	grad_norm 4.4043 (5.2691)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:01:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:09:02 lr 0.000005	 wd 0.0000	time 0.3251 (0.3614)	loss 1.2590 (1.2879)	grad_norm 5.8591 (5.2574)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:02:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:08:27 lr 0.000005	 wd 0.0000	time 0.3113 (0.3623)	loss 1.4068 (1.2862)	grad_norm 4.8400 (nan)	loss_scale 512.0000 (525.9510)	mem 12173MB
[2024-07-03 15:02:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:49 lr 0.000005	 wd 0.0000	time 0.2941 (0.3602)	loss 1.1323 (1.2851)	grad_norm 4.3222 (nan)	loss_scale 512.0000 (524.7893)	mem 12173MB
[2024-07-03 15:03:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:07:13 lr 0.000005	 wd 0.0000	time 0.3422 (0.3606)	loss 1.2127 (1.2861)	grad_norm 4.7827 (nan)	loss_scale 512.0000 (523.8063)	mem 12173MB
[2024-07-03 15:04:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:06:36 lr 0.000005	 wd 0.0000	time 0.3266 (0.3598)	loss 1.1927 (1.2849)	grad_norm 3.3040 (nan)	loss_scale 512.0000 (522.9636)	mem 12173MB
[2024-07-03 15:04:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:59 lr 0.000005	 wd 0.0000	time 0.3438 (0.3585)	loss 0.9589 (1.2838)	grad_norm 4.1052 (nan)	loss_scale 512.0000 (522.2332)	mem 12173MB
[2024-07-03 15:05:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:05:23 lr 0.000005	 wd 0.0000	time 0.3401 (0.3583)	loss 1.3479 (1.2832)	grad_norm 5.0786 (nan)	loss_scale 512.0000 (521.5940)	mem 12173MB
[2024-07-03 15:05:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:46 lr 0.000005	 wd 0.0000	time 0.2943 (0.3574)	loss 1.6754 (1.2854)	grad_norm 16.5933 (nan)	loss_scale 512.0000 (521.0300)	mem 12173MB
[2024-07-03 15:06:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:04:10 lr 0.000005	 wd 0.0000	time 0.4212 (0.3573)	loss 1.1210 (1.2847)	grad_norm 3.4753 (nan)	loss_scale 512.0000 (520.5286)	mem 12173MB
[2024-07-03 15:07:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:35 lr 0.000005	 wd 0.0000	time 0.3298 (0.3574)	loss 1.4005 (1.2859)	grad_norm 3.6609 (nan)	loss_scale 512.0000 (520.0800)	mem 12173MB
[2024-07-03 15:07:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:58 lr 0.000004	 wd 0.0000	time 0.3433 (0.3565)	loss 1.4791 (1.2859)	grad_norm 5.7229 (nan)	loss_scale 512.0000 (519.6762)	mem 12173MB
[2024-07-03 15:08:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:23 lr 0.000004	 wd 0.0000	time 0.3154 (0.3562)	loss 1.4525 (1.2844)	grad_norm 6.4996 (nan)	loss_scale 512.0000 (519.3108)	mem 12173MB
[2024-07-03 15:08:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:47 lr 0.000004	 wd 0.0000	time 0.3258 (0.3562)	loss 1.2119 (1.2839)	grad_norm 4.0294 (nan)	loss_scale 512.0000 (518.9786)	mem 12173MB
[2024-07-03 15:09:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:11 lr 0.000004	 wd 0.0000	time 0.3222 (0.3553)	loss 1.6186 (1.2835)	grad_norm 4.2754 (nan)	loss_scale 512.0000 (518.6754)	mem 12173MB
[2024-07-03 15:09:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:36 lr 0.000004	 wd 0.0000	time 0.3437 (0.3552)	loss 1.4556 (1.2836)	grad_norm 3.3915 (nan)	loss_scale 512.0000 (518.3973)	mem 12173MB
[2024-07-03 15:10:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0000	time 0.2981 (0.3545)	loss 1.1864 (1.2841)	grad_norm 4.6664 (nan)	loss_scale 512.0000 (518.1415)	mem 12173MB
[2024-07-03 15:10:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 24 training takes 0:14:56
[2024-07-03 15:10:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.929 (16.929)	Loss 0.3901 (0.3901)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 12173MB
[2024-07-03 15:11:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.050 Acc@5 97.564
[2024-07-03 15:11:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-03 15:11:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.05%
[2024-07-03 15:11:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 15:11:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 15:11:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][0/2502]	eta 11:46:13 lr 0.000004	 wd 0.0000	time 16.9360 (16.9360)	loss 1.0122 (1.0122)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:12:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:20:38 lr 0.000004	 wd 0.0000	time 0.3233 (0.5156)	loss 1.2662 (1.2556)	grad_norm 3.4534 (4.7900)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:12:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:16:23 lr 0.000004	 wd 0.0000	time 0.3277 (0.4273)	loss 1.3515 (1.2767)	grad_norm 3.8255 (5.0634)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:13:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:14:40 lr 0.000004	 wd 0.0000	time 0.3206 (0.3999)	loss 0.9766 (1.2709)	grad_norm 3.1789 (5.3065)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:13:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:13:31 lr 0.000004	 wd 0.0000	time 0.3154 (0.3860)	loss 1.2102 (1.2842)	grad_norm 4.5336 (5.1717)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:14:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:12:38 lr 0.000004	 wd 0.0000	time 0.3366 (0.3790)	loss 1.3556 (1.2846)	grad_norm 20.7365 (5.1806)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:15:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:11:56 lr 0.000004	 wd 0.0000	time 0.4132 (0.3767)	loss 1.0888 (1.2864)	grad_norm 4.0621 (5.1400)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:15:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:11:10 lr 0.000004	 wd 0.0000	time 0.3336 (0.3719)	loss 1.1589 (1.2863)	grad_norm 5.0393 (5.1815)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:16:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:10:26 lr 0.000004	 wd 0.0000	time 0.3124 (0.3680)	loss 1.0269 (1.2868)	grad_norm 4.3550 (5.2809)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:16:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:09:54 lr 0.000004	 wd 0.0000	time 0.3115 (0.3713)	loss 1.3604 (1.2891)	grad_norm 4.3104 (5.2485)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:17:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:09:14 lr 0.000004	 wd 0.0000	time 0.3219 (0.3689)	loss 1.2564 (1.2900)	grad_norm 3.9898 (5.3480)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:18:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:08:38 lr 0.000004	 wd 0.0000	time 0.3327 (0.3699)	loss 1.2855 (1.2875)	grad_norm 4.4231 (5.3501)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:18:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:07:59 lr 0.000004	 wd 0.0000	time 0.3100 (0.3679)	loss 1.2154 (1.2890)	grad_norm 5.4649 (5.2939)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:19:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:07:19 lr 0.000003	 wd 0.0000	time 0.3194 (0.3659)	loss 1.4435 (1.2891)	grad_norm 4.6702 (5.3161)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:19:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:06:44 lr 0.000003	 wd 0.0000	time 0.6066 (0.3672)	loss 1.1533 (1.2900)	grad_norm 3.5859 (5.2515)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:20:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:06:07 lr 0.000003	 wd 0.0000	time 0.3611 (0.3667)	loss 1.1890 (1.2893)	grad_norm 6.0979 (5.2280)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:21:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:05:29 lr 0.000003	 wd 0.0000	time 0.3069 (0.3651)	loss 1.4405 (1.2894)	grad_norm 8.1359 (5.2121)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:21:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:52 lr 0.000003	 wd 0.0000	time 0.3185 (0.3641)	loss 1.2733 (1.2893)	grad_norm 4.1784 (5.1839)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:22:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:04:14 lr 0.000003	 wd 0.0000	time 0.2939 (0.3627)	loss 1.3140 (1.2886)	grad_norm 5.0103 (5.2168)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:22:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:38 lr 0.000003	 wd 0.0000	time 0.3107 (0.3629)	loss 1.5327 (1.2888)	grad_norm 3.9530 (5.2005)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:23:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:03:01 lr 0.000003	 wd 0.0000	time 0.3409 (0.3623)	loss 1.2405 (1.2883)	grad_norm 7.4256 (5.1862)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:23:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:25 lr 0.000003	 wd 0.0000	time 0.2964 (0.3622)	loss 0.8391 (1.2885)	grad_norm 5.0539 (5.1864)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:24:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:49 lr 0.000003	 wd 0.0000	time 0.3287 (0.3612)	loss 1.3529 (1.2872)	grad_norm 3.1792 (5.1788)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:25:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:13 lr 0.000003	 wd 0.0000	time 0.3441 (0.3619)	loss 1.3882 (1.2858)	grad_norm 3.8356 (5.1576)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:25:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:36 lr 0.000003	 wd 0.0000	time 0.3193 (0.3609)	loss 1.3672 (1.2871)	grad_norm 3.3958 (5.1726)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:26:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.3195 (0.3598)	loss 1.4354 (1.2873)	grad_norm 4.0163 (5.1860)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:26:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 25 training takes 0:15:16
[2024-07-03 15:26:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.864 (17.864)	Loss 0.3928 (0.3928)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 12173MB
[2024-07-03 15:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.094 Acc@5 97.560
[2024-07-03 15:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-03 15:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.09%
[2024-07-03 15:27:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 15:27:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 15:27:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][0/2502]	eta 10:47:44 lr 0.000003	 wd 0.0000	time 15.5333 (15.5333)	loss 1.1087 (1.1087)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 15:28:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:21:06 lr 0.000003	 wd 0.0000	time 0.3070 (0.5272)	loss 1.4578 (1.2774)	grad_norm 6.5968 (4.8853)	loss_scale 1024.0000 (806.0198)	mem 12173MB
[2024-07-03 15:28:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:16:40 lr 0.000003	 wd 0.0000	time 0.3225 (0.4344)	loss 1.3704 (1.3029)	grad_norm 21.8152 (5.6093)	loss_scale 1024.0000 (914.4677)	mem 12173MB
[2024-07-03 15:29:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:14:47 lr 0.000003	 wd 0.0000	time 0.3152 (0.4030)	loss 1.2664 (1.2956)	grad_norm 4.9479 (5.4219)	loss_scale 1024.0000 (950.8571)	mem 12173MB
[2024-07-03 15:29:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:13:37 lr 0.000003	 wd 0.0000	time 0.3016 (0.3888)	loss 1.5985 (1.2974)	grad_norm 23.7998 (5.3793)	loss_scale 1024.0000 (969.0973)	mem 12173MB
[2024-07-03 15:30:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:12:37 lr 0.000003	 wd 0.0000	time 0.3347 (0.3786)	loss 1.6194 (1.3044)	grad_norm 3.6206 (5.3558)	loss_scale 1024.0000 (980.0559)	mem 12173MB
[2024-07-03 15:30:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:11:51 lr 0.000003	 wd 0.0000	time 0.3118 (0.3738)	loss 1.2352 (1.3003)	grad_norm 3.8557 (5.3002)	loss_scale 1024.0000 (987.3677)	mem 12173MB
[2024-07-03 15:31:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:11:08 lr 0.000003	 wd 0.0000	time 0.3336 (0.3712)	loss 1.3347 (1.2939)	grad_norm 4.1084 (5.2844)	loss_scale 1024.0000 (992.5934)	mem 12173MB
[2024-07-03 15:32:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:10:25 lr 0.000002	 wd 0.0000	time 0.3382 (0.3675)	loss 1.3635 (1.2911)	grad_norm 7.8467 (5.2594)	loss_scale 1024.0000 (996.5144)	mem 12173MB
[2024-07-03 15:32:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:09:43 lr 0.000002	 wd 0.0000	time 0.3108 (0.3644)	loss 1.3085 (1.2925)	grad_norm 7.4784 (5.2063)	loss_scale 1024.0000 (999.5649)	mem 12173MB
[2024-07-03 15:33:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:09:06 lr 0.000002	 wd 0.0000	time 0.3205 (0.3641)	loss 1.4239 (1.2926)	grad_norm 5.4894 (5.2477)	loss_scale 1024.0000 (1002.0060)	mem 12173MB
[2024-07-03 15:33:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:08:27 lr 0.000002	 wd 0.0000	time 0.3375 (0.3620)	loss 1.3894 (1.2944)	grad_norm 4.3442 (5.2401)	loss_scale 1024.0000 (1004.0036)	mem 12173MB
[2024-07-03 15:34:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:07:52 lr 0.000002	 wd 0.0000	time 0.5345 (0.3626)	loss 1.0032 (1.2950)	grad_norm 8.2799 (5.2434)	loss_scale 1024.0000 (1005.6686)	mem 12173MB
[2024-07-03 15:35:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:07:20 lr 0.000002	 wd 0.0000	time 0.3225 (0.3665)	loss 0.9390 (1.2927)	grad_norm 3.3939 (5.2049)	loss_scale 1024.0000 (1007.0776)	mem 12173MB
[2024-07-03 15:35:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:06:41 lr 0.000002	 wd 0.0000	time 0.3166 (0.3648)	loss 1.7032 (1.2927)	grad_norm 5.6279 (5.2013)	loss_scale 1024.0000 (1008.2855)	mem 12173MB
[2024-07-03 15:36:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:06:04 lr 0.000002	 wd 0.0000	time 0.3273 (0.3640)	loss 1.4506 (1.2915)	grad_norm 3.8265 (5.2630)	loss_scale 1024.0000 (1009.3324)	mem 12173MB
[2024-07-03 15:36:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:05:26 lr 0.000002	 wd 0.0000	time 0.3097 (0.3625)	loss 1.3635 (1.2904)	grad_norm 5.9637 (5.2699)	loss_scale 1024.0000 (1010.2486)	mem 12173MB
[2024-07-03 15:37:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:50 lr 0.000002	 wd 0.0000	time 0.3295 (0.3620)	loss 1.2862 (1.2909)	grad_norm 5.0898 (5.2647)	loss_scale 1024.0000 (1011.0570)	mem 12173MB
[2024-07-03 15:38:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:04:14 lr 0.000002	 wd 0.0000	time 0.3234 (0.3619)	loss 1.3918 (1.2925)	grad_norm 3.7109 (5.2223)	loss_scale 1024.0000 (1011.7757)	mem 12173MB
[2024-07-03 15:38:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:37 lr 0.000002	 wd 0.0000	time 0.3130 (0.3608)	loss 1.0229 (1.2905)	grad_norm 8.6072 (5.2368)	loss_scale 1024.0000 (1012.4187)	mem 12173MB
[2024-07-03 15:39:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:03:00 lr 0.000002	 wd 0.0000	time 0.3188 (0.3596)	loss 1.1478 (1.2904)	grad_norm 6.4724 (5.2425)	loss_scale 1024.0000 (1012.9975)	mem 12173MB
[2024-07-03 15:39:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:24 lr 0.000002	 wd 0.0000	time 0.3322 (0.3595)	loss 0.9249 (1.2889)	grad_norm 5.3190 (5.2355)	loss_scale 1024.0000 (1013.5212)	mem 12173MB
[2024-07-03 15:40:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:48 lr 0.000002	 wd 0.0000	time 0.3338 (0.3587)	loss 0.8728 (1.2886)	grad_norm 5.4155 (5.2363)	loss_scale 1024.0000 (1013.9973)	mem 12173MB
[2024-07-03 15:40:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:12 lr 0.000002	 wd 0.0000	time 0.3479 (0.3579)	loss 1.0563 (1.2877)	grad_norm 4.8386 (5.2371)	loss_scale 1024.0000 (1014.4320)	mem 12173MB
[2024-07-03 15:41:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:36 lr 0.000002	 wd 0.0000	time 0.3301 (0.3577)	loss 1.0460 (1.2874)	grad_norm 4.8028 (5.2068)	loss_scale 1024.0000 (1014.8305)	mem 12173MB
[2024-07-03 15:42:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.3122 (0.3567)	loss 1.0056 (1.2859)	grad_norm 4.7288 (5.2104)	loss_scale 1024.0000 (1015.1971)	mem 12173MB
[2024-07-03 15:42:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 26 training takes 0:15:02
[2024-07-03 15:42:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.307 (16.307)	Loss 0.3884 (0.3884)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 12173MB
[2024-07-03 15:42:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.068 Acc@5 97.576
[2024-07-03 15:42:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-03 15:42:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.09%
[2024-07-03 15:43:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][0/2502]	eta 11:01:28 lr 0.000002	 wd 0.0000	time 15.8627 (15.8627)	loss 0.8929 (0.8929)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:43:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:19:59 lr 0.000002	 wd 0.0000	time 0.2916 (0.4994)	loss 1.3803 (1.3033)	grad_norm 7.1081 (5.1827)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:44:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:16:24 lr 0.000002	 wd 0.0000	time 0.3488 (0.4276)	loss 1.4412 (1.2932)	grad_norm 5.0177 (4.9368)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:44:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:14:40 lr 0.000002	 wd 0.0000	time 0.3223 (0.3998)	loss 1.2275 (1.2860)	grad_norm 5.8201 (4.9454)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:45:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:13:28 lr 0.000002	 wd 0.0000	time 0.3092 (0.3849)	loss 1.2882 (1.2856)	grad_norm 3.2635 (5.0092)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:46:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:12:32 lr 0.000002	 wd 0.0000	time 0.3008 (0.3760)	loss 0.8015 (1.2791)	grad_norm 9.0947 (4.9979)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:46:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:11:46 lr 0.000002	 wd 0.0000	time 0.3116 (0.3717)	loss 1.3213 (1.2801)	grad_norm 3.5626 (5.0839)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:47:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:11:03 lr 0.000002	 wd 0.0000	time 0.3238 (0.3683)	loss 1.3850 (1.2848)	grad_norm 2.9757 (5.0277)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:47:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:10:24 lr 0.000002	 wd 0.0000	time 0.3385 (0.3672)	loss 1.5078 (1.2881)	grad_norm 3.2806 (5.0407)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:48:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:09:44 lr 0.000001	 wd 0.0000	time 0.3070 (0.3651)	loss 1.3438 (1.2927)	grad_norm 3.9362 (5.0083)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:48:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:09:04 lr 0.000001	 wd 0.0000	time 0.3190 (0.3624)	loss 1.3201 (1.2919)	grad_norm 5.7539 (5.0486)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:49:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:08:27 lr 0.000001	 wd 0.0000	time 0.3549 (0.3618)	loss 1.3058 (1.2929)	grad_norm 4.7200 (5.1500)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:50:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:49 lr 0.000001	 wd 0.0000	time 0.3337 (0.3608)	loss 1.4807 (1.2930)	grad_norm 5.6631 (5.0975)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:50:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:07:11 lr 0.000001	 wd 0.0000	time 0.3049 (0.3594)	loss 1.4009 (1.2913)	grad_norm 4.3746 (5.0947)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:51:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:06:36 lr 0.000001	 wd 0.0000	time 0.3388 (0.3601)	loss 1.2761 (1.2924)	grad_norm 7.5056 (5.1008)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:51:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:59 lr 0.000001	 wd 0.0000	time 0.3435 (0.3587)	loss 1.4334 (1.2924)	grad_norm 4.0255 (5.1527)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:52:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:05:22 lr 0.000001	 wd 0.0000	time 0.3745 (0.3574)	loss 1.4317 (1.2905)	grad_norm 4.6756 (5.1546)	loss_scale 2048.0000 (1062.3760)	mem 12173MB
[2024-07-03 15:53:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:48 lr 0.000001	 wd 0.0000	time 0.3060 (0.3592)	loss 1.3366 (1.2886)	grad_norm 6.1558 (5.1611)	loss_scale 2048.0000 (1120.3198)	mem 12173MB
[2024-07-03 15:53:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:04:11 lr 0.000001	 wd 0.0000	time 0.3341 (0.3581)	loss 1.3151 (1.2889)	grad_norm 3.8612 (5.1540)	loss_scale 2048.0000 (1171.8290)	mem 12173MB
[2024-07-03 15:54:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:35 lr 0.000001	 wd 0.0000	time 0.3334 (0.3581)	loss 0.9142 (1.2900)	grad_norm 5.3091 (5.1518)	loss_scale 2048.0000 (1217.9190)	mem 12173MB
[2024-07-03 15:54:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:59 lr 0.000001	 wd 0.0000	time 0.3143 (0.3576)	loss 0.8696 (1.2886)	grad_norm 3.6660 (5.1397)	loss_scale 2048.0000 (1259.4023)	mem 12173MB
[2024-07-03 15:55:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:23 lr 0.000001	 wd 0.0000	time 0.3225 (0.3568)	loss 1.6540 (1.2873)	grad_norm 4.4946 (5.1544)	loss_scale 2048.0000 (1296.9367)	mem 12173MB
[2024-07-03 15:56:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:47 lr 0.000001	 wd 0.0000	time 0.3099 (0.3565)	loss 0.9624 (1.2881)	grad_norm 6.8801 (5.1488)	loss_scale 2048.0000 (1331.0604)	mem 12173MB
[2024-07-03 15:56:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:11 lr 0.000001	 wd 0.0000	time 0.3525 (0.3558)	loss 1.4004 (1.2884)	grad_norm 3.5849 (5.1646)	loss_scale 2048.0000 (1362.2182)	mem 12173MB
[2024-07-03 15:57:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:36 lr 0.000001	 wd 0.0000	time 0.2919 (0.3551)	loss 1.1414 (1.2881)	grad_norm 4.5429 (inf)	loss_scale 1024.0000 (1349.8376)	mem 12173MB
[2024-07-03 15:57:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.3020 (0.3543)	loss 1.3879 (1.2866)	grad_norm 4.0997 (inf)	loss_scale 1024.0000 (1336.8093)	mem 12173MB
[2024-07-03 15:57:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 27 training takes 0:15:02
[2024-07-03 15:58:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 17.243 (17.243)	Loss 0.3904 (0.3904)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 12173MB
[2024-07-03 15:58:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.088 Acc@5 97.598
[2024-07-03 15:58:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-03 15:58:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.09%
[2024-07-03 15:58:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][0/2502]	eta 10:41:57 lr 0.000001	 wd 0.0000	time 15.3948 (15.3948)	loss 1.1948 (1.1948)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:59:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:21:12 lr 0.000001	 wd 0.0000	time 0.3160 (0.5299)	loss 0.9505 (1.2702)	grad_norm 3.3925 (4.8266)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 15:59:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:16:41 lr 0.000001	 wd 0.0000	time 0.3309 (0.4350)	loss 1.0442 (1.2737)	grad_norm 3.9446 (5.1726)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:00:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:14:47 lr 0.000001	 wd 0.0000	time 0.3180 (0.4031)	loss 1.4907 (1.2881)	grad_norm 5.4227 (5.1141)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:01:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:13:37 lr 0.000001	 wd 0.0000	time 0.3227 (0.3889)	loss 1.4530 (1.2926)	grad_norm 4.2030 (4.9362)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:01:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:12:38 lr 0.000001	 wd 0.0000	time 0.2956 (0.3789)	loss 1.5372 (1.2919)	grad_norm 3.2239 (4.9364)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:02:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:11:48 lr 0.000001	 wd 0.0000	time 0.3700 (0.3725)	loss 1.3407 (1.2922)	grad_norm 5.0968 (4.9371)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:02:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:11:08 lr 0.000001	 wd 0.0000	time 0.3176 (0.3711)	loss 0.9841 (1.2945)	grad_norm 3.5883 (4.9004)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:03:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:10:24 lr 0.000001	 wd 0.0000	time 0.3414 (0.3671)	loss 1.5226 (1.2922)	grad_norm 6.7181 (4.9612)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:03:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:09:43 lr 0.000001	 wd 0.0000	time 0.3790 (0.3642)	loss 1.4003 (1.2923)	grad_norm 5.9875 (4.9457)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:04:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:09:04 lr 0.000001	 wd 0.0000	time 0.3210 (0.3627)	loss 1.3590 (1.2928)	grad_norm 4.2936 (5.0332)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:05:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:08:25 lr 0.000001	 wd 0.0000	time 0.3214 (0.3608)	loss 1.0285 (1.2914)	grad_norm 5.1873 (5.0653)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:05:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:07:49 lr 0.000001	 wd 0.0000	time 0.3063 (0.3608)	loss 1.0770 (1.2899)	grad_norm 4.4356 (5.0624)	loss_scale 1024.0000 (1024.0000)	mem 12173MB
[2024-07-03 16:06:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:07:11 lr 0.000001	 wd 0.0000	time 0.3070 (0.3591)	loss 1.3553 (1.2872)	grad_norm 4.0979 (inf)	loss_scale 512.0000 (1003.5357)	mem 12173MB
[2024-07-03 16:06:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:06:34 lr 0.000001	 wd 0.0000	time 0.3017 (0.3577)	loss 0.9170 (1.2878)	grad_norm 3.5261 (inf)	loss_scale 512.0000 (968.4511)	mem 12173MB
[2024-07-03 16:07:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:58 lr 0.000001	 wd 0.0000	time 0.2967 (0.3578)	loss 1.4202 (1.2851)	grad_norm 4.1629 (inf)	loss_scale 512.0000 (938.0413)	mem 12173MB
[2024-07-03 16:08:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:05:21 lr 0.000001	 wd 0.0000	time 0.3300 (0.3568)	loss 1.1670 (1.2851)	grad_norm 5.6226 (inf)	loss_scale 512.0000 (911.4304)	mem 12173MB
[2024-07-03 16:08:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:45 lr 0.000001	 wd 0.0000	time 0.3249 (0.3556)	loss 1.4327 (1.2861)	grad_norm 10.6983 (inf)	loss_scale 512.0000 (887.9483)	mem 12173MB
[2024-07-03 16:09:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:04:10 lr 0.000001	 wd 0.0000	time 0.3234 (0.3565)	loss 1.3009 (1.2852)	grad_norm 4.3546 (inf)	loss_scale 512.0000 (867.0738)	mem 12173MB
[2024-07-03 16:09:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:34 lr 0.000001	 wd 0.0000	time 0.3086 (0.3562)	loss 1.5619 (1.2839)	grad_norm 4.5566 (inf)	loss_scale 512.0000 (848.3956)	mem 12173MB
[2024-07-03 16:10:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:58 lr 0.000001	 wd 0.0000	time 0.3142 (0.3552)	loss 0.8396 (1.2834)	grad_norm 4.7473 (inf)	loss_scale 512.0000 (831.5842)	mem 12173MB
[2024-07-03 16:10:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:22 lr 0.000001	 wd 0.0000	time 0.3188 (0.3552)	loss 1.1148 (1.2836)	grad_norm 3.9623 (inf)	loss_scale 512.0000 (816.3732)	mem 12173MB
[2024-07-03 16:11:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:47 lr 0.000001	 wd 0.0000	time 0.3103 (0.3548)	loss 1.4218 (1.2826)	grad_norm 4.5061 (inf)	loss_scale 512.0000 (802.5443)	mem 12173MB
[2024-07-03 16:12:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:11 lr 0.000001	 wd 0.0000	time 0.3198 (0.3542)	loss 1.1610 (1.2829)	grad_norm 4.3187 (inf)	loss_scale 512.0000 (789.9174)	mem 12173MB
[2024-07-03 16:12:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:36 lr 0.000001	 wd 0.0000	time 0.3298 (0.3546)	loss 1.4901 (1.2818)	grad_norm 4.8570 (inf)	loss_scale 512.0000 (778.3424)	mem 12173MB
[2024-07-03 16:13:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.2995 (0.3537)	loss 1.1283 (1.2805)	grad_norm 4.4016 (inf)	loss_scale 512.0000 (767.6929)	mem 12173MB
[2024-07-03 16:13:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 28 training takes 0:14:52
[2024-07-03 16:13:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 16.192 (16.192)	Loss 0.3909 (0.3909)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 12173MB
[2024-07-03 16:14:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.130 Acc@5 97.572
[2024-07-03 16:14:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-03 16:14:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.13%
[2024-07-03 16:14:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saving......
[2024-07-03 16:14:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-03 16:14:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][0/2502]	eta 10:21:14 lr 0.000001	 wd 0.0000	time 14.8980 (14.8980)	loss 1.4201 (1.4201)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:15:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:20:03 lr 0.000001	 wd 0.0000	time 0.3200 (0.5011)	loss 1.4246 (1.2592)	grad_norm 4.2332 (5.6080)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:15:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:16:19 lr 0.000001	 wd 0.0000	time 0.2950 (0.4254)	loss 1.1471 (1.2760)	grad_norm 4.4562 (5.4786)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:16:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:14:35 lr 0.000001	 wd 0.0000	time 0.3394 (0.3974)	loss 1.4401 (1.2761)	grad_norm 4.7016 (5.3800)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:16:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:13:26 lr 0.000001	 wd 0.0000	time 0.2746 (0.3837)	loss 1.1628 (1.2804)	grad_norm 3.3105 (5.3155)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:17:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:12:39 lr 0.000001	 wd 0.0000	time 0.3367 (0.3794)	loss 0.8502 (1.2847)	grad_norm 4.4988 (5.3638)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:17:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:11:51 lr 0.000000	 wd 0.0000	time 0.3552 (0.3743)	loss 1.2863 (1.2889)	grad_norm 7.8526 (5.3310)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:18:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:11:05 lr 0.000000	 wd 0.0000	time 0.3423 (0.3694)	loss 1.5711 (1.2888)	grad_norm 5.4469 (5.3261)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:19:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:10:29 lr 0.000000	 wd 0.0000	time 0.3204 (0.3697)	loss 0.8013 (1.2879)	grad_norm 3.9078 (5.2975)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:19:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:09:48 lr 0.000000	 wd 0.0000	time 0.3290 (0.3671)	loss 1.2648 (1.2882)	grad_norm 6.4544 (5.2705)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:20:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:09:07 lr 0.000000	 wd 0.0000	time 0.3110 (0.3643)	loss 1.2445 (1.2869)	grad_norm 9.7811 (5.3141)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:20:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:08:30 lr 0.000000	 wd 0.0000	time 0.3381 (0.3643)	loss 1.2541 (1.2870)	grad_norm 3.6389 (5.2872)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:21:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:07:52 lr 0.000000	 wd 0.0000	time 0.3245 (0.3626)	loss 1.3199 (1.2844)	grad_norm 4.2078 (5.2749)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:22:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:07:15 lr 0.000000	 wd 0.0000	time 0.3317 (0.3622)	loss 1.4504 (1.2851)	grad_norm 13.6890 (5.2529)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:22:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:06:39 lr 0.000000	 wd 0.0000	time 0.3484 (0.3624)	loss 1.5343 (1.2825)	grad_norm 6.6551 (5.2741)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:23:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:06:01 lr 0.000000	 wd 0.0000	time 0.3022 (0.3609)	loss 1.3012 (1.2836)	grad_norm 13.7693 (5.2993)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:23:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:05:24 lr 0.000000	 wd 0.0000	time 0.2928 (0.3599)	loss 1.2536 (1.2852)	grad_norm 4.2241 (5.2909)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:24:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:48 lr 0.000000	 wd 0.0000	time 0.3389 (0.3591)	loss 1.0829 (1.2832)	grad_norm 4.8940 (5.2880)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:24:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:04:11 lr 0.000000	 wd 0.0000	time 0.3553 (0.3580)	loss 1.1115 (1.2848)	grad_norm 4.5960 (5.2776)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:25:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:35 lr 0.000000	 wd 0.0000	time 0.3442 (0.3576)	loss 1.2316 (1.2840)	grad_norm 5.3547 (5.2730)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:26:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:59 lr 0.000000	 wd 0.0000	time 0.3053 (0.3578)	loss 1.5014 (1.2839)	grad_norm 4.4014 (5.2477)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:26:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:23 lr 0.000000	 wd 0.0000	time 0.2998 (0.3577)	loss 1.3688 (1.2845)	grad_norm 11.1455 (5.2440)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:27:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:48 lr 0.000000	 wd 0.0000	time 0.2990 (0.3581)	loss 1.4240 (1.2862)	grad_norm 11.3842 (5.2601)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:27:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:12 lr 0.000000	 wd 0.0000	time 0.3036 (0.3577)	loss 1.4821 (1.2872)	grad_norm 4.2277 (5.2573)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:28:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:36 lr 0.000000	 wd 0.0000	time 0.3199 (0.3571)	loss 1.4556 (1.2879)	grad_norm 6.9729 (5.2458)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:29:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0000	time 0.3027 (0.3572)	loss 1.3519 (1.2873)	grad_norm 4.6801 (5.2483)	loss_scale 512.0000 (512.0000)	mem 12173MB
[2024-07-03 16:29:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 249): INFO EPOCH 29 training takes 0:15:06
[2024-07-03 16:29:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_29.pth saving......
[2024-07-03 16:29:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune/diffusion_ft_swin_base_patch4_22kto1k_finetune_sequence_crosslayer_full-finetune/ckpt_epoch_29.pth saved !!!
[2024-07-03 16:29:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 289): INFO Test: [0/98]	Time 15.773 (15.773)	Loss 0.3911 (0.3911)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 12173MB
[2024-07-03 16:29:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 296): INFO  * Acc@1 85.114 Acc@5 97.588
[2024-07-03 16:29:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-03 16:29:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 182): INFO Max accuracy: 85.13%
[2024-07-03 16:29:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process1-full-finetune] (main.py 189): INFO Training time 7:45:04
