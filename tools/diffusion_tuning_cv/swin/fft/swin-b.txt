[2024-05-27 19:07:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 352): INFO Full config saved to /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/config.json
[2024-05-27 19:07:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 355): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x
  NUM_CLASSES: 1000
  PRETRAINED: /root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SMT:
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    HEAD_CONV: 3
    IN_CHANS: 3
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    NUM_STAGES: 4
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 16
    - 32
    - 64
    - 128
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin_mam_v2
OUTPUT: /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k
PRINT_FREQ: 100
SAVE_FREQ: 30
SEED: 0
TAG: swin_mam_v2_base_p4_w7_224_22kto1k
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-05-27 19:07:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 356): INFO {"cfg": "/root/mam_classifier/configs/swin_mam_v2/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "/mnt/data/pretrain_weights/swin-mam/", "tag": "swin_mam_v2_base_p4_w7_224_22kto1k", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-05-27 19:07:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 93): INFO Creating model:swin_mam_v2/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x
[2024-05-27 19:07:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 95): INFO MemorySwinTransformerV2(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=128, out_features=16, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=16, out_features=128, bias=True)
            (ltm_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=256, out_features=32, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=32, out_features=256, bias=True)
            (ltm_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=1024, out_features=12, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=12, out_features=1024, bias=True)
            (ltm_norm): LayerNorm((12,), eps=1e-05, elementwise_affine=True)
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-05-27 19:07:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 98): INFO number of params: 88941196
[2024-05-27 19:07:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 101): INFO number of GFLOPs: 15.438473216
[2024-05-27 19:07:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 136): INFO no checkpoint found in /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k, ignoring auto resume
[2024-05-27 19:07:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 46): INFO ==============> Loading weight /root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-05-27 19:08:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-05-27 19:08:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.0.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.0.blocks.0.attn.ltm_norm.weight', 'layers.0.blocks.0.attn.ltm_norm.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.0.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.0.blocks.1.attn.ltm_norm.weight', 'layers.0.blocks.1.attn.ltm_norm.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.1.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.1.blocks.0.attn.ltm_norm.weight', 'layers.1.blocks.0.attn.ltm_norm.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.1.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.1.blocks.1.attn.ltm_norm.weight', 'layers.1.blocks.1.attn.ltm_norm.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.0.attn.ltm_norm.weight', 'layers.2.blocks.0.attn.ltm_norm.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.1.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.1.attn.ltm_norm.weight', 'layers.2.blocks.1.attn.ltm_norm.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.2.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.2.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.2.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.2.attn.ltm_norm.weight', 'layers.2.blocks.2.attn.ltm_norm.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.3.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.3.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.3.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.3.attn.ltm_norm.weight', 'layers.2.blocks.3.attn.ltm_norm.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.4.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.4.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.4.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.4.attn.ltm_norm.weight', 'layers.2.blocks.4.attn.ltm_norm.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.5.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.5.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.5.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.5.attn.ltm_norm.weight', 'layers.2.blocks.5.attn.ltm_norm.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.6.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.6.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.6.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.6.attn.ltm_norm.weight', 'layers.2.blocks.6.attn.ltm_norm.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.7.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.7.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.7.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.7.attn.ltm_norm.weight', 'layers.2.blocks.7.attn.ltm_norm.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.8.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.8.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.8.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.8.attn.ltm_norm.weight', 'layers.2.blocks.8.attn.ltm_norm.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.9.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.9.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.9.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.9.attn.ltm_norm.weight', 'layers.2.blocks.9.attn.ltm_norm.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.10.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.10.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.10.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.10.attn.ltm_norm.weight', 'layers.2.blocks.10.attn.ltm_norm.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.11.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.11.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.11.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.11.attn.ltm_norm.weight', 'layers.2.blocks.11.attn.ltm_norm.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.12.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.12.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.12.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.12.attn.ltm_norm.weight', 'layers.2.blocks.12.attn.ltm_norm.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.13.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.13.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.13.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.13.attn.ltm_norm.weight', 'layers.2.blocks.13.attn.ltm_norm.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.14.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.14.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.14.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.14.attn.ltm_norm.weight', 'layers.2.blocks.14.attn.ltm_norm.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.15.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.15.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.15.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.15.attn.ltm_norm.weight', 'layers.2.blocks.15.attn.ltm_norm.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.16.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.16.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.16.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.16.attn.ltm_norm.weight', 'layers.2.blocks.16.attn.ltm_norm.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.17.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.17.attn.ltm_norm.weight', 'layers.2.blocks.17.attn.ltm_norm.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.3.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.3.blocks.0.attn.ltm_norm.weight', 'layers.3.blocks.0.attn.ltm_norm.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.3.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.3.blocks.1.attn.ltm_norm.weight', 'layers.3.blocks.1.attn.ltm_norm.bias'], unexpected_keys=[])
[2024-05-27 19:08:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 129): INFO => loaded successfully '/root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth'
[2024-05-27 19:09:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 95.413 (95.413)	Loss 0.3591 (0.3591)	Acc@1 90.625 (90.625)	Acc@5 98.047 (98.047)	Mem 1743MB
[2024-05-27 19:09:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 82.460 Acc@5 96.534
[2024-05-27 19:09:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 148): INFO Accuracy of the network on the 50000 test images: 82.5%
[2024-05-27 19:09:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 154): INFO Start training
[2024-05-27 19:10:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][0/2502]	eta 13:10:08 lr 0.000000	 wd 0.0000	time 18.9482 (18.9482)	loss 1.6204 (1.6204)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 11626MB
[2024-05-27 19:32:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 352): INFO Full config saved to /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/config.json
[2024-05-27 19:32:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 355): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x
  NUM_CLASSES: 1000
  PRETRAINED: /root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SMT:
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    HEAD_CONV: 3
    IN_CHANS: 3
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    NUM_STAGES: 4
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 16
    - 32
    - 64
    - 128
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin_mam_v2
OUTPUT: /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k
PRINT_FREQ: 100
SAVE_FREQ: 30
SEED: 0
TAG: swin_mam_v2_base_p4_w7_224_22kto1k
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-05-27 19:32:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 356): INFO {"cfg": "/root/mam_classifier/configs/swin_mam_v2/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "/mnt/data/pretrain_weights/swin-mam/", "tag": "swin_mam_v2_base_p4_w7_224_22kto1k", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-05-27 19:32:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 93): INFO Creating model:swin_mam_v2/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x
[2024-05-27 19:32:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 95): INFO MemorySwinTransformerV2(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=128, out_features=16, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=16, out_features=128, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=256, out_features=32, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=32, out_features=256, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=512, out_features=64, bias=True)
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): MemorySwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=64, out_features=512, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): MemorySwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Linear(in_features=1024, out_features=128, bias=True)
            (proj_latent_space_to_feature): Identity()
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): MemorySwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): MemoryWindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (proj_feature_to_latent_space): Identity()
            (proj_latent_space_to_feature): Linear(in_features=128, out_features=1024, bias=True)
            (ltm_norm): Identity()
            (act): GELU()
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-05-27 19:32:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 98): INFO number of params: 89176336
[2024-05-27 19:32:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 101): INFO number of GFLOPs: 15.438473216
[2024-05-27 19:32:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 136): INFO no checkpoint found in /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k, ignoring auto resume
[2024-05-27 19:32:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 46): INFO ==============> Loading weight /root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-05-27 19:32:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-05-27 19:32:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.0.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.0.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.1.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.1.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.1.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.1.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.2.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.2.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.2.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.3.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.3.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.3.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.4.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.4.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.4.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.5.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.5.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.5.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.6.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.6.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.6.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.7.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.7.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.7.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.8.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.8.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.8.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.9.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.9.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.9.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.10.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.10.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.10.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.11.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.11.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.11.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.12.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.12.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.12.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.13.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.13.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.13.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.14.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.14.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.14.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.15.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.15.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.15.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.attn.proj_feature_to_latent_space.weight', 'layers.2.blocks.16.attn.proj_feature_to_latent_space.bias', 'layers.2.blocks.16.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.16.attn.proj_latent_space_to_feature.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.attn.proj_latent_space_to_feature.weight', 'layers.2.blocks.17.attn.proj_latent_space_to_feature.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.attn.proj_feature_to_latent_space.weight', 'layers.3.blocks.0.attn.proj_feature_to_latent_space.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.attn.proj_latent_space_to_feature.weight', 'layers.3.blocks.1.attn.proj_latent_space_to_feature.bias'], unexpected_keys=[])
[2024-05-27 19:32:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 129): INFO => loaded successfully '/root/mam_classifier/pretrain_weights/swin_transformer/22k/swin_base_patch4_window7_224_22k.pth'
[2024-05-27 19:33:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 89.429 (89.429)	Loss 0.3540 (0.3540)	Acc@1 91.602 (91.602)	Acc@5 98.242 (98.242)	Mem 1745MB
[2024-05-27 19:34:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 82.626 Acc@5 96.554
[2024-05-27 19:34:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 148): INFO Accuracy of the network on the 50000 test images: 82.6%
[2024-05-27 19:34:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 154): INFO Start training
[2024-05-27 19:35:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][0/2502]	eta 2 days, 8:11:49 lr 0.000000	 wd 0.0000	time 80.8591 (80.8591)	loss 1.8497 (1.8497)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 11561MB
[2024-05-27 19:36:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][100/2502]	eta 0:47:05 lr 0.000000	 wd 0.0000	time 0.3856 (1.1764)	loss 1.4697 (1.5329)	grad_norm 6.4809 (nan)	loss_scale 4096.0000 (7056.4752)	mem 12561MB
[2024-05-27 19:36:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][200/2502]	eta 0:29:45 lr 0.000001	 wd 0.0000	time 0.3087 (0.7757)	loss 1.3858 (1.5108)	grad_norm 7.2564 (nan)	loss_scale 2048.0000 (4605.4527)	mem 12561MB
[2024-05-27 19:37:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][300/2502]	eta 0:23:32 lr 0.000001	 wd 0.0000	time 0.3258 (0.6413)	loss 1.4755 (1.4840)	grad_norm 5.9225 (nan)	loss_scale 2048.0000 (3755.8007)	mem 12561MB
[2024-05-27 19:38:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][400/2502]	eta 0:20:06 lr 0.000001	 wd 0.0000	time 0.3496 (0.5740)	loss 1.8849 (1.4792)	grad_norm 7.4637 (nan)	loss_scale 2048.0000 (3329.9152)	mem 12561MB
[2024-05-27 19:38:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][500/2502]	eta 0:17:47 lr 0.000002	 wd 0.0000	time 0.3736 (0.5333)	loss 1.5141 (1.4735)	grad_norm 5.3124 (nan)	loss_scale 2048.0000 (3074.0439)	mem 12561MB
[2024-05-27 19:39:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][600/2502]	eta 0:16:02 lr 0.000002	 wd 0.0000	time 0.3554 (0.5060)	loss 0.9643 (1.4712)	grad_norm 4.5147 (nan)	loss_scale 1024.0000 (2896.5058)	mem 12561MB
[2024-05-27 19:39:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][700/2502]	eta 0:14:37 lr 0.000002	 wd 0.0000	time 0.3507 (0.4870)	loss 1.5082 (1.4644)	grad_norm 3.8392 (nan)	loss_scale 1024.0000 (2629.3866)	mem 12561MB
[2024-05-27 19:40:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][800/2502]	eta 0:13:23 lr 0.000003	 wd 0.0000	time 0.3267 (0.4722)	loss 1.6341 (1.4611)	grad_norm 5.0627 (nan)	loss_scale 1024.0000 (2428.9638)	mem 12561MB
[2024-05-27 19:41:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][900/2502]	eta 0:12:19 lr 0.000003	 wd 0.0000	time 0.3292 (0.4616)	loss 1.6665 (1.4503)	grad_norm 4.1472 (nan)	loss_scale 1024.0000 (2273.0300)	mem 12561MB
[2024-05-27 19:41:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1000/2502]	eta 0:11:30 lr 0.000003	 wd 0.0000	time 0.3713 (0.4600)	loss 1.2658 (1.4481)	grad_norm 4.5844 (nan)	loss_scale 1024.0000 (2148.2517)	mem 12561MB
[2024-05-27 19:42:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1100/2502]	eta 0:10:41 lr 0.000004	 wd 0.0000	time 0.3876 (0.4578)	loss 1.6315 (1.4457)	grad_norm 3.1944 (nan)	loss_scale 1024.0000 (2046.1399)	mem 12561MB
[2024-05-27 19:43:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1200/2502]	eta 0:10:01 lr 0.000004	 wd 0.0000	time 0.3545 (0.4618)	loss 1.4777 (1.4460)	grad_norm 3.9597 (nan)	loss_scale 1024.0000 (1961.0325)	mem 12561MB
[2024-05-27 19:44:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1300/2502]	eta 0:09:13 lr 0.000004	 wd 0.0000	time 0.3245 (0.4603)	loss 1.5209 (1.4450)	grad_norm 5.0058 (nan)	loss_scale 1024.0000 (1889.0085)	mem 12561MB
[2024-05-27 19:45:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1400/2502]	eta 0:08:37 lr 0.000005	 wd 0.0000	time 0.4616 (0.4692)	loss 1.5245 (1.4442)	grad_norm 6.4039 (nan)	loss_scale 1024.0000 (1827.2662)	mem 12561MB
[2024-05-27 19:45:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1500/2502]	eta 0:07:48 lr 0.000005	 wd 0.0000	time 0.3929 (0.4679)	loss 1.3954 (1.4412)	grad_norm 6.1269 (nan)	loss_scale 1024.0000 (1773.7508)	mem 12561MB
[2024-05-27 19:46:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1600/2502]	eta 0:07:08 lr 0.000005	 wd 0.0000	time 0.3812 (0.4749)	loss 1.6379 (1.4392)	grad_norm 8.6900 (nan)	loss_scale 1024.0000 (1726.9207)	mem 12561MB
[2024-05-27 19:47:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1700/2502]	eta 0:06:19 lr 0.000005	 wd 0.0000	time 0.5556 (0.4733)	loss 1.7471 (1.4367)	grad_norm 7.4928 (nan)	loss_scale 1024.0000 (1685.5967)	mem 12561MB
[2024-05-27 19:48:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1800/2502]	eta 0:05:36 lr 0.000006	 wd 0.0000	time 0.4949 (0.4796)	loss 1.2549 (1.4356)	grad_norm 6.9930 (nan)	loss_scale 1024.0000 (1648.8617)	mem 12561MB
[2024-05-27 19:49:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][1900/2502]	eta 0:04:47 lr 0.000006	 wd 0.0000	time 0.6457 (0.4776)	loss 1.5636 (1.4337)	grad_norm 5.3749 (nan)	loss_scale 1024.0000 (1615.9916)	mem 12561MB
[2024-05-27 19:50:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][2000/2502]	eta 0:04:01 lr 0.000006	 wd 0.0000	time 0.4158 (0.4804)	loss 1.5169 (1.4304)	grad_norm 4.2172 (nan)	loss_scale 1024.0000 (1586.4068)	mem 12561MB
[2024-05-27 19:51:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][2100/2502]	eta 0:03:12 lr 0.000007	 wd 0.0000	time 0.7413 (0.4795)	loss 1.4834 (1.4303)	grad_norm 5.9571 (nan)	loss_scale 1024.0000 (1559.6383)	mem 12561MB
[2024-05-27 19:51:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][2200/2502]	eta 0:02:25 lr 0.000007	 wd 0.0000	time 0.3870 (0.4824)	loss 1.6160 (1.4291)	grad_norm 3.9166 (nan)	loss_scale 1024.0000 (1535.3021)	mem 12561MB
[2024-05-27 19:52:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][2300/2502]	eta 0:01:37 lr 0.000007	 wd 0.0000	time 0.4008 (0.4811)	loss 1.6012 (1.4268)	grad_norm 4.3254 (nan)	loss_scale 1024.0000 (1513.0813)	mem 12561MB
[2024-05-27 19:53:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][2400/2502]	eta 0:00:49 lr 0.000008	 wd 0.0000	time 0.3936 (0.4808)	loss 1.6126 (1.4254)	grad_norm 11.4219 (nan)	loss_scale 1024.0000 (1492.7114)	mem 12561MB
[2024-05-27 19:54:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.3451 (0.4776)	loss 1.6600 (1.4243)	grad_norm 4.6628 (nan)	loss_scale 1024.0000 (1473.9704)	mem 12561MB
[2024-05-27 19:54:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 0 training takes 0:20:17
[2024-05-27 19:54:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 145): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_0.pth saving......
[2024-05-27 19:54:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 147): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_0.pth saved !!!
[2024-05-27 19:54:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 21.224 (21.224)	Loss 0.4438 (0.4438)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-27 19:55:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 83.364 Acc@5 96.918
[2024-05-27 19:55:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 83.4%
[2024-05-27 19:55:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 83.36%
[2024-05-27 19:55:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 19:55:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 19:55:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][0/2502]	eta 12:05:09 lr 0.000008	 wd 0.0000	time 17.3898 (17.3898)	loss 1.1653 (1.1653)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 19:56:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][100/2502]	eta 0:23:29 lr 0.000008	 wd 0.0000	time 0.3497 (0.5869)	loss 1.2278 (1.4403)	grad_norm 6.2512 (5.0947)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 19:56:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][200/2502]	eta 0:18:10 lr 0.000009	 wd 0.0000	time 0.3366 (0.4737)	loss 1.4629 (1.4330)	grad_norm 5.6287 (5.5826)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 19:57:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][300/2502]	eta 0:16:01 lr 0.000009	 wd 0.0000	time 0.3217 (0.4368)	loss 1.6199 (1.4141)	grad_norm 4.2421 (5.5664)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 19:58:00 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][400/2502]	eta 0:14:44 lr 0.000009	 wd 0.0000	time 0.3813 (0.4207)	loss 0.9429 (1.4039)	grad_norm 4.5939 (5.5216)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 19:58:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][500/2502]	eta 0:13:39 lr 0.000010	 wd 0.0000	time 0.3046 (0.4092)	loss 1.6480 (1.3990)	grad_norm 4.4691 (5.6091)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 19:59:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][600/2502]	eta 0:12:44 lr 0.000010	 wd 0.0000	time 0.3419 (0.4020)	loss 1.2960 (1.3976)	grad_norm 6.7065 (5.5807)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 19:59:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][700/2502]	eta 0:11:54 lr 0.000010	 wd 0.0000	time 0.3315 (0.3967)	loss 1.5875 (1.3940)	grad_norm 6.2570 (5.5910)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:00:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][800/2502]	eta 0:11:07 lr 0.000011	 wd 0.0000	time 0.3595 (0.3925)	loss 1.6283 (1.3970)	grad_norm 3.8654 (5.5435)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:01:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][900/2502]	eta 0:10:33 lr 0.000011	 wd 0.0000	time 0.5579 (0.3951)	loss 1.5048 (1.3934)	grad_norm 4.8065 (5.6265)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:01:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1000/2502]	eta 0:10:03 lr 0.000011	 wd 0.0000	time 0.4660 (0.4015)	loss 1.5355 (1.3922)	grad_norm 16.6325 (5.5990)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:02:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1100/2502]	eta 0:09:46 lr 0.000012	 wd 0.0000	time 2.5560 (0.4186)	loss 0.9829 (1.3915)	grad_norm 6.8268 (5.5896)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:03:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1200/2502]	eta 0:09:09 lr 0.000012	 wd 0.0000	time 0.3452 (0.4223)	loss 1.3760 (1.3938)	grad_norm 3.0926 (5.5662)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:04:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1300/2502]	eta 0:08:38 lr 0.000012	 wd 0.0000	time 0.4123 (0.4313)	loss 1.5034 (1.3951)	grad_norm 4.8336 (5.5374)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:05:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1400/2502]	eta 0:07:56 lr 0.000012	 wd 0.0000	time 0.3712 (0.4323)	loss 1.4658 (1.3927)	grad_norm 6.7391 (5.5100)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:06:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1500/2502]	eta 0:07:25 lr 0.000013	 wd 0.0000	time 0.3514 (0.4450)	loss 0.9300 (1.3909)	grad_norm 4.9756 (5.5270)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:07:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1600/2502]	eta 0:06:42 lr 0.000013	 wd 0.0000	time 0.3825 (0.4462)	loss 0.9947 (1.3895)	grad_norm 4.7335 (5.4837)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:07:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1700/2502]	eta 0:06:01 lr 0.000013	 wd 0.0000	time 0.3488 (0.4503)	loss 1.4158 (1.3896)	grad_norm 6.3951 (5.5049)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:08:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1800/2502]	eta 0:05:16 lr 0.000014	 wd 0.0000	time 0.3802 (0.4504)	loss 1.4939 (1.3902)	grad_norm 10.8141 (5.5270)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:09:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][1900/2502]	eta 0:04:33 lr 0.000014	 wd 0.0000	time 0.3804 (0.4538)	loss 1.5051 (1.3908)	grad_norm 14.4170 (5.5154)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:10:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][2000/2502]	eta 0:03:47 lr 0.000014	 wd 0.0000	time 0.4702 (0.4541)	loss 1.5221 (1.3910)	grad_norm 4.5403 (5.4982)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 20:11:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][2100/2502]	eta 0:03:04 lr 0.000015	 wd 0.0000	time 0.3573 (0.4594)	loss 1.5538 (1.3921)	grad_norm 3.8381 (5.5141)	loss_scale 2048.0000 (1026.9243)	mem 12561MB
[2024-05-27 20:12:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][2200/2502]	eta 0:02:18 lr 0.000015	 wd 0.0000	time 0.4115 (0.4592)	loss 1.0938 (1.3932)	grad_norm 5.3748 (5.4895)	loss_scale 2048.0000 (1073.3158)	mem 12561MB
[2024-05-27 20:12:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][2300/2502]	eta 0:01:33 lr 0.000015	 wd 0.0000	time 0.3486 (0.4638)	loss 1.4365 (1.3932)	grad_norm 4.5250 (5.4666)	loss_scale 2048.0000 (1115.6749)	mem 12561MB
[2024-05-27 20:13:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][2400/2502]	eta 0:00:47 lr 0.000016	 wd 0.0000	time 0.3936 (0.4630)	loss 1.6214 (1.3915)	grad_norm 4.8025 (5.4570)	loss_scale 2048.0000 (1154.5056)	mem 12561MB
[2024-05-27 20:14:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.3221 (0.4614)	loss 1.0840 (1.3926)	grad_norm 4.5745 (5.4440)	loss_scale 2048.0000 (1190.2311)	mem 12561MB
[2024-05-27 20:14:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 1 training takes 0:19:25
[2024-05-27 20:15:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 52.442 (52.442)	Loss 0.4495 (0.4495)	Acc@1 91.797 (91.797)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-27 20:15:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 83.728 Acc@5 97.088
[2024-05-27 20:15:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-05-27 20:15:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 83.73%
[2024-05-27 20:15:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 20:15:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 20:16:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][0/2502]	eta 9:35:45 lr 0.000016	 wd 0.0000	time 13.8073 (13.8073)	loss 1.5335 (1.5335)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:16:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][100/2502]	eta 0:19:46 lr 0.000016	 wd 0.0000	time 0.3213 (0.4939)	loss 1.5716 (1.3706)	grad_norm 6.4159 (4.8657)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:17:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][200/2502]	eta 0:16:40 lr 0.000017	 wd 0.0000	time 0.3139 (0.4346)	loss 1.4904 (1.3852)	grad_norm 10.7806 (5.3685)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:17:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][300/2502]	eta 0:15:03 lr 0.000017	 wd 0.0000	time 0.3176 (0.4101)	loss 1.2691 (1.3882)	grad_norm 5.3340 (5.4208)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:18:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][400/2502]	eta 0:13:56 lr 0.000017	 wd 0.0000	time 0.3253 (0.3981)	loss 1.7089 (1.3855)	grad_norm 4.8334 (5.3690)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:19:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][500/2502]	eta 0:13:06 lr 0.000018	 wd 0.0000	time 0.3365 (0.3927)	loss 1.4954 (1.3841)	grad_norm 4.9345 (5.4204)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:19:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][600/2502]	eta 0:12:17 lr 0.000018	 wd 0.0000	time 0.2927 (0.3877)	loss 1.1595 (1.3768)	grad_norm 4.7575 (5.4620)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:20:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][700/2502]	eta 0:11:33 lr 0.000018	 wd 0.0000	time 0.3593 (0.3847)	loss 1.1645 (1.3794)	grad_norm 4.8720 (5.4614)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:21:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][800/2502]	eta 0:11:17 lr 0.000019	 wd 0.0000	time 0.3575 (0.3978)	loss 1.5524 (1.3785)	grad_norm 4.7921 (5.4469)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:21:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][900/2502]	eta 0:10:50 lr 0.000019	 wd 0.0000	time 0.3750 (0.4058)	loss 1.6710 (1.3838)	grad_norm 5.4195 (5.4827)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:22:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1000/2502]	eta 0:10:32 lr 0.000019	 wd 0.0000	time 0.3515 (0.4214)	loss 1.5264 (1.3862)	grad_norm 5.9977 (5.5090)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:23:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1100/2502]	eta 0:09:55 lr 0.000020	 wd 0.0000	time 0.3509 (0.4246)	loss 1.1440 (1.3857)	grad_norm 6.3352 (5.4839)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:24:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1200/2502]	eta 0:09:31 lr 0.000020	 wd 0.0000	time 0.4340 (0.4388)	loss 1.2270 (1.3839)	grad_norm 3.7486 (5.4608)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:25:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1300/2502]	eta 0:08:49 lr 0.000020	 wd 0.0000	time 0.4182 (0.4402)	loss 1.6251 (1.3869)	grad_norm 6.0391 (5.4743)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:26:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1400/2502]	eta 0:08:23 lr 0.000020	 wd 0.0000	time 0.4398 (0.4572)	loss 1.5658 (1.3866)	grad_norm 6.4223 (5.4767)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:27:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1500/2502]	eta 0:07:45 lr 0.000021	 wd 0.0000	time 0.3783 (0.4648)	loss 1.5232 (1.3837)	grad_norm 4.0503 (5.5177)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:28:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1600/2502]	eta 0:06:59 lr 0.000021	 wd 0.0000	time 0.3915 (0.4653)	loss 1.2195 (1.3843)	grad_norm 4.1180 (5.5192)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:29:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1700/2502]	eta 0:06:16 lr 0.000021	 wd 0.0000	time 0.3475 (0.4691)	loss 1.3207 (1.3834)	grad_norm 5.4721 (5.4988)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:29:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1800/2502]	eta 0:05:28 lr 0.000022	 wd 0.0000	time 0.4333 (0.4686)	loss 1.3883 (1.3829)	grad_norm 5.0539 (5.4858)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:30:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][1900/2502]	eta 0:04:43 lr 0.000022	 wd 0.0000	time 0.3515 (0.4709)	loss 0.9878 (1.3818)	grad_norm 3.8740 (5.4675)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:31:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][2000/2502]	eta 0:03:55 lr 0.000022	 wd 0.0000	time 0.4239 (0.4700)	loss 1.2941 (1.3794)	grad_norm 5.5320 (5.4605)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:32:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][2100/2502]	eta 0:03:10 lr 0.000023	 wd 0.0000	time 0.3533 (0.4750)	loss 1.4386 (1.3797)	grad_norm 4.7570 (5.4301)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:33:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][2200/2502]	eta 0:02:23 lr 0.000023	 wd 0.0000	time 0.5471 (0.4746)	loss 1.4993 (1.3798)	grad_norm 8.6839 (5.4393)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:34:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][2300/2502]	eta 0:01:36 lr 0.000023	 wd 0.0000	time 0.3533 (0.4801)	loss 1.6377 (1.3804)	grad_norm 4.3951 (5.4398)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:35:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][2400/2502]	eta 0:00:48 lr 0.000024	 wd 0.0000	time 0.4327 (0.4787)	loss 1.5092 (1.3797)	grad_norm 4.7132 (5.4330)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:35:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.3163 (0.4750)	loss 1.4638 (1.3793)	grad_norm 4.3037 (5.4295)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:35:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 2 training takes 0:19:54
[2024-05-27 20:36:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 40.323 (40.323)	Loss 0.4304 (0.4304)	Acc@1 91.602 (91.602)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-27 20:36:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 83.852 Acc@5 97.146
[2024-05-27 20:36:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-05-27 20:36:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 83.85%
[2024-05-27 20:36:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 20:36:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 20:37:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][0/2502]	eta 1 day, 3:53:07 lr 0.000024	 wd 0.0000	time 40.1229 (40.1229)	loss 1.1958 (1.1958)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:38:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][100/2502]	eta 0:32:44 lr 0.000024	 wd 0.0000	time 0.4053 (0.8178)	loss 1.4960 (1.3685)	grad_norm 4.1385 (5.9464)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:38:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][200/2502]	eta 0:25:21 lr 0.000025	 wd 0.0000	time 0.3714 (0.6609)	loss 1.4797 (1.3743)	grad_norm 4.3484 (5.6967)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 20:39:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][300/2502]	eta 0:21:37 lr 0.000025	 wd 0.0000	time 0.3686 (0.5894)	loss 1.5738 (1.3671)	grad_norm 3.6547 (inf)	loss_scale 1024.0000 (1925.5282)	mem 12561MB
[2024-05-27 20:40:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][400/2502]	eta 0:20:16 lr 0.000025	 wd 0.0000	time 0.4228 (0.5787)	loss 1.7025 (1.3723)	grad_norm 4.7741 (inf)	loss_scale 1024.0000 (1700.7082)	mem 12561MB
[2024-05-27 20:41:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][500/2502]	eta 0:18:24 lr 0.000026	 wd 0.0000	time 0.4597 (0.5517)	loss 1.4867 (1.3686)	grad_norm 5.2543 (inf)	loss_scale 1024.0000 (1565.6367)	mem 12561MB
[2024-05-27 20:42:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][600/2502]	eta 0:17:38 lr 0.000026	 wd 0.0000	time 0.3695 (0.5564)	loss 1.5169 (1.3677)	grad_norm 5.3661 (inf)	loss_scale 1024.0000 (1475.5141)	mem 12561MB
[2024-05-27 20:43:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][700/2502]	eta 0:16:15 lr 0.000026	 wd 0.0000	time 0.4087 (0.5412)	loss 1.7332 (1.3702)	grad_norm 6.7040 (inf)	loss_scale 1024.0000 (1411.1041)	mem 12561MB
[2024-05-27 20:43:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][800/2502]	eta 0:15:12 lr 0.000027	 wd 0.0000	time 0.4282 (0.5364)	loss 1.2019 (1.3698)	grad_norm 3.9463 (inf)	loss_scale 1024.0000 (1362.7765)	mem 12561MB
[2024-05-27 20:44:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][900/2502]	eta 0:14:03 lr 0.000027	 wd 0.0000	time 0.4599 (0.5264)	loss 1.4124 (1.3713)	grad_norm 6.3737 (inf)	loss_scale 1024.0000 (1325.1765)	mem 12561MB
[2024-05-27 20:45:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1000/2502]	eta 0:13:24 lr 0.000027	 wd 0.0000	time 0.4296 (0.5358)	loss 1.5175 (1.3733)	grad_norm 9.7182 (inf)	loss_scale 1024.0000 (1295.0889)	mem 12561MB
[2024-05-27 20:46:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1100/2502]	eta 0:12:22 lr 0.000028	 wd 0.0000	time 0.4154 (0.5294)	loss 1.4458 (1.3755)	grad_norm 3.7238 (inf)	loss_scale 1024.0000 (1270.4668)	mem 12561MB
[2024-05-27 20:47:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1200/2502]	eta 0:11:36 lr 0.000028	 wd 0.0000	time 0.3127 (0.5349)	loss 1.2665 (1.3750)	grad_norm 3.3206 (inf)	loss_scale 1024.0000 (1249.9450)	mem 12561MB
[2024-05-27 20:48:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1300/2502]	eta 0:10:35 lr 0.000028	 wd 0.0000	time 0.3636 (0.5285)	loss 1.3186 (1.3771)	grad_norm 7.8479 (inf)	loss_scale 1024.0000 (1232.5780)	mem 12561MB
[2024-05-27 20:49:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1400/2502]	eta 0:09:41 lr 0.000028	 wd 0.0000	time 0.3735 (0.5275)	loss 1.2851 (1.3779)	grad_norm 4.9516 (inf)	loss_scale 1024.0000 (1217.6902)	mem 12561MB
[2024-05-27 20:49:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1500/2502]	eta 0:08:44 lr 0.000029	 wd 0.0000	time 0.3470 (0.5233)	loss 1.5131 (1.3797)	grad_norm 4.2029 (inf)	loss_scale 1024.0000 (1204.7861)	mem 12561MB
[2024-05-27 20:50:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1600/2502]	eta 0:07:54 lr 0.000029	 wd 0.0000	time 0.4280 (0.5256)	loss 1.5157 (1.3785)	grad_norm 4.2289 (inf)	loss_scale 1024.0000 (1193.4941)	mem 12561MB
[2024-05-27 20:51:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1700/2502]	eta 0:06:58 lr 0.000029	 wd 0.0000	time 0.3826 (0.5224)	loss 1.0484 (1.3775)	grad_norm 4.1898 (inf)	loss_scale 1024.0000 (1183.5297)	mem 12561MB
[2024-05-27 20:52:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1800/2502]	eta 0:06:07 lr 0.000030	 wd 0.0000	time 0.3488 (0.5239)	loss 1.5859 (1.3782)	grad_norm 10.6315 (inf)	loss_scale 1024.0000 (1174.6718)	mem 12561MB
[2024-05-27 20:53:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][1900/2502]	eta 0:05:13 lr 0.000030	 wd 0.0000	time 0.3581 (0.5205)	loss 1.5553 (1.3778)	grad_norm 21.3064 (inf)	loss_scale 1024.0000 (1166.7459)	mem 12561MB
[2024-05-27 20:54:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][2000/2502]	eta 0:04:20 lr 0.000030	 wd 0.0000	time 0.3572 (0.5197)	loss 1.0645 (1.3785)	grad_norm 5.5692 (inf)	loss_scale 1024.0000 (1159.6122)	mem 12561MB
[2024-05-27 20:54:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][2100/2502]	eta 0:03:27 lr 0.000031	 wd 0.0000	time 0.4343 (0.5165)	loss 1.2342 (1.3773)	grad_norm 4.6823 (inf)	loss_scale 1024.0000 (1153.1575)	mem 12561MB
[2024-05-27 20:55:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][2200/2502]	eta 0:02:36 lr 0.000031	 wd 0.0000	time 0.3414 (0.5178)	loss 1.4379 (1.3780)	grad_norm 5.2002 (inf)	loss_scale 1024.0000 (1147.2894)	mem 12561MB
[2024-05-27 20:56:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][2300/2502]	eta 0:01:44 lr 0.000031	 wd 0.0000	time 0.4058 (0.5150)	loss 1.5771 (1.3780)	grad_norm 3.5522 (inf)	loss_scale 1024.0000 (1141.9313)	mem 12561MB
[2024-05-27 20:57:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][2400/2502]	eta 0:00:53 lr 0.000032	 wd 0.0000	time 0.4314 (0.5200)	loss 0.9763 (1.3776)	grad_norm 3.6744 (inf)	loss_scale 1024.0000 (1137.0196)	mem 12561MB
[2024-05-27 20:58:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000032	 wd 0.0000	time 0.3373 (0.5151)	loss 1.3888 (1.3778)	grad_norm 4.4900 (inf)	loss_scale 1024.0000 (1132.5006)	mem 12561MB
[2024-05-27 20:58:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 3 training takes 0:21:32
[2024-05-27 20:59:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 65.996 (65.996)	Loss 0.4363 (0.4363)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-27 20:59:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 83.910 Acc@5 97.202
[2024-05-27 20:59:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-05-27 20:59:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 83.91%
[2024-05-27 20:59:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 20:59:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 21:00:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][0/2502]	eta 21:08:13 lr 0.000032	 wd 0.0000	time 30.4129 (30.4129)	loss 1.4681 (1.4681)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:00:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][100/2502]	eta 0:26:50 lr 0.000032	 wd 0.0000	time 0.3211 (0.6706)	loss 1.2231 (1.4035)	grad_norm 4.7854 (5.5838)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:01:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][200/2502]	eta 0:19:57 lr 0.000033	 wd 0.0000	time 0.3307 (0.5201)	loss 1.1090 (1.3924)	grad_norm 4.0361 (5.3165)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:02:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][300/2502]	eta 0:17:14 lr 0.000033	 wd 0.0000	time 0.3200 (0.4697)	loss 1.0579 (1.3947)	grad_norm 10.0053 (5.3798)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:02:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][400/2502]	eta 0:15:31 lr 0.000033	 wd 0.0000	time 0.3325 (0.4434)	loss 1.4019 (1.3857)	grad_norm 3.0867 (5.4016)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:03:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][500/2502]	eta 0:14:17 lr 0.000034	 wd 0.0000	time 0.3893 (0.4283)	loss 1.4912 (1.3811)	grad_norm 7.2120 (5.4212)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:03:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][600/2502]	eta 0:13:16 lr 0.000034	 wd 0.0000	time 0.3367 (0.4188)	loss 1.6246 (1.3783)	grad_norm 18.8605 (5.3934)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:04:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][700/2502]	eta 0:12:20 lr 0.000034	 wd 0.0000	time 0.3075 (0.4111)	loss 1.3560 (1.3775)	grad_norm 5.3238 (5.3141)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:05:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][800/2502]	eta 0:11:31 lr 0.000035	 wd 0.0000	time 0.3832 (0.4064)	loss 1.5084 (1.3750)	grad_norm 14.0988 (nan)	loss_scale 512.0000 (1020.1648)	mem 12561MB
[2024-05-27 21:05:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][900/2502]	eta 0:10:43 lr 0.000035	 wd 0.0000	time 0.3428 (0.4017)	loss 1.5334 (1.3768)	grad_norm 4.2423 (nan)	loss_scale 512.0000 (963.7647)	mem 12561MB
[2024-05-27 21:06:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1000/2502]	eta 0:09:57 lr 0.000035	 wd 0.0000	time 0.3238 (0.3981)	loss 1.4751 (1.3767)	grad_norm 3.2621 (nan)	loss_scale 512.0000 (918.6334)	mem 12561MB
[2024-05-27 21:07:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1100/2502]	eta 0:09:29 lr 0.000036	 wd 0.0000	time 0.3350 (0.4062)	loss 1.4813 (1.3776)	grad_norm 5.2986 (nan)	loss_scale 512.0000 (881.7003)	mem 12561MB
[2024-05-27 21:07:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1200/2502]	eta 0:08:56 lr 0.000036	 wd 0.0000	time 0.3521 (0.4118)	loss 1.0245 (1.3768)	grad_norm 4.9768 (nan)	loss_scale 512.0000 (850.9176)	mem 12561MB
[2024-05-27 21:08:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1300/2502]	eta 0:08:29 lr 0.000036	 wd 0.0000	time 0.3356 (0.4239)	loss 1.4483 (1.3756)	grad_norm 4.2287 (nan)	loss_scale 512.0000 (824.8670)	mem 12561MB
[2024-05-27 21:09:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1400/2502]	eta 0:07:50 lr 0.000036	 wd 0.0000	time 0.4121 (0.4271)	loss 1.4773 (1.3761)	grad_norm 4.0239 (nan)	loss_scale 512.0000 (802.5353)	mem 12561MB
[2024-05-27 21:10:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1500/2502]	eta 0:07:13 lr 0.000037	 wd 0.0000	time 0.3585 (0.4326)	loss 1.2692 (1.3792)	grad_norm 4.1676 (nan)	loss_scale 512.0000 (783.1792)	mem 12561MB
[2024-05-27 21:11:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1600/2502]	eta 0:06:32 lr 0.000037	 wd 0.0000	time 0.4248 (0.4350)	loss 1.1113 (1.3782)	grad_norm 4.8975 (nan)	loss_scale 512.0000 (766.2411)	mem 12561MB
[2024-05-27 21:12:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1700/2502]	eta 0:05:53 lr 0.000037	 wd 0.0000	time 0.3504 (0.4403)	loss 1.5854 (1.3771)	grad_norm 3.8958 (nan)	loss_scale 512.0000 (751.2945)	mem 12561MB
[2024-05-27 21:12:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1800/2502]	eta 0:05:09 lr 0.000038	 wd 0.0000	time 0.3366 (0.4415)	loss 1.4749 (1.3769)	grad_norm 4.0615 (nan)	loss_scale 512.0000 (738.0078)	mem 12561MB
[2024-05-27 21:13:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][1900/2502]	eta 0:04:29 lr 0.000038	 wd 0.0000	time 0.3231 (0.4483)	loss 1.5960 (1.3765)	grad_norm 4.3124 (nan)	loss_scale 512.0000 (726.1189)	mem 12561MB
[2024-05-27 21:14:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][2000/2502]	eta 0:03:45 lr 0.000038	 wd 0.0000	time 0.3632 (0.4485)	loss 1.3489 (1.3760)	grad_norm 4.5122 (nan)	loss_scale 512.0000 (715.4183)	mem 12561MB
[2024-05-27 21:15:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][2100/2502]	eta 0:03:01 lr 0.000039	 wd 0.0000	time 0.3063 (0.4511)	loss 1.2144 (1.3755)	grad_norm 6.1453 (nan)	loss_scale 512.0000 (705.7363)	mem 12561MB
[2024-05-27 21:16:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][2200/2502]	eta 0:02:16 lr 0.000039	 wd 0.0000	time 0.4186 (0.4512)	loss 1.0736 (1.3728)	grad_norm 4.6115 (nan)	loss_scale 512.0000 (696.9341)	mem 12561MB
[2024-05-27 21:17:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][2300/2502]	eta 0:01:31 lr 0.000039	 wd 0.0000	time 0.3397 (0.4533)	loss 1.2848 (1.3733)	grad_norm 4.4861 (nan)	loss_scale 512.0000 (688.8970)	mem 12561MB
[2024-05-27 21:17:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][2400/2502]	eta 0:00:46 lr 0.000040	 wd 0.0000	time 0.4773 (0.4534)	loss 1.4833 (1.3731)	grad_norm 3.3362 (nan)	loss_scale 512.0000 (681.5294)	mem 12561MB
[2024-05-27 21:18:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.3401 (0.4512)	loss 0.8625 (1.3723)	grad_norm 6.4324 (nan)	loss_scale 512.0000 (674.7509)	mem 12561MB
[2024-05-27 21:18:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 4 training takes 0:19:06
[2024-05-27 21:19:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 56.743 (56.743)	Loss 0.4365 (0.4365)	Acc@1 91.406 (91.406)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-27 21:20:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.086 Acc@5 97.236
[2024-05-27 21:20:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-05-27 21:20:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.09%
[2024-05-27 21:20:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 21:20:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 21:20:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][0/2502]	eta 8:56:26 lr 0.000040	 wd 0.0000	time 12.8645 (12.8645)	loss 1.7152 (1.7152)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:20:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][100/2502]	eta 0:19:27 lr 0.000040	 wd 0.0000	time 0.3234 (0.4859)	loss 1.1781 (1.3937)	grad_norm 5.4036 (5.9060)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:21:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][200/2502]	eta 0:16:36 lr 0.000040	 wd 0.0000	time 0.3000 (0.4329)	loss 1.3795 (1.3922)	grad_norm 3.2987 (5.5249)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:22:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][300/2502]	eta 0:14:59 lr 0.000040	 wd 0.0000	time 0.3071 (0.4087)	loss 1.5711 (1.3827)	grad_norm 3.7717 (5.4133)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:22:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][400/2502]	eta 0:13:55 lr 0.000040	 wd 0.0000	time 0.3355 (0.3974)	loss 1.0179 (1.3838)	grad_norm 4.2372 (5.2920)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:23:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][500/2502]	eta 0:13:05 lr 0.000040	 wd 0.0000	time 0.3239 (0.3925)	loss 1.7068 (1.3814)	grad_norm 4.3506 (5.3291)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:23:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][600/2502]	eta 0:12:17 lr 0.000040	 wd 0.0000	time 0.3339 (0.3878)	loss 1.5539 (1.3831)	grad_norm 3.8720 (5.1747)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:24:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][700/2502]	eta 0:11:32 lr 0.000040	 wd 0.0000	time 0.3388 (0.3844)	loss 1.3850 (1.3801)	grad_norm 4.6424 (5.2000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:25:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][800/2502]	eta 0:10:51 lr 0.000040	 wd 0.0000	time 0.3270 (0.3830)	loss 0.9464 (1.3747)	grad_norm 5.2932 (5.1543)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:25:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][900/2502]	eta 0:10:09 lr 0.000040	 wd 0.0000	time 0.3126 (0.3807)	loss 0.9864 (1.3729)	grad_norm 4.3950 (5.1643)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:26:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1000/2502]	eta 0:09:31 lr 0.000040	 wd 0.0000	time 0.5110 (0.3803)	loss 1.5141 (1.3767)	grad_norm 3.3033 (5.1243)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:27:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1100/2502]	eta 0:09:05 lr 0.000040	 wd 0.0000	time 0.3781 (0.3887)	loss 1.5258 (1.3729)	grad_norm 4.9883 (5.1510)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:27:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1200/2502]	eta 0:08:33 lr 0.000040	 wd 0.0000	time 0.4888 (0.3943)	loss 1.5288 (1.3715)	grad_norm 7.7151 (5.1477)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:28:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1300/2502]	eta 0:08:08 lr 0.000040	 wd 0.0000	time 0.5239 (0.4067)	loss 1.5270 (1.3719)	grad_norm 3.7737 (5.1486)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:29:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1400/2502]	eta 0:07:33 lr 0.000040	 wd 0.0000	time 0.5391 (0.4113)	loss 1.4818 (1.3713)	grad_norm 4.4828 (5.2347)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:30:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1500/2502]	eta 0:07:08 lr 0.000040	 wd 0.0000	time 0.3378 (0.4279)	loss 1.2617 (1.3715)	grad_norm 3.6569 (5.2258)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:31:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1600/2502]	eta 0:06:31 lr 0.000040	 wd 0.0000	time 0.3809 (0.4346)	loss 1.5087 (1.3736)	grad_norm 4.8906 (5.2819)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:32:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1700/2502]	eta 0:05:49 lr 0.000040	 wd 0.0000	time 0.4354 (0.4362)	loss 1.3983 (1.3737)	grad_norm 6.6766 (5.2917)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:33:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1800/2502]	eta 0:05:12 lr 0.000040	 wd 0.0000	time 0.3987 (0.4446)	loss 1.2030 (1.3733)	grad_norm 4.7362 (5.2639)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:34:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][1900/2502]	eta 0:04:27 lr 0.000040	 wd 0.0000	time 0.3866 (0.4450)	loss 1.4234 (1.3753)	grad_norm 4.0172 (5.2832)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:35:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][2000/2502]	eta 0:03:49 lr 0.000040	 wd 0.0000	time 0.5443 (0.4577)	loss 1.4896 (1.3765)	grad_norm 5.2854 (5.2709)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:36:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][2100/2502]	eta 0:03:04 lr 0.000040	 wd 0.0000	time 0.4051 (0.4579)	loss 1.6678 (1.3768)	grad_norm 3.3492 (5.2722)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:37:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][2200/2502]	eta 0:02:20 lr 0.000040	 wd 0.0000	time 0.3888 (0.4643)	loss 1.3727 (1.3746)	grad_norm 7.2902 (5.3727)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 21:37:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][2300/2502]	eta 0:01:33 lr 0.000040	 wd 0.0000	time 0.6344 (0.4630)	loss 0.9189 (1.3737)	grad_norm 2.6216 (5.3553)	loss_scale 1024.0000 (513.7801)	mem 12561MB
[2024-05-27 21:38:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][2400/2502]	eta 0:00:47 lr 0.000040	 wd 0.0000	time 0.4032 (0.4647)	loss 1.4496 (1.3736)	grad_norm 4.6253 (5.3756)	loss_scale 1024.0000 (535.0304)	mem 12561MB
[2024-05-27 21:39:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.3732 (0.4625)	loss 1.5179 (1.3732)	grad_norm 4.0865 (5.3684)	loss_scale 1024.0000 (554.5814)	mem 12561MB
[2024-05-27 21:39:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 5 training takes 0:19:21
[2024-05-27 21:40:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 49.554 (49.554)	Loss 0.4487 (0.4487)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 12561MB
[2024-05-27 21:40:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.082 Acc@5 97.284
[2024-05-27 21:40:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-05-27 21:40:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.09%
[2024-05-27 21:40:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][0/2502]	eta 10:56:19 lr 0.000040	 wd 0.0000	time 15.7390 (15.7390)	loss 1.6772 (1.6772)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:41:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][100/2502]	eta 0:22:41 lr 0.000040	 wd 0.0000	time 0.3551 (0.5667)	loss 1.3611 (1.3503)	grad_norm 6.0392 (5.7305)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:42:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][200/2502]	eta 0:17:48 lr 0.000040	 wd 0.0000	time 0.3470 (0.4643)	loss 1.5046 (1.3696)	grad_norm 5.1277 (5.5142)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:42:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][300/2502]	eta 0:15:52 lr 0.000040	 wd 0.0000	time 0.3389 (0.4324)	loss 1.6505 (1.3792)	grad_norm 5.6145 (5.5925)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:43:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][400/2502]	eta 0:14:36 lr 0.000040	 wd 0.0000	time 0.3744 (0.4170)	loss 1.3312 (1.3731)	grad_norm 5.0377 (5.4301)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:43:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][500/2502]	eta 0:13:31 lr 0.000040	 wd 0.0000	time 0.3249 (0.4054)	loss 1.3308 (1.3663)	grad_norm 3.6319 (5.3875)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:44:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][600/2502]	eta 0:12:39 lr 0.000040	 wd 0.0000	time 0.3463 (0.3995)	loss 1.6225 (1.3670)	grad_norm 5.2870 (5.3390)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:45:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][700/2502]	eta 0:11:50 lr 0.000040	 wd 0.0000	time 0.3446 (0.3943)	loss 1.4239 (1.3681)	grad_norm 4.6706 (5.2796)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:45:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][800/2502]	eta 0:11:04 lr 0.000040	 wd 0.0000	time 0.3649 (0.3903)	loss 1.0323 (1.3663)	grad_norm 8.1473 (5.3634)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:46:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][900/2502]	eta 0:10:22 lr 0.000040	 wd 0.0000	time 0.3665 (0.3888)	loss 1.5572 (1.3678)	grad_norm 4.9523 (5.3198)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:46:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1000/2502]	eta 0:09:40 lr 0.000040	 wd 0.0000	time 0.3111 (0.3864)	loss 1.2864 (1.3643)	grad_norm 5.2531 (5.3191)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:47:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1100/2502]	eta 0:08:59 lr 0.000040	 wd 0.0000	time 0.3392 (0.3847)	loss 1.1805 (1.3635)	grad_norm 5.7570 (5.3490)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:48:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1200/2502]	eta 0:08:30 lr 0.000040	 wd 0.0000	time 0.3968 (0.3922)	loss 1.0224 (1.3643)	grad_norm 4.5786 (5.3691)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:49:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1300/2502]	eta 0:07:57 lr 0.000040	 wd 0.0000	time 0.3495 (0.3969)	loss 1.5437 (1.3625)	grad_norm 4.2198 (5.3390)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:50:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1400/2502]	eta 0:07:40 lr 0.000040	 wd 0.0000	time 0.4144 (0.4183)	loss 1.6218 (1.3610)	grad_norm 6.9253 (5.3478)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:51:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1500/2502]	eta 0:07:03 lr 0.000040	 wd 0.0000	time 0.4656 (0.4225)	loss 1.4782 (1.3630)	grad_norm 5.7373 (5.3750)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:52:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1600/2502]	eta 0:06:33 lr 0.000040	 wd 0.0000	time 0.4192 (0.4364)	loss 1.5748 (1.3617)	grad_norm 8.4272 (5.3760)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:53:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1700/2502]	eta 0:05:57 lr 0.000040	 wd 0.0000	time 0.3307 (0.4461)	loss 1.5476 (1.3622)	grad_norm 6.5869 (5.3641)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:53:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1800/2502]	eta 0:05:13 lr 0.000040	 wd 0.0000	time 0.3370 (0.4465)	loss 1.2634 (1.3622)	grad_norm 6.0668 (5.3882)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:54:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][1900/2502]	eta 0:04:34 lr 0.000040	 wd 0.0000	time 0.4371 (0.4564)	loss 1.2103 (1.3614)	grad_norm 3.4556 (5.3513)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:55:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][2000/2502]	eta 0:03:48 lr 0.000039	 wd 0.0000	time 0.3294 (0.4560)	loss 1.1104 (1.3618)	grad_norm 5.7379 (5.3339)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:56:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][2100/2502]	eta 0:03:04 lr 0.000039	 wd 0.0000	time 0.3766 (0.4596)	loss 1.3938 (1.3611)	grad_norm 29.5737 (5.3699)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:57:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][2200/2502]	eta 0:02:18 lr 0.000039	 wd 0.0000	time 0.3594 (0.4593)	loss 1.2621 (1.3612)	grad_norm 5.4153 (5.3814)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:58:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][2300/2502]	eta 0:01:34 lr 0.000039	 wd 0.0000	time 0.4390 (0.4693)	loss 1.4559 (1.3610)	grad_norm 4.3068 (5.3617)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 21:59:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][2400/2502]	eta 0:00:48 lr 0.000039	 wd 0.0000	time 0.5640 (0.4708)	loss 1.1836 (1.3607)	grad_norm 4.5218 (5.3629)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:00:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.3307 (0.4684)	loss 1.0523 (1.3613)	grad_norm 8.2623 (5.3517)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:00:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 6 training takes 0:19:40
[2024-05-27 22:01:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 65.659 (65.659)	Loss 0.4365 (0.4365)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-27 22:01:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.138 Acc@5 97.314
[2024-05-27 22:01:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-05-27 22:01:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.14%
[2024-05-27 22:01:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 22:01:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 22:01:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][0/2502]	eta 14:31:20 lr 0.000039	 wd 0.0000	time 20.8956 (20.8956)	loss 1.6300 (1.6300)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:02:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][100/2502]	eta 0:24:36 lr 0.000039	 wd 0.0000	time 0.3743 (0.6146)	loss 1.2749 (1.3708)	grad_norm 4.5507 (4.9426)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:03:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][200/2502]	eta 0:18:48 lr 0.000039	 wd 0.0000	time 0.3534 (0.4904)	loss 1.4751 (1.3527)	grad_norm 4.2757 (5.1111)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:03:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][300/2502]	eta 0:16:25 lr 0.000039	 wd 0.0000	time 0.3311 (0.4476)	loss 1.3819 (1.3553)	grad_norm 5.1942 (5.0808)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:04:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][400/2502]	eta 0:14:57 lr 0.000039	 wd 0.0000	time 0.3411 (0.4269)	loss 1.3286 (1.3589)	grad_norm 6.5971 (5.1062)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:05:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][500/2502]	eta 0:13:48 lr 0.000039	 wd 0.0000	time 0.3529 (0.4137)	loss 1.5047 (1.3539)	grad_norm 5.6454 (5.2072)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:05:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][600/2502]	eta 0:12:51 lr 0.000039	 wd 0.0000	time 0.3210 (0.4059)	loss 1.3785 (1.3586)	grad_norm 3.4907 (5.1975)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:06:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][700/2502]	eta 0:12:02 lr 0.000039	 wd 0.0000	time 0.3677 (0.4008)	loss 1.3400 (1.3599)	grad_norm 5.1228 (5.1869)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:06:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][800/2502]	eta 0:11:14 lr 0.000039	 wd 0.0000	time 0.2875 (0.3961)	loss 0.9292 (1.3611)	grad_norm 5.6728 (5.1863)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:07:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][900/2502]	eta 0:10:35 lr 0.000039	 wd 0.0000	time 0.4777 (0.3966)	loss 1.0936 (1.3629)	grad_norm 3.9997 (5.1809)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:08:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1000/2502]	eta 0:10:09 lr 0.000039	 wd 0.0000	time 0.4592 (0.4055)	loss 1.1796 (1.3588)	grad_norm 5.2771 (5.1711)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:09:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1100/2502]	eta 0:09:46 lr 0.000039	 wd 0.0000	time 0.4391 (0.4181)	loss 1.3163 (1.3556)	grad_norm 4.7805 (5.1551)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:10:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1200/2502]	eta 0:09:39 lr 0.000039	 wd 0.0000	time 0.3862 (0.4448)	loss 1.2845 (1.3534)	grad_norm 4.0103 (5.1909)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:11:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1300/2502]	eta 0:08:51 lr 0.000039	 wd 0.0000	time 0.3142 (0.4426)	loss 1.6195 (1.3544)	grad_norm 3.8687 (5.1596)	loss_scale 2048.0000 (1033.4450)	mem 12561MB
[2024-05-27 22:11:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1400/2502]	eta 0:08:01 lr 0.000039	 wd 0.0000	time 0.3234 (0.4370)	loss 1.4488 (1.3549)	grad_norm 2.9486 (inf)	loss_scale 1024.0000 (1063.4690)	mem 12561MB
[2024-05-27 22:12:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1500/2502]	eta 0:07:13 lr 0.000039	 wd 0.0000	time 0.3846 (0.4330)	loss 1.3930 (1.3548)	grad_norm 4.8907 (inf)	loss_scale 1024.0000 (1060.8394)	mem 12561MB
[2024-05-27 22:13:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1600/2502]	eta 0:06:27 lr 0.000039	 wd 0.0000	time 0.3556 (0.4291)	loss 1.6545 (1.3564)	grad_norm 4.6192 (inf)	loss_scale 1024.0000 (1058.5384)	mem 12561MB
[2024-05-27 22:13:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1700/2502]	eta 0:05:41 lr 0.000039	 wd 0.0000	time 0.3586 (0.4254)	loss 1.4345 (1.3564)	grad_norm 8.2731 (inf)	loss_scale 1024.0000 (1056.5079)	mem 12561MB
[2024-05-27 22:14:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1800/2502]	eta 0:04:57 lr 0.000039	 wd 0.0000	time 0.4068 (0.4231)	loss 1.3855 (1.3560)	grad_norm 4.7960 (inf)	loss_scale 1024.0000 (1054.7029)	mem 12561MB
[2024-05-27 22:14:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][1900/2502]	eta 0:04:13 lr 0.000039	 wd 0.0000	time 0.3775 (0.4204)	loss 1.2831 (1.3558)	grad_norm 4.3406 (inf)	loss_scale 1024.0000 (1053.0878)	mem 12561MB
[2024-05-27 22:15:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][2000/2502]	eta 0:03:29 lr 0.000039	 wd 0.0000	time 0.3034 (0.4178)	loss 1.4506 (1.3570)	grad_norm 5.4652 (inf)	loss_scale 1024.0000 (1051.6342)	mem 12561MB
[2024-05-27 22:16:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][2100/2502]	eta 0:02:49 lr 0.000039	 wd 0.0000	time 0.4180 (0.4204)	loss 1.5002 (1.3587)	grad_norm 4.0167 (inf)	loss_scale 1024.0000 (1050.3189)	mem 12561MB
[2024-05-27 22:17:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][2200/2502]	eta 0:02:07 lr 0.000039	 wd 0.0000	time 0.3640 (0.4222)	loss 1.2553 (1.3574)	grad_norm 6.6513 (inf)	loss_scale 1024.0000 (1049.1231)	mem 12561MB
[2024-05-27 22:17:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][2300/2502]	eta 0:01:26 lr 0.000039	 wd 0.0000	time 0.3559 (0.4262)	loss 1.3831 (1.3576)	grad_norm 4.5455 (inf)	loss_scale 1024.0000 (1048.0313)	mem 12561MB
[2024-05-27 22:18:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][2400/2502]	eta 0:00:43 lr 0.000039	 wd 0.0000	time 0.3841 (0.4280)	loss 1.3831 (1.3570)	grad_norm 3.3289 (inf)	loss_scale 1024.0000 (1047.0304)	mem 12561MB
[2024-05-27 22:19:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.3303 (0.4295)	loss 1.2578 (1.3569)	grad_norm 7.6011 (inf)	loss_scale 1024.0000 (1046.1096)	mem 12561MB
[2024-05-27 22:19:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 7 training takes 0:18:03
[2024-05-27 22:20:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 58.541 (58.541)	Loss 0.4263 (0.4263)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 12561MB
[2024-05-27 22:20:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.048 Acc@5 97.318
[2024-05-27 22:20:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-05-27 22:20:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.14%
[2024-05-27 22:21:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][0/2502]	eta 11:13:59 lr 0.000039	 wd 0.0000	time 16.1628 (16.1628)	loss 1.5676 (1.5676)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:21:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][100/2502]	eta 0:22:52 lr 0.000039	 wd 0.0000	time 0.3649 (0.5714)	loss 1.4995 (1.3882)	grad_norm 3.5693 (5.0370)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:22:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][200/2502]	eta 0:18:50 lr 0.000039	 wd 0.0000	time 0.3261 (0.4910)	loss 1.5137 (1.3663)	grad_norm 5.1602 (5.6900)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:23:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][300/2502]	eta 0:16:26 lr 0.000038	 wd 0.0000	time 0.3273 (0.4481)	loss 1.4648 (1.3643)	grad_norm 3.4771 (5.4128)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:23:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][400/2502]	eta 0:14:58 lr 0.000038	 wd 0.0000	time 0.3669 (0.4277)	loss 1.5977 (1.3606)	grad_norm 4.4954 (5.1561)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:24:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][500/2502]	eta 0:13:51 lr 0.000038	 wd 0.0000	time 0.3247 (0.4152)	loss 1.6152 (1.3659)	grad_norm 8.0012 (5.0874)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:25:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][600/2502]	eta 0:12:55 lr 0.000038	 wd 0.0000	time 0.3747 (0.4077)	loss 1.4590 (1.3684)	grad_norm 4.9940 (5.1323)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:25:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][700/2502]	eta 0:12:44 lr 0.000038	 wd 0.0000	time 0.4243 (0.4241)	loss 1.5518 (1.3689)	grad_norm 8.0387 (5.0988)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:26:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][800/2502]	eta 0:12:05 lr 0.000038	 wd 0.0000	time 0.3546 (0.4264)	loss 1.3522 (1.3674)	grad_norm 4.5062 (5.0851)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:27:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][900/2502]	eta 0:12:02 lr 0.000038	 wd 0.0000	time 0.3519 (0.4510)	loss 1.4988 (1.3662)	grad_norm 5.6581 (5.1554)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:28:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1000/2502]	eta 0:11:18 lr 0.000038	 wd 0.0000	time 0.3742 (0.4516)	loss 1.5083 (1.3633)	grad_norm 4.2991 (5.1056)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:29:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1100/2502]	eta 0:11:14 lr 0.000038	 wd 0.0000	time 0.5277 (0.4809)	loss 1.4070 (1.3591)	grad_norm 5.2878 (5.0749)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:30:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1200/2502]	eta 0:10:23 lr 0.000038	 wd 0.0000	time 0.3092 (0.4786)	loss 1.4713 (1.3595)	grad_norm 3.4856 (5.0800)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:31:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1300/2502]	eta 0:09:24 lr 0.000038	 wd 0.0000	time 0.3388 (0.4700)	loss 1.2176 (1.3576)	grad_norm 5.0726 (5.0437)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:31:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1400/2502]	eta 0:08:29 lr 0.000038	 wd 0.0000	time 0.3491 (0.4626)	loss 0.8455 (1.3581)	grad_norm 4.5804 (5.0752)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:32:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1500/2502]	eta 0:07:37 lr 0.000038	 wd 0.0000	time 0.3198 (0.4567)	loss 0.9194 (1.3553)	grad_norm 4.5502 (5.0659)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:33:00 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1600/2502]	eta 0:06:47 lr 0.000038	 wd 0.0000	time 0.3075 (0.4512)	loss 1.2338 (1.3517)	grad_norm 3.6805 (5.0680)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:33:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1700/2502]	eta 0:05:58 lr 0.000038	 wd 0.0000	time 0.3620 (0.4466)	loss 1.2192 (1.3530)	grad_norm 3.9864 (5.0749)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:34:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1800/2502]	eta 0:05:10 lr 0.000038	 wd 0.0000	time 0.3532 (0.4428)	loss 1.5190 (1.3541)	grad_norm 4.2571 (5.0683)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:34:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][1900/2502]	eta 0:04:24 lr 0.000038	 wd 0.0000	time 0.3998 (0.4392)	loss 1.4555 (1.3545)	grad_norm 5.3839 (5.0757)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:35:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][2000/2502]	eta 0:03:40 lr 0.000038	 wd 0.0000	time 0.4168 (0.4396)	loss 1.7478 (1.3553)	grad_norm 2.9974 (5.0582)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:36:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][2100/2502]	eta 0:02:57 lr 0.000038	 wd 0.0000	time 0.3747 (0.4413)	loss 1.5059 (1.3548)	grad_norm 4.6053 (5.0383)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:37:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][2200/2502]	eta 0:02:14 lr 0.000038	 wd 0.0000	time 0.4991 (0.4458)	loss 1.4667 (1.3529)	grad_norm 4.2357 (5.0392)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:38:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][2300/2502]	eta 0:01:30 lr 0.000038	 wd 0.0000	time 0.4432 (0.4464)	loss 1.5537 (1.3532)	grad_norm 4.1821 (5.0295)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:39:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][2400/2502]	eta 0:00:46 lr 0.000038	 wd 0.0000	time 0.3743 (0.4532)	loss 1.3768 (1.3551)	grad_norm 4.3786 (5.0246)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:39:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000038	 wd 0.0000	time 0.3254 (0.4498)	loss 1.8808 (1.3567)	grad_norm 3.6248 (5.0145)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:39:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 8 training takes 0:18:54
[2024-05-27 22:41:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 74.211 (74.211)	Loss 0.4185 (0.4185)	Acc@1 91.992 (91.992)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-27 22:41:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.336 Acc@5 97.338
[2024-05-27 22:41:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.3%
[2024-05-27 22:41:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.34%
[2024-05-27 22:41:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 22:41:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 22:41:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][0/2502]	eta 11:07:18 lr 0.000038	 wd 0.0000	time 16.0028 (16.0028)	loss 1.4601 (1.4601)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:42:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][100/2502]	eta 0:22:14 lr 0.000038	 wd 0.0000	time 0.3310 (0.5556)	loss 1.2979 (1.3269)	grad_norm 4.4920 (4.8826)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:42:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][200/2502]	eta 0:17:35 lr 0.000037	 wd 0.0000	time 0.3703 (0.4585)	loss 1.5245 (1.3347)	grad_norm 4.8050 (4.9287)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:43:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][300/2502]	eta 0:15:46 lr 0.000037	 wd 0.0000	time 0.4260 (0.4297)	loss 1.3654 (1.3426)	grad_norm 6.3508 (4.8729)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-27 22:44:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][400/2502]	eta 0:14:29 lr 0.000037	 wd 0.0000	time 0.3267 (0.4135)	loss 1.4772 (1.3347)	grad_norm 5.4176 (4.8793)	loss_scale 2048.0000 (1182.3242)	mem 12561MB
[2024-05-27 22:44:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][500/2502]	eta 0:13:26 lr 0.000037	 wd 0.0000	time 0.3358 (0.4028)	loss 1.3968 (1.3414)	grad_norm 4.0642 (4.8114)	loss_scale 2048.0000 (1355.1138)	mem 12561MB
[2024-05-27 22:45:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][600/2502]	eta 0:12:37 lr 0.000037	 wd 0.0000	time 0.3575 (0.3981)	loss 1.4870 (1.3360)	grad_norm 6.7373 (4.8461)	loss_scale 2048.0000 (1470.4027)	mem 12561MB
[2024-05-27 22:46:00 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][700/2502]	eta 0:11:48 lr 0.000037	 wd 0.0000	time 0.3176 (0.3933)	loss 1.6318 (1.3367)	grad_norm 3.3631 (4.8395)	loss_scale 2048.0000 (1552.7989)	mem 12561MB
[2024-05-27 22:46:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][800/2502]	eta 0:11:03 lr 0.000037	 wd 0.0000	time 0.3239 (0.3898)	loss 1.3067 (1.3391)	grad_norm 5.7472 (4.8443)	loss_scale 2048.0000 (1614.6217)	mem 12561MB
[2024-05-27 22:47:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][900/2502]	eta 0:10:22 lr 0.000037	 wd 0.0000	time 0.3398 (0.3884)	loss 1.4212 (1.3369)	grad_norm 3.7706 (4.8551)	loss_scale 2048.0000 (1662.7214)	mem 12561MB
[2024-05-27 22:47:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1000/2502]	eta 0:09:39 lr 0.000037	 wd 0.0000	time 0.3200 (0.3859)	loss 1.5437 (1.3391)	grad_norm 3.7083 (4.8455)	loss_scale 2048.0000 (1701.2108)	mem 12561MB
[2024-05-27 22:48:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1100/2502]	eta 0:08:58 lr 0.000037	 wd 0.0000	time 0.3920 (0.3842)	loss 1.4503 (1.3424)	grad_norm 3.7179 (4.8948)	loss_scale 2048.0000 (1732.7084)	mem 12561MB
[2024-05-27 22:49:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1200/2502]	eta 0:08:31 lr 0.000037	 wd 0.0000	time 0.5263 (0.3926)	loss 1.5909 (1.3433)	grad_norm 3.0765 (4.8975)	loss_scale 2048.0000 (1758.9609)	mem 12561MB
[2024-05-27 22:50:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1300/2502]	eta 0:07:58 lr 0.000037	 wd 0.0000	time 0.4776 (0.3979)	loss 0.9293 (1.3405)	grad_norm 9.0049 (4.9565)	loss_scale 2048.0000 (1781.1776)	mem 12561MB
[2024-05-27 22:50:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1400/2502]	eta 0:07:27 lr 0.000037	 wd 0.0000	time 0.4100 (0.4064)	loss 1.2776 (1.3419)	grad_norm 4.3477 (4.9946)	loss_scale 2048.0000 (1800.2227)	mem 12561MB
[2024-05-27 22:51:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1500/2502]	eta 0:06:51 lr 0.000037	 wd 0.0000	time 0.3885 (0.4105)	loss 1.4435 (1.3440)	grad_norm 3.5717 (4.9837)	loss_scale 2048.0000 (1816.7302)	mem 12561MB
[2024-05-27 22:52:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1600/2502]	eta 0:06:16 lr 0.000037	 wd 0.0000	time 0.3472 (0.4175)	loss 0.9720 (1.3430)	grad_norm 4.7597 (4.9678)	loss_scale 2048.0000 (1831.1755)	mem 12561MB
[2024-05-27 22:53:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1700/2502]	eta 0:05:36 lr 0.000037	 wd 0.0000	time 0.3823 (0.4197)	loss 1.1282 (1.3447)	grad_norm 3.6317 (4.9436)	loss_scale 2048.0000 (1843.9224)	mem 12561MB
[2024-05-27 22:54:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1800/2502]	eta 0:05:02 lr 0.000037	 wd 0.0000	time 0.4770 (0.4309)	loss 1.5010 (1.3456)	grad_norm 4.3716 (4.9348)	loss_scale 2048.0000 (1855.2537)	mem 12561MB
[2024-05-27 22:55:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][1900/2502]	eta 0:04:20 lr 0.000037	 wd 0.0000	time 0.5912 (0.4321)	loss 1.3446 (1.3448)	grad_norm 3.8492 (4.9409)	loss_scale 2048.0000 (1865.3930)	mem 12561MB
[2024-05-27 22:56:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][2000/2502]	eta 0:03:40 lr 0.000037	 wd 0.0000	time 0.4315 (0.4388)	loss 1.4446 (1.3449)	grad_norm 5.7154 (4.9295)	loss_scale 2048.0000 (1874.5187)	mem 12561MB
[2024-05-27 22:56:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][2100/2502]	eta 0:02:57 lr 0.000036	 wd 0.0000	time 0.4598 (0.4410)	loss 1.1679 (1.3462)	grad_norm 4.3802 (4.9392)	loss_scale 2048.0000 (1882.7758)	mem 12561MB
[2024-05-27 22:57:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][2200/2502]	eta 0:02:15 lr 0.000036	 wd 0.0000	time 0.4314 (0.4478)	loss 1.4985 (1.3471)	grad_norm 5.7960 (4.9378)	loss_scale 2048.0000 (1890.2826)	mem 12561MB
[2024-05-27 22:58:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][2300/2502]	eta 0:01:30 lr 0.000036	 wd 0.0000	time 0.5118 (0.4497)	loss 1.4748 (1.3474)	grad_norm 16.2251 (4.9415)	loss_scale 2048.0000 (1897.1369)	mem 12561MB
[2024-05-27 22:59:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][2400/2502]	eta 0:00:45 lr 0.000036	 wd 0.0000	time 0.4220 (0.4502)	loss 1.1843 (1.3475)	grad_norm 4.8833 (4.9264)	loss_scale 2048.0000 (1903.4202)	mem 12561MB
[2024-05-27 23:00:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000036	 wd 0.0000	time 0.3465 (0.4480)	loss 0.8806 (1.3486)	grad_norm 3.4567 (4.9157)	loss_scale 2048.0000 (1909.2011)	mem 12561MB
[2024-05-27 23:00:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 9 training takes 0:18:59
[2024-05-27 23:01:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 47.204 (47.204)	Loss 0.4180 (0.4180)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-27 23:01:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.282 Acc@5 97.340
[2024-05-27 23:01:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.3%
[2024-05-27 23:01:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.34%
[2024-05-27 23:02:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][0/2502]	eta 1 day, 2:09:12 lr 0.000036	 wd 0.0000	time 37.6310 (37.6310)	loss 1.2822 (1.2822)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:02:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][100/2502]	eta 0:29:17 lr 0.000036	 wd 0.0000	time 0.3273 (0.7316)	loss 1.2797 (1.3211)	grad_norm 4.9654 (5.0890)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:03:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][200/2502]	eta 0:21:10 lr 0.000036	 wd 0.0000	time 0.3671 (0.5520)	loss 1.2869 (1.3431)	grad_norm 8.6935 (4.8271)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:03:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][300/2502]	eta 0:18:01 lr 0.000036	 wd 0.0000	time 0.3455 (0.4913)	loss 0.7939 (1.3441)	grad_norm 3.3724 (5.3148)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:04:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][400/2502]	eta 0:16:06 lr 0.000036	 wd 0.0000	time 0.3412 (0.4598)	loss 1.4898 (1.3447)	grad_norm 5.8269 (5.1706)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:05:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][500/2502]	eta 0:14:46 lr 0.000036	 wd 0.0000	time 0.3631 (0.4428)	loss 1.3199 (1.3411)	grad_norm 3.9052 (5.0973)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:05:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][600/2502]	eta 0:13:38 lr 0.000036	 wd 0.0000	time 0.3154 (0.4305)	loss 1.4634 (1.3412)	grad_norm 3.7521 (4.9916)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:06:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][700/2502]	eta 0:12:39 lr 0.000036	 wd 0.0000	time 0.3330 (0.4213)	loss 1.4927 (1.3382)	grad_norm 4.1342 (5.0891)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:06:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][800/2502]	eta 0:11:47 lr 0.000036	 wd 0.0000	time 0.3510 (0.4156)	loss 1.2606 (1.3373)	grad_norm 6.4764 (5.0487)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:07:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][900/2502]	eta 0:10:56 lr 0.000036	 wd 0.0000	time 0.3515 (0.4099)	loss 1.5034 (1.3388)	grad_norm 6.2606 (4.9676)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:08:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1000/2502]	eta 0:10:08 lr 0.000036	 wd 0.0000	time 0.3503 (0.4053)	loss 1.4965 (1.3365)	grad_norm 4.6103 (4.9523)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:08:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1100/2502]	eta 0:09:38 lr 0.000036	 wd 0.0000	time 0.3639 (0.4124)	loss 1.3394 (1.3346)	grad_norm 3.0169 (4.9921)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:09:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1200/2502]	eta 0:09:03 lr 0.000035	 wd 0.0000	time 0.3989 (0.4171)	loss 1.3145 (1.3338)	grad_norm 5.6461 (4.9871)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:10:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1300/2502]	eta 0:08:47 lr 0.000035	 wd 0.0000	time 0.4864 (0.4385)	loss 1.5626 (1.3360)	grad_norm 4.3209 (5.0638)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:11:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1400/2502]	eta 0:08:13 lr 0.000035	 wd 0.0000	time 0.5782 (0.4477)	loss 1.3224 (1.3377)	grad_norm 7.3929 (5.0408)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:12:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1500/2502]	eta 0:07:35 lr 0.000035	 wd 0.0000	time 0.3649 (0.4542)	loss 1.1528 (1.3372)	grad_norm 3.1867 (4.9938)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:13:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1600/2502]	eta 0:06:52 lr 0.000035	 wd 0.0000	time 0.3913 (0.4577)	loss 1.1371 (1.3380)	grad_norm 4.9483 (4.9604)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:14:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1700/2502]	eta 0:06:07 lr 0.000035	 wd 0.0000	time 0.3754 (0.4581)	loss 1.3795 (1.3392)	grad_norm 5.0518 (4.9891)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:15:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1800/2502]	eta 0:05:29 lr 0.000035	 wd 0.0000	time 0.3758 (0.4699)	loss 1.6203 (1.3403)	grad_norm 5.7643 (5.0005)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:16:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][1900/2502]	eta 0:04:42 lr 0.000035	 wd 0.0000	time 0.3766 (0.4700)	loss 1.5664 (1.3418)	grad_norm 4.3827 (4.9793)	loss_scale 4096.0000 (2116.9490)	mem 12561MB
[2024-05-27 23:17:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][2000/2502]	eta 0:03:57 lr 0.000035	 wd 0.0000	time 0.3718 (0.4727)	loss 1.2983 (1.3423)	grad_norm 4.5786 (5.0009)	loss_scale 4096.0000 (2215.8521)	mem 12561MB
[2024-05-27 23:17:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][2100/2502]	eta 0:03:09 lr 0.000035	 wd 0.0000	time 0.3617 (0.4720)	loss 1.5651 (1.3440)	grad_norm 3.6497 (4.9834)	loss_scale 4096.0000 (2305.3403)	mem 12561MB
[2024-05-27 23:18:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][2200/2502]	eta 0:02:22 lr 0.000035	 wd 0.0000	time 0.3216 (0.4732)	loss 0.9156 (1.3445)	grad_norm 5.8159 (nan)	loss_scale 2048.0000 (2302.9532)	mem 12561MB
[2024-05-27 23:19:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][2300/2502]	eta 0:01:35 lr 0.000035	 wd 0.0000	time 0.3701 (0.4727)	loss 1.5047 (1.3439)	grad_norm 16.6374 (nan)	loss_scale 2048.0000 (2291.8731)	mem 12561MB
[2024-05-27 23:20:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][2400/2502]	eta 0:00:48 lr 0.000035	 wd 0.0000	time 0.3508 (0.4748)	loss 1.4615 (1.3438)	grad_norm 5.3155 (nan)	loss_scale 2048.0000 (2281.7160)	mem 12561MB
[2024-05-27 23:21:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.3340 (0.4721)	loss 1.3798 (1.3426)	grad_norm 5.3732 (nan)	loss_scale 2048.0000 (2272.3711)	mem 12561MB
[2024-05-27 23:21:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 10 training takes 0:19:47
[2024-05-27 23:22:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 74.388 (74.388)	Loss 0.4119 (0.4119)	Acc@1 91.602 (91.602)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-27 23:22:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.378 Acc@5 97.422
[2024-05-27 23:22:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-05-27 23:22:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.38%
[2024-05-27 23:22:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 23:22:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 23:23:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][0/2502]	eta 22:04:20 lr 0.000035	 wd 0.0000	time 31.7590 (31.7590)	loss 1.4456 (1.4456)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-27 23:23:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][100/2502]	eta 0:27:10 lr 0.000035	 wd 0.0000	time 0.3064 (0.6788)	loss 1.4036 (1.3410)	grad_norm 4.4846 (inf)	loss_scale 1024.0000 (1115.2475)	mem 12561MB
[2024-05-27 23:24:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][200/2502]	eta 0:20:03 lr 0.000034	 wd 0.0000	time 0.3482 (0.5230)	loss 1.5371 (1.3404)	grad_norm 3.6259 (inf)	loss_scale 1024.0000 (1069.8507)	mem 12561MB
[2024-05-27 23:25:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][300/2502]	eta 0:17:18 lr 0.000034	 wd 0.0000	time 0.3200 (0.4716)	loss 1.3316 (1.3385)	grad_norm 3.4694 (inf)	loss_scale 1024.0000 (1054.6179)	mem 12561MB
[2024-05-27 23:25:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][400/2502]	eta 0:15:33 lr 0.000034	 wd 0.0000	time 0.3469 (0.4442)	loss 1.5745 (1.3400)	grad_norm 4.8969 (inf)	loss_scale 1024.0000 (1046.9825)	mem 12561MB
[2024-05-27 23:26:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][500/2502]	eta 0:14:18 lr 0.000034	 wd 0.0000	time 0.3748 (0.4289)	loss 1.4114 (1.3368)	grad_norm 4.9729 (inf)	loss_scale 1024.0000 (1042.3952)	mem 12561MB
[2024-05-27 23:26:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][600/2502]	eta 0:13:17 lr 0.000034	 wd 0.0000	time 0.3550 (0.4195)	loss 1.3226 (1.3327)	grad_norm 4.0614 (inf)	loss_scale 1024.0000 (1039.3344)	mem 12561MB
[2024-05-27 23:27:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][700/2502]	eta 0:12:21 lr 0.000034	 wd 0.0000	time 0.3266 (0.4116)	loss 1.4307 (1.3324)	grad_norm 3.3574 (inf)	loss_scale 1024.0000 (1037.1469)	mem 12561MB
[2024-05-27 23:28:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][800/2502]	eta 0:11:32 lr 0.000034	 wd 0.0000	time 0.3266 (0.4069)	loss 1.7027 (1.3350)	grad_norm 4.5018 (inf)	loss_scale 1024.0000 (1035.5056)	mem 12561MB
[2024-05-27 23:28:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][900/2502]	eta 0:10:44 lr 0.000034	 wd 0.0000	time 0.3370 (0.4023)	loss 1.6368 (1.3357)	grad_norm 5.1053 (inf)	loss_scale 1024.0000 (1034.2286)	mem 12561MB
[2024-05-27 23:29:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1000/2502]	eta 0:09:58 lr 0.000034	 wd 0.0000	time 0.3661 (0.3983)	loss 1.4986 (1.3366)	grad_norm 4.0868 (inf)	loss_scale 1024.0000 (1033.2068)	mem 12561MB
[2024-05-27 23:30:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1100/2502]	eta 0:09:27 lr 0.000034	 wd 0.0000	time 0.3393 (0.4048)	loss 1.5737 (1.3388)	grad_norm 3.5523 (inf)	loss_scale 1024.0000 (1032.3706)	mem 12561MB
[2024-05-27 23:30:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1200/2502]	eta 0:08:53 lr 0.000034	 wd 0.0000	time 0.4178 (0.4094)	loss 1.5427 (1.3398)	grad_norm 3.7370 (inf)	loss_scale 1024.0000 (1031.6736)	mem 12561MB
[2024-05-27 23:32:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1300/2502]	eta 0:08:36 lr 0.000034	 wd 0.0000	time 0.4854 (0.4297)	loss 0.9717 (1.3387)	grad_norm 4.3893 (inf)	loss_scale 1024.0000 (1031.0838)	mem 12561MB
[2024-05-27 23:32:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1400/2502]	eta 0:07:55 lr 0.000034	 wd 0.0000	time 0.3986 (0.4314)	loss 0.9034 (1.3379)	grad_norm 4.8819 (inf)	loss_scale 1024.0000 (1030.5782)	mem 12561MB
[2024-05-27 23:34:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1500/2502]	eta 0:07:35 lr 0.000034	 wd 0.0000	time 0.3758 (0.4543)	loss 1.5488 (1.3376)	grad_norm 3.5529 (inf)	loss_scale 1024.0000 (1030.1399)	mem 12561MB
[2024-05-27 23:35:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1600/2502]	eta 0:06:59 lr 0.000034	 wd 0.0000	time 0.3764 (0.4646)	loss 1.2339 (1.3380)	grad_norm 4.0107 (inf)	loss_scale 1024.0000 (1029.7564)	mem 12561MB
[2024-05-27 23:35:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1700/2502]	eta 0:06:12 lr 0.000033	 wd 0.0000	time 0.4490 (0.4641)	loss 1.4664 (1.3371)	grad_norm 4.1066 (inf)	loss_scale 1024.0000 (1029.4180)	mem 12561MB
[2024-05-27 23:37:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1800/2502]	eta 0:05:37 lr 0.000033	 wd 0.0000	time 0.5372 (0.4808)	loss 1.2370 (1.3381)	grad_norm 4.0304 (inf)	loss_scale 1024.0000 (1029.1172)	mem 12561MB
[2024-05-27 23:38:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][1900/2502]	eta 0:04:51 lr 0.000033	 wd 0.0000	time 0.4583 (0.4835)	loss 1.5150 (1.3389)	grad_norm 4.1327 (inf)	loss_scale 1024.0000 (1028.8480)	mem 12561MB
[2024-05-27 23:39:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][2000/2502]	eta 0:04:05 lr 0.000033	 wd 0.0000	time 0.3723 (0.4898)	loss 1.1441 (1.3384)	grad_norm 6.4596 (inf)	loss_scale 1024.0000 (1028.6057)	mem 12561MB
[2024-05-27 23:39:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][2100/2502]	eta 0:03:17 lr 0.000033	 wd 0.0000	time 0.3409 (0.4907)	loss 1.5143 (1.3379)	grad_norm 3.9956 (inf)	loss_scale 1024.0000 (1028.3865)	mem 12561MB
[2024-05-27 23:40:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][2200/2502]	eta 0:02:27 lr 0.000033	 wd 0.0000	time 0.4418 (0.4893)	loss 1.3376 (1.3388)	grad_norm 3.2279 (inf)	loss_scale 1024.0000 (1028.1872)	mem 12561MB
[2024-05-27 23:41:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][2300/2502]	eta 0:01:38 lr 0.000033	 wd 0.0000	time 0.4166 (0.4901)	loss 1.4157 (1.3388)	grad_norm 4.1161 (inf)	loss_scale 512.0000 (1012.4294)	mem 12561MB
[2024-05-27 23:42:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][2400/2502]	eta 0:00:49 lr 0.000033	 wd 0.0000	time 0.3850 (0.4881)	loss 1.2617 (1.3389)	grad_norm 4.1921 (inf)	loss_scale 512.0000 (991.5868)	mem 12561MB
[2024-05-27 23:42:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000033	 wd 0.0000	time 0.3278 (0.4840)	loss 1.2912 (1.3387)	grad_norm 3.6871 (inf)	loss_scale 512.0000 (972.4110)	mem 12561MB
[2024-05-27 23:43:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 11 training takes 0:20:27
[2024-05-27 23:43:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 45.600 (45.600)	Loss 0.3987 (0.3987)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-27 23:44:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.558 Acc@5 97.372
[2024-05-27 23:44:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-05-27 23:44:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.56%
[2024-05-27 23:44:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-27 23:44:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-27 23:44:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][0/2502]	eta 23:47:38 lr 0.000033	 wd 0.0000	time 34.2362 (34.2362)	loss 1.5185 (1.5185)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:45:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][100/2502]	eta 0:28:06 lr 0.000033	 wd 0.0000	time 0.3367 (0.7022)	loss 1.4575 (1.3314)	grad_norm 3.3382 (4.6012)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:46:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][200/2502]	eta 0:20:43 lr 0.000033	 wd 0.0000	time 0.4010 (0.5400)	loss 1.0724 (1.3419)	grad_norm 3.1268 (5.7488)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:46:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][300/2502]	eta 0:17:39 lr 0.000033	 wd 0.0000	time 0.3213 (0.4809)	loss 1.3417 (1.3431)	grad_norm 4.9653 (5.6275)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:47:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][400/2502]	eta 0:15:47 lr 0.000033	 wd 0.0000	time 0.3306 (0.4507)	loss 1.3210 (1.3401)	grad_norm 5.0470 (5.4502)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:47:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][500/2502]	eta 0:14:32 lr 0.000032	 wd 0.0000	time 0.3667 (0.4356)	loss 0.9041 (1.3369)	grad_norm 2.9344 (5.5787)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:48:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][600/2502]	eta 0:13:25 lr 0.000032	 wd 0.0000	time 0.3074 (0.4237)	loss 1.3038 (1.3375)	grad_norm 5.1242 (5.5021)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:49:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][700/2502]	eta 0:12:28 lr 0.000032	 wd 0.0000	time 0.3362 (0.4154)	loss 1.2594 (1.3352)	grad_norm 6.9582 (5.5240)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:49:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][800/2502]	eta 0:11:37 lr 0.000032	 wd 0.0000	time 0.3294 (0.4099)	loss 1.6185 (1.3421)	grad_norm 4.6668 (5.4149)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:50:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][900/2502]	eta 0:10:48 lr 0.000032	 wd 0.0000	time 0.3221 (0.4048)	loss 0.9763 (1.3433)	grad_norm 6.2072 (5.4144)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:50:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1000/2502]	eta 0:10:02 lr 0.000032	 wd 0.0000	time 0.3411 (0.4011)	loss 1.4293 (1.3422)	grad_norm 4.5537 (5.3680)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:51:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1100/2502]	eta 0:09:34 lr 0.000032	 wd 0.0000	time 0.5031 (0.4099)	loss 1.3411 (1.3461)	grad_norm 3.6348 (5.3126)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:52:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1200/2502]	eta 0:08:58 lr 0.000032	 wd 0.0000	time 0.4580 (0.4136)	loss 1.3719 (1.3435)	grad_norm 5.7705 (5.2999)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:53:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1300/2502]	eta 0:08:44 lr 0.000032	 wd 0.0000	time 0.4472 (0.4364)	loss 1.4096 (1.3424)	grad_norm 6.7351 (5.2824)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:54:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1400/2502]	eta 0:08:18 lr 0.000032	 wd 0.0000	time 0.6648 (0.4527)	loss 1.6020 (1.3433)	grad_norm 3.9650 (5.2741)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:55:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1500/2502]	eta 0:07:36 lr 0.000032	 wd 0.0000	time 0.3360 (0.4551)	loss 1.5592 (1.3448)	grad_norm 5.4377 (5.2723)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:56:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1600/2502]	eta 0:06:55 lr 0.000032	 wd 0.0000	time 0.3830 (0.4606)	loss 1.4886 (1.3430)	grad_norm 10.0487 (5.2343)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:57:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1700/2502]	eta 0:06:09 lr 0.000031	 wd 0.0000	time 0.3780 (0.4604)	loss 1.3571 (1.3452)	grad_norm 4.1010 (5.1906)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:58:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1800/2502]	eta 0:05:35 lr 0.000031	 wd 0.0000	time 0.4337 (0.4776)	loss 1.4786 (1.3461)	grad_norm 8.8931 (5.1713)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-27 23:59:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][1900/2502]	eta 0:04:49 lr 0.000031	 wd 0.0000	time 0.4856 (0.4804)	loss 1.3025 (1.3457)	grad_norm 4.7449 (5.1486)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:00:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][2000/2502]	eta 0:04:01 lr 0.000031	 wd 0.0000	time 0.3652 (0.4807)	loss 1.4359 (1.3446)	grad_norm 7.5542 (5.1598)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:01:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][2100/2502]	eta 0:03:14 lr 0.000031	 wd 0.0000	time 0.4290 (0.4841)	loss 1.4445 (1.3458)	grad_norm 3.6534 (5.1461)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:01:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][2200/2502]	eta 0:02:25 lr 0.000031	 wd 0.0000	time 0.3977 (0.4825)	loss 1.5542 (1.3466)	grad_norm 4.1967 (5.1390)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:02:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][2300/2502]	eta 0:01:38 lr 0.000031	 wd 0.0000	time 0.5775 (0.4853)	loss 1.5074 (1.3464)	grad_norm 4.9778 (5.1094)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:03:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][2400/2502]	eta 0:00:49 lr 0.000031	 wd 0.0000	time 0.3823 (0.4835)	loss 1.4520 (1.3458)	grad_norm 4.0470 (5.0856)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:04:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000031	 wd 0.0000	time 0.3403 (0.4798)	loss 1.2132 (1.3445)	grad_norm 3.2486 (5.0841)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:04:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 12 training takes 0:20:17
[2024-05-28 00:05:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 48.645 (48.645)	Loss 0.4070 (0.4070)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-28 00:05:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.534 Acc@5 97.440
[2024-05-28 00:05:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-05-28 00:05:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.56%
[2024-05-28 00:06:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][0/2502]	eta 1 day, 0:29:52 lr 0.000031	 wd 0.0000	time 35.2488 (35.2488)	loss 1.5334 (1.5334)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:06:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][100/2502]	eta 0:28:49 lr 0.000031	 wd 0.0000	time 0.3389 (0.7200)	loss 1.4411 (1.3304)	grad_norm 4.0875 (4.9924)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:07:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][200/2502]	eta 0:21:04 lr 0.000031	 wd 0.0000	time 0.3693 (0.5493)	loss 1.4749 (1.3369)	grad_norm 4.6542 (5.0113)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:08:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][300/2502]	eta 0:17:54 lr 0.000031	 wd 0.0000	time 0.3343 (0.4881)	loss 1.4987 (1.3395)	grad_norm 4.2507 (5.0246)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:08:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][400/2502]	eta 0:15:59 lr 0.000030	 wd 0.0000	time 0.3404 (0.4567)	loss 1.4186 (1.3392)	grad_norm 7.4487 (4.8919)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:09:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][500/2502]	eta 0:14:39 lr 0.000030	 wd 0.0000	time 0.3149 (0.4393)	loss 1.4676 (1.3320)	grad_norm 5.2224 (4.9152)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:09:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][600/2502]	eta 0:13:33 lr 0.000030	 wd 0.0000	time 0.3695 (0.4276)	loss 1.6090 (1.3369)	grad_norm 5.0081 (4.8969)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:10:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][700/2502]	eta 0:12:35 lr 0.000030	 wd 0.0000	time 0.3460 (0.4193)	loss 1.5270 (1.3337)	grad_norm 4.7052 (4.8786)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:11:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][800/2502]	eta 0:11:45 lr 0.000030	 wd 0.0000	time 0.3900 (0.4146)	loss 1.4743 (1.3375)	grad_norm 4.8541 (4.8990)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:11:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][900/2502]	eta 0:10:54 lr 0.000030	 wd 0.0000	time 0.3651 (0.4088)	loss 1.4722 (1.3340)	grad_norm 4.5954 (4.9064)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:12:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1000/2502]	eta 0:10:08 lr 0.000030	 wd 0.0000	time 0.3592 (0.4049)	loss 1.5814 (1.3369)	grad_norm 3.5033 (4.9190)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:13:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1100/2502]	eta 0:09:41 lr 0.000030	 wd 0.0000	time 0.4077 (0.4151)	loss 1.3629 (1.3384)	grad_norm 3.5294 (5.0207)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:13:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1200/2502]	eta 0:09:05 lr 0.000030	 wd 0.0000	time 0.3985 (0.4187)	loss 1.6334 (1.3348)	grad_norm 7.4921 (4.9969)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 00:15:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1300/2502]	eta 0:08:56 lr 0.000030	 wd 0.0000	time 0.3724 (0.4461)	loss 1.3557 (1.3353)	grad_norm 4.6026 (4.9618)	loss_scale 1024.0000 (541.1222)	mem 12561MB
[2024-05-28 00:16:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1400/2502]	eta 0:08:28 lr 0.000030	 wd 0.0000	time 0.4079 (0.4610)	loss 1.7407 (1.3355)	grad_norm 6.1788 (4.9469)	loss_scale 1024.0000 (575.5889)	mem 12561MB
[2024-05-28 00:17:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1500/2502]	eta 0:07:41 lr 0.000030	 wd 0.0000	time 0.3536 (0.4608)	loss 1.5322 (1.3347)	grad_norm 5.5742 (5.0058)	loss_scale 1024.0000 (605.4630)	mem 12561MB
[2024-05-28 00:18:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1600/2502]	eta 0:07:12 lr 0.000029	 wd 0.0000	time 0.4389 (0.4799)	loss 1.4005 (1.3349)	grad_norm 4.0839 (4.9917)	loss_scale 1024.0000 (631.6052)	mem 12561MB
[2024-05-28 00:19:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1700/2502]	eta 0:06:27 lr 0.000029	 wd 0.0000	time 0.3646 (0.4835)	loss 1.0347 (1.3324)	grad_norm 3.0874 (4.9942)	loss_scale 1024.0000 (654.6737)	mem 12561MB
[2024-05-28 00:20:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1800/2502]	eta 0:05:38 lr 0.000029	 wd 0.0000	time 0.3460 (0.4829)	loss 1.3420 (1.3337)	grad_norm 4.7428 (5.0131)	loss_scale 1024.0000 (675.1805)	mem 12561MB
[2024-05-28 00:21:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][1900/2502]	eta 0:04:58 lr 0.000029	 wd 0.0000	time 0.4092 (0.4954)	loss 1.2060 (1.3336)	grad_norm 6.2172 (5.0162)	loss_scale 1024.0000 (693.5297)	mem 12561MB
[2024-05-28 00:22:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][2000/2502]	eta 0:04:07 lr 0.000029	 wd 0.0000	time 0.3620 (0.4939)	loss 1.2417 (1.3348)	grad_norm 4.8032 (5.0261)	loss_scale 1024.0000 (710.0450)	mem 12561MB
[2024-05-28 00:22:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][2100/2502]	eta 0:03:19 lr 0.000029	 wd 0.0000	time 0.5121 (0.4960)	loss 1.1019 (1.3335)	grad_norm 4.9258 (5.0360)	loss_scale 1024.0000 (724.9881)	mem 12561MB
[2024-05-28 00:23:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][2200/2502]	eta 0:02:29 lr 0.000029	 wd 0.0000	time 0.4202 (0.4934)	loss 1.4845 (1.3351)	grad_norm 8.5914 (5.0493)	loss_scale 1024.0000 (738.5734)	mem 12561MB
[2024-05-28 00:24:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][2300/2502]	eta 0:01:39 lr 0.000029	 wd 0.0000	time 0.4140 (0.4947)	loss 1.3750 (1.3344)	grad_norm 3.9718 (5.0357)	loss_scale 1024.0000 (750.9778)	mem 12561MB
[2024-05-28 00:25:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][2400/2502]	eta 0:00:50 lr 0.000029	 wd 0.0000	time 0.3550 (0.4931)	loss 1.5672 (1.3356)	grad_norm 3.4761 (5.0280)	loss_scale 1024.0000 (762.3490)	mem 12561MB
[2024-05-28 00:26:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000029	 wd 0.0000	time 0.3301 (0.4919)	loss 1.4735 (1.3353)	grad_norm 4.8011 (5.0195)	loss_scale 1024.0000 (772.8109)	mem 12561MB
[2024-05-28 00:26:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 13 training takes 0:20:35
[2024-05-28 00:26:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 47.497 (47.497)	Loss 0.4072 (0.4072)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 00:27:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.604 Acc@5 97.432
[2024-05-28 00:27:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-05-28 00:27:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.60%
[2024-05-28 00:27:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 00:27:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 00:28:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][0/2502]	eta 17:27:00 lr 0.000029	 wd 0.0000	time 25.1083 (25.1083)	loss 1.4904 (1.4904)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:28:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][100/2502]	eta 0:27:53 lr 0.000029	 wd 0.0000	time 0.3970 (0.6967)	loss 1.5353 (1.3429)	grad_norm 3.5316 (5.9509)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:29:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][200/2502]	eta 0:20:21 lr 0.000028	 wd 0.0000	time 0.3380 (0.5307)	loss 1.1624 (1.3540)	grad_norm 3.6509 (5.5737)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:29:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][300/2502]	eta 0:17:24 lr 0.000028	 wd 0.0000	time 0.3564 (0.4744)	loss 1.2266 (1.3410)	grad_norm 6.3157 (5.2677)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:30:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][400/2502]	eta 0:15:41 lr 0.000028	 wd 0.0000	time 0.3343 (0.4481)	loss 1.3224 (1.3312)	grad_norm 4.5666 (5.1902)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:31:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][500/2502]	eta 0:14:24 lr 0.000028	 wd 0.0000	time 0.3314 (0.4320)	loss 1.3128 (1.3352)	grad_norm 8.1054 (5.1111)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:31:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][600/2502]	eta 0:13:22 lr 0.000028	 wd 0.0000	time 0.3368 (0.4217)	loss 1.5964 (1.3387)	grad_norm 5.2542 (5.0795)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:32:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][700/2502]	eta 0:12:28 lr 0.000028	 wd 0.0000	time 0.3376 (0.4155)	loss 1.2989 (1.3347)	grad_norm 5.3478 (5.1024)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:33:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][800/2502]	eta 0:11:36 lr 0.000028	 wd 0.0000	time 0.3492 (0.4094)	loss 1.2628 (1.3324)	grad_norm 5.1488 (5.2093)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:33:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][900/2502]	eta 0:10:48 lr 0.000028	 wd 0.0000	time 0.4080 (0.4051)	loss 1.3802 (1.3293)	grad_norm 4.9789 (5.1882)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:34:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1000/2502]	eta 0:10:36 lr 0.000028	 wd 0.0000	time 0.4379 (0.4236)	loss 1.2046 (1.3279)	grad_norm 4.1165 (5.1655)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:35:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1100/2502]	eta 0:10:32 lr 0.000028	 wd 0.0000	time 0.5594 (0.4510)	loss 1.3884 (1.3260)	grad_norm 9.5996 (5.1590)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:37:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1200/2502]	eta 0:10:23 lr 0.000028	 wd 0.0000	time 0.4092 (0.4785)	loss 1.6347 (1.3269)	grad_norm 2.6994 (5.1537)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:37:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1300/2502]	eta 0:09:24 lr 0.000027	 wd 0.0000	time 0.3739 (0.4700)	loss 1.3301 (1.3278)	grad_norm 3.6234 (5.1659)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:38:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1400/2502]	eta 0:08:30 lr 0.000027	 wd 0.0000	time 0.3321 (0.4631)	loss 1.4452 (1.3260)	grad_norm 3.7887 (5.1477)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:39:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1500/2502]	eta 0:07:38 lr 0.000027	 wd 0.0000	time 0.3341 (0.4573)	loss 1.1634 (1.3256)	grad_norm 3.0958 (5.1455)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:39:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1600/2502]	eta 0:06:47 lr 0.000027	 wd 0.0000	time 0.3402 (0.4515)	loss 1.2883 (1.3262)	grad_norm 5.1372 (5.1319)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:40:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1700/2502]	eta 0:05:58 lr 0.000027	 wd 0.0000	time 0.4644 (0.4470)	loss 1.2076 (1.3259)	grad_norm 5.4273 (5.1410)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:40:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1800/2502]	eta 0:05:10 lr 0.000027	 wd 0.0000	time 0.3229 (0.4430)	loss 1.2525 (1.3254)	grad_norm 7.7689 (5.1205)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:41:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][1900/2502]	eta 0:04:24 lr 0.000027	 wd 0.0000	time 0.3397 (0.4392)	loss 1.4757 (1.3255)	grad_norm 5.6493 (5.1154)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:42:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][2000/2502]	eta 0:03:41 lr 0.000027	 wd 0.0000	time 0.3346 (0.4414)	loss 0.9813 (1.3271)	grad_norm 4.5182 (5.1013)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:43:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][2100/2502]	eta 0:02:57 lr 0.000027	 wd 0.0000	time 0.4301 (0.4415)	loss 1.4529 (1.3271)	grad_norm 5.0086 (5.1164)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:44:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][2200/2502]	eta 0:02:16 lr 0.000027	 wd 0.0000	time 0.5481 (0.4533)	loss 1.1518 (1.3273)	grad_norm 3.4385 (5.1181)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:44:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][2300/2502]	eta 0:01:31 lr 0.000027	 wd 0.0000	time 0.4196 (0.4532)	loss 1.4073 (1.3282)	grad_norm 5.0990 (5.1173)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:46:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][2400/2502]	eta 0:00:47 lr 0.000026	 wd 0.0000	time 0.5289 (0.4626)	loss 1.1761 (1.3285)	grad_norm 4.4066 (5.1051)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:46:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.3442 (0.4596)	loss 1.3048 (1.3279)	grad_norm 4.9695 (5.1101)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:47:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 14 training takes 0:19:31
[2024-05-28 00:48:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 60.766 (60.766)	Loss 0.4062 (0.4062)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 00:48:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.654 Acc@5 97.412
[2024-05-28 00:48:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-05-28 00:48:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.65%
[2024-05-28 00:48:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 00:48:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 00:48:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][0/2502]	eta 13:51:35 lr 0.000026	 wd 0.0000	time 19.9421 (19.9421)	loss 1.3064 (1.3064)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:49:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][100/2502]	eta 0:22:21 lr 0.000026	 wd 0.0000	time 0.3569 (0.5583)	loss 0.9572 (1.3271)	grad_norm 3.8225 (5.6214)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:50:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][200/2502]	eta 0:17:54 lr 0.000026	 wd 0.0000	time 0.3323 (0.4667)	loss 1.5587 (1.3372)	grad_norm 4.8450 (5.2740)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 00:50:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][300/2502]	eta 0:15:54 lr 0.000026	 wd 0.0000	time 0.3475 (0.4336)	loss 1.4410 (1.3226)	grad_norm 3.7172 (5.2835)	loss_scale 2048.0000 (1289.3555)	mem 12561MB
[2024-05-28 00:51:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][400/2502]	eta 0:14:33 lr 0.000026	 wd 0.0000	time 0.3718 (0.4155)	loss 1.3927 (1.3256)	grad_norm 4.4939 (5.2120)	loss_scale 2048.0000 (1478.5436)	mem 12561MB
[2024-05-28 00:51:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][500/2502]	eta 0:13:34 lr 0.000026	 wd 0.0000	time 0.4033 (0.4069)	loss 0.8089 (1.3192)	grad_norm 3.8186 (5.2680)	loss_scale 2048.0000 (1592.2076)	mem 12561MB
[2024-05-28 00:52:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][600/2502]	eta 0:12:40 lr 0.000026	 wd 0.0000	time 0.3634 (0.4000)	loss 0.8269 (1.3201)	grad_norm 4.4619 (5.2794)	loss_scale 2048.0000 (1668.0466)	mem 12561MB
[2024-05-28 00:53:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][700/2502]	eta 0:11:52 lr 0.000026	 wd 0.0000	time 0.3494 (0.3955)	loss 1.1644 (1.3217)	grad_norm 4.0637 (5.2249)	loss_scale 2048.0000 (1722.2482)	mem 12561MB
[2024-05-28 00:53:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][800/2502]	eta 0:11:09 lr 0.000026	 wd 0.0000	time 0.3641 (0.3932)	loss 1.4369 (1.3225)	grad_norm 5.6787 (5.1771)	loss_scale 2048.0000 (1762.9164)	mem 12561MB
[2024-05-28 00:54:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][900/2502]	eta 0:10:25 lr 0.000025	 wd 0.0000	time 0.3453 (0.3903)	loss 1.0644 (1.3232)	grad_norm 4.5626 (5.2117)	loss_scale 2048.0000 (1794.5572)	mem 12561MB
[2024-05-28 00:54:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1000/2502]	eta 0:09:42 lr 0.000025	 wd 0.0000	time 0.3752 (0.3875)	loss 1.3542 (1.3246)	grad_norm 3.1677 (5.1785)	loss_scale 2048.0000 (1819.8761)	mem 12561MB
[2024-05-28 00:55:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1100/2502]	eta 0:09:15 lr 0.000025	 wd 0.0000	time 0.3642 (0.3959)	loss 1.4022 (1.3213)	grad_norm 3.8995 (5.1614)	loss_scale 2048.0000 (1840.5958)	mem 12561MB
[2024-05-28 00:56:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1200/2502]	eta 0:08:43 lr 0.000025	 wd 0.0000	time 0.4639 (0.4019)	loss 1.4418 (1.3210)	grad_norm 3.8739 (5.1438)	loss_scale 2048.0000 (1857.8651)	mem 12561MB
[2024-05-28 00:58:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1300/2502]	eta 0:08:55 lr 0.000025	 wd 0.0000	time 0.5462 (0.4457)	loss 1.2397 (1.3196)	grad_norm 3.7511 (inf)	loss_scale 1024.0000 (1804.7902)	mem 12561MB
[2024-05-28 00:58:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1400/2502]	eta 0:08:14 lr 0.000025	 wd 0.0000	time 0.3599 (0.4487)	loss 1.4258 (1.3201)	grad_norm 3.5590 (inf)	loss_scale 1024.0000 (1749.0592)	mem 12561MB
[2024-05-28 00:59:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1500/2502]	eta 0:07:23 lr 0.000025	 wd 0.0000	time 0.3259 (0.4430)	loss 1.7058 (1.3191)	grad_norm 6.1569 (inf)	loss_scale 1024.0000 (1700.7542)	mem 12561MB
[2024-05-28 01:00:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1600/2502]	eta 0:06:35 lr 0.000025	 wd 0.0000	time 0.3420 (0.4384)	loss 1.2903 (1.3207)	grad_norm 3.3290 (inf)	loss_scale 1024.0000 (1658.4834)	mem 12561MB
[2024-05-28 01:00:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1700/2502]	eta 0:05:48 lr 0.000025	 wd 0.0000	time 0.3693 (0.4344)	loss 1.4234 (1.3216)	grad_norm 7.7570 (inf)	loss_scale 1024.0000 (1621.1828)	mem 12561MB
[2024-05-28 01:01:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1800/2502]	eta 0:05:02 lr 0.000025	 wd 0.0000	time 0.3474 (0.4309)	loss 0.9553 (1.3211)	grad_norm 4.1243 (inf)	loss_scale 1024.0000 (1588.0244)	mem 12561MB
[2024-05-28 01:02:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][1900/2502]	eta 0:04:17 lr 0.000024	 wd 0.0000	time 0.3119 (0.4284)	loss 1.4745 (1.3218)	grad_norm 5.1375 (inf)	loss_scale 1024.0000 (1558.3546)	mem 12561MB
[2024-05-28 01:02:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][2000/2502]	eta 0:03:33 lr 0.000024	 wd 0.0000	time 0.3368 (0.4253)	loss 1.8324 (1.3242)	grad_norm 4.6637 (inf)	loss_scale 1024.0000 (1531.6502)	mem 12561MB
[2024-05-28 01:03:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][2100/2502]	eta 0:02:51 lr 0.000024	 wd 0.0000	time 0.3963 (0.4257)	loss 1.3634 (1.3247)	grad_norm 7.5594 (inf)	loss_scale 1024.0000 (1507.4879)	mem 12561MB
[2024-05-28 01:04:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][2200/2502]	eta 0:02:09 lr 0.000024	 wd 0.0000	time 0.4383 (0.4280)	loss 0.9941 (1.3261)	grad_norm 4.6932 (inf)	loss_scale 1024.0000 (1485.5211)	mem 12561MB
[2024-05-28 01:05:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][2300/2502]	eta 0:01:27 lr 0.000024	 wd 0.0000	time 0.5128 (0.4345)	loss 1.5381 (1.3266)	grad_norm 3.7539 (inf)	loss_scale 1024.0000 (1465.4637)	mem 12561MB
[2024-05-28 01:06:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][2400/2502]	eta 0:00:44 lr 0.000024	 wd 0.0000	time 0.4158 (0.4402)	loss 1.0515 (1.3255)	grad_norm 3.8387 (inf)	loss_scale 1024.0000 (1447.0771)	mem 12561MB
[2024-05-28 01:06:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.3238 (0.4380)	loss 1.3608 (1.3253)	grad_norm 4.9910 (inf)	loss_scale 1024.0000 (1430.1607)	mem 12561MB
[2024-05-28 01:06:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 15 training takes 0:18:29
[2024-05-28 01:07:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 55.605 (55.605)	Loss 0.3948 (0.3948)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 01:08:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.812 Acc@5 97.478
[2024-05-28 01:08:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-05-28 01:08:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.81%
[2024-05-28 01:08:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 01:08:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 01:08:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][0/2502]	eta 11:40:11 lr 0.000024	 wd 0.0000	time 16.7910 (16.7910)	loss 1.3084 (1.3084)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:09:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][100/2502]	eta 0:21:15 lr 0.000024	 wd 0.0000	time 0.3340 (0.5308)	loss 1.5021 (1.3161)	grad_norm 3.9246 (4.8766)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:09:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][200/2502]	eta 0:17:21 lr 0.000024	 wd 0.0000	time 0.3510 (0.4522)	loss 1.2216 (1.3233)	grad_norm 4.4952 (4.8308)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:10:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][300/2502]	eta 0:15:36 lr 0.000024	 wd 0.0000	time 0.3333 (0.4253)	loss 1.2639 (1.3341)	grad_norm 5.3570 (5.1000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:10:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][400/2502]	eta 0:14:20 lr 0.000024	 wd 0.0000	time 0.4147 (0.4095)	loss 1.0929 (1.3288)	grad_norm 3.4206 (5.0235)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:11:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][500/2502]	eta 0:13:24 lr 0.000023	 wd 0.0000	time 0.3391 (0.4017)	loss 1.3405 (1.3390)	grad_norm 3.9781 (5.0247)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:12:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][600/2502]	eta 0:12:33 lr 0.000023	 wd 0.0000	time 0.3848 (0.3961)	loss 0.9643 (1.3314)	grad_norm 4.3344 (5.0297)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:12:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][700/2502]	eta 0:11:45 lr 0.000023	 wd 0.0000	time 0.3205 (0.3915)	loss 1.4298 (1.3319)	grad_norm 3.1979 (5.0436)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:13:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][800/2502]	eta 0:11:03 lr 0.000023	 wd 0.0000	time 0.3160 (0.3898)	loss 1.2711 (1.3274)	grad_norm 3.2497 (5.0097)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:14:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][900/2502]	eta 0:10:20 lr 0.000023	 wd 0.0000	time 0.3628 (0.3872)	loss 1.3702 (1.3260)	grad_norm 3.9348 (5.0116)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:14:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1000/2502]	eta 0:09:38 lr 0.000023	 wd 0.0000	time 0.3480 (0.3851)	loss 1.3543 (1.3256)	grad_norm 5.4342 (4.9905)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:15:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1100/2502]	eta 0:08:58 lr 0.000023	 wd 0.0000	time 0.3555 (0.3841)	loss 1.3925 (1.3273)	grad_norm 5.2408 (4.9816)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:15:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1200/2502]	eta 0:08:18 lr 0.000023	 wd 0.0000	time 0.3196 (0.3826)	loss 1.4122 (1.3265)	grad_norm 6.2055 (4.9605)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:16:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1300/2502]	eta 0:07:47 lr 0.000023	 wd 0.0000	time 0.5083 (0.3890)	loss 1.0807 (1.3264)	grad_norm 4.3385 (4.9178)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:17:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1400/2502]	eta 0:07:13 lr 0.000023	 wd 0.0000	time 0.4750 (0.3931)	loss 1.3447 (1.3248)	grad_norm 6.2526 (4.9707)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:18:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1500/2502]	eta 0:06:40 lr 0.000022	 wd 0.0000	time 0.4103 (0.3998)	loss 1.3950 (1.3224)	grad_norm 7.4023 (4.9683)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:19:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1600/2502]	eta 0:06:07 lr 0.000022	 wd 0.0000	time 0.3656 (0.4071)	loss 1.4320 (1.3223)	grad_norm 4.0468 (4.9450)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:20:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1700/2502]	eta 0:05:34 lr 0.000022	 wd 0.0000	time 0.3919 (0.4171)	loss 1.0909 (1.3204)	grad_norm 5.0485 (4.8993)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:20:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1800/2502]	eta 0:04:53 lr 0.000022	 wd 0.0000	time 0.3934 (0.4187)	loss 1.2301 (1.3200)	grad_norm 4.2049 (4.9004)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:21:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][1900/2502]	eta 0:04:16 lr 0.000022	 wd 0.0000	time 0.3592 (0.4258)	loss 1.2871 (1.3190)	grad_norm 3.9766 (4.8787)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:22:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][2000/2502]	eta 0:03:34 lr 0.000022	 wd 0.0000	time 0.3952 (0.4266)	loss 1.5011 (1.3200)	grad_norm 4.3441 (4.8654)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:23:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][2100/2502]	eta 0:02:55 lr 0.000022	 wd 0.0000	time 0.3255 (0.4368)	loss 1.3624 (1.3201)	grad_norm 4.4135 (4.8919)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:24:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][2200/2502]	eta 0:02:12 lr 0.000022	 wd 0.0000	time 0.4773 (0.4375)	loss 1.3210 (1.3206)	grad_norm 4.2266 (4.8823)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:25:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][2300/2502]	eta 0:01:28 lr 0.000022	 wd 0.0000	time 0.3561 (0.4402)	loss 1.2757 (1.3209)	grad_norm 4.9579 (4.8818)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:25:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][2400/2502]	eta 0:00:44 lr 0.000022	 wd 0.0000	time 0.4321 (0.4405)	loss 1.3945 (1.3208)	grad_norm 5.6083 (4.8931)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:26:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.3263 (0.4382)	loss 1.3380 (1.3210)	grad_norm 3.3717 (4.8965)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:26:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 16 training takes 0:18:33
[2024-05-28 01:27:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 55.802 (55.802)	Loss 0.4031 (0.4031)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-28 01:28:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.818 Acc@5 97.464
[2024-05-28 01:28:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-05-28 01:28:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.82%
[2024-05-28 01:28:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 01:28:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 01:28:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][0/2502]	eta 9:55:04 lr 0.000021	 wd 0.0000	time 14.2706 (14.2706)	loss 1.5195 (1.5195)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:28:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][100/2502]	eta 0:20:04 lr 0.000021	 wd 0.0000	time 0.3496 (0.5014)	loss 1.5328 (1.2986)	grad_norm 5.4347 (5.0899)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:29:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][200/2502]	eta 0:16:51 lr 0.000021	 wd 0.0000	time 0.3441 (0.4395)	loss 1.5730 (1.3152)	grad_norm 5.9472 (4.9034)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:30:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][300/2502]	eta 0:15:09 lr 0.000021	 wd 0.0000	time 0.2851 (0.4130)	loss 1.2220 (1.3110)	grad_norm 5.0776 (inf)	loss_scale 1024.0000 (1037.6080)	mem 12561MB
[2024-05-28 01:30:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][400/2502]	eta 0:14:00 lr 0.000021	 wd 0.0000	time 0.3145 (0.4000)	loss 1.0420 (1.3080)	grad_norm 3.7207 (inf)	loss_scale 1024.0000 (1034.2145)	mem 12561MB
[2024-05-28 01:31:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][500/2502]	eta 0:13:07 lr 0.000021	 wd 0.0000	time 0.3876 (0.3932)	loss 1.2356 (1.3048)	grad_norm 3.6094 (inf)	loss_scale 1024.0000 (1032.1756)	mem 12561MB
[2024-05-28 01:32:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][600/2502]	eta 0:12:19 lr 0.000021	 wd 0.0000	time 0.3522 (0.3887)	loss 1.1358 (1.3098)	grad_norm 5.3091 (inf)	loss_scale 1024.0000 (1030.8153)	mem 12561MB
[2024-05-28 01:32:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][700/2502]	eta 0:11:34 lr 0.000021	 wd 0.0000	time 0.3364 (0.3854)	loss 1.4545 (1.3154)	grad_norm 7.7892 (inf)	loss_scale 1024.0000 (1029.8431)	mem 12561MB
[2024-05-28 01:33:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][800/2502]	eta 0:10:53 lr 0.000021	 wd 0.0000	time 0.3491 (0.3842)	loss 1.3096 (1.3188)	grad_norm 3.9816 (inf)	loss_scale 1024.0000 (1029.1136)	mem 12561MB
[2024-05-28 01:33:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][900/2502]	eta 0:10:11 lr 0.000021	 wd 0.0000	time 0.3292 (0.3818)	loss 1.4898 (1.3218)	grad_norm 4.1000 (inf)	loss_scale 1024.0000 (1028.5461)	mem 12561MB
[2024-05-28 01:34:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1000/2502]	eta 0:09:32 lr 0.000020	 wd 0.0000	time 0.4138 (0.3809)	loss 1.5354 (1.3201)	grad_norm 5.4195 (inf)	loss_scale 1024.0000 (1028.0919)	mem 12561MB
[2024-05-28 01:35:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1100/2502]	eta 0:08:52 lr 0.000020	 wd 0.0000	time 0.3526 (0.3798)	loss 1.0242 (1.3183)	grad_norm 4.2144 (inf)	loss_scale 1024.0000 (1027.7203)	mem 12561MB
[2024-05-28 01:35:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1200/2502]	eta 0:08:13 lr 0.000020	 wd 0.0000	time 0.3408 (0.3790)	loss 1.3622 (1.3205)	grad_norm 7.6324 (inf)	loss_scale 1024.0000 (1027.4105)	mem 12561MB
[2024-05-28 01:36:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1300/2502]	eta 0:07:35 lr 0.000020	 wd 0.0000	time 0.3261 (0.3791)	loss 1.2311 (1.3201)	grad_norm 6.0681 (inf)	loss_scale 1024.0000 (1027.1483)	mem 12561MB
[2024-05-28 01:36:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1400/2502]	eta 0:06:57 lr 0.000020	 wd 0.0000	time 0.3478 (0.3784)	loss 1.4549 (1.3198)	grad_norm 3.9691 (inf)	loss_scale 1024.0000 (1026.9236)	mem 12561MB
[2024-05-28 01:37:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1500/2502]	eta 0:06:18 lr 0.000020	 wd 0.0000	time 0.3713 (0.3779)	loss 1.3937 (1.3213)	grad_norm 4.2234 (inf)	loss_scale 1024.0000 (1026.7288)	mem 12561MB
[2024-05-28 01:38:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1600/2502]	eta 0:05:47 lr 0.000020	 wd 0.0000	time 0.3851 (0.3848)	loss 1.4040 (1.3232)	grad_norm 3.6952 (inf)	loss_scale 1024.0000 (1026.5584)	mem 12561MB
[2024-05-28 01:39:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1700/2502]	eta 0:05:11 lr 0.000020	 wd 0.0000	time 0.3841 (0.3885)	loss 1.6181 (1.3246)	grad_norm 4.6268 (inf)	loss_scale 1024.0000 (1026.4080)	mem 12561MB
[2024-05-28 01:40:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1800/2502]	eta 0:04:38 lr 0.000020	 wd 0.0000	time 0.3521 (0.3967)	loss 1.3494 (1.3244)	grad_norm 4.3811 (inf)	loss_scale 1024.0000 (1026.2743)	mem 12561MB
[2024-05-28 01:40:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][1900/2502]	eta 0:04:00 lr 0.000020	 wd 0.0000	time 0.3628 (0.3994)	loss 1.2188 (1.3242)	grad_norm 3.6732 (inf)	loss_scale 1024.0000 (1026.1547)	mem 12561MB
[2024-05-28 01:41:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][2000/2502]	eta 0:03:23 lr 0.000019	 wd 0.0000	time 0.3543 (0.4058)	loss 1.4681 (1.3226)	grad_norm 4.8063 (inf)	loss_scale 1024.0000 (1026.0470)	mem 12561MB
[2024-05-28 01:42:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][2100/2502]	eta 0:02:44 lr 0.000019	 wd 0.0000	time 0.4283 (0.4087)	loss 1.3588 (1.3220)	grad_norm 4.9522 (inf)	loss_scale 1024.0000 (1025.9495)	mem 12561MB
[2024-05-28 01:43:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][2200/2502]	eta 0:02:04 lr 0.000019	 wd 0.0000	time 0.3880 (0.4127)	loss 1.2523 (1.3227)	grad_norm 3.4399 (inf)	loss_scale 1024.0000 (1025.8610)	mem 12561MB
[2024-05-28 01:44:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][2300/2502]	eta 0:01:23 lr 0.000019	 wd 0.0000	time 0.4016 (0.4141)	loss 1.4858 (1.3222)	grad_norm 3.4153 (inf)	loss_scale 1024.0000 (1025.7801)	mem 12561MB
[2024-05-28 01:44:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][2400/2502]	eta 0:00:42 lr 0.000019	 wd 0.0000	time 0.3345 (0.4193)	loss 1.3673 (1.3210)	grad_norm 3.4892 (inf)	loss_scale 1024.0000 (1025.7060)	mem 12561MB
[2024-05-28 01:45:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0000	time 0.3060 (0.4183)	loss 1.3821 (1.3216)	grad_norm 3.9548 (inf)	loss_scale 1024.0000 (1025.6377)	mem 12561MB
[2024-05-28 01:45:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 17 training takes 0:17:33
[2024-05-28 01:46:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 75.537 (75.537)	Loss 0.4023 (0.4023)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-28 01:47:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.896 Acc@5 97.432
[2024-05-28 01:47:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-05-28 01:47:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.90%
[2024-05-28 01:47:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 01:47:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 01:47:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][0/2502]	eta 21:17:59 lr 0.000019	 wd 0.0000	time 30.6473 (30.6473)	loss 1.5428 (1.5428)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:48:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][100/2502]	eta 0:26:31 lr 0.000019	 wd 0.0000	time 0.3355 (0.6625)	loss 1.6534 (1.3625)	grad_norm 4.4162 (4.6914)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:48:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][200/2502]	eta 0:19:39 lr 0.000019	 wd 0.0000	time 0.3280 (0.5123)	loss 1.5727 (1.3364)	grad_norm 3.4282 (4.8325)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:49:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][300/2502]	eta 0:17:07 lr 0.000019	 wd 0.0000	time 0.3132 (0.4664)	loss 1.4948 (1.3323)	grad_norm 3.6487 (4.7747)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:50:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][400/2502]	eta 0:15:25 lr 0.000019	 wd 0.0000	time 0.3541 (0.4402)	loss 1.3592 (1.3272)	grad_norm 4.9595 (4.8455)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:50:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][500/2502]	eta 0:14:10 lr 0.000018	 wd 0.0000	time 0.3338 (0.4250)	loss 1.2395 (1.3189)	grad_norm 5.3001 (4.8615)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:51:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][600/2502]	eta 0:13:11 lr 0.000018	 wd 0.0000	time 0.3555 (0.4162)	loss 1.6764 (1.3187)	grad_norm 3.8158 (4.9282)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:52:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][700/2502]	eta 0:12:17 lr 0.000018	 wd 0.0000	time 0.3098 (0.4090)	loss 1.3632 (1.3187)	grad_norm 8.6895 (5.0070)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:52:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][800/2502]	eta 0:11:29 lr 0.000018	 wd 0.0000	time 0.3932 (0.4053)	loss 1.0868 (1.3159)	grad_norm 5.0222 (5.0957)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:53:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][900/2502]	eta 0:10:42 lr 0.000018	 wd 0.0000	time 0.3608 (0.4011)	loss 1.1730 (1.3127)	grad_norm 4.7417 (5.0527)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:53:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1000/2502]	eta 0:09:56 lr 0.000018	 wd 0.0000	time 0.3198 (0.3972)	loss 1.0089 (1.3153)	grad_norm 4.2862 (5.0427)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:54:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1100/2502]	eta 0:09:14 lr 0.000018	 wd 0.0000	time 0.3225 (0.3954)	loss 1.3441 (1.3145)	grad_norm 3.2791 (5.0042)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:55:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1200/2502]	eta 0:08:31 lr 0.000018	 wd 0.0000	time 0.3812 (0.3931)	loss 1.5222 (1.3157)	grad_norm 3.4456 (5.0458)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:55:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1300/2502]	eta 0:07:49 lr 0.000018	 wd 0.0000	time 0.3795 (0.3909)	loss 1.5858 (1.3173)	grad_norm 9.9447 (5.0225)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:56:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1400/2502]	eta 0:07:16 lr 0.000018	 wd 0.0000	time 0.3242 (0.3960)	loss 1.2409 (1.3173)	grad_norm 5.1990 (5.0357)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:57:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1500/2502]	eta 0:06:41 lr 0.000017	 wd 0.0000	time 0.3578 (0.4004)	loss 1.4829 (1.3170)	grad_norm 3.7235 (5.0092)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:58:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1600/2502]	eta 0:06:16 lr 0.000017	 wd 0.0000	time 0.4785 (0.4172)	loss 1.4499 (1.3171)	grad_norm 8.7253 (5.0808)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 01:59:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1700/2502]	eta 0:05:36 lr 0.000017	 wd 0.0000	time 0.6176 (0.4191)	loss 1.4961 (1.3165)	grad_norm 3.9305 (5.1140)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 02:00:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1800/2502]	eta 0:05:00 lr 0.000017	 wd 0.0000	time 0.4994 (0.4284)	loss 1.3212 (1.3175)	grad_norm 4.2946 (5.0975)	loss_scale 2048.0000 (1074.0344)	mem 12561MB
[2024-05-28 02:00:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][1900/2502]	eta 0:04:18 lr 0.000017	 wd 0.0000	time 0.5373 (0.4297)	loss 1.4731 (1.3169)	grad_norm 2.9486 (5.1090)	loss_scale 2048.0000 (1125.2688)	mem 12561MB
[2024-05-28 02:01:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][2000/2502]	eta 0:03:40 lr 0.000017	 wd 0.0000	time 0.3675 (0.4394)	loss 1.3210 (1.3189)	grad_norm 4.4504 (5.1154)	loss_scale 2048.0000 (1171.3823)	mem 12561MB
[2024-05-28 02:02:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][2100/2502]	eta 0:02:58 lr 0.000017	 wd 0.0000	time 0.4525 (0.4437)	loss 1.4713 (1.3179)	grad_norm 3.2471 (5.1245)	loss_scale 2048.0000 (1213.1061)	mem 12561MB
[2024-05-28 02:03:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][2200/2502]	eta 0:02:14 lr 0.000017	 wd 0.0000	time 0.4068 (0.4440)	loss 1.4240 (1.3184)	grad_norm 4.4016 (5.1288)	loss_scale 2048.0000 (1251.0386)	mem 12561MB
[2024-05-28 02:04:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][2300/2502]	eta 0:01:30 lr 0.000017	 wd 0.0000	time 0.4164 (0.4477)	loss 1.3171 (1.3181)	grad_norm 3.9540 (5.1525)	loss_scale 2048.0000 (1285.6741)	mem 12561MB
[2024-05-28 02:05:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][2400/2502]	eta 0:00:45 lr 0.000017	 wd 0.0000	time 0.4999 (0.4480)	loss 1.5292 (1.3196)	grad_norm 5.2340 (5.1363)	loss_scale 2048.0000 (1317.4244)	mem 12561MB
[2024-05-28 02:05:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.3294 (0.4455)	loss 1.4316 (1.3190)	grad_norm 3.4875 (5.1278)	loss_scale 2048.0000 (1346.6357)	mem 12561MB
[2024-05-28 02:05:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 18 training takes 0:18:42
[2024-05-28 02:06:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 38.692 (38.692)	Loss 0.3945 (0.3945)	Acc@1 91.602 (91.602)	Acc@5 98.633 (98.633)	Mem 12561MB
[2024-05-28 02:06:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.812 Acc@5 97.444
[2024-05-28 02:06:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-05-28 02:06:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.90%
[2024-05-28 02:07:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][0/2502]	eta 10:32:00 lr 0.000016	 wd 0.0000	time 15.1560 (15.1560)	loss 1.4022 (1.4022)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-28 02:07:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][100/2502]	eta 0:21:59 lr 0.000016	 wd 0.0000	time 0.3316 (0.5493)	loss 1.0521 (1.3170)	grad_norm 4.8005 (5.6343)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-28 02:08:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][200/2502]	eta 0:17:29 lr 0.000016	 wd 0.0000	time 0.3311 (0.4559)	loss 1.4786 (1.3158)	grad_norm 4.0692 (5.3255)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-28 02:09:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][300/2502]	eta 0:15:44 lr 0.000016	 wd 0.0000	time 0.3423 (0.4289)	loss 1.2248 (1.3289)	grad_norm 4.3069 (5.3811)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-28 02:09:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][400/2502]	eta 0:14:28 lr 0.000016	 wd 0.0000	time 0.3347 (0.4133)	loss 1.3606 (1.3276)	grad_norm 3.9398 (5.3464)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-28 02:10:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][500/2502]	eta 0:13:25 lr 0.000016	 wd 0.0000	time 0.3208 (0.4024)	loss 1.4534 (1.3250)	grad_norm 8.5287 (5.4123)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-28 02:10:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][600/2502]	eta 0:12:36 lr 0.000016	 wd 0.0000	time 0.3379 (0.3976)	loss 1.3953 (1.3202)	grad_norm 3.5119 (5.3355)	loss_scale 2048.0000 (2048.0000)	mem 12561MB
[2024-05-28 02:11:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][700/2502]	eta 0:11:48 lr 0.000016	 wd 0.0000	time 0.3032 (0.3932)	loss 1.6055 (1.3224)	grad_norm 4.1451 (inf)	loss_scale 1024.0000 (2021.7061)	mem 12561MB
[2024-05-28 02:12:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][800/2502]	eta 0:11:03 lr 0.000016	 wd 0.0000	time 0.3376 (0.3897)	loss 1.2689 (1.3206)	grad_norm 4.9514 (inf)	loss_scale 1024.0000 (1897.1486)	mem 12561MB
[2024-05-28 02:12:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][900/2502]	eta 0:10:21 lr 0.000016	 wd 0.0000	time 0.3463 (0.3881)	loss 1.2593 (1.3213)	grad_norm 3.7909 (inf)	loss_scale 1024.0000 (1800.2397)	mem 12561MB
[2024-05-28 02:13:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1000/2502]	eta 0:09:39 lr 0.000016	 wd 0.0000	time 0.3162 (0.3857)	loss 1.3538 (1.3227)	grad_norm 4.9088 (inf)	loss_scale 1024.0000 (1722.6933)	mem 12561MB
[2024-05-28 02:13:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1100/2502]	eta 0:08:59 lr 0.000015	 wd 0.0000	time 0.3674 (0.3850)	loss 0.7570 (1.3217)	grad_norm 3.4920 (inf)	loss_scale 1024.0000 (1659.2334)	mem 12561MB
[2024-05-28 02:14:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1200/2502]	eta 0:08:37 lr 0.000015	 wd 0.0000	time 0.4977 (0.3972)	loss 1.1126 (1.3195)	grad_norm 6.2260 (inf)	loss_scale 1024.0000 (1606.3414)	mem 12561MB
[2024-05-28 02:15:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1300/2502]	eta 0:08:07 lr 0.000015	 wd 0.0000	time 3.1768 (0.4052)	loss 1.4021 (1.3199)	grad_norm 9.6664 (inf)	loss_scale 1024.0000 (1561.5803)	mem 12561MB
[2024-05-28 02:16:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1400/2502]	eta 0:07:35 lr 0.000015	 wd 0.0000	time 0.4385 (0.4135)	loss 1.4291 (1.3206)	grad_norm 4.6227 (inf)	loss_scale 1024.0000 (1523.2091)	mem 12561MB
[2024-05-28 02:17:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1500/2502]	eta 0:07:04 lr 0.000015	 wd 0.0000	time 0.4255 (0.4236)	loss 1.1196 (1.3207)	grad_norm 3.8161 (inf)	loss_scale 1024.0000 (1489.9507)	mem 12561MB
[2024-05-28 02:18:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1600/2502]	eta 0:06:28 lr 0.000015	 wd 0.0000	time 0.3475 (0.4309)	loss 1.2766 (1.3209)	grad_norm 4.1168 (inf)	loss_scale 1024.0000 (1460.8470)	mem 12561MB
[2024-05-28 02:19:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1700/2502]	eta 0:05:49 lr 0.000015	 wd 0.0000	time 0.3728 (0.4356)	loss 1.4592 (1.3215)	grad_norm 5.1729 (inf)	loss_scale 1024.0000 (1435.1652)	mem 12561MB
[2024-05-28 02:19:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1800/2502]	eta 0:05:06 lr 0.000015	 wd 0.0000	time 0.3957 (0.4367)	loss 1.2485 (1.3202)	grad_norm 6.1740 (inf)	loss_scale 1024.0000 (1412.3354)	mem 12561MB
[2024-05-28 02:20:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][1900/2502]	eta 0:04:24 lr 0.000015	 wd 0.0000	time 0.3818 (0.4400)	loss 1.3589 (1.3213)	grad_norm 3.2826 (nan)	loss_scale 512.0000 (1365.5129)	mem 12561MB
[2024-05-28 02:21:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][2000/2502]	eta 0:03:41 lr 0.000015	 wd 0.0000	time 0.3484 (0.4403)	loss 0.9769 (1.3205)	grad_norm 5.4763 (nan)	loss_scale 512.0000 (1322.8586)	mem 12561MB
[2024-05-28 02:22:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][2100/2502]	eta 0:03:01 lr 0.000014	 wd 0.0000	time 0.4296 (0.4519)	loss 1.1238 (1.3191)	grad_norm 3.6193 (nan)	loss_scale 512.0000 (1284.2646)	mem 12561MB
[2024-05-28 02:23:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][2200/2502]	eta 0:02:16 lr 0.000014	 wd 0.0000	time 0.3653 (0.4523)	loss 1.5877 (1.3174)	grad_norm 3.9432 (nan)	loss_scale 512.0000 (1249.1776)	mem 12561MB
[2024-05-28 02:24:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][2300/2502]	eta 0:01:32 lr 0.000014	 wd 0.0000	time 0.4090 (0.4568)	loss 1.3090 (1.3172)	grad_norm 4.4000 (nan)	loss_scale 512.0000 (1217.1404)	mem 12561MB
[2024-05-28 02:25:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][2400/2502]	eta 0:00:46 lr 0.000014	 wd 0.0000	time 0.3612 (0.4567)	loss 1.6292 (1.3182)	grad_norm 3.3544 (nan)	loss_scale 512.0000 (1187.7718)	mem 12561MB
[2024-05-28 02:25:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.3212 (0.4558)	loss 1.2311 (1.3176)	grad_norm 4.4555 (nan)	loss_scale 512.0000 (1160.7517)	mem 12561MB
[2024-05-28 02:26:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 19 training takes 0:19:14
[2024-05-28 02:26:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 50.457 (50.457)	Loss 0.3906 (0.3906)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 02:27:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.852 Acc@5 97.484
[2024-05-28 02:27:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-05-28 02:27:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.90%
[2024-05-28 02:27:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][0/2502]	eta 10:42:07 lr 0.000014	 wd 0.0000	time 15.3988 (15.3988)	loss 0.9455 (0.9455)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:28:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][100/2502]	eta 0:20:26 lr 0.000014	 wd 0.0000	time 0.3173 (0.5107)	loss 1.5037 (1.3048)	grad_norm 3.6910 (5.1164)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:28:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][200/2502]	eta 0:17:04 lr 0.000014	 wd 0.0000	time 0.3572 (0.4452)	loss 1.3812 (1.3112)	grad_norm 3.5759 (5.0445)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:29:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][300/2502]	eta 0:15:19 lr 0.000014	 wd 0.0000	time 0.3403 (0.4178)	loss 1.3555 (1.3106)	grad_norm 5.0181 (4.9336)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:30:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][400/2502]	eta 0:14:08 lr 0.000014	 wd 0.0000	time 0.3178 (0.4036)	loss 1.3473 (1.3232)	grad_norm 3.6293 (4.9362)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:30:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][500/2502]	eta 0:13:13 lr 0.000014	 wd 0.0000	time 0.3340 (0.3964)	loss 1.4998 (1.3267)	grad_norm 5.6263 (4.9713)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:31:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][600/2502]	eta 0:12:23 lr 0.000014	 wd 0.0000	time 0.3578 (0.3911)	loss 1.3177 (1.3235)	grad_norm 5.0620 (5.0787)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:31:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][700/2502]	eta 0:11:39 lr 0.000013	 wd 0.0000	time 0.3308 (0.3881)	loss 1.3124 (1.3265)	grad_norm 4.3043 (5.1440)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:32:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][800/2502]	eta 0:10:56 lr 0.000013	 wd 0.0000	time 0.3568 (0.3857)	loss 1.1496 (1.3220)	grad_norm 4.5457 (5.1917)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:33:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][900/2502]	eta 0:10:14 lr 0.000013	 wd 0.0000	time 0.3679 (0.3833)	loss 1.3807 (1.3253)	grad_norm 3.5758 (5.2457)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:33:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1000/2502]	eta 0:09:48 lr 0.000013	 wd 0.0000	time 0.5155 (0.3920)	loss 1.3013 (1.3250)	grad_norm 5.3430 (5.2288)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:34:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1100/2502]	eta 0:09:18 lr 0.000013	 wd 0.0000	time 0.3837 (0.3985)	loss 1.4142 (1.3224)	grad_norm 4.3239 (5.2143)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:35:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1200/2502]	eta 0:08:56 lr 0.000013	 wd 0.0000	time 0.4780 (0.4118)	loss 1.3065 (1.3184)	grad_norm 4.6756 (5.2038)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:36:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1300/2502]	eta 0:08:21 lr 0.000013	 wd 0.0000	time 0.4533 (0.4175)	loss 1.1440 (1.3164)	grad_norm 5.9870 (5.2133)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:37:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1400/2502]	eta 0:07:48 lr 0.000013	 wd 0.0000	time 0.3608 (0.4255)	loss 1.4811 (1.3167)	grad_norm 4.2271 (5.2187)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:38:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1500/2502]	eta 0:07:07 lr 0.000013	 wd 0.0000	time 0.4282 (0.4271)	loss 1.3163 (1.3171)	grad_norm 6.6601 (5.2547)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:39:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1600/2502]	eta 0:06:37 lr 0.000013	 wd 0.0000	time 0.3433 (0.4407)	loss 1.6324 (1.3169)	grad_norm 4.1614 (5.2535)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:39:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1700/2502]	eta 0:05:54 lr 0.000012	 wd 0.0000	time 0.3306 (0.4419)	loss 1.5055 (1.3159)	grad_norm 10.3157 (5.2467)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:40:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1800/2502]	eta 0:05:14 lr 0.000012	 wd 0.0000	time 0.3239 (0.4474)	loss 1.2828 (1.3151)	grad_norm 4.7013 (5.2592)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:41:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][1900/2502]	eta 0:04:29 lr 0.000012	 wd 0.0000	time 0.4399 (0.4477)	loss 1.3174 (1.3154)	grad_norm 3.5436 (5.2460)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:42:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][2000/2502]	eta 0:03:46 lr 0.000012	 wd 0.0000	time 0.4182 (0.4516)	loss 1.3245 (1.3146)	grad_norm 5.1943 (5.2361)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:43:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][2100/2502]	eta 0:03:01 lr 0.000012	 wd 0.0000	time 0.3544 (0.4515)	loss 1.3116 (1.3148)	grad_norm 7.5271 (5.2194)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:44:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][2200/2502]	eta 0:02:18 lr 0.000012	 wd 0.0000	time 0.3685 (0.4580)	loss 1.0439 (1.3168)	grad_norm 17.6320 (5.2080)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:44:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][2300/2502]	eta 0:01:32 lr 0.000012	 wd 0.0000	time 0.4722 (0.4572)	loss 1.3457 (1.3169)	grad_norm 5.8935 (5.1940)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:45:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][2400/2502]	eta 0:00:46 lr 0.000012	 wd 0.0000	time 0.2973 (0.4598)	loss 1.3125 (1.3171)	grad_norm 4.8528 (5.2214)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:46:26 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0000	time 0.3366 (0.4574)	loss 1.4663 (1.3177)	grad_norm 4.0063 (5.2039)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:46:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 20 training takes 0:19:14
[2024-05-28 02:47:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 72.279 (72.279)	Loss 0.3945 (0.3945)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 12561MB
[2024-05-28 02:48:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.896 Acc@5 97.500
[2024-05-28 02:48:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-05-28 02:48:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.90%
[2024-05-28 02:48:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 02:48:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 02:48:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][0/2502]	eta 1 day, 0:27:32 lr 0.000012	 wd 0.0000	time 35.1928 (35.1928)	loss 1.4665 (1.4665)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:49:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][100/2502]	eta 0:28:15 lr 0.000012	 wd 0.0000	time 0.3816 (0.7061)	loss 1.4905 (1.3433)	grad_norm 4.3827 (5.6569)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:49:59 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][200/2502]	eta 0:20:45 lr 0.000012	 wd 0.0000	time 0.3173 (0.5412)	loss 0.7948 (1.3188)	grad_norm 4.7476 (5.6180)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:50:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][300/2502]	eta 0:17:42 lr 0.000012	 wd 0.0000	time 0.3493 (0.4825)	loss 1.3666 (1.3116)	grad_norm 15.1257 (5.5893)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:51:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][400/2502]	eta 0:15:50 lr 0.000011	 wd 0.0000	time 0.3330 (0.4523)	loss 1.4894 (1.3146)	grad_norm 4.7771 (5.4402)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:51:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][500/2502]	eta 0:14:34 lr 0.000011	 wd 0.0000	time 0.3537 (0.4370)	loss 1.3630 (1.3138)	grad_norm 3.7331 (5.5411)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:52:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][600/2502]	eta 0:13:29 lr 0.000011	 wd 0.0000	time 0.3600 (0.4258)	loss 1.2980 (1.3108)	grad_norm 6.1192 (5.5058)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:53:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][700/2502]	eta 0:12:31 lr 0.000011	 wd 0.0000	time 0.3258 (0.4170)	loss 1.1863 (1.3069)	grad_norm 4.0657 (5.5060)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 02:53:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][800/2502]	eta 0:11:42 lr 0.000011	 wd 0.0000	time 0.3412 (0.4126)	loss 1.1692 (1.3046)	grad_norm 3.7637 (5.3767)	loss_scale 1024.0000 (513.2784)	mem 12561MB
[2024-05-28 02:54:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][900/2502]	eta 0:10:52 lr 0.000011	 wd 0.0000	time 0.3600 (0.4073)	loss 1.6196 (1.3053)	grad_norm 4.1149 (5.3125)	loss_scale 1024.0000 (569.9623)	mem 12561MB
[2024-05-28 02:54:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1000/2502]	eta 0:10:05 lr 0.000011	 wd 0.0000	time 0.3763 (0.4035)	loss 0.9606 (1.3052)	grad_norm 3.8130 (5.2647)	loss_scale 1024.0000 (615.3207)	mem 12561MB
[2024-05-28 02:55:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1100/2502]	eta 0:09:21 lr 0.000011	 wd 0.0000	time 0.3599 (0.4008)	loss 1.5294 (1.3059)	grad_norm 3.8475 (5.2386)	loss_scale 1024.0000 (652.4396)	mem 12561MB
[2024-05-28 02:56:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1200/2502]	eta 0:08:37 lr 0.000011	 wd 0.0000	time 0.3502 (0.3978)	loss 1.4753 (1.3061)	grad_norm 5.3142 (5.2368)	loss_scale 1024.0000 (683.3772)	mem 12561MB
[2024-05-28 02:56:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1300/2502]	eta 0:07:59 lr 0.000011	 wd 0.0000	time 0.5476 (0.3985)	loss 1.4340 (1.3064)	grad_norm 4.7261 (5.2363)	loss_scale 1024.0000 (709.5588)	mem 12561MB
[2024-05-28 02:57:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1400/2502]	eta 0:07:29 lr 0.000011	 wd 0.0000	time 0.4648 (0.4078)	loss 1.4325 (1.3062)	grad_norm 4.3206 (5.2320)	loss_scale 1024.0000 (732.0029)	mem 12561MB
[2024-05-28 02:58:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1500/2502]	eta 0:06:56 lr 0.000010	 wd 0.0000	time 0.4673 (0.4158)	loss 1.5022 (1.3062)	grad_norm 7.5034 (5.2677)	loss_scale 1024.0000 (751.4564)	mem 12561MB
[2024-05-28 02:59:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1600/2502]	eta 0:06:21 lr 0.000010	 wd 0.0000	time 0.4043 (0.4232)	loss 1.2040 (1.3060)	grad_norm 4.1534 (5.2596)	loss_scale 1024.0000 (768.4797)	mem 12561MB
[2024-05-28 03:00:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1700/2502]	eta 0:05:45 lr 0.000010	 wd 0.0000	time 0.4598 (0.4307)	loss 1.4287 (1.3062)	grad_norm 7.0292 (5.2469)	loss_scale 1024.0000 (783.5015)	mem 12561MB
[2024-05-28 03:01:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1800/2502]	eta 0:05:03 lr 0.000010	 wd 0.0000	time 0.4187 (0.4319)	loss 1.0522 (1.3067)	grad_norm 4.2533 (5.2489)	loss_scale 1024.0000 (796.8551)	mem 12561MB
[2024-05-28 03:02:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][1900/2502]	eta 0:04:23 lr 0.000010	 wd 0.0000	time 0.4399 (0.4377)	loss 1.0105 (1.3049)	grad_norm 4.7219 (5.2116)	loss_scale 1024.0000 (808.8038)	mem 12561MB
[2024-05-28 03:02:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][2000/2502]	eta 0:03:39 lr 0.000010	 wd 0.0000	time 0.4331 (0.4380)	loss 1.4438 (1.3041)	grad_norm 5.4191 (5.2324)	loss_scale 1024.0000 (819.5582)	mem 12561MB
[2024-05-28 03:03:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][2100/2502]	eta 0:02:57 lr 0.000010	 wd 0.0000	time 0.3887 (0.4422)	loss 1.1170 (1.3047)	grad_norm 6.7999 (5.2800)	loss_scale 1024.0000 (829.2889)	mem 12561MB
[2024-05-28 03:04:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][2200/2502]	eta 0:02:13 lr 0.000010	 wd 0.0000	time 0.4141 (0.4421)	loss 1.4476 (1.3022)	grad_norm 4.2240 (5.2846)	loss_scale 1024.0000 (838.1354)	mem 12561MB
[2024-05-28 03:05:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][2300/2502]	eta 0:01:30 lr 0.000010	 wd 0.0000	time 0.4387 (0.4485)	loss 1.4238 (1.3017)	grad_norm 4.5350 (5.2853)	loss_scale 1024.0000 (846.2130)	mem 12561MB
[2024-05-28 03:06:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][2400/2502]	eta 0:00:45 lr 0.000010	 wd 0.0000	time 0.3259 (0.4487)	loss 1.4215 (1.3030)	grad_norm 4.3158 (5.3108)	loss_scale 1024.0000 (853.6177)	mem 12561MB
[2024-05-28 03:06:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.3314 (0.4461)	loss 1.4298 (1.3030)	grad_norm 4.1916 (5.2934)	loss_scale 1024.0000 (860.4302)	mem 12561MB
[2024-05-28 03:06:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 21 training takes 0:18:47
[2024-05-28 03:07:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 49.865 (49.865)	Loss 0.3911 (0.3911)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 03:08:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.932 Acc@5 97.522
[2024-05-28 03:08:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-05-28 03:08:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.93%
[2024-05-28 03:08:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 03:08:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 03:08:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][0/2502]	eta 17:05:35 lr 0.000010	 wd 0.0000	time 24.5945 (24.5945)	loss 1.2220 (1.2220)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:09:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][100/2502]	eta 0:24:18 lr 0.000010	 wd 0.0000	time 0.3007 (0.6070)	loss 1.2554 (1.3200)	grad_norm 3.7630 (5.3825)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:09:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][200/2502]	eta 0:18:52 lr 0.000009	 wd 0.0000	time 0.3388 (0.4919)	loss 1.0232 (1.3132)	grad_norm 11.2949 (5.0562)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:10:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][300/2502]	eta 0:16:29 lr 0.000009	 wd 0.0000	time 0.3526 (0.4492)	loss 1.4516 (1.3118)	grad_norm 7.3328 (5.0124)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:11:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][400/2502]	eta 0:14:58 lr 0.000009	 wd 0.0000	time 0.3427 (0.4275)	loss 1.3072 (1.3162)	grad_norm 4.6547 (5.0554)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:11:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][500/2502]	eta 0:13:55 lr 0.000009	 wd 0.0000	time 0.3660 (0.4175)	loss 1.0566 (1.3174)	grad_norm 4.4873 (5.0566)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:12:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][600/2502]	eta 0:12:57 lr 0.000009	 wd 0.0000	time 0.3329 (0.4088)	loss 1.2784 (1.3199)	grad_norm 4.9352 (5.0327)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:12:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][700/2502]	eta 0:12:05 lr 0.000009	 wd 0.0000	time 0.3500 (0.4026)	loss 0.8162 (1.3146)	grad_norm 6.8937 (5.0384)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:13:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][800/2502]	eta 0:11:19 lr 0.000009	 wd 0.0000	time 0.3315 (0.3990)	loss 1.2407 (1.3096)	grad_norm 4.6695 (nan)	loss_scale 512.0000 (988.2047)	mem 12561MB
[2024-05-28 03:14:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][900/2502]	eta 0:10:33 lr 0.000009	 wd 0.0000	time 0.3290 (0.3952)	loss 1.1796 (1.3073)	grad_norm 4.8348 (nan)	loss_scale 512.0000 (935.3518)	mem 12561MB
[2024-05-28 03:14:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1000/2502]	eta 0:09:50 lr 0.000009	 wd 0.0000	time 0.3410 (0.3929)	loss 1.3956 (1.3095)	grad_norm 5.8094 (nan)	loss_scale 512.0000 (893.0589)	mem 12561MB
[2024-05-28 03:15:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1100/2502]	eta 0:09:22 lr 0.000009	 wd 0.0000	time 0.4756 (0.4014)	loss 1.4052 (1.3083)	grad_norm 5.4063 (nan)	loss_scale 512.0000 (858.4487)	mem 12561MB
[2024-05-28 03:16:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1200/2502]	eta 0:08:49 lr 0.000009	 wd 0.0000	time 0.5347 (0.4068)	loss 1.3489 (1.3077)	grad_norm 4.4098 (nan)	loss_scale 512.0000 (829.6020)	mem 12561MB
[2024-05-28 03:18:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1300/2502]	eta 0:09:11 lr 0.000009	 wd 0.0000	time 0.3978 (0.4592)	loss 0.9396 (1.3079)	grad_norm 4.9752 (nan)	loss_scale 512.0000 (805.1899)	mem 12561MB
[2024-05-28 03:18:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1400/2502]	eta 0:08:19 lr 0.000008	 wd 0.0000	time 0.3243 (0.4532)	loss 1.2844 (1.3098)	grad_norm 4.1175 (nan)	loss_scale 512.0000 (784.2627)	mem 12561MB
[2024-05-28 03:19:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1500/2502]	eta 0:07:28 lr 0.000008	 wd 0.0000	time 0.3361 (0.4475)	loss 1.2951 (1.3093)	grad_norm 4.1339 (nan)	loss_scale 512.0000 (766.1239)	mem 12561MB
[2024-05-28 03:20:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1600/2502]	eta 0:06:39 lr 0.000008	 wd 0.0000	time 0.3267 (0.4427)	loss 1.4775 (1.3087)	grad_norm 6.0052 (nan)	loss_scale 512.0000 (750.2511)	mem 12561MB
[2024-05-28 03:20:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1700/2502]	eta 0:05:51 lr 0.000008	 wd 0.0000	time 0.3495 (0.4381)	loss 1.3010 (1.3091)	grad_norm 4.3921 (nan)	loss_scale 512.0000 (736.2446)	mem 12561MB
[2024-05-28 03:21:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1800/2502]	eta 0:05:05 lr 0.000008	 wd 0.0000	time 0.3902 (0.4345)	loss 1.5801 (1.3085)	grad_norm 5.1085 (nan)	loss_scale 512.0000 (723.7934)	mem 12561MB
[2024-05-28 03:21:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][1900/2502]	eta 0:04:20 lr 0.000008	 wd 0.0000	time 0.3235 (0.4319)	loss 1.4633 (1.3081)	grad_norm 4.4321 (nan)	loss_scale 512.0000 (712.6523)	mem 12561MB
[2024-05-28 03:22:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][2000/2502]	eta 0:03:35 lr 0.000008	 wd 0.0000	time 0.3417 (0.4288)	loss 1.4579 (1.3069)	grad_norm 4.0630 (nan)	loss_scale 512.0000 (702.6247)	mem 12561MB
[2024-05-28 03:23:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][2100/2502]	eta 0:02:52 lr 0.000008	 wd 0.0000	time 0.3601 (0.4302)	loss 1.5878 (1.3077)	grad_norm 8.6527 (nan)	loss_scale 512.0000 (693.5516)	mem 12561MB
[2024-05-28 03:24:00 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][2200/2502]	eta 0:02:10 lr 0.000008	 wd 0.0000	time 0.3829 (0.4309)	loss 1.3162 (1.3061)	grad_norm 6.4579 (nan)	loss_scale 512.0000 (685.3030)	mem 12561MB
[2024-05-28 03:25:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][2300/2502]	eta 0:01:29 lr 0.000008	 wd 0.0000	time 0.6232 (0.4421)	loss 1.2871 (1.3063)	grad_norm 4.1495 (nan)	loss_scale 512.0000 (677.7714)	mem 12561MB
[2024-05-28 03:25:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][2400/2502]	eta 0:00:45 lr 0.000008	 wd 0.0000	time 0.3492 (0.4441)	loss 1.1018 (1.3057)	grad_norm 4.6425 (nan)	loss_scale 512.0000 (670.8671)	mem 12561MB
[2024-05-28 03:26:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.3496 (0.4441)	loss 0.9704 (1.3042)	grad_norm 4.1039 (nan)	loss_scale 512.0000 (664.5150)	mem 12561MB
[2024-05-28 03:26:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 22 training takes 0:18:46
[2024-05-28 03:28:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 63.140 (63.140)	Loss 0.3899 (0.3899)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 03:28:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 84.952 Acc@5 97.528
[2024-05-28 03:28:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-05-28 03:28:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 84.95%
[2024-05-28 03:28:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 03:28:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 03:28:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][0/2502]	eta 8:46:17 lr 0.000008	 wd 0.0000	time 12.6210 (12.6210)	loss 1.0796 (1.0796)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:29:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][100/2502]	eta 0:19:29 lr 0.000008	 wd 0.0000	time 0.3864 (0.4868)	loss 1.4409 (1.3289)	grad_norm 3.7169 (5.6496)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:29:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][200/2502]	eta 0:16:32 lr 0.000007	 wd 0.0000	time 0.3332 (0.4312)	loss 1.5342 (1.3119)	grad_norm 5.7362 (5.1784)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:30:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][300/2502]	eta 0:14:59 lr 0.000007	 wd 0.0000	time 0.3099 (0.4087)	loss 1.3964 (1.3116)	grad_norm 4.7827 (5.2064)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:31:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][400/2502]	eta 0:13:55 lr 0.000007	 wd 0.0000	time 0.3202 (0.3975)	loss 1.6406 (1.3026)	grad_norm 4.6834 (5.0845)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:31:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][500/2502]	eta 0:13:04 lr 0.000007	 wd 0.0000	time 0.4038 (0.3918)	loss 1.5490 (1.3065)	grad_norm 5.6068 (5.1681)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:32:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][600/2502]	eta 0:12:17 lr 0.000007	 wd 0.0000	time 0.3274 (0.3875)	loss 1.0791 (1.3081)	grad_norm 4.0002 (5.1860)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:32:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][700/2502]	eta 0:11:34 lr 0.000007	 wd 0.0000	time 0.3148 (0.3856)	loss 1.3596 (1.3034)	grad_norm 3.4012 (5.1707)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:33:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][800/2502]	eta 0:10:53 lr 0.000007	 wd 0.0000	time 0.3317 (0.3837)	loss 1.6700 (1.3031)	grad_norm 3.3108 (5.1370)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:34:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][900/2502]	eta 0:10:11 lr 0.000007	 wd 0.0000	time 0.3376 (0.3818)	loss 1.5543 (1.3005)	grad_norm 5.2480 (5.1933)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:34:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1000/2502]	eta 0:09:48 lr 0.000007	 wd 0.0000	time 0.3494 (0.3921)	loss 1.0017 (1.3009)	grad_norm 4.1511 (5.1974)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:35:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1100/2502]	eta 0:09:18 lr 0.000007	 wd 0.0000	time 0.4128 (0.3986)	loss 0.8402 (1.2976)	grad_norm 6.1214 (5.1901)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:37:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1200/2502]	eta 0:09:32 lr 0.000007	 wd 0.0000	time 0.5025 (0.4400)	loss 1.3663 (1.2997)	grad_norm 4.3039 (5.2901)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:38:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1300/2502]	eta 0:09:00 lr 0.000007	 wd 0.0000	time 0.3736 (0.4499)	loss 1.3345 (1.3015)	grad_norm 4.6344 (5.2764)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:38:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1400/2502]	eta 0:08:09 lr 0.000007	 wd 0.0000	time 0.3313 (0.4439)	loss 1.4356 (1.3001)	grad_norm 11.0570 (5.2794)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:39:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1500/2502]	eta 0:07:20 lr 0.000006	 wd 0.0000	time 0.3411 (0.4395)	loss 1.2268 (1.3023)	grad_norm 4.5416 (5.2628)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:40:00 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1600/2502]	eta 0:06:32 lr 0.000006	 wd 0.0000	time 0.3348 (0.4348)	loss 1.1411 (1.3021)	grad_norm 4.5762 (5.2280)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:40:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1700/2502]	eta 0:05:45 lr 0.000006	 wd 0.0000	time 0.3620 (0.4308)	loss 1.1924 (1.3019)	grad_norm 4.9719 (5.2169)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:41:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1800/2502]	eta 0:05:00 lr 0.000006	 wd 0.0000	time 0.3637 (0.4278)	loss 1.6157 (1.3016)	grad_norm 5.9727 (5.2113)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:41:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][1900/2502]	eta 0:04:15 lr 0.000006	 wd 0.0000	time 0.3409 (0.4248)	loss 1.1305 (1.3037)	grad_norm 5.2756 (5.1950)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:42:28 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][2000/2502]	eta 0:03:31 lr 0.000006	 wd 0.0000	time 0.3498 (0.4218)	loss 1.5476 (1.3029)	grad_norm 4.5882 (5.1809)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:43:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][2100/2502]	eta 0:02:51 lr 0.000006	 wd 0.0000	time 0.3253 (0.4256)	loss 1.4911 (1.3030)	grad_norm 9.1228 (5.1674)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:44:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][2200/2502]	eta 0:02:08 lr 0.000006	 wd 0.0000	time 0.3508 (0.4267)	loss 1.6176 (1.3006)	grad_norm 14.6546 (5.1795)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 03:44:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][2300/2502]	eta 0:01:26 lr 0.000006	 wd 0.0000	time 0.3424 (0.4306)	loss 1.5373 (1.3001)	grad_norm 4.4746 (5.1785)	loss_scale 1024.0000 (524.9057)	mem 12561MB
[2024-05-28 03:45:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][2400/2502]	eta 0:00:44 lr 0.000006	 wd 0.0000	time 0.3440 (0.4318)	loss 1.5780 (1.3006)	grad_norm 7.1062 (5.1633)	loss_scale 1024.0000 (545.6926)	mem 12561MB
[2024-05-28 03:46:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0000	time 0.3232 (0.4321)	loss 1.3636 (1.3013)	grad_norm 3.6311 (5.1399)	loss_scale 1024.0000 (564.8173)	mem 12561MB
[2024-05-28 03:46:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 23 training takes 0:18:16
[2024-05-28 03:47:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 63.992 (63.992)	Loss 0.3894 (0.3894)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 03:48:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 85.030 Acc@5 97.538
[2024-05-28 03:48:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-05-28 03:48:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 85.03%
[2024-05-28 03:48:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 03:48:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 03:48:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][0/2502]	eta 9:15:14 lr 0.000006	 wd 0.0000	time 13.3152 (13.3152)	loss 1.4010 (1.4010)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:48:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][100/2502]	eta 0:19:44 lr 0.000006	 wd 0.0000	time 0.3293 (0.4933)	loss 1.0099 (1.3314)	grad_norm 4.3930 (5.8587)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:49:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][200/2502]	eta 0:16:39 lr 0.000006	 wd 0.0000	time 0.3431 (0.4342)	loss 1.3790 (1.3159)	grad_norm 4.2437 (5.3694)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:50:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][300/2502]	eta 0:15:05 lr 0.000006	 wd 0.0000	time 0.3345 (0.4112)	loss 1.4944 (1.3059)	grad_norm 3.8352 (5.4268)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:50:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][400/2502]	eta 0:13:59 lr 0.000005	 wd 0.0000	time 0.2999 (0.3994)	loss 1.3807 (1.3057)	grad_norm 5.2727 (5.3346)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:51:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][500/2502]	eta 0:13:07 lr 0.000005	 wd 0.0000	time 0.3572 (0.3934)	loss 1.4583 (1.3086)	grad_norm 11.6527 (5.4712)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:52:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][600/2502]	eta 0:12:19 lr 0.000005	 wd 0.0000	time 0.3220 (0.3890)	loss 1.4672 (1.3058)	grad_norm 5.4145 (5.4073)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:52:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][700/2502]	eta 0:11:36 lr 0.000005	 wd 0.0000	time 0.3297 (0.3866)	loss 1.2891 (1.3076)	grad_norm 4.3075 (5.4015)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:53:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][800/2502]	eta 0:10:53 lr 0.000005	 wd 0.0000	time 0.3118 (0.3841)	loss 1.4361 (1.3086)	grad_norm 3.7313 (5.3723)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:53:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][900/2502]	eta 0:10:11 lr 0.000005	 wd 0.0000	time 0.4012 (0.3819)	loss 1.4240 (1.3060)	grad_norm 3.1072 (5.3691)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:54:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1000/2502]	eta 0:09:52 lr 0.000005	 wd 0.0000	time 0.3156 (0.3943)	loss 1.2085 (1.3074)	grad_norm 7.8415 (5.4405)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:55:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1100/2502]	eta 0:09:23 lr 0.000005	 wd 0.0000	time 0.3978 (0.4018)	loss 1.4619 (1.3065)	grad_norm 4.6453 (5.3742)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:56:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1200/2502]	eta 0:09:08 lr 0.000005	 wd 0.0000	time 0.4275 (0.4210)	loss 1.1145 (1.3044)	grad_norm 8.5915 (5.3502)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:57:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1300/2502]	eta 0:08:28 lr 0.000005	 wd 0.0000	time 0.4094 (0.4231)	loss 1.1987 (1.3043)	grad_norm 5.3726 (5.3842)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:58:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1400/2502]	eta 0:08:12 lr 0.000005	 wd 0.0000	time 0.4790 (0.4470)	loss 1.2614 (1.3026)	grad_norm 4.1607 (5.3842)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 03:59:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1500/2502]	eta 0:07:30 lr 0.000005	 wd 0.0000	time 0.7599 (0.4492)	loss 0.9380 (1.3010)	grad_norm 4.3292 (5.3841)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:00:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1600/2502]	eta 0:06:47 lr 0.000005	 wd 0.0000	time 0.4076 (0.4515)	loss 1.3372 (1.3012)	grad_norm 7.4041 (5.3957)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:01:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1700/2502]	eta 0:06:04 lr 0.000005	 wd 0.0000	time 0.5721 (0.4551)	loss 1.5496 (1.3031)	grad_norm 4.8078 (5.3790)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:01:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1800/2502]	eta 0:05:21 lr 0.000005	 wd 0.0000	time 0.3905 (0.4580)	loss 1.1456 (1.3017)	grad_norm 3.5177 (5.3847)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:02:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][1900/2502]	eta 0:04:39 lr 0.000005	 wd 0.0000	time 0.4084 (0.4636)	loss 1.4987 (1.3032)	grad_norm 3.7866 (5.3758)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:03:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][2000/2502]	eta 0:03:52 lr 0.000004	 wd 0.0000	time 0.3935 (0.4634)	loss 1.4196 (1.3028)	grad_norm 4.5017 (5.3589)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:04:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][2100/2502]	eta 0:03:07 lr 0.000004	 wd 0.0000	time 0.4104 (0.4665)	loss 1.3858 (1.3018)	grad_norm 4.6005 (5.3471)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:05:11 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][2200/2502]	eta 0:02:20 lr 0.000004	 wd 0.0000	time 0.3533 (0.4654)	loss 1.3418 (1.3012)	grad_norm 4.5799 (5.3295)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:06:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][2300/2502]	eta 0:01:35 lr 0.000004	 wd 0.0000	time 0.4216 (0.4742)	loss 1.5831 (1.3008)	grad_norm 4.2175 (5.3336)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:07:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][2400/2502]	eta 0:00:48 lr 0.000004	 wd 0.0000	time 0.3662 (0.4743)	loss 1.5009 (1.3009)	grad_norm 4.5537 (5.3150)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:07:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0000	time 0.3158 (0.4733)	loss 1.1767 (1.3015)	grad_norm 4.0754 (5.3076)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:08:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 24 training takes 0:20:00
[2024-05-28 04:09:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 67.922 (67.922)	Loss 0.3931 (0.3931)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 04:09:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 85.068 Acc@5 97.534
[2024-05-28 04:09:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-05-28 04:09:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 85.07%
[2024-05-28 04:09:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 04:09:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 04:10:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][0/2502]	eta 17:01:25 lr 0.000004	 wd 0.0000	time 24.4945 (24.4945)	loss 1.1263 (1.1263)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:10:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][100/2502]	eta 0:25:46 lr 0.000004	 wd 0.0000	time 0.3476 (0.6440)	loss 1.2836 (1.2731)	grad_norm 4.7323 (5.7878)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:11:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][200/2502]	eta 0:19:24 lr 0.000004	 wd 0.0000	time 0.3769 (0.5058)	loss 1.3564 (1.2946)	grad_norm 3.4215 (5.3387)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:11:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][300/2502]	eta 0:16:48 lr 0.000004	 wd 0.0000	time 0.3237 (0.4578)	loss 1.0216 (1.2856)	grad_norm 3.3692 (5.3461)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:12:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][400/2502]	eta 0:15:16 lr 0.000004	 wd 0.0000	time 0.3288 (0.4362)	loss 1.1546 (1.3003)	grad_norm 4.4450 (5.3065)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:13:10 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][500/2502]	eta 0:14:03 lr 0.000004	 wd 0.0000	time 0.3494 (0.4211)	loss 1.4110 (1.3013)	grad_norm 8.7454 (5.2161)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:13:47 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][600/2502]	eta 0:13:04 lr 0.000004	 wd 0.0000	time 0.3823 (0.4122)	loss 1.1892 (1.3033)	grad_norm 4.8739 (5.3966)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:14:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][700/2502]	eta 0:12:12 lr 0.000004	 wd 0.0000	time 0.3064 (0.4067)	loss 1.1371 (1.3020)	grad_norm 5.0511 (5.4037)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:15:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][800/2502]	eta 0:11:23 lr 0.000004	 wd 0.0000	time 0.3473 (0.4013)	loss 0.9897 (1.3017)	grad_norm 9.1116 (nan)	loss_scale 512.0000 (1009.9376)	mem 12561MB
[2024-05-28 04:15:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][900/2502]	eta 0:10:41 lr 0.000004	 wd 0.0000	time 0.7192 (0.4002)	loss 1.4190 (1.3040)	grad_norm 4.4583 (nan)	loss_scale 512.0000 (954.6726)	mem 12561MB
[2024-05-28 04:16:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1000/2502]	eta 0:10:34 lr 0.000004	 wd 0.0000	time 0.4140 (0.4223)	loss 1.2223 (1.3055)	grad_norm 3.3958 (nan)	loss_scale 512.0000 (910.4496)	mem 12561MB
[2024-05-28 04:17:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1100/2502]	eta 0:10:30 lr 0.000004	 wd 0.0000	time 0.4702 (0.4499)	loss 1.4093 (1.3037)	grad_norm 4.4043 (nan)	loss_scale 512.0000 (874.2598)	mem 12561MB
[2024-05-28 04:18:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1200/2502]	eta 0:09:59 lr 0.000004	 wd 0.0000	time 0.3532 (0.4603)	loss 1.4116 (1.3072)	grad_norm 6.6167 (nan)	loss_scale 512.0000 (844.0966)	mem 12561MB
[2024-05-28 04:19:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1300/2502]	eta 0:09:04 lr 0.000003	 wd 0.0000	time 0.3045 (0.4533)	loss 1.4578 (1.3088)	grad_norm 5.1540 (nan)	loss_scale 512.0000 (818.5703)	mem 12561MB
[2024-05-28 04:20:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1400/2502]	eta 0:08:12 lr 0.000003	 wd 0.0000	time 0.3114 (0.4468)	loss 1.1680 (1.3086)	grad_norm 4.2928 (nan)	loss_scale 512.0000 (796.6881)	mem 12561MB
[2024-05-28 04:20:43 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1500/2502]	eta 0:07:23 lr 0.000003	 wd 0.0000	time 0.3371 (0.4423)	loss 1.3050 (1.3086)	grad_norm 5.2503 (nan)	loss_scale 512.0000 (777.7215)	mem 12561MB
[2024-05-28 04:21:20 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1600/2502]	eta 0:06:34 lr 0.000003	 wd 0.0000	time 0.3201 (0.4376)	loss 1.4394 (1.3083)	grad_norm 3.8345 (nan)	loss_scale 512.0000 (761.1243)	mem 12561MB
[2024-05-28 04:21:57 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1700/2502]	eta 0:05:47 lr 0.000003	 wd 0.0000	time 0.3763 (0.4334)	loss 1.2085 (1.3080)	grad_norm 3.9099 (nan)	loss_scale 512.0000 (746.4785)	mem 12561MB
[2024-05-28 04:22:35 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1800/2502]	eta 0:05:02 lr 0.000003	 wd 0.0000	time 0.3586 (0.4303)	loss 1.3307 (1.3076)	grad_norm 28.8687 (nan)	loss_scale 512.0000 (733.4592)	mem 12561MB
[2024-05-28 04:23:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][1900/2502]	eta 0:04:17 lr 0.000003	 wd 0.0000	time 0.3414 (0.4272)	loss 1.5712 (1.3075)	grad_norm 4.4151 (nan)	loss_scale 512.0000 (721.8096)	mem 12561MB
[2024-05-28 04:23:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][2000/2502]	eta 0:03:33 lr 0.000003	 wd 0.0000	time 0.5120 (0.4247)	loss 1.2765 (1.3063)	grad_norm 5.8491 (nan)	loss_scale 512.0000 (711.3243)	mem 12561MB
[2024-05-28 04:24:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][2100/2502]	eta 0:02:52 lr 0.000003	 wd 0.0000	time 0.6157 (0.4282)	loss 0.8403 (1.3064)	grad_norm 5.4084 (nan)	loss_scale 512.0000 (701.8372)	mem 12561MB
[2024-05-28 04:25:24 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][2200/2502]	eta 0:02:09 lr 0.000003	 wd 0.0000	time 0.4915 (0.4290)	loss 1.3527 (1.3050)	grad_norm 4.4779 (nan)	loss_scale 512.0000 (693.2122)	mem 12561MB
[2024-05-28 04:26:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][2300/2502]	eta 0:01:29 lr 0.000003	 wd 0.0000	time 0.3915 (0.4420)	loss 1.2749 (1.3034)	grad_norm 4.3064 (nan)	loss_scale 512.0000 (685.3368)	mem 12561MB
[2024-05-28 04:27:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][2400/2502]	eta 0:00:45 lr 0.000003	 wd 0.0000	time 0.3669 (0.4496)	loss 1.4040 (1.3043)	grad_norm 5.6046 (nan)	loss_scale 512.0000 (678.1175)	mem 12561MB
[2024-05-28 04:28:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.3212 (0.4470)	loss 1.4808 (1.3038)	grad_norm 4.7926 (nan)	loss_scale 512.0000 (671.4754)	mem 12561MB
[2024-05-28 04:28:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 25 training takes 0:18:51
[2024-05-28 04:29:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 82.074 (82.074)	Loss 0.3911 (0.3911)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 04:30:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 85.046 Acc@5 97.508
[2024-05-28 04:30:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-05-28 04:30:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 85.07%
[2024-05-28 04:30:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][0/2502]	eta 1 day, 2:51:43 lr 0.000003	 wd 0.0000	time 38.6506 (38.6506)	loss 1.1852 (1.1852)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:31:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][100/2502]	eta 0:29:49 lr 0.000003	 wd 0.0000	time 0.3324 (0.7448)	loss 1.3531 (1.3098)	grad_norm 5.8482 (5.3260)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:32:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][200/2502]	eta 0:21:26 lr 0.000003	 wd 0.0000	time 0.3229 (0.5590)	loss 1.3708 (1.3304)	grad_norm 4.7707 (5.0714)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:32:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][300/2502]	eta 0:18:09 lr 0.000003	 wd 0.0000	time 0.4083 (0.4947)	loss 1.3388 (1.3131)	grad_norm 3.4825 (4.9705)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:33:18 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][400/2502]	eta 0:16:09 lr 0.000003	 wd 0.0000	time 0.3459 (0.4610)	loss 1.4756 (1.3152)	grad_norm 4.1150 (5.1191)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:33:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][500/2502]	eta 0:14:47 lr 0.000003	 wd 0.0000	time 0.3038 (0.4432)	loss 1.5045 (1.3195)	grad_norm 4.2237 (5.0665)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:34:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][600/2502]	eta 0:13:38 lr 0.000003	 wd 0.0000	time 0.3346 (0.4302)	loss 1.1599 (1.3138)	grad_norm 4.4101 (5.1652)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:35:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][700/2502]	eta 0:12:38 lr 0.000003	 wd 0.0000	time 0.3723 (0.4210)	loss 1.3728 (1.3078)	grad_norm 5.4831 (5.1709)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:35:46 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][800/2502]	eta 0:11:46 lr 0.000002	 wd 0.0000	time 0.3343 (0.4154)	loss 1.5680 (1.3063)	grad_norm 4.3428 (5.1736)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:36:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][900/2502]	eta 0:10:56 lr 0.000002	 wd 0.0000	time 0.3039 (0.4099)	loss 1.4419 (1.3075)	grad_norm 4.7590 (5.1674)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:37:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1000/2502]	eta 0:10:16 lr 0.000002	 wd 0.0000	time 0.4201 (0.4104)	loss 1.2750 (1.3074)	grad_norm 5.0883 (5.1792)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:37:51 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1100/2502]	eta 0:09:42 lr 0.000002	 wd 0.0000	time 0.5737 (0.4158)	loss 1.3067 (1.3088)	grad_norm 7.3051 (5.2938)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:38:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1200/2502]	eta 0:09:23 lr 0.000002	 wd 0.0000	time 0.4282 (0.4325)	loss 1.0706 (1.3087)	grad_norm 11.9852 (5.3748)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:40:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1300/2502]	eta 0:09:08 lr 0.000002	 wd 0.0000	time 0.5224 (0.4564)	loss 0.9234 (1.3066)	grad_norm 5.6352 (5.3442)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:40:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1400/2502]	eta 0:08:19 lr 0.000002	 wd 0.0000	time 0.3285 (0.4528)	loss 1.5354 (1.3067)	grad_norm 4.2509 (5.3573)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:41:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1500/2502]	eta 0:07:27 lr 0.000002	 wd 0.0000	time 0.3433 (0.4471)	loss 1.3679 (1.3050)	grad_norm 4.5274 (5.3437)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:42:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1600/2502]	eta 0:06:39 lr 0.000002	 wd 0.0000	time 0.3868 (0.4429)	loss 1.3659 (1.3043)	grad_norm 37.3692 (5.3738)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:42:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1700/2502]	eta 0:05:51 lr 0.000002	 wd 0.0000	time 0.3352 (0.4386)	loss 1.3413 (1.3046)	grad_norm 4.9221 (5.3629)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:43:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1800/2502]	eta 0:05:05 lr 0.000002	 wd 0.0000	time 0.3549 (0.4347)	loss 1.4197 (1.3067)	grad_norm 3.6891 (5.3480)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:43:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][1900/2502]	eta 0:04:19 lr 0.000002	 wd 0.0000	time 0.3229 (0.4318)	loss 1.0546 (1.3053)	grad_norm 2.9035 (5.3242)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:44:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][2000/2502]	eta 0:03:35 lr 0.000002	 wd 0.0000	time 0.3599 (0.4287)	loss 0.8906 (1.3049)	grad_norm 5.0214 (5.3284)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:45:09 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][2100/2502]	eta 0:02:51 lr 0.000002	 wd 0.0000	time 0.3477 (0.4261)	loss 1.0644 (1.3034)	grad_norm 4.9409 (5.3379)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:46:05 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][2200/2502]	eta 0:02:10 lr 0.000002	 wd 0.0000	time 0.4809 (0.4323)	loss 0.9559 (1.3030)	grad_norm 3.5447 (5.3222)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 04:46:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][2300/2502]	eta 0:01:27 lr 0.000002	 wd 0.0000	time 0.4996 (0.4351)	loss 1.2381 (1.3016)	grad_norm 5.3141 (5.3528)	loss_scale 1024.0000 (517.3403)	mem 12561MB
[2024-05-28 04:47:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][2400/2502]	eta 0:00:44 lr 0.000002	 wd 0.0000	time 0.4305 (0.4365)	loss 1.0313 (1.3019)	grad_norm 4.6308 (5.3845)	loss_scale 1024.0000 (538.4423)	mem 12561MB
[2024-05-28 04:48:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.3302 (0.4352)	loss 0.8790 (1.3000)	grad_norm 3.8666 (5.3949)	loss_scale 1024.0000 (557.8569)	mem 12561MB
[2024-05-28 04:48:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 26 training takes 0:18:29
[2024-05-28 04:49:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 47.300 (47.300)	Loss 0.3899 (0.3899)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 04:49:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 85.026 Acc@5 97.522
[2024-05-28 04:49:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-05-28 04:49:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 85.07%
[2024-05-28 04:50:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][0/2502]	eta 1 day, 5:08:39 lr 0.000002	 wd 0.0000	time 41.9343 (41.9343)	loss 0.8953 (0.8953)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:51:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][100/2502]	eta 0:30:56 lr 0.000002	 wd 0.0000	time 0.3304 (0.7730)	loss 1.5315 (1.3138)	grad_norm 5.7669 (5.1828)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:51:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][200/2502]	eta 0:22:02 lr 0.000002	 wd 0.0000	time 0.3319 (0.5745)	loss 1.3204 (1.3081)	grad_norm 5.2119 (5.1762)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:52:21 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][300/2502]	eta 0:18:28 lr 0.000002	 wd 0.0000	time 0.3544 (0.5036)	loss 1.2867 (1.3022)	grad_norm 5.0516 (5.0154)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:52:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][400/2502]	eta 0:16:24 lr 0.000002	 wd 0.0000	time 0.3335 (0.4684)	loss 1.4385 (1.2981)	grad_norm 4.2172 (5.1759)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:53:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][500/2502]	eta 0:15:02 lr 0.000002	 wd 0.0000	time 0.3285 (0.4506)	loss 0.7816 (1.2938)	grad_norm 4.6806 (5.2258)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:54:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][600/2502]	eta 0:13:51 lr 0.000002	 wd 0.0000	time 0.3288 (0.4370)	loss 1.2772 (1.2970)	grad_norm 4.4093 (5.3575)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:54:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][700/2502]	eta 0:12:49 lr 0.000002	 wd 0.0000	time 0.3617 (0.4270)	loss 1.3151 (1.3034)	grad_norm 3.4582 (5.3472)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:55:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][800/2502]	eta 0:11:55 lr 0.000002	 wd 0.0000	time 0.3733 (0.4205)	loss 1.6532 (1.3059)	grad_norm 4.2213 (5.3336)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:56:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][900/2502]	eta 0:11:03 lr 0.000001	 wd 0.0000	time 0.2878 (0.4141)	loss 1.2458 (1.3109)	grad_norm 3.4919 (5.2924)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:56:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1000/2502]	eta 0:10:16 lr 0.000001	 wd 0.0000	time 0.4591 (0.4103)	loss 1.3871 (1.3099)	grad_norm 4.3919 (5.2661)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:57:33 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1100/2502]	eta 0:09:49 lr 0.000001	 wd 0.0000	time 0.3884 (0.4204)	loss 1.4704 (1.3103)	grad_norm 6.4879 (5.2398)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:58:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1200/2502]	eta 0:09:32 lr 0.000001	 wd 0.0000	time 0.5423 (0.4395)	loss 1.4025 (1.3098)	grad_norm 4.2629 (5.2395)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 04:59:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1300/2502]	eta 0:08:51 lr 0.000001	 wd 0.0000	time 0.3817 (0.4421)	loss 1.3613 (1.3074)	grad_norm 5.5524 (5.2366)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 05:00:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1400/2502]	eta 0:08:24 lr 0.000001	 wd 0.0000	time 0.5103 (0.4576)	loss 1.2088 (1.3074)	grad_norm 3.8128 (5.2050)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 05:01:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1500/2502]	eta 0:07:38 lr 0.000001	 wd 0.0000	time 0.3782 (0.4574)	loss 1.5160 (1.3069)	grad_norm 5.4366 (5.2355)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 05:02:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1600/2502]	eta 0:07:03 lr 0.000001	 wd 0.0000	time 0.5340 (0.4696)	loss 1.4818 (1.3054)	grad_norm 4.9144 (5.2550)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 05:03:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1700/2502]	eta 0:06:16 lr 0.000001	 wd 0.0000	time 0.4628 (0.4690)	loss 1.3675 (1.3032)	grad_norm 4.5419 (5.2902)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 05:04:13 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1800/2502]	eta 0:05:36 lr 0.000001	 wd 0.0000	time 0.5018 (0.4792)	loss 1.4067 (1.3033)	grad_norm 6.7529 (5.2828)	loss_scale 1024.0000 (1024.0000)	mem 12561MB
[2024-05-28 05:05:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][1900/2502]	eta 0:04:50 lr 0.000001	 wd 0.0000	time 0.5051 (0.4819)	loss 0.9549 (1.3047)	grad_norm 7.1825 (inf)	loss_scale 512.0000 (1005.6854)	mem 12561MB
[2024-05-28 05:05:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][2000/2502]	eta 0:04:01 lr 0.000001	 wd 0.0000	time 0.3481 (0.4810)	loss 0.8014 (1.3034)	grad_norm 3.2613 (inf)	loss_scale 512.0000 (981.0135)	mem 12561MB
[2024-05-28 05:06:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][2100/2502]	eta 0:03:13 lr 0.000001	 wd 0.0000	time 0.7544 (0.4825)	loss 1.7778 (1.3023)	grad_norm 6.6148 (inf)	loss_scale 512.0000 (958.6901)	mem 12561MB
[2024-05-28 05:07:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][2200/2502]	eta 0:02:25 lr 0.000001	 wd 0.0000	time 0.4160 (0.4814)	loss 1.1084 (1.3022)	grad_norm 4.1300 (inf)	loss_scale 512.0000 (938.3953)	mem 12561MB
[2024-05-28 05:08:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][2300/2502]	eta 0:01:37 lr 0.000001	 wd 0.0000	time 0.4838 (0.4822)	loss 1.3799 (1.3018)	grad_norm 3.4743 (inf)	loss_scale 512.0000 (919.8644)	mem 12561MB
[2024-05-28 05:09:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][2400/2502]	eta 0:00:49 lr 0.000001	 wd 0.0000	time 0.4445 (0.4814)	loss 1.1543 (1.3011)	grad_norm 3.9855 (inf)	loss_scale 512.0000 (902.8771)	mem 12561MB
[2024-05-28 05:09:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.3238 (0.4778)	loss 1.5308 (1.2994)	grad_norm 3.8504 (inf)	loss_scale 512.0000 (887.2483)	mem 12561MB
[2024-05-28 05:10:06 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 27 training takes 0:20:15
[2024-05-28 05:10:55 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 49.265 (49.265)	Loss 0.3887 (0.3887)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 05:11:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 85.046 Acc@5 97.514
[2024-05-28 05:11:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-05-28 05:11:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 85.07%
[2024-05-28 05:11:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][0/2502]	eta 1 day, 0:33:38 lr 0.000001	 wd 0.0000	time 35.3390 (35.3390)	loss 1.3070 (1.3070)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:12:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][100/2502]	eta 0:28:34 lr 0.000001	 wd 0.0000	time 0.3612 (0.7136)	loss 0.9430 (1.2803)	grad_norm 7.2094 (5.6441)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:13:04 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][200/2502]	eta 0:20:50 lr 0.000001	 wd 0.0000	time 0.3247 (0.5434)	loss 1.0414 (1.2847)	grad_norm 3.6156 (5.6106)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:13:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][300/2502]	eta 0:17:43 lr 0.000001	 wd 0.0000	time 0.4125 (0.4832)	loss 1.4789 (1.3011)	grad_norm 4.7400 (5.4558)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:14:17 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][400/2502]	eta 0:15:53 lr 0.000001	 wd 0.0000	time 0.3446 (0.4537)	loss 1.4352 (1.3027)	grad_norm 4.2584 (6.1564)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:14:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][500/2502]	eta 0:14:35 lr 0.000001	 wd 0.0000	time 0.3081 (0.4373)	loss 1.4308 (1.3029)	grad_norm 3.4751 (5.9130)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:15:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][600/2502]	eta 0:13:28 lr 0.000001	 wd 0.0000	time 0.3279 (0.4249)	loss 1.3406 (1.3032)	grad_norm 6.7181 (5.8943)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:16:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][700/2502]	eta 0:12:31 lr 0.000001	 wd 0.0000	time 0.3017 (0.4169)	loss 0.9792 (1.3065)	grad_norm 2.8998 (5.9430)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:16:45 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][800/2502]	eta 0:11:41 lr 0.000001	 wd 0.0000	time 0.3222 (0.4124)	loss 1.4543 (1.3027)	grad_norm 3.3994 (5.9539)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:17:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][900/2502]	eta 0:10:52 lr 0.000001	 wd 0.0000	time 0.3452 (0.4075)	loss 1.4815 (1.3023)	grad_norm 5.6628 (5.8441)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:18:00 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1000/2502]	eta 0:10:08 lr 0.000001	 wd 0.0000	time 0.5518 (0.4048)	loss 1.3053 (1.3037)	grad_norm 5.7314 (5.8064)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:18:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1100/2502]	eta 0:09:46 lr 0.000001	 wd 0.0000	time 0.4343 (0.4186)	loss 0.8596 (1.3010)	grad_norm 5.4058 (5.7312)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:19:49 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1200/2502]	eta 0:09:17 lr 0.000001	 wd 0.0000	time 0.4644 (0.4284)	loss 1.0543 (1.3008)	grad_norm 4.1413 (5.6745)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:20:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1300/2502]	eta 0:08:38 lr 0.000001	 wd 0.0000	time 0.3515 (0.4315)	loss 1.5460 (1.2984)	grad_norm 5.2622 (5.6476)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:21:37 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1400/2502]	eta 0:08:09 lr 0.000001	 wd 0.0000	time 0.6389 (0.4438)	loss 0.9322 (1.2992)	grad_norm 5.6639 (5.6154)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:22:31 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1500/2502]	eta 0:07:31 lr 0.000001	 wd 0.0000	time 0.3720 (0.4502)	loss 1.2651 (1.2970)	grad_norm 3.6825 (5.5716)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:23:22 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1600/2502]	eta 0:06:49 lr 0.000001	 wd 0.0000	time 0.4326 (0.4542)	loss 1.2113 (1.2966)	grad_norm 5.3246 (5.5254)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:24:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1700/2502]	eta 0:06:04 lr 0.000001	 wd 0.0000	time 0.4253 (0.4542)	loss 1.5051 (1.2981)	grad_norm 4.0801 (5.5018)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:25:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1800/2502]	eta 0:05:22 lr 0.000001	 wd 0.0000	time 0.4036 (0.4599)	loss 1.3582 (1.2973)	grad_norm 3.8772 (5.4624)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:25:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][1900/2502]	eta 0:04:36 lr 0.000001	 wd 0.0000	time 0.3784 (0.4594)	loss 1.5325 (1.2967)	grad_norm 7.1435 (5.4547)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:26:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][2000/2502]	eta 0:03:52 lr 0.000001	 wd 0.0000	time 0.3567 (0.4628)	loss 0.8595 (1.2969)	grad_norm 6.1543 (5.4518)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:27:27 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][2100/2502]	eta 0:03:05 lr 0.000001	 wd 0.0000	time 0.3726 (0.4626)	loss 1.2274 (1.2975)	grad_norm 2.9988 (5.4380)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:28:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][2200/2502]	eta 0:02:20 lr 0.000001	 wd 0.0000	time 0.3712 (0.4651)	loss 1.4487 (1.2960)	grad_norm 4.5844 (5.4590)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:29:03 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][2300/2502]	eta 0:01:33 lr 0.000001	 wd 0.0000	time 0.3692 (0.4643)	loss 1.1060 (1.2958)	grad_norm 3.9248 (5.4365)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:29:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][2400/2502]	eta 0:00:47 lr 0.000001	 wd 0.0000	time 0.4852 (0.4670)	loss 1.5983 (1.2944)	grad_norm 3.1860 (5.4346)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:30:34 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.3418 (0.4634)	loss 1.1584 (1.2935)	grad_norm 4.3645 (5.4195)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:30:41 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 28 training takes 0:19:26
[2024-05-28 05:31:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 66.875 (66.875)	Loss 0.3877 (0.3877)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 05:32:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 85.080 Acc@5 97.524
[2024-05-28 05:32:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-05-28 05:32:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 85.08%
[2024-05-28 05:32:08 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 05:32:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 05:32:32 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][0/2502]	eta 14:19:12 lr 0.000001	 wd 0.0000	time 20.6043 (20.6043)	loss 1.4687 (1.4687)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:33:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][100/2502]	eta 0:23:52 lr 0.000001	 wd 0.0000	time 0.2883 (0.5964)	loss 1.3684 (1.2747)	grad_norm 4.2560 (6.5550)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:33:48 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][200/2502]	eta 0:18:25 lr 0.000001	 wd 0.0000	time 0.3483 (0.4800)	loss 1.1058 (1.2883)	grad_norm 4.7905 (6.1138)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:34:25 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][300/2502]	eta 0:16:17 lr 0.000001	 wd 0.0000	time 0.3230 (0.4437)	loss 1.4143 (1.2897)	grad_norm 4.4219 (6.1769)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:35:02 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][400/2502]	eta 0:14:51 lr 0.000001	 wd 0.0000	time 0.3244 (0.4242)	loss 1.1531 (1.2948)	grad_norm 8.2395 (5.8566)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:35:38 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][500/2502]	eta 0:13:44 lr 0.000001	 wd 0.0000	time 0.3232 (0.4119)	loss 0.8103 (1.2967)	grad_norm 4.6369 (5.7497)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:36:16 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][600/2502]	eta 0:12:53 lr 0.000000	 wd 0.0000	time 0.3417 (0.4064)	loss 1.2636 (1.2982)	grad_norm 4.1693 (5.6651)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:36:53 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][700/2502]	eta 0:12:02 lr 0.000000	 wd 0.0000	time 0.3844 (0.4010)	loss 1.4845 (1.2980)	grad_norm 9.9430 (5.6057)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:37:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][800/2502]	eta 0:11:15 lr 0.000000	 wd 0.0000	time 0.3405 (0.3969)	loss 0.8588 (1.2973)	grad_norm 5.6747 (5.5763)	loss_scale 512.0000 (512.0000)	mem 12561MB
[2024-05-28 05:38:07 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][900/2502]	eta 0:10:32 lr 0.000000	 wd 0.0000	time 0.3579 (0.3949)	loss 1.2285 (1.2976)	grad_norm 4.5724 (5.4771)	loss_scale 1024.0000 (552.9145)	mem 12561MB
[2024-05-28 05:38:44 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1000/2502]	eta 0:09:49 lr 0.000000	 wd 0.0000	time 0.3326 (0.3921)	loss 1.3054 (1.2974)	grad_norm 11.7761 (5.4849)	loss_scale 1024.0000 (599.9760)	mem 12561MB
[2024-05-28 05:39:23 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1100/2502]	eta 0:09:09 lr 0.000000	 wd 0.0000	time 0.4088 (0.3916)	loss 1.4071 (1.2984)	grad_norm 2.9685 (5.4683)	loss_scale 1024.0000 (638.4886)	mem 12561MB
[2024-05-28 05:40:12 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1200/2502]	eta 0:08:40 lr 0.000000	 wd 0.0000	time 0.4042 (0.3998)	loss 1.2821 (1.2953)	grad_norm 6.3080 (5.4231)	loss_scale 1024.0000 (670.5878)	mem 12561MB
[2024-05-28 05:40:58 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1300/2502]	eta 0:08:06 lr 0.000000	 wd 0.0000	time 0.5650 (0.4045)	loss 1.3887 (1.2957)	grad_norm 39.9476 (5.4769)	loss_scale 1024.0000 (697.7525)	mem 12561MB
[2024-05-28 05:42:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1400/2502]	eta 0:07:57 lr 0.000000	 wd 0.0000	time 0.3620 (0.4334)	loss 1.4628 (1.2939)	grad_norm 4.0101 (5.4321)	loss_scale 1024.0000 (721.0393)	mem 12561MB
[2024-05-28 05:43:00 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1500/2502]	eta 0:07:12 lr 0.000000	 wd 0.0000	time 0.3161 (0.4317)	loss 1.3109 (1.2952)	grad_norm 6.7701 (5.4009)	loss_scale 1024.0000 (741.2232)	mem 12561MB
[2024-05-28 05:43:36 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1600/2502]	eta 0:06:25 lr 0.000000	 wd 0.0000	time 0.3279 (0.4277)	loss 1.3746 (1.2962)	grad_norm 3.9716 (5.3596)	loss_scale 1024.0000 (758.8857)	mem 12561MB
[2024-05-28 05:44:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1700/2502]	eta 0:05:40 lr 0.000000	 wd 0.0000	time 0.3531 (0.4245)	loss 1.2076 (1.2945)	grad_norm 4.1106 (5.3430)	loss_scale 1024.0000 (774.4715)	mem 12561MB
[2024-05-28 05:44:52 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1800/2502]	eta 0:04:56 lr 0.000000	 wd 0.0000	time 0.3457 (0.4219)	loss 1.1789 (1.2956)	grad_norm 6.9703 (5.3984)	loss_scale 1024.0000 (788.3265)	mem 12561MB
[2024-05-28 05:45:29 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][1900/2502]	eta 0:04:12 lr 0.000000	 wd 0.0000	time 0.3663 (0.4192)	loss 1.2838 (1.2953)	grad_norm 3.8114 (5.3882)	loss_scale 1024.0000 (800.7238)	mem 12561MB
[2024-05-28 05:46:14 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][2000/2502]	eta 0:03:31 lr 0.000000	 wd 0.0000	time 0.4603 (0.4209)	loss 1.4556 (1.2956)	grad_norm 12.9605 (5.4013)	loss_scale 1024.0000 (811.8821)	mem 12561MB
[2024-05-28 05:47:01 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][2100/2502]	eta 0:02:50 lr 0.000000	 wd 0.0000	time 0.5295 (0.4231)	loss 1.3059 (1.2965)	grad_norm 5.4957 (5.4078)	loss_scale 1024.0000 (821.9781)	mem 12561MB
[2024-05-28 05:47:54 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][2200/2502]	eta 0:02:09 lr 0.000000	 wd 0.0000	time 0.6387 (0.4282)	loss 1.2962 (1.2978)	grad_norm 3.7298 (5.4048)	loss_scale 1024.0000 (831.1567)	mem 12561MB
[2024-05-28 05:48:39 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][2300/2502]	eta 0:01:26 lr 0.000000	 wd 0.0000	time 0.3436 (0.4290)	loss 1.2794 (1.2987)	grad_norm 7.3785 (5.4015)	loss_scale 1024.0000 (839.5376)	mem 12561MB
[2024-05-28 05:49:50 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][2400/2502]	eta 0:00:44 lr 0.000000	 wd 0.0000	time 0.4808 (0.4407)	loss 1.4903 (1.2995)	grad_norm 4.5561 (5.3998)	loss_scale 1024.0000 (847.2203)	mem 12561MB
[2024-05-28 05:50:30 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 226): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0000	time 0.3305 (0.4391)	loss 1.5468 (1.2991)	grad_norm 4.5630 (5.3841)	loss_scale 1024.0000 (854.2887)	mem 12561MB
[2024-05-28 05:50:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 235): INFO EPOCH 29 training takes 0:18:28
[2024-05-28 05:50:40 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 145): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_29.pth saving......
[2024-05-28 05:50:42 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 147): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_29.pth saved !!!
[2024-05-28 05:51:56 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 275): INFO Test: [0/98]	Time 73.443 (73.443)	Loss 0.3887 (0.3887)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12561MB
[2024-05-28 05:52:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 282): INFO  * Acc@1 85.080 Acc@5 97.522
[2024-05-28 05:52:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-05-28 05:52:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 168): INFO Max accuracy: 85.08%
[2024-05-28 05:52:15 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 159): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saving......
[2024-05-28 05:52:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (utils.py 161): INFO /mnt/data/pretrain_weights/swin-mam/swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x/swin_mam_v2_base_p4_w7_224_22kto1k/ckpt_epoch_best.pth saved !!!
[2024-05-28 05:52:19 swin_mam_v2_base_patch4_window7_224_22kto1k_finetune_4x] (main.py 175): INFO Training time 10:18:02
