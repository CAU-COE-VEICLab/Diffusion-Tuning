[2024-07-04 22:15:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/config.json
[2024-07-04 22:15:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_stage_process2/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process2/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-04 22:15:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility/configs/swin/diffusion_ft_swin_base_patch4_window7_224_22kto1k_finetune_full.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_stage_process2/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process2/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-04 22:15:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 108): INFO Creating model:swin_diffusion_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full
[2024-07-04 22:15:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 110): INFO SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-04 22:15:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 113): INFO number of params: 87768224
[2024-07-04 22:15:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-04 22:15:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune, ignoring auto resume
[2024-07-04 22:15:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_stage_process2/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process2/ckpt_epoch_best.pth for fine-tuning......
[2024-07-04 22:15:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=[])
[2024-07-04 22:15:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility/pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_stage_process2/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process2/ckpt_epoch_best.pth'
[2024-07-04 22:15:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 12.623 (12.623)	Loss 0.3982 (0.3982)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 1731MB
[2024-07-04 22:16:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.742 Acc@5 97.506
[2024-07-04 22:16:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 162): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-04 22:16:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 168): INFO Start training
[2024-07-04 22:16:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:18:12 lr 0.000000	 wd 0.0000	time 11.9474 (11.9474)	loss 1.5371 (1.5371)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 11181MB
[2024-07-04 22:17:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:25:37 lr 0.000000	 wd 0.0000	time 0.5097 (0.6401)	loss 1.3763 (1.3565)	grad_norm 9.5642 (nan)	loss_scale 2048.0000 (5718.1782)	mem 12176MB
[2024-07-04 22:18:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:22:22 lr 0.000001	 wd 0.0000	time 0.5147 (0.5831)	loss 1.2496 (1.3379)	grad_norm 5.7298 (nan)	loss_scale 2048.0000 (3892.2189)	mem 12176MB
[2024-07-04 22:18:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:20:42 lr 0.000001	 wd 0.0000	time 0.4992 (0.5640)	loss 1.2599 (1.3208)	grad_norm 4.5326 (nan)	loss_scale 2048.0000 (3279.5216)	mem 12176MB
[2024-07-04 22:19:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:19:25 lr 0.000001	 wd 0.0000	time 0.5049 (0.5546)	loss 1.8143 (1.3232)	grad_norm 4.3087 (nan)	loss_scale 2048.0000 (2972.4090)	mem 12176MB
[2024-07-04 22:20:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:18:19 lr 0.000002	 wd 0.0000	time 0.5073 (0.5490)	loss 1.5350 (1.3236)	grad_norm 7.6832 (nan)	loss_scale 2048.0000 (2787.8962)	mem 12176MB
[2024-07-04 22:21:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:17:17 lr 0.000002	 wd 0.0000	time 0.5187 (0.5453)	loss 0.9375 (1.3274)	grad_norm 3.5800 (nan)	loss_scale 2048.0000 (2664.7854)	mem 12176MB
[2024-07-04 22:22:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:16:17 lr 0.000002	 wd 0.0000	time 0.5176 (0.5427)	loss 1.4040 (1.3255)	grad_norm 3.6440 (nan)	loss_scale 2048.0000 (2576.7989)	mem 12176MB
[2024-07-04 22:23:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:15:20 lr 0.000003	 wd 0.0000	time 0.5025 (0.5406)	loss 1.5385 (1.3254)	grad_norm 4.5814 (nan)	loss_scale 2048.0000 (2510.7815)	mem 12176MB
[2024-07-04 22:24:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:14:23 lr 0.000003	 wd 0.0000	time 0.5113 (0.5391)	loss 1.5194 (1.3211)	grad_norm 4.9204 (nan)	loss_scale 2048.0000 (2459.4184)	mem 12176MB
[2024-07-04 22:25:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:13:27 lr 0.000003	 wd 0.0000	time 0.5094 (0.5378)	loss 1.3372 (1.3204)	grad_norm 5.1197 (nan)	loss_scale 2048.0000 (2418.3177)	mem 12176MB
[2024-07-04 22:25:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:12:32 lr 0.000004	 wd 0.0000	time 0.5007 (0.5367)	loss 1.4643 (1.3212)	grad_norm 4.2445 (nan)	loss_scale 2048.0000 (2384.6830)	mem 12176MB
[2024-07-04 22:26:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:11:37 lr 0.000004	 wd 0.0000	time 0.5152 (0.5359)	loss 1.3050 (1.3242)	grad_norm 3.9367 (nan)	loss_scale 2048.0000 (2356.6495)	mem 12176MB
[2024-07-04 22:27:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:10:43 lr 0.000004	 wd 0.0000	time 0.5057 (0.5351)	loss 1.4737 (1.3259)	grad_norm 4.9816 (nan)	loss_scale 1024.0000 (2263.6618)	mem 12176MB
[2024-07-04 22:28:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:09:48 lr 0.000005	 wd 0.0000	time 0.5074 (0.5344)	loss 1.5315 (1.3266)	grad_norm 4.6658 (nan)	loss_scale 1024.0000 (2175.1777)	mem 12176MB
[2024-07-04 22:29:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:08:54 lr 0.000005	 wd 0.0000	time 0.5156 (0.5338)	loss 1.3080 (1.3260)	grad_norm 4.3484 (nan)	loss_scale 1024.0000 (2098.4837)	mem 12176MB
[2024-07-04 22:30:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:08:01 lr 0.000005	 wd 0.0000	time 0.5108 (0.5334)	loss 1.6659 (1.3268)	grad_norm 3.7654 (nan)	loss_scale 1024.0000 (2031.3704)	mem 12176MB
[2024-07-04 22:31:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:07:07 lr 0.000005	 wd 0.0000	time 0.5068 (0.5330)	loss 1.5405 (1.3263)	grad_norm 3.6187 (nan)	loss_scale 1024.0000 (1972.1481)	mem 12176MB
[2024-07-04 22:32:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:06:13 lr 0.000006	 wd 0.0000	time 0.5182 (0.5326)	loss 1.1875 (1.3262)	grad_norm 4.3060 (nan)	loss_scale 1024.0000 (1919.5025)	mem 12176MB
[2024-07-04 22:32:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:05:20 lr 0.000006	 wd 0.0000	time 0.5086 (0.5322)	loss 1.3860 (1.3254)	grad_norm 5.5097 (nan)	loss_scale 1024.0000 (1872.3956)	mem 12176MB
[2024-07-04 22:33:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:04:27 lr 0.000006	 wd 0.0000	time 0.5080 (0.5320)	loss 1.5083 (1.3231)	grad_norm 5.3533 (nan)	loss_scale 1024.0000 (1829.9970)	mem 12176MB
[2024-07-04 22:34:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:03:33 lr 0.000007	 wd 0.0000	time 0.5102 (0.5317)	loss 1.3490 (1.3241)	grad_norm 4.7399 (nan)	loss_scale 1024.0000 (1791.6345)	mem 12176MB
[2024-07-04 22:35:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:02:40 lr 0.000007	 wd 0.0000	time 0.5137 (0.5314)	loss 1.4733 (1.3240)	grad_norm 6.8142 (nan)	loss_scale 1024.0000 (1756.7578)	mem 12176MB
[2024-07-04 22:36:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:47 lr 0.000007	 wd 0.0000	time 0.5102 (0.5312)	loss 1.3568 (1.3226)	grad_norm 4.7833 (nan)	loss_scale 1024.0000 (1724.9126)	mem 12176MB
[2024-07-04 22:37:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:54 lr 0.000008	 wd 0.0000	time 0.5023 (0.5310)	loss 1.4024 (1.3230)	grad_norm 4.5566 (nan)	loss_scale 1024.0000 (1695.7201)	mem 12176MB
[2024-07-04 22:38:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0000	time 0.5045 (0.5308)	loss 1.5464 (1.3232)	grad_norm 3.5011 (nan)	loss_scale 1024.0000 (1668.8621)	mem 12176MB
[2024-07-04 22:38:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 0 training takes 0:22:10
[2024-07-04 22:38:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_0.pth saving......
[2024-07-04 22:38:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_0.pth saved !!!
[2024-07-04 22:38:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 9.674 (9.674)	Loss 0.3972 (0.3972)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-04 22:38:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.670 Acc@5 97.506
[2024-07-04 22:38:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-04 22:38:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.67%
[2024-07-04 22:38:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-04 22:38:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-04 22:38:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:22:55 lr 0.000008	 wd 0.0000	time 10.6217 (10.6217)	loss 1.2086 (1.2086)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 22:39:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:25:05 lr 0.000008	 wd 0.0000	time 0.5035 (0.6269)	loss 1.1400 (1.3471)	grad_norm 4.7433 (5.9760)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 22:40:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:22:07 lr 0.000009	 wd 0.0000	time 0.5158 (0.5767)	loss 1.3896 (1.3539)	grad_norm 4.1459 (5.4631)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 22:41:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:20:32 lr 0.000009	 wd 0.0000	time 0.5063 (0.5598)	loss 1.6735 (1.3415)	grad_norm 4.8123 (5.5715)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 22:42:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:19:18 lr 0.000009	 wd 0.0000	time 0.5014 (0.5511)	loss 0.8870 (1.3275)	grad_norm 4.6723 (inf)	loss_scale 512.0000 (926.9626)	mem 12176MB
[2024-07-04 22:43:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:18:13 lr 0.000010	 wd 0.0000	time 0.5057 (0.5461)	loss 1.5779 (1.3255)	grad_norm 5.1957 (inf)	loss_scale 512.0000 (844.1357)	mem 12176MB
[2024-07-04 22:44:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:17:12 lr 0.000010	 wd 0.0000	time 0.5201 (0.5427)	loss 1.3331 (1.3221)	grad_norm 6.2441 (inf)	loss_scale 512.0000 (788.8719)	mem 12176MB
[2024-07-04 22:45:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:16:13 lr 0.000010	 wd 0.0000	time 0.5133 (0.5403)	loss 1.4156 (1.3206)	grad_norm 6.7037 (inf)	loss_scale 512.0000 (749.3752)	mem 12176MB
[2024-07-04 22:45:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:15:16 lr 0.000011	 wd 0.0000	time 0.5138 (0.5386)	loss 1.4549 (1.3238)	grad_norm 4.1325 (inf)	loss_scale 512.0000 (719.7403)	mem 12176MB
[2024-07-04 22:46:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:14:20 lr 0.000011	 wd 0.0000	time 0.5031 (0.5372)	loss 1.4946 (1.3221)	grad_norm 4.1714 (inf)	loss_scale 512.0000 (696.6837)	mem 12176MB
[2024-07-04 22:47:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:13:25 lr 0.000011	 wd 0.0000	time 0.5096 (0.5361)	loss 1.4329 (1.3203)	grad_norm 4.7791 (inf)	loss_scale 512.0000 (678.2338)	mem 12176MB
[2024-07-04 22:48:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:12:30 lr 0.000012	 wd 0.0000	time 0.5102 (0.5352)	loss 1.0819 (1.3205)	grad_norm 3.9052 (inf)	loss_scale 512.0000 (663.1353)	mem 12176MB
[2024-07-04 22:49:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:11:35 lr 0.000012	 wd 0.0000	time 0.5005 (0.5344)	loss 1.3334 (1.3232)	grad_norm 3.3921 (inf)	loss_scale 512.0000 (650.5512)	mem 12176MB
[2024-07-04 22:50:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:10:41 lr 0.000012	 wd 0.0000	time 0.5042 (0.5338)	loss 1.4980 (1.3246)	grad_norm 5.0617 (inf)	loss_scale 512.0000 (639.9016)	mem 12176MB
[2024-07-04 22:51:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:09:47 lr 0.000012	 wd 0.0000	time 0.5095 (0.5333)	loss 1.4222 (1.3227)	grad_norm 4.0213 (inf)	loss_scale 512.0000 (630.7723)	mem 12176MB
[2024-07-04 22:52:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:08:53 lr 0.000013	 wd 0.0000	time 0.5145 (0.5328)	loss 0.9067 (1.3210)	grad_norm 4.0877 (inf)	loss_scale 512.0000 (622.8594)	mem 12176MB
[2024-07-04 22:52:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:08:00 lr 0.000013	 wd 0.0000	time 0.5104 (0.5325)	loss 0.9492 (1.3200)	grad_norm 3.5236 (inf)	loss_scale 512.0000 (615.9350)	mem 12176MB
[2024-07-04 22:53:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:07:06 lr 0.000013	 wd 0.0000	time 0.4959 (0.5321)	loss 1.3550 (1.3199)	grad_norm 10.9819 (inf)	loss_scale 512.0000 (609.8248)	mem 12176MB
[2024-07-04 22:54:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:06:13 lr 0.000014	 wd 0.0000	time 0.5181 (0.5318)	loss 1.3107 (1.3192)	grad_norm 5.2221 (inf)	loss_scale 512.0000 (604.3931)	mem 12176MB
[2024-07-04 22:55:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:05:19 lr 0.000014	 wd 0.0000	time 0.5059 (0.5315)	loss 1.3394 (1.3201)	grad_norm 4.4095 (inf)	loss_scale 512.0000 (599.5329)	mem 12176MB
[2024-07-04 22:56:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:04:26 lr 0.000014	 wd 0.0000	time 0.5088 (0.5312)	loss 1.4210 (1.3200)	grad_norm 4.3423 (inf)	loss_scale 512.0000 (595.1584)	mem 12176MB
[2024-07-04 22:57:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:03:33 lr 0.000015	 wd 0.0000	time 0.5134 (0.5310)	loss 1.4913 (1.3209)	grad_norm 3.8431 (inf)	loss_scale 512.0000 (591.2004)	mem 12176MB
[2024-07-04 22:58:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:02:40 lr 0.000015	 wd 0.0000	time 0.5170 (0.5308)	loss 1.0560 (1.3215)	grad_norm 5.0054 (inf)	loss_scale 512.0000 (587.6020)	mem 12176MB
[2024-07-04 22:59:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:47 lr 0.000015	 wd 0.0000	time 0.5132 (0.5307)	loss 1.3355 (1.3217)	grad_norm 4.1166 (inf)	loss_scale 512.0000 (584.3164)	mem 12176MB
[2024-07-04 22:59:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:54 lr 0.000016	 wd 0.0000	time 0.5149 (0.5305)	loss 1.4671 (1.3201)	grad_norm 6.3781 (inf)	loss_scale 512.0000 (581.3045)	mem 12176MB
[2024-07-04 23:00:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0000	time 0.5038 (0.5303)	loss 1.0868 (1.3214)	grad_norm 6.3813 (inf)	loss_scale 512.0000 (578.5334)	mem 12176MB
[2024-07-04 23:00:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 1 training takes 0:22:09
[2024-07-04 23:01:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.716 (11.716)	Loss 0.3975 (0.3975)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-04 23:01:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.718 Acc@5 97.474
[2024-07-04 23:01:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-04 23:01:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-04 23:01:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-04 23:01:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-04 23:01:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][0/2502]	eta 6:48:18 lr 0.000016	 wd 0.0000	time 9.7914 (9.7914)	loss 1.4294 (1.4294)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:02:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:24:44 lr 0.000016	 wd 0.0000	time 0.5062 (0.6180)	loss 1.4414 (1.3057)	grad_norm 4.8577 (5.0431)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:03:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:21:57 lr 0.000017	 wd 0.0000	time 0.5107 (0.5722)	loss 1.4284 (1.3247)	grad_norm 5.8571 (5.2701)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:04:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:20:26 lr 0.000017	 wd 0.0000	time 0.5118 (0.5569)	loss 1.1483 (1.3279)	grad_norm 3.9610 (5.1068)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:04:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:19:14 lr 0.000017	 wd 0.0000	time 0.5122 (0.5492)	loss 1.4558 (1.3247)	grad_norm 6.0789 (5.1042)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:05:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:18:09 lr 0.000018	 wd 0.0000	time 0.5088 (0.5443)	loss 1.3932 (1.3276)	grad_norm 10.8475 (5.1530)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:06:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:17:09 lr 0.000018	 wd 0.0000	time 0.5052 (0.5412)	loss 1.2335 (1.3220)	grad_norm 4.2535 (5.1135)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:07:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:16:11 lr 0.000018	 wd 0.0000	time 0.5083 (0.5390)	loss 1.2014 (1.3257)	grad_norm 5.2571 (5.0723)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:08:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:15:14 lr 0.000019	 wd 0.0000	time 0.5082 (0.5372)	loss 1.5257 (1.3244)	grad_norm 4.3271 (5.0611)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:09:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:14:18 lr 0.000019	 wd 0.0000	time 0.5001 (0.5359)	loss 1.4735 (1.3297)	grad_norm 6.6537 (5.0589)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:10:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:13:23 lr 0.000019	 wd 0.0000	time 0.5056 (0.5349)	loss 1.4416 (1.3313)	grad_norm 7.2076 (5.2081)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:11:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:12:28 lr 0.000020	 wd 0.0000	time 0.5102 (0.5340)	loss 1.1696 (1.3325)	grad_norm 6.4157 (5.2259)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:11:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:11:34 lr 0.000020	 wd 0.0000	time 0.5126 (0.5334)	loss 1.3055 (1.3320)	grad_norm 4.6389 (5.2231)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:12:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:10:40 lr 0.000020	 wd 0.0000	time 0.5162 (0.5328)	loss 1.6045 (1.3347)	grad_norm 6.3157 (5.2888)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:13:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:09:46 lr 0.000020	 wd 0.0000	time 0.5099 (0.5323)	loss 1.3851 (1.3353)	grad_norm 4.7125 (5.3283)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:14:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:08:53 lr 0.000021	 wd 0.0000	time 0.5102 (0.5319)	loss 1.5180 (1.3337)	grad_norm 4.0953 (5.2983)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:15:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:07:59 lr 0.000021	 wd 0.0000	time 0.5137 (0.5315)	loss 1.1926 (1.3340)	grad_norm 4.8551 (5.2869)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:16:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:07:06 lr 0.000021	 wd 0.0000	time 0.5159 (0.5312)	loss 1.2909 (1.3332)	grad_norm 5.0443 (5.2993)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:17:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:06:12 lr 0.000022	 wd 0.0000	time 0.5123 (0.5308)	loss 1.2695 (1.3332)	grad_norm 6.1096 (5.3546)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-04 23:18:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:05:19 lr 0.000022	 wd 0.0000	time 0.5042 (0.5306)	loss 0.9480 (1.3318)	grad_norm 4.5379 (5.3810)	loss_scale 1024.0000 (533.0079)	mem 12176MB
[2024-07-04 23:18:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:04:26 lr 0.000022	 wd 0.0000	time 0.5167 (0.5304)	loss 1.1428 (1.3296)	grad_norm 6.6762 (5.3962)	loss_scale 1024.0000 (557.5452)	mem 12176MB
[2024-07-04 23:19:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:03:33 lr 0.000023	 wd 0.0000	time 0.5201 (0.5302)	loss 1.3913 (1.3299)	grad_norm 38.5917 (5.4109)	loss_scale 1024.0000 (579.7468)	mem 12176MB
[2024-07-04 23:20:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:02:40 lr 0.000023	 wd 0.0000	time 0.5138 (0.5301)	loss 1.5802 (1.3297)	grad_norm 4.5859 (5.4437)	loss_scale 1024.0000 (599.9309)	mem 12176MB
[2024-07-04 23:21:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:47 lr 0.000023	 wd 0.0000	time 0.5194 (0.5299)	loss 1.5892 (1.3299)	grad_norm 4.2385 (5.4122)	loss_scale 1024.0000 (618.3607)	mem 12176MB
[2024-07-04 23:22:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:54 lr 0.000024	 wd 0.0000	time 0.5136 (0.5298)	loss 1.4594 (1.3291)	grad_norm 4.3913 (5.4006)	loss_scale 1024.0000 (635.2553)	mem 12176MB
[2024-07-04 23:23:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000024	 wd 0.0000	time 0.5082 (0.5296)	loss 1.4566 (1.3293)	grad_norm 4.8000 (5.4199)	loss_scale 1024.0000 (650.7989)	mem 12176MB
[2024-07-04 23:23:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 2 training takes 0:22:07
[2024-07-04 23:23:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.454 (11.454)	Loss 0.3938 (0.3938)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-04 23:23:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.596 Acc@5 97.494
[2024-07-04 23:23:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-04 23:23:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-04 23:23:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:01:39 lr 0.000024	 wd 0.0000	time 11.5505 (11.5505)	loss 0.9462 (0.9462)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:24:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:25:26 lr 0.000024	 wd 0.0000	time 0.5067 (0.6356)	loss 1.4599 (1.3012)	grad_norm 4.4292 (5.5568)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:25:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:22:16 lr 0.000025	 wd 0.0000	time 0.5133 (0.5805)	loss 1.3677 (1.3195)	grad_norm 5.2070 (5.5375)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:26:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:20:38 lr 0.000025	 wd 0.0000	time 0.5030 (0.5626)	loss 1.5485 (1.3147)	grad_norm 4.2436 (5.4043)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:27:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:19:24 lr 0.000025	 wd 0.0000	time 0.5094 (0.5538)	loss 1.5038 (1.3176)	grad_norm 5.3978 (5.4333)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:28:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:18:17 lr 0.000026	 wd 0.0000	time 0.5134 (0.5483)	loss 1.4031 (1.3166)	grad_norm 4.0831 (5.2835)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:29:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:17:15 lr 0.000026	 wd 0.0000	time 0.5088 (0.5445)	loss 1.3406 (1.3174)	grad_norm 5.1557 (5.2452)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:30:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:16:16 lr 0.000026	 wd 0.0000	time 0.5078 (0.5420)	loss 1.6561 (1.3196)	grad_norm 6.4823 (5.2136)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:31:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:15:19 lr 0.000027	 wd 0.0000	time 0.5022 (0.5400)	loss 1.0723 (1.3197)	grad_norm 5.6973 (5.2627)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:31:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:14:22 lr 0.000027	 wd 0.0000	time 0.4885 (0.5384)	loss 1.4581 (1.3211)	grad_norm 12.4012 (5.2481)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:32:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:13:26 lr 0.000027	 wd 0.0000	time 0.5020 (0.5372)	loss 1.4756 (1.3237)	grad_norm 4.2361 (5.2364)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:33:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:12:31 lr 0.000028	 wd 0.0000	time 0.5162 (0.5362)	loss 1.3594 (1.3259)	grad_norm 5.5762 (5.2817)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:34:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:11:37 lr 0.000028	 wd 0.0000	time 0.5066 (0.5354)	loss 1.2206 (1.3241)	grad_norm 2.3003 (5.2857)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:35:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:10:42 lr 0.000028	 wd 0.0000	time 0.5114 (0.5347)	loss 1.2515 (1.3268)	grad_norm 3.9152 (5.2921)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:36:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:09:48 lr 0.000028	 wd 0.0000	time 0.5048 (0.5341)	loss 1.2397 (1.3273)	grad_norm 9.8121 (5.2657)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:37:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:08:54 lr 0.000029	 wd 0.0000	time 0.5114 (0.5337)	loss 1.4211 (1.3292)	grad_norm 5.1262 (5.2851)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:38:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:08:00 lr 0.000029	 wd 0.0000	time 0.5087 (0.5332)	loss 1.4963 (1.3283)	grad_norm 5.1138 (5.2883)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:38:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:07:07 lr 0.000029	 wd 0.0000	time 0.5203 (0.5328)	loss 1.0952 (1.3275)	grad_norm 7.1337 (5.2567)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:39:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:06:13 lr 0.000030	 wd 0.0000	time 0.5015 (0.5325)	loss 1.4469 (1.3280)	grad_norm 4.2154 (5.2784)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:40:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:05:20 lr 0.000030	 wd 0.0000	time 0.5083 (0.5322)	loss 1.4398 (1.3274)	grad_norm 7.0022 (5.2849)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:41:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:04:27 lr 0.000030	 wd 0.0000	time 0.5087 (0.5319)	loss 1.1548 (1.3281)	grad_norm 4.4429 (5.2669)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:42:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:03:33 lr 0.000031	 wd 0.0000	time 0.5107 (0.5316)	loss 1.1331 (1.3271)	grad_norm 4.0990 (5.2671)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:43:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:40 lr 0.000031	 wd 0.0000	time 0.5119 (0.5315)	loss 1.3570 (1.3280)	grad_norm 6.4613 (5.2513)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:44:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:47 lr 0.000031	 wd 0.0000	time 0.5221 (0.5313)	loss 1.4664 (1.3290)	grad_norm 3.1192 (5.2449)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:45:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:54 lr 0.000032	 wd 0.0000	time 0.5059 (0.5311)	loss 0.9963 (1.3283)	grad_norm 6.7551 (5.2306)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:45:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000032	 wd 0.0000	time 0.5037 (0.5308)	loss 1.4176 (1.3289)	grad_norm 4.5112 (5.2237)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:45:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 3 training takes 0:22:10
[2024-07-04 23:46:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.376 (11.376)	Loss 0.3989 (0.3989)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 12176MB
[2024-07-04 23:46:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.620 Acc@5 97.426
[2024-07-04 23:46:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-04 23:46:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-04 23:46:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:58:28 lr 0.000032	 wd 0.0000	time 11.4743 (11.4743)	loss 1.3710 (1.3710)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:47:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:25:20 lr 0.000032	 wd 0.0000	time 0.4950 (0.6331)	loss 1.2208 (1.3600)	grad_norm 9.1541 (5.3365)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:48:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:22:12 lr 0.000033	 wd 0.0000	time 0.5101 (0.5789)	loss 1.1499 (1.3488)	grad_norm 6.8636 (5.3144)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:49:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:20:35 lr 0.000033	 wd 0.0000	time 0.5092 (0.5609)	loss 1.0767 (1.3494)	grad_norm 3.9171 (5.2267)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:50:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:19:20 lr 0.000033	 wd 0.0000	time 0.5072 (0.5520)	loss 1.3863 (1.3393)	grad_norm 6.8902 (5.2617)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:50:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:18:14 lr 0.000034	 wd 0.0000	time 0.5055 (0.5465)	loss 1.3892 (1.3351)	grad_norm 6.1981 (5.3853)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:51:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:17:12 lr 0.000034	 wd 0.0000	time 0.5151 (0.5429)	loss 1.3939 (1.3298)	grad_norm 3.9409 (5.2630)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:52:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:16:13 lr 0.000034	 wd 0.0000	time 0.5109 (0.5404)	loss 1.2363 (1.3300)	grad_norm 3.7505 (5.3481)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:53:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:15:16 lr 0.000035	 wd 0.0000	time 0.5125 (0.5387)	loss 1.2753 (1.3310)	grad_norm 3.7650 (5.3735)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-04 23:54:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:14:20 lr 0.000035	 wd 0.0000	time 0.5011 (0.5372)	loss 1.4497 (1.3318)	grad_norm 6.0385 (5.3835)	loss_scale 2048.0000 (1117.1942)	mem 12176MB
[2024-07-04 23:55:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:13:25 lr 0.000035	 wd 0.0000	time 0.5085 (0.5362)	loss 1.4826 (1.3327)	grad_norm 3.2811 (5.4497)	loss_scale 2048.0000 (1210.1818)	mem 12176MB
[2024-07-04 23:56:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:12:30 lr 0.000036	 wd 0.0000	time 0.5050 (0.5352)	loss 1.5493 (1.3331)	grad_norm 3.8702 (5.4370)	loss_scale 2048.0000 (1286.2779)	mem 12176MB
[2024-07-04 23:57:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:11:35 lr 0.000036	 wd 0.0000	time 0.5187 (0.5344)	loss 0.9998 (1.3324)	grad_norm 4.3004 (5.3877)	loss_scale 2048.0000 (1349.7019)	mem 12176MB
[2024-07-04 23:57:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:10:41 lr 0.000036	 wd 0.0000	time 0.5110 (0.5337)	loss 1.4946 (1.3312)	grad_norm 4.4190 (5.3880)	loss_scale 2048.0000 (1403.3759)	mem 12176MB
[2024-07-04 23:58:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:09:47 lr 0.000036	 wd 0.0000	time 0.5132 (0.5332)	loss 1.4100 (1.3313)	grad_norm 10.5531 (5.3614)	loss_scale 2048.0000 (1449.3876)	mem 12176MB
[2024-07-04 23:59:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:08:53 lr 0.000037	 wd 0.0000	time 0.5115 (0.5328)	loss 1.2594 (1.3343)	grad_norm 4.6343 (5.3938)	loss_scale 2048.0000 (1489.2685)	mem 12176MB
[2024-07-05 00:00:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:08:00 lr 0.000037	 wd 0.0000	time 0.5076 (0.5324)	loss 1.1871 (1.3338)	grad_norm 3.5065 (5.3686)	loss_scale 2048.0000 (1524.1674)	mem 12176MB
[2024-07-05 00:01:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:07:06 lr 0.000037	 wd 0.0000	time 0.5102 (0.5320)	loss 1.4840 (1.3335)	grad_norm 8.9810 (5.3521)	loss_scale 2048.0000 (1554.9630)	mem 12176MB
[2024-07-05 00:02:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:06:13 lr 0.000038	 wd 0.0000	time 0.5063 (0.5317)	loss 1.5382 (1.3334)	grad_norm 4.8878 (5.3473)	loss_scale 2048.0000 (1582.3387)	mem 12176MB
[2024-07-05 00:03:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:05:19 lr 0.000038	 wd 0.0000	time 0.5084 (0.5314)	loss 1.4781 (1.3333)	grad_norm 7.3564 (5.3415)	loss_scale 2048.0000 (1606.8343)	mem 12176MB
[2024-07-05 00:04:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:04:26 lr 0.000038	 wd 0.0000	time 0.5083 (0.5311)	loss 1.5207 (1.3335)	grad_norm 4.4581 (5.3222)	loss_scale 2048.0000 (1628.8816)	mem 12176MB
[2024-07-05 00:04:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:03:33 lr 0.000039	 wd 0.0000	time 0.5062 (0.5309)	loss 1.3682 (1.3335)	grad_norm 4.9450 (5.3269)	loss_scale 2048.0000 (1648.8301)	mem 12176MB
[2024-07-05 00:05:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:02:40 lr 0.000039	 wd 0.0000	time 0.5170 (0.5307)	loss 0.9746 (1.3310)	grad_norm 3.6069 (5.3141)	loss_scale 2048.0000 (1666.9659)	mem 12176MB
[2024-07-05 00:06:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:47 lr 0.000039	 wd 0.0000	time 0.5152 (0.5305)	loss 1.2301 (1.3322)	grad_norm 3.3681 (5.2944)	loss_scale 2048.0000 (1683.5254)	mem 12176MB
[2024-07-05 00:07:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:54 lr 0.000040	 wd 0.0000	time 0.5074 (0.5303)	loss 1.3995 (1.3319)	grad_norm 3.5008 (5.2917)	loss_scale 2048.0000 (1698.7055)	mem 12176MB
[2024-07-05 00:08:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000040	 wd 0.0000	time 0.5071 (0.5301)	loss 0.8735 (1.3317)	grad_norm 3.8212 (5.2833)	loss_scale 2048.0000 (1712.6717)	mem 12176MB
[2024-07-05 00:08:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 4 training takes 0:22:08
[2024-07-05 00:08:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.877 (10.877)	Loss 0.4023 (0.4023)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 00:08:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.556 Acc@5 97.388
[2024-07-05 00:08:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-05 00:08:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-05 00:09:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:49:21 lr 0.000040	 wd 0.0000	time 11.2558 (11.2558)	loss 1.5319 (1.5319)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12176MB
[2024-07-05 00:09:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:25:21 lr 0.000040	 wd 0.0000	time 0.5037 (0.6334)	loss 1.2540 (1.3460)	grad_norm 5.0185 (inf)	loss_scale 1024.0000 (1743.8416)	mem 12176MB
[2024-07-05 00:10:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:22:15 lr 0.000040	 wd 0.0000	time 0.5105 (0.5801)	loss 1.3832 (1.3600)	grad_norm 4.2373 (inf)	loss_scale 1024.0000 (1385.7114)	mem 12176MB
[2024-07-05 00:11:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:20:38 lr 0.000040	 wd 0.0000	time 0.5073 (0.5622)	loss 1.6127 (1.3468)	grad_norm 3.6799 (inf)	loss_scale 1024.0000 (1265.5415)	mem 12176MB
[2024-07-05 00:12:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:19:23 lr 0.000040	 wd 0.0000	time 0.5071 (0.5534)	loss 1.0855 (1.3479)	grad_norm 3.7981 (inf)	loss_scale 1024.0000 (1205.3067)	mem 12176MB
[2024-07-05 00:13:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:18:16 lr 0.000040	 wd 0.0000	time 0.5071 (0.5478)	loss 1.6507 (1.3444)	grad_norm 3.0524 (inf)	loss_scale 1024.0000 (1169.1178)	mem 12176MB
[2024-07-05 00:14:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:17:14 lr 0.000040	 wd 0.0000	time 0.5060 (0.5440)	loss 1.4727 (1.3476)	grad_norm 4.1005 (inf)	loss_scale 1024.0000 (1144.9717)	mem 12176MB
[2024-07-05 00:15:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:16:15 lr 0.000040	 wd 0.0000	time 0.5096 (0.5415)	loss 1.4729 (1.3455)	grad_norm 4.9543 (inf)	loss_scale 1024.0000 (1127.7147)	mem 12176MB
[2024-07-05 00:16:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:15:18 lr 0.000040	 wd 0.0000	time 0.5065 (0.5396)	loss 1.0991 (1.3392)	grad_norm 4.0163 (inf)	loss_scale 1024.0000 (1114.7665)	mem 12176MB
[2024-07-05 00:16:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:14:21 lr 0.000040	 wd 0.0000	time 0.5124 (0.5381)	loss 0.8013 (1.3381)	grad_norm 4.9734 (inf)	loss_scale 1024.0000 (1104.6926)	mem 12176MB
[2024-07-05 00:17:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:13:26 lr 0.000040	 wd 0.0000	time 0.5016 (0.5370)	loss 1.3532 (1.3403)	grad_norm 3.4438 (inf)	loss_scale 1024.0000 (1096.6314)	mem 12176MB
[2024-07-05 00:18:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:12:31 lr 0.000040	 wd 0.0000	time 0.5241 (0.5359)	loss 1.5155 (1.3361)	grad_norm 4.4124 (inf)	loss_scale 1024.0000 (1090.0345)	mem 12176MB
[2024-07-05 00:19:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:11:36 lr 0.000040	 wd 0.0000	time 0.5071 (0.5352)	loss 1.4364 (1.3355)	grad_norm 3.9743 (inf)	loss_scale 1024.0000 (1084.5362)	mem 12176MB
[2024-07-05 00:20:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:10:42 lr 0.000040	 wd 0.0000	time 0.5147 (0.5345)	loss 1.4660 (1.3353)	grad_norm 4.7714 (inf)	loss_scale 1024.0000 (1079.8832)	mem 12176MB
[2024-07-05 00:21:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:09:48 lr 0.000040	 wd 0.0000	time 0.5088 (0.5339)	loss 1.4321 (1.3349)	grad_norm 4.0136 (inf)	loss_scale 1024.0000 (1075.8944)	mem 12176MB
[2024-07-05 00:22:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:08:54 lr 0.000040	 wd 0.0000	time 0.5065 (0.5333)	loss 1.2551 (1.3349)	grad_norm 4.9126 (inf)	loss_scale 1024.0000 (1072.4370)	mem 12176MB
[2024-07-05 00:23:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:08:00 lr 0.000040	 wd 0.0000	time 0.5081 (0.5329)	loss 1.4214 (1.3362)	grad_norm 4.3316 (inf)	loss_scale 1024.0000 (1069.4116)	mem 12176MB
[2024-07-05 00:23:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:07:07 lr 0.000040	 wd 0.0000	time 0.5050 (0.5325)	loss 1.3823 (1.3365)	grad_norm 10.0826 (inf)	loss_scale 1024.0000 (1066.7419)	mem 12176MB
[2024-07-05 00:24:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:06:13 lr 0.000040	 wd 0.0000	time 0.5124 (0.5321)	loss 1.0126 (1.3355)	grad_norm 3.4173 (inf)	loss_scale 1024.0000 (1064.3687)	mem 12176MB
[2024-07-05 00:25:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:05:20 lr 0.000040	 wd 0.0000	time 0.5168 (0.5318)	loss 1.4968 (1.3374)	grad_norm 3.8667 (inf)	loss_scale 1024.0000 (1062.2451)	mem 12176MB
[2024-07-05 00:26:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:04:26 lr 0.000040	 wd 0.0000	time 0.5026 (0.5315)	loss 1.3825 (1.3387)	grad_norm 4.5504 (inf)	loss_scale 1024.0000 (1060.3338)	mem 12176MB
[2024-07-05 00:27:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:03:33 lr 0.000040	 wd 0.0000	time 0.5117 (0.5312)	loss 1.5738 (1.3398)	grad_norm 4.6431 (inf)	loss_scale 1024.0000 (1058.6045)	mem 12176MB
[2024-07-05 00:28:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:02:40 lr 0.000040	 wd 0.0000	time 0.5184 (0.5310)	loss 1.2965 (1.3378)	grad_norm 3.6240 (inf)	loss_scale 1024.0000 (1057.0323)	mem 12176MB
[2024-07-05 00:29:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:47 lr 0.000040	 wd 0.0000	time 0.5152 (0.5308)	loss 0.8983 (1.3373)	grad_norm 3.2645 (inf)	loss_scale 1024.0000 (1055.5967)	mem 12176MB
[2024-07-05 00:30:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:54 lr 0.000040	 wd 0.0000	time 0.5087 (0.5306)	loss 1.6294 (1.3375)	grad_norm 4.2143 (inf)	loss_scale 1024.0000 (1054.2807)	mem 12176MB
[2024-07-05 00:31:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000040	 wd 0.0000	time 0.5044 (0.5303)	loss 1.4049 (1.3375)	grad_norm 3.9212 (inf)	loss_scale 1024.0000 (1053.0700)	mem 12176MB
[2024-07-05 00:31:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 5 training takes 0:22:09
[2024-07-05 00:31:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.351 (11.351)	Loss 0.4124 (0.4124)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 00:31:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.376 Acc@5 97.370
[2024-07-05 00:31:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-07-05 00:31:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-05 00:31:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:45:35 lr 0.000040	 wd 0.0000	time 11.1653 (11.1653)	loss 1.7110 (1.7110)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:32:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:25:16 lr 0.000040	 wd 0.0000	time 0.5073 (0.6315)	loss 1.2141 (1.3220)	grad_norm 4.8949 (5.1661)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:33:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:22:14 lr 0.000040	 wd 0.0000	time 0.4875 (0.5797)	loss 1.4285 (1.3364)	grad_norm 5.6815 (4.8336)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:34:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:20:36 lr 0.000040	 wd 0.0000	time 0.4975 (0.5617)	loss 1.5151 (1.3426)	grad_norm 4.4840 (4.9859)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:35:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:19:22 lr 0.000040	 wd 0.0000	time 0.5113 (0.5528)	loss 1.3721 (1.3357)	grad_norm 6.3662 (4.9484)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:36:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:18:15 lr 0.000040	 wd 0.0000	time 0.5071 (0.5474)	loss 1.4821 (1.3334)	grad_norm 3.6545 (4.9117)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:36:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:17:14 lr 0.000040	 wd 0.0000	time 0.5057 (0.5438)	loss 1.5080 (1.3356)	grad_norm 4.2235 (5.1668)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:37:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:16:15 lr 0.000040	 wd 0.0000	time 0.5035 (0.5413)	loss 1.3378 (1.3358)	grad_norm 5.9320 (5.1411)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:38:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:15:18 lr 0.000040	 wd 0.0000	time 0.5016 (0.5394)	loss 1.1016 (1.3360)	grad_norm 4.6513 (5.1520)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:14:21 lr 0.000040	 wd 0.0000	time 0.5072 (0.5379)	loss 1.4840 (1.3379)	grad_norm 3.8766 (5.1468)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:40:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:13:26 lr 0.000040	 wd 0.0000	time 0.5067 (0.5368)	loss 1.3173 (1.3346)	grad_norm 3.6993 (5.1518)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:41:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:12:31 lr 0.000040	 wd 0.0000	time 0.5054 (0.5358)	loss 1.2868 (1.3325)	grad_norm 4.5511 (5.1376)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:42:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:11:36 lr 0.000040	 wd 0.0000	time 0.5076 (0.5349)	loss 1.0825 (1.3334)	grad_norm 6.0859 (5.1687)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:43:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:10:42 lr 0.000040	 wd 0.0000	time 0.5045 (0.5342)	loss 1.4855 (1.3326)	grad_norm 4.7143 (5.1705)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:43:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:09:48 lr 0.000040	 wd 0.0000	time 0.5128 (0.5336)	loss 1.7321 (1.3307)	grad_norm 4.8706 (5.1697)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:44:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:08:54 lr 0.000040	 wd 0.0000	time 0.5083 (0.5331)	loss 1.3874 (1.3330)	grad_norm 4.9326 (5.1547)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:45:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:08:00 lr 0.000040	 wd 0.0000	time 0.5054 (0.5327)	loss 1.4181 (1.3313)	grad_norm 4.3568 (5.1365)	loss_scale 2048.0000 (1044.4672)	mem 12176MB
[2024-07-05 00:46:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:07:06 lr 0.000040	 wd 0.0000	time 0.4979 (0.5323)	loss 1.5127 (1.3313)	grad_norm 4.3188 (5.1642)	loss_scale 2048.0000 (1103.4638)	mem 12176MB
[2024-07-05 00:47:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:06:13 lr 0.000040	 wd 0.0000	time 0.5063 (0.5319)	loss 1.0860 (1.3311)	grad_norm 13.4645 (5.1715)	loss_scale 2048.0000 (1155.9089)	mem 12176MB
[2024-07-05 00:48:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:05:20 lr 0.000040	 wd 0.0000	time 0.5226 (0.5316)	loss 1.2270 (1.3311)	grad_norm 3.9215 (5.1619)	loss_scale 2048.0000 (1202.8364)	mem 12176MB
[2024-07-05 00:49:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:04:26 lr 0.000039	 wd 0.0000	time 0.5138 (0.5313)	loss 1.0744 (1.3315)	grad_norm 7.0117 (nan)	loss_scale 1024.0000 (1223.5802)	mem 12176MB
[2024-07-05 00:50:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:03:33 lr 0.000039	 wd 0.0000	time 0.5089 (0.5311)	loss 1.5064 (1.3308)	grad_norm 4.7422 (nan)	loss_scale 1024.0000 (1214.0809)	mem 12176MB
[2024-07-05 00:50:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:02:40 lr 0.000039	 wd 0.0000	time 0.5115 (0.5309)	loss 1.2168 (1.3307)	grad_norm 4.6236 (nan)	loss_scale 1024.0000 (1205.4448)	mem 12176MB
[2024-07-05 00:51:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:47 lr 0.000039	 wd 0.0000	time 0.4996 (0.5307)	loss 1.4818 (1.3313)	grad_norm 5.0636 (nan)	loss_scale 1024.0000 (1197.5593)	mem 12176MB
[2024-07-05 00:52:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:54 lr 0.000039	 wd 0.0000	time 0.5074 (0.5306)	loss 1.1269 (1.3308)	grad_norm 3.1770 (nan)	loss_scale 1024.0000 (1190.3307)	mem 12176MB
[2024-07-05 00:53:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000039	 wd 0.0000	time 0.5053 (0.5303)	loss 0.9504 (1.3312)	grad_norm 3.8731 (nan)	loss_scale 1024.0000 (1183.6801)	mem 12176MB
[2024-07-05 00:53:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 6 training takes 0:22:09
[2024-07-05 00:53:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.495 (10.495)	Loss 0.4133 (0.4133)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 00:54:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.626 Acc@5 97.440
[2024-07-05 00:54:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.6%
[2024-07-05 00:54:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-05 00:54:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:05:03 lr 0.000039	 wd 0.0000	time 11.6323 (11.6323)	loss 1.6307 (1.6307)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:55:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:25:26 lr 0.000039	 wd 0.0000	time 0.5092 (0.6354)	loss 1.3376 (1.3444)	grad_norm 5.1467 (4.7364)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:55:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:22:17 lr 0.000039	 wd 0.0000	time 0.5139 (0.5811)	loss 1.4979 (1.3234)	grad_norm 5.4116 (4.7831)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:56:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:20:39 lr 0.000039	 wd 0.0000	time 0.5056 (0.5627)	loss 1.2961 (1.3223)	grad_norm 9.4164 (4.8765)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:57:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:19:23 lr 0.000039	 wd 0.0000	time 0.5062 (0.5534)	loss 1.3919 (1.3259)	grad_norm 4.8873 (4.8866)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:58:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:18:16 lr 0.000039	 wd 0.0000	time 0.5098 (0.5479)	loss 1.4879 (1.3218)	grad_norm 4.7342 (5.0403)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 00:59:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:17:15 lr 0.000039	 wd 0.0000	time 0.5063 (0.5442)	loss 1.3613 (1.3253)	grad_norm 4.3495 (5.0778)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:00:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:16:15 lr 0.000039	 wd 0.0000	time 0.5029 (0.5415)	loss 1.3705 (1.3276)	grad_norm 2.9640 (5.0610)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:01:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:15:18 lr 0.000039	 wd 0.0000	time 0.5149 (0.5395)	loss 0.9937 (1.3295)	grad_norm 6.3307 (5.0923)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:02:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:14:22 lr 0.000039	 wd 0.0000	time 0.5060 (0.5381)	loss 1.1611 (1.3320)	grad_norm 4.0697 (5.0467)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:02:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:13:26 lr 0.000039	 wd 0.0000	time 0.5042 (0.5369)	loss 1.2230 (1.3289)	grad_norm 4.3584 (5.0917)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:03:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:12:31 lr 0.000039	 wd 0.0000	time 0.5022 (0.5359)	loss 1.3351 (1.3276)	grad_norm 7.8820 (5.0756)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:04:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:11:36 lr 0.000039	 wd 0.0000	time 0.5131 (0.5351)	loss 1.2756 (1.3249)	grad_norm 3.8534 (5.1139)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:05:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:10:42 lr 0.000039	 wd 0.0000	time 0.5060 (0.5344)	loss 1.4049 (1.3258)	grad_norm 3.5492 (5.1327)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:06:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:09:48 lr 0.000039	 wd 0.0000	time 0.5085 (0.5337)	loss 1.5140 (1.3262)	grad_norm 3.8804 (5.1605)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:07:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:08:54 lr 0.000039	 wd 0.0000	time 0.5097 (0.5332)	loss 1.4175 (1.3255)	grad_norm 4.7725 (5.1563)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:08:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:08:00 lr 0.000039	 wd 0.0000	time 0.5297 (0.5328)	loss 1.4472 (1.3268)	grad_norm 4.3015 (5.1597)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:09:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:07:06 lr 0.000039	 wd 0.0000	time 0.5073 (0.5324)	loss 1.5151 (1.3271)	grad_norm 3.9625 (5.1427)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:09:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:06:13 lr 0.000039	 wd 0.0000	time 0.5052 (0.5320)	loss 1.2783 (1.3267)	grad_norm 7.1197 (5.1252)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:10:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:05:20 lr 0.000039	 wd 0.0000	time 0.5066 (0.5317)	loss 1.2805 (1.3259)	grad_norm 3.8212 (5.0961)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:11:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:04:26 lr 0.000039	 wd 0.0000	time 0.5141 (0.5314)	loss 1.4188 (1.3265)	grad_norm 3.9480 (5.0730)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:12:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:03:33 lr 0.000039	 wd 0.0000	time 0.5027 (0.5312)	loss 1.5937 (1.3282)	grad_norm 4.4800 (5.0586)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:13:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:02:40 lr 0.000039	 wd 0.0000	time 0.5085 (0.5309)	loss 1.2202 (1.3267)	grad_norm 3.3137 (5.0336)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:14:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:47 lr 0.000039	 wd 0.0000	time 0.5164 (0.5307)	loss 1.4683 (1.3272)	grad_norm 3.8499 (5.0390)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:15:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:54 lr 0.000039	 wd 0.0000	time 0.5104 (0.5305)	loss 1.3603 (1.3269)	grad_norm 17.9559 (5.0545)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:16:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000039	 wd 0.0000	time 0.5112 (0.5302)	loss 1.2785 (1.3270)	grad_norm 4.2828 (5.0631)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:16:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 7 training takes 0:22:09
[2024-07-05 01:16:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.222 (10.222)	Loss 0.4050 (0.4050)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 01:16:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.504 Acc@5 97.416
[2024-07-05 01:16:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-05 01:16:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-05 01:16:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:49:56 lr 0.000039	 wd 0.0000	time 11.2694 (11.2694)	loss 1.5569 (1.5569)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 01:17:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:25:21 lr 0.000039	 wd 0.0000	time 0.5058 (0.6334)	loss 1.4321 (1.3510)	grad_norm 4.2116 (nan)	loss_scale 512.0000 (628.5941)	mem 12176MB
[2024-07-05 01:18:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:22:14 lr 0.000039	 wd 0.0000	time 0.5097 (0.5798)	loss 1.3841 (1.3270)	grad_norm 16.5499 (nan)	loss_scale 512.0000 (570.5871)	mem 12176MB
[2024-07-05 01:19:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:20:37 lr 0.000038	 wd 0.0000	time 0.5073 (0.5619)	loss 1.2960 (1.3207)	grad_norm 5.7393 (nan)	loss_scale 512.0000 (551.1229)	mem 12176MB
[2024-07-05 01:20:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:19:22 lr 0.000038	 wd 0.0000	time 0.5107 (0.5529)	loss 1.3932 (1.3190)	grad_norm 4.0171 (nan)	loss_scale 512.0000 (541.3666)	mem 12176MB
[2024-07-05 01:21:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:18:15 lr 0.000038	 wd 0.0000	time 0.5068 (0.5473)	loss 1.4934 (1.3259)	grad_norm 9.3992 (nan)	loss_scale 512.0000 (535.5050)	mem 12176MB
[2024-07-05 01:21:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:17:14 lr 0.000038	 wd 0.0000	time 0.5000 (0.5437)	loss 1.5189 (1.3300)	grad_norm 4.0533 (nan)	loss_scale 512.0000 (531.5940)	mem 12176MB
[2024-07-05 01:22:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:16:15 lr 0.000038	 wd 0.0000	time 0.5068 (0.5412)	loss 1.4966 (1.3307)	grad_norm 4.9992 (nan)	loss_scale 512.0000 (528.7989)	mem 12176MB
[2024-07-05 01:23:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:15:17 lr 0.000038	 wd 0.0000	time 0.5065 (0.5393)	loss 1.4020 (1.3302)	grad_norm 11.7284 (nan)	loss_scale 512.0000 (526.7016)	mem 12176MB
[2024-07-05 01:24:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:14:21 lr 0.000038	 wd 0.0000	time 0.5050 (0.5378)	loss 1.4806 (1.3309)	grad_norm 4.4286 (nan)	loss_scale 512.0000 (525.0699)	mem 12176MB
[2024-07-05 01:25:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:13:26 lr 0.000038	 wd 0.0000	time 0.5053 (0.5367)	loss 1.4072 (1.3285)	grad_norm 7.8863 (nan)	loss_scale 512.0000 (523.7642)	mem 12176MB
[2024-07-05 01:26:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:12:31 lr 0.000038	 wd 0.0000	time 0.5128 (0.5357)	loss 1.4533 (1.3254)	grad_norm 7.3922 (nan)	loss_scale 512.0000 (522.6957)	mem 12176MB
[2024-07-05 01:27:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:11:36 lr 0.000038	 wd 0.0000	time 0.5066 (0.5349)	loss 1.3817 (1.3247)	grad_norm 3.7304 (nan)	loss_scale 512.0000 (521.8052)	mem 12176MB
[2024-07-05 01:28:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:10:42 lr 0.000038	 wd 0.0000	time 0.5048 (0.5342)	loss 1.0368 (1.3227)	grad_norm 4.7492 (nan)	loss_scale 512.0000 (521.0515)	mem 12176MB
[2024-07-05 01:29:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:09:48 lr 0.000038	 wd 0.0000	time 0.5181 (0.5336)	loss 0.9096 (1.3234)	grad_norm 4.5974 (nan)	loss_scale 512.0000 (520.4054)	mem 12176MB
[2024-07-05 01:29:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:08:54 lr 0.000038	 wd 0.0000	time 0.5163 (0.5332)	loss 0.9301 (1.3201)	grad_norm 4.3651 (nan)	loss_scale 512.0000 (519.8454)	mem 12176MB
[2024-07-05 01:30:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:08:00 lr 0.000038	 wd 0.0000	time 0.5087 (0.5327)	loss 1.2824 (1.3174)	grad_norm 4.2087 (nan)	loss_scale 512.0000 (519.3554)	mem 12176MB
[2024-07-05 01:31:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:07:06 lr 0.000038	 wd 0.0000	time 0.5120 (0.5323)	loss 1.1707 (1.3187)	grad_norm 4.0054 (nan)	loss_scale 512.0000 (518.9230)	mem 12176MB
[2024-07-05 01:32:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:06:13 lr 0.000038	 wd 0.0000	time 0.5006 (0.5320)	loss 1.4546 (1.3206)	grad_norm 5.7413 (nan)	loss_scale 512.0000 (518.5386)	mem 12176MB
[2024-07-05 01:33:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:05:20 lr 0.000038	 wd 0.0000	time 0.5115 (0.5317)	loss 1.2215 (1.3208)	grad_norm 3.9266 (nan)	loss_scale 512.0000 (518.1946)	mem 12176MB
[2024-07-05 01:34:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:04:26 lr 0.000038	 wd 0.0000	time 0.5076 (0.5314)	loss 1.7015 (1.3228)	grad_norm 3.9475 (nan)	loss_scale 512.0000 (517.8851)	mem 12176MB
[2024-07-05 01:35:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:03:33 lr 0.000038	 wd 0.0000	time 0.5022 (0.5312)	loss 1.3112 (1.3228)	grad_norm 5.2556 (nan)	loss_scale 512.0000 (517.6050)	mem 12176MB
[2024-07-05 01:36:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:02:40 lr 0.000038	 wd 0.0000	time 0.5074 (0.5310)	loss 1.4354 (1.3218)	grad_norm 4.5379 (nan)	loss_scale 512.0000 (517.3503)	mem 12176MB
[2024-07-05 01:36:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:47 lr 0.000038	 wd 0.0000	time 0.5081 (0.5308)	loss 1.3403 (1.3224)	grad_norm 4.1669 (nan)	loss_scale 512.0000 (517.1178)	mem 12176MB
[2024-07-05 01:37:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:54 lr 0.000038	 wd 0.0000	time 0.5022 (0.5306)	loss 1.4233 (1.3242)	grad_norm 9.6683 (nan)	loss_scale 512.0000 (516.9046)	mem 12176MB
[2024-07-05 01:38:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000038	 wd 0.0000	time 0.5026 (0.5303)	loss 1.5443 (1.3255)	grad_norm 4.1546 (nan)	loss_scale 512.0000 (516.7085)	mem 12176MB
[2024-07-05 01:38:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 8 training takes 0:22:09
[2024-07-05 01:38:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.405 (11.405)	Loss 0.4136 (0.4136)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 01:39:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.656 Acc@5 97.380
[2024-07-05 01:39:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-05 01:39:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-05 01:39:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:59:04 lr 0.000038	 wd 0.0000	time 11.4887 (11.4887)	loss 1.3472 (1.3472)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:40:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:25:25 lr 0.000038	 wd 0.0000	time 0.5052 (0.6350)	loss 1.2632 (1.2954)	grad_norm 7.8706 (4.9611)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:41:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:22:17 lr 0.000037	 wd 0.0000	time 0.5102 (0.5809)	loss 1.4796 (1.3006)	grad_norm 3.5765 (4.6254)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:41:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:20:39 lr 0.000037	 wd 0.0000	time 0.5124 (0.5628)	loss 1.3948 (1.3115)	grad_norm 8.6680 (4.6841)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:42:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:19:23 lr 0.000037	 wd 0.0000	time 0.5114 (0.5534)	loss 1.5648 (1.3078)	grad_norm 3.3244 (4.7018)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:43:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:18:16 lr 0.000037	 wd 0.0000	time 0.5045 (0.5479)	loss 1.2901 (1.3139)	grad_norm 7.0785 (4.7208)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:44:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:17:14 lr 0.000037	 wd 0.0000	time 0.5063 (0.5441)	loss 1.3392 (1.3083)	grad_norm 5.6907 (4.9545)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:45:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:16:16 lr 0.000037	 wd 0.0000	time 0.5146 (0.5417)	loss 1.5068 (1.3089)	grad_norm 4.1954 (4.9850)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:46:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:15:18 lr 0.000037	 wd 0.0000	time 0.5064 (0.5397)	loss 1.3480 (1.3135)	grad_norm 2.9335 (5.0329)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:47:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:14:22 lr 0.000037	 wd 0.0000	time 0.5072 (0.5382)	loss 1.5095 (1.3110)	grad_norm 5.8114 (5.0945)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:48:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:13:26 lr 0.000037	 wd 0.0000	time 0.5089 (0.5370)	loss 1.4497 (1.3134)	grad_norm 4.2088 (5.1115)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:48:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:12:31 lr 0.000037	 wd 0.0000	time 0.5120 (0.5360)	loss 1.2864 (1.3173)	grad_norm 3.6268 (5.0589)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:49:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:11:36 lr 0.000037	 wd 0.0000	time 0.5082 (0.5352)	loss 1.5584 (1.3197)	grad_norm 2.7289 (5.0760)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:50:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:10:42 lr 0.000037	 wd 0.0000	time 0.5099 (0.5344)	loss 0.9340 (1.3168)	grad_norm 11.6592 (5.0603)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:51:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:09:48 lr 0.000037	 wd 0.0000	time 0.5090 (0.5339)	loss 1.3709 (1.3190)	grad_norm 3.7178 (5.0392)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:52:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:08:54 lr 0.000037	 wd 0.0000	time 0.5075 (0.5334)	loss 1.4922 (1.3212)	grad_norm 4.2866 (5.0528)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 01:53:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:08:00 lr 0.000037	 wd 0.0000	time 0.5102 (0.5330)	loss 0.9406 (1.3210)	grad_norm 3.5663 (5.0533)	loss_scale 1024.0000 (537.5840)	mem 12176MB
[2024-07-05 01:54:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:07:07 lr 0.000037	 wd 0.0000	time 0.5083 (0.5326)	loss 1.0486 (1.3225)	grad_norm 4.6098 (5.0416)	loss_scale 1024.0000 (566.1799)	mem 12176MB
[2024-07-05 01:55:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:06:13 lr 0.000037	 wd 0.0000	time 0.5040 (0.5322)	loss 1.4535 (1.3231)	grad_norm 5.0456 (5.0210)	loss_scale 1024.0000 (591.6002)	mem 12176MB
[2024-07-05 01:55:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:05:20 lr 0.000037	 wd 0.0000	time 0.5073 (0.5319)	loss 1.3541 (1.3222)	grad_norm 3.8198 (5.0429)	loss_scale 1024.0000 (614.3461)	mem 12176MB
[2024-07-05 01:56:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:04:26 lr 0.000037	 wd 0.0000	time 0.5125 (0.5317)	loss 1.4824 (1.3215)	grad_norm 11.1977 (5.0668)	loss_scale 1024.0000 (634.8186)	mem 12176MB
[2024-07-05 01:57:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:03:33 lr 0.000036	 wd 0.0000	time 0.5032 (0.5314)	loss 1.1239 (1.3222)	grad_norm 4.8724 (5.0535)	loss_scale 1024.0000 (653.3422)	mem 12176MB
[2024-07-05 01:58:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:40 lr 0.000036	 wd 0.0000	time 0.5132 (0.5312)	loss 1.4190 (1.3236)	grad_norm 3.9150 (5.0510)	loss_scale 1024.0000 (670.1826)	mem 12176MB
[2024-07-05 01:59:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:47 lr 0.000036	 wd 0.0000	time 0.5053 (0.5310)	loss 1.3657 (1.3243)	grad_norm 3.7400 (5.0644)	loss_scale 1024.0000 (685.5593)	mem 12176MB
[2024-07-05 02:00:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:54 lr 0.000036	 wd 0.0000	time 0.5088 (0.5307)	loss 1.2306 (1.3241)	grad_norm 4.4954 (5.0361)	loss_scale 1024.0000 (699.6551)	mem 12176MB
[2024-07-05 02:01:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000036	 wd 0.0000	time 0.5109 (0.5305)	loss 0.8708 (1.3251)	grad_norm 3.5852 (5.0286)	loss_scale 1024.0000 (712.6238)	mem 12176MB
[2024-07-05 02:01:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 9 training takes 0:22:09
[2024-07-05 02:01:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.113 (11.113)	Loss 0.3867 (0.3867)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 02:01:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.700 Acc@5 97.410
[2024-07-05 02:01:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-05 02:01:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.72%
[2024-07-05 02:01:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:19:26 lr 0.000036	 wd 0.0000	time 10.5383 (10.5383)	loss 1.2623 (1.2623)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:02:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:25:10 lr 0.000036	 wd 0.0000	time 0.5047 (0.6287)	loss 1.1757 (1.2852)	grad_norm 3.7705 (5.1243)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:03:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:22:09 lr 0.000036	 wd 0.0000	time 0.5098 (0.5774)	loss 1.3553 (1.3132)	grad_norm 7.2604 (4.9352)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:04:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:20:33 lr 0.000036	 wd 0.0000	time 0.5084 (0.5601)	loss 0.8802 (1.3155)	grad_norm 3.7095 (5.0718)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:05:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:19:18 lr 0.000036	 wd 0.0000	time 0.4992 (0.5512)	loss 1.4907 (1.3201)	grad_norm 4.8444 (5.0407)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:06:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:18:12 lr 0.000036	 wd 0.0000	time 0.5021 (0.5459)	loss 1.4746 (1.3178)	grad_norm 4.2618 (5.0028)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:07:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:17:12 lr 0.000036	 wd 0.0000	time 0.5013 (0.5426)	loss 1.5742 (1.3160)	grad_norm 9.8670 (4.9742)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:07:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:16:13 lr 0.000036	 wd 0.0000	time 0.5074 (0.5402)	loss 1.4069 (1.3134)	grad_norm 6.9298 (4.9911)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:08:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:15:16 lr 0.000036	 wd 0.0000	time 0.5091 (0.5384)	loss 1.2708 (1.3146)	grad_norm 7.6822 (5.0026)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:09:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:14:20 lr 0.000036	 wd 0.0000	time 0.5089 (0.5370)	loss 1.4342 (1.3148)	grad_norm 5.6903 (4.9643)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:10:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:13:25 lr 0.000036	 wd 0.0000	time 0.5105 (0.5360)	loss 1.5251 (1.3128)	grad_norm 4.3003 (4.9517)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:11:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:12:30 lr 0.000036	 wd 0.0000	time 0.5136 (0.5350)	loss 1.3256 (1.3107)	grad_norm 5.9516 (4.9926)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 02:12:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:11:35 lr 0.000035	 wd 0.0000	time 0.5151 (0.5343)	loss 1.2112 (1.3110)	grad_norm 4.3770 (nan)	loss_scale 512.0000 (1022.2948)	mem 12176MB
[2024-07-05 02:13:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:10:41 lr 0.000035	 wd 0.0000	time 0.5101 (0.5337)	loss 1.5619 (1.3139)	grad_norm 3.5145 (nan)	loss_scale 512.0000 (983.0715)	mem 12176MB
[2024-07-05 02:14:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:09:47 lr 0.000035	 wd 0.0000	time 0.4993 (0.5331)	loss 1.3847 (1.3161)	grad_norm 4.9293 (nan)	loss_scale 512.0000 (949.4475)	mem 12176MB
[2024-07-05 02:14:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:08:53 lr 0.000035	 wd 0.0000	time 0.5095 (0.5326)	loss 1.2276 (1.3164)	grad_norm 3.8439 (nan)	loss_scale 512.0000 (920.3038)	mem 12176MB
[2024-07-05 02:15:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:08:00 lr 0.000035	 wd 0.0000	time 0.5129 (0.5322)	loss 1.1684 (1.3177)	grad_norm 3.5212 (nan)	loss_scale 512.0000 (894.8007)	mem 12176MB
[2024-07-05 02:16:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:07:06 lr 0.000035	 wd 0.0000	time 0.5109 (0.5319)	loss 1.3751 (1.3185)	grad_norm 6.2229 (nan)	loss_scale 512.0000 (872.2963)	mem 12176MB
[2024-07-05 02:17:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:06:13 lr 0.000035	 wd 0.0000	time 0.4996 (0.5315)	loss 1.3374 (1.3187)	grad_norm 5.0806 (nan)	loss_scale 512.0000 (852.2909)	mem 12176MB
[2024-07-05 02:18:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:05:19 lr 0.000035	 wd 0.0000	time 0.4886 (0.5313)	loss 1.3913 (1.3196)	grad_norm 44.0171 (nan)	loss_scale 512.0000 (834.3903)	mem 12176MB
[2024-07-05 02:19:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:04:26 lr 0.000035	 wd 0.0000	time 0.4917 (0.5310)	loss 1.2809 (1.3197)	grad_norm 4.0485 (nan)	loss_scale 512.0000 (818.2789)	mem 12176MB
[2024-07-05 02:20:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:03:33 lr 0.000035	 wd 0.0000	time 0.5100 (0.5307)	loss 1.4489 (1.3211)	grad_norm 22.6049 (nan)	loss_scale 512.0000 (803.7011)	mem 12176MB
[2024-07-05 02:21:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:02:40 lr 0.000035	 wd 0.0000	time 0.5127 (0.5305)	loss 0.9512 (1.3215)	grad_norm 4.3959 (nan)	loss_scale 512.0000 (790.4480)	mem 12176MB
[2024-07-05 02:21:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:47 lr 0.000035	 wd 0.0000	time 0.5112 (0.5303)	loss 1.4325 (1.3208)	grad_norm 5.1185 (nan)	loss_scale 512.0000 (778.3468)	mem 12176MB
[2024-07-05 02:22:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:54 lr 0.000035	 wd 0.0000	time 0.5172 (0.5300)	loss 1.3728 (1.3209)	grad_norm 6.5520 (nan)	loss_scale 512.0000 (767.2536)	mem 12176MB
[2024-07-05 02:23:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000035	 wd 0.0000	time 0.5117 (0.5298)	loss 1.4924 (1.3204)	grad_norm 6.1831 (nan)	loss_scale 512.0000 (757.0476)	mem 12176MB
[2024-07-05 02:23:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 10 training takes 0:22:08
[2024-07-05 02:23:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.005 (11.005)	Loss 0.4045 (0.4045)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 02:24:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.782 Acc@5 97.440
[2024-07-05 02:24:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-05 02:24:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.78%
[2024-07-05 02:24:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 02:24:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 02:24:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:04:04 lr 0.000035	 wd 0.0000	time 10.1697 (10.1697)	loss 1.5108 (1.5108)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:25:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:24:54 lr 0.000035	 wd 0.0000	time 0.5059 (0.6221)	loss 1.2855 (1.3130)	grad_norm 7.1687 (5.0878)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:26:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:22:00 lr 0.000034	 wd 0.0000	time 0.5009 (0.5737)	loss 1.5186 (1.3164)	grad_norm 5.6393 (4.8222)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:27:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:20:28 lr 0.000034	 wd 0.0000	time 0.5155 (0.5578)	loss 1.1759 (1.3171)	grad_norm 3.7772 (4.7662)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:27:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:19:15 lr 0.000034	 wd 0.0000	time 0.5010 (0.5498)	loss 1.4324 (1.3214)	grad_norm 4.1701 (4.7961)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:28:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:18:11 lr 0.000034	 wd 0.0000	time 0.5039 (0.5451)	loss 1.2851 (1.3145)	grad_norm 4.3191 (4.7771)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:29:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:17:10 lr 0.000034	 wd 0.0000	time 0.4972 (0.5418)	loss 1.3839 (1.3134)	grad_norm 3.9165 (4.7338)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:30:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:16:12 lr 0.000034	 wd 0.0000	time 0.5101 (0.5396)	loss 1.3513 (1.3138)	grad_norm 4.0265 (4.8284)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:31:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:15:15 lr 0.000034	 wd 0.0000	time 0.5050 (0.5379)	loss 1.6177 (1.3167)	grad_norm 7.4007 (4.8468)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:32:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:14:19 lr 0.000034	 wd 0.0000	time 0.5094 (0.5366)	loss 1.4499 (1.3171)	grad_norm 3.8319 (4.8636)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:33:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:13:24 lr 0.000034	 wd 0.0000	time 0.5040 (0.5357)	loss 1.4181 (1.3171)	grad_norm 4.5857 (4.8495)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:34:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:12:29 lr 0.000034	 wd 0.0000	time 0.5151 (0.5348)	loss 1.3779 (1.3192)	grad_norm 4.6467 (4.9160)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:34:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:11:35 lr 0.000034	 wd 0.0000	time 0.5119 (0.5340)	loss 1.5419 (1.3197)	grad_norm 3.7802 (4.9100)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:35:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:10:41 lr 0.000034	 wd 0.0000	time 0.5102 (0.5334)	loss 1.0397 (1.3182)	grad_norm 3.3945 (4.9515)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:36:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:09:47 lr 0.000034	 wd 0.0000	time 0.5016 (0.5330)	loss 0.9518 (1.3178)	grad_norm 4.4953 (4.9382)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:37:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:08:53 lr 0.000034	 wd 0.0000	time 0.5109 (0.5325)	loss 1.4137 (1.3170)	grad_norm 4.2888 (4.9145)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:38:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:07:59 lr 0.000034	 wd 0.0000	time 0.5049 (0.5321)	loss 1.2426 (1.3181)	grad_norm 4.0603 (4.9641)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:39:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:07:06 lr 0.000033	 wd 0.0000	time 0.5123 (0.5318)	loss 1.3577 (1.3169)	grad_norm 3.7438 (4.9679)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:40:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:06:13 lr 0.000033	 wd 0.0000	time 0.5078 (0.5314)	loss 1.2825 (1.3183)	grad_norm 4.0025 (4.9956)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:41:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:05:19 lr 0.000033	 wd 0.0000	time 0.5066 (0.5311)	loss 1.5141 (1.3181)	grad_norm 4.5369 (4.9855)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:41:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:04:26 lr 0.000033	 wd 0.0000	time 0.5024 (0.5309)	loss 1.1601 (1.3179)	grad_norm 3.9587 (5.0015)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:42:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:03:33 lr 0.000033	 wd 0.0000	time 0.5078 (0.5306)	loss 1.3830 (1.3170)	grad_norm 3.6602 (5.1332)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:43:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:40 lr 0.000033	 wd 0.0000	time 0.5119 (0.5304)	loss 1.3727 (1.3185)	grad_norm 4.1778 (5.1686)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:44:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:47 lr 0.000033	 wd 0.0000	time 0.5189 (0.5302)	loss 1.4043 (1.3180)	grad_norm 6.3900 (5.1494)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:45:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:54 lr 0.000033	 wd 0.0000	time 0.5072 (0.5301)	loss 1.3166 (1.3172)	grad_norm 4.6843 (5.1456)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:46:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000033	 wd 0.0000	time 0.5148 (0.5298)	loss 1.3691 (1.3174)	grad_norm 4.7175 (5.1659)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:46:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 11 training takes 0:22:08
[2024-07-05 02:46:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.343 (11.343)	Loss 0.4033 (0.4033)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 02:46:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.666 Acc@5 97.432
[2024-07-05 02:46:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-05 02:46:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.78%
[2024-07-05 02:46:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:10:52 lr 0.000033	 wd 0.0000	time 11.7718 (11.7718)	loss 1.5929 (1.5929)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:47:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:25:32 lr 0.000033	 wd 0.0000	time 0.5111 (0.6380)	loss 1.4997 (1.3138)	grad_norm 4.9385 (5.6390)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 02:48:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:22:20 lr 0.000033	 wd 0.0000	time 0.5026 (0.5824)	loss 0.8815 (1.3195)	grad_norm 3.1808 (5.4816)	loss_scale 1024.0000 (532.3781)	mem 12176MB
[2024-07-05 02:49:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:20:40 lr 0.000033	 wd 0.0000	time 0.5171 (0.5635)	loss 1.2753 (1.3192)	grad_norm 4.4384 (5.4967)	loss_scale 1024.0000 (695.7076)	mem 12176MB
[2024-07-05 02:50:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:19:25 lr 0.000033	 wd 0.0000	time 0.5063 (0.5544)	loss 1.2823 (1.3131)	grad_norm 2.2028 (5.3633)	loss_scale 1024.0000 (777.5761)	mem 12176MB
[2024-07-05 02:51:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:18:18 lr 0.000032	 wd 0.0000	time 0.4940 (0.5486)	loss 1.0312 (1.3152)	grad_norm 3.3407 (5.4226)	loss_scale 1024.0000 (826.7625)	mem 12176MB
[2024-07-05 02:52:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:17:16 lr 0.000032	 wd 0.0000	time 0.5185 (0.5450)	loss 1.3294 (1.3120)	grad_norm 4.8253 (5.4451)	loss_scale 1024.0000 (859.5807)	mem 12176MB
[2024-07-05 02:53:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:16:17 lr 0.000032	 wd 0.0000	time 0.5141 (0.5423)	loss 1.4090 (1.3120)	grad_norm 3.9445 (5.4341)	loss_scale 1024.0000 (883.0357)	mem 12176MB
[2024-07-05 02:53:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:15:19 lr 0.000032	 wd 0.0000	time 0.5094 (0.5402)	loss 1.6343 (1.3207)	grad_norm 4.3941 (5.3987)	loss_scale 1024.0000 (900.6342)	mem 12176MB
[2024-07-05 02:54:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:14:22 lr 0.000032	 wd 0.0000	time 0.4950 (0.5386)	loss 1.0322 (1.3224)	grad_norm 5.7037 (5.3586)	loss_scale 1024.0000 (914.3263)	mem 12176MB
[2024-07-05 02:55:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:13:27 lr 0.000032	 wd 0.0000	time 0.5138 (0.5374)	loss 1.3423 (1.3209)	grad_norm 7.3384 (5.3235)	loss_scale 1024.0000 (925.2827)	mem 12176MB
[2024-07-05 02:56:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:12:31 lr 0.000032	 wd 0.0000	time 0.5068 (0.5363)	loss 1.4489 (1.3257)	grad_norm 3.2523 (5.3839)	loss_scale 1024.0000 (934.2489)	mem 12176MB
[2024-07-05 02:57:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:11:37 lr 0.000032	 wd 0.0000	time 0.5080 (0.5354)	loss 1.5214 (1.3241)	grad_norm 3.7792 (5.3335)	loss_scale 1024.0000 (941.7219)	mem 12176MB
[2024-07-05 02:58:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:10:42 lr 0.000032	 wd 0.0000	time 0.5086 (0.5346)	loss 1.5359 (1.3243)	grad_norm 4.3633 (5.3035)	loss_scale 1024.0000 (948.0461)	mem 12176MB
[2024-07-05 02:59:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:09:48 lr 0.000032	 wd 0.0000	time 0.4993 (0.5339)	loss 1.4989 (1.3243)	grad_norm 3.9780 (5.2976)	loss_scale 1024.0000 (953.4675)	mem 12176MB
[2024-07-05 03:00:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:08:54 lr 0.000032	 wd 0.0000	time 0.5050 (0.5333)	loss 1.4056 (1.3250)	grad_norm 3.7666 (5.3121)	loss_scale 1024.0000 (958.1666)	mem 12176MB
[2024-07-05 03:00:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:08:00 lr 0.000032	 wd 0.0000	time 0.5097 (0.5329)	loss 1.4455 (1.3234)	grad_norm 3.6758 (5.2854)	loss_scale 1024.0000 (962.2786)	mem 12176MB
[2024-07-05 03:01:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:07:07 lr 0.000031	 wd 0.0000	time 0.5059 (0.5324)	loss 1.3819 (1.3245)	grad_norm 4.0598 (5.2687)	loss_scale 1024.0000 (965.9071)	mem 12176MB
[2024-07-05 03:02:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:06:13 lr 0.000031	 wd 0.0000	time 0.5149 (0.5320)	loss 1.2974 (1.3252)	grad_norm 4.4123 (5.2634)	loss_scale 1024.0000 (969.1327)	mem 12176MB
[2024-07-05 03:03:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:05:20 lr 0.000031	 wd 0.0000	time 0.5102 (0.5317)	loss 1.2781 (1.3249)	grad_norm 9.0971 (5.2455)	loss_scale 1024.0000 (972.0189)	mem 12176MB
[2024-07-05 03:04:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:04:26 lr 0.000031	 wd 0.0000	time 0.5123 (0.5315)	loss 1.3894 (1.3244)	grad_norm 3.3957 (5.2638)	loss_scale 1024.0000 (974.6167)	mem 12176MB
[2024-07-05 03:05:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:03:33 lr 0.000031	 wd 0.0000	time 0.5035 (0.5313)	loss 1.4719 (1.3250)	grad_norm 4.0743 (5.2773)	loss_scale 1024.0000 (976.9672)	mem 12176MB
[2024-07-05 03:06:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:02:40 lr 0.000031	 wd 0.0000	time 0.5283 (0.5310)	loss 1.4278 (1.3256)	grad_norm 4.2104 (5.2926)	loss_scale 1024.0000 (979.1040)	mem 12176MB
[2024-07-05 03:07:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:47 lr 0.000031	 wd 0.0000	time 0.5019 (0.5308)	loss 1.3451 (1.3252)	grad_norm 4.7968 (5.2855)	loss_scale 1024.0000 (981.0552)	mem 12176MB
[2024-07-05 03:07:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:54 lr 0.000031	 wd 0.0000	time 0.5093 (0.5306)	loss 1.5144 (1.3249)	grad_norm 4.5057 (inf)	loss_scale 512.0000 (979.0054)	mem 12176MB
[2024-07-05 03:08:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000031	 wd 0.0000	time 0.5184 (0.5303)	loss 1.0593 (1.3233)	grad_norm 3.0241 (inf)	loss_scale 512.0000 (960.3327)	mem 12176MB
[2024-07-05 03:08:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 12 training takes 0:22:09
[2024-07-05 03:09:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.968 (10.968)	Loss 0.3994 (0.3994)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 03:09:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.788 Acc@5 97.450
[2024-07-05 03:09:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-05 03:09:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.79%
[2024-07-05 03:09:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 03:09:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 03:09:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:08:17 lr 0.000031	 wd 0.0000	time 10.2709 (10.2709)	loss 1.2989 (1.2989)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:10:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:24:56 lr 0.000031	 wd 0.0000	time 0.5160 (0.6230)	loss 1.4552 (1.3119)	grad_norm 7.3250 (4.9210)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:11:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:22:04 lr 0.000031	 wd 0.0000	time 0.5113 (0.5752)	loss 1.5437 (1.3159)	grad_norm 4.4691 (4.8665)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:12:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:20:30 lr 0.000031	 wd 0.0000	time 0.5111 (0.5590)	loss 1.5029 (1.3215)	grad_norm 3.7271 (4.9662)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:13:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:19:18 lr 0.000030	 wd 0.0000	time 0.5103 (0.5509)	loss 1.3916 (1.3193)	grad_norm 4.3162 (4.9719)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:13:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:18:13 lr 0.000030	 wd 0.0000	time 0.5093 (0.5461)	loss 1.4744 (1.3142)	grad_norm 3.7502 (5.0455)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:14:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:17:12 lr 0.000030	 wd 0.0000	time 0.5022 (0.5428)	loss 1.4670 (1.3183)	grad_norm 4.2443 (5.1702)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:15:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:16:13 lr 0.000030	 wd 0.0000	time 0.5035 (0.5405)	loss 1.6437 (1.3144)	grad_norm 4.8516 (5.1591)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:16:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:15:17 lr 0.000030	 wd 0.0000	time 0.5163 (0.5388)	loss 1.5035 (1.3173)	grad_norm 5.1691 (5.1390)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:17:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:14:20 lr 0.000030	 wd 0.0000	time 0.5075 (0.5374)	loss 1.5306 (1.3131)	grad_norm 5.4769 (5.1618)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:18:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:13:25 lr 0.000030	 wd 0.0000	time 0.5016 (0.5364)	loss 1.4468 (1.3157)	grad_norm 4.6765 (5.1243)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:19:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:12:30 lr 0.000030	 wd 0.0000	time 0.5089 (0.5353)	loss 1.4205 (1.3175)	grad_norm 4.3795 (5.2320)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:20:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:11:35 lr 0.000030	 wd 0.0000	time 0.5067 (0.5345)	loss 1.6232 (1.3148)	grad_norm 3.7854 (5.2149)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:20:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:10:41 lr 0.000030	 wd 0.0000	time 0.5117 (0.5339)	loss 1.4873 (1.3142)	grad_norm 4.6632 (5.1991)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:21:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:09:47 lr 0.000030	 wd 0.0000	time 0.5125 (0.5334)	loss 1.5602 (1.3148)	grad_norm 3.2438 (5.1739)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:22:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:08:53 lr 0.000030	 wd 0.0000	time 0.5074 (0.5329)	loss 1.4904 (1.3142)	grad_norm 5.2211 (5.1456)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:23:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:08:00 lr 0.000029	 wd 0.0000	time 0.5167 (0.5325)	loss 1.4498 (1.3138)	grad_norm 6.5985 (5.1109)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:24:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:07:06 lr 0.000029	 wd 0.0000	time 0.5119 (0.5322)	loss 0.8832 (1.3114)	grad_norm 4.5622 (5.0831)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:25:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:06:13 lr 0.000029	 wd 0.0000	time 0.5113 (0.5318)	loss 1.3180 (1.3128)	grad_norm 5.2230 (5.1046)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:26:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:05:19 lr 0.000029	 wd 0.0000	time 0.5220 (0.5315)	loss 1.1208 (1.3128)	grad_norm 7.8003 (5.1179)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:27:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:04:26 lr 0.000029	 wd 0.0000	time 0.5066 (0.5312)	loss 1.3386 (1.3134)	grad_norm 4.5335 (5.0889)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:27:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:03:33 lr 0.000029	 wd 0.0000	time 0.5062 (0.5310)	loss 1.0839 (1.3130)	grad_norm 5.5605 (5.1017)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:28:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:40 lr 0.000029	 wd 0.0000	time 0.5085 (0.5308)	loss 1.5009 (1.3148)	grad_norm 3.3216 (5.1069)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:29:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:47 lr 0.000029	 wd 0.0000	time 0.5132 (0.5306)	loss 1.3291 (1.3138)	grad_norm 5.5915 (5.1024)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:30:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:54 lr 0.000029	 wd 0.0000	time 0.5122 (0.5304)	loss 1.4920 (1.3145)	grad_norm 4.4196 (5.0930)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:31:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000029	 wd 0.0000	time 0.5059 (0.5302)	loss 1.4154 (1.3140)	grad_norm 4.3059 (5.1399)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:31:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 13 training takes 0:22:09
[2024-07-05 03:31:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.356 (10.356)	Loss 0.3989 (0.3989)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 12176MB
[2024-07-05 03:31:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.670 Acc@5 97.490
[2024-07-05 03:31:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-05 03:31:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.79%
[2024-07-05 03:32:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:49:26 lr 0.000029	 wd 0.0000	time 11.2577 (11.2577)	loss 1.5169 (1.5169)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:32:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:25:20 lr 0.000029	 wd 0.0000	time 0.5093 (0.6332)	loss 1.4946 (1.3086)	grad_norm 4.0316 (4.8205)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:33:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:22:15 lr 0.000028	 wd 0.0000	time 0.5072 (0.5801)	loss 1.2149 (1.3323)	grad_norm 3.1528 (4.8356)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:34:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:20:37 lr 0.000028	 wd 0.0000	time 0.5105 (0.5622)	loss 1.0866 (1.3202)	grad_norm 4.3178 (4.9303)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:35:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:19:22 lr 0.000028	 wd 0.0000	time 0.5089 (0.5530)	loss 1.1177 (1.3079)	grad_norm 3.7699 (4.9895)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:36:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:18:16 lr 0.000028	 wd 0.0000	time 0.5092 (0.5476)	loss 1.4015 (1.3089)	grad_norm 4.0403 (4.9787)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:37:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:17:14 lr 0.000028	 wd 0.0000	time 0.5026 (0.5440)	loss 1.5766 (1.3127)	grad_norm 6.6550 (5.2007)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:38:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:16:16 lr 0.000028	 wd 0.0000	time 0.5077 (0.5416)	loss 1.2926 (1.3089)	grad_norm 8.7769 (5.1683)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:39:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:15:18 lr 0.000028	 wd 0.0000	time 0.5038 (0.5397)	loss 1.2892 (1.3060)	grad_norm 3.5276 (5.2634)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:39:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:14:22 lr 0.000028	 wd 0.0000	time 0.5207 (0.5382)	loss 1.5191 (1.3041)	grad_norm 4.1796 (5.2706)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:40:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:13:26 lr 0.000028	 wd 0.0000	time 0.5094 (0.5370)	loss 1.0825 (1.3030)	grad_norm 5.5652 (5.2333)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:41:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:12:31 lr 0.000028	 wd 0.0000	time 0.5171 (0.5360)	loss 1.3200 (1.3006)	grad_norm 3.6392 (5.2633)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:42:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:11:36 lr 0.000028	 wd 0.0000	time 0.5105 (0.5351)	loss 1.4746 (1.3008)	grad_norm 4.6558 (5.2129)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:43:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:10:42 lr 0.000027	 wd 0.0000	time 0.5136 (0.5345)	loss 1.1692 (1.3020)	grad_norm 3.8388 (5.2178)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 03:44:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:09:48 lr 0.000027	 wd 0.0000	time 0.5012 (0.5339)	loss 1.3938 (1.3015)	grad_norm 4.0390 (5.2143)	loss_scale 1024.0000 (520.0400)	mem 12176MB
[2024-07-05 03:45:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:08:54 lr 0.000027	 wd 0.0000	time 0.4992 (0.5334)	loss 1.3859 (1.3023)	grad_norm 3.1602 (5.1622)	loss_scale 1024.0000 (553.6149)	mem 12176MB
[2024-07-05 03:46:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:08:00 lr 0.000027	 wd 0.0000	time 0.5059 (0.5329)	loss 1.3666 (1.3032)	grad_norm 6.9178 (5.1494)	loss_scale 1024.0000 (582.9956)	mem 12176MB
[2024-07-05 03:46:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:07:07 lr 0.000027	 wd 0.0000	time 0.5074 (0.5325)	loss 1.1592 (1.3035)	grad_norm 3.5448 (5.1650)	loss_scale 1024.0000 (608.9218)	mem 12176MB
[2024-07-05 03:47:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:06:13 lr 0.000027	 wd 0.0000	time 0.5137 (0.5322)	loss 1.2572 (1.3029)	grad_norm 4.6472 (5.1391)	loss_scale 1024.0000 (631.9689)	mem 12176MB
[2024-07-05 03:48:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:05:20 lr 0.000027	 wd 0.0000	time 0.5201 (0.5319)	loss 1.3435 (1.3029)	grad_norm 4.7514 (5.1392)	loss_scale 1024.0000 (652.5913)	mem 12176MB
[2024-07-05 03:49:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:04:26 lr 0.000027	 wd 0.0000	time 0.5054 (0.5316)	loss 0.9980 (1.3045)	grad_norm 4.4560 (5.1226)	loss_scale 1024.0000 (671.1524)	mem 12176MB
[2024-07-05 03:50:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:03:33 lr 0.000027	 wd 0.0000	time 0.5154 (0.5313)	loss 1.5697 (1.3038)	grad_norm 4.6190 (5.1432)	loss_scale 1024.0000 (687.9467)	mem 12176MB
[2024-07-05 03:51:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:40 lr 0.000027	 wd 0.0000	time 0.5095 (0.5311)	loss 1.2230 (1.3042)	grad_norm 5.0558 (5.1364)	loss_scale 1024.0000 (703.2149)	mem 12176MB
[2024-07-05 03:52:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:47 lr 0.000027	 wd 0.0000	time 0.5088 (0.5309)	loss 1.3200 (1.3053)	grad_norm 3.9059 (5.1065)	loss_scale 1024.0000 (717.1560)	mem 12176MB
[2024-07-05 03:53:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:54 lr 0.000026	 wd 0.0000	time 0.5093 (0.5307)	loss 1.2278 (1.3063)	grad_norm 7.2488 (5.1058)	loss_scale 1024.0000 (729.9359)	mem 12176MB
[2024-07-05 03:53:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000026	 wd 0.0000	time 0.5073 (0.5304)	loss 1.3537 (1.3056)	grad_norm 4.9740 (5.1017)	loss_scale 1024.0000 (741.6937)	mem 12176MB
[2024-07-05 03:54:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 14 training takes 0:22:09
[2024-07-05 03:54:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.961 (10.961)	Loss 0.4014 (0.4014)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 12176MB
[2024-07-05 03:54:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.816 Acc@5 97.486
[2024-07-05 03:54:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.8%
[2024-07-05 03:54:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.82%
[2024-07-05 03:54:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 03:54:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 03:54:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:11:00 lr 0.000026	 wd 0.0000	time 10.3361 (10.3361)	loss 1.2995 (1.2995)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 03:55:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:24:56 lr 0.000026	 wd 0.0000	time 0.5049 (0.6231)	loss 1.0241 (1.2949)	grad_norm 3.9141 (5.0172)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 03:56:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:22:03 lr 0.000026	 wd 0.0000	time 0.4992 (0.5748)	loss 1.4164 (1.3060)	grad_norm 3.9653 (5.0037)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 03:57:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:20:29 lr 0.000026	 wd 0.0000	time 0.5060 (0.5585)	loss 1.3174 (1.2930)	grad_norm 2.7375 (5.2017)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 03:58:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:19:16 lr 0.000026	 wd 0.0000	time 0.5093 (0.5502)	loss 1.3103 (1.2966)	grad_norm 5.0091 (5.0581)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 03:59:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:18:11 lr 0.000026	 wd 0.0000	time 0.5128 (0.5454)	loss 0.8554 (1.2910)	grad_norm 3.3917 (5.0673)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 03:59:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:17:11 lr 0.000026	 wd 0.0000	time 0.5093 (0.5422)	loss 0.8405 (1.2945)	grad_norm 4.1066 (5.0356)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:00:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:16:12 lr 0.000026	 wd 0.0000	time 0.5112 (0.5399)	loss 1.1508 (1.2977)	grad_norm 5.9325 (5.0027)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:01:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:15:15 lr 0.000026	 wd 0.0000	time 0.5115 (0.5381)	loss 1.5094 (1.2989)	grad_norm 9.8059 (5.0130)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:02:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:14:19 lr 0.000025	 wd 0.0000	time 0.5120 (0.5367)	loss 1.1312 (1.2985)	grad_norm 4.1142 (4.9983)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:03:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:13:24 lr 0.000025	 wd 0.0000	time 0.5088 (0.5357)	loss 1.3103 (1.2986)	grad_norm 2.6368 (5.0047)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:04:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:12:29 lr 0.000025	 wd 0.0000	time 0.5045 (0.5348)	loss 1.3008 (1.2955)	grad_norm 3.7196 (5.0426)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:05:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:11:35 lr 0.000025	 wd 0.0000	time 0.5047 (0.5341)	loss 1.5277 (1.2953)	grad_norm 4.4827 (5.0211)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:06:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:10:41 lr 0.000025	 wd 0.0000	time 0.5142 (0.5335)	loss 1.3013 (1.2942)	grad_norm 3.2655 (5.0333)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:06:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:09:47 lr 0.000025	 wd 0.0000	time 0.5057 (0.5329)	loss 1.2708 (1.2950)	grad_norm 3.2578 (5.0335)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:07:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:08:53 lr 0.000025	 wd 0.0000	time 0.5106 (0.5325)	loss 1.6009 (1.2945)	grad_norm 5.2853 (5.0152)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:08:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:07:59 lr 0.000025	 wd 0.0000	time 0.5064 (0.5321)	loss 1.1955 (1.2972)	grad_norm 5.0267 (4.9825)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:09:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:07:06 lr 0.000025	 wd 0.0000	time 0.4992 (0.5317)	loss 1.5169 (1.2975)	grad_norm 4.5510 (4.9689)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:10:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:06:12 lr 0.000025	 wd 0.0000	time 0.5087 (0.5313)	loss 1.0049 (1.2972)	grad_norm 4.4397 (4.9629)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:11:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:05:19 lr 0.000024	 wd 0.0000	time 0.5038 (0.5309)	loss 1.5616 (1.2981)	grad_norm 3.4442 (4.9642)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:12:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:04:26 lr 0.000024	 wd 0.0000	time 0.5076 (0.5307)	loss 1.6553 (1.3007)	grad_norm 3.8676 (4.9868)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:13:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:03:33 lr 0.000024	 wd 0.0000	time 0.5099 (0.5305)	loss 1.5643 (1.3013)	grad_norm 3.3055 (4.9768)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:13:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:40 lr 0.000024	 wd 0.0000	time 0.5085 (0.5303)	loss 0.8770 (1.3022)	grad_norm 4.0553 (4.9918)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:14:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:47 lr 0.000024	 wd 0.0000	time 0.5024 (0.5301)	loss 1.4626 (1.3024)	grad_norm 4.0597 (4.9824)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:15:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:54 lr 0.000024	 wd 0.0000	time 0.4997 (0.5300)	loss 1.1525 (1.3015)	grad_norm 3.8129 (4.9749)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:16:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000024	 wd 0.0000	time 0.4855 (0.5297)	loss 1.3551 (1.3018)	grad_norm 10.3126 (4.9821)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:16:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 15 training takes 0:22:07
[2024-07-05 04:16:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_15.pth saving......
[2024-07-05 04:16:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_15.pth saved !!!
[2024-07-05 04:16:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 9.406 (9.406)	Loss 0.3916 (0.3916)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 04:16:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.894 Acc@5 97.502
[2024-07-05 04:16:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-05 04:16:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.89%
[2024-07-05 04:16:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 04:17:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 04:17:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:28:16 lr 0.000024	 wd 0.0000	time 10.7500 (10.7500)	loss 1.1429 (1.1429)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:18:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:25:06 lr 0.000024	 wd 0.0000	time 0.5115 (0.6271)	loss 1.4320 (1.3046)	grad_norm 3.4532 (5.7992)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:18:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:22:08 lr 0.000024	 wd 0.0000	time 0.5088 (0.5770)	loss 1.2644 (1.3113)	grad_norm 3.7845 (5.5584)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:19:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:20:33 lr 0.000024	 wd 0.0000	time 0.5095 (0.5600)	loss 1.2963 (1.3186)	grad_norm 3.8031 (5.4036)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 04:20:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:19:18 lr 0.000024	 wd 0.0000	time 0.4980 (0.5513)	loss 1.1575 (1.3132)	grad_norm 3.7047 (5.3258)	loss_scale 2048.0000 (1090.3940)	mem 12176MB
[2024-07-05 04:21:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:18:13 lr 0.000023	 wd 0.0000	time 0.5083 (0.5463)	loss 1.3205 (1.3209)	grad_norm 3.9779 (5.3925)	loss_scale 2048.0000 (1281.5329)	mem 12176MB
[2024-07-05 04:22:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:17:12 lr 0.000023	 wd 0.0000	time 0.5065 (0.5429)	loss 1.0439 (1.3126)	grad_norm 3.6972 (5.3324)	loss_scale 2048.0000 (1409.0649)	mem 12176MB
[2024-07-05 04:23:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:16:13 lr 0.000023	 wd 0.0000	time 0.5106 (0.5404)	loss 1.2696 (1.3149)	grad_norm 5.9068 (5.4075)	loss_scale 2048.0000 (1500.2111)	mem 12176MB
[2024-07-05 04:24:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:15:16 lr 0.000023	 wd 0.0000	time 0.5147 (0.5385)	loss 1.2766 (1.3087)	grad_norm 3.2998 (5.4115)	loss_scale 2048.0000 (1568.5993)	mem 12176MB
[2024-07-05 04:25:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:14:20 lr 0.000023	 wd 0.0000	time 0.5058 (0.5370)	loss 1.2879 (1.3084)	grad_norm 2.9245 (nan)	loss_scale 1024.0000 (1528.6127)	mem 12176MB
[2024-07-05 04:25:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:13:24 lr 0.000023	 wd 0.0000	time 0.5108 (0.5359)	loss 1.4447 (1.3087)	grad_norm 6.2293 (nan)	loss_scale 1024.0000 (1478.2018)	mem 12176MB
[2024-07-05 04:26:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:12:30 lr 0.000023	 wd 0.0000	time 0.5138 (0.5350)	loss 1.2782 (1.3107)	grad_norm 4.4688 (nan)	loss_scale 1024.0000 (1436.9482)	mem 12176MB
[2024-07-05 04:27:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:11:35 lr 0.000023	 wd 0.0000	time 0.5070 (0.5343)	loss 1.4311 (1.3099)	grad_norm 5.3986 (nan)	loss_scale 1024.0000 (1402.5645)	mem 12176MB
[2024-07-05 04:28:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:10:41 lr 0.000023	 wd 0.0000	time 0.5032 (0.5336)	loss 1.0525 (1.3101)	grad_norm 4.5406 (nan)	loss_scale 512.0000 (1360.8732)	mem 12176MB
[2024-07-05 04:29:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:09:47 lr 0.000023	 wd 0.0000	time 0.5202 (0.5331)	loss 1.2422 (1.3076)	grad_norm 3.8974 (nan)	loss_scale 512.0000 (1300.2827)	mem 12176MB
[2024-07-05 04:30:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:08:53 lr 0.000022	 wd 0.0000	time 0.4908 (0.5326)	loss 1.3727 (1.3050)	grad_norm 4.1462 (nan)	loss_scale 512.0000 (1247.7655)	mem 12176MB
[2024-07-05 04:31:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:08:00 lr 0.000022	 wd 0.0000	time 0.5093 (0.5322)	loss 1.3718 (1.3044)	grad_norm 4.9299 (nan)	loss_scale 512.0000 (1201.8089)	mem 12176MB
[2024-07-05 04:32:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:07:06 lr 0.000022	 wd 0.0000	time 0.5098 (0.5319)	loss 1.0535 (1.3030)	grad_norm 6.0882 (nan)	loss_scale 512.0000 (1161.2557)	mem 12176MB
[2024-07-05 04:32:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:06:13 lr 0.000022	 wd 0.0000	time 0.5104 (0.5315)	loss 1.0754 (1.3020)	grad_norm 4.2218 (nan)	loss_scale 512.0000 (1125.2060)	mem 12176MB
[2024-07-05 04:33:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:05:19 lr 0.000022	 wd 0.0000	time 0.5076 (0.5312)	loss 1.2338 (1.3013)	grad_norm 6.1859 (nan)	loss_scale 512.0000 (1092.9490)	mem 12176MB
[2024-07-05 04:34:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:04:26 lr 0.000022	 wd 0.0000	time 0.5068 (0.5309)	loss 1.4952 (1.3013)	grad_norm 3.6293 (nan)	loss_scale 512.0000 (1063.9160)	mem 12176MB
[2024-07-05 04:35:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:03:33 lr 0.000022	 wd 0.0000	time 0.5014 (0.5307)	loss 1.3699 (1.3009)	grad_norm 3.6627 (nan)	loss_scale 512.0000 (1037.6468)	mem 12176MB
[2024-07-05 04:36:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:02:40 lr 0.000022	 wd 0.0000	time 0.5117 (0.5305)	loss 1.2863 (1.3016)	grad_norm 3.5867 (nan)	loss_scale 512.0000 (1013.7647)	mem 12176MB
[2024-07-05 04:37:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:47 lr 0.000022	 wd 0.0000	time 0.5122 (0.5303)	loss 1.2500 (1.3019)	grad_norm 4.1825 (nan)	loss_scale 256.0000 (986.6180)	mem 12176MB
[2024-07-05 04:38:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:54 lr 0.000022	 wd 0.0000	time 0.5120 (0.5301)	loss 1.2756 (1.3018)	grad_norm 5.9145 (nan)	loss_scale 256.0000 (956.1883)	mem 12176MB
[2024-07-05 04:39:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000021	 wd 0.0000	time 0.5040 (0.5299)	loss 1.3696 (1.3016)	grad_norm 3.3296 (nan)	loss_scale 256.0000 (928.1919)	mem 12176MB
[2024-07-05 04:39:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 16 training takes 0:22:08
[2024-07-05 04:39:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.310 (11.310)	Loss 0.4001 (0.4001)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 04:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.928 Acc@5 97.506
[2024-07-05 04:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-05 04:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.93%
[2024-07-05 04:39:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 04:39:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 04:39:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][0/2502]	eta 6:48:44 lr 0.000021	 wd 0.0000	time 9.8019 (9.8019)	loss 1.3721 (1.3721)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:40:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:24:45 lr 0.000021	 wd 0.0000	time 0.5124 (0.6184)	loss 1.5287 (1.2795)	grad_norm 3.4398 (4.8940)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:41:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:21:57 lr 0.000021	 wd 0.0000	time 0.5097 (0.5724)	loss 1.4500 (1.2923)	grad_norm 4.1934 (4.8494)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:42:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:20:26 lr 0.000021	 wd 0.0000	time 0.5100 (0.5569)	loss 1.3777 (1.2895)	grad_norm 5.5450 (4.8877)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:43:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:19:14 lr 0.000021	 wd 0.0000	time 0.5072 (0.5492)	loss 1.0876 (1.2885)	grad_norm 3.4552 (4.8586)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:44:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:18:10 lr 0.000021	 wd 0.0000	time 0.5021 (0.5446)	loss 1.3043 (1.2860)	grad_norm 5.9649 (4.9346)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:44:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:17:10 lr 0.000021	 wd 0.0000	time 0.5053 (0.5416)	loss 1.1543 (1.2915)	grad_norm 4.7836 (5.0113)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:45:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:16:11 lr 0.000021	 wd 0.0000	time 0.5099 (0.5393)	loss 1.3001 (1.2952)	grad_norm 5.4592 (5.0284)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:46:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:15:15 lr 0.000021	 wd 0.0000	time 0.5037 (0.5376)	loss 1.2472 (1.2985)	grad_norm 5.6852 (5.0149)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:47:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:14:19 lr 0.000021	 wd 0.0000	time 0.5066 (0.5363)	loss 1.3989 (1.3009)	grad_norm 6.3939 (5.0451)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:48:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:13:23 lr 0.000020	 wd 0.0000	time 0.5061 (0.5353)	loss 1.4519 (1.2993)	grad_norm 6.8015 (5.0473)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:49:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:12:29 lr 0.000020	 wd 0.0000	time 0.5114 (0.5345)	loss 1.0846 (1.2975)	grad_norm 5.1039 (5.0140)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:50:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:11:34 lr 0.000020	 wd 0.0000	time 0.4985 (0.5338)	loss 1.3745 (1.2996)	grad_norm 3.0212 (5.0182)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:51:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:10:41 lr 0.000020	 wd 0.0000	time 0.5063 (0.5333)	loss 1.2267 (1.2997)	grad_norm 4.8041 (5.1419)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:52:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:09:47 lr 0.000020	 wd 0.0000	time 0.5063 (0.5328)	loss 1.4188 (1.3002)	grad_norm 6.8739 (5.1017)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:52:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:08:53 lr 0.000020	 wd 0.0000	time 0.5020 (0.5323)	loss 1.4621 (1.3010)	grad_norm 4.8750 (5.0930)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:53:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:07:59 lr 0.000020	 wd 0.0000	time 0.5055 (0.5320)	loss 1.3787 (1.3022)	grad_norm 3.4039 (5.0761)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:54:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:07:06 lr 0.000020	 wd 0.0000	time 0.5110 (0.5317)	loss 1.5273 (1.3033)	grad_norm 4.9636 (5.1126)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:55:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:06:13 lr 0.000020	 wd 0.0000	time 0.5062 (0.5314)	loss 1.3708 (1.3040)	grad_norm 5.7889 (5.1859)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:56:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:05:19 lr 0.000020	 wd 0.0000	time 0.5092 (0.5312)	loss 1.1963 (1.3045)	grad_norm 3.2472 (5.1732)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:57:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:04:26 lr 0.000019	 wd 0.0000	time 0.5179 (0.5309)	loss 1.3759 (1.3022)	grad_norm 4.4897 (5.1462)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:58:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:03:33 lr 0.000019	 wd 0.0000	time 0.5050 (0.5307)	loss 1.4934 (1.3021)	grad_norm 3.5811 (5.1280)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:59:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:40 lr 0.000019	 wd 0.0000	time 0.5128 (0.5306)	loss 1.2810 (1.3026)	grad_norm 3.3310 (5.1892)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 04:59:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:47 lr 0.000019	 wd 0.0000	time 0.5115 (0.5304)	loss 1.5375 (1.3020)	grad_norm 4.2089 (5.1796)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:00:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:54 lr 0.000019	 wd 0.0000	time 0.5079 (0.5302)	loss 1.3768 (1.3001)	grad_norm 3.6753 (5.1792)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:01:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0000	time 0.5128 (0.5300)	loss 1.2748 (1.3007)	grad_norm 4.2055 (5.1767)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:01:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 17 training takes 0:22:08
[2024-07-05 05:01:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.281 (11.281)	Loss 0.3896 (0.3896)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 05:02:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.906 Acc@5 97.552
[2024-07-05 05:02:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-05 05:02:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.93%
[2024-07-05 05:02:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:31:12 lr 0.000019	 wd 0.0000	time 10.8204 (10.8204)	loss 1.5137 (1.5137)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:03:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:25:13 lr 0.000019	 wd 0.0000	time 0.5112 (0.6303)	loss 1.4933 (1.3334)	grad_norm 4.1873 (5.0971)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:04:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:22:11 lr 0.000019	 wd 0.0000	time 0.5072 (0.5782)	loss 1.4834 (1.3126)	grad_norm 4.7828 (4.9524)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:04:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:20:35 lr 0.000019	 wd 0.0000	time 0.5156 (0.5612)	loss 1.4584 (1.3095)	grad_norm 8.0556 (5.0307)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:05:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:19:20 lr 0.000019	 wd 0.0000	time 0.5076 (0.5523)	loss 1.3770 (1.3069)	grad_norm 5.9836 (5.0160)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:06:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:18:15 lr 0.000018	 wd 0.0000	time 0.5055 (0.5471)	loss 1.1919 (1.3006)	grad_norm 5.3452 (5.0189)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:07:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:17:13 lr 0.000018	 wd 0.0000	time 0.5111 (0.5435)	loss 1.5195 (1.2993)	grad_norm 4.4247 (5.0179)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:08:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:16:14 lr 0.000018	 wd 0.0000	time 0.5063 (0.5411)	loss 1.5278 (1.3004)	grad_norm 4.6562 (4.9934)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:09:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:15:17 lr 0.000018	 wd 0.0000	time 0.5090 (0.5391)	loss 1.0568 (1.2974)	grad_norm 3.2928 (5.0246)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:10:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:14:21 lr 0.000018	 wd 0.0000	time 0.5080 (0.5377)	loss 1.3319 (1.2945)	grad_norm 5.4729 (5.0130)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:11:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:13:26 lr 0.000018	 wd 0.0000	time 0.5261 (0.5366)	loss 1.0892 (1.2983)	grad_norm 5.1832 (5.0007)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:11:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:12:31 lr 0.000018	 wd 0.0000	time 0.5162 (0.5358)	loss 1.2698 (1.2969)	grad_norm 4.8937 (5.0520)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:12:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:11:36 lr 0.000018	 wd 0.0000	time 0.5161 (0.5350)	loss 1.4398 (1.2973)	grad_norm 9.4464 (5.0398)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 05:13:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:10:42 lr 0.000018	 wd 0.0000	time 0.5129 (0.5343)	loss 1.4945 (1.2980)	grad_norm 5.1956 (5.0339)	loss_scale 512.0000 (266.2321)	mem 12176MB
[2024-07-05 05:14:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:09:48 lr 0.000018	 wd 0.0000	time 0.5067 (0.5337)	loss 1.1157 (1.2970)	grad_norm 4.9078 (5.0457)	loss_scale 512.0000 (283.7744)	mem 12176MB
[2024-07-05 05:15:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:08:54 lr 0.000017	 wd 0.0000	time 0.5107 (0.5332)	loss 1.4788 (1.2954)	grad_norm 4.1867 (5.0768)	loss_scale 512.0000 (298.9793)	mem 12176MB
[2024-07-05 05:16:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:08:00 lr 0.000017	 wd 0.0000	time 0.5113 (0.5328)	loss 1.5453 (1.2955)	grad_norm 3.7430 (5.0715)	loss_scale 512.0000 (312.2848)	mem 12176MB
[2024-07-05 05:17:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:07:07 lr 0.000017	 wd 0.0000	time 0.5249 (0.5325)	loss 1.4211 (1.2944)	grad_norm 4.8540 (5.0639)	loss_scale 512.0000 (324.0259)	mem 12176MB
[2024-07-05 05:18:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:06:13 lr 0.000017	 wd 0.0000	time 0.5060 (0.5321)	loss 1.3510 (1.2957)	grad_norm 5.7153 (5.0652)	loss_scale 512.0000 (334.4631)	mem 12176MB
[2024-07-05 05:18:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:05:20 lr 0.000017	 wd 0.0000	time 0.5119 (0.5319)	loss 1.4807 (1.2957)	grad_norm 4.5686 (5.0747)	loss_scale 512.0000 (343.8022)	mem 12176MB
[2024-07-05 05:19:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:04:26 lr 0.000017	 wd 0.0000	time 0.5002 (0.5316)	loss 1.3348 (1.2975)	grad_norm 3.6369 (5.0773)	loss_scale 512.0000 (352.2079)	mem 12176MB
[2024-07-05 05:20:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:03:33 lr 0.000017	 wd 0.0000	time 0.5238 (0.5313)	loss 1.2998 (1.2970)	grad_norm 3.5607 (5.0543)	loss_scale 512.0000 (359.8134)	mem 12176MB
[2024-07-05 05:21:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:02:40 lr 0.000017	 wd 0.0000	time 0.5172 (0.5311)	loss 1.3850 (1.2968)	grad_norm 3.5416 (5.0609)	loss_scale 512.0000 (366.7279)	mem 12176MB
[2024-07-05 05:22:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:47 lr 0.000017	 wd 0.0000	time 0.5145 (0.5309)	loss 1.4022 (1.2968)	grad_norm 6.5391 (5.0632)	loss_scale 512.0000 (373.0413)	mem 12176MB
[2024-07-05 05:23:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:54 lr 0.000017	 wd 0.0000	time 0.5117 (0.5308)	loss 1.4892 (1.2984)	grad_norm 6.0852 (5.0676)	loss_scale 512.0000 (378.8288)	mem 12176MB
[2024-07-05 05:24:12 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0000	time 0.5168 (0.5306)	loss 1.4711 (1.2982)	grad_norm 5.8101 (5.0904)	loss_scale 512.0000 (384.1535)	mem 12176MB
[2024-07-05 05:24:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 18 training takes 0:22:10
[2024-07-05 05:24:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.456 (11.456)	Loss 0.3857 (0.3857)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 05:24:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.864 Acc@5 97.560
[2024-07-05 05:24:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-05 05:24:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.93%
[2024-07-05 05:24:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:46:24 lr 0.000016	 wd 0.0000	time 11.1849 (11.1849)	loss 1.3999 (1.3999)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:25:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:25:20 lr 0.000016	 wd 0.0000	time 0.5129 (0.6329)	loss 1.0579 (1.3030)	grad_norm 4.2592 (5.4553)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:26:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:22:14 lr 0.000016	 wd 0.0000	time 0.5096 (0.5796)	loss 1.5029 (1.3000)	grad_norm 9.3124 (5.7462)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:27:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:20:37 lr 0.000016	 wd 0.0000	time 0.5090 (0.5619)	loss 1.2931 (1.3095)	grad_norm 7.2239 (5.5258)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:28:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:19:22 lr 0.000016	 wd 0.0000	time 0.5074 (0.5532)	loss 1.4734 (1.3091)	grad_norm 5.0206 (5.3420)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:29:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:18:16 lr 0.000016	 wd 0.0000	time 0.5076 (0.5477)	loss 1.5771 (1.3069)	grad_norm 4.5853 (5.2726)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:30:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:17:14 lr 0.000016	 wd 0.0000	time 0.5104 (0.5441)	loss 1.5114 (1.3021)	grad_norm 3.5995 (5.2855)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:30:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:16:16 lr 0.000016	 wd 0.0000	time 0.5078 (0.5416)	loss 1.5481 (1.3035)	grad_norm 5.2232 (5.2701)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:31:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:15:18 lr 0.000016	 wd 0.0000	time 0.4959 (0.5396)	loss 1.2503 (1.3003)	grad_norm 4.1684 (5.3267)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:32:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:14:22 lr 0.000016	 wd 0.0000	time 0.5109 (0.5382)	loss 1.3706 (1.3020)	grad_norm 3.8764 (5.3273)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:33:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:13:26 lr 0.000016	 wd 0.0000	time 0.5101 (0.5369)	loss 1.4430 (1.3046)	grad_norm 4.9341 (5.3104)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:34:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:12:31 lr 0.000015	 wd 0.0000	time 0.5029 (0.5359)	loss 0.8351 (1.3030)	grad_norm 4.9577 (5.2958)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:35:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:11:36 lr 0.000015	 wd 0.0000	time 0.5007 (0.5351)	loss 1.0749 (1.3005)	grad_norm 3.3944 (5.3145)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:36:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:10:42 lr 0.000015	 wd 0.0000	time 0.5256 (0.5345)	loss 1.3862 (1.3004)	grad_norm 5.3429 (5.3007)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:37:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:09:48 lr 0.000015	 wd 0.0000	time 0.5272 (0.5338)	loss 1.3470 (1.3018)	grad_norm 4.3125 (5.3201)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:37:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:08:54 lr 0.000015	 wd 0.0000	time 0.5126 (0.5333)	loss 1.2149 (1.3016)	grad_norm 4.3651 (5.3791)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:38:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:08:00 lr 0.000015	 wd 0.0000	time 0.4953 (0.5329)	loss 1.2953 (1.3023)	grad_norm 7.0016 (5.3737)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:39:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:07:07 lr 0.000015	 wd 0.0000	time 0.5092 (0.5325)	loss 1.4505 (1.3029)	grad_norm 3.3852 (5.3531)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:40:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:06:13 lr 0.000015	 wd 0.0000	time 0.5164 (0.5322)	loss 1.2887 (1.3027)	grad_norm 5.7275 (5.3702)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:41:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:05:20 lr 0.000015	 wd 0.0000	time 0.4866 (0.5320)	loss 1.3059 (1.3029)	grad_norm 4.3225 (5.3386)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:42:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:04:26 lr 0.000015	 wd 0.0000	time 0.5044 (0.5317)	loss 0.9209 (1.3022)	grad_norm 5.2993 (5.3360)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:43:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:03:33 lr 0.000014	 wd 0.0000	time 0.5106 (0.5315)	loss 1.1210 (1.3016)	grad_norm 4.6378 (5.3104)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:44:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:02:40 lr 0.000014	 wd 0.0000	time 0.5044 (0.5312)	loss 1.4113 (1.3000)	grad_norm 6.6355 (5.3132)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:45:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:47 lr 0.000014	 wd 0.0000	time 0.5165 (0.5310)	loss 1.3739 (1.2994)	grad_norm 5.5310 (5.3285)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:45:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:54 lr 0.000014	 wd 0.0000	time 0.5134 (0.5308)	loss 1.4941 (1.3005)	grad_norm 3.7987 (5.3394)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:46:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0000	time 0.5036 (0.5306)	loss 1.2037 (1.3004)	grad_norm 5.8090 (5.3282)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:46:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 19 training takes 0:22:10
[2024-07-05 05:47:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.906 (10.906)	Loss 0.3877 (0.3877)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 05:47:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.994 Acc@5 97.516
[2024-07-05 05:47:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-05 05:47:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.99%
[2024-07-05 05:47:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 05:47:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 05:47:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][0/2502]	eta 6:36:51 lr 0.000014	 wd 0.0000	time 9.5170 (9.5170)	loss 1.0248 (1.0248)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:48:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:24:35 lr 0.000014	 wd 0.0000	time 0.4939 (0.6142)	loss 1.6450 (1.2920)	grad_norm 5.4275 (5.4619)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:49:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:21:52 lr 0.000014	 wd 0.0000	time 0.5036 (0.5703)	loss 1.3354 (1.2968)	grad_norm 6.3873 (5.2482)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 05:50:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:20:23 lr 0.000014	 wd 0.0000	time 0.5133 (0.5557)	loss 1.3771 (1.2968)	grad_norm 5.6462 (5.3377)	loss_scale 1024.0000 (607.2558)	mem 12176MB
[2024-07-05 05:50:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:19:12 lr 0.000014	 wd 0.0000	time 0.5128 (0.5482)	loss 1.4607 (1.3041)	grad_norm 3.6439 (5.3449)	loss_scale 1024.0000 (711.1820)	mem 12176MB
[2024-07-05 05:51:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:18:09 lr 0.000014	 wd 0.0000	time 0.5065 (0.5441)	loss 1.3076 (1.3028)	grad_norm 4.3082 (5.5271)	loss_scale 1024.0000 (773.6208)	mem 12176MB
[2024-07-05 05:52:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:17:08 lr 0.000014	 wd 0.0000	time 0.5100 (0.5410)	loss 1.3940 (1.2979)	grad_norm 4.1557 (5.4804)	loss_scale 1024.0000 (815.2812)	mem 12176MB
[2024-07-05 05:53:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:16:10 lr 0.000013	 wd 0.0000	time 0.5070 (0.5388)	loss 1.4057 (1.3037)	grad_norm 3.5071 (5.5225)	loss_scale 1024.0000 (845.0556)	mem 12176MB
[2024-07-05 05:54:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:15:14 lr 0.000013	 wd 0.0000	time 0.5135 (0.5372)	loss 1.2303 (1.3009)	grad_norm 3.1675 (5.4561)	loss_scale 1024.0000 (867.3958)	mem 12176MB
[2024-07-05 05:55:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:14:18 lr 0.000013	 wd 0.0000	time 0.5107 (0.5359)	loss 1.5778 (1.3038)	grad_norm 3.3427 (5.3947)	loss_scale 1024.0000 (884.7769)	mem 12176MB
[2024-07-05 05:56:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:13:23 lr 0.000013	 wd 0.0000	time 0.5102 (0.5350)	loss 1.2964 (1.3033)	grad_norm 4.4251 (5.3716)	loss_scale 1024.0000 (898.6853)	mem 12176MB
[2024-07-05 05:57:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:12:28 lr 0.000013	 wd 0.0000	time 0.5165 (0.5341)	loss 1.3752 (1.3009)	grad_norm 9.3608 (5.4535)	loss_scale 1024.0000 (910.0672)	mem 12176MB
[2024-07-05 05:57:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:11:34 lr 0.000013	 wd 0.0000	time 0.5086 (0.5335)	loss 1.3639 (1.2981)	grad_norm 7.1387 (5.4615)	loss_scale 1024.0000 (919.5537)	mem 12176MB
[2024-07-05 05:58:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:10:40 lr 0.000013	 wd 0.0000	time 0.5117 (0.5329)	loss 1.1367 (1.2946)	grad_norm 5.7419 (5.4323)	loss_scale 1024.0000 (927.5819)	mem 12176MB
[2024-07-05 05:59:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:09:46 lr 0.000013	 wd 0.0000	time 0.5055 (0.5324)	loss 1.2992 (1.2962)	grad_norm 4.5565 (5.4538)	loss_scale 1024.0000 (934.4640)	mem 12176MB
[2024-07-05 06:00:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:08:53 lr 0.000013	 wd 0.0000	time 0.5077 (0.5321)	loss 1.2018 (1.2976)	grad_norm 4.8085 (5.4456)	loss_scale 1024.0000 (940.4290)	mem 12176MB
[2024-07-05 06:01:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:07:59 lr 0.000013	 wd 0.0000	time 0.5006 (0.5317)	loss 1.4900 (1.2983)	grad_norm 4.2375 (5.4176)	loss_scale 1024.0000 (945.6490)	mem 12176MB
[2024-07-05 06:02:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:07:06 lr 0.000012	 wd 0.0000	time 0.5020 (0.5314)	loss 1.5089 (1.2978)	grad_norm 3.6009 (5.4020)	loss_scale 1024.0000 (950.2551)	mem 12176MB
[2024-07-05 06:03:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:06:12 lr 0.000012	 wd 0.0000	time 0.5126 (0.5311)	loss 1.2336 (1.2969)	grad_norm 5.4511 (5.3834)	loss_scale 1024.0000 (954.3498)	mem 12176MB
[2024-07-05 06:04:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:05:19 lr 0.000012	 wd 0.0000	time 0.5185 (0.5309)	loss 1.5622 (1.2978)	grad_norm 2.8379 (5.4022)	loss_scale 1024.0000 (958.0137)	mem 12176MB
[2024-07-05 06:04:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:04:26 lr 0.000012	 wd 0.0000	time 0.5049 (0.5307)	loss 1.4029 (1.2973)	grad_norm 4.9981 (nan)	loss_scale 512.0000 (951.0765)	mem 12176MB
[2024-07-05 06:05:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:03:33 lr 0.000012	 wd 0.0000	time 0.5046 (0.5305)	loss 1.3819 (1.2980)	grad_norm 3.9598 (nan)	loss_scale 512.0000 (930.1780)	mem 12176MB
[2024-07-05 06:06:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:02:40 lr 0.000012	 wd 0.0000	time 0.5052 (0.5303)	loss 1.2269 (1.2991)	grad_norm 6.4644 (nan)	loss_scale 512.0000 (911.1786)	mem 12176MB
[2024-07-05 06:07:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:47 lr 0.000012	 wd 0.0000	time 0.5052 (0.5301)	loss 1.2233 (1.2994)	grad_norm 3.7457 (nan)	loss_scale 512.0000 (893.8305)	mem 12176MB
[2024-07-05 06:08:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:54 lr 0.000012	 wd 0.0000	time 0.5087 (0.5299)	loss 1.1749 (1.2997)	grad_norm 5.5951 (nan)	loss_scale 512.0000 (877.9275)	mem 12176MB
[2024-07-05 06:09:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0000	time 0.5095 (0.5297)	loss 1.4389 (1.3003)	grad_norm 4.3520 (nan)	loss_scale 512.0000 (863.2963)	mem 12176MB
[2024-07-05 06:09:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 20 training takes 0:22:08
[2024-07-05 06:09:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.456 (11.456)	Loss 0.3975 (0.3975)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 06:09:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.988 Acc@5 97.562
[2024-07-05 06:09:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-05 06:09:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.99%
[2024-07-05 06:10:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:16:02 lr 0.000012	 wd 0.0000	time 11.8956 (11.8956)	loss 1.4872 (1.4872)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:10:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:25:34 lr 0.000012	 wd 0.0000	time 0.5042 (0.6388)	loss 1.5483 (1.3342)	grad_norm 3.6230 (5.0682)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:11:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:22:21 lr 0.000012	 wd 0.0000	time 0.5072 (0.5826)	loss 0.9327 (1.3030)	grad_norm 3.9697 (5.6048)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:12:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:20:42 lr 0.000012	 wd 0.0000	time 0.5103 (0.5642)	loss 1.3452 (1.2955)	grad_norm 4.4436 (5.4298)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:13:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:19:26 lr 0.000011	 wd 0.0000	time 0.5202 (0.5548)	loss 1.2763 (1.2980)	grad_norm 5.4435 (5.3036)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:14:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:18:19 lr 0.000011	 wd 0.0000	time 0.5175 (0.5492)	loss 1.2624 (1.2961)	grad_norm 5.0409 (5.2437)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:15:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:17:17 lr 0.000011	 wd 0.0000	time 0.5073 (0.5453)	loss 1.1700 (1.2976)	grad_norm 5.3361 (5.3092)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:16:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:16:17 lr 0.000011	 wd 0.0000	time 0.5142 (0.5427)	loss 1.2300 (1.2919)	grad_norm 4.1438 (5.2992)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:17:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:15:20 lr 0.000011	 wd 0.0000	time 0.5050 (0.5408)	loss 1.2249 (1.2903)	grad_norm 3.7546 (5.2608)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:17:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:14:23 lr 0.000011	 wd 0.0000	time 0.5048 (0.5391)	loss 1.5059 (1.2893)	grad_norm 3.5655 (5.2642)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:18:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:13:27 lr 0.000011	 wd 0.0000	time 0.5059 (0.5379)	loss 1.0468 (1.2885)	grad_norm 4.9057 (5.2555)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:19:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:12:32 lr 0.000011	 wd 0.0000	time 0.5122 (0.5368)	loss 1.3839 (1.2886)	grad_norm 5.4423 (5.2793)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:20:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:11:37 lr 0.000011	 wd 0.0000	time 0.5054 (0.5360)	loss 1.3328 (1.2886)	grad_norm 6.2793 (5.2704)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:21:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:10:43 lr 0.000011	 wd 0.0000	time 0.5150 (0.5352)	loss 1.4919 (1.2880)	grad_norm 5.4756 (5.3500)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:22:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:09:49 lr 0.000011	 wd 0.0000	time 0.5127 (0.5346)	loss 1.2424 (1.2887)	grad_norm 7.4311 (5.3733)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:23:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:08:55 lr 0.000010	 wd 0.0000	time 0.5028 (0.5341)	loss 1.5837 (1.2884)	grad_norm 3.9854 (5.3832)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:24:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:08:01 lr 0.000010	 wd 0.0000	time 0.5150 (0.5336)	loss 1.2178 (1.2880)	grad_norm 4.5512 (5.3971)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:24:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:07:07 lr 0.000010	 wd 0.0000	time 0.5061 (0.5332)	loss 1.4154 (1.2886)	grad_norm 5.2331 (5.3630)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:25:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:06:14 lr 0.000010	 wd 0.0000	time 0.5111 (0.5328)	loss 1.0296 (1.2896)	grad_norm 4.2941 (5.3740)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:26:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:05:20 lr 0.000010	 wd 0.0000	time 0.5139 (0.5325)	loss 1.0431 (1.2879)	grad_norm 5.5204 (5.3620)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:27:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:04:27 lr 0.000010	 wd 0.0000	time 0.5081 (0.5322)	loss 1.3254 (1.2876)	grad_norm 6.7568 (5.3565)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:28:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:03:33 lr 0.000010	 wd 0.0000	time 0.5117 (0.5319)	loss 1.1329 (1.2891)	grad_norm 6.9303 (5.3485)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:29:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:02:40 lr 0.000010	 wd 0.0000	time 0.5100 (0.5317)	loss 1.5374 (1.2874)	grad_norm 4.7073 (5.4128)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:30:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:47 lr 0.000010	 wd 0.0000	time 0.5089 (0.5314)	loss 1.5296 (1.2866)	grad_norm 4.8345 (5.4092)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:31:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:54 lr 0.000010	 wd 0.0000	time 0.5170 (0.5312)	loss 1.2968 (1.2879)	grad_norm 2.9920 (5.3808)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:31:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000010	 wd 0.0000	time 0.5189 (0.5309)	loss 1.3772 (1.2874)	grad_norm 4.5729 (5.3572)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:31:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 21 training takes 0:22:11
[2024-07-05 06:32:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.301 (11.301)	Loss 0.3865 (0.3865)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 06:32:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.962 Acc@5 97.566
[2024-07-05 06:32:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-05 06:32:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.99%
[2024-07-05 06:32:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:47:34 lr 0.000010	 wd 0.0000	time 11.2128 (11.2128)	loss 1.1718 (1.1718)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:33:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:25:20 lr 0.000010	 wd 0.0000	time 0.5069 (0.6332)	loss 1.2925 (1.3072)	grad_norm 6.6521 (5.1113)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:34:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:22:14 lr 0.000009	 wd 0.0000	time 0.5055 (0.5797)	loss 0.9780 (1.2952)	grad_norm 3.8396 (5.0007)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:35:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:20:37 lr 0.000009	 wd 0.0000	time 0.5087 (0.5619)	loss 1.4282 (1.2988)	grad_norm 3.7521 (5.1452)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:36:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:19:21 lr 0.000009	 wd 0.0000	time 0.5066 (0.5528)	loss 1.2584 (1.3018)	grad_norm 4.1118 (5.1862)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:37:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:18:15 lr 0.000009	 wd 0.0000	time 0.5129 (0.5474)	loss 0.9733 (1.3011)	grad_norm 3.8198 (5.2852)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:37:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:17:14 lr 0.000009	 wd 0.0000	time 0.5012 (0.5439)	loss 1.3914 (1.3012)	grad_norm 3.7357 (5.2560)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:38:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:16:15 lr 0.000009	 wd 0.0000	time 0.4919 (0.5415)	loss 0.8498 (1.2952)	grad_norm 4.5880 (5.4066)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:39:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:15:18 lr 0.000009	 wd 0.0000	time 0.5146 (0.5396)	loss 1.2562 (1.2906)	grad_norm 4.5099 (5.3943)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:40:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:14:22 lr 0.000009	 wd 0.0000	time 0.5076 (0.5381)	loss 1.2431 (1.2886)	grad_norm 4.6812 (5.3707)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 06:41:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:13:26 lr 0.000009	 wd 0.0000	time 0.5216 (0.5371)	loss 1.4464 (1.2904)	grad_norm 8.7012 (5.3522)	loss_scale 1024.0000 (534.5055)	mem 12176MB
[2024-07-05 06:42:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:12:31 lr 0.000009	 wd 0.0000	time 0.5012 (0.5361)	loss 1.2906 (1.2887)	grad_norm 8.0532 (5.3396)	loss_scale 1024.0000 (578.9646)	mem 12176MB
[2024-07-05 06:43:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:11:36 lr 0.000009	 wd 0.0000	time 0.5082 (0.5352)	loss 1.2710 (1.2876)	grad_norm 12.4441 (5.4063)	loss_scale 1024.0000 (616.0200)	mem 12176MB
[2024-07-05 06:44:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:10:42 lr 0.000009	 wd 0.0000	time 0.5090 (0.5346)	loss 0.9688 (1.2884)	grad_norm 7.3196 (5.3870)	loss_scale 1024.0000 (647.3789)	mem 12176MB
[2024-07-05 06:44:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:09:48 lr 0.000008	 wd 0.0000	time 0.5327 (0.5339)	loss 1.4092 (1.2899)	grad_norm 7.5035 (5.3779)	loss_scale 1024.0000 (674.2612)	mem 12176MB
[2024-07-05 06:45:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:08:54 lr 0.000008	 wd 0.0000	time 0.5103 (0.5334)	loss 1.1181 (1.2901)	grad_norm 5.0967 (5.3473)	loss_scale 1024.0000 (697.5616)	mem 12176MB
[2024-07-05 06:46:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:08:00 lr 0.000008	 wd 0.0000	time 0.5207 (0.5330)	loss 1.4523 (1.2901)	grad_norm 8.4175 (5.3306)	loss_scale 1024.0000 (717.9513)	mem 12176MB
[2024-07-05 06:47:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:07:07 lr 0.000008	 wd 0.0000	time 0.5054 (0.5325)	loss 1.2580 (1.2893)	grad_norm 4.4171 (5.3490)	loss_scale 1024.0000 (735.9436)	mem 12176MB
[2024-07-05 06:48:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:06:13 lr 0.000008	 wd 0.0000	time 0.5119 (0.5322)	loss 1.5339 (1.2890)	grad_norm 3.7841 (5.3338)	loss_scale 1024.0000 (751.9378)	mem 12176MB
[2024-07-05 06:49:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:05:20 lr 0.000008	 wd 0.0000	time 0.5049 (0.5319)	loss 1.4618 (1.2882)	grad_norm 11.8728 (5.3307)	loss_scale 1024.0000 (766.2493)	mem 12176MB
[2024-07-05 06:50:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:04:26 lr 0.000008	 wd 0.0000	time 0.5109 (0.5316)	loss 1.3738 (1.2873)	grad_norm 4.2845 (5.3797)	loss_scale 1024.0000 (779.1304)	mem 12176MB
[2024-07-05 06:51:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:03:33 lr 0.000008	 wd 0.0000	time 0.5174 (0.5314)	loss 1.4205 (1.2878)	grad_norm 4.7255 (5.3674)	loss_scale 1024.0000 (790.7853)	mem 12176MB
[2024-07-05 06:51:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:40 lr 0.000008	 wd 0.0000	time 0.5103 (0.5311)	loss 1.4322 (1.2868)	grad_norm 4.3748 (5.3561)	loss_scale 1024.0000 (801.3812)	mem 12176MB
[2024-07-05 06:52:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:47 lr 0.000008	 wd 0.0000	time 0.5025 (0.5309)	loss 1.5166 (1.2870)	grad_norm 3.7352 (5.3568)	loss_scale 1024.0000 (811.0561)	mem 12176MB
[2024-07-05 06:53:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:54 lr 0.000008	 wd 0.0000	time 0.5164 (0.5307)	loss 1.0253 (1.2865)	grad_norm 4.3498 (5.3561)	loss_scale 1024.0000 (819.9250)	mem 12176MB
[2024-07-05 06:54:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0000	time 0.5013 (0.5304)	loss 1.0074 (1.2857)	grad_norm 3.4538 (5.3433)	loss_scale 1024.0000 (828.0848)	mem 12176MB
[2024-07-05 06:54:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 22 training takes 0:22:11
[2024-07-05 06:54:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.903 (11.903)	Loss 0.3872 (0.3872)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 06:55:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 84.964 Acc@5 97.602
[2024-07-05 06:55:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-05 06:55:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 84.99%
[2024-07-05 06:55:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:31:42 lr 0.000008	 wd 0.0000	time 10.8323 (10.8323)	loss 1.0715 (1.0715)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 06:56:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:25:18 lr 0.000008	 wd 0.0000	time 0.5030 (0.6321)	loss 1.4025 (1.3244)	grad_norm 5.4836 (5.5663)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 06:57:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:22:14 lr 0.000007	 wd 0.0000	time 0.5065 (0.5795)	loss 1.4665 (1.3035)	grad_norm 4.0933 (5.4222)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 06:57:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:20:36 lr 0.000007	 wd 0.0000	time 0.5088 (0.5617)	loss 1.2896 (1.2993)	grad_norm 4.0831 (5.2488)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 06:58:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:19:22 lr 0.000007	 wd 0.0000	time 0.5091 (0.5529)	loss 1.6428 (1.2935)	grad_norm 4.4076 (5.3712)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 06:59:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:18:15 lr 0.000007	 wd 0.0000	time 0.5065 (0.5474)	loss 1.5290 (1.2970)	grad_norm 4.9169 (5.3247)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:00:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:17:14 lr 0.000007	 wd 0.0000	time 0.5281 (0.5439)	loss 1.1549 (1.2971)	grad_norm 3.4132 (5.2849)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:01:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:16:15 lr 0.000007	 wd 0.0000	time 0.5124 (0.5415)	loss 1.4191 (1.2944)	grad_norm 3.9991 (5.3004)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:02:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:15:18 lr 0.000007	 wd 0.0000	time 0.5100 (0.5397)	loss 1.6197 (1.2919)	grad_norm 4.2762 (5.3162)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:03:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:14:22 lr 0.000007	 wd 0.0000	time 0.5106 (0.5382)	loss 1.6073 (1.2890)	grad_norm 5.9147 (5.2803)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:04:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:13:26 lr 0.000007	 wd 0.0000	time 0.5129 (0.5371)	loss 1.0202 (1.2887)	grad_norm 3.5288 (5.2690)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:04:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:12:31 lr 0.000007	 wd 0.0000	time 0.4944 (0.5362)	loss 0.8048 (1.2836)	grad_norm 4.3870 (5.2385)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:05:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:11:37 lr 0.000007	 wd 0.0000	time 0.5124 (0.5354)	loss 1.5180 (1.2854)	grad_norm 3.7077 (5.2011)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:06:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:10:42 lr 0.000007	 wd 0.0000	time 0.5029 (0.5347)	loss 1.4568 (1.2866)	grad_norm 5.0365 (5.2145)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:07:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:09:48 lr 0.000007	 wd 0.0000	time 0.5077 (0.5341)	loss 1.2745 (1.2858)	grad_norm 4.5952 (5.2122)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:08:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:08:54 lr 0.000006	 wd 0.0000	time 0.5056 (0.5336)	loss 1.1608 (1.2873)	grad_norm 5.2007 (5.2063)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:09:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:08:00 lr 0.000006	 wd 0.0000	time 0.5105 (0.5331)	loss 1.1647 (1.2874)	grad_norm 4.1820 (5.1946)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:10:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:07:07 lr 0.000006	 wd 0.0000	time 0.4837 (0.5327)	loss 1.2846 (1.2877)	grad_norm 6.8146 (5.2633)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:11:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:06:13 lr 0.000006	 wd 0.0000	time 0.5165 (0.5324)	loss 1.2685 (1.2870)	grad_norm 4.5579 (5.2746)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:11:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:05:20 lr 0.000006	 wd 0.0000	time 0.5075 (0.5320)	loss 1.1905 (1.2893)	grad_norm 4.5676 (5.3022)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:12:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:04:26 lr 0.000006	 wd 0.0000	time 0.5123 (0.5318)	loss 1.3210 (1.2881)	grad_norm 3.9543 (5.3155)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:13:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:03:33 lr 0.000006	 wd 0.0000	time 0.5151 (0.5316)	loss 1.6036 (1.2881)	grad_norm 4.2756 (5.3342)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:14:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:02:40 lr 0.000006	 wd 0.0000	time 0.5079 (0.5313)	loss 1.4440 (1.2856)	grad_norm 6.7271 (5.3500)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 07:15:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:47 lr 0.000006	 wd 0.0000	time 0.5096 (0.5311)	loss 1.4559 (1.2854)	grad_norm 6.2643 (nan)	loss_scale 512.0000 (1013.7645)	mem 12176MB
[2024-07-05 07:16:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:54 lr 0.000006	 wd 0.0000	time 0.5074 (0.5309)	loss 1.5260 (1.2861)	grad_norm 2.7107 (nan)	loss_scale 512.0000 (992.8663)	mem 12176MB
[2024-07-05 07:17:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0000	time 0.5140 (0.5306)	loss 1.3854 (1.2867)	grad_norm 4.8731 (nan)	loss_scale 512.0000 (973.6393)	mem 12176MB
[2024-07-05 07:17:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 23 training takes 0:22:13
[2024-07-05 07:17:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.027 (11.027)	Loss 0.3857 (0.3857)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 07:17:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 85.050 Acc@5 97.554
[2024-07-05 07:17:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.0%
[2024-07-05 07:17:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 85.05%
[2024-07-05 07:17:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 07:17:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 07:17:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:10:50 lr 0.000006	 wd 0.0000	time 10.3318 (10.3318)	loss 1.4844 (1.4844)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:18:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:24:53 lr 0.000006	 wd 0.0000	time 0.5045 (0.6218)	loss 1.1715 (1.3161)	grad_norm 4.7094 (5.5909)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:19:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:22:00 lr 0.000006	 wd 0.0000	time 0.5127 (0.5737)	loss 1.5824 (1.3052)	grad_norm 4.6969 (5.4864)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:20:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:20:27 lr 0.000006	 wd 0.0000	time 0.5066 (0.5574)	loss 1.5223 (1.2946)	grad_norm 3.8400 (5.2343)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:21:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:19:14 lr 0.000005	 wd 0.0000	time 0.5151 (0.5495)	loss 1.3754 (1.2894)	grad_norm 9.8238 (5.4010)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:22:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:18:10 lr 0.000005	 wd 0.0000	time 0.5126 (0.5447)	loss 1.4716 (1.2912)	grad_norm 6.4035 (5.4163)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:23:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:17:10 lr 0.000005	 wd 0.0000	time 0.5242 (0.5416)	loss 1.2740 (1.2873)	grad_norm 4.9886 (5.3492)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:24:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:16:11 lr 0.000005	 wd 0.0000	time 0.5061 (0.5393)	loss 1.2460 (1.2895)	grad_norm 5.4816 (5.3218)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:24:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:15:14 lr 0.000005	 wd 0.0000	time 0.5202 (0.5376)	loss 1.3381 (1.2904)	grad_norm 4.1652 (5.3232)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:25:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:14:19 lr 0.000005	 wd 0.0000	time 0.5097 (0.5362)	loss 1.3745 (1.2884)	grad_norm 3.6466 (5.3807)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:26:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:13:23 lr 0.000005	 wd 0.0000	time 0.4954 (0.5352)	loss 1.2682 (1.2885)	grad_norm 4.5077 (5.4262)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:27:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:12:29 lr 0.000005	 wd 0.0000	time 0.5201 (0.5344)	loss 1.4196 (1.2869)	grad_norm 4.9225 (5.4110)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:28:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:11:34 lr 0.000005	 wd 0.0000	time 0.5014 (0.5336)	loss 1.1182 (1.2857)	grad_norm 4.7826 (5.3866)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:29:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:10:40 lr 0.000005	 wd 0.0000	time 0.5055 (0.5330)	loss 1.2323 (1.2866)	grad_norm 5.2710 (5.3543)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:30:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:09:46 lr 0.000005	 wd 0.0000	time 0.5043 (0.5325)	loss 1.1664 (1.2854)	grad_norm 5.3854 (5.3413)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:31:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:08:53 lr 0.000005	 wd 0.0000	time 0.5055 (0.5321)	loss 0.9444 (1.2844)	grad_norm 4.7928 (5.3294)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:31:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:07:59 lr 0.000005	 wd 0.0000	time 0.5149 (0.5317)	loss 1.3359 (1.2839)	grad_norm 3.6370 (5.3139)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:32:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:07:06 lr 0.000005	 wd 0.0000	time 0.4872 (0.5314)	loss 1.6633 (1.2860)	grad_norm 9.0307 (5.2910)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:33:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:06:12 lr 0.000005	 wd 0.0000	time 0.5132 (0.5311)	loss 1.1287 (1.2853)	grad_norm 3.9155 (5.2883)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:34:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:05:19 lr 0.000005	 wd 0.0000	time 0.5057 (0.5308)	loss 1.4098 (1.2866)	grad_norm 3.5094 (5.2601)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:35:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:04:26 lr 0.000004	 wd 0.0000	time 0.5080 (0.5306)	loss 1.4608 (1.2864)	grad_norm 4.5357 (5.2503)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:36:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:03:33 lr 0.000004	 wd 0.0000	time 0.5019 (0.5304)	loss 1.4460 (1.2849)	grad_norm 13.3665 (5.2906)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:37:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:02:40 lr 0.000004	 wd 0.0000	time 0.5059 (0.5301)	loss 1.1890 (1.2844)	grad_norm 4.7164 (5.3145)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:38:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:47 lr 0.000004	 wd 0.0000	time 0.5043 (0.5299)	loss 1.6211 (1.2839)	grad_norm 3.1731 (5.3291)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:39:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:54 lr 0.000004	 wd 0.0000	time 0.5064 (0.5298)	loss 1.4875 (1.2839)	grad_norm 3.8782 (5.3197)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:39:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0000	time 0.5107 (0.5296)	loss 1.2100 (1.2845)	grad_norm 5.9519 (5.3305)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:39:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 24 training takes 0:22:10
[2024-07-05 07:40:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.422 (11.422)	Loss 0.3896 (0.3896)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 07:40:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 85.052 Acc@5 97.538
[2024-07-05 07:40:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-05 07:40:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 85.05%
[2024-07-05 07:40:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 07:40:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 07:40:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][0/2502]	eta 6:49:39 lr 0.000004	 wd 0.0000	time 9.8241 (9.8241)	loss 1.0272 (1.0272)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:41:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:24:43 lr 0.000004	 wd 0.0000	time 0.5014 (0.6176)	loss 1.2626 (1.2550)	grad_norm 4.1086 (5.2460)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:42:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:21:55 lr 0.000004	 wd 0.0000	time 0.5027 (0.5717)	loss 1.3575 (1.2766)	grad_norm 2.9477 (5.2921)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:43:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:20:24 lr 0.000004	 wd 0.0000	time 0.5111 (0.5563)	loss 0.9719 (1.2711)	grad_norm 3.6050 (5.5206)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:44:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:19:13 lr 0.000004	 wd 0.0000	time 0.4988 (0.5487)	loss 1.2239 (1.2839)	grad_norm 3.6118 (5.4749)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:45:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:18:09 lr 0.000004	 wd 0.0000	time 0.5061 (0.5442)	loss 1.3349 (1.2847)	grad_norm 4.1489 (5.4310)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:45:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:17:09 lr 0.000004	 wd 0.0000	time 0.5117 (0.5412)	loss 1.1254 (1.2866)	grad_norm 7.9669 (5.6141)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:46:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:16:11 lr 0.000004	 wd 0.0000	time 0.5093 (0.5390)	loss 1.1697 (1.2860)	grad_norm 4.0789 (5.6164)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:47:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:15:14 lr 0.000004	 wd 0.0000	time 0.5166 (0.5374)	loss 1.0194 (1.2868)	grad_norm 4.6125 (5.6433)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:48:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:14:18 lr 0.000004	 wd 0.0000	time 0.5123 (0.5361)	loss 1.3399 (1.2887)	grad_norm 5.0876 (5.6323)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:49:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:13:23 lr 0.000004	 wd 0.0000	time 0.5073 (0.5350)	loss 1.2758 (1.2895)	grad_norm 4.0056 (5.6569)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:50:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:12:28 lr 0.000004	 wd 0.0000	time 0.5108 (0.5342)	loss 1.2912 (1.2871)	grad_norm 7.3580 (5.7054)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:51:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:11:34 lr 0.000004	 wd 0.0000	time 0.5072 (0.5335)	loss 1.2538 (1.2886)	grad_norm 4.5492 (5.6330)	loss_scale 512.0000 (512.0000)	mem 12176MB
[2024-07-05 07:52:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:10:40 lr 0.000003	 wd 0.0000	time 0.5050 (0.5328)	loss 1.4543 (1.2888)	grad_norm 13.2685 (5.5912)	loss_scale 1024.0000 (531.6772)	mem 12176MB
[2024-07-05 07:52:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:09:46 lr 0.000003	 wd 0.0000	time 0.5075 (0.5323)	loss 1.1303 (1.2897)	grad_norm 4.2399 (5.6348)	loss_scale 1024.0000 (566.8180)	mem 12176MB
[2024-07-05 07:53:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:08:52 lr 0.000003	 wd 0.0000	time 0.4923 (0.5319)	loss 1.1769 (1.2890)	grad_norm 5.4018 (5.6584)	loss_scale 1024.0000 (597.2765)	mem 12176MB
[2024-07-05 07:54:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:07:59 lr 0.000003	 wd 0.0000	time 0.5078 (0.5315)	loss 1.4453 (1.2890)	grad_norm 4.3327 (5.6509)	loss_scale 1024.0000 (623.9300)	mem 12176MB
[2024-07-05 07:55:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:07:05 lr 0.000003	 wd 0.0000	time 0.5070 (0.5312)	loss 1.2553 (1.2889)	grad_norm 7.1897 (5.6199)	loss_scale 1024.0000 (647.4497)	mem 12176MB
[2024-07-05 07:56:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:06:12 lr 0.000003	 wd 0.0000	time 0.5085 (0.5309)	loss 1.3452 (1.2883)	grad_norm 4.2859 (5.6155)	loss_scale 1024.0000 (668.3576)	mem 12176MB
[2024-07-05 07:57:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:05:19 lr 0.000003	 wd 0.0000	time 0.5058 (0.5307)	loss 1.5343 (1.2884)	grad_norm 4.3859 (5.6013)	loss_scale 1024.0000 (687.0658)	mem 12176MB
[2024-07-05 07:58:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:04:26 lr 0.000003	 wd 0.0000	time 0.5041 (0.5305)	loss 1.2328 (1.2879)	grad_norm 4.9352 (5.5603)	loss_scale 1024.0000 (703.9040)	mem 12176MB
[2024-07-05 07:59:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:03:33 lr 0.000003	 wd 0.0000	time 0.5066 (0.5303)	loss 0.8191 (1.2881)	grad_norm 3.9928 (5.5442)	loss_scale 1024.0000 (719.1395)	mem 12176MB
[2024-07-05 07:59:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:02:40 lr 0.000003	 wd 0.0000	time 0.5102 (0.5301)	loss 1.3666 (1.2868)	grad_norm 3.8324 (5.5226)	loss_scale 1024.0000 (732.9905)	mem 12176MB
[2024-07-05 08:00:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:47 lr 0.000003	 wd 0.0000	time 0.5125 (0.5299)	loss 1.3686 (1.2855)	grad_norm 3.4378 (5.4997)	loss_scale 1024.0000 (745.6375)	mem 12176MB
[2024-07-05 08:01:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:54 lr 0.000003	 wd 0.0000	time 0.5193 (0.5298)	loss 1.3531 (1.2867)	grad_norm 4.4243 (5.4952)	loss_scale 1024.0000 (757.2312)	mem 12176MB
[2024-07-05 08:02:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0000	time 0.5067 (0.5296)	loss 1.4451 (1.2869)	grad_norm 12.6279 (5.5042)	loss_scale 1024.0000 (767.8976)	mem 12176MB
[2024-07-05 08:02:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 25 training takes 0:22:11
[2024-07-05 08:02:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.503 (11.503)	Loss 0.3838 (0.3838)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 08:03:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 85.076 Acc@5 97.558
[2024-07-05 08:03:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-05 08:03:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 85.08%
[2024-07-05 08:03:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 08:03:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 08:03:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:13:24 lr 0.000003	 wd 0.0000	time 10.3936 (10.3936)	loss 1.1088 (1.1088)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:04:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:24:58 lr 0.000003	 wd 0.0000	time 0.5113 (0.6237)	loss 1.4841 (1.2768)	grad_norm 8.7366 (5.7200)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:05:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:22:02 lr 0.000003	 wd 0.0000	time 0.4925 (0.5745)	loss 1.3674 (1.3037)	grad_norm 5.4330 (5.3942)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:05:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:20:29 lr 0.000003	 wd 0.0000	time 0.5090 (0.5583)	loss 1.2813 (1.2965)	grad_norm 3.8693 (5.3206)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:06:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:19:16 lr 0.000003	 wd 0.0000	time 0.5132 (0.5502)	loss 1.6119 (1.2979)	grad_norm 3.1544 (5.2821)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:07:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:18:11 lr 0.000003	 wd 0.0000	time 0.5124 (0.5454)	loss 1.6077 (1.3047)	grad_norm 4.6780 (5.4255)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:08:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:17:11 lr 0.000003	 wd 0.0000	time 0.5043 (0.5421)	loss 1.2464 (1.3006)	grad_norm 6.7663 (5.3802)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:09:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:16:12 lr 0.000003	 wd 0.0000	time 0.5121 (0.5398)	loss 1.3588 (1.2942)	grad_norm 4.2743 (5.3196)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:10:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:15:15 lr 0.000002	 wd 0.0000	time 0.5047 (0.5380)	loss 1.3533 (1.2910)	grad_norm 3.4199 (5.2376)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:11:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:14:19 lr 0.000002	 wd 0.0000	time 0.5169 (0.5367)	loss 1.3164 (1.2928)	grad_norm 4.3267 (5.2295)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:12:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:13:24 lr 0.000002	 wd 0.0000	time 0.5115 (0.5356)	loss 1.4264 (1.2925)	grad_norm 5.5138 (5.2743)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:12:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:12:29 lr 0.000002	 wd 0.0000	time 0.5115 (0.5348)	loss 1.4004 (1.2942)	grad_norm 3.8704 (5.3130)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:13:52 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:11:35 lr 0.000002	 wd 0.0000	time 0.4988 (0.5340)	loss 1.0135 (1.2946)	grad_norm 5.7533 (5.3518)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:14:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:10:41 lr 0.000002	 wd 0.0000	time 0.5200 (0.5334)	loss 0.8957 (1.2924)	grad_norm 3.8214 (5.3240)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:15:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:09:47 lr 0.000002	 wd 0.0000	time 0.5151 (0.5329)	loss 1.7116 (1.2923)	grad_norm 6.1950 (5.3002)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:16:30 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:08:53 lr 0.000002	 wd 0.0000	time 0.5083 (0.5324)	loss 1.4539 (1.2910)	grad_norm 4.6216 (5.3096)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:17:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:07:59 lr 0.000002	 wd 0.0000	time 0.4918 (0.5321)	loss 1.3633 (1.2897)	grad_norm 4.5917 (5.2959)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:18:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:07:06 lr 0.000002	 wd 0.0000	time 0.5119 (0.5318)	loss 1.3191 (1.2900)	grad_norm 4.4764 (5.3193)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:19:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:06:13 lr 0.000002	 wd 0.0000	time 0.4986 (0.5314)	loss 1.3821 (1.2917)	grad_norm 12.1220 (5.3019)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:20:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:05:19 lr 0.000002	 wd 0.0000	time 0.5167 (0.5312)	loss 1.0182 (1.2897)	grad_norm 3.2717 (5.2875)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:20:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:04:26 lr 0.000002	 wd 0.0000	time 0.5021 (0.5309)	loss 1.1288 (1.2895)	grad_norm 4.3839 (5.3156)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:21:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:03:33 lr 0.000002	 wd 0.0000	time 0.4998 (0.5307)	loss 0.9732 (1.2880)	grad_norm 5.2771 (5.3183)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:22:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:02:40 lr 0.000002	 wd 0.0000	time 0.5085 (0.5305)	loss 0.8837 (1.2878)	grad_norm 3.9229 (5.3206)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:23:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:47 lr 0.000002	 wd 0.0000	time 0.5087 (0.5303)	loss 1.0495 (1.2869)	grad_norm 3.9451 (5.3311)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:24:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:54 lr 0.000002	 wd 0.0000	time 0.5109 (0.5302)	loss 1.0551 (1.2866)	grad_norm 21.9921 (5.3187)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:25:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0000	time 0.5053 (0.5300)	loss 1.0099 (1.2849)	grad_norm 4.8876 (5.3067)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:25:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 26 training takes 0:22:12
[2024-07-05 08:25:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.288 (11.288)	Loss 0.3848 (0.3848)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 12176MB
[2024-07-05 08:25:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 85.096 Acc@5 97.564
[2024-07-05 08:25:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-05 08:25:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 85.10%
[2024-07-05 08:25:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 08:25:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 08:26:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][0/2502]	eta 6:47:58 lr 0.000002	 wd 0.0000	time 9.7836 (9.7836)	loss 0.8656 (0.8656)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:26:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:24:44 lr 0.000002	 wd 0.0000	time 0.5107 (0.6179)	loss 1.3824 (1.3018)	grad_norm 4.4105 (4.9615)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:27:50 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:21:57 lr 0.000002	 wd 0.0000	time 0.5057 (0.5723)	loss 1.4276 (1.2916)	grad_norm 3.9801 (5.4151)	loss_scale 1024.0000 (1024.0000)	mem 12176MB
[2024-07-05 08:28:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:20:25 lr 0.000002	 wd 0.0000	time 0.5101 (0.5567)	loss 1.2314 (1.2839)	grad_norm 5.8015 (5.5825)	loss_scale 2048.0000 (1207.7076)	mem 12176MB
[2024-07-05 08:29:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:19:14 lr 0.000002	 wd 0.0000	time 0.5051 (0.5492)	loss 1.3063 (1.2837)	grad_norm 5.5187 (5.5710)	loss_scale 2048.0000 (1417.2569)	mem 12176MB
[2024-07-05 08:30:27 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:18:10 lr 0.000002	 wd 0.0000	time 0.5144 (0.5445)	loss 0.7990 (1.2766)	grad_norm 5.8640 (5.4371)	loss_scale 2048.0000 (1543.1537)	mem 12176MB
[2024-07-05 08:31:20 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:17:10 lr 0.000002	 wd 0.0000	time 0.5156 (0.5416)	loss 1.3139 (1.2776)	grad_norm 6.1663 (5.4064)	loss_scale 2048.0000 (1627.1547)	mem 12176MB
[2024-07-05 08:32:13 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:16:12 lr 0.000002	 wd 0.0000	time 0.5119 (0.5394)	loss 1.4030 (1.2822)	grad_norm 3.3061 (5.4168)	loss_scale 2048.0000 (1687.1897)	mem 12176MB
[2024-07-05 08:33:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:15:15 lr 0.000002	 wd 0.0000	time 0.5114 (0.5378)	loss 1.4992 (1.2853)	grad_norm 3.8197 (5.4492)	loss_scale 2048.0000 (1732.2347)	mem 12176MB
[2024-07-05 08:33:58 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:14:19 lr 0.000001	 wd 0.0000	time 0.5148 (0.5365)	loss 1.3444 (1.2901)	grad_norm 4.7842 (5.4251)	loss_scale 2048.0000 (1767.2808)	mem 12176MB
[2024-07-05 08:34:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:13:24 lr 0.000001	 wd 0.0000	time 0.5086 (0.5355)	loss 1.2948 (1.2891)	grad_norm 7.3517 (5.4387)	loss_scale 2048.0000 (1795.3247)	mem 12176MB
[2024-07-05 08:35:43 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:12:29 lr 0.000001	 wd 0.0000	time 0.5000 (0.5347)	loss 1.3132 (1.2902)	grad_norm 3.6291 (inf)	loss_scale 1024.0000 (1742.0091)	mem 12176MB
[2024-07-05 08:36:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:11:35 lr 0.000001	 wd 0.0000	time 0.4956 (0.5339)	loss 1.4544 (1.2904)	grad_norm 5.2156 (inf)	loss_scale 1024.0000 (1682.2248)	mem 12176MB
[2024-07-05 08:37:28 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:10:41 lr 0.000001	 wd 0.0000	time 0.5189 (0.5333)	loss 1.3953 (1.2886)	grad_norm 3.1487 (inf)	loss_scale 1024.0000 (1631.6311)	mem 12176MB
[2024-07-05 08:38:21 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:09:47 lr 0.000001	 wd 0.0000	time 0.5061 (0.5327)	loss 1.2697 (1.2898)	grad_norm 27.2324 (inf)	loss_scale 1024.0000 (1588.2598)	mem 12176MB
[2024-07-05 08:39:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:08:53 lr 0.000001	 wd 0.0000	time 0.5057 (0.5323)	loss 1.4542 (1.2899)	grad_norm 5.0491 (inf)	loss_scale 1024.0000 (1550.6676)	mem 12176MB
[2024-07-05 08:40:06 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:07:59 lr 0.000001	 wd 0.0000	time 0.5084 (0.5319)	loss 1.4168 (1.2881)	grad_norm 4.3328 (inf)	loss_scale 1024.0000 (1517.7714)	mem 12176MB
[2024-07-05 08:40:59 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:07:06 lr 0.000001	 wd 0.0000	time 0.5076 (0.5316)	loss 1.3321 (1.2862)	grad_norm 4.4024 (inf)	loss_scale 1024.0000 (1488.7431)	mem 12176MB
[2024-07-05 08:41:51 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:06:12 lr 0.000001	 wd 0.0000	time 0.5087 (0.5313)	loss 1.3293 (1.2868)	grad_norm 5.2528 (inf)	loss_scale 1024.0000 (1462.9384)	mem 12176MB
[2024-07-05 08:42:44 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:05:19 lr 0.000001	 wd 0.0000	time 0.5154 (0.5310)	loss 0.9165 (1.2879)	grad_norm 5.6055 (inf)	loss_scale 1024.0000 (1439.8485)	mem 12176MB
[2024-07-05 08:43:37 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:04:26 lr 0.000001	 wd 0.0000	time 0.5026 (0.5308)	loss 0.8419 (1.2866)	grad_norm 4.3255 (inf)	loss_scale 1024.0000 (1419.0665)	mem 12176MB
[2024-07-05 08:44:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:03:33 lr 0.000001	 wd 0.0000	time 0.5120 (0.5306)	loss 1.6448 (1.2853)	grad_norm 21.4615 (inf)	loss_scale 1024.0000 (1400.2627)	mem 12176MB
[2024-07-05 08:45:22 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:02:40 lr 0.000001	 wd 0.0000	time 0.5113 (0.5303)	loss 0.9644 (1.2864)	grad_norm 5.8591 (inf)	loss_scale 256.0000 (1370.8387)	mem 12176MB
[2024-07-05 08:46:14 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:47 lr 0.000001	 wd 0.0000	time 0.5071 (0.5301)	loss 1.4022 (1.2866)	grad_norm 5.2869 (inf)	loss_scale 256.0000 (1322.3885)	mem 12176MB
[2024-07-05 08:47:07 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:54 lr 0.000001	 wd 0.0000	time 0.5113 (0.5300)	loss 1.1414 (1.2864)	grad_norm 3.5402 (inf)	loss_scale 256.0000 (1277.9742)	mem 12176MB
[2024-07-05 08:48:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0000	time 0.5066 (0.5298)	loss 1.3614 (1.2849)	grad_norm 4.1081 (inf)	loss_scale 256.0000 (1237.1116)	mem 12176MB
[2024-07-05 08:48:05 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 27 training takes 0:22:10
[2024-07-05 08:48:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 10.825 (10.825)	Loss 0.3845 (0.3845)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 08:48:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 85.114 Acc@5 97.574
[2024-07-05 08:48:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-05 08:48:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 85.11%
[2024-07-05 08:48:36 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 160): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saving......
[2024-07-05 08:48:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 162): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_best.pth saved !!!
[2024-07-05 08:48:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][0/2502]	eta 6:49:16 lr 0.000001	 wd 0.0000	time 9.8146 (9.8146)	loss 1.2020 (1.2020)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:49:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:24:42 lr 0.000001	 wd 0.0000	time 0.5045 (0.6170)	loss 0.9408 (1.2678)	grad_norm 10.1200 (6.3262)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:50:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:21:55 lr 0.000001	 wd 0.0000	time 0.5079 (0.5713)	loss 1.0607 (1.2728)	grad_norm 5.2969 (5.8313)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:51:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:20:24 lr 0.000001	 wd 0.0000	time 0.5060 (0.5561)	loss 1.5134 (1.2866)	grad_norm 4.3668 (5.5180)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:52:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:19:13 lr 0.000001	 wd 0.0000	time 0.5135 (0.5486)	loss 1.4415 (1.2907)	grad_norm 3.5456 (5.4758)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:53:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:18:08 lr 0.000001	 wd 0.0000	time 0.5140 (0.5439)	loss 1.5335 (1.2905)	grad_norm 4.1316 (5.4376)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:54:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:17:08 lr 0.000001	 wd 0.0000	time 0.5185 (0.5409)	loss 1.3595 (1.2903)	grad_norm 5.1022 (5.3776)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:54:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:16:10 lr 0.000001	 wd 0.0000	time 0.5049 (0.5387)	loss 1.0154 (1.2927)	grad_norm 5.3124 (5.3541)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:55:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:15:14 lr 0.000001	 wd 0.0000	time 0.5154 (0.5371)	loss 1.5269 (1.2904)	grad_norm 3.5984 (5.3394)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:56:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:14:18 lr 0.000001	 wd 0.0000	time 0.5130 (0.5359)	loss 1.4108 (1.2906)	grad_norm 4.7883 (5.3523)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:57:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:13:23 lr 0.000001	 wd 0.0000	time 0.5133 (0.5348)	loss 1.3850 (1.2910)	grad_norm 4.5797 (5.3422)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:58:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:12:28 lr 0.000001	 wd 0.0000	time 0.5035 (0.5340)	loss 1.0263 (1.2897)	grad_norm 4.1151 (5.3840)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 08:59:18 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:11:34 lr 0.000001	 wd 0.0000	time 0.5077 (0.5333)	loss 1.0617 (1.2878)	grad_norm 8.2347 (5.3926)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:00:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:10:40 lr 0.000001	 wd 0.0000	time 0.5044 (0.5327)	loss 1.3228 (1.2851)	grad_norm 7.0375 (5.5147)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:01:03 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:09:46 lr 0.000001	 wd 0.0000	time 0.5185 (0.5322)	loss 0.9349 (1.2858)	grad_norm 4.9776 (5.5087)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:01:56 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:08:52 lr 0.000001	 wd 0.0000	time 0.5047 (0.5318)	loss 1.4101 (1.2830)	grad_norm 5.1015 (5.4993)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:02:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:07:59 lr 0.000001	 wd 0.0000	time 0.4987 (0.5314)	loss 1.1681 (1.2830)	grad_norm 5.4389 (5.5108)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:03:41 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:07:05 lr 0.000001	 wd 0.0000	time 0.5056 (0.5311)	loss 1.4333 (1.2840)	grad_norm 5.1211 (5.4869)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:04:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:06:12 lr 0.000001	 wd 0.0000	time 0.5112 (0.5308)	loss 1.3002 (1.2831)	grad_norm 4.7449 (5.5262)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:05:26 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:05:19 lr 0.000001	 wd 0.0000	time 0.5123 (0.5305)	loss 1.5698 (1.2820)	grad_norm 5.1570 (5.4974)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:06:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:04:26 lr 0.000001	 wd 0.0000	time 0.5083 (0.5303)	loss 0.8309 (1.2813)	grad_norm 5.6274 (5.5027)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:07:11 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:03:33 lr 0.000001	 wd 0.0000	time 0.5095 (0.5301)	loss 1.1077 (1.2816)	grad_norm 3.3326 (5.4903)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:08:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:02:40 lr 0.000001	 wd 0.0000	time 0.5082 (0.5299)	loss 1.4013 (1.2806)	grad_norm 5.4558 (5.4936)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:08:57 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:47 lr 0.000001	 wd 0.0000	time 0.5038 (0.5298)	loss 1.1757 (1.2808)	grad_norm 4.2305 (5.4859)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:09:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:54 lr 0.000001	 wd 0.0000	time 0.5062 (0.5296)	loss 1.4873 (1.2797)	grad_norm 4.8719 (5.4752)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:10:42 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0000	time 0.5154 (0.5294)	loss 1.1303 (1.2783)	grad_norm 5.1556 (5.4611)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:10:49 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 28 training takes 0:22:11
[2024-07-05 09:11:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 11.419 (11.419)	Loss 0.3848 (0.3848)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 09:11:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 85.104 Acc@5 97.580
[2024-07-05 09:11:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-05 09:11:19 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 85.11%
[2024-07-05 09:11:29 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][0/2502]	eta 6:56:54 lr 0.000001	 wd 0.0000	time 9.9978 (9.9978)	loss 1.4275 (1.4275)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:12:23 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:25:08 lr 0.000001	 wd 0.0000	time 0.5141 (0.6282)	loss 1.4187 (1.2562)	grad_norm 5.2323 (5.1529)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:13:15 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:22:08 lr 0.000001	 wd 0.0000	time 0.5026 (0.5771)	loss 1.1312 (1.2739)	grad_norm 4.4566 (5.2990)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:14:08 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:20:33 lr 0.000001	 wd 0.0000	time 0.5175 (0.5600)	loss 1.4418 (1.2729)	grad_norm 5.9053 (5.2057)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:15:00 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:19:18 lr 0.000001	 wd 0.0000	time 0.5081 (0.5514)	loss 1.1723 (1.2772)	grad_norm 4.2844 (5.4236)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:15:53 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:18:13 lr 0.000001	 wd 0.0000	time 0.5066 (0.5462)	loss 0.8414 (1.2822)	grad_norm 4.0815 (5.4241)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:16:46 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:17:12 lr 0.000000	 wd 0.0000	time 0.5173 (0.5430)	loss 1.2448 (1.2864)	grad_norm 3.4693 (5.4376)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:17:38 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:16:14 lr 0.000000	 wd 0.0000	time 0.4984 (0.5407)	loss 1.5766 (1.2866)	grad_norm 21.6413 (5.5886)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:18:31 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:15:17 lr 0.000000	 wd 0.0000	time 0.5058 (0.5388)	loss 0.8001 (1.2855)	grad_norm 6.0948 (5.6531)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:19:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:14:21 lr 0.000000	 wd 0.0000	time 0.5085 (0.5375)	loss 1.2581 (1.2857)	grad_norm 5.0119 (5.6470)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:20:16 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:13:25 lr 0.000000	 wd 0.0000	time 0.5145 (0.5363)	loss 1.2455 (1.2845)	grad_norm 4.7144 (5.6480)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:21:09 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:12:30 lr 0.000000	 wd 0.0000	time 0.5083 (0.5354)	loss 1.2542 (1.2845)	grad_norm 2.9288 (5.6264)	loss_scale 256.0000 (256.0000)	mem 12176MB
[2024-07-05 09:22:01 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:11:36 lr 0.000000	 wd 0.0000	time 0.5110 (0.5347)	loss 1.3111 (1.2817)	grad_norm 5.8409 (5.5991)	loss_scale 512.0000 (258.1316)	mem 12176MB
[2024-07-05 09:22:54 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:10:41 lr 0.000000	 wd 0.0000	time 0.5079 (0.5340)	loss 1.4356 (1.2823)	grad_norm 5.8245 (5.5739)	loss_scale 512.0000 (277.6449)	mem 12176MB
[2024-07-05 09:23:47 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:09:47 lr 0.000000	 wd 0.0000	time 0.5076 (0.5334)	loss 1.5155 (1.2798)	grad_norm 6.3759 (5.5904)	loss_scale 512.0000 (294.3726)	mem 12176MB
[2024-07-05 09:24:39 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:08:53 lr 0.000000	 wd 0.0000	time 0.5052 (0.5329)	loss 1.3021 (1.2809)	grad_norm 4.6551 (5.5594)	loss_scale 512.0000 (308.8714)	mem 12176MB
[2024-07-05 09:25:32 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:08:00 lr 0.000000	 wd 0.0000	time 0.5064 (0.5324)	loss 1.2650 (1.2824)	grad_norm 5.2547 (5.5247)	loss_scale 512.0000 (321.5590)	mem 12176MB
[2024-07-05 09:26:24 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:07:06 lr 0.000000	 wd 0.0000	time 0.5055 (0.5321)	loss 1.0804 (1.2804)	grad_norm 7.8686 (5.5327)	loss_scale 512.0000 (332.7549)	mem 12176MB
[2024-07-05 09:27:17 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:06:13 lr 0.000000	 wd 0.0000	time 0.5024 (0.5318)	loss 1.0851 (1.2822)	grad_norm 4.6579 (5.5600)	loss_scale 512.0000 (342.7074)	mem 12176MB
[2024-07-05 09:28:10 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:05:20 lr 0.000000	 wd 0.0000	time 0.5145 (0.5316)	loss 1.2208 (1.2813)	grad_norm 4.9708 (5.5347)	loss_scale 512.0000 (351.6128)	mem 12176MB
[2024-07-05 09:29:02 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:04:26 lr 0.000000	 wd 0.0000	time 0.5055 (0.5313)	loss 1.4948 (1.2813)	grad_norm 4.8032 (5.5289)	loss_scale 512.0000 (359.6282)	mem 12176MB
[2024-07-05 09:29:55 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:03:33 lr 0.000000	 wd 0.0000	time 0.5178 (0.5311)	loss 1.3555 (1.2819)	grad_norm 3.9322 (5.5250)	loss_scale 512.0000 (366.8805)	mem 12176MB
[2024-07-05 09:30:48 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:02:40 lr 0.000000	 wd 0.0000	time 0.5087 (0.5309)	loss 1.3984 (1.2836)	grad_norm 3.9201 (5.6023)	loss_scale 512.0000 (373.4739)	mem 12176MB
[2024-07-05 09:31:40 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:47 lr 0.000000	 wd 0.0000	time 0.4987 (0.5307)	loss 1.4846 (1.2846)	grad_norm 8.1689 (5.5962)	loss_scale 512.0000 (379.4941)	mem 12176MB
[2024-07-05 09:32:33 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:54 lr 0.000000	 wd 0.0000	time 0.5125 (0.5305)	loss 1.4493 (1.2853)	grad_norm 4.7466 (5.6061)	loss_scale 512.0000 (385.0129)	mem 12176MB
[2024-07-05 09:33:25 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0000	time 0.5090 (0.5302)	loss 1.3432 (1.2847)	grad_norm 4.4464 (5.5865)	loss_scale 512.0000 (390.0904)	mem 12176MB
[2024-07-05 09:33:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 249): INFO EPOCH 29 training takes 0:22:14
[2024-07-05 09:33:34 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 145): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_29.pth saving......
[2024-07-05 09:33:35 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (utils.py 147): INFO pretrain/vcnu_finetune/swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full/diffusion_ft_swin_base_patch4_22kto1k_finetune_seq_stage_process3-full-finetune/ckpt_epoch_29.pth saved !!!
[2024-07-05 09:33:45 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 289): INFO Test: [0/98]	Time 9.582 (9.582)	Loss 0.3853 (0.3853)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 12176MB
[2024-07-05 09:34:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 296): INFO  * Acc@1 85.096 Acc@5 97.580
[2024-07-05 09:34:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-05 09:34:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 182): INFO Max accuracy: 85.11%
[2024-07-05 09:34:04 swin_diffusion_finetune_base_patch4_window7_224_22kto1k_seq_stage3_finetune_full] (main.py 189): INFO Training time 11:17:57
