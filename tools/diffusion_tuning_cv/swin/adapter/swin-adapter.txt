[2024-06-29 17:11:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/config.json
[2024-06-29 17:11:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_swin
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: adapter_swin_b_22kto1k_efficient_finetune
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-06-29 17:11:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/vcnu_swin/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "adapter_swin_b_22kto1k_efficient_finetune", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-06-29 17:11:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 108): INFO Creating model:adapter_swin/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune
[2024-06-29 17:11:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 110): INFO Adapter_SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-06-29 17:11:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 113): INFO number of params: 4531880
[2024-06-29 17:11:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-06-29 17:11:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune, ignoring auto resume
[2024-06-29 17:11:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-06-29 17:11:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-06-29 17:11:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.adapter.down.weight', 'layers.0.blocks.0.adapter.down.bias', 'layers.0.blocks.0.adapter.up.weight', 'layers.0.blocks.0.adapter.up.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.adapter.down.weight', 'layers.0.blocks.1.adapter.down.bias', 'layers.0.blocks.1.adapter.up.weight', 'layers.0.blocks.1.adapter.up.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.adapter.down.weight', 'layers.1.blocks.0.adapter.down.bias', 'layers.1.blocks.0.adapter.up.weight', 'layers.1.blocks.0.adapter.up.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.adapter.down.weight', 'layers.1.blocks.1.adapter.down.bias', 'layers.1.blocks.1.adapter.up.weight', 'layers.1.blocks.1.adapter.up.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.adapter.down.weight', 'layers.2.blocks.0.adapter.down.bias', 'layers.2.blocks.0.adapter.up.weight', 'layers.2.blocks.0.adapter.up.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.adapter.down.weight', 'layers.2.blocks.1.adapter.down.bias', 'layers.2.blocks.1.adapter.up.weight', 'layers.2.blocks.1.adapter.up.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.adapter.down.weight', 'layers.2.blocks.2.adapter.down.bias', 'layers.2.blocks.2.adapter.up.weight', 'layers.2.blocks.2.adapter.up.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.adapter.down.weight', 'layers.2.blocks.3.adapter.down.bias', 'layers.2.blocks.3.adapter.up.weight', 'layers.2.blocks.3.adapter.up.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.adapter.down.weight', 'layers.2.blocks.4.adapter.down.bias', 'layers.2.blocks.4.adapter.up.weight', 'layers.2.blocks.4.adapter.up.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.adapter.down.weight', 'layers.2.blocks.5.adapter.down.bias', 'layers.2.blocks.5.adapter.up.weight', 'layers.2.blocks.5.adapter.up.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.adapter.down.weight', 'layers.2.blocks.6.adapter.down.bias', 'layers.2.blocks.6.adapter.up.weight', 'layers.2.blocks.6.adapter.up.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.adapter.down.weight', 'layers.2.blocks.7.adapter.down.bias', 'layers.2.blocks.7.adapter.up.weight', 'layers.2.blocks.7.adapter.up.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.adapter.down.weight', 'layers.2.blocks.8.adapter.down.bias', 'layers.2.blocks.8.adapter.up.weight', 'layers.2.blocks.8.adapter.up.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.adapter.down.weight', 'layers.2.blocks.9.adapter.down.bias', 'layers.2.blocks.9.adapter.up.weight', 'layers.2.blocks.9.adapter.up.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.adapter.down.weight', 'layers.2.blocks.10.adapter.down.bias', 'layers.2.blocks.10.adapter.up.weight', 'layers.2.blocks.10.adapter.up.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.adapter.down.weight', 'layers.2.blocks.11.adapter.down.bias', 'layers.2.blocks.11.adapter.up.weight', 'layers.2.blocks.11.adapter.up.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.adapter.down.weight', 'layers.2.blocks.12.adapter.down.bias', 'layers.2.blocks.12.adapter.up.weight', 'layers.2.blocks.12.adapter.up.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.adapter.down.weight', 'layers.2.blocks.13.adapter.down.bias', 'layers.2.blocks.13.adapter.up.weight', 'layers.2.blocks.13.adapter.up.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.adapter.down.weight', 'layers.2.blocks.14.adapter.down.bias', 'layers.2.blocks.14.adapter.up.weight', 'layers.2.blocks.14.adapter.up.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.adapter.down.weight', 'layers.2.blocks.15.adapter.down.bias', 'layers.2.blocks.15.adapter.up.weight', 'layers.2.blocks.15.adapter.up.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.adapter.down.weight', 'layers.2.blocks.16.adapter.down.bias', 'layers.2.blocks.16.adapter.up.weight', 'layers.2.blocks.16.adapter.up.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.adapter.down.weight', 'layers.2.blocks.17.adapter.down.bias', 'layers.2.blocks.17.adapter.up.weight', 'layers.2.blocks.17.adapter.up.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.adapter.down.weight', 'layers.3.blocks.0.adapter.down.bias', 'layers.3.blocks.0.adapter.up.weight', 'layers.3.blocks.0.adapter.up.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.adapter.down.weight', 'layers.3.blocks.1.adapter.down.bias', 'layers.3.blocks.1.adapter.up.weight', 'layers.3.blocks.1.adapter.up.bias'], unexpected_keys=[])
[2024-06-29 17:11:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth'
[2024-06-29 17:12:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 69.242 (69.242)	Loss 0.3616 (0.3616)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 1490MB
[2024-06-29 17:13:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 82.636 Acc@5 96.604
[2024-06-29 17:13:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 162): INFO Accuracy of the network on the 50000 test images: 82.6%
[2024-06-29 17:13:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 168): INFO Start training
[2024-06-29 17:13:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][0/2502]	eta 14:43:54 lr 0.000000	 wd 0.0000	time 21.1970 (21.1970)	loss 1.7768 (1.7768)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 7938MB
[2024-06-29 17:13:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:21:28 lr 0.000000	 wd 0.0000	time 0.2783 (0.5365)	loss 1.5630 (1.5354)	grad_norm 0.6654 (inf)	loss_scale 16384.0000 (37958.9703)	mem 7984MB
[2024-06-29 17:14:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:55 lr 0.000001	 wd 0.0000	time 0.2347 (0.3892)	loss 1.5622 (1.5203)	grad_norm 0.7041 (inf)	loss_scale 16384.0000 (27225.1542)	mem 7984MB
[2024-06-29 17:14:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:12:22 lr 0.000001	 wd 0.0000	time 0.2275 (0.3370)	loss 1.5446 (1.4981)	grad_norm 1.1093 (inf)	loss_scale 16384.0000 (23623.4419)	mem 7984MB
[2024-06-29 17:15:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:10:53 lr 0.000001	 wd 0.0000	time 0.2697 (0.3108)	loss 2.0033 (1.4999)	grad_norm 0.6376 (inf)	loss_scale 8192.0000 (20102.0648)	mem 7984MB
[2024-06-29 17:15:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:10:03 lr 0.000002	 wd 0.0000	time 0.4589 (0.3014)	loss 1.6668 (1.4979)	grad_norm 0.6983 (inf)	loss_scale 8192.0000 (17724.8064)	mem 7984MB
[2024-06-29 17:15:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:09:14 lr 0.000002	 wd 0.0000	time 0.2118 (0.2913)	loss 1.2641 (1.4970)	grad_norm 0.6327 (inf)	loss_scale 8192.0000 (16138.6489)	mem 7984MB
[2024-06-29 17:16:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:08:29 lr 0.000002	 wd 0.0000	time 0.2221 (0.2828)	loss 1.5622 (1.4914)	grad_norm 0.6237 (inf)	loss_scale 8192.0000 (15005.0328)	mem 7984MB
[2024-06-29 17:16:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:07:50 lr 0.000003	 wd 0.0000	time 0.2572 (0.2764)	loss 1.6300 (1.4912)	grad_norm 0.5961 (inf)	loss_scale 8192.0000 (14154.4669)	mem 7984MB
[2024-06-29 17:17:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:07:19 lr 0.000003	 wd 0.0000	time 0.4756 (0.2743)	loss 1.7029 (1.4841)	grad_norm 0.5771 (inf)	loss_scale 8192.0000 (13492.7059)	mem 7984MB
[2024-06-29 17:17:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:06:47 lr 0.000003	 wd 0.0000	time 0.2011 (0.2713)	loss 1.4708 (1.4819)	grad_norm 0.6453 (inf)	loss_scale 8192.0000 (12963.1648)	mem 7984MB
[2024-06-29 17:17:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:06:15 lr 0.000004	 wd 0.0000	time 0.2317 (0.2679)	loss 1.6214 (1.4805)	grad_norm 0.4941 (inf)	loss_scale 8192.0000 (12529.8165)	mem 7984MB
[2024-06-29 17:18:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:05:44 lr 0.000004	 wd 0.0000	time 0.2159 (0.2649)	loss 1.5894 (1.4816)	grad_norm 0.5686 (inf)	loss_scale 8192.0000 (12168.6328)	mem 7984MB
[2024-06-29 17:18:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:05:17 lr 0.000004	 wd 0.0000	time 0.2158 (0.2639)	loss 1.5140 (1.4811)	grad_norm 0.5049 (inf)	loss_scale 8192.0000 (11862.9731)	mem 7984MB
[2024-06-29 17:19:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:04:49 lr 0.000005	 wd 0.0000	time 0.2422 (0.2627)	loss 1.6608 (1.4807)	grad_norm 0.6453 (inf)	loss_scale 8192.0000 (11600.9479)	mem 7984MB
[2024-06-29 17:19:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:04:21 lr 0.000005	 wd 0.0000	time 0.2369 (0.2607)	loss 1.5614 (1.4783)	grad_norm 0.5344 (inf)	loss_scale 8192.0000 (11373.8361)	mem 7984MB
[2024-06-29 17:19:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:03:53 lr 0.000005	 wd 0.0000	time 0.2298 (0.2588)	loss 1.6947 (1.4771)	grad_norm 0.4938 (inf)	loss_scale 8192.0000 (11175.0956)	mem 7984MB
[2024-06-29 17:20:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:03:27 lr 0.000005	 wd 0.0000	time 1.0682 (0.2589)	loss 1.5825 (1.4743)	grad_norm 0.5210 (inf)	loss_scale 8192.0000 (10999.7225)	mem 7984MB
[2024-06-29 17:20:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:01 lr 0.000006	 wd 0.0000	time 0.2394 (0.2590)	loss 1.2605 (1.4730)	grad_norm 0.5254 (inf)	loss_scale 8192.0000 (10843.8245)	mem 7984MB
[2024-06-29 17:21:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:35 lr 0.000006	 wd 0.0000	time 0.2136 (0.2576)	loss 1.7041 (1.4705)	grad_norm 0.5252 (inf)	loss_scale 8192.0000 (10704.3282)	mem 7984MB
[2024-06-29 17:21:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:08 lr 0.000006	 wd 0.0000	time 0.2316 (0.2565)	loss 1.5861 (1.4666)	grad_norm 0.5082 (inf)	loss_scale 8192.0000 (10578.7746)	mem 7984MB
[2024-06-29 17:22:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:43 lr 0.000007	 wd 0.0000	time 0.2412 (0.2569)	loss 1.4552 (1.4656)	grad_norm 0.6315 (inf)	loss_scale 8192.0000 (10465.1728)	mem 7984MB
[2024-06-29 17:22:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:17 lr 0.000007	 wd 0.0000	time 0.2251 (0.2562)	loss 1.6778 (1.4648)	grad_norm 0.4694 (inf)	loss_scale 8192.0000 (10361.8937)	mem 7984MB
[2024-06-29 17:22:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:51 lr 0.000007	 wd 0.0000	time 0.2359 (0.2554)	loss 1.5710 (1.4622)	grad_norm 0.4775 (inf)	loss_scale 8192.0000 (10267.5915)	mem 7984MB
[2024-06-29 17:23:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:25 lr 0.000008	 wd 0.0000	time 0.2195 (0.2545)	loss 1.5243 (1.4609)	grad_norm 0.4713 (inf)	loss_scale 4096.0000 (10024.1966)	mem 7984MB
[2024-06-29 17:23:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.1860 (0.2528)	loss 1.6966 (1.4596)	grad_norm 0.4784 (inf)	loss_scale 4096.0000 (9787.1635)	mem 7984MB
[2024-06-29 17:23:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 0 training takes 0:10:40
[2024-06-29 17:23:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_0.pth saving......
[2024-06-29 17:23:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_0.pth saved !!!
[2024-06-29 17:24:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 47.514 (47.514)	Loss 0.4568 (0.4568)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 17:24:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 82.832 Acc@5 96.714
[2024-06-29 17:24:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 82.8%
[2024-06-29 17:24:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 82.83%
[2024-06-29 17:24:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 17:24:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 17:25:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][0/2502]	eta 1 day, 3:42:17 lr 0.000008	 wd 0.0000	time 39.8633 (39.8633)	loss 1.2826 (1.2826)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:25:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:25:17 lr 0.000008	 wd 0.0000	time 0.2151 (0.6316)	loss 1.2131 (1.4691)	grad_norm 0.4726 (0.5047)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:26:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:16:29 lr 0.000009	 wd 0.0000	time 0.2188 (0.4297)	loss 1.4286 (1.4659)	grad_norm 0.5861 (0.5106)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:26:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:13:28 lr 0.000009	 wd 0.0000	time 0.2672 (0.3673)	loss 1.7816 (1.4446)	grad_norm 0.4506 (0.5081)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:27:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:11:51 lr 0.000009	 wd 0.0000	time 0.2167 (0.3387)	loss 1.0199 (1.4326)	grad_norm 0.4713 (nan)	loss_scale 2048.0000 (3707.8504)	mem 7984MB
[2024-06-29 17:27:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:10:33 lr 0.000010	 wd 0.0000	time 0.2213 (0.3166)	loss 1.5789 (1.4304)	grad_norm 0.4594 (nan)	loss_scale 2048.0000 (3376.5429)	mem 7984MB
[2024-06-29 17:27:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:09:33 lr 0.000010	 wd 0.0000	time 0.2057 (0.3018)	loss 1.4660 (1.4290)	grad_norm 0.4523 (nan)	loss_scale 2048.0000 (3155.4875)	mem 7984MB
[2024-06-29 17:28:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:08:47 lr 0.000010	 wd 0.0000	time 0.2529 (0.2928)	loss 1.7092 (1.4269)	grad_norm 0.5154 (nan)	loss_scale 2048.0000 (2997.5007)	mem 7984MB
[2024-06-29 17:28:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:08:11 lr 0.000011	 wd 0.0000	time 0.2722 (0.2886)	loss 1.6577 (1.4306)	grad_norm 0.4162 (nan)	loss_scale 2048.0000 (2878.9613)	mem 7984MB
[2024-06-29 17:29:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:07:33 lr 0.000011	 wd 0.0000	time 0.2348 (0.2828)	loss 1.6246 (1.4273)	grad_norm 0.5088 (nan)	loss_scale 2048.0000 (2786.7347)	mem 7984MB
[2024-06-29 17:29:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:06:57 lr 0.000011	 wd 0.0000	time 0.2142 (0.2777)	loss 1.7175 (1.4254)	grad_norm 0.4593 (nan)	loss_scale 2048.0000 (2712.9351)	mem 7984MB
[2024-06-29 17:29:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:06:23 lr 0.000012	 wd 0.0000	time 0.2310 (0.2739)	loss 1.0834 (1.4241)	grad_norm 0.4423 (nan)	loss_scale 2048.0000 (2652.5413)	mem 7984MB
[2024-06-29 17:30:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:05:55 lr 0.000012	 wd 0.0000	time 0.2076 (0.2734)	loss 1.4292 (1.4264)	grad_norm 0.4524 (nan)	loss_scale 2048.0000 (2602.2048)	mem 7984MB
[2024-06-29 17:30:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:05:25 lr 0.000012	 wd 0.0000	time 0.2137 (0.2707)	loss 1.5872 (1.4295)	grad_norm 0.4700 (nan)	loss_scale 2048.0000 (2559.6065)	mem 7984MB
[2024-06-29 17:31:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:04:55 lr 0.000012	 wd 0.0000	time 0.2304 (0.2682)	loss 1.4731 (1.4269)	grad_norm 0.4885 (nan)	loss_scale 2048.0000 (2523.0892)	mem 7984MB
[2024-06-29 17:31:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:04:26 lr 0.000013	 wd 0.0000	time 0.2816 (0.2663)	loss 0.9954 (1.4249)	grad_norm 0.5311 (nan)	loss_scale 2048.0000 (2491.4377)	mem 7984MB
[2024-06-29 17:31:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:00 lr 0.000013	 wd 0.0000	time 0.2396 (0.2664)	loss 0.9786 (1.4230)	grad_norm 0.4655 (nan)	loss_scale 2048.0000 (2463.7402)	mem 7984MB
[2024-06-29 17:32:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:03:32 lr 0.000013	 wd 0.0000	time 0.2407 (0.2648)	loss 1.3988 (1.4219)	grad_norm 0.4801 (nan)	loss_scale 2048.0000 (2439.2992)	mem 7984MB
[2024-06-29 17:32:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:04 lr 0.000014	 wd 0.0000	time 0.2451 (0.2632)	loss 1.4985 (1.4218)	grad_norm 0.4694 (nan)	loss_scale 2048.0000 (2417.5725)	mem 7984MB
[2024-06-29 17:33:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:37 lr 0.000014	 wd 0.0000	time 0.2563 (0.2621)	loss 1.4939 (1.4225)	grad_norm 0.4766 (nan)	loss_scale 2048.0000 (2398.1315)	mem 7984MB
[2024-06-29 17:33:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:11 lr 0.000014	 wd 0.0000	time 0.2251 (0.2623)	loss 1.4027 (1.4215)	grad_norm 0.4535 (nan)	loss_scale 2048.0000 (2380.6337)	mem 7984MB
[2024-06-29 17:33:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:45 lr 0.000015	 wd 0.0000	time 0.2248 (0.2613)	loss 1.4551 (1.4223)	grad_norm 0.4391 (nan)	loss_scale 2048.0000 (2364.8015)	mem 7984MB
[2024-06-29 17:34:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:18 lr 0.000015	 wd 0.0000	time 0.2081 (0.2601)	loss 1.2335 (1.4233)	grad_norm 0.4986 (nan)	loss_scale 2048.0000 (2350.4080)	mem 7984MB
[2024-06-29 17:34:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:52 lr 0.000015	 wd 0.0000	time 0.2465 (0.2593)	loss 1.6981 (1.4237)	grad_norm 0.5373 (nan)	loss_scale 2048.0000 (2337.2655)	mem 7984MB
[2024-06-29 17:35:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:27 lr 0.000016	 wd 0.0000	time 0.3139 (0.2684)	loss 1.5147 (1.4214)	grad_norm 0.4578 (nan)	loss_scale 2048.0000 (2325.2178)	mem 7984MB
[2024-06-29 17:35:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.1689 (0.2665)	loss 1.1257 (1.4226)	grad_norm 0.4818 (nan)	loss_scale 2048.0000 (2314.1335)	mem 7984MB
[2024-06-29 17:36:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 1 training takes 0:11:12
[2024-06-29 17:37:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 75.651 (75.651)	Loss 0.4404 (0.4404)	Acc@1 91.211 (91.211)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 17:37:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.202 Acc@5 96.858
[2024-06-29 17:37:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.2%
[2024-06-29 17:37:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.20%
[2024-06-29 17:37:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 17:37:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 17:38:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][0/2502]	eta 1 day, 3:25:32 lr 0.000016	 wd 0.0000	time 39.4613 (39.4613)	loss 1.6902 (1.6902)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:38:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:24:55 lr 0.000016	 wd 0.0000	time 0.2144 (0.6227)	loss 1.4848 (1.3877)	grad_norm 0.4907 (0.4918)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:39:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:16:22 lr 0.000017	 wd 0.0000	time 0.2363 (0.4269)	loss 1.4898 (1.4077)	grad_norm 0.4281 (0.4864)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:39:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:13:17 lr 0.000017	 wd 0.0000	time 0.2370 (0.3620)	loss 1.3198 (1.4184)	grad_norm 0.4966 (0.5007)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:39:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:11:45 lr 0.000017	 wd 0.0000	time 0.2384 (0.3358)	loss 1.5964 (1.4157)	grad_norm 0.4773 (0.4976)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:40:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:10:30 lr 0.000018	 wd 0.0000	time 0.2066 (0.3150)	loss 1.5875 (1.4144)	grad_norm 0.4547 (0.4916)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:40:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:09:32 lr 0.000018	 wd 0.0000	time 0.2357 (0.3009)	loss 1.3117 (1.4088)	grad_norm 0.4403 (0.4880)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:41:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:08:43 lr 0.000018	 wd 0.0000	time 0.2472 (0.2906)	loss 1.2764 (1.4108)	grad_norm 0.5287 (0.4881)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:41:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:08:07 lr 0.000019	 wd 0.0000	time 0.2200 (0.2866)	loss 1.4758 (1.4089)	grad_norm 0.4249 (0.4855)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:41:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:07:30 lr 0.000019	 wd 0.0000	time 0.2032 (0.2814)	loss 1.5736 (1.4134)	grad_norm 0.4767 (0.4871)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:42:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:06:55 lr 0.000019	 wd 0.0000	time 0.1988 (0.2767)	loss 1.4832 (1.4145)	grad_norm 0.4646 (0.4871)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:42:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:06:22 lr 0.000020	 wd 0.0000	time 0.2273 (0.2726)	loss 1.2281 (1.4139)	grad_norm 0.4450 (0.4857)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:43:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:05:55 lr 0.000020	 wd 0.0000	time 1.6130 (0.2734)	loss 1.2860 (1.4127)	grad_norm 0.4352 (0.4849)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:43:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:05:25 lr 0.000020	 wd 0.0000	time 0.2242 (0.2706)	loss 1.7753 (1.4154)	grad_norm 0.5551 (0.4840)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:43:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:04:55 lr 0.000020	 wd 0.0000	time 0.2206 (0.2681)	loss 1.4826 (1.4152)	grad_norm 0.4621 (0.4836)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:44:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:04:26 lr 0.000021	 wd 0.0000	time 0.2328 (0.2658)	loss 1.5585 (1.4129)	grad_norm 0.4823 (0.4838)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:44:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:03:59 lr 0.000021	 wd 0.0000	time 0.2395 (0.2654)	loss 1.3426 (1.4130)	grad_norm 0.5288 (0.4838)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:45:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:03:32 lr 0.000021	 wd 0.0000	time 0.2162 (0.2646)	loss 1.4536 (1.4124)	grad_norm 0.4932 (0.4833)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:45:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:04 lr 0.000022	 wd 0.0000	time 0.2104 (0.2631)	loss 1.3889 (1.4124)	grad_norm 0.4497 (0.4830)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 17:45:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:02:37 lr 0.000022	 wd 0.0000	time 0.2452 (0.2616)	loss 1.1065 (1.4107)	grad_norm 0.4370 (0.4826)	loss_scale 4096.0000 (2132.0316)	mem 7984MB
[2024-06-29 17:46:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:11 lr 0.000022	 wd 0.0000	time 0.2383 (0.2613)	loss 1.1485 (1.4087)	grad_norm 0.4372 (0.4869)	loss_scale 4096.0000 (2230.1809)	mem 7984MB
[2024-06-29 17:46:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:44 lr 0.000023	 wd 0.0000	time 0.2206 (0.2611)	loss 1.4856 (1.4089)	grad_norm 1.2228 (0.4872)	loss_scale 4096.0000 (2318.9871)	mem 7984MB
[2024-06-29 17:47:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:18 lr 0.000023	 wd 0.0000	time 0.2246 (0.2599)	loss 1.5433 (1.4087)	grad_norm 0.4422 (0.4881)	loss_scale 4096.0000 (2399.7238)	mem 7984MB
[2024-06-29 17:47:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:52 lr 0.000023	 wd 0.0000	time 0.2480 (0.2589)	loss 1.6182 (1.4086)	grad_norm 0.5315 (0.4880)	loss_scale 4096.0000 (2473.4429)	mem 7984MB
[2024-06-29 17:48:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:26 lr 0.000024	 wd 0.0000	time 0.2313 (0.2587)	loss 1.4825 (1.4072)	grad_norm 0.4427 (0.4874)	loss_scale 4096.0000 (2541.0212)	mem 7984MB
[2024-06-29 17:48:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.1936 (0.2566)	loss 1.4719 (1.4070)	grad_norm 0.4743 (0.4877)	loss_scale 4096.0000 (2603.1955)	mem 7984MB
[2024-06-29 17:48:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 2 training takes 0:10:47
[2024-06-29 17:48:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 24.564 (24.564)	Loss 0.4321 (0.4321)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 17:49:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.316 Acc@5 96.978
[2024-06-29 17:49:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.3%
[2024-06-29 17:49:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.32%
[2024-06-29 17:49:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 17:49:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 17:49:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][0/2502]	eta 20:37:17 lr 0.000024	 wd 0.0000	time 29.6712 (29.6712)	loss 1.1040 (1.1040)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:50:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:21:36 lr 0.000024	 wd 0.0000	time 0.2409 (0.5400)	loss 1.6971 (1.3886)	grad_norm 0.5163 (0.4899)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:50:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:14:44 lr 0.000025	 wd 0.0000	time 0.2343 (0.3841)	loss 1.5379 (1.4022)	grad_norm 0.4510 (0.4802)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:50:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:12:10 lr 0.000025	 wd 0.0000	time 0.2062 (0.3316)	loss 1.4437 (1.3900)	grad_norm 0.4422 (0.4869)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:51:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:10:56 lr 0.000025	 wd 0.0000	time 0.2777 (0.3125)	loss 1.5718 (1.3964)	grad_norm 0.4300 (0.4927)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:51:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:09:54 lr 0.000026	 wd 0.0000	time 0.2243 (0.2970)	loss 1.4397 (1.3935)	grad_norm 0.5082 (0.4937)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:51:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:09:04 lr 0.000026	 wd 0.0000	time 0.2164 (0.2861)	loss 1.4868 (1.3926)	grad_norm 0.5097 (0.4924)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:52:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:08:21 lr 0.000026	 wd 0.0000	time 0.2443 (0.2783)	loss 1.5356 (1.3943)	grad_norm 0.7310 (0.4967)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:52:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:07:48 lr 0.000027	 wd 0.0000	time 0.2612 (0.2753)	loss 1.1493 (1.3950)	grad_norm 0.4351 (0.5012)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:53:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:07:15 lr 0.000027	 wd 0.0000	time 0.2164 (0.2720)	loss 1.3922 (1.3964)	grad_norm 0.5252 (0.5004)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:53:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:06:42 lr 0.000027	 wd 0.0000	time 0.2296 (0.2680)	loss 1.6293 (1.3998)	grad_norm 0.4740 (0.4978)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:53:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:06:10 lr 0.000028	 wd 0.0000	time 0.2143 (0.2645)	loss 1.4410 (1.4020)	grad_norm 0.4479 (0.4972)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:54:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:05:42 lr 0.000028	 wd 0.0000	time 0.2464 (0.2632)	loss 1.3333 (1.4008)	grad_norm 0.4083 (0.4962)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:54:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:05:15 lr 0.000028	 wd 0.0000	time 0.2295 (0.2627)	loss 1.2510 (1.4034)	grad_norm 0.4580 (0.4943)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:55:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:04:47 lr 0.000028	 wd 0.0000	time 0.2328 (0.2608)	loss 1.2643 (1.4040)	grad_norm 0.4868 (0.4936)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:55:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:04:19 lr 0.000029	 wd 0.0000	time 0.2059 (0.2588)	loss 1.4263 (1.4041)	grad_norm 0.4925 (0.4922)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:55:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:03:52 lr 0.000029	 wd 0.0000	time 0.2273 (0.2579)	loss 1.4386 (1.4028)	grad_norm 0.4331 (0.4913)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:56:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:03:26 lr 0.000029	 wd 0.0000	time 0.2273 (0.2580)	loss 1.0897 (1.4016)	grad_norm 0.4238 (0.4912)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:56:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:00 lr 0.000030	 wd 0.0000	time 0.2320 (0.2569)	loss 1.6825 (1.4030)	grad_norm 0.4849 (0.4899)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:57:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:02:33 lr 0.000030	 wd 0.0000	time 0.2091 (0.2556)	loss 1.5691 (1.4026)	grad_norm 0.5944 (0.4897)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:57:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:08 lr 0.000030	 wd 0.0000	time 0.2257 (0.2552)	loss 1.1690 (1.4032)	grad_norm 0.4396 (0.4909)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:58:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:48 lr 0.000031	 wd 0.0000	time 0.3353 (0.2710)	loss 1.2505 (1.4014)	grad_norm 0.4546 (0.4904)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 17:59:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:21 lr 0.000031	 wd 0.0000	time 0.2385 (0.2710)	loss 1.3812 (1.4023)	grad_norm 0.4649 (0.4893)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:00:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:00 lr 0.000031	 wd 0.0000	time 0.2913 (0.3019)	loss 1.5809 (1.4028)	grad_norm 0.4571 (0.4893)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:01:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:31 lr 0.000032	 wd 0.0000	time 0.2100 (0.3048)	loss 1.0225 (1.4019)	grad_norm 0.4948 (0.4894)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:01:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000032	 wd 0.0000	time 0.1884 (0.3008)	loss 1.5796 (1.4022)	grad_norm 0.4451 (0.4895)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:01:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 3 training takes 0:12:36
[2024-06-29 18:02:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 22.981 (22.981)	Loss 0.4341 (0.4341)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 18:02:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.486 Acc@5 97.040
[2024-06-29 18:02:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.5%
[2024-06-29 18:02:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.49%
[2024-06-29 18:02:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 18:02:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 18:02:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][0/2502]	eta 1 day, 1:19:22 lr 0.000032	 wd 0.0000	time 36.4359 (36.4359)	loss 1.4693 (1.4693)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:03:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:23:52 lr 0.000032	 wd 0.0000	time 0.2113 (0.5962)	loss 1.1796 (1.4128)	grad_norm 0.5051 (0.5048)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:03:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:15:50 lr 0.000033	 wd 0.0000	time 0.2288 (0.4129)	loss 1.1664 (1.4131)	grad_norm 0.5606 (0.5120)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:04:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:13:19 lr 0.000033	 wd 0.0000	time 0.2779 (0.3629)	loss 1.1327 (1.4120)	grad_norm 0.4793 (0.5034)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:04:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:11:40 lr 0.000033	 wd 0.0000	time 0.2112 (0.3334)	loss 1.5957 (1.4071)	grad_norm 0.7069 (0.4994)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:04:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:10:25 lr 0.000034	 wd 0.0000	time 0.2088 (0.3123)	loss 1.3540 (1.4005)	grad_norm 0.4794 (0.4946)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:05:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:09:27 lr 0.000034	 wd 0.0000	time 0.1976 (0.2981)	loss 1.5262 (1.3963)	grad_norm 1.0048 (0.4931)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:05:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:08:44 lr 0.000034	 wd 0.0000	time 0.2225 (0.2910)	loss 1.3332 (1.3962)	grad_norm 0.4640 (0.4941)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:06:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:08:05 lr 0.000035	 wd 0.0000	time 0.2048 (0.2853)	loss 1.5268 (1.3954)	grad_norm 0.4745 (0.4951)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:06:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:07:27 lr 0.000035	 wd 0.0000	time 0.2287 (0.2792)	loss 1.5274 (1.3967)	grad_norm 0.4688 (0.4930)	loss_scale 8192.0000 (4468.7769)	mem 7984MB
[2024-06-29 18:06:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:06:52 lr 0.000035	 wd 0.0000	time 0.2247 (0.2745)	loss 1.5751 (1.3971)	grad_norm 0.4362 (0.4924)	loss_scale 8192.0000 (4840.7273)	mem 7984MB
[2024-06-29 18:07:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:06:21 lr 0.000036	 wd 0.0000	time 0.2377 (0.2721)	loss 1.5719 (1.3979)	grad_norm 0.5841 (0.4904)	loss_scale 8192.0000 (5145.1117)	mem 7984MB
[2024-06-29 18:07:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:05:52 lr 0.000036	 wd 0.0000	time 0.3362 (0.2706)	loss 1.1058 (1.3962)	grad_norm 0.4365 (0.4908)	loss_scale 8192.0000 (5398.8077)	mem 7984MB
[2024-06-29 18:08:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:05:21 lr 0.000036	 wd 0.0000	time 0.2262 (0.2677)	loss 1.6121 (1.3953)	grad_norm 0.4658 (0.4902)	loss_scale 8192.0000 (5613.5035)	mem 7984MB
[2024-06-29 18:08:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:04:52 lr 0.000036	 wd 0.0000	time 0.1992 (0.2651)	loss 1.5333 (1.3952)	grad_norm 0.5032 (0.4893)	loss_scale 8192.0000 (5797.5503)	mem 7984MB
[2024-06-29 18:08:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:04:24 lr 0.000037	 wd 0.0000	time 0.2514 (0.2636)	loss 1.2832 (1.3976)	grad_norm 0.4451 (0.4886)	loss_scale 8192.0000 (5957.0740)	mem 7984MB
[2024-06-29 18:09:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:03:57 lr 0.000037	 wd 0.0000	time 0.2135 (0.2631)	loss 1.0711 (1.3970)	grad_norm 0.5164 (0.4869)	loss_scale 8192.0000 (6096.6696)	mem 7984MB
[2024-06-29 18:09:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:03:29 lr 0.000037	 wd 0.0000	time 0.2530 (0.2616)	loss 1.5872 (1.3963)	grad_norm 0.4524 (0.4873)	loss_scale 8192.0000 (6219.8519)	mem 7984MB
[2024-06-29 18:10:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:02 lr 0.000038	 wd 0.0000	time 0.2168 (0.2602)	loss 1.4010 (1.3961)	grad_norm 0.4483 (0.4884)	loss_scale 8192.0000 (6329.3548)	mem 7984MB
[2024-06-29 18:10:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:36 lr 0.000038	 wd 0.0000	time 0.2470 (0.2595)	loss 1.6642 (1.3958)	grad_norm 0.4456 (0.4885)	loss_scale 8192.0000 (6427.3372)	mem 7984MB
[2024-06-29 18:10:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:10 lr 0.000038	 wd 0.0000	time 0.3023 (0.2593)	loss 1.4941 (1.3951)	grad_norm 0.4920 (0.4889)	loss_scale 8192.0000 (6515.5262)	mem 7984MB
[2024-06-29 18:11:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:43 lr 0.000039	 wd 0.0000	time 0.2434 (0.2581)	loss 1.4497 (1.3955)	grad_norm 0.4683 (nan)	loss_scale 4096.0000 (6478.3475)	mem 7984MB
[2024-06-29 18:11:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:17 lr 0.000039	 wd 0.0000	time 0.2028 (0.2571)	loss 1.0111 (1.3930)	grad_norm 0.5149 (nan)	loss_scale 4096.0000 (6370.1081)	mem 7984MB
[2024-06-29 18:12:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:51 lr 0.000039	 wd 0.0000	time 0.2357 (0.2565)	loss 1.3390 (1.3932)	grad_norm 0.4488 (nan)	loss_scale 4096.0000 (6271.2768)	mem 7984MB
[2024-06-29 18:12:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:26 lr 0.000040	 wd 0.0000	time 0.2032 (0.2618)	loss 1.6155 (1.3930)	grad_norm 0.4691 (nan)	loss_scale 4096.0000 (6180.6781)	mem 7984MB
[2024-06-29 18:13:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.1723 (0.2600)	loss 0.9461 (1.3927)	grad_norm 0.4202 (nan)	loss_scale 4096.0000 (6097.3243)	mem 7984MB
[2024-06-29 18:13:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 4 training takes 0:10:57
[2024-06-29 18:14:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 97.207 (97.207)	Loss 0.4331 (0.4331)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 18:15:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.560 Acc@5 97.094
[2024-06-29 18:15:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-06-29 18:15:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.56%
[2024-06-29 18:15:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 18:15:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 18:15:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][0/2502]	eta 1 day, 6:04:02 lr 0.000040	 wd 0.0000	time 43.2623 (43.2623)	loss 1.6152 (1.6152)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:16:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:26:14 lr 0.000040	 wd 0.0000	time 0.2148 (0.6555)	loss 1.2967 (1.4121)	grad_norm 0.4818 (0.4860)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:16:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:16:54 lr 0.000040	 wd 0.0000	time 0.2241 (0.4407)	loss 1.5031 (1.4138)	grad_norm 0.4641 (nan)	loss_scale 2048.0000 (3403.1443)	mem 7984MB
[2024-06-29 18:17:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:13:52 lr 0.000040	 wd 0.0000	time 0.2303 (0.3779)	loss 1.6351 (1.4039)	grad_norm 0.5087 (nan)	loss_scale 2048.0000 (2952.9302)	mem 7984MB
[2024-06-29 18:17:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:12:00 lr 0.000040	 wd 0.0000	time 0.2028 (0.3428)	loss 1.0206 (1.4056)	grad_norm 0.4720 (nan)	loss_scale 2048.0000 (2727.2618)	mem 7984MB
[2024-06-29 18:17:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:10:39 lr 0.000040	 wd 0.0000	time 0.2211 (0.3193)	loss 1.5602 (1.4007)	grad_norm 0.4349 (nan)	loss_scale 2048.0000 (2591.6806)	mem 7984MB
[2024-06-29 18:18:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:09:38 lr 0.000040	 wd 0.0000	time 0.2308 (0.3043)	loss 1.4544 (1.4016)	grad_norm 0.4539 (nan)	loss_scale 2048.0000 (2501.2180)	mem 7984MB
[2024-06-29 18:18:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:08:52 lr 0.000040	 wd 0.0000	time 0.2690 (0.2957)	loss 1.4832 (1.3990)	grad_norm 0.5000 (nan)	loss_scale 2048.0000 (2436.5649)	mem 7984MB
[2024-06-29 18:19:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:08:13 lr 0.000040	 wd 0.0000	time 0.2285 (0.2900)	loss 0.9666 (1.3928)	grad_norm 0.4261 (nan)	loss_scale 2048.0000 (2388.0549)	mem 7984MB
[2024-06-29 18:19:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:07:34 lr 0.000040	 wd 0.0000	time 0.2044 (0.2834)	loss 0.9157 (1.3921)	grad_norm 0.4622 (nan)	loss_scale 2048.0000 (2350.3130)	mem 7984MB
[2024-06-29 18:19:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:06:57 lr 0.000040	 wd 0.0000	time 0.2188 (0.2782)	loss 1.5414 (1.3952)	grad_norm 0.4246 (nan)	loss_scale 2048.0000 (2320.1119)	mem 7984MB
[2024-06-29 18:20:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:06:25 lr 0.000040	 wd 0.0000	time 0.2718 (0.2748)	loss 1.5850 (1.3911)	grad_norm 0.4461 (nan)	loss_scale 2048.0000 (2295.3969)	mem 7984MB
[2024-06-29 18:20:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:05:55 lr 0.000040	 wd 0.0000	time 0.2234 (0.2729)	loss 1.5657 (1.3889)	grad_norm 0.4444 (nan)	loss_scale 2048.0000 (2274.7977)	mem 7984MB
[2024-06-29 18:21:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:05:24 lr 0.000040	 wd 0.0000	time 0.2229 (0.2698)	loss 1.5245 (1.3893)	grad_norm 0.6314 (nan)	loss_scale 2048.0000 (2257.3651)	mem 7984MB
[2024-06-29 18:21:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:04:54 lr 0.000040	 wd 0.0000	time 0.2264 (0.2671)	loss 1.5491 (1.3887)	grad_norm 0.8054 (nan)	loss_scale 2048.0000 (2242.4211)	mem 7984MB
[2024-06-29 18:21:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:04:25 lr 0.000040	 wd 0.0000	time 0.2511 (0.2652)	loss 1.4166 (1.3886)	grad_norm 0.4460 (nan)	loss_scale 2048.0000 (2229.4684)	mem 7984MB
[2024-06-29 18:22:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:03:58 lr 0.000040	 wd 0.0000	time 0.2135 (0.2649)	loss 1.4823 (1.3904)	grad_norm 0.4621 (nan)	loss_scale 2048.0000 (2218.1337)	mem 7984MB
[2024-06-29 18:22:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:03:31 lr 0.000040	 wd 0.0000	time 0.2115 (0.2636)	loss 1.3745 (1.3899)	grad_norm 0.4408 (nan)	loss_scale 2048.0000 (2208.1317)	mem 7984MB
[2024-06-29 18:23:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:03 lr 0.000040	 wd 0.0000	time 0.2222 (0.2620)	loss 1.0043 (1.3892)	grad_norm 0.5173 (nan)	loss_scale 2048.0000 (2199.2404)	mem 7984MB
[2024-06-29 18:23:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:37 lr 0.000040	 wd 0.0000	time 0.2548 (0.2609)	loss 1.4940 (1.3912)	grad_norm 0.4683 (nan)	loss_scale 2048.0000 (2191.2846)	mem 7984MB
[2024-06-29 18:23:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:10 lr 0.000040	 wd 0.0000	time 0.2137 (0.2608)	loss 1.5339 (1.3922)	grad_norm 0.6028 (nan)	loss_scale 2048.0000 (2184.1239)	mem 7984MB
[2024-06-29 18:24:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:44 lr 0.000040	 wd 0.0000	time 0.2529 (0.2597)	loss 1.5246 (1.3926)	grad_norm 0.4283 (nan)	loss_scale 2048.0000 (2177.6449)	mem 7984MB
[2024-06-29 18:24:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:18 lr 0.000040	 wd 0.0000	time 0.2310 (0.2585)	loss 1.2708 (1.3905)	grad_norm 0.4403 (nan)	loss_scale 2048.0000 (2171.7547)	mem 7984MB
[2024-06-29 18:25:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:52 lr 0.000040	 wd 0.0000	time 0.2522 (0.2577)	loss 0.9115 (1.3895)	grad_norm 0.4192 (nan)	loss_scale 2048.0000 (2166.3764)	mem 7984MB
[2024-06-29 18:25:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:26 lr 0.000040	 wd 0.0000	time 0.2025 (0.2643)	loss 1.5369 (1.3895)	grad_norm 0.4611 (nan)	loss_scale 2048.0000 (2161.4461)	mem 7984MB
[2024-06-29 18:26:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.1704 (0.2624)	loss 1.5448 (1.3893)	grad_norm 0.4397 (nan)	loss_scale 2048.0000 (2156.9100)	mem 7984MB
[2024-06-29 18:26:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 5 training takes 0:11:03
[2024-06-29 18:27:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 87.081 (87.081)	Loss 0.4268 (0.4268)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 18:28:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.714 Acc@5 97.132
[2024-06-29 18:28:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-06-29 18:28:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.71%
[2024-06-29 18:28:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 18:28:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 18:28:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][0/2502]	eta 1 day, 7:01:13 lr 0.000040	 wd 0.0000	time 44.6338 (44.6338)	loss 1.5914 (1.5914)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:29:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:26:53 lr 0.000040	 wd 0.0000	time 0.1997 (0.6719)	loss 1.2138 (1.3575)	grad_norm 0.5451 (0.4869)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:29:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:17:18 lr 0.000040	 wd 0.0000	time 0.2166 (0.4513)	loss 1.6308 (1.3797)	grad_norm 0.4312 (0.4834)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:29:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:14:03 lr 0.000040	 wd 0.0000	time 0.2306 (0.3831)	loss 1.4913 (1.3900)	grad_norm 0.5927 (0.4814)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:30:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:12:16 lr 0.000040	 wd 0.0000	time 0.2211 (0.3502)	loss 1.3934 (1.3834)	grad_norm 0.5182 (0.4819)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:30:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:10:52 lr 0.000040	 wd 0.0000	time 0.2191 (0.3257)	loss 1.3832 (1.3777)	grad_norm 0.4151 (0.4858)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:31:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:09:49 lr 0.000040	 wd 0.0000	time 0.2140 (0.3098)	loss 1.6149 (1.3819)	grad_norm 0.4478 (0.4859)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:31:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:09:00 lr 0.000040	 wd 0.0000	time 0.2315 (0.2998)	loss 1.4354 (1.3844)	grad_norm 0.4846 (0.4910)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:31:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:08:20 lr 0.000040	 wd 0.0000	time 0.2150 (0.2943)	loss 1.0233 (1.3831)	grad_norm 0.4603 (0.4895)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:32:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:07:40 lr 0.000040	 wd 0.0000	time 0.2124 (0.2874)	loss 1.6737 (1.3872)	grad_norm 0.4035 (0.4888)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:32:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:07:03 lr 0.000040	 wd 0.0000	time 0.2031 (0.2818)	loss 1.3224 (1.3825)	grad_norm 0.4625 (0.4890)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:33:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:06:29 lr 0.000040	 wd 0.0000	time 0.2806 (0.2778)	loss 1.2169 (1.3817)	grad_norm 0.4802 (0.4886)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:33:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:06:00 lr 0.000040	 wd 0.0000	time 0.2259 (0.2767)	loss 1.0770 (1.3833)	grad_norm 0.4842 (0.4870)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:33:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:05:28 lr 0.000040	 wd 0.0000	time 0.2186 (0.2736)	loss 1.4156 (1.3824)	grad_norm 0.4377 (0.4904)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:34:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:04:58 lr 0.000040	 wd 0.0000	time 0.2239 (0.2708)	loss 1.6831 (1.3809)	grad_norm 0.4450 (0.4909)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:34:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:04:29 lr 0.000040	 wd 0.0000	time 0.2285 (0.2686)	loss 1.4753 (1.3823)	grad_norm 0.4464 (0.4906)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:35:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:04:01 lr 0.000040	 wd 0.0000	time 0.2167 (0.2681)	loss 1.5526 (1.3810)	grad_norm 0.4860 (0.4916)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 18:35:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:03:33 lr 0.000040	 wd 0.0000	time 0.2518 (0.2664)	loss 1.5337 (1.3802)	grad_norm 0.4401 (0.4924)	loss_scale 4096.0000 (2132.2798)	mem 7984MB
[2024-06-29 18:36:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:05 lr 0.000040	 wd 0.0000	time 0.2206 (0.2648)	loss 1.1833 (1.3799)	grad_norm 0.4848 (0.4911)	loss_scale 4096.0000 (2241.3148)	mem 7984MB
[2024-06-29 18:36:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:38 lr 0.000040	 wd 0.0000	time 0.2552 (0.2635)	loss 1.2840 (1.3793)	grad_norm 0.4533 (0.4915)	loss_scale 4096.0000 (2338.8785)	mem 7984MB
[2024-06-29 18:36:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:12 lr 0.000039	 wd 0.0000	time 0.2595 (0.2638)	loss 0.9964 (1.3791)	grad_norm 0.4618 (0.4903)	loss_scale 4096.0000 (2426.6907)	mem 7984MB
[2024-06-29 18:37:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:45 lr 0.000039	 wd 0.0000	time 0.2068 (0.2628)	loss 1.5811 (1.3787)	grad_norm 0.5145 (0.4899)	loss_scale 4096.0000 (2506.1437)	mem 7984MB
[2024-06-29 18:37:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:18 lr 0.000039	 wd 0.0000	time 0.2325 (0.2616)	loss 1.2815 (1.3792)	grad_norm 0.5010 (0.4895)	loss_scale 4096.0000 (2578.3771)	mem 7984MB
[2024-06-29 18:38:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:52 lr 0.000039	 wd 0.0000	time 0.2239 (0.2606)	loss 1.4749 (1.3799)	grad_norm 0.4510 (0.4888)	loss_scale 4096.0000 (2644.3320)	mem 7984MB
[2024-06-29 18:38:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:27 lr 0.000039	 wd 0.0000	time 0.2867 (0.2722)	loss 1.2073 (1.3790)	grad_norm 0.4029 (0.4892)	loss_scale 4096.0000 (2704.7930)	mem 7984MB
[2024-06-29 18:39:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.1800 (0.2710)	loss 1.0562 (1.3797)	grad_norm 0.4453 (0.4884)	loss_scale 4096.0000 (2760.4190)	mem 7984MB
[2024-06-29 18:39:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 6 training takes 0:11:24
[2024-06-29 18:40:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 89.691 (89.691)	Loss 0.4282 (0.4282)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 18:41:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.786 Acc@5 97.158
[2024-06-29 18:41:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-06-29 18:41:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.79%
[2024-06-29 18:41:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 18:41:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 18:41:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][0/2502]	eta 1 day, 5:22:44 lr 0.000039	 wd 0.0000	time 42.2719 (42.2719)	loss 1.6188 (1.6188)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:42:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:26:05 lr 0.000039	 wd 0.0000	time 0.2190 (0.6518)	loss 1.2508 (1.4034)	grad_norm 0.8423 (0.4815)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:42:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:16:51 lr 0.000039	 wd 0.0000	time 0.1983 (0.4392)	loss 1.4723 (1.3776)	grad_norm 0.4594 (0.4856)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:43:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:13:52 lr 0.000039	 wd 0.0000	time 0.2529 (0.3782)	loss 1.3200 (1.3758)	grad_norm 0.4562 (0.4803)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:43:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:01 lr 0.000039	 wd 0.0000	time 0.2121 (0.3434)	loss 1.3450 (1.3778)	grad_norm 0.4439 (0.4781)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:43:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:10:40 lr 0.000039	 wd 0.0000	time 0.2210 (0.3200)	loss 1.4559 (1.3728)	grad_norm 0.4309 (0.4819)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:44:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:09:40 lr 0.000039	 wd 0.0000	time 0.2486 (0.3052)	loss 1.3553 (1.3766)	grad_norm 0.4238 (0.4825)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:44:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:08:55 lr 0.000039	 wd 0.0000	time 0.2403 (0.2974)	loss 1.4877 (1.3805)	grad_norm 0.4570 (0.4828)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:45:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:08:15 lr 0.000039	 wd 0.0000	time 0.2182 (0.2913)	loss 0.9220 (1.3827)	grad_norm 0.4718 (0.4822)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:45:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:07:36 lr 0.000039	 wd 0.0000	time 0.2618 (0.2848)	loss 1.0974 (1.3845)	grad_norm 0.5076 (0.4832)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:45:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:06:59 lr 0.000039	 wd 0.0000	time 0.2068 (0.2794)	loss 1.1917 (1.3803)	grad_norm 0.4598 (0.4817)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:46:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:06:27 lr 0.000039	 wd 0.0000	time 0.2187 (0.2766)	loss 1.2608 (1.3788)	grad_norm 0.4130 (0.4814)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:46:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:05:57 lr 0.000039	 wd 0.0000	time 0.2290 (0.2747)	loss 1.4334 (1.3776)	grad_norm 0.4927 (0.4813)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:47:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:05:26 lr 0.000039	 wd 0.0000	time 0.2420 (0.2716)	loss 1.4980 (1.3775)	grad_norm 0.4365 (0.4807)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:47:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:04:55 lr 0.000039	 wd 0.0000	time 0.2270 (0.2685)	loss 1.4274 (1.3782)	grad_norm 0.4265 (0.4827)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:47:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:04:27 lr 0.000039	 wd 0.0000	time 0.2696 (0.2671)	loss 1.6569 (1.3774)	grad_norm 0.4766 (0.4826)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:48:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:04:00 lr 0.000039	 wd 0.0000	time 0.2668 (0.2664)	loss 1.5358 (1.3792)	grad_norm 0.4561 (0.4822)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:48:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:03:32 lr 0.000039	 wd 0.0000	time 0.2443 (0.2645)	loss 1.5870 (1.3796)	grad_norm 0.4537 (0.4841)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:49:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:04 lr 0.000039	 wd 0.0000	time 0.2369 (0.2629)	loss 1.1463 (1.3790)	grad_norm 0.5084 (0.4835)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:49:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:37 lr 0.000039	 wd 0.0000	time 0.2697 (0.2622)	loss 1.3379 (1.3785)	grad_norm 0.4642 (0.4833)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:49:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:11 lr 0.000039	 wd 0.0000	time 0.2269 (0.2620)	loss 1.4625 (1.3796)	grad_norm 0.4216 (0.4828)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:50:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:44 lr 0.000039	 wd 0.0000	time 0.2491 (0.2609)	loss 1.5111 (1.3807)	grad_norm 0.5613 (0.4829)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:50:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:18 lr 0.000039	 wd 0.0000	time 0.2663 (0.2595)	loss 1.1245 (1.3787)	grad_norm 0.4192 (0.4842)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:51:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:52 lr 0.000039	 wd 0.0000	time 0.2261 (0.2590)	loss 1.6003 (1.3792)	grad_norm 0.4348 (0.4835)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:51:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:26 lr 0.000039	 wd 0.0000	time 0.2077 (0.2589)	loss 1.4809 (1.3787)	grad_norm 0.4347 (0.4827)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:51:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.1802 (0.2567)	loss 1.2890 (1.3790)	grad_norm 0.4806 (0.4826)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:52:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 7 training takes 0:10:46
[2024-06-29 18:52:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 20.640 (20.640)	Loss 0.4321 (0.4321)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 7984MB
[2024-06-29 18:52:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.804 Acc@5 97.158
[2024-06-29 18:52:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-06-29 18:52:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.80%
[2024-06-29 18:52:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 18:52:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 18:53:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][0/2502]	eta 1 day, 22:05:09 lr 0.000039	 wd 0.0000	time 66.3107 (66.3107)	loss 1.4402 (1.4402)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:54:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:39:03 lr 0.000039	 wd 0.0000	time 0.2265 (0.9758)	loss 1.4617 (1.4145)	grad_norm 0.4374 (0.4709)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:54:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:25:06 lr 0.000039	 wd 0.0000	time 0.2242 (0.6545)	loss 1.5520 (1.3896)	grad_norm 0.4173 (0.4781)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:55:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:18:48 lr 0.000038	 wd 0.0000	time 0.2254 (0.5126)	loss 1.3773 (1.3866)	grad_norm 0.4515 (0.4802)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:55:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:15:27 lr 0.000038	 wd 0.0000	time 0.2183 (0.4415)	loss 1.6419 (1.3829)	grad_norm 0.4493 (0.4840)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:55:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:13:23 lr 0.000038	 wd 0.0000	time 0.2849 (0.4014)	loss 1.4704 (1.3858)	grad_norm 0.4737 (0.4853)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:56:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:11:56 lr 0.000038	 wd 0.0000	time 0.2001 (0.3767)	loss 1.5535 (1.3907)	grad_norm 0.4243 (0.4857)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 18:56:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:10:41 lr 0.000038	 wd 0.0000	time 0.1933 (0.3558)	loss 1.6654 (1.3890)	grad_norm 0.7725 (0.4853)	loss_scale 8192.0000 (4528.3880)	mem 7984MB
[2024-06-29 18:57:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:38 lr 0.000038	 wd 0.0000	time 0.2310 (0.3400)	loss 1.4895 (1.3866)	grad_norm 0.4183 (0.4839)	loss_scale 8192.0000 (4985.7678)	mem 7984MB
[2024-06-29 18:57:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:08:46 lr 0.000038	 wd 0.0000	time 0.2349 (0.3284)	loss 1.6758 (1.3867)	grad_norm 0.4326 (0.4827)	loss_scale 8192.0000 (5341.6204)	mem 7984MB
[2024-06-29 18:57:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:02 lr 0.000038	 wd 0.0000	time 0.2095 (0.3213)	loss 1.5114 (1.3828)	grad_norm 0.4914 (0.4831)	loss_scale 8192.0000 (5626.3736)	mem 7984MB
[2024-06-29 18:58:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:19 lr 0.000038	 wd 0.0000	time 0.2418 (0.3134)	loss 1.3487 (1.3797)	grad_norm 0.4787 (0.4821)	loss_scale 8192.0000 (5859.4005)	mem 7984MB
[2024-06-29 18:58:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:06:39 lr 0.000038	 wd 0.0000	time 0.2251 (0.3068)	loss 1.4653 (1.3799)	grad_norm 0.4621 (0.4826)	loss_scale 8192.0000 (6053.6220)	mem 7984MB
[2024-06-29 18:59:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:02 lr 0.000038	 wd 0.0000	time 0.2274 (0.3015)	loss 1.1950 (1.3783)	grad_norm 0.4570 (0.4822)	loss_scale 8192.0000 (6217.9862)	mem 7984MB
[2024-06-29 18:59:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:05:29 lr 0.000038	 wd 0.0000	time 0.2400 (0.2989)	loss 0.9682 (1.3800)	grad_norm 0.4982 (0.4814)	loss_scale 8192.0000 (6358.8865)	mem 7984MB
[2024-06-29 18:59:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:04:55 lr 0.000038	 wd 0.0000	time 0.2008 (0.2946)	loss 0.8410 (1.3778)	grad_norm 0.4595 (0.4806)	loss_scale 8192.0000 (6481.0127)	mem 7984MB
[2024-06-29 19:00:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:04:22 lr 0.000038	 wd 0.0000	time 0.2209 (0.2908)	loss 1.2387 (1.3750)	grad_norm 0.4800 (0.4794)	loss_scale 8192.0000 (6587.8826)	mem 7984MB
[2024-06-29 19:00:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:03:50 lr 0.000038	 wd 0.0000	time 0.2329 (0.2878)	loss 1.1954 (1.3768)	grad_norm 0.4567 (0.4796)	loss_scale 8192.0000 (6682.1869)	mem 7984MB
[2024-06-29 19:01:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:21 lr 0.000038	 wd 0.0000	time 0.2206 (0.2867)	loss 1.5483 (1.3778)	grad_norm 0.4259 (0.4808)	loss_scale 8192.0000 (6766.0189)	mem 7984MB
[2024-06-29 19:01:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:51 lr 0.000038	 wd 0.0000	time 0.1872 (0.2842)	loss 1.3264 (1.3769)	grad_norm 0.4430 (0.4811)	loss_scale 8192.0000 (6841.0310)	mem 7984MB
[2024-06-29 19:01:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:21 lr 0.000038	 wd 0.0000	time 0.2119 (0.2818)	loss 1.7035 (1.3789)	grad_norm 0.5851 (0.4825)	loss_scale 8192.0000 (6908.5457)	mem 7984MB
[2024-06-29 19:02:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:52 lr 0.000038	 wd 0.0000	time 0.2182 (0.2797)	loss 1.4171 (1.3780)	grad_norm 0.4474 (0.4817)	loss_scale 8192.0000 (6969.6335)	mem 7984MB
[2024-06-29 19:02:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:24 lr 0.000038	 wd 0.0000	time 0.2724 (0.2787)	loss 1.5746 (1.3766)	grad_norm 0.4950 (0.4828)	loss_scale 8192.0000 (7025.1704)	mem 7984MB
[2024-06-29 19:03:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:55 lr 0.000038	 wd 0.0000	time 0.2153 (0.2769)	loss 1.5131 (1.3772)	grad_norm 0.6229 (nan)	loss_scale 4096.0000 (6919.2316)	mem 7984MB
[2024-06-29 19:03:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:28 lr 0.000038	 wd 0.0000	time 0.2312 (0.2751)	loss 1.3227 (1.3787)	grad_norm 0.5112 (nan)	loss_scale 4096.0000 (6801.6460)	mem 7984MB
[2024-06-29 19:03:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000038	 wd 0.0000	time 0.1839 (0.2725)	loss 1.7639 (1.3801)	grad_norm 0.4143 (nan)	loss_scale 4096.0000 (6693.4634)	mem 7984MB
[2024-06-29 19:04:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 8 training takes 0:11:26
[2024-06-29 19:04:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 38.298 (38.298)	Loss 0.4299 (0.4299)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 7984MB
[2024-06-29 19:04:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.832 Acc@5 97.192
[2024-06-29 19:04:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-06-29 19:04:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.83%
[2024-06-29 19:04:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 19:04:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 19:05:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][0/2502]	eta 10:09:14 lr 0.000038	 wd 0.0000	time 14.6101 (14.6101)	loss 1.3944 (1.3944)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:05:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:15:14 lr 0.000038	 wd 0.0000	time 0.2325 (0.3808)	loss 1.2572 (1.3444)	grad_norm 0.4694 (0.4903)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:05:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:12:23 lr 0.000037	 wd 0.0000	time 0.2214 (0.3231)	loss 1.5206 (1.3569)	grad_norm 0.4806 (0.4844)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:06:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:10:42 lr 0.000037	 wd 0.0000	time 0.2299 (0.2919)	loss 1.4460 (1.3688)	grad_norm 0.4392 (0.4917)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:06:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:09:39 lr 0.000037	 wd 0.0000	time 0.2031 (0.2756)	loss 1.5823 (1.3635)	grad_norm 0.4384 (0.4888)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:07:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:08:51 lr 0.000037	 wd 0.0000	time 0.2505 (0.2655)	loss 1.4061 (1.3701)	grad_norm 0.4371 (0.4858)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:07:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:08:22 lr 0.000037	 wd 0.0000	time 0.2771 (0.2644)	loss 1.5275 (1.3646)	grad_norm 0.4731 (0.4897)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:07:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:07:48 lr 0.000037	 wd 0.0000	time 0.2027 (0.2602)	loss 1.5415 (1.3659)	grad_norm 0.5094 (0.4894)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:08:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:07:16 lr 0.000037	 wd 0.0000	time 0.1979 (0.2564)	loss 1.4042 (1.3690)	grad_norm 0.4896 (0.4907)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:08:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:06:46 lr 0.000037	 wd 0.0000	time 0.2281 (0.2535)	loss 1.6166 (1.3659)	grad_norm 0.4251 (0.4919)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:09:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:06:20 lr 0.000037	 wd 0.0000	time 0.2546 (0.2533)	loss 1.4925 (1.3687)	grad_norm 0.4883 (0.4909)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:09:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:05:55 lr 0.000037	 wd 0.0000	time 0.2398 (0.2533)	loss 1.4052 (1.3718)	grad_norm 0.4916 (0.4903)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:09:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:05:27 lr 0.000037	 wd 0.0000	time 0.2350 (0.2517)	loss 1.5556 (1.3727)	grad_norm 0.4059 (0.4904)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:10:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:05:00 lr 0.000037	 wd 0.0000	time 0.2350 (0.2502)	loss 0.9734 (1.3707)	grad_norm 0.4864 (0.4902)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:10:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:04:35 lr 0.000037	 wd 0.0000	time 0.2494 (0.2500)	loss 1.3534 (1.3720)	grad_norm 0.4129 (0.4914)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:11:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:04:10 lr 0.000037	 wd 0.0000	time 0.2033 (0.2503)	loss 1.4867 (1.3740)	grad_norm 0.4699 (0.4910)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:11:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:03:44 lr 0.000037	 wd 0.0000	time 0.2317 (0.2493)	loss 0.9486 (1.3731)	grad_norm 0.4600 (0.4932)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:11:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:03:19 lr 0.000037	 wd 0.0000	time 0.2333 (0.2482)	loss 1.1139 (1.3743)	grad_norm 0.4384 (0.4921)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:12:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:02:54 lr 0.000037	 wd 0.0000	time 0.2543 (0.2483)	loss 1.5548 (1.3745)	grad_norm 0.4551 (0.4920)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:12:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:29 lr 0.000037	 wd 0.0000	time 0.2252 (0.2488)	loss 1.3469 (1.3732)	grad_norm 0.4561 (0.4918)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:13:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:04 lr 0.000037	 wd 0.0000	time 0.2302 (0.2481)	loss 1.5075 (1.3733)	grad_norm 0.5366 (0.4911)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:13:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:39 lr 0.000036	 wd 0.0000	time 0.2369 (0.2474)	loss 1.0853 (1.3734)	grad_norm 0.4575 (0.4909)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:13:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:14 lr 0.000036	 wd 0.0000	time 0.3422 (0.2474)	loss 1.4907 (1.3744)	grad_norm 0.4454 (0.4916)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:14:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:50 lr 0.000036	 wd 0.0000	time 0.2243 (0.2476)	loss 1.4307 (1.3750)	grad_norm 0.9403 (0.4918)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:14:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:25 lr 0.000036	 wd 0.0000	time 0.2193 (0.2471)	loss 1.2870 (1.3747)	grad_norm 0.5051 (0.4918)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:15:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000036	 wd 0.0000	time 0.1699 (0.2454)	loss 1.1391 (1.3758)	grad_norm 0.4248 (0.4918)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:15:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 9 training takes 0:10:18
[2024-06-29 19:15:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 35.320 (35.320)	Loss 0.4175 (0.4175)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 19:15:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.818 Acc@5 97.186
[2024-06-29 19:15:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-06-29 19:15:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.83%
[2024-06-29 19:16:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][0/2502]	eta 10:49:28 lr 0.000036	 wd 0.0000	time 15.5748 (15.5748)	loss 1.2900 (1.2900)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:16:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:15:33 lr 0.000036	 wd 0.0000	time 0.1786 (0.3886)	loss 1.3682 (1.3563)	grad_norm 0.4220 (0.4933)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:17:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:12:20 lr 0.000036	 wd 0.0000	time 0.3018 (0.3215)	loss 1.3221 (1.3769)	grad_norm 0.4714 (0.4845)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:17:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:10:53 lr 0.000036	 wd 0.0000	time 0.2195 (0.2968)	loss 0.9014 (1.3738)	grad_norm 0.4837 (0.4825)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:17:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:09:46 lr 0.000036	 wd 0.0000	time 0.2063 (0.2791)	loss 1.5263 (1.3770)	grad_norm 0.4943 (0.4779)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:18:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:08:57 lr 0.000036	 wd 0.0000	time 0.2195 (0.2685)	loss 1.4281 (1.3728)	grad_norm 0.4275 (0.4767)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:18:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:08:23 lr 0.000036	 wd 0.0000	time 0.2614 (0.2645)	loss 1.5240 (1.3731)	grad_norm 0.4796 (0.4787)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:19:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:07:54 lr 0.000036	 wd 0.0000	time 0.2214 (0.2633)	loss 1.4966 (1.3688)	grad_norm 0.7263 (0.4835)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:19:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:07:21 lr 0.000036	 wd 0.0000	time 0.2056 (0.2593)	loss 1.2543 (1.3680)	grad_norm 0.4441 (0.4836)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:19:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:06:50 lr 0.000036	 wd 0.0000	time 0.2226 (0.2562)	loss 1.5509 (1.3680)	grad_norm 0.4776 (0.4849)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:20:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:06:22 lr 0.000036	 wd 0.0000	time 0.2075 (0.2544)	loss 1.4975 (1.3665)	grad_norm 0.5324 (0.4825)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:20:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:05:57 lr 0.000036	 wd 0.0000	time 0.2010 (0.2547)	loss 1.3089 (1.3641)	grad_norm 0.8089 (0.4827)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:21:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:05:29 lr 0.000035	 wd 0.0000	time 0.2142 (0.2528)	loss 1.3527 (1.3647)	grad_norm 0.4904 (0.4825)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:21:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:05:01 lr 0.000035	 wd 0.0000	time 0.1847 (0.2512)	loss 1.6087 (1.3673)	grad_norm 0.4487 (0.4809)	loss_scale 8192.0000 (4385.6480)	mem 7984MB
[2024-06-29 19:21:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:04:35 lr 0.000035	 wd 0.0000	time 0.2371 (0.2503)	loss 1.3545 (1.3678)	grad_norm 0.5348 (0.4810)	loss_scale 8192.0000 (4657.3362)	mem 7984MB
[2024-06-29 19:22:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:04:11 lr 0.000035	 wd 0.0000	time 0.2705 (0.2510)	loss 1.2683 (1.3677)	grad_norm 0.4187 (0.4802)	loss_scale 8192.0000 (4892.8235)	mem 7984MB
[2024-06-29 19:22:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:03:45 lr 0.000035	 wd 0.0000	time 0.2277 (0.2501)	loss 1.2385 (1.3692)	grad_norm 0.4414 (0.4803)	loss_scale 8192.0000 (5098.8932)	mem 7984MB
[2024-06-29 19:23:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:03:19 lr 0.000035	 wd 0.0000	time 0.2377 (0.2492)	loss 1.3203 (1.3702)	grad_norm 0.4400 (0.4839)	loss_scale 8192.0000 (5280.7337)	mem 7984MB
[2024-06-29 19:23:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:02:54 lr 0.000035	 wd 0.0000	time 0.2611 (0.2486)	loss 1.4104 (1.3708)	grad_norm 0.5698 (0.4850)	loss_scale 8192.0000 (5442.3809)	mem 7984MB
[2024-06-29 19:23:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:02:30 lr 0.000035	 wd 0.0000	time 0.2539 (0.2494)	loss 1.5205 (1.3722)	grad_norm 0.4912 (0.4847)	loss_scale 8192.0000 (5587.0216)	mem 7984MB
[2024-06-29 19:24:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:04 lr 0.000035	 wd 0.0000	time 0.2069 (0.2488)	loss 1.2669 (1.3732)	grad_norm 0.4784 (0.4841)	loss_scale 8192.0000 (5717.2054)	mem 7984MB
[2024-06-29 19:24:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:39 lr 0.000035	 wd 0.0000	time 0.2108 (0.2480)	loss 1.5800 (1.3749)	grad_norm 0.4451 (0.4857)	loss_scale 8192.0000 (5834.9967)	mem 7984MB
[2024-06-29 19:25:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:14 lr 0.000035	 wd 0.0000	time 0.3323 (0.2476)	loss 0.9627 (1.3746)	grad_norm 0.5732 (0.4856)	loss_scale 8192.0000 (5942.0845)	mem 7984MB
[2024-06-29 19:25:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:50 lr 0.000035	 wd 0.0000	time 0.2264 (0.2479)	loss 1.5007 (1.3742)	grad_norm 0.4560 (0.4853)	loss_scale 8192.0000 (6039.8644)	mem 7984MB
[2024-06-29 19:25:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:25 lr 0.000035	 wd 0.0000	time 0.2081 (0.2473)	loss 1.5477 (1.3741)	grad_norm 0.4474 (nan)	loss_scale 4096.0000 (6102.2041)	mem 7984MB
[2024-06-29 19:26:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.1765 (0.2457)	loss 1.3371 (1.3731)	grad_norm 0.4767 (nan)	loss_scale 4096.0000 (6021.9880)	mem 7984MB
[2024-06-29 19:26:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 10 training takes 0:10:19
[2024-06-29 19:26:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_10.pth saving......
[2024-06-29 19:26:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_10.pth saved !!!
[2024-06-29 19:26:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 30.792 (30.792)	Loss 0.4248 (0.4248)	Acc@1 91.406 (91.406)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 19:27:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.900 Acc@5 97.210
[2024-06-29 19:27:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-06-29 19:27:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.90%
[2024-06-29 19:27:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 19:27:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 19:27:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][0/2502]	eta 10:56:48 lr 0.000035	 wd 0.0000	time 15.7510 (15.7510)	loss 1.5070 (1.5070)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:27:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:15:23 lr 0.000035	 wd 0.0000	time 0.2068 (0.3847)	loss 1.3932 (1.3644)	grad_norm 0.4530 (0.5082)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:28:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:11:47 lr 0.000034	 wd 0.0000	time 0.2249 (0.3074)	loss 1.4674 (1.3684)	grad_norm 0.4513 (0.4930)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:28:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:10:47 lr 0.000034	 wd 0.0000	time 0.2961 (0.2939)	loss 1.3351 (1.3709)	grad_norm 0.4461 (0.4922)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:29:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:09:44 lr 0.000034	 wd 0.0000	time 0.2273 (0.2780)	loss 1.5426 (1.3706)	grad_norm 0.5647 (0.5006)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:29:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:08:55 lr 0.000034	 wd 0.0000	time 0.2189 (0.2674)	loss 1.4004 (1.3672)	grad_norm 0.4076 (0.4934)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:29:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:08:15 lr 0.000034	 wd 0.0000	time 0.2098 (0.2607)	loss 1.4921 (1.3648)	grad_norm 0.4451 (0.4897)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:30:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:07:49 lr 0.000034	 wd 0.0000	time 0.2043 (0.2603)	loss 1.4886 (1.3652)	grad_norm 0.5388 (0.4899)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:30:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:07:18 lr 0.000034	 wd 0.0000	time 0.2038 (0.2576)	loss 1.6240 (1.3677)	grad_norm 0.4571 (0.4863)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:31:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:06:48 lr 0.000034	 wd 0.0000	time 0.2107 (0.2548)	loss 1.5297 (1.3675)	grad_norm 0.4524 (0.4871)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:31:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:06:19 lr 0.000034	 wd 0.0000	time 0.2453 (0.2523)	loss 1.4571 (1.3695)	grad_norm 0.4897 (0.4851)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:31:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:05:54 lr 0.000034	 wd 0.0000	time 0.2138 (0.2525)	loss 1.5615 (1.3708)	grad_norm 0.4508 (0.4836)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:32:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:05:28 lr 0.000034	 wd 0.0000	time 0.2029 (0.2521)	loss 1.7092 (1.3707)	grad_norm 0.4882 (0.4830)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:32:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:05:01 lr 0.000034	 wd 0.0000	time 0.2181 (0.2505)	loss 0.9028 (1.3696)	grad_norm 0.4104 (0.4834)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:32:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:04:34 lr 0.000034	 wd 0.0000	time 0.2330 (0.2490)	loss 0.8935 (1.3692)	grad_norm 0.4253 (0.4837)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:33:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:04:09 lr 0.000034	 wd 0.0000	time 0.2338 (0.2489)	loss 1.4405 (1.3682)	grad_norm 0.4292 (0.4835)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:33:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:03:44 lr 0.000034	 wd 0.0000	time 0.2531 (0.2491)	loss 1.2678 (1.3691)	grad_norm 0.4665 (0.4843)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:34:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:03:19 lr 0.000033	 wd 0.0000	time 0.2300 (0.2481)	loss 1.4712 (1.3683)	grad_norm 0.4171 (0.4842)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:34:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:02:53 lr 0.000033	 wd 0.0000	time 0.2321 (0.2472)	loss 1.2936 (1.3699)	grad_norm 0.4368 (0.4844)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:35:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:28 lr 0.000033	 wd 0.0000	time 0.2350 (0.2473)	loss 1.5074 (1.3701)	grad_norm 0.4289 (0.4841)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:35:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:04 lr 0.000033	 wd 0.0000	time 0.2025 (0.2477)	loss 1.1179 (1.3694)	grad_norm 0.4755 (0.4849)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:35:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:39 lr 0.000033	 wd 0.0000	time 0.2164 (0.2471)	loss 1.5536 (1.3692)	grad_norm 0.5015 (0.4847)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:36:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:14 lr 0.000033	 wd 0.0000	time 0.2017 (0.2463)	loss 1.2888 (1.3701)	grad_norm 0.4872 (0.4867)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:36:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:49 lr 0.000033	 wd 0.0000	time 0.2124 (0.2462)	loss 1.5486 (1.3698)	grad_norm 0.4695 (0.4866)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:37:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:25 lr 0.000033	 wd 0.0000	time 0.2160 (0.2462)	loss 1.2737 (1.3690)	grad_norm 0.4710 (0.4869)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:37:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000033	 wd 0.0000	time 0.1859 (0.2446)	loss 1.4234 (1.3686)	grad_norm 0.4503 (0.4866)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:37:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 11 training takes 0:10:16
[2024-06-29 19:37:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 19.605 (19.605)	Loss 0.4224 (0.4224)	Acc@1 91.797 (91.797)	Acc@5 98.438 (98.438)	Mem 7984MB
[2024-06-29 19:37:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.906 Acc@5 97.194
[2024-06-29 19:37:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-06-29 19:37:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.91%
[2024-06-29 19:37:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 19:38:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 19:38:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][0/2502]	eta 22:14:00 lr 0.000033	 wd 0.0000	time 31.9906 (31.9906)	loss 1.6300 (1.6300)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:38:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:22:13 lr 0.000033	 wd 0.0000	time 0.2287 (0.5553)	loss 1.5130 (1.3747)	grad_norm 0.4377 (0.4805)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:39:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:14:59 lr 0.000033	 wd 0.0000	time 0.1980 (0.3908)	loss 0.9906 (1.3801)	grad_norm 0.4685 (0.4836)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:39:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:12:32 lr 0.000033	 wd 0.0000	time 0.2614 (0.3416)	loss 1.3089 (1.3802)	grad_norm 0.5007 (0.4786)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:40:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:11:08 lr 0.000033	 wd 0.0000	time 0.2170 (0.3179)	loss 1.2388 (1.3728)	grad_norm 0.4891 (0.4791)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:40:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:10:00 lr 0.000032	 wd 0.0000	time 0.2094 (0.2997)	loss 1.0260 (1.3735)	grad_norm 0.4738 (0.4797)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:40:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:09:07 lr 0.000032	 wd 0.0000	time 0.1987 (0.2881)	loss 1.3660 (1.3707)	grad_norm 0.4412 (0.4804)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:41:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:08:25 lr 0.000032	 wd 0.0000	time 0.2649 (0.2808)	loss 1.4702 (1.3685)	grad_norm 0.4687 (0.4808)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:41:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:07:52 lr 0.000032	 wd 0.0000	time 0.2064 (0.2776)	loss 1.4498 (1.3763)	grad_norm 0.4392 (0.4802)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:42:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:07:16 lr 0.000032	 wd 0.0000	time 0.1751 (0.2727)	loss 1.0653 (1.3766)	grad_norm 0.4665 (0.4826)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:42:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:06:43 lr 0.000032	 wd 0.0000	time 0.2057 (0.2684)	loss 1.6407 (1.3742)	grad_norm 0.4796 (0.4821)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:42:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:06:11 lr 0.000032	 wd 0.0000	time 0.2534 (0.2653)	loss 1.4822 (1.3782)	grad_norm 0.4490 (0.4830)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:43:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:05:44 lr 0.000032	 wd 0.0000	time 0.2222 (0.2648)	loss 1.5165 (1.3771)	grad_norm 0.6425 (0.4841)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:43:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:05:15 lr 0.000032	 wd 0.0000	time 0.2560 (0.2624)	loss 1.4449 (1.3775)	grad_norm 0.5305 (0.4837)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 19:44:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:04:46 lr 0.000032	 wd 0.0000	time 0.2157 (0.2602)	loss 1.6208 (1.3774)	grad_norm 0.4565 (0.4828)	loss_scale 8192.0000 (4154.4725)	mem 7984MB
[2024-06-29 19:44:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:04:18 lr 0.000032	 wd 0.0000	time 0.2222 (0.2584)	loss 1.4379 (1.3780)	grad_norm 0.4868 (0.4825)	loss_scale 8192.0000 (4423.4617)	mem 7984MB
[2024-06-29 19:44:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:03:52 lr 0.000032	 wd 0.0000	time 0.2170 (0.2583)	loss 1.4366 (1.3768)	grad_norm 0.4847 (0.4821)	loss_scale 8192.0000 (4658.8482)	mem 7984MB
[2024-06-29 19:45:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:03:26 lr 0.000031	 wd 0.0000	time 0.2337 (0.2571)	loss 1.5684 (1.3779)	grad_norm 0.4687 (0.4829)	loss_scale 8192.0000 (4866.5585)	mem 7984MB
[2024-06-29 19:45:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:02:59 lr 0.000031	 wd 0.0000	time 0.2323 (0.2559)	loss 1.2837 (1.3787)	grad_norm 0.4505 (0.4824)	loss_scale 8192.0000 (5051.2027)	mem 7984MB
[2024-06-29 19:46:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:33 lr 0.000031	 wd 0.0000	time 0.2270 (0.2548)	loss 1.4209 (1.3782)	grad_norm 0.4668 (0.4821)	loss_scale 8192.0000 (5216.4208)	mem 7984MB
[2024-06-29 19:46:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:08 lr 0.000031	 wd 0.0000	time 0.1690 (0.2551)	loss 1.4044 (1.3772)	grad_norm 0.4268 (0.4820)	loss_scale 8192.0000 (5365.1254)	mem 7984MB
[2024-06-29 19:46:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:42 lr 0.000031	 wd 0.0000	time 0.2309 (0.2547)	loss 1.4586 (1.3783)	grad_norm 0.4609 (0.4829)	loss_scale 8192.0000 (5499.6744)	mem 7984MB
[2024-06-29 19:47:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:16 lr 0.000031	 wd 0.0000	time 0.2173 (0.2538)	loss 1.5696 (1.3798)	grad_norm 0.4339 (0.4822)	loss_scale 8192.0000 (5621.9973)	mem 7984MB
[2024-06-29 19:47:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:51 lr 0.000031	 wd 0.0000	time 0.2412 (0.2530)	loss 1.4358 (1.3797)	grad_norm 0.4423 (0.4819)	loss_scale 8192.0000 (5733.6880)	mem 7984MB
[2024-06-29 19:48:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:25 lr 0.000031	 wd 0.0000	time 0.2350 (0.2530)	loss 1.5092 (1.3793)	grad_norm 0.5211 (0.4828)	loss_scale 8192.0000 (5836.0750)	mem 7984MB
[2024-06-29 19:48:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000031	 wd 0.0000	time 0.1719 (0.2512)	loss 1.1254 (1.3781)	grad_norm 0.4486 (0.4822)	loss_scale 8192.0000 (5930.2743)	mem 7984MB
[2024-06-29 19:48:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 12 training takes 0:10:33
[2024-06-29 19:48:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 21.064 (21.064)	Loss 0.4209 (0.4209)	Acc@1 91.992 (91.992)	Acc@5 98.242 (98.242)	Mem 7984MB
[2024-06-29 19:49:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.936 Acc@5 97.210
[2024-06-29 19:49:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-06-29 19:49:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 83.94%
[2024-06-29 19:49:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 19:49:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 19:49:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][0/2502]	eta 15:56:50 lr 0.000031	 wd 0.0000	time 22.9459 (22.9459)	loss 1.3642 (1.3642)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:49:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:18:49 lr 0.000031	 wd 0.0000	time 0.2226 (0.4701)	loss 1.4034 (1.3642)	grad_norm 0.4858 (0.5031)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:50:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:13:27 lr 0.000031	 wd 0.0000	time 0.1921 (0.3506)	loss 1.6944 (1.3659)	grad_norm 0.4953 (0.4882)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:50:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:11:21 lr 0.000031	 wd 0.0000	time 0.2468 (0.3093)	loss 1.4474 (1.3741)	grad_norm 0.4469 (0.4821)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:51:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:10:14 lr 0.000030	 wd 0.0000	time 0.2668 (0.2925)	loss 1.3466 (1.3730)	grad_norm 0.5070 (0.4792)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:51:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:09:26 lr 0.000030	 wd 0.0000	time 0.1996 (0.2828)	loss 1.5324 (1.3677)	grad_norm 0.4894 (0.4782)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:51:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:08:42 lr 0.000030	 wd 0.0000	time 0.2450 (0.2746)	loss 1.5368 (1.3741)	grad_norm 0.5914 (0.4837)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:52:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:08:03 lr 0.000030	 wd 0.0000	time 0.2526 (0.2681)	loss 1.4481 (1.3693)	grad_norm 0.5544 (0.4860)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:52:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:07:29 lr 0.000030	 wd 0.0000	time 0.2037 (0.2641)	loss 1.5567 (1.3732)	grad_norm 0.4502 (0.4844)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:53:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:07:01 lr 0.000030	 wd 0.0000	time 0.2246 (0.2630)	loss 1.5410 (1.3701)	grad_norm 0.4911 (0.4815)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:53:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:06:30 lr 0.000030	 wd 0.0000	time 0.2283 (0.2599)	loss 1.6190 (1.3757)	grad_norm 0.4344 (0.4816)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:53:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:06:00 lr 0.000030	 wd 0.0000	time 0.2450 (0.2573)	loss 1.4046 (1.3772)	grad_norm 0.5129 (0.4835)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:54:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:05:32 lr 0.000030	 wd 0.0000	time 0.2979 (0.2554)	loss 1.5714 (1.3734)	grad_norm 0.4337 (0.4890)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:54:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:05:07 lr 0.000030	 wd 0.0000	time 0.2460 (0.2557)	loss 1.4417 (1.3720)	grad_norm 0.4215 (0.4894)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:55:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:04:40 lr 0.000030	 wd 0.0000	time 0.2133 (0.2541)	loss 1.6606 (1.3726)	grad_norm 0.5310 (0.4904)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:55:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:04:13 lr 0.000030	 wd 0.0000	time 0.2228 (0.2526)	loss 1.6507 (1.3713)	grad_norm 0.4379 (0.4924)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:55:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:03:46 lr 0.000029	 wd 0.0000	time 0.2069 (0.2516)	loss 1.5736 (1.3709)	grad_norm 0.4451 (0.4915)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:56:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:03:22 lr 0.000029	 wd 0.0000	time 0.2964 (0.2525)	loss 0.8690 (1.3685)	grad_norm 0.4476 (0.4910)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:56:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:02:56 lr 0.000029	 wd 0.0000	time 0.1849 (0.2518)	loss 1.4405 (1.3702)	grad_norm 0.4187 (0.4913)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:57:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:31 lr 0.000029	 wd 0.0000	time 0.2187 (0.2509)	loss 1.2220 (1.3706)	grad_norm 0.4447 (0.4903)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:57:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:05 lr 0.000029	 wd 0.0000	time 0.2334 (0.2501)	loss 1.4395 (1.3716)	grad_norm 0.4439 (0.4898)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:57:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:40 lr 0.000029	 wd 0.0000	time 0.2133 (0.2506)	loss 1.1283 (1.3713)	grad_norm 0.4461 (0.4893)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:58:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:15 lr 0.000029	 wd 0.0000	time 0.2358 (0.2499)	loss 1.5624 (1.3728)	grad_norm 0.5386 (0.4884)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:58:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:50 lr 0.000029	 wd 0.0000	time 0.2360 (0.2492)	loss 1.3142 (1.3722)	grad_norm 0.5611 (0.4886)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:59:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:25 lr 0.000029	 wd 0.0000	time 0.2358 (0.2486)	loss 1.7004 (1.3733)	grad_norm 0.4279 (0.4885)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:59:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000029	 wd 0.0000	time 0.1738 (0.2473)	loss 1.5323 (1.3727)	grad_norm 0.4158 (0.4885)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 19:59:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 13 training takes 0:10:28
[2024-06-29 20:00:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 26.488 (26.488)	Loss 0.4180 (0.4180)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 20:00:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.008 Acc@5 97.232
[2024-06-29 20:00:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 20:00:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.01%
[2024-06-29 20:00:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 20:00:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 20:00:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][0/2502]	eta 10:48:49 lr 0.000029	 wd 0.0000	time 15.5593 (15.5593)	loss 1.4968 (1.4968)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:00:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:16:17 lr 0.000029	 wd 0.0000	time 0.2496 (0.4071)	loss 1.5594 (1.3820)	grad_norm 0.4316 (0.4986)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:01:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:12:33 lr 0.000028	 wd 0.0000	time 0.2129 (0.3274)	loss 1.3071 (1.4014)	grad_norm 0.5306 (0.5084)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:01:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:10:46 lr 0.000028	 wd 0.0000	time 0.2276 (0.2937)	loss 1.0248 (1.3894)	grad_norm 0.4344 (0.4923)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:02:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:09:41 lr 0.000028	 wd 0.0000	time 0.2172 (0.2765)	loss 1.2228 (1.3744)	grad_norm 0.4864 (0.4903)	loss_scale 16384.0000 (8682.2943)	mem 7984MB
[2024-06-29 20:02:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:09:01 lr 0.000028	 wd 0.0000	time 0.2483 (0.2706)	loss 1.3020 (1.3771)	grad_norm 0.4716 (0.4884)	loss_scale 16384.0000 (10219.5609)	mem 7984MB
[2024-06-29 20:02:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:08:26 lr 0.000028	 wd 0.0000	time 0.2265 (0.2661)	loss 1.6577 (1.3827)	grad_norm 0.4848 (0.4944)	loss_scale 16384.0000 (11245.2579)	mem 7984MB
[2024-06-29 20:03:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:07:50 lr 0.000028	 wd 0.0000	time 0.2207 (0.2609)	loss 1.4520 (1.3786)	grad_norm 0.4840 (0.4915)	loss_scale 16384.0000 (11978.3167)	mem 7984MB
[2024-06-29 20:03:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:07:17 lr 0.000028	 wd 0.0000	time 0.2044 (0.2568)	loss 1.3856 (1.3749)	grad_norm 0.4300 (nan)	loss_scale 4096.0000 (11209.0287)	mem 7984MB
[2024-06-29 20:04:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:06:49 lr 0.000028	 wd 0.0000	time 0.2356 (0.2557)	loss 1.4429 (1.3709)	grad_norm 0.4515 (nan)	loss_scale 4096.0000 (10419.5694)	mem 7984MB
[2024-06-29 20:04:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:06:23 lr 0.000028	 wd 0.0000	time 0.2335 (0.2551)	loss 1.3243 (1.3693)	grad_norm 0.4471 (nan)	loss_scale 4096.0000 (9787.8442)	mem 7984MB
[2024-06-29 20:04:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:05:54 lr 0.000028	 wd 0.0000	time 0.2114 (0.2527)	loss 1.5081 (1.3669)	grad_norm 0.5367 (nan)	loss_scale 4096.0000 (9270.8738)	mem 7984MB
[2024-06-29 20:05:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:05:26 lr 0.000028	 wd 0.0000	time 0.2163 (0.2508)	loss 1.4125 (1.3670)	grad_norm 0.4351 (nan)	loss_scale 4096.0000 (8839.9933)	mem 7984MB
[2024-06-29 20:05:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:05:00 lr 0.000027	 wd 0.0000	time 0.2385 (0.2502)	loss 1.1907 (1.3672)	grad_norm 0.5554 (nan)	loss_scale 4096.0000 (8475.3513)	mem 7984MB
[2024-06-29 20:06:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:04:36 lr 0.000027	 wd 0.0000	time 0.3049 (0.2509)	loss 1.6641 (1.3662)	grad_norm 0.8790 (nan)	loss_scale 4096.0000 (8162.7637)	mem 7984MB
[2024-06-29 20:06:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:04:10 lr 0.000027	 wd 0.0000	time 0.2245 (0.2495)	loss 1.3771 (1.3670)	grad_norm 0.5920 (nan)	loss_scale 4096.0000 (7891.8268)	mem 7984MB
[2024-06-29 20:06:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:03:44 lr 0.000027	 wd 0.0000	time 0.2029 (0.2486)	loss 1.3224 (1.3668)	grad_norm 0.4927 (nan)	loss_scale 4096.0000 (7654.7358)	mem 7984MB
[2024-06-29 20:07:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:03:19 lr 0.000027	 wd 0.0000	time 0.2657 (0.2485)	loss 1.1963 (1.3661)	grad_norm 0.4761 (nan)	loss_scale 4096.0000 (7445.5215)	mem 7984MB
[2024-06-29 20:07:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:02:55 lr 0.000027	 wd 0.0000	time 0.2268 (0.2493)	loss 1.3189 (1.3659)	grad_norm 0.4891 (nan)	loss_scale 4096.0000 (7259.5403)	mem 7984MB
[2024-06-29 20:08:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:29 lr 0.000027	 wd 0.0000	time 0.2494 (0.2485)	loss 1.5472 (1.3661)	grad_norm 0.5201 (nan)	loss_scale 4096.0000 (7093.1257)	mem 7984MB
[2024-06-29 20:08:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:04 lr 0.000027	 wd 0.0000	time 0.2030 (0.2477)	loss 1.0756 (1.3680)	grad_norm 0.4343 (nan)	loss_scale 4096.0000 (6943.3443)	mem 7984MB
[2024-06-29 20:08:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:39 lr 0.000027	 wd 0.0000	time 0.2293 (0.2475)	loss 1.5612 (1.3677)	grad_norm 0.5105 (nan)	loss_scale 4096.0000 (6807.8210)	mem 7984MB
[2024-06-29 20:09:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:14 lr 0.000027	 wd 0.0000	time 0.2275 (0.2478)	loss 1.2185 (1.3678)	grad_norm 0.5116 (nan)	loss_scale 4096.0000 (6684.6124)	mem 7984MB
[2024-06-29 20:09:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:49 lr 0.000027	 wd 0.0000	time 0.2333 (0.2472)	loss 1.3926 (1.3686)	grad_norm 0.5396 (nan)	loss_scale 4096.0000 (6572.1130)	mem 7984MB
[2024-06-29 20:10:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:25 lr 0.000026	 wd 0.0000	time 0.2081 (0.2466)	loss 1.1328 (1.3697)	grad_norm 0.6162 (nan)	loss_scale 4096.0000 (6468.9846)	mem 7984MB
[2024-06-29 20:10:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.1846 (0.2450)	loss 1.2657 (1.3688)	grad_norm 0.4742 (nan)	loss_scale 4096.0000 (6374.1032)	mem 7984MB
[2024-06-29 20:10:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 14 training takes 0:10:17
[2024-06-29 20:11:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 33.483 (33.483)	Loss 0.4182 (0.4182)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 20:11:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 83.988 Acc@5 97.236
[2024-06-29 20:11:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 20:11:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.01%
[2024-06-29 20:11:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][0/2502]	eta 11:12:12 lr 0.000026	 wd 0.0000	time 16.1203 (16.1203)	loss 1.4697 (1.4697)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:12:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:15:36 lr 0.000026	 wd 0.0000	time 0.2171 (0.3901)	loss 1.0390 (1.3549)	grad_norm 0.4266 (0.4811)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:12:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:12:30 lr 0.000026	 wd 0.0000	time 0.2128 (0.3261)	loss 1.4675 (1.3706)	grad_norm 0.4815 (0.4781)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:12:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:10:46 lr 0.000026	 wd 0.0000	time 0.2192 (0.2938)	loss 1.4053 (1.3571)	grad_norm 0.4215 (0.4762)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:13:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:09:44 lr 0.000026	 wd 0.0000	time 0.2563 (0.2781)	loss 1.3963 (1.3605)	grad_norm 0.5543 (0.4742)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:13:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:08:58 lr 0.000026	 wd 0.0000	time 0.2810 (0.2691)	loss 0.8764 (1.3585)	grad_norm 0.4768 (0.4817)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:14:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:08:28 lr 0.000026	 wd 0.0000	time 0.2033 (0.2674)	loss 0.8500 (1.3619)	grad_norm 0.4998 (0.4815)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:14:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:07:54 lr 0.000026	 wd 0.0000	time 0.1964 (0.2634)	loss 1.1070 (1.3644)	grad_norm 0.4556 (0.4847)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:14:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:07:21 lr 0.000026	 wd 0.0000	time 0.2310 (0.2594)	loss 1.4212 (1.3647)	grad_norm 0.4166 (0.4876)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:15:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:06:50 lr 0.000025	 wd 0.0000	time 0.2365 (0.2564)	loss 1.2329 (1.3655)	grad_norm 0.4813 (0.4936)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:15:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:06:25 lr 0.000025	 wd 0.0000	time 0.2243 (0.2566)	loss 1.4297 (1.3668)	grad_norm 0.4157 (0.4948)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:16:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:05:57 lr 0.000025	 wd 0.0000	time 0.2186 (0.2552)	loss 1.2600 (1.3640)	grad_norm 0.4808 (0.4929)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:16:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:05:29 lr 0.000025	 wd 0.0000	time 0.2426 (0.2534)	loss 1.3986 (1.3647)	grad_norm 0.4486 (0.4925)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:16:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:05:02 lr 0.000025	 wd 0.0000	time 0.2269 (0.2516)	loss 1.3788 (1.3630)	grad_norm 0.4244 (0.4922)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:17:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:04:37 lr 0.000025	 wd 0.0000	time 0.2080 (0.2518)	loss 1.4546 (1.3631)	grad_norm 0.5037 (0.4928)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:17:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:04:11 lr 0.000025	 wd 0.0000	time 0.2218 (0.2511)	loss 1.5731 (1.3629)	grad_norm 0.4703 (0.4932)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:18:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:03:45 lr 0.000025	 wd 0.0000	time 0.2093 (0.2500)	loss 1.2877 (1.3652)	grad_norm 0.4611 (0.4933)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:18:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:03:19 lr 0.000025	 wd 0.0000	time 0.2082 (0.2488)	loss 1.4328 (1.3643)	grad_norm 0.4644 (0.4921)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:18:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:02:54 lr 0.000025	 wd 0.0000	time 0.2571 (0.2488)	loss 1.0821 (1.3641)	grad_norm 0.4954 (0.4913)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:19:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:30 lr 0.000024	 wd 0.0000	time 0.2182 (0.2494)	loss 1.6210 (1.3645)	grad_norm 0.4785 (0.4912)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:19:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:04 lr 0.000024	 wd 0.0000	time 0.2192 (0.2488)	loss 1.8622 (1.3667)	grad_norm 0.4650 (0.4907)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:20:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:39 lr 0.000024	 wd 0.0000	time 0.2318 (0.2480)	loss 1.5325 (1.3675)	grad_norm 0.5025 (0.4900)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:20:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:14 lr 0.000024	 wd 0.0000	time 0.1950 (0.2479)	loss 1.0725 (1.3687)	grad_norm 0.7913 (0.4896)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:20:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:50 lr 0.000024	 wd 0.0000	time 0.2199 (0.2484)	loss 1.5847 (1.3688)	grad_norm 0.4704 (0.4895)	loss_scale 8192.0000 (4217.0465)	mem 7984MB
[2024-06-29 20:21:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:25 lr 0.000024	 wd 0.0000	time 0.2185 (0.2478)	loss 1.0770 (1.3676)	grad_norm 0.4657 (0.4891)	loss_scale 8192.0000 (4382.6006)	mem 7984MB
[2024-06-29 20:21:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.1879 (0.2461)	loss 1.2162 (1.3678)	grad_norm 0.4191 (0.4892)	loss_scale 8192.0000 (4534.9156)	mem 7984MB
[2024-06-29 20:21:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 15 training takes 0:10:20
[2024-06-29 20:22:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 33.921 (33.921)	Loss 0.4163 (0.4163)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 20:22:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.020 Acc@5 97.226
[2024-06-29 20:22:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 20:22:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.02%
[2024-06-29 20:22:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 20:22:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 20:22:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][0/2502]	eta 11:07:56 lr 0.000024	 wd 0.0000	time 16.0176 (16.0176)	loss 1.2781 (1.2781)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:23:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:15:37 lr 0.000024	 wd 0.0000	time 0.2114 (0.3904)	loss 1.4807 (1.3612)	grad_norm 0.4863 (0.4861)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:23:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:12:22 lr 0.000024	 wd 0.0000	time 0.2408 (0.3224)	loss 1.3229 (1.3704)	grad_norm 0.4541 (0.4941)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:24:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:10:56 lr 0.000024	 wd 0.0000	time 0.2509 (0.2983)	loss 1.3671 (1.3828)	grad_norm 0.4174 (0.4870)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:24:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:09:49 lr 0.000024	 wd 0.0000	time 0.2435 (0.2806)	loss 1.2823 (1.3768)	grad_norm 0.4417 (0.4847)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:24:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:09:01 lr 0.000023	 wd 0.0000	time 0.2167 (0.2704)	loss 1.2623 (1.3869)	grad_norm 0.4267 (0.4884)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:25:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:08:25 lr 0.000023	 wd 0.0000	time 0.2562 (0.2658)	loss 1.0412 (1.3792)	grad_norm 0.4850 (0.4883)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:25:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:07:54 lr 0.000023	 wd 0.0000	time 0.2313 (0.2632)	loss 1.3379 (1.3808)	grad_norm 0.4756 (0.4877)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:25:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:07:20 lr 0.000023	 wd 0.0000	time 0.2352 (0.2590)	loss 1.4000 (1.3754)	grad_norm 0.4193 (0.4875)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:26:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:06:49 lr 0.000023	 wd 0.0000	time 0.2320 (0.2558)	loss 1.5622 (1.3731)	grad_norm 0.4482 (0.4885)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:26:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:06:22 lr 0.000023	 wd 0.0000	time 0.2801 (0.2545)	loss 1.5625 (1.3732)	grad_norm 0.4960 (0.4981)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:27:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:05:57 lr 0.000023	 wd 0.0000	time 0.2331 (0.2549)	loss 1.3357 (1.3742)	grad_norm 0.4904 (0.4978)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:27:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:05:29 lr 0.000023	 wd 0.0000	time 0.2610 (0.2528)	loss 1.4291 (1.3755)	grad_norm 0.4936 (0.4966)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:27:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:05:01 lr 0.000023	 wd 0.0000	time 0.2316 (0.2511)	loss 1.0987 (1.3760)	grad_norm 0.4384 (0.4961)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:28:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:04:35 lr 0.000023	 wd 0.0000	time 0.2395 (0.2502)	loss 1.2076 (1.3733)	grad_norm 0.5852 (0.4953)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:28:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:04:11 lr 0.000022	 wd 0.0000	time 0.2183 (0.2512)	loss 1.4036 (1.3702)	grad_norm 0.6160 (0.4955)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:29:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:03:45 lr 0.000022	 wd 0.0000	time 0.2166 (0.2502)	loss 1.3900 (1.3706)	grad_norm 0.5075 (0.4963)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:29:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:03:19 lr 0.000022	 wd 0.0000	time 0.2413 (0.2493)	loss 1.1131 (1.3686)	grad_norm 0.5627 (0.4956)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:29:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:02:54 lr 0.000022	 wd 0.0000	time 0.2358 (0.2490)	loss 1.2438 (1.3669)	grad_norm 0.5117 (0.4950)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:30:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:30 lr 0.000022	 wd 0.0000	time 0.2083 (0.2497)	loss 1.2042 (1.3658)	grad_norm 0.4348 (0.4937)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:30:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:05 lr 0.000022	 wd 0.0000	time 0.2545 (0.2491)	loss 1.4784 (1.3660)	grad_norm 0.4622 (0.4937)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:31:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:39 lr 0.000022	 wd 0.0000	time 0.2049 (0.2482)	loss 1.3499 (1.3655)	grad_norm 0.4727 (0.4946)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:31:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:14 lr 0.000022	 wd 0.0000	time 0.2226 (0.2479)	loss 1.4767 (1.3662)	grad_norm 0.4266 (0.4955)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:32:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:50 lr 0.000022	 wd 0.0000	time 0.2022 (0.2482)	loss 1.2319 (1.3662)	grad_norm 0.4867 (0.4955)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:32:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:25 lr 0.000022	 wd 0.0000	time 0.2079 (0.2477)	loss 1.5518 (1.3664)	grad_norm 0.4693 (0.4959)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:32:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.1784 (0.2460)	loss 1.3826 (1.3663)	grad_norm 0.4315 (0.4957)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:32:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 16 training takes 0:10:19
[2024-06-29 20:33:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 40.717 (40.717)	Loss 0.4180 (0.4180)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 20:33:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.020 Acc@5 97.248
[2024-06-29 20:33:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 20:33:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.02%
[2024-06-29 20:33:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 20:33:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 20:34:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][0/2502]	eta 10:33:24 lr 0.000021	 wd 0.0000	time 15.1898 (15.1898)	loss 1.4706 (1.4706)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:34:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:15:15 lr 0.000021	 wd 0.0000	time 0.2203 (0.3812)	loss 1.5652 (1.3400)	grad_norm 0.4669 (0.5019)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:34:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:11:56 lr 0.000021	 wd 0.0000	time 0.2558 (0.3114)	loss 1.6016 (1.3554)	grad_norm 0.6028 (0.4944)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:35:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:10:46 lr 0.000021	 wd 0.0000	time 0.2195 (0.2938)	loss 1.3415 (1.3518)	grad_norm 0.5177 (0.4910)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:35:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:09:44 lr 0.000021	 wd 0.0000	time 0.2122 (0.2779)	loss 1.1325 (1.3461)	grad_norm 0.6076 (0.4899)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:36:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:08:56 lr 0.000021	 wd 0.0000	time 0.2093 (0.2678)	loss 1.3704 (1.3457)	grad_norm 0.4786 (0.4873)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:36:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:08:18 lr 0.000021	 wd 0.0000	time 0.2655 (0.2621)	loss 1.1868 (1.3527)	grad_norm 0.4964 (0.4890)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:36:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:07:50 lr 0.000021	 wd 0.0000	time 0.2821 (0.2609)	loss 1.3326 (1.3584)	grad_norm 0.4494 (0.4918)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:37:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:07:18 lr 0.000021	 wd 0.0000	time 0.1980 (0.2574)	loss 1.3887 (1.3622)	grad_norm 0.4590 (0.4939)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:37:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:06:47 lr 0.000021	 wd 0.0000	time 0.2062 (0.2543)	loss 1.6072 (1.3635)	grad_norm 0.4409 (0.4955)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:37:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:06:18 lr 0.000020	 wd 0.0000	time 0.1968 (0.2521)	loss 1.5308 (1.3619)	grad_norm 0.4595 (0.4927)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:38:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:05:54 lr 0.000020	 wd 0.0000	time 0.2060 (0.2528)	loss 1.1199 (1.3624)	grad_norm 0.4092 (0.4915)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:38:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:05:27 lr 0.000020	 wd 0.0000	time 0.2070 (0.2515)	loss 1.6039 (1.3641)	grad_norm 0.4228 (0.4934)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 20:39:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:05:00 lr 0.000020	 wd 0.0000	time 0.2128 (0.2500)	loss 1.1743 (1.3641)	grad_norm 0.4503 (0.4927)	loss_scale 16384.0000 (8645.3620)	mem 7984MB
[2024-06-29 20:39:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:04:34 lr 0.000020	 wd 0.0000	time 0.2395 (0.2488)	loss 1.4882 (1.3648)	grad_norm 0.4863 (0.4929)	loss_scale 16384.0000 (9197.7273)	mem 7984MB
[2024-06-29 20:39:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:04:09 lr 0.000020	 wd 0.0000	time 0.2341 (0.2490)	loss 1.3967 (1.3655)	grad_norm 0.4914 (0.4934)	loss_scale 16384.0000 (9676.4930)	mem 7984MB
[2024-06-29 20:40:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:03:44 lr 0.000020	 wd 0.0000	time 0.2319 (0.2485)	loss 1.5519 (1.3668)	grad_norm 0.4406 (nan)	loss_scale 8192.0000 (9634.9382)	mem 7984MB
[2024-06-29 20:40:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:18 lr 0.000020	 wd 0.0000	time 0.2333 (0.2476)	loss 1.5886 (1.3687)	grad_norm 0.4420 (nan)	loss_scale 8192.0000 (9550.1093)	mem 7984MB
[2024-06-29 20:41:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:02:53 lr 0.000020	 wd 0.0000	time 0.2207 (0.2469)	loss 1.3164 (1.3696)	grad_norm 0.4793 (nan)	loss_scale 8192.0000 (9474.7007)	mem 7984MB
[2024-06-29 20:41:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:29 lr 0.000020	 wd 0.0000	time 0.2192 (0.2478)	loss 1.1676 (1.3703)	grad_norm 0.5018 (nan)	loss_scale 8192.0000 (9407.2257)	mem 7984MB
[2024-06-29 20:42:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:04 lr 0.000019	 wd 0.0000	time 0.2360 (0.2477)	loss 1.5624 (1.3682)	grad_norm 0.9219 (nan)	loss_scale 4096.0000 (9272.8036)	mem 7984MB
[2024-06-29 20:42:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:39 lr 0.000019	 wd 0.0000	time 0.2414 (0.2471)	loss 1.4418 (1.3680)	grad_norm 0.4487 (nan)	loss_scale 4096.0000 (9026.4065)	mem 7984MB
[2024-06-29 20:42:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:14 lr 0.000019	 wd 0.0000	time 0.2492 (0.2465)	loss 1.3448 (1.3691)	grad_norm 0.5069 (nan)	loss_scale 4096.0000 (8802.3989)	mem 7984MB
[2024-06-29 20:43:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:49 lr 0.000019	 wd 0.0000	time 0.2417 (0.2471)	loss 1.4764 (1.3684)	grad_norm 0.4466 (nan)	loss_scale 4096.0000 (8597.8618)	mem 7984MB
[2024-06-29 20:43:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:25 lr 0.000019	 wd 0.0000	time 0.2346 (0.2468)	loss 1.4568 (1.3671)	grad_norm 0.4343 (nan)	loss_scale 4096.0000 (8410.3623)	mem 7984MB
[2024-06-29 20:43:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0000	time 0.1996 (0.2452)	loss 1.5058 (1.3676)	grad_norm 0.4465 (nan)	loss_scale 4096.0000 (8237.8569)	mem 7984MB
[2024-06-29 20:44:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 17 training takes 0:10:17
[2024-06-29 20:44:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 18.627 (18.627)	Loss 0.4187 (0.4187)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 20:44:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.058 Acc@5 97.260
[2024-06-29 20:44:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 20:44:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 20:44:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 20:44:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 20:45:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][0/2502]	eta 21:04:10 lr 0.000019	 wd 0.0000	time 30.3160 (30.3160)	loss 1.6007 (1.6007)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:45:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:21:25 lr 0.000019	 wd 0.0000	time 0.2391 (0.5351)	loss 1.5694 (1.4068)	grad_norm 0.4224 (0.4769)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:45:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:39 lr 0.000019	 wd 0.0000	time 0.2171 (0.3820)	loss 1.4926 (1.3875)	grad_norm 0.4416 (0.4802)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:46:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:12:25 lr 0.000019	 wd 0.0000	time 0.2382 (0.3384)	loss 1.7181 (1.3869)	grad_norm 0.4459 (0.4809)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:46:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:11:00 lr 0.000019	 wd 0.0000	time 0.2410 (0.3143)	loss 1.4551 (1.3802)	grad_norm 0.4547 (0.4874)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:47:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:09:54 lr 0.000018	 wd 0.0000	time 0.2296 (0.2967)	loss 1.3124 (1.3699)	grad_norm 0.4991 (0.4875)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:47:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:09:03 lr 0.000018	 wd 0.0000	time 0.2134 (0.2856)	loss 1.6294 (1.3706)	grad_norm 0.4054 (0.4863)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:47:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:08:23 lr 0.000018	 wd 0.0000	time 0.2760 (0.2795)	loss 1.4449 (1.3712)	grad_norm 0.4733 (0.4852)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:48:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:07:48 lr 0.000018	 wd 0.0000	time 0.2242 (0.2755)	loss 1.1067 (1.3685)	grad_norm 0.4477 (0.4843)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:48:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:07:13 lr 0.000018	 wd 0.0000	time 0.2438 (0.2705)	loss 1.1880 (1.3646)	grad_norm 0.4900 (0.4844)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:49:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:06:40 lr 0.000018	 wd 0.0000	time 0.2103 (0.2666)	loss 1.0509 (1.3682)	grad_norm 0.4558 (0.4860)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:49:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:06:10 lr 0.000018	 wd 0.0000	time 0.2422 (0.2641)	loss 1.2673 (1.3677)	grad_norm 0.4189 (0.4857)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:49:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:05:43 lr 0.000018	 wd 0.0000	time 0.2031 (0.2636)	loss 1.6648 (1.3680)	grad_norm 0.4591 (0.4852)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:50:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:05:14 lr 0.000018	 wd 0.0000	time 0.2516 (0.2613)	loss 1.4255 (1.3686)	grad_norm 0.4940 (0.4859)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:50:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:04:45 lr 0.000018	 wd 0.0000	time 0.2493 (0.2591)	loss 1.2273 (1.3685)	grad_norm 0.4585 (0.4872)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:51:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:04:18 lr 0.000017	 wd 0.0000	time 0.2514 (0.2577)	loss 1.6063 (1.3675)	grad_norm 0.6121 (0.4878)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:51:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:03:52 lr 0.000017	 wd 0.0000	time 0.2429 (0.2576)	loss 1.4883 (1.3669)	grad_norm 0.5224 (0.4903)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:51:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:03:25 lr 0.000017	 wd 0.0000	time 0.2115 (0.2561)	loss 1.4735 (1.3672)	grad_norm 0.4901 (0.4901)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:52:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:02:58 lr 0.000017	 wd 0.0000	time 0.2107 (0.2550)	loss 1.3225 (1.3679)	grad_norm 0.6242 (0.4898)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:52:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:33 lr 0.000017	 wd 0.0000	time 0.2755 (0.2542)	loss 1.4805 (1.3672)	grad_norm 0.4318 (0.4896)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:53:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:07 lr 0.000017	 wd 0.0000	time 0.2123 (0.2544)	loss 1.3888 (1.3694)	grad_norm 0.5140 (0.4898)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:53:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:42 lr 0.000017	 wd 0.0000	time 0.1848 (0.2538)	loss 1.4567 (1.3687)	grad_norm 0.4339 (0.4897)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:53:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:16 lr 0.000017	 wd 0.0000	time 0.2152 (0.2529)	loss 1.3527 (1.3686)	grad_norm 0.4646 (0.4899)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:54:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:50 lr 0.000017	 wd 0.0000	time 0.2411 (0.2523)	loss 1.4989 (1.3689)	grad_norm 0.4677 (0.4897)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:54:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:25 lr 0.000017	 wd 0.0000	time 0.2245 (0.2524)	loss 1.5182 (1.3703)	grad_norm 0.6217 (0.4901)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:55:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.1835 (0.2505)	loss 1.5038 (1.3698)	grad_norm 0.4420 (0.4896)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:55:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 18 training takes 0:10:31
[2024-06-29 20:55:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 20.448 (20.448)	Loss 0.4189 (0.4189)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 20:55:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.026 Acc@5 97.250
[2024-06-29 20:55:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 20:55:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 20:56:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][0/2502]	eta 18:29:50 lr 0.000016	 wd 0.0000	time 26.6151 (26.6151)	loss 1.4420 (1.4420)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:56:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:25:35 lr 0.000016	 wd 0.0000	time 0.2970 (0.6392)	loss 1.0987 (1.3810)	grad_norm 0.4885 (0.4834)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:57:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:18:39 lr 0.000016	 wd 0.0000	time 0.2330 (0.4863)	loss 1.4378 (1.3742)	grad_norm 0.4590 (0.4801)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:58:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:18:04 lr 0.000016	 wd 0.0000	time 0.5115 (0.4924)	loss 1.3072 (1.3866)	grad_norm 0.4629 (0.4844)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:58:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:16:20 lr 0.000016	 wd 0.0000	time 0.3078 (0.4663)	loss 1.5097 (1.3862)	grad_norm 0.4324 (0.4846)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:59:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:14:18 lr 0.000016	 wd 0.0000	time 0.2984 (0.4290)	loss 1.5227 (1.3814)	grad_norm 0.4630 (0.4882)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 20:59:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:13:05 lr 0.000016	 wd 0.0000	time 0.2157 (0.4131)	loss 1.4705 (1.3763)	grad_norm 0.5104 (0.4917)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:00:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:11:37 lr 0.000016	 wd 0.0000	time 0.2045 (0.3870)	loss 1.6469 (1.3784)	grad_norm 0.5132 (0.4937)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:00:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:10:25 lr 0.000016	 wd 0.0000	time 0.2312 (0.3673)	loss 1.3610 (1.3772)	grad_norm 0.4888 (0.4927)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:01:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:09:28 lr 0.000016	 wd 0.0000	time 0.2995 (0.3549)	loss 1.2433 (1.3768)	grad_norm 0.4466 (0.5013)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:01:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:37 lr 0.000016	 wd 0.0000	time 0.1929 (0.3444)	loss 1.5831 (1.3775)	grad_norm 0.5048 (0.4991)	loss_scale 8192.0000 (4259.6763)	mem 7984MB
[2024-06-29 21:01:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:48 lr 0.000015	 wd 0.0000	time 0.1949 (0.3341)	loss 0.9214 (1.3761)	grad_norm 0.4522 (0.4963)	loss_scale 8192.0000 (4616.8356)	mem 7984MB
[2024-06-29 21:02:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:03 lr 0.000015	 wd 0.0000	time 0.2215 (0.3254)	loss 1.1280 (1.3743)	grad_norm 0.4766 (0.4951)	loss_scale 8192.0000 (4914.5179)	mem 7984MB
[2024-06-29 21:02:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:24 lr 0.000015	 wd 0.0000	time 0.2232 (0.3199)	loss 1.5763 (1.3747)	grad_norm 0.4854 (0.4947)	loss_scale 8192.0000 (5166.4381)	mem 7984MB
[2024-06-29 21:03:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:05:46 lr 0.000015	 wd 0.0000	time 0.2350 (0.3148)	loss 1.4629 (1.3756)	grad_norm 0.5152 (0.4938)	loss_scale 8192.0000 (5382.3954)	mem 7984MB
[2024-06-29 21:03:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:10 lr 0.000015	 wd 0.0000	time 0.2274 (0.3094)	loss 1.2208 (1.3754)	grad_norm 0.4244 (0.4972)	loss_scale 8192.0000 (5569.5776)	mem 7984MB
[2024-06-29 21:03:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:34 lr 0.000015	 wd 0.0000	time 0.2121 (0.3045)	loss 1.3121 (1.3757)	grad_norm 0.4775 (0.4986)	loss_scale 8192.0000 (5733.3766)	mem 7984MB
[2024-06-29 21:04:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:01 lr 0.000015	 wd 0.0000	time 0.2563 (0.3012)	loss 1.4094 (1.3765)	grad_norm 0.5130 (0.4979)	loss_scale 8192.0000 (5877.9165)	mem 7984MB
[2024-06-29 21:04:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:29 lr 0.000015	 wd 0.0000	time 0.2169 (0.2984)	loss 1.3442 (1.3759)	grad_norm 0.4749 (0.4965)	loss_scale 8192.0000 (6006.4053)	mem 7984MB
[2024-06-29 21:05:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:57 lr 0.000015	 wd 0.0000	time 0.2437 (0.2951)	loss 1.4274 (1.3763)	grad_norm 0.4324 (0.4962)	loss_scale 8192.0000 (6121.3761)	mem 7984MB
[2024-06-29 21:05:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:26 lr 0.000015	 wd 0.0000	time 0.2681 (0.2918)	loss 1.0856 (1.3758)	grad_norm 0.6881 (0.4958)	loss_scale 8192.0000 (6224.8556)	mem 7984MB
[2024-06-29 21:05:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:56 lr 0.000014	 wd 0.0000	time 0.2356 (0.2898)	loss 1.1928 (1.3747)	grad_norm 0.4617 (0.4959)	loss_scale 8192.0000 (6318.4845)	mem 7984MB
[2024-06-29 21:06:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:27 lr 0.000014	 wd 0.0000	time 0.2248 (0.2881)	loss 1.4968 (1.3726)	grad_norm 0.4726 (0.4956)	loss_scale 8192.0000 (6403.6056)	mem 7984MB
[2024-06-29 21:06:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:57 lr 0.000014	 wd 0.0000	time 0.2242 (0.2858)	loss 1.4059 (1.3722)	grad_norm 0.5270 (0.4957)	loss_scale 8192.0000 (6481.3281)	mem 7984MB
[2024-06-29 21:07:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:28 lr 0.000014	 wd 0.0000	time 0.2470 (0.2835)	loss 1.5585 (1.3736)	grad_norm 0.4870 (0.4953)	loss_scale 8192.0000 (6552.5764)	mem 7984MB
[2024-06-29 21:07:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.1745 (0.2805)	loss 1.2302 (1.3735)	grad_norm 0.5184 (0.4951)	loss_scale 8192.0000 (6618.1271)	mem 7984MB
[2024-06-29 21:07:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 19 training takes 0:11:46
[2024-06-29 21:08:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 34.867 (34.867)	Loss 0.4175 (0.4175)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 21:08:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.052 Acc@5 97.236
[2024-06-29 21:08:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 21:08:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 21:08:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][0/2502]	eta 10:59:44 lr 0.000014	 wd 0.0000	time 15.8213 (15.8213)	loss 1.0101 (1.0101)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:08:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:16:08 lr 0.000014	 wd 0.0000	time 0.2458 (0.4033)	loss 1.5853 (1.3693)	grad_norm 0.4780 (0.5017)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:09:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:12:31 lr 0.000014	 wd 0.0000	time 0.1966 (0.3264)	loss 1.4450 (1.3706)	grad_norm 1.0888 (0.5025)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:09:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:10:46 lr 0.000014	 wd 0.0000	time 0.2174 (0.2937)	loss 1.4791 (1.3670)	grad_norm 0.4700 (0.4970)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:10:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:09:42 lr 0.000014	 wd 0.0000	time 0.2174 (0.2773)	loss 1.4073 (1.3762)	grad_norm 0.4460 (0.4956)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:10:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:08:57 lr 0.000014	 wd 0.0000	time 0.2803 (0.2683)	loss 1.5307 (1.3782)	grad_norm 0.5015 (0.5032)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:10:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:08:26 lr 0.000014	 wd 0.0000	time 0.2372 (0.2665)	loss 1.3386 (1.3746)	grad_norm 0.6417 (0.4995)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:11:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:07:50 lr 0.000013	 wd 0.0000	time 0.2300 (0.2613)	loss 1.4867 (1.3769)	grad_norm 0.4835 (0.4974)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:11:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:07:18 lr 0.000013	 wd 0.0000	time 0.2151 (0.2576)	loss 1.2706 (1.3741)	grad_norm 0.4638 (0.5026)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:12:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:06:48 lr 0.000013	 wd 0.0000	time 0.2324 (0.2550)	loss 1.5947 (1.3767)	grad_norm 0.4319 (0.5004)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:12:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:06:23 lr 0.000013	 wd 0.0000	time 0.2791 (0.2550)	loss 1.3064 (1.3758)	grad_norm 0.4240 (0.5005)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:12:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:05:54 lr 0.000013	 wd 0.0000	time 0.2288 (0.2531)	loss 1.3520 (1.3734)	grad_norm 0.8403 (0.4995)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:13:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:05:27 lr 0.000013	 wd 0.0000	time 0.2100 (0.2514)	loss 1.4311 (1.3704)	grad_norm 0.4540 (0.5014)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:13:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:05:00 lr 0.000013	 wd 0.0000	time 0.2262 (0.2500)	loss 1.3264 (1.3690)	grad_norm 0.5058 (0.5003)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:14:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:04:35 lr 0.000013	 wd 0.0000	time 0.2324 (0.2504)	loss 1.4026 (1.3708)	grad_norm 0.4733 (0.5010)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:14:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:04:10 lr 0.000013	 wd 0.0000	time 0.2438 (0.2495)	loss 1.4412 (1.3713)	grad_norm 0.5352 (0.5018)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:14:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:03:44 lr 0.000013	 wd 0.0000	time 0.2350 (0.2485)	loss 1.6962 (1.3716)	grad_norm 0.4590 (0.5012)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:15:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:03:18 lr 0.000012	 wd 0.0000	time 0.2623 (0.2476)	loss 1.5226 (1.3708)	grad_norm 0.4368 (0.5025)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:15:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:02:55 lr 0.000012	 wd 0.0000	time 0.2080 (0.2493)	loss 1.3050 (1.3701)	grad_norm 0.4342 (0.5026)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:16:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:29 lr 0.000012	 wd 0.0000	time 0.2246 (0.2488)	loss 1.5397 (1.3721)	grad_norm 0.4602 (0.5028)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:16:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:04 lr 0.000012	 wd 0.0000	time 0.2206 (0.2480)	loss 1.5088 (1.3716)	grad_norm 0.4447 (0.5023)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:16:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:39 lr 0.000012	 wd 0.0000	time 0.2586 (0.2476)	loss 1.3154 (1.3723)	grad_norm 0.4345 (0.5028)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:17:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:14 lr 0.000012	 wd 0.0000	time 0.2119 (0.2481)	loss 1.2539 (1.3738)	grad_norm 0.4809 (0.5023)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:17:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:50 lr 0.000012	 wd 0.0000	time 0.2222 (0.2477)	loss 1.3994 (1.3738)	grad_norm 0.4489 (0.5011)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:18:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:25 lr 0.000012	 wd 0.0000	time 0.2236 (0.2472)	loss 1.3635 (1.3741)	grad_norm 0.4465 (0.5013)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:18:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0000	time 0.1850 (0.2454)	loss 1.6374 (1.3749)	grad_norm 0.4760 (0.5007)	loss_scale 16384.0000 (8329.5706)	mem 7984MB
[2024-06-29 21:18:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 20 training takes 0:10:18
[2024-06-29 21:18:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_20.pth saving......
[2024-06-29 21:18:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_20.pth saved !!!
[2024-06-29 21:19:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 34.204 (34.204)	Loss 0.4194 (0.4194)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 21:19:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.000 Acc@5 97.248
[2024-06-29 21:19:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 21:19:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 21:19:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][0/2502]	eta 11:27:19 lr 0.000012	 wd 0.0000	time 16.4825 (16.4825)	loss 1.5652 (1.5652)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 7984MB
[2024-06-29 21:19:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:15:22 lr 0.000012	 wd 0.0000	time 0.2160 (0.3840)	loss 1.4607 (1.3946)	grad_norm 0.4245 (0.4830)	loss_scale 16384.0000 (16384.0000)	mem 7984MB
[2024-06-29 21:20:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:13:25 lr 0.000012	 wd 0.0000	time 0.2377 (0.3501)	loss 0.8992 (1.3747)	grad_norm 0.5205 (0.5050)	loss_scale 16384.0000 (16384.0000)	mem 7984MB
[2024-06-29 21:20:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:11:26 lr 0.000012	 wd 0.0000	time 0.2382 (0.3117)	loss 1.4039 (1.3676)	grad_norm 0.4734 (inf)	loss_scale 8192.0000 (15186.4983)	mem 7984MB
[2024-06-29 21:21:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:10:10 lr 0.000011	 wd 0.0000	time 0.2175 (0.2905)	loss 1.4531 (1.3688)	grad_norm 0.4653 (inf)	loss_scale 8192.0000 (13442.2344)	mem 7984MB
[2024-06-29 21:21:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:09:17 lr 0.000011	 wd 0.0000	time 0.2292 (0.2783)	loss 1.3597 (1.3681)	grad_norm 0.5048 (inf)	loss_scale 8192.0000 (12394.2834)	mem 7984MB
[2024-06-29 21:22:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:08:44 lr 0.000011	 wd 0.0000	time 0.4104 (0.2759)	loss 1.2084 (1.3687)	grad_norm 0.5530 (inf)	loss_scale 8192.0000 (11695.0682)	mem 7984MB
[2024-06-29 21:22:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:08:07 lr 0.000011	 wd 0.0000	time 0.2498 (0.2703)	loss 1.2976 (1.3632)	grad_norm 0.4432 (inf)	loss_scale 8192.0000 (11195.3438)	mem 7984MB
[2024-06-29 21:22:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:07:31 lr 0.000011	 wd 0.0000	time 0.2155 (0.2655)	loss 1.2228 (1.3628)	grad_norm 0.4520 (inf)	loss_scale 8192.0000 (10820.3945)	mem 7984MB
[2024-06-29 21:23:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:06:58 lr 0.000011	 wd 0.0000	time 0.2249 (0.2614)	loss 1.5448 (1.3635)	grad_norm 0.4904 (inf)	loss_scale 8192.0000 (10528.6748)	mem 7984MB
[2024-06-29 21:23:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:06:31 lr 0.000011	 wd 0.0000	time 0.1983 (0.2606)	loss 1.0574 (1.3626)	grad_norm 0.5167 (inf)	loss_scale 8192.0000 (10295.2408)	mem 7984MB
[2024-06-29 21:24:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:06:03 lr 0.000011	 wd 0.0000	time 0.2254 (0.2595)	loss 1.4645 (1.3626)	grad_norm 0.4496 (inf)	loss_scale 8192.0000 (10104.2107)	mem 7984MB
[2024-06-29 21:24:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:05:34 lr 0.000011	 wd 0.0000	time 0.2597 (0.2572)	loss 1.3324 (1.3637)	grad_norm 0.4859 (inf)	loss_scale 8192.0000 (9944.9925)	mem 7984MB
[2024-06-29 21:24:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:05:06 lr 0.000011	 wd 0.0000	time 0.2187 (0.2550)	loss 1.5177 (1.3638)	grad_norm 0.4255 (inf)	loss_scale 8192.0000 (9810.2506)	mem 7984MB
[2024-06-29 21:25:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:04:40 lr 0.000011	 wd 0.0000	time 0.2517 (0.2547)	loss 1.4462 (1.3643)	grad_norm 0.5210 (inf)	loss_scale 8192.0000 (9694.7438)	mem 7984MB
[2024-06-29 21:25:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:04:14 lr 0.000010	 wd 0.0000	time 0.2381 (0.2543)	loss 1.6427 (1.3640)	grad_norm 0.6640 (inf)	loss_scale 8192.0000 (9594.6276)	mem 7984MB
[2024-06-29 21:26:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:03:48 lr 0.000010	 wd 0.0000	time 0.2385 (0.2530)	loss 1.2169 (1.3635)	grad_norm 0.5283 (inf)	loss_scale 8192.0000 (9507.0181)	mem 7984MB
[2024-06-29 21:26:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:03:22 lr 0.000010	 wd 0.0000	time 0.2530 (0.2520)	loss 1.4861 (1.3636)	grad_norm 0.5198 (inf)	loss_scale 8192.0000 (9429.7096)	mem 7984MB
[2024-06-29 21:26:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:02:56 lr 0.000010	 wd 0.0000	time 0.2420 (0.2517)	loss 1.0297 (1.3641)	grad_norm 0.4585 (inf)	loss_scale 8192.0000 (9360.9861)	mem 7984MB
[2024-06-29 21:27:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:32 lr 0.000010	 wd 0.0000	time 0.2312 (0.2527)	loss 1.0011 (1.3627)	grad_norm 0.4892 (inf)	loss_scale 8192.0000 (9299.4929)	mem 7984MB
[2024-06-29 21:27:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:06 lr 0.000010	 wd 0.0000	time 0.2065 (0.2519)	loss 1.4028 (1.3623)	grad_norm 0.4595 (inf)	loss_scale 8192.0000 (9244.1459)	mem 7984MB
[2024-06-29 21:28:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:40 lr 0.000010	 wd 0.0000	time 0.2388 (0.2510)	loss 1.2007 (1.3626)	grad_norm 0.4665 (inf)	loss_scale 8192.0000 (9194.0676)	mem 7984MB
[2024-06-29 21:28:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:15 lr 0.000010	 wd 0.0000	time 0.2745 (0.2508)	loss 1.5634 (1.3605)	grad_norm 0.7606 (inf)	loss_scale 8192.0000 (9148.5398)	mem 7984MB
[2024-06-29 21:28:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:50 lr 0.000010	 wd 0.0000	time 0.2635 (0.2510)	loss 1.5794 (1.3603)	grad_norm 0.4878 (inf)	loss_scale 8192.0000 (9106.9691)	mem 7984MB
[2024-06-29 21:29:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:25 lr 0.000010	 wd 0.0000	time 0.2138 (0.2504)	loss 1.3808 (1.3616)	grad_norm 0.4508 (inf)	loss_scale 8192.0000 (9068.8613)	mem 7984MB
[2024-06-29 21:29:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.1896 (0.2486)	loss 1.3328 (1.3613)	grad_norm 0.5146 (inf)	loss_scale 8192.0000 (9033.8009)	mem 7984MB
[2024-06-29 21:29:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 21 training takes 0:10:26
[2024-06-29 21:30:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 34.991 (34.991)	Loss 0.4172 (0.4172)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 21:30:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.028 Acc@5 97.260
[2024-06-29 21:30:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 21:30:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 21:30:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][0/2502]	eta 11:20:06 lr 0.000010	 wd 0.0000	time 16.3094 (16.3094)	loss 1.3122 (1.3122)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:31:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:15:38 lr 0.000010	 wd 0.0000	time 0.1942 (0.3908)	loss 1.2704 (1.3718)	grad_norm 0.4422 (0.5129)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:31:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:12:27 lr 0.000009	 wd 0.0000	time 0.2763 (0.3246)	loss 1.1965 (1.3687)	grad_norm 0.4138 (0.4897)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:32:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:10:56 lr 0.000009	 wd 0.0000	time 0.2174 (0.2983)	loss 1.5252 (1.3729)	grad_norm 0.5693 (0.4907)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:32:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:09:48 lr 0.000009	 wd 0.0000	time 0.2039 (0.2800)	loss 1.4368 (1.3768)	grad_norm 0.5502 (0.4891)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:32:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:08:59 lr 0.000009	 wd 0.0000	time 0.2152 (0.2693)	loss 0.9040 (1.3760)	grad_norm 0.5414 (0.4975)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:33:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:08:25 lr 0.000009	 wd 0.0000	time 0.2939 (0.2656)	loss 1.3727 (1.3790)	grad_norm 0.5180 (0.4978)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:33:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:07:54 lr 0.000009	 wd 0.0000	time 0.2278 (0.2636)	loss 0.9073 (1.3743)	grad_norm 0.4778 (0.5012)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:34:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:07:21 lr 0.000009	 wd 0.0000	time 0.2282 (0.2593)	loss 1.3804 (1.3702)	grad_norm 0.4673 (0.5000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:34:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:06:50 lr 0.000009	 wd 0.0000	time 0.2243 (0.2561)	loss 1.3597 (1.3692)	grad_norm 0.4940 (0.4982)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:34:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:06:22 lr 0.000009	 wd 0.0000	time 0.2517 (0.2547)	loss 1.5583 (1.3714)	grad_norm 0.4874 (0.5004)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:35:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:05:57 lr 0.000009	 wd 0.0000	time 0.2451 (0.2547)	loss 1.4149 (1.3690)	grad_norm 0.4884 (0.5008)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:35:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:05:28 lr 0.000009	 wd 0.0000	time 0.2354 (0.2527)	loss 1.3470 (1.3677)	grad_norm 0.4767 (0.4999)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:36:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:05:01 lr 0.000009	 wd 0.0000	time 0.2434 (0.2511)	loss 0.9376 (1.3682)	grad_norm 0.6341 (0.4996)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 21:36:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:04:35 lr 0.000008	 wd 0.0000	time 0.2427 (0.2503)	loss 1.5731 (1.3702)	grad_norm 0.4695 (nan)	loss_scale 4096.0000 (7917.1792)	mem 7984MB
[2024-06-29 21:36:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:04:10 lr 0.000008	 wd 0.0000	time 0.2590 (0.2505)	loss 1.3265 (1.3708)	grad_norm 0.4513 (nan)	loss_scale 4096.0000 (7662.6036)	mem 7984MB
[2024-06-29 21:37:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:03:45 lr 0.000008	 wd 0.0000	time 0.2355 (0.2496)	loss 1.5545 (1.3711)	grad_norm 0.5269 (nan)	loss_scale 4096.0000 (7439.8301)	mem 7984MB
[2024-06-29 21:37:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:03:19 lr 0.000008	 wd 0.0000	time 0.2113 (0.2487)	loss 1.3175 (1.3707)	grad_norm 0.5735 (nan)	loss_scale 4096.0000 (7243.2499)	mem 7984MB
[2024-06-29 21:38:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:02:54 lr 0.000008	 wd 0.0000	time 0.2262 (0.2481)	loss 1.5926 (1.3700)	grad_norm 0.5169 (nan)	loss_scale 4096.0000 (7068.4997)	mem 7984MB
[2024-06-29 21:38:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:29 lr 0.000008	 wd 0.0000	time 0.2351 (0.2491)	loss 1.5529 (1.3694)	grad_norm 0.5017 (nan)	loss_scale 4096.0000 (6912.1347)	mem 7984MB
[2024-06-29 21:38:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:04 lr 0.000008	 wd 0.0000	time 0.2607 (0.2484)	loss 1.5225 (1.3685)	grad_norm 0.5229 (nan)	loss_scale 4096.0000 (6771.3983)	mem 7984MB
[2024-06-29 21:39:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:39 lr 0.000008	 wd 0.0000	time 0.2296 (0.2478)	loss 1.4374 (1.3690)	grad_norm 0.5141 (nan)	loss_scale 4096.0000 (6644.0590)	mem 7984MB
[2024-06-29 21:39:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:14 lr 0.000008	 wd 0.0000	time 0.2222 (0.2473)	loss 1.4417 (1.3672)	grad_norm 0.4904 (nan)	loss_scale 4096.0000 (6528.2908)	mem 7984MB
[2024-06-29 21:40:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:49 lr 0.000008	 wd 0.0000	time 0.2034 (0.2474)	loss 1.5086 (1.3673)	grad_norm 0.5061 (nan)	loss_scale 4096.0000 (6422.5850)	mem 7984MB
[2024-06-29 21:40:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:25 lr 0.000008	 wd 0.0000	time 0.2408 (0.2469)	loss 1.2135 (1.3667)	grad_norm 0.4496 (nan)	loss_scale 4096.0000 (6325.6843)	mem 7984MB
[2024-06-29 21:40:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.1912 (0.2451)	loss 0.9929 (1.3657)	grad_norm 0.5220 (nan)	loss_scale 4096.0000 (6236.5326)	mem 7984MB
[2024-06-29 21:40:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 22 training takes 0:10:19
[2024-06-29 21:41:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 29.341 (29.341)	Loss 0.4180 (0.4180)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 21:41:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.050 Acc@5 97.276
[2024-06-29 21:41:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 21:41:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 21:42:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][0/2502]	eta 11:06:14 lr 0.000008	 wd 0.0000	time 15.9769 (15.9769)	loss 1.1649 (1.1649)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:42:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:15:49 lr 0.000008	 wd 0.0000	time 0.2207 (0.3953)	loss 1.5788 (1.4033)	grad_norm 0.4511 (0.5406)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:42:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:12:01 lr 0.000007	 wd 0.0000	time 0.2607 (0.3134)	loss 1.5502 (1.3820)	grad_norm 0.4732 (0.5440)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:43:14 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:10:50 lr 0.000007	 wd 0.0000	time 0.2245 (0.2952)	loss 1.5231 (1.3755)	grad_norm 0.5418 (0.5252)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:43:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:09:46 lr 0.000007	 wd 0.0000	time 0.2187 (0.2792)	loss 1.5369 (1.3671)	grad_norm 0.4586 (0.5169)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:44:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:08:58 lr 0.000007	 wd 0.0000	time 0.2158 (0.2690)	loss 1.6952 (1.3721)	grad_norm 0.4846 (0.5117)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:44:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:08:20 lr 0.000007	 wd 0.0000	time 0.2096 (0.2629)	loss 1.1455 (1.3720)	grad_norm 0.4356 (0.5058)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:44:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:07:52 lr 0.000007	 wd 0.0000	time 0.2693 (0.2621)	loss 1.4211 (1.3693)	grad_norm 0.5109 (0.5063)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:45:13 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:07:21 lr 0.000007	 wd 0.0000	time 0.2270 (0.2592)	loss 1.6993 (1.3681)	grad_norm 0.4495 (0.5053)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:45:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:06:49 lr 0.000007	 wd 0.0000	time 0.1978 (0.2559)	loss 1.7121 (1.3645)	grad_norm 0.5351 (0.5025)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:45:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:06:20 lr 0.000007	 wd 0.0000	time 0.2491 (0.2531)	loss 1.1566 (1.3625)	grad_norm 0.4581 (0.5006)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:46:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:05:54 lr 0.000007	 wd 0.0000	time 0.2447 (0.2531)	loss 0.8894 (1.3580)	grad_norm 0.4833 (0.4999)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:46:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:05:28 lr 0.000007	 wd 0.0000	time 0.2129 (0.2523)	loss 1.5778 (1.3598)	grad_norm 0.6062 (0.4994)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:47:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:05:01 lr 0.000007	 wd 0.0000	time 0.2445 (0.2507)	loss 1.4570 (1.3612)	grad_norm 0.4623 (0.4987)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:47:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:04:34 lr 0.000007	 wd 0.0000	time 0.2168 (0.2492)	loss 1.5976 (1.3612)	grad_norm 0.6792 (0.4992)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:48:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:04:09 lr 0.000006	 wd 0.0000	time 0.2197 (0.2493)	loss 1.2782 (1.3633)	grad_norm 0.5076 (0.5012)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:48:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:03:45 lr 0.000006	 wd 0.0000	time 0.2151 (0.2499)	loss 1.1715 (1.3626)	grad_norm 0.4761 (0.5045)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:48:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:03:19 lr 0.000006	 wd 0.0000	time 0.2149 (0.2489)	loss 1.3178 (1.3629)	grad_norm 0.6240 (0.5082)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:49:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:02:54 lr 0.000006	 wd 0.0000	time 0.2664 (0.2480)	loss 1.4337 (1.3623)	grad_norm 0.4595 (0.5064)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:49:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:02:29 lr 0.000006	 wd 0.0000	time 0.2360 (0.2481)	loss 1.2459 (1.3649)	grad_norm 0.5140 (0.5073)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:50:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:04 lr 0.000006	 wd 0.0000	time 0.2806 (0.2487)	loss 1.4542 (1.3637)	grad_norm 0.5258 (0.5053)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:50:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:39 lr 0.000006	 wd 0.0000	time 0.2210 (0.2482)	loss 1.7081 (1.3641)	grad_norm 0.4952 (0.5050)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:50:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:14 lr 0.000006	 wd 0.0000	time 0.2807 (0.2474)	loss 1.4784 (1.3619)	grad_norm 0.4661 (0.5038)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:51:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:49 lr 0.000006	 wd 0.0000	time 0.2229 (0.2475)	loss 1.6003 (1.3621)	grad_norm 0.4854 (0.5031)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:51:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:25 lr 0.000006	 wd 0.0000	time 0.2202 (0.2473)	loss 1.5460 (1.3627)	grad_norm 0.4646 (0.5038)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:51:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0000	time 0.1722 (0.2455)	loss 1.4369 (1.3634)	grad_norm 0.4457 (0.5032)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:52:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 23 training takes 0:10:22
[2024-06-29 21:52:26 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 18.184 (18.184)	Loss 0.4175 (0.4175)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 21:52:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.022 Acc@5 97.262
[2024-06-29 21:52:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 21:52:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 21:53:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][0/2502]	eta 1 day, 1:03:23 lr 0.000006	 wd 0.0000	time 36.0524 (36.0524)	loss 1.5952 (1.5952)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:53:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:23:26 lr 0.000006	 wd 0.0000	time 0.2206 (0.5857)	loss 1.1545 (1.4030)	grad_norm 0.8762 (0.5190)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:54:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:15:32 lr 0.000006	 wd 0.0000	time 0.2088 (0.4051)	loss 1.4238 (1.3848)	grad_norm 0.6770 (0.5110)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:54:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:13:03 lr 0.000006	 wd 0.0000	time 0.2553 (0.3559)	loss 1.6341 (1.3780)	grad_norm 0.6181 (0.5058)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 21:54:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:11:25 lr 0.000005	 wd 0.0000	time 0.2036 (0.3262)	loss 1.4167 (1.3745)	grad_norm 0.4431 (0.4994)	loss_scale 8192.0000 (5097.0175)	mem 7984MB
[2024-06-29 21:55:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:10:12 lr 0.000005	 wd 0.0000	time 0.2345 (0.3059)	loss 1.6686 (1.3758)	grad_norm 0.4653 (0.4977)	loss_scale 8192.0000 (5714.7784)	mem 7984MB
[2024-06-29 21:55:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:09:17 lr 0.000005	 wd 0.0000	time 0.2026 (0.2931)	loss 1.4033 (1.3728)	grad_norm 0.4721 (0.5021)	loss_scale 8192.0000 (6126.9617)	mem 7984MB
[2024-06-29 21:56:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:08:37 lr 0.000005	 wd 0.0000	time 0.2785 (0.2870)	loss 1.3515 (1.3736)	grad_norm 0.4645 (0.5010)	loss_scale 8192.0000 (6421.5464)	mem 7984MB
[2024-06-29 21:56:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:08:00 lr 0.000005	 wd 0.0000	time 0.2268 (0.2825)	loss 1.4216 (1.3745)	grad_norm 0.4772 (0.5009)	loss_scale 8192.0000 (6642.5768)	mem 7984MB
[2024-06-29 21:56:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:07:23 lr 0.000005	 wd 0.0000	time 0.2040 (0.2767)	loss 1.5625 (1.3714)	grad_norm 1.3242 (0.5042)	loss_scale 8192.0000 (6814.5438)	mem 7984MB
[2024-06-29 21:57:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:06:48 lr 0.000005	 wd 0.0000	time 0.2352 (0.2721)	loss 1.2196 (1.3717)	grad_norm 0.5814 (0.5033)	loss_scale 8192.0000 (6952.1518)	mem 7984MB
[2024-06-29 21:57:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:06:18 lr 0.000005	 wd 0.0000	time 0.2546 (0.2702)	loss 1.5050 (1.3706)	grad_norm 0.4973 (0.5024)	loss_scale 8192.0000 (7064.7629)	mem 7984MB
[2024-06-29 21:58:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:05:49 lr 0.000005	 wd 0.0000	time 0.2293 (0.2683)	loss 1.2558 (1.3687)	grad_norm 0.5053 (0.5029)	loss_scale 8192.0000 (7158.6211)	mem 7984MB
[2024-06-29 21:58:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:05:19 lr 0.000005	 wd 0.0000	time 0.2329 (0.2654)	loss 1.3978 (1.3680)	grad_norm 0.5489 (0.5023)	loss_scale 8192.0000 (7238.0507)	mem 7984MB
[2024-06-29 21:58:51 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:04:49 lr 0.000005	 wd 0.0000	time 0.1948 (0.2629)	loss 1.3584 (1.3664)	grad_norm 0.4890 (0.5016)	loss_scale 8192.0000 (7306.1413)	mem 7984MB
[2024-06-29 21:59:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:04:22 lr 0.000005	 wd 0.0000	time 0.2739 (0.2616)	loss 0.9772 (1.3649)	grad_norm 0.5260 (0.5013)	loss_scale 8192.0000 (7365.1592)	mem 7984MB
[2024-06-29 21:59:41 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:03:55 lr 0.000005	 wd 0.0000	time 0.2657 (0.2616)	loss 1.5171 (1.3645)	grad_norm 0.4663 (0.5006)	loss_scale 8192.0000 (7416.8045)	mem 7984MB
[2024-06-29 22:00:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:03:28 lr 0.000005	 wd 0.0000	time 0.2231 (0.2600)	loss 1.6924 (1.3657)	grad_norm 0.4473 (0.4993)	loss_scale 8192.0000 (7462.3774)	mem 7984MB
[2024-06-29 22:00:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:01 lr 0.000005	 wd 0.0000	time 0.2313 (0.2584)	loss 1.1439 (1.3636)	grad_norm 0.4624 (0.4996)	loss_scale 8192.0000 (7502.8895)	mem 7984MB
[2024-06-29 22:00:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:02:35 lr 0.000005	 wd 0.0000	time 0.2575 (0.2578)	loss 1.4095 (1.3647)	grad_norm 0.4874 (0.5011)	loss_scale 8192.0000 (7539.1394)	mem 7984MB
[2024-06-29 22:01:19 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:09 lr 0.000004	 wd 0.0000	time 0.2382 (0.2582)	loss 1.3791 (1.3653)	grad_norm 0.4618 (0.5011)	loss_scale 8192.0000 (7571.7661)	mem 7984MB
[2024-06-29 22:01:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:43 lr 0.000004	 wd 0.0000	time 0.2324 (0.2571)	loss 1.5416 (1.3645)	grad_norm 0.4965 (0.5007)	loss_scale 8192.0000 (7601.2870)	mem 7984MB
[2024-06-29 22:02:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:17 lr 0.000004	 wd 0.0000	time 0.2257 (0.2561)	loss 1.4119 (1.3643)	grad_norm 0.4485 (0.5005)	loss_scale 8192.0000 (7628.1254)	mem 7984MB
[2024-06-29 22:02:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:51 lr 0.000004	 wd 0.0000	time 0.2328 (0.2556)	loss 1.7364 (1.3640)	grad_norm 0.5392 (0.4997)	loss_scale 8192.0000 (7652.6310)	mem 7984MB
[2024-06-29 22:02:56 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:26 lr 0.000004	 wd 0.0000	time 0.2616 (0.2558)	loss 1.5146 (1.3639)	grad_norm 0.4454 (0.4998)	loss_scale 8192.0000 (7675.0954)	mem 7984MB
[2024-06-29 22:03:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0000	time 0.1693 (0.2538)	loss 1.2712 (1.3640)	grad_norm 0.4762 (0.4996)	loss_scale 8192.0000 (7695.7633)	mem 7984MB
[2024-06-29 22:03:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 24 training takes 0:10:42
[2024-06-29 22:03:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 19.048 (19.048)	Loss 0.4180 (0.4180)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 22:04:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.062 Acc@5 97.268
[2024-06-29 22:04:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 22:04:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 22:04:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 22:04:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 22:04:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][0/2502]	eta 23:59:13 lr 0.000004	 wd 0.0000	time 34.5137 (34.5137)	loss 1.2647 (1.2647)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7984MB
[2024-06-29 22:05:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:22:42 lr 0.000004	 wd 0.0000	time 0.2278 (0.5672)	loss 1.3858 (1.3323)	grad_norm 0.4412 (nan)	loss_scale 4096.0000 (5920.9505)	mem 7984MB
[2024-06-29 22:05:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:15:15 lr 0.000004	 wd 0.0000	time 0.2091 (0.3976)	loss 1.4986 (1.3595)	grad_norm 0.4498 (nan)	loss_scale 4096.0000 (5013.0149)	mem 7984MB
[2024-06-29 22:05:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:12:52 lr 0.000004	 wd 0.0000	time 0.2669 (0.3507)	loss 1.1301 (1.3496)	grad_norm 0.4174 (nan)	loss_scale 4096.0000 (4708.3588)	mem 7984MB
[2024-06-29 22:06:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:11:18 lr 0.000004	 wd 0.0000	time 0.2251 (0.3227)	loss 1.2601 (1.3647)	grad_norm 0.4509 (nan)	loss_scale 4096.0000 (4555.6509)	mem 7984MB
[2024-06-29 22:06:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:10:06 lr 0.000004	 wd 0.0000	time 0.2221 (0.3028)	loss 1.5540 (1.3650)	grad_norm 0.4430 (nan)	loss_scale 4096.0000 (4463.9042)	mem 7984MB
[2024-06-29 22:06:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:09:13 lr 0.000004	 wd 0.0000	time 0.2206 (0.2910)	loss 1.1603 (1.3664)	grad_norm 0.4411 (nan)	loss_scale 4096.0000 (4402.6889)	mem 7984MB
[2024-06-29 22:07:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:08:33 lr 0.000004	 wd 0.0000	time 0.2679 (0.2849)	loss 1.2510 (1.3669)	grad_norm 0.4408 (nan)	loss_scale 4096.0000 (4358.9387)	mem 7984MB
[2024-06-29 22:07:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:07:56 lr 0.000004	 wd 0.0000	time 0.1971 (0.2802)	loss 1.0136 (1.3671)	grad_norm 0.5390 (nan)	loss_scale 4096.0000 (4326.1124)	mem 7984MB
[2024-06-29 22:08:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:07:19 lr 0.000004	 wd 0.0000	time 0.2069 (0.2746)	loss 1.5278 (1.3690)	grad_norm 0.4662 (nan)	loss_scale 4096.0000 (4300.5727)	mem 7984MB
[2024-06-29 22:08:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:06:45 lr 0.000004	 wd 0.0000	time 0.2128 (0.2701)	loss 1.2272 (1.3696)	grad_norm 0.4517 (nan)	loss_scale 4096.0000 (4280.1359)	mem 7984MB
[2024-06-29 22:08:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:06:15 lr 0.000004	 wd 0.0000	time 0.2539 (0.2679)	loss 1.3068 (1.3677)	grad_norm 0.4505 (nan)	loss_scale 4096.0000 (4263.4114)	mem 7984MB
[2024-06-29 22:09:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:05:47 lr 0.000004	 wd 0.0000	time 0.2321 (0.2666)	loss 1.3577 (1.3700)	grad_norm 0.5158 (nan)	loss_scale 4096.0000 (4249.4721)	mem 7984MB
[2024-06-29 22:09:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:05:16 lr 0.000003	 wd 0.0000	time 0.2033 (0.2637)	loss 1.4892 (1.3717)	grad_norm 0.4908 (nan)	loss_scale 4096.0000 (4237.6756)	mem 7984MB
[2024-06-29 22:10:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:04:48 lr 0.000003	 wd 0.0000	time 0.2222 (0.2616)	loss 1.1206 (1.3711)	grad_norm 0.4799 (nan)	loss_scale 4096.0000 (4227.5632)	mem 7984MB
[2024-06-29 22:10:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:04:20 lr 0.000003	 wd 0.0000	time 0.2790 (0.2604)	loss 1.2473 (1.3712)	grad_norm 0.4726 (nan)	loss_scale 4096.0000 (4218.7981)	mem 7984MB
[2024-06-29 22:11:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:03:54 lr 0.000003	 wd 0.0000	time 0.1916 (0.2603)	loss 1.4974 (1.3705)	grad_norm 0.4621 (nan)	loss_scale 4096.0000 (4211.1280)	mem 7984MB
[2024-06-29 22:11:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:03:27 lr 0.000003	 wd 0.0000	time 0.2137 (0.2587)	loss 1.3843 (1.3698)	grad_norm 0.4549 (nan)	loss_scale 4096.0000 (4204.3598)	mem 7984MB
[2024-06-29 22:11:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:00 lr 0.000003	 wd 0.0000	time 0.2275 (0.2573)	loss 1.5693 (1.3696)	grad_norm 0.4736 (nan)	loss_scale 4096.0000 (4198.3431)	mem 7984MB
[2024-06-29 22:12:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:02:34 lr 0.000003	 wd 0.0000	time 0.2761 (0.2566)	loss 1.9504 (1.3693)	grad_norm 0.6240 (nan)	loss_scale 4096.0000 (4192.9595)	mem 7984MB
[2024-06-29 22:12:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:09 lr 0.000003	 wd 0.0000	time 0.3040 (0.2572)	loss 1.2707 (1.3687)	grad_norm 0.6888 (nan)	loss_scale 4096.0000 (4188.1139)	mem 7984MB
[2024-06-29 22:13:02 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:43 lr 0.000003	 wd 0.0000	time 0.2532 (0.2563)	loss 0.8831 (1.3687)	grad_norm 0.4768 (nan)	loss_scale 4096.0000 (4183.7297)	mem 7984MB
[2024-06-29 22:13:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:17 lr 0.000003	 wd 0.0000	time 0.2041 (0.2552)	loss 1.4351 (1.3674)	grad_norm 0.4973 (nan)	loss_scale 4096.0000 (4179.7438)	mem 7984MB
[2024-06-29 22:13:49 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:51 lr 0.000003	 wd 0.0000	time 0.2511 (0.2548)	loss 1.2232 (1.3661)	grad_norm 0.4378 (nan)	loss_scale 4096.0000 (4176.1043)	mem 7984MB
[2024-06-29 22:14:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:26 lr 0.000003	 wd 0.0000	time 0.2043 (0.2549)	loss 1.3969 (1.3676)	grad_norm 0.4437 (nan)	loss_scale 4096.0000 (4172.7680)	mem 7984MB
[2024-06-29 22:14:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.1873 (0.2528)	loss 1.4885 (1.3677)	grad_norm 0.4664 (nan)	loss_scale 4096.0000 (4169.6985)	mem 7984MB
[2024-06-29 22:14:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 25 training takes 0:10:41
[2024-06-29 22:15:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 19.831 (19.831)	Loss 0.4182 (0.4182)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 22:15:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.058 Acc@5 97.264
[2024-06-29 22:15:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 22:15:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 22:15:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][0/2502]	eta 1 day, 1:42:00 lr 0.000003	 wd 0.0000	time 36.9788 (36.9788)	loss 1.3280 (1.3280)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:16:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:24:11 lr 0.000003	 wd 0.0000	time 0.2201 (0.6042)	loss 1.5150 (1.3596)	grad_norm 0.4374 (0.4891)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:16:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:15:54 lr 0.000003	 wd 0.0000	time 0.2475 (0.4148)	loss 1.4861 (1.3811)	grad_norm 0.4541 (0.4951)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:17:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:13:16 lr 0.000003	 wd 0.0000	time 0.2193 (0.3617)	loss 1.4981 (1.3729)	grad_norm 0.5162 (0.4956)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:17:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:11:35 lr 0.000003	 wd 0.0000	time 0.1989 (0.3308)	loss 1.6538 (1.3744)	grad_norm 0.4437 (0.4958)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:17:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:10:20 lr 0.000003	 wd 0.0000	time 0.2257 (0.3098)	loss 1.6174 (1.3808)	grad_norm 0.5265 (inf)	loss_scale 2048.0000 (3826.2036)	mem 7984MB
[2024-06-29 22:18:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:09:24 lr 0.000003	 wd 0.0000	time 0.2321 (0.2965)	loss 1.4013 (1.3759)	grad_norm 0.4955 (inf)	loss_scale 2048.0000 (3530.3295)	mem 7984MB
[2024-06-29 22:18:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:08:42 lr 0.000003	 wd 0.0000	time 0.2554 (0.2900)	loss 1.5298 (1.3717)	grad_norm 0.4222 (inf)	loss_scale 2048.0000 (3318.8702)	mem 7984MB
[2024-06-29 22:19:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:08:04 lr 0.000002	 wd 0.0000	time 0.2135 (0.2849)	loss 1.5966 (1.3704)	grad_norm 0.4314 (inf)	loss_scale 2048.0000 (3160.2097)	mem 7984MB
[2024-06-29 22:19:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:07:26 lr 0.000002	 wd 0.0000	time 0.1973 (0.2788)	loss 1.3841 (1.3704)	grad_norm 0.4530 (inf)	loss_scale 2048.0000 (3036.7680)	mem 7984MB
[2024-06-29 22:19:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:06:51 lr 0.000002	 wd 0.0000	time 0.2093 (0.2739)	loss 1.4155 (1.3710)	grad_norm 0.4598 (inf)	loss_scale 2048.0000 (2937.9900)	mem 7984MB
[2024-06-29 22:20:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:06:21 lr 0.000002	 wd 0.0000	time 0.2467 (0.2718)	loss 1.5930 (1.3726)	grad_norm 0.4713 (inf)	loss_scale 2048.0000 (2857.1553)	mem 7984MB
[2024-06-29 22:20:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:05:51 lr 0.000002	 wd 0.0000	time 0.2041 (0.2701)	loss 1.0860 (1.3731)	grad_norm 0.4857 (inf)	loss_scale 2048.0000 (2789.7818)	mem 7984MB
[2024-06-29 22:21:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:05:21 lr 0.000002	 wd 0.0000	time 0.2637 (0.2673)	loss 0.9597 (1.3714)	grad_norm 0.6864 (inf)	loss_scale 2048.0000 (2732.7656)	mem 7984MB
[2024-06-29 22:21:33 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:04:51 lr 0.000002	 wd 0.0000	time 0.2103 (0.2646)	loss 1.6120 (1.3708)	grad_norm 0.7337 (inf)	loss_scale 2048.0000 (2683.8887)	mem 7984MB
[2024-06-29 22:21:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:04:24 lr 0.000002	 wd 0.0000	time 0.2661 (0.2636)	loss 1.4094 (1.3695)	grad_norm 0.5049 (inf)	loss_scale 2048.0000 (2641.5243)	mem 7984MB
[2024-06-29 22:22:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:03:57 lr 0.000002	 wd 0.0000	time 0.2125 (0.2633)	loss 1.4560 (1.3684)	grad_norm 0.5159 (inf)	loss_scale 2048.0000 (2604.4522)	mem 7984MB
[2024-06-29 22:22:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:03:29 lr 0.000002	 wd 0.0000	time 0.2237 (0.2614)	loss 1.2843 (1.3695)	grad_norm 0.5263 (inf)	loss_scale 2048.0000 (2571.7390)	mem 7984MB
[2024-06-29 22:23:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:02 lr 0.000002	 wd 0.0000	time 0.2165 (0.2598)	loss 1.5335 (1.3723)	grad_norm 0.4987 (inf)	loss_scale 2048.0000 (2542.6585)	mem 7984MB
[2024-06-29 22:23:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:02:35 lr 0.000002	 wd 0.0000	time 0.2348 (0.2591)	loss 0.9901 (1.3709)	grad_norm 0.4275 (inf)	loss_scale 2048.0000 (2516.6376)	mem 7984MB
[2024-06-29 22:24:00 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:09 lr 0.000002	 wd 0.0000	time 0.2317 (0.2589)	loss 0.9252 (1.3699)	grad_norm 0.5075 (inf)	loss_scale 2048.0000 (2493.2174)	mem 7984MB
[2024-06-29 22:24:24 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:43 lr 0.000002	 wd 0.0000	time 0.2216 (0.2578)	loss 1.1312 (1.3688)	grad_norm 0.4652 (inf)	loss_scale 2048.0000 (2472.0267)	mem 7984MB
[2024-06-29 22:24:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:17 lr 0.000002	 wd 0.0000	time 0.2238 (0.2567)	loss 0.9590 (1.3686)	grad_norm 0.5040 (inf)	loss_scale 2048.0000 (2452.7615)	mem 7984MB
[2024-06-29 22:25:12 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:51 lr 0.000002	 wd 0.0000	time 0.2484 (0.2561)	loss 1.2015 (1.3674)	grad_norm 0.5688 (inf)	loss_scale 2048.0000 (2435.1708)	mem 7984MB
[2024-06-29 22:25:37 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:26 lr 0.000002	 wd 0.0000	time 0.2286 (0.2561)	loss 1.1292 (1.3675)	grad_norm 0.4741 (inf)	loss_scale 2048.0000 (2419.0454)	mem 7984MB
[2024-06-29 22:25:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.1730 (0.2541)	loss 0.9494 (1.3657)	grad_norm 0.4542 (inf)	loss_scale 2048.0000 (2404.2095)	mem 7984MB
[2024-06-29 22:26:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 26 training takes 0:10:44
[2024-06-29 22:26:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 19.285 (19.285)	Loss 0.4180 (0.4180)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 22:26:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.052 Acc@5 97.282
[2024-06-29 22:26:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 22:26:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 22:27:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][0/2502]	eta 1 day, 3:24:38 lr 0.000002	 wd 0.0000	time 39.4399 (39.4399)	loss 0.9402 (0.9402)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:27:46 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:24:34 lr 0.000002	 wd 0.0000	time 0.2115 (0.6140)	loss 1.5017 (1.3719)	grad_norm 0.5144 (0.4927)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:28:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:16:11 lr 0.000002	 wd 0.0000	time 0.2503 (0.4218)	loss 1.5791 (1.3671)	grad_norm 0.4786 (0.4961)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:28:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:13:43 lr 0.000002	 wd 0.0000	time 0.1795 (0.3740)	loss 1.3665 (1.3671)	grad_norm 0.4457 (0.4962)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:28:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:11:51 lr 0.000002	 wd 0.0000	time 0.2065 (0.3384)	loss 1.4436 (1.3652)	grad_norm 0.4634 (0.4968)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:29:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:10:32 lr 0.000002	 wd 0.0000	time 0.2154 (0.3159)	loss 0.9826 (1.3611)	grad_norm 0.6010 (0.4966)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:29:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:09:32 lr 0.000002	 wd 0.0000	time 0.2413 (0.3012)	loss 1.2749 (1.3644)	grad_norm 0.4605 (0.4970)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:30:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:08:50 lr 0.000002	 wd 0.0000	time 0.2665 (0.2942)	loss 1.4831 (1.3702)	grad_norm 0.4657 (0.4944)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:30:35 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:08:10 lr 0.000002	 wd 0.0000	time 0.1985 (0.2883)	loss 1.5359 (1.3738)	grad_norm 0.4433 (0.4941)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:30:58 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:07:31 lr 0.000001	 wd 0.0000	time 0.2404 (0.2818)	loss 1.2918 (1.3784)	grad_norm 0.4363 (0.4935)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:31:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:06:54 lr 0.000001	 wd 0.0000	time 0.2194 (0.2762)	loss 1.4202 (1.3766)	grad_norm 0.5333 (0.4958)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:31:45 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:06:23 lr 0.000001	 wd 0.0000	time 0.2071 (0.2736)	loss 1.5279 (1.3764)	grad_norm 0.5155 (0.4977)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:32:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:05:54 lr 0.000001	 wd 0.0000	time 0.1932 (0.2726)	loss 1.4934 (1.3763)	grad_norm 0.4563 (0.5025)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:32:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:05:24 lr 0.000001	 wd 0.0000	time 0.2192 (0.2696)	loss 1.5011 (1.3735)	grad_norm 0.4191 (0.5007)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:32:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:04:53 lr 0.000001	 wd 0.0000	time 0.2643 (0.2668)	loss 1.2801 (1.3740)	grad_norm 0.4508 (0.4997)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:33:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:04:25 lr 0.000001	 wd 0.0000	time 0.2640 (0.2655)	loss 1.5507 (1.3736)	grad_norm 0.4490 (0.5016)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:33:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:03:58 lr 0.000001	 wd 0.0000	time 0.2415 (0.2649)	loss 1.5774 (1.3715)	grad_norm 0.6661 (0.5018)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:34:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:03:31 lr 0.000001	 wd 0.0000	time 0.2361 (0.2631)	loss 1.3964 (1.3692)	grad_norm 0.5163 (0.5031)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:34:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:03 lr 0.000001	 wd 0.0000	time 0.2350 (0.2613)	loss 1.5158 (1.3697)	grad_norm 0.4408 (0.5034)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:34:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:02:36 lr 0.000001	 wd 0.0000	time 0.2573 (0.2606)	loss 1.0864 (1.3712)	grad_norm 0.4444 (0.5021)	loss_scale 2048.0000 (2048.0000)	mem 7984MB
[2024-06-29 22:35:25 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:10 lr 0.000001	 wd 0.0000	time 0.2593 (0.2604)	loss 1.0564 (1.3700)	grad_norm 0.4537 (0.5014)	loss_scale 4096.0000 (2117.5972)	mem 7984MB
[2024-06-29 22:35:48 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:44 lr 0.000001	 wd 0.0000	time 0.2236 (0.2592)	loss 1.7160 (1.3693)	grad_norm 0.4784 (0.5013)	loss_scale 4096.0000 (2211.7620)	mem 7984MB
[2024-06-29 22:36:11 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:17 lr 0.000001	 wd 0.0000	time 0.2404 (0.2579)	loss 1.0832 (1.3700)	grad_norm 0.5222 (0.5006)	loss_scale 4096.0000 (2297.3703)	mem 7984MB
[2024-06-29 22:36:36 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:52 lr 0.000001	 wd 0.0000	time 0.2519 (0.2574)	loss 1.4572 (1.3701)	grad_norm 0.5570 (0.5012)	loss_scale 4096.0000 (2375.5376)	mem 7984MB
[2024-06-29 22:37:01 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:26 lr 0.000001	 wd 0.0000	time 0.2337 (0.2572)	loss 1.3805 (1.3697)	grad_norm 0.4490 (0.5042)	loss_scale 4096.0000 (2447.1937)	mem 7984MB
[2024-06-29 22:37:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1976 (0.2552)	loss 1.5097 (1.3680)	grad_norm 0.4921 (0.5044)	loss_scale 4096.0000 (2513.1196)	mem 7984MB
[2024-06-29 22:37:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 27 training takes 0:10:48
[2024-06-29 22:37:50 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 18.260 (18.260)	Loss 0.4185 (0.4185)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 22:38:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.046 Acc@5 97.284
[2024-06-29 22:38:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-06-29 22:38:10 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 22:38:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][0/2502]	eta 23:03:21 lr 0.000001	 wd 0.0000	time 33.1740 (33.1740)	loss 1.2914 (1.2914)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:39:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:22:16 lr 0.000001	 wd 0.0000	time 0.2303 (0.5562)	loss 1.0736 (1.3489)	grad_norm 0.5075 (0.4975)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:39:28 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:14:57 lr 0.000001	 wd 0.0000	time 0.2486 (0.3897)	loss 1.0774 (1.3522)	grad_norm 0.4665 (0.5259)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:39:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:12:41 lr 0.000001	 wd 0.0000	time 0.2640 (0.3460)	loss 1.6525 (1.3622)	grad_norm 0.4969 (0.5156)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:40:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:11:08 lr 0.000001	 wd 0.0000	time 0.2195 (0.3180)	loss 1.3873 (1.3668)	grad_norm 0.4644 (0.5161)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:40:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:10:00 lr 0.000001	 wd 0.0000	time 0.2357 (0.2998)	loss 1.5750 (1.3691)	grad_norm 0.4775 (0.5101)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:41:03 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:09:06 lr 0.000001	 wd 0.0000	time 0.2076 (0.2875)	loss 1.3876 (1.3691)	grad_norm 0.4839 (0.5148)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:41:27 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:08:28 lr 0.000001	 wd 0.0000	time 0.2527 (0.2819)	loss 1.0336 (1.3710)	grad_norm 0.4651 (0.5115)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:41:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:07:53 lr 0.000001	 wd 0.0000	time 0.2227 (0.2784)	loss 1.6190 (1.3682)	grad_norm 0.5576 (0.5081)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:42:16 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:07:17 lr 0.000001	 wd 0.0000	time 0.1914 (0.2729)	loss 1.5351 (1.3688)	grad_norm 0.4702 (0.5092)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:42:39 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:06:43 lr 0.000001	 wd 0.0000	time 0.2312 (0.2687)	loss 1.5656 (1.3714)	grad_norm 0.4706 (0.5116)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:43:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:06:14 lr 0.000001	 wd 0.0000	time 0.2385 (0.2670)	loss 0.9527 (1.3678)	grad_norm 0.4643 (0.5098)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:43:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:05:46 lr 0.000001	 wd 0.0000	time 0.2506 (0.2658)	loss 1.2188 (1.3681)	grad_norm 0.4939 (0.5090)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:43:52 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:05:16 lr 0.000001	 wd 0.0000	time 0.2111 (0.2631)	loss 1.5828 (1.3649)	grad_norm 0.4931 (0.5070)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:44:15 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:04:47 lr 0.000001	 wd 0.0000	time 0.2097 (0.2607)	loss 0.9353 (1.3660)	grad_norm 0.4463 (0.5071)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:44:40 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:04:20 lr 0.000001	 wd 0.0000	time 0.2329 (0.2597)	loss 1.4949 (1.3645)	grad_norm 0.4793 (0.5074)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:45:06 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:03:54 lr 0.000001	 wd 0.0000	time 0.2385 (0.2600)	loss 1.1843 (1.3640)	grad_norm 0.4762 (0.5066)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:45:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:03:27 lr 0.000001	 wd 0.0000	time 0.2273 (0.2584)	loss 1.4206 (1.3654)	grad_norm 0.4668 (0.5063)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:45:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:00 lr 0.000001	 wd 0.0000	time 0.2032 (0.2569)	loss 1.3871 (1.3652)	grad_norm 0.5521 (0.5057)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:46:17 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:02:34 lr 0.000001	 wd 0.0000	time 0.2452 (0.2562)	loss 1.5467 (1.3641)	grad_norm 0.4801 (0.5051)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:46:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:08 lr 0.000001	 wd 0.0000	time 0.2544 (0.2559)	loss 0.8901 (1.3640)	grad_norm 0.4632 (0.5055)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:47:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:42 lr 0.000001	 wd 0.0000	time 0.2280 (0.2549)	loss 1.0929 (1.3643)	grad_norm 0.4341 (0.5044)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:47:29 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:16 lr 0.000001	 wd 0.0000	time 0.2210 (0.2538)	loss 1.5410 (1.3631)	grad_norm 0.4747 (0.5042)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:47:53 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:51 lr 0.000001	 wd 0.0000	time 0.2606 (0.2534)	loss 1.0871 (1.3632)	grad_norm 0.4927 (0.5044)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:48:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:25 lr 0.000001	 wd 0.0000	time 0.2257 (0.2532)	loss 1.5848 (1.3617)	grad_norm 0.4575 (0.5040)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:48:38 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1693 (0.2512)	loss 1.2061 (1.3609)	grad_norm 0.4967 (0.5031)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:48:47 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 28 training takes 0:10:37
[2024-06-29 22:49:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 18.024 (18.024)	Loss 0.4185 (0.4185)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 22:49:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.060 Acc@5 97.282
[2024-06-29 22:49:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 22:49:22 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 22:49:59 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][0/2502]	eta 1 day, 1:06:59 lr 0.000001	 wd 0.0000	time 36.1391 (36.1391)	loss 1.4131 (1.4131)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:50:21 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:23:22 lr 0.000001	 wd 0.0000	time 0.2162 (0.5837)	loss 1.4754 (1.3434)	grad_norm 0.4567 (0.4949)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:50:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:15:29 lr 0.000001	 wd 0.0000	time 0.2286 (0.4039)	loss 1.1537 (1.3584)	grad_norm 0.5330 (0.5035)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:51:09 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:12:56 lr 0.000001	 wd 0.0000	time 0.2472 (0.3528)	loss 1.5020 (1.3570)	grad_norm 0.4791 (0.4994)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:51:32 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:11:21 lr 0.000001	 wd 0.0000	time 0.2019 (0.3243)	loss 1.1875 (1.3655)	grad_norm 0.4855 (0.5000)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:51:55 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:10:09 lr 0.000001	 wd 0.0000	time 0.1885 (0.3045)	loss 0.9494 (1.3671)	grad_norm 0.4410 (0.4987)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:52:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:09:15 lr 0.000000	 wd 0.0000	time 0.2348 (0.2920)	loss 1.3543 (1.3679)	grad_norm 0.4219 (0.4972)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:52:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:08:33 lr 0.000000	 wd 0.0000	time 0.3004 (0.2851)	loss 1.5924 (1.3686)	grad_norm 0.5240 (0.4945)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:53:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:07:58 lr 0.000000	 wd 0.0000	time 0.2154 (0.2814)	loss 0.8033 (1.3696)	grad_norm 0.4719 (0.5037)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:53:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:07:21 lr 0.000000	 wd 0.0000	time 0.2140 (0.2758)	loss 1.4240 (1.3692)	grad_norm 0.5066 (0.5054)	loss_scale 4096.0000 (4096.0000)	mem 7984MB
[2024-06-29 22:53:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:06:47 lr 0.000000	 wd 0.0000	time 0.2155 (0.2713)	loss 1.3299 (1.3690)	grad_norm 0.5095 (0.5032)	loss_scale 8192.0000 (4390.6174)	mem 7984MB
[2024-06-29 22:54:18 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:06:16 lr 0.000000	 wd 0.0000	time 0.2984 (0.2685)	loss 1.4699 (1.3695)	grad_norm 0.4253 (0.5045)	loss_scale 8192.0000 (4735.8837)	mem 7984MB
[2024-06-29 22:54:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:05:48 lr 0.000000	 wd 0.0000	time 0.2627 (0.2674)	loss 1.2753 (1.3665)	grad_norm 0.4644 (0.5082)	loss_scale 8192.0000 (5023.6536)	mem 7984MB
[2024-06-29 22:55:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:05:18 lr 0.000000	 wd 0.0000	time 0.2376 (0.2647)	loss 1.5526 (1.3674)	grad_norm 0.4976 (0.5097)	loss_scale 8192.0000 (5267.1852)	mem 7984MB
[2024-06-29 22:55:30 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:04:49 lr 0.000000	 wd 0.0000	time 0.1939 (0.2623)	loss 1.7650 (1.3663)	grad_norm 0.4591 (nan)	loss_scale 4096.0000 (5201.1306)	mem 7984MB
[2024-06-29 22:55:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:04:21 lr 0.000000	 wd 0.0000	time 0.2520 (0.2606)	loss 1.2890 (1.3675)	grad_norm 0.4870 (nan)	loss_scale 4096.0000 (5127.5043)	mem 7984MB
[2024-06-29 22:56:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:03:55 lr 0.000000	 wd 0.0000	time 0.2507 (0.2606)	loss 1.4875 (1.3689)	grad_norm 0.4535 (nan)	loss_scale 4096.0000 (5063.0756)	mem 7984MB
[2024-06-29 22:56:43 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:03:27 lr 0.000000	 wd 0.0000	time 0.2459 (0.2593)	loss 1.3107 (1.3658)	grad_norm 0.6997 (nan)	loss_scale 4096.0000 (5006.2222)	mem 7984MB
[2024-06-29 22:57:07 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:01 lr 0.000000	 wd 0.0000	time 0.2049 (0.2579)	loss 1.1707 (1.3674)	grad_norm 0.5386 (nan)	loss_scale 4096.0000 (4955.6824)	mem 7984MB
[2024-06-29 22:57:31 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:02:34 lr 0.000000	 wd 0.0000	time 0.2646 (0.2570)	loss 1.2365 (1.3671)	grad_norm 0.5462 (nan)	loss_scale 4096.0000 (4910.4598)	mem 7984MB
[2024-06-29 22:57:57 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:08 lr 0.000000	 wd 0.0000	time 0.2061 (0.2570)	loss 1.5011 (1.3672)	grad_norm 0.5752 (nan)	loss_scale 4096.0000 (4869.7571)	mem 7984MB
[2024-06-29 22:58:20 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:42 lr 0.000000	 wd 0.0000	time 0.2257 (0.2560)	loss 1.4995 (1.3677)	grad_norm 0.4712 (nan)	loss_scale 4096.0000 (4832.9291)	mem 7984MB
[2024-06-29 22:58:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:17 lr 0.000000	 wd 0.0000	time 0.2272 (0.2551)	loss 1.4169 (1.3693)	grad_norm 0.5551 (nan)	loss_scale 4096.0000 (4799.4475)	mem 7984MB
[2024-06-29 22:59:08 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:51 lr 0.000000	 wd 0.0000	time 0.2488 (0.2544)	loss 1.4227 (1.3705)	grad_norm 0.4604 (nan)	loss_scale 4096.0000 (4768.8761)	mem 7984MB
[2024-06-29 22:59:34 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:25 lr 0.000000	 wd 0.0000	time 0.2436 (0.2546)	loss 1.4087 (1.3711)	grad_norm 0.4765 (nan)	loss_scale 4096.0000 (4740.8513)	mem 7984MB
[2024-06-29 22:59:54 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0000	time 0.1802 (0.2527)	loss 1.5699 (1.3703)	grad_norm 0.5882 (nan)	loss_scale 4096.0000 (4715.0676)	mem 7984MB
[2024-06-29 23:00:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 249): INFO EPOCH 29 training takes 0:10:41
[2024-06-29 23:00:04 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_29.pth saving......
[2024-06-29 23:00:05 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_29.pth saved !!!
[2024-06-29 23:00:23 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 289): INFO Test: [0/98]	Time 17.941 (17.941)	Loss 0.4185 (0.4185)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7984MB
[2024-06-29 23:00:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 296): INFO  * Acc@1 84.064 Acc@5 97.278
[2024-06-29 23:00:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-06-29 23:00:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 182): INFO Max accuracy: 84.06%
[2024-06-29 23:00:42 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saving......
[2024-06-29 23:00:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune/adapter_swin_b_22kto1k_efficient_finetune/ckpt_epoch_best.pth saved !!!
[2024-06-29 23:00:44 adapter_swin_base_patch4_window7_224_22kto1k_efficient_finetune] (main.py 189): INFO Training time 5:47:39
