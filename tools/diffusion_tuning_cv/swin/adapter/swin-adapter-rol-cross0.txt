[2024-07-09 19:21:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/config.json
[2024-07-09 19:21:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-09 19:21:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/swin/diffusion_ft_adapter_swin_base_patch4_window7_224_22kto1k_sequence_cross_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-09 19:21:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 108): INFO Creating model:adapter_swin_diffusion_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0
[2024-07-09 19:21:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 110): INFO Adapter_SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-09 19:21:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 113): INFO number of params: 1093224
[2024-07-09 19:21:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-09 19:21:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0, ignoring auto resume
[2024-07-09 19:21:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-07-09 19:21:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-09 19:21:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.adapter.down.weight', 'layers.0.blocks.0.adapter.down.bias', 'layers.0.blocks.0.adapter.up.weight', 'layers.0.blocks.0.adapter.up.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.adapter.down.weight', 'layers.0.blocks.1.adapter.down.bias', 'layers.0.blocks.1.adapter.up.weight', 'layers.0.blocks.1.adapter.up.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.adapter.down.weight', 'layers.1.blocks.0.adapter.down.bias', 'layers.1.blocks.0.adapter.up.weight', 'layers.1.blocks.0.adapter.up.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.adapter.down.weight', 'layers.1.blocks.1.adapter.down.bias', 'layers.1.blocks.1.adapter.up.weight', 'layers.1.blocks.1.adapter.up.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.adapter.down.weight', 'layers.2.blocks.0.adapter.down.bias', 'layers.2.blocks.0.adapter.up.weight', 'layers.2.blocks.0.adapter.up.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.adapter.down.weight', 'layers.2.blocks.1.adapter.down.bias', 'layers.2.blocks.1.adapter.up.weight', 'layers.2.blocks.1.adapter.up.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.adapter.down.weight', 'layers.2.blocks.2.adapter.down.bias', 'layers.2.blocks.2.adapter.up.weight', 'layers.2.blocks.2.adapter.up.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.adapter.down.weight', 'layers.2.blocks.3.adapter.down.bias', 'layers.2.blocks.3.adapter.up.weight', 'layers.2.blocks.3.adapter.up.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.adapter.down.weight', 'layers.2.blocks.4.adapter.down.bias', 'layers.2.blocks.4.adapter.up.weight', 'layers.2.blocks.4.adapter.up.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.adapter.down.weight', 'layers.2.blocks.5.adapter.down.bias', 'layers.2.blocks.5.adapter.up.weight', 'layers.2.blocks.5.adapter.up.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.adapter.down.weight', 'layers.2.blocks.6.adapter.down.bias', 'layers.2.blocks.6.adapter.up.weight', 'layers.2.blocks.6.adapter.up.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.adapter.down.weight', 'layers.2.blocks.7.adapter.down.bias', 'layers.2.blocks.7.adapter.up.weight', 'layers.2.blocks.7.adapter.up.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.adapter.down.weight', 'layers.2.blocks.8.adapter.down.bias', 'layers.2.blocks.8.adapter.up.weight', 'layers.2.blocks.8.adapter.up.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.adapter.down.weight', 'layers.2.blocks.9.adapter.down.bias', 'layers.2.blocks.9.adapter.up.weight', 'layers.2.blocks.9.adapter.up.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.adapter.down.weight', 'layers.2.blocks.10.adapter.down.bias', 'layers.2.blocks.10.adapter.up.weight', 'layers.2.blocks.10.adapter.up.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.adapter.down.weight', 'layers.2.blocks.11.adapter.down.bias', 'layers.2.blocks.11.adapter.up.weight', 'layers.2.blocks.11.adapter.up.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.adapter.down.weight', 'layers.2.blocks.12.adapter.down.bias', 'layers.2.blocks.12.adapter.up.weight', 'layers.2.blocks.12.adapter.up.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.adapter.down.weight', 'layers.2.blocks.13.adapter.down.bias', 'layers.2.blocks.13.adapter.up.weight', 'layers.2.blocks.13.adapter.up.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.adapter.down.weight', 'layers.2.blocks.14.adapter.down.bias', 'layers.2.blocks.14.adapter.up.weight', 'layers.2.blocks.14.adapter.up.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.adapter.down.weight', 'layers.2.blocks.15.adapter.down.bias', 'layers.2.blocks.15.adapter.up.weight', 'layers.2.blocks.15.adapter.up.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.adapter.down.weight', 'layers.2.blocks.16.adapter.down.bias', 'layers.2.blocks.16.adapter.up.weight', 'layers.2.blocks.16.adapter.up.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.adapter.down.weight', 'layers.2.blocks.17.adapter.down.bias', 'layers.2.blocks.17.adapter.up.weight', 'layers.2.blocks.17.adapter.up.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.adapter.down.weight', 'layers.3.blocks.0.adapter.down.bias', 'layers.3.blocks.0.adapter.up.weight', 'layers.3.blocks.0.adapter.up.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.adapter.down.weight', 'layers.3.blocks.1.adapter.down.bias', 'layers.3.blocks.1.adapter.up.weight', 'layers.3.blocks.1.adapter.up.bias'], unexpected_keys=[])
[2024-07-09 19:21:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth'
[2024-07-09 19:22:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 72.552 (72.552)	Loss 0.3616 (0.3616)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 1477MB
[2024-07-09 19:22:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 82.636 Acc@5 96.604
[2024-07-09 19:22:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 82.6%
[2024-07-09 19:22:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 168): INFO Start training
[2024-07-09 19:26:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/config.json
[2024-07-09 19:26:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-09 19:26:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/swin/diffusion_ft_adapter_swin_base_patch4_window7_224_22kto1k_sequence_cross_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-09 19:26:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 108): INFO Creating model:adapter_swin_diffusion_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0
[2024-07-09 19:26:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 110): INFO Adapter_SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-09 19:26:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 113): INFO number of params: 1093224
[2024-07-09 19:26:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-09 19:26:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0, ignoring auto resume
[2024-07-09 19:26:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-07-09 19:26:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-09 19:26:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.adapter.down.weight', 'layers.0.blocks.0.adapter.down.bias', 'layers.0.blocks.0.adapter.up.weight', 'layers.0.blocks.0.adapter.up.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.adapter.down.weight', 'layers.0.blocks.1.adapter.down.bias', 'layers.0.blocks.1.adapter.up.weight', 'layers.0.blocks.1.adapter.up.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.adapter.down.weight', 'layers.1.blocks.0.adapter.down.bias', 'layers.1.blocks.0.adapter.up.weight', 'layers.1.blocks.0.adapter.up.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.adapter.down.weight', 'layers.1.blocks.1.adapter.down.bias', 'layers.1.blocks.1.adapter.up.weight', 'layers.1.blocks.1.adapter.up.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.adapter.down.weight', 'layers.2.blocks.0.adapter.down.bias', 'layers.2.blocks.0.adapter.up.weight', 'layers.2.blocks.0.adapter.up.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.adapter.down.weight', 'layers.2.blocks.1.adapter.down.bias', 'layers.2.blocks.1.adapter.up.weight', 'layers.2.blocks.1.adapter.up.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.adapter.down.weight', 'layers.2.blocks.2.adapter.down.bias', 'layers.2.blocks.2.adapter.up.weight', 'layers.2.blocks.2.adapter.up.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.adapter.down.weight', 'layers.2.blocks.3.adapter.down.bias', 'layers.2.blocks.3.adapter.up.weight', 'layers.2.blocks.3.adapter.up.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.adapter.down.weight', 'layers.2.blocks.4.adapter.down.bias', 'layers.2.blocks.4.adapter.up.weight', 'layers.2.blocks.4.adapter.up.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.adapter.down.weight', 'layers.2.blocks.5.adapter.down.bias', 'layers.2.blocks.5.adapter.up.weight', 'layers.2.blocks.5.adapter.up.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.adapter.down.weight', 'layers.2.blocks.6.adapter.down.bias', 'layers.2.blocks.6.adapter.up.weight', 'layers.2.blocks.6.adapter.up.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.adapter.down.weight', 'layers.2.blocks.7.adapter.down.bias', 'layers.2.blocks.7.adapter.up.weight', 'layers.2.blocks.7.adapter.up.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.adapter.down.weight', 'layers.2.blocks.8.adapter.down.bias', 'layers.2.blocks.8.adapter.up.weight', 'layers.2.blocks.8.adapter.up.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.adapter.down.weight', 'layers.2.blocks.9.adapter.down.bias', 'layers.2.blocks.9.adapter.up.weight', 'layers.2.blocks.9.adapter.up.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.adapter.down.weight', 'layers.2.blocks.10.adapter.down.bias', 'layers.2.blocks.10.adapter.up.weight', 'layers.2.blocks.10.adapter.up.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.adapter.down.weight', 'layers.2.blocks.11.adapter.down.bias', 'layers.2.blocks.11.adapter.up.weight', 'layers.2.blocks.11.adapter.up.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.adapter.down.weight', 'layers.2.blocks.12.adapter.down.bias', 'layers.2.blocks.12.adapter.up.weight', 'layers.2.blocks.12.adapter.up.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.adapter.down.weight', 'layers.2.blocks.13.adapter.down.bias', 'layers.2.blocks.13.adapter.up.weight', 'layers.2.blocks.13.adapter.up.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.adapter.down.weight', 'layers.2.blocks.14.adapter.down.bias', 'layers.2.blocks.14.adapter.up.weight', 'layers.2.blocks.14.adapter.up.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.adapter.down.weight', 'layers.2.blocks.15.adapter.down.bias', 'layers.2.blocks.15.adapter.up.weight', 'layers.2.blocks.15.adapter.up.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.adapter.down.weight', 'layers.2.blocks.16.adapter.down.bias', 'layers.2.blocks.16.adapter.up.weight', 'layers.2.blocks.16.adapter.up.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.adapter.down.weight', 'layers.2.blocks.17.adapter.down.bias', 'layers.2.blocks.17.adapter.up.weight', 'layers.2.blocks.17.adapter.up.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.adapter.down.weight', 'layers.3.blocks.0.adapter.down.bias', 'layers.3.blocks.0.adapter.up.weight', 'layers.3.blocks.0.adapter.up.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.adapter.down.weight', 'layers.3.blocks.1.adapter.down.bias', 'layers.3.blocks.1.adapter.up.weight', 'layers.3.blocks.1.adapter.up.bias'], unexpected_keys=[])
[2024-07-09 19:26:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth'
[2024-07-09 19:30:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/config.json
[2024-07-09 19:30:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_part0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-09 19:30:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/swin/diffusion_ft_adapter_swin_base_patch4_window7_224_22kto1k_sequence_cross_process0.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-09 19:31:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 108): INFO Creating model:adapter_swin_diffusion_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0
[2024-07-09 19:31:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 110): INFO Adapter_SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-09 19:31:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 113): INFO number of params: 2779464
[2024-07-09 19:31:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-09 19:31:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0, ignoring auto resume
[2024-07-09 19:31:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth for fine-tuning......
[2024-07-09 19:31:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-09 19:31:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.0.adapter.down.weight', 'layers.0.blocks.0.adapter.down.bias', 'layers.0.blocks.0.adapter.up.weight', 'layers.0.blocks.0.adapter.up.bias', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.0.blocks.1.adapter.down.weight', 'layers.0.blocks.1.adapter.down.bias', 'layers.0.blocks.1.adapter.up.weight', 'layers.0.blocks.1.adapter.up.bias', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.adapter.down.weight', 'layers.1.blocks.0.adapter.down.bias', 'layers.1.blocks.0.adapter.up.weight', 'layers.1.blocks.0.adapter.up.bias', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.1.blocks.1.adapter.down.weight', 'layers.1.blocks.1.adapter.down.bias', 'layers.1.blocks.1.adapter.up.weight', 'layers.1.blocks.1.adapter.up.bias', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.adapter.down.weight', 'layers.2.blocks.0.adapter.down.bias', 'layers.2.blocks.0.adapter.up.weight', 'layers.2.blocks.0.adapter.up.bias', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.1.adapter.down.weight', 'layers.2.blocks.1.adapter.down.bias', 'layers.2.blocks.1.adapter.up.weight', 'layers.2.blocks.1.adapter.up.bias', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.2.adapter.down.weight', 'layers.2.blocks.2.adapter.down.bias', 'layers.2.blocks.2.adapter.up.weight', 'layers.2.blocks.2.adapter.up.bias', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.3.adapter.down.weight', 'layers.2.blocks.3.adapter.down.bias', 'layers.2.blocks.3.adapter.up.weight', 'layers.2.blocks.3.adapter.up.bias', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.4.adapter.down.weight', 'layers.2.blocks.4.adapter.down.bias', 'layers.2.blocks.4.adapter.up.weight', 'layers.2.blocks.4.adapter.up.bias', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.5.adapter.down.weight', 'layers.2.blocks.5.adapter.down.bias', 'layers.2.blocks.5.adapter.up.weight', 'layers.2.blocks.5.adapter.up.bias', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.6.adapter.down.weight', 'layers.2.blocks.6.adapter.down.bias', 'layers.2.blocks.6.adapter.up.weight', 'layers.2.blocks.6.adapter.up.bias', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.7.adapter.down.weight', 'layers.2.blocks.7.adapter.down.bias', 'layers.2.blocks.7.adapter.up.weight', 'layers.2.blocks.7.adapter.up.bias', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.8.adapter.down.weight', 'layers.2.blocks.8.adapter.down.bias', 'layers.2.blocks.8.adapter.up.weight', 'layers.2.blocks.8.adapter.up.bias', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.9.adapter.down.weight', 'layers.2.blocks.9.adapter.down.bias', 'layers.2.blocks.9.adapter.up.weight', 'layers.2.blocks.9.adapter.up.bias', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.10.adapter.down.weight', 'layers.2.blocks.10.adapter.down.bias', 'layers.2.blocks.10.adapter.up.weight', 'layers.2.blocks.10.adapter.up.bias', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.11.adapter.down.weight', 'layers.2.blocks.11.adapter.down.bias', 'layers.2.blocks.11.adapter.up.weight', 'layers.2.blocks.11.adapter.up.bias', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.12.adapter.down.weight', 'layers.2.blocks.12.adapter.down.bias', 'layers.2.blocks.12.adapter.up.weight', 'layers.2.blocks.12.adapter.up.bias', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.13.adapter.down.weight', 'layers.2.blocks.13.adapter.down.bias', 'layers.2.blocks.13.adapter.up.weight', 'layers.2.blocks.13.adapter.up.bias', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.14.adapter.down.weight', 'layers.2.blocks.14.adapter.down.bias', 'layers.2.blocks.14.adapter.up.weight', 'layers.2.blocks.14.adapter.up.bias', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.15.adapter.down.weight', 'layers.2.blocks.15.adapter.down.bias', 'layers.2.blocks.15.adapter.up.weight', 'layers.2.blocks.15.adapter.up.bias', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.16.adapter.down.weight', 'layers.2.blocks.16.adapter.down.bias', 'layers.2.blocks.16.adapter.up.weight', 'layers.2.blocks.16.adapter.up.bias', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.2.blocks.17.adapter.down.weight', 'layers.2.blocks.17.adapter.down.bias', 'layers.2.blocks.17.adapter.up.weight', 'layers.2.blocks.17.adapter.up.bias', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.0.adapter.down.weight', 'layers.3.blocks.0.adapter.down.bias', 'layers.3.blocks.0.adapter.up.weight', 'layers.3.blocks.0.adapter.up.bias', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.1.adapter.down.weight', 'layers.3.blocks.1.adapter.down.bias', 'layers.3.blocks.1.adapter.up.weight', 'layers.3.blocks.1.adapter.up.bias'], unexpected_keys=[])
[2024-07-09 19:31:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/swin-b/swin_base_patch4_window7_224_22k.pth'
[2024-07-09 19:32:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 80.776 (80.776)	Loss 0.3616 (0.3616)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 1482MB
[2024-07-09 19:32:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 82.636 Acc@5 96.604
[2024-07-09 19:32:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 82.6%
[2024-07-09 19:32:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 168): INFO Start training
[2024-07-09 19:33:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][0/2502]	eta 23:06:44 lr 0.000000	 wd 0.0000	time 33.2550 (33.2550)	loss 1.7768 (1.7768)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 7702MB
[2024-07-09 19:33:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:26:08 lr 0.000000	 wd 0.0000	time 0.2035 (0.6529)	loss 1.5633 (1.5355)	grad_norm 0.5076 (nan)	loss_scale 32768.0000 (35687.9208)	mem 7723MB
[2024-07-09 19:34:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:16:53 lr 0.000001	 wd 0.0000	time 0.2294 (0.4403)	loss 1.5631 (1.5206)	grad_norm 0.5797 (nan)	loss_scale 16384.0000 (28040.2786)	mem 7723MB
[2024-07-09 19:34:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:28 lr 0.000001	 wd 0.0000	time 0.1951 (0.3671)	loss 1.5457 (1.4986)	grad_norm 0.9654 (nan)	loss_scale 8192.0000 (23296.8505)	mem 7723MB
[2024-07-09 19:34:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:11:47 lr 0.000001	 wd 0.0000	time 0.2287 (0.3365)	loss 2.0063 (1.5009)	grad_norm 0.5969 (nan)	loss_scale 8192.0000 (19530.0549)	mem 7723MB
[2024-07-09 19:35:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:10:27 lr 0.000002	 wd 0.0000	time 0.2424 (0.3133)	loss 1.6721 (1.4994)	grad_norm 0.5229 (nan)	loss_scale 8192.0000 (17266.9701)	mem 7723MB
[2024-07-09 19:35:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:09:24 lr 0.000002	 wd 0.0000	time 0.2149 (0.2970)	loss 1.2672 (1.4992)	grad_norm 0.5995 (nan)	loss_scale 8192.0000 (15756.9917)	mem 7723MB
[2024-07-09 19:36:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:08:33 lr 0.000002	 wd 0.0000	time 0.2143 (0.2849)	loss 1.5720 (1.4942)	grad_norm 0.5066 (nan)	loss_scale 8192.0000 (14677.8203)	mem 7723MB
[2024-07-09 19:36:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:07:56 lr 0.000003	 wd 0.0000	time 0.3007 (0.2799)	loss 1.6405 (1.4947)	grad_norm 0.5351 (nan)	loss_scale 4096.0000 (13663.5605)	mem 7723MB
[2024-07-09 19:36:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:07:31 lr 0.000003	 wd 0.0000	time 0.1938 (0.2820)	loss 1.7138 (1.4883)	grad_norm 0.6092 (nan)	loss_scale 2048.0000 (12388.0133)	mem 7723MB
[2024-07-09 19:37:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:06:54 lr 0.000003	 wd 0.0000	time 0.2348 (0.2763)	loss 1.4845 (1.4869)	grad_norm 0.5716 (nan)	loss_scale 2048.0000 (11355.0450)	mem 7723MB
[2024-07-09 19:37:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:06:20 lr 0.000004	 wd 0.0000	time 0.2332 (0.2715)	loss 1.6389 (1.4863)	grad_norm 0.4641 (nan)	loss_scale 2048.0000 (10509.7184)	mem 7723MB
[2024-07-09 19:38:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:05:50 lr 0.000004	 wd 0.0000	time 0.2358 (0.2689)	loss 1.6065 (1.4881)	grad_norm 2.9496 (nan)	loss_scale 2048.0000 (9805.1624)	mem 7723MB
[2024-07-09 19:38:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:05:21 lr 0.000004	 wd 0.0000	time 0.2285 (0.2678)	loss 1.5244 (1.4884)	grad_norm 0.5384 (nan)	loss_scale 2048.0000 (9208.9162)	mem 7723MB
[2024-07-09 19:38:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:04:51 lr 0.000005	 wd 0.0000	time 0.2066 (0.2643)	loss 1.6854 (1.4887)	grad_norm 0.5792 (nan)	loss_scale 2048.0000 (8697.7873)	mem 7723MB
[2024-07-09 19:39:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:04:21 lr 0.000005	 wd 0.0000	time 0.2359 (0.2611)	loss 1.5810 (1.4869)	grad_norm 0.5125 (nan)	loss_scale 2048.0000 (8254.7635)	mem 7723MB
[2024-07-09 19:39:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:03:54 lr 0.000005	 wd 0.0000	time 0.2590 (0.2595)	loss 1.7286 (1.4863)	grad_norm 0.5042 (nan)	loss_scale 2048.0000 (7867.0831)	mem 7723MB
[2024-07-09 19:40:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:03:27 lr 0.000005	 wd 0.0000	time 0.2095 (0.2588)	loss 1.5992 (1.4839)	grad_norm 0.5026 (nan)	loss_scale 2048.0000 (7524.9853)	mem 7723MB
[2024-07-09 19:40:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:00 lr 0.000006	 wd 0.0000	time 0.2341 (0.2567)	loss 1.2711 (1.4830)	grad_norm 0.4528 (nan)	loss_scale 2048.0000 (7220.8773)	mem 7723MB
[2024-07-09 19:40:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:33 lr 0.000006	 wd 0.0000	time 0.2301 (0.2549)	loss 1.7189 (1.4807)	grad_norm 0.4967 (nan)	loss_scale 2048.0000 (6948.7638)	mem 7723MB
[2024-07-09 19:41:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:07 lr 0.000006	 wd 0.0000	time 0.2253 (0.2538)	loss 1.6055 (1.4768)	grad_norm 0.4743 (nan)	loss_scale 2048.0000 (6703.8481)	mem 7723MB
[2024-07-09 19:41:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:41 lr 0.000007	 wd 0.0000	time 0.2154 (0.2535)	loss 1.4695 (1.4759)	grad_norm 1.0148 (nan)	loss_scale 2048.0000 (6482.2465)	mem 7723MB
[2024-07-09 19:41:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:16 lr 0.000007	 wd 0.0000	time 0.2144 (0.2522)	loss 1.6802 (1.4751)	grad_norm 0.5081 (nan)	loss_scale 2048.0000 (6280.7815)	mem 7723MB
[2024-07-09 19:42:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:50 lr 0.000007	 wd 0.0000	time 0.2297 (0.2510)	loss 1.5740 (1.4725)	grad_norm 0.4575 (nan)	loss_scale 2048.0000 (6096.8275)	mem 7723MB
[2024-07-09 19:42:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:25 lr 0.000008	 wd 0.0000	time 0.3048 (0.2500)	loss 1.5366 (1.4711)	grad_norm 0.4381 (nan)	loss_scale 2048.0000 (5928.1966)	mem 7723MB
[2024-07-09 19:43:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.1687 (0.2482)	loss 1.7053 (1.4699)	grad_norm 0.3964 (nan)	loss_scale 2048.0000 (5773.0508)	mem 7723MB
[2024-07-09 19:43:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 0 training takes 0:10:30
[2024-07-09 19:43:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_0.pth saving......
[2024-07-09 19:43:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_0.pth saved !!!
[2024-07-09 19:43:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 32.567 (32.567)	Loss 0.4497 (0.4497)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 7723MB
[2024-07-09 19:44:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 82.730 Acc@5 96.678
[2024-07-09 19:44:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 82.7%
[2024-07-09 19:44:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 82.73%
[2024-07-09 19:44:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 19:44:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 19:44:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][0/2502]	eta 11:02:16 lr 0.000008	 wd 0.0000	time 15.8817 (15.8817)	loss 1.2897 (1.2897)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:44:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:18:05 lr 0.000008	 wd 0.0000	time 0.2301 (0.4519)	loss 1.2196 (1.4783)	grad_norm 0.4122 (0.4518)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:45:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:12:53 lr 0.000009	 wd 0.0000	time 0.1926 (0.3359)	loss 1.4380 (1.4748)	grad_norm 0.5269 (0.4537)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:45:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:10:50 lr 0.000009	 wd 0.0000	time 0.2201 (0.2956)	loss 1.7919 (1.4528)	grad_norm 0.5189 (0.4606)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:45:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:09:36 lr 0.000009	 wd 0.0000	time 0.2077 (0.2744)	loss 1.0151 (1.4407)	grad_norm 0.4258 (0.4626)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:46:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:09:39 lr 0.000010	 wd 0.0000	time 0.2544 (0.2896)	loss 1.5923 (1.4383)	grad_norm 0.4627 (0.4590)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:46:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:08:54 lr 0.000010	 wd 0.0000	time 0.2070 (0.2811)	loss 1.4690 (1.4368)	grad_norm 0.4611 (0.4546)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:47:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:08:10 lr 0.000010	 wd 0.0000	time 0.2020 (0.2720)	loss 1.7200 (1.4346)	grad_norm 0.4525 (0.4536)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:47:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:07:31 lr 0.000011	 wd 0.0000	time 0.2183 (0.2652)	loss 1.6664 (1.4383)	grad_norm 0.4103 (0.4512)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:48:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:07:03 lr 0.000011	 wd 0.0000	time 0.2072 (0.2645)	loss 1.6295 (1.4348)	grad_norm 0.4250 (0.4501)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:48:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:06:31 lr 0.000011	 wd 0.0000	time 0.1929 (0.2608)	loss 1.7278 (1.4327)	grad_norm 0.4301 (0.4493)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:48:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:06:00 lr 0.000012	 wd 0.0000	time 0.1933 (0.2570)	loss 1.0865 (1.4314)	grad_norm 0.5015 (0.4493)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:49:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:05:30 lr 0.000012	 wd 0.0000	time 0.2206 (0.2537)	loss 1.4439 (1.4336)	grad_norm 0.3967 (0.4490)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:49:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:05:04 lr 0.000012	 wd 0.0000	time 0.1994 (0.2530)	loss 1.5992 (1.4366)	grad_norm 0.4011 (0.4479)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:49:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:04:38 lr 0.000012	 wd 0.0000	time 0.2134 (0.2529)	loss 1.4898 (1.4340)	grad_norm 0.4382 (0.4469)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:50:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:04:11 lr 0.000013	 wd 0.0000	time 0.2165 (0.2511)	loss 0.9894 (1.4318)	grad_norm 0.4048 (0.4476)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:50:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:03:44 lr 0.000013	 wd 0.0000	time 0.2362 (0.2492)	loss 0.9855 (1.4299)	grad_norm 0.4073 (0.4471)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:51:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:03:19 lr 0.000013	 wd 0.0000	time 0.2527 (0.2489)	loss 1.4017 (1.4287)	grad_norm 0.4529 (0.4497)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:51:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:02:54 lr 0.000014	 wd 0.0000	time 0.2113 (0.2492)	loss 1.5046 (1.4285)	grad_norm 0.3860 (0.4510)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:51:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:29 lr 0.000014	 wd 0.0000	time 0.2333 (0.2477)	loss 1.5003 (1.4291)	grad_norm 0.4610 (0.4509)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:52:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:03 lr 0.000014	 wd 0.0000	time 0.1914 (0.2464)	loss 1.4077 (1.4280)	grad_norm 0.4058 (0.4501)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:52:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:38 lr 0.000015	 wd 0.0000	time 0.2155 (0.2458)	loss 1.4553 (1.4287)	grad_norm 0.4205 (0.4500)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:53:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:14 lr 0.000015	 wd 0.0000	time 0.2190 (0.2457)	loss 1.2356 (1.4297)	grad_norm 0.4117 (0.4515)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:53:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:49 lr 0.000015	 wd 0.0000	time 0.1857 (0.2449)	loss 1.7051 (1.4299)	grad_norm 0.4998 (0.4508)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 19:53:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:24 lr 0.000016	 wd 0.0000	time 0.2077 (0.2442)	loss 1.5150 (1.4276)	grad_norm 0.3998 (0.4500)	loss_scale 4096.0000 (2129.8859)	mem 7723MB
[2024-07-09 19:54:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.1599 (0.2423)	loss 1.1285 (1.4287)	grad_norm 0.3844 (0.4497)	loss_scale 4096.0000 (2208.4990)	mem 7723MB
[2024-07-09 19:54:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 1 training takes 0:10:10
[2024-07-09 19:54:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 38.919 (38.919)	Loss 0.4409 (0.4409)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 7723MB
[2024-07-09 19:55:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.122 Acc@5 96.838
[2024-07-09 19:55:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.1%
[2024-07-09 19:55:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.12%
[2024-07-09 19:55:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 19:55:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 19:55:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][0/2502]	eta 10:28:23 lr 0.000016	 wd 0.0000	time 15.0692 (15.0692)	loss 1.6908 (1.6908)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:55:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:15:04 lr 0.000016	 wd 0.0000	time 0.2667 (0.3765)	loss 1.4900 (1.3915)	grad_norm 0.4093 (0.4285)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:56:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:13:27 lr 0.000017	 wd 0.0000	time 0.2240 (0.3506)	loss 1.4909 (1.4121)	grad_norm 0.3835 (0.4353)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:56:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:11:10 lr 0.000017	 wd 0.0000	time 0.2030 (0.3047)	loss 1.3205 (1.4230)	grad_norm 0.4210 (0.4374)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:56:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:09:53 lr 0.000017	 wd 0.0000	time 0.2093 (0.2823)	loss 1.6066 (1.4203)	grad_norm 0.4554 (0.4346)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:57:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:08:59 lr 0.000018	 wd 0.0000	time 0.2376 (0.2697)	loss 1.5874 (1.4190)	grad_norm 0.4122 (0.4387)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:57:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:08:29 lr 0.000018	 wd 0.0000	time 0.2157 (0.2679)	loss 1.3183 (1.4135)	grad_norm 0.4310 (0.4381)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:58:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:07:50 lr 0.000018	 wd 0.0000	time 0.2420 (0.2613)	loss 1.2827 (1.4153)	grad_norm 0.4428 (0.4378)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:58:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:07:15 lr 0.000019	 wd 0.0000	time 0.2210 (0.2557)	loss 1.4793 (1.4134)	grad_norm 0.3900 (0.4385)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:58:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:06:43 lr 0.000019	 wd 0.0000	time 0.2360 (0.2517)	loss 1.5860 (1.4178)	grad_norm 0.4768 (0.4451)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:59:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:06:17 lr 0.000019	 wd 0.0000	time 0.2274 (0.2511)	loss 1.4941 (1.4191)	grad_norm 0.4534 (0.4441)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 19:59:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:05:50 lr 0.000020	 wd 0.0000	time 0.1977 (0.2500)	loss 1.2339 (1.4184)	grad_norm 0.4153 (0.4450)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:00:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:05:22 lr 0.000020	 wd 0.0000	time 0.2036 (0.2475)	loss 1.2858 (1.4171)	grad_norm 0.4130 (0.4459)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:00:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:04:55 lr 0.000020	 wd 0.0000	time 0.2092 (0.2455)	loss 1.7768 (1.4199)	grad_norm 0.7731 (0.4455)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:00:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:04:30 lr 0.000020	 wd 0.0000	time 0.2156 (0.2451)	loss 1.4824 (1.4197)	grad_norm 0.4312 (0.4444)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:01:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:04:05 lr 0.000021	 wd 0.0000	time 0.2409 (0.2448)	loss 1.5712 (1.4174)	grad_norm 0.4179 (0.4432)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:01:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:03:39 lr 0.000021	 wd 0.0000	time 0.2125 (0.2433)	loss 1.3499 (1.4175)	grad_norm 0.5198 (0.4428)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:01:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:03:14 lr 0.000021	 wd 0.0000	time 0.2242 (0.2420)	loss 1.4581 (1.4169)	grad_norm 0.4173 (0.4415)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:02:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:02:49 lr 0.000022	 wd 0.0000	time 0.2481 (0.2416)	loss 1.3877 (1.4169)	grad_norm 0.4609 (0.4416)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:02:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:02:26 lr 0.000022	 wd 0.0000	time 0.2374 (0.2426)	loss 1.1047 (1.4152)	grad_norm 0.4028 (0.4412)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:03:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:01 lr 0.000022	 wd 0.0000	time 0.2073 (0.2416)	loss 1.1434 (1.4132)	grad_norm 0.4727 (0.4415)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:03:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:36 lr 0.000023	 wd 0.0000	time 0.2037 (0.2407)	loss 1.4940 (1.4134)	grad_norm 0.4146 (0.4406)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:03:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:12 lr 0.000023	 wd 0.0000	time 0.2178 (0.2403)	loss 1.5448 (1.4132)	grad_norm 0.4120 (0.4402)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:04:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:48 lr 0.000023	 wd 0.0000	time 0.2347 (0.2409)	loss 1.6196 (1.4131)	grad_norm 0.4170 (0.4398)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:04:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:24 lr 0.000024	 wd 0.0000	time 0.2114 (0.2404)	loss 1.4857 (1.4118)	grad_norm 0.4135 (0.4401)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:05:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.1620 (0.2384)	loss 1.4767 (1.4115)	grad_norm 0.4052 (0.4399)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:05:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 2 training takes 0:10:00
[2024-07-09 20:05:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 43.734 (43.734)	Loss 0.4319 (0.4319)	Acc@1 91.406 (91.406)	Acc@5 98.242 (98.242)	Mem 7723MB
[2024-07-09 20:06:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.340 Acc@5 96.960
[2024-07-09 20:06:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.3%
[2024-07-09 20:06:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.34%
[2024-07-09 20:06:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 20:06:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 20:06:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][0/2502]	eta 10:53:51 lr 0.000024	 wd 0.0000	time 15.6800 (15.6800)	loss 1.1069 (1.1069)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:06:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:14:47 lr 0.000024	 wd 0.0000	time 0.1947 (0.3695)	loss 1.7004 (1.3930)	grad_norm 0.4333 (0.4441)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:07:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:11:27 lr 0.000025	 wd 0.0000	time 0.2579 (0.2986)	loss 1.5395 (1.4068)	grad_norm 0.3934 (0.4361)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:07:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:10:59 lr 0.000025	 wd 0.0000	time 0.2071 (0.2996)	loss 1.4451 (1.3943)	grad_norm 0.4006 (0.4362)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:07:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:09:44 lr 0.000025	 wd 0.0000	time 0.1933 (0.2779)	loss 1.5713 (1.4006)	grad_norm 0.4524 (0.4335)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:08:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:08:51 lr 0.000026	 wd 0.0000	time 0.1803 (0.2653)	loss 1.4475 (1.3978)	grad_norm 0.5742 (0.4331)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:08:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:08:12 lr 0.000026	 wd 0.0000	time 0.2746 (0.2588)	loss 1.4902 (1.3970)	grad_norm 0.4167 (0.4321)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:09:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:07:45 lr 0.000026	 wd 0.0000	time 0.2397 (0.2584)	loss 1.5363 (1.3986)	grad_norm 0.4478 (0.4347)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:09:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:07:12 lr 0.000027	 wd 0.0000	time 0.2081 (0.2543)	loss 1.1563 (1.3994)	grad_norm 0.4179 (0.4333)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:09:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:06:41 lr 0.000027	 wd 0.0000	time 0.2153 (0.2505)	loss 1.3903 (1.4007)	grad_norm 0.4186 (0.4349)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:10:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:06:11 lr 0.000027	 wd 0.0000	time 0.2559 (0.2473)	loss 1.6368 (1.4042)	grad_norm 0.3856 (0.4351)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:10:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:05:49 lr 0.000028	 wd 0.0000	time 0.2620 (0.2490)	loss 1.4507 (1.4065)	grad_norm 0.5659 (0.4351)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:11:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:05:21 lr 0.000028	 wd 0.0000	time 0.2138 (0.2467)	loss 1.3438 (1.4051)	grad_norm 0.4229 (0.4355)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:11:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:04:54 lr 0.000028	 wd 0.0000	time 0.2135 (0.2446)	loss 1.2519 (1.4078)	grad_norm 0.3998 (0.4359)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:11:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:04:27 lr 0.000028	 wd 0.0000	time 0.2239 (0.2427)	loss 1.2696 (1.4084)	grad_norm 0.4371 (0.4364)	loss_scale 8192.0000 (4388.3626)	mem 7723MB
[2024-07-09 20:12:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:04:03 lr 0.000029	 wd 0.0000	time 0.2589 (0.2426)	loss 1.4397 (1.4084)	grad_norm 0.4341 (0.4361)	loss_scale 8192.0000 (4641.7695)	mem 7723MB
[2024-07-09 20:12:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:03:38 lr 0.000029	 wd 0.0000	time 0.2137 (0.2427)	loss 1.4401 (1.4071)	grad_norm 0.4082 (0.4363)	loss_scale 8192.0000 (4863.5203)	mem 7723MB
[2024-07-09 20:12:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:03:13 lr 0.000029	 wd 0.0000	time 0.2214 (0.2414)	loss 1.0915 (1.4059)	grad_norm 0.4055 (0.4354)	loss_scale 8192.0000 (5059.1981)	mem 7723MB
[2024-07-09 20:13:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:02:48 lr 0.000030	 wd 0.0000	time 0.2121 (0.2403)	loss 1.6872 (1.4073)	grad_norm 0.4605 (0.4349)	loss_scale 8192.0000 (5233.1460)	mem 7723MB
[2024-07-09 20:13:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:02:24 lr 0.000030	 wd 0.0000	time 0.2486 (0.2404)	loss 1.5701 (1.4069)	grad_norm 0.4085 (0.4368)	loss_scale 8192.0000 (5388.7933)	mem 7723MB
[2024-07-09 20:14:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:00 lr 0.000030	 wd 0.0000	time 0.2051 (0.2409)	loss 1.1705 (1.4075)	grad_norm 0.4136 (0.4377)	loss_scale 8192.0000 (5528.8836)	mem 7723MB
[2024-07-09 20:14:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:36 lr 0.000031	 wd 0.0000	time 0.2100 (0.2402)	loss 1.2542 (1.4056)	grad_norm 0.3984 (0.4376)	loss_scale 8192.0000 (5655.6383)	mem 7723MB
[2024-07-09 20:14:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:12 lr 0.000031	 wd 0.0000	time 0.1969 (0.2393)	loss 1.3851 (1.4065)	grad_norm 0.4088 (0.4371)	loss_scale 8192.0000 (5770.8751)	mem 7723MB
[2024-07-09 20:15:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:00:48 lr 0.000031	 wd 0.0000	time 0.2271 (0.2389)	loss 1.5894 (1.4070)	grad_norm 0.4234 (0.4370)	loss_scale 8192.0000 (5876.0956)	mem 7723MB
[2024-07-09 20:15:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:24 lr 0.000032	 wd 0.0000	time 0.2287 (0.2390)	loss 1.0253 (1.4062)	grad_norm 0.3932 (0.4368)	loss_scale 8192.0000 (5972.5514)	mem 7723MB
[2024-07-09 20:15:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000032	 wd 0.0000	time 0.1526 (0.2372)	loss 1.5800 (1.4065)	grad_norm 0.4202 (0.4368)	loss_scale 8192.0000 (6061.2939)	mem 7723MB
[2024-07-09 20:16:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 3 training takes 0:09:58
[2024-07-09 20:16:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 21.706 (21.706)	Loss 0.4355 (0.4355)	Acc@1 91.797 (91.797)	Acc@5 98.242 (98.242)	Mem 7723MB
[2024-07-09 20:16:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.464 Acc@5 97.060
[2024-07-09 20:16:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.5%
[2024-07-09 20:16:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.46%
[2024-07-09 20:16:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 20:16:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 20:17:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][0/2502]	eta 1 day, 0:01:00 lr 0.000032	 wd 0.0000	time 34.5564 (34.5564)	loss 1.4718 (1.4718)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:17:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:22:24 lr 0.000032	 wd 0.0000	time 0.1973 (0.5596)	loss 1.1825 (1.4170)	grad_norm 0.4916 (0.4365)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:17:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:14:53 lr 0.000033	 wd 0.0000	time 0.2081 (0.3883)	loss 1.1747 (1.4175)	grad_norm 0.4107 (0.4394)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:18:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:12:08 lr 0.000033	 wd 0.0000	time 0.2504 (0.3307)	loss 1.1417 (1.4164)	grad_norm 0.4144 (0.4431)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:18:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:11:41 lr 0.000033	 wd 0.0000	time 0.2903 (0.3339)	loss 1.6024 (1.4114)	grad_norm 0.3954 (0.4434)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:19:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:10:22 lr 0.000034	 wd 0.0000	time 0.2015 (0.3109)	loss 1.3602 (1.4047)	grad_norm 0.4169 (0.4467)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:19:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:09:20 lr 0.000034	 wd 0.0000	time 0.2079 (0.2948)	loss 1.5243 (1.4005)	grad_norm 0.4460 (0.4432)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:19:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:08:31 lr 0.000034	 wd 0.0000	time 0.2327 (0.2841)	loss 1.3445 (1.4004)	grad_norm 0.4166 (0.4414)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:20:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:08:17 lr 0.000035	 wd 0.0000	time 0.2174 (0.2920)	loss 1.5377 (1.3997)	grad_norm 0.4155 (0.4415)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:20:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:07:34 lr 0.000035	 wd 0.0000	time 0.2056 (0.2840)	loss 1.5322 (1.4009)	grad_norm 0.4922 (0.4425)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:21:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:06:56 lr 0.000035	 wd 0.0000	time 0.2065 (0.2773)	loss 1.5837 (1.4013)	grad_norm 0.3811 (0.4410)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:21:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:06:22 lr 0.000036	 wd 0.0000	time 0.2238 (0.2727)	loss 1.5761 (1.4020)	grad_norm 0.4227 (0.4408)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:22:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:05:53 lr 0.000036	 wd 0.0000	time 0.2262 (0.2714)	loss 1.1052 (1.4003)	grad_norm 0.4401 (0.4401)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:22:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:05:22 lr 0.000036	 wd 0.0000	time 0.2329 (0.2679)	loss 1.6141 (1.3995)	grad_norm 0.4178 (0.4397)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 20:22:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:04:51 lr 0.000036	 wd 0.0000	time 0.2265 (0.2645)	loss 1.5384 (1.3993)	grad_norm 0.4274 (inf)	loss_scale 4096.0000 (7963.9572)	mem 7723MB
[2024-07-09 20:23:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:04:22 lr 0.000037	 wd 0.0000	time 0.2183 (0.2616)	loss 1.2853 (1.4018)	grad_norm 0.3866 (inf)	loss_scale 4096.0000 (7706.2652)	mem 7723MB
[2024-07-09 20:23:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:03:55 lr 0.000037	 wd 0.0000	time 0.1909 (0.2615)	loss 1.0729 (1.4011)	grad_norm 0.3947 (inf)	loss_scale 4096.0000 (7480.7645)	mem 7723MB
[2024-07-09 20:24:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:03:28 lr 0.000037	 wd 0.0000	time 0.2254 (0.2595)	loss 1.5922 (1.4005)	grad_norm 0.3990 (inf)	loss_scale 4096.0000 (7281.7778)	mem 7723MB
[2024-07-09 20:24:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:00 lr 0.000038	 wd 0.0000	time 0.2045 (0.2575)	loss 1.4017 (1.4002)	grad_norm 0.4151 (inf)	loss_scale 4096.0000 (7104.8884)	mem 7723MB
[2024-07-09 20:24:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:33 lr 0.000038	 wd 0.0000	time 0.2160 (0.2556)	loss 1.6670 (1.3999)	grad_norm 0.4239 (inf)	loss_scale 4096.0000 (6946.6092)	mem 7723MB
[2024-07-09 20:25:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:07 lr 0.000038	 wd 0.0000	time 0.2571 (0.2546)	loss 1.4995 (1.3992)	grad_norm 0.4572 (inf)	loss_scale 4096.0000 (6804.1499)	mem 7723MB
[2024-07-09 20:25:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:42 lr 0.000039	 wd 0.0000	time 0.1963 (0.2544)	loss 1.4475 (1.3996)	grad_norm 0.4400 (inf)	loss_scale 4096.0000 (6675.2518)	mem 7723MB
[2024-07-09 20:25:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:16 lr 0.000039	 wd 0.0000	time 0.1978 (0.2529)	loss 1.0156 (1.3971)	grad_norm 0.3709 (inf)	loss_scale 4096.0000 (6558.0663)	mem 7723MB
[2024-07-09 20:26:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:50 lr 0.000039	 wd 0.0000	time 0.2100 (0.2514)	loss 1.3356 (1.3973)	grad_norm 0.3900 (inf)	loss_scale 4096.0000 (6451.0665)	mem 7723MB
[2024-07-09 20:26:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:25 lr 0.000040	 wd 0.0000	time 0.2532 (0.2509)	loss 1.6235 (1.3970)	grad_norm 0.4238 (inf)	loss_scale 4096.0000 (6352.9796)	mem 7723MB
[2024-07-09 20:27:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.1687 (0.2490)	loss 0.9490 (1.3968)	grad_norm 0.5521 (inf)	loss_scale 4096.0000 (6262.7365)	mem 7723MB
[2024-07-09 20:27:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 4 training takes 0:10:30
[2024-07-09 20:27:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 20.972 (20.972)	Loss 0.4346 (0.4346)	Acc@1 91.992 (91.992)	Acc@5 98.242 (98.242)	Mem 7723MB
[2024-07-09 20:27:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.554 Acc@5 97.094
[2024-07-09 20:27:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.6%
[2024-07-09 20:27:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.55%
[2024-07-09 20:27:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 20:27:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 20:28:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][0/2502]	eta 11:10:37 lr 0.000040	 wd 0.0000	time 16.0820 (16.0820)	loss 1.6090 (1.6090)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:28:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:16:15 lr 0.000040	 wd 0.0000	time 0.2346 (0.4062)	loss 1.2976 (1.4160)	grad_norm 0.4404 (0.4352)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:28:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:12:12 lr 0.000040	 wd 0.0000	time 0.2052 (0.3183)	loss 1.5129 (1.4178)	grad_norm 0.4101 (0.4319)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:29:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:10:25 lr 0.000040	 wd 0.0000	time 0.2075 (0.2842)	loss 1.6417 (1.4079)	grad_norm 0.4201 (0.4306)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:29:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:09:20 lr 0.000040	 wd 0.0000	time 0.2096 (0.2668)	loss 1.0198 (1.4096)	grad_norm 0.4340 (0.4300)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:29:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:08:41 lr 0.000040	 wd 0.0000	time 0.2376 (0.2605)	loss 1.5564 (1.4047)	grad_norm 0.3942 (0.4351)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:30:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:08:09 lr 0.000040	 wd 0.0000	time 0.2128 (0.2575)	loss 1.4632 (1.4056)	grad_norm 0.3897 (0.4354)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:30:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:07:33 lr 0.000040	 wd 0.0000	time 0.1983 (0.2517)	loss 1.4939 (1.4030)	grad_norm 0.4379 (0.4351)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:31:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:07:01 lr 0.000040	 wd 0.0000	time 0.2259 (0.2474)	loss 0.9676 (1.3968)	grad_norm 0.3984 (0.4358)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:31:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:06:33 lr 0.000040	 wd 0.0000	time 0.3065 (0.2455)	loss 0.9157 (1.3960)	grad_norm 0.4498 (0.4364)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:31:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:06:09 lr 0.000040	 wd 0.0000	time 0.2014 (0.2458)	loss 1.5535 (1.3991)	grad_norm 0.3809 (0.4356)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:32:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:05:41 lr 0.000040	 wd 0.0000	time 0.1985 (0.2435)	loss 1.5888 (1.3949)	grad_norm 0.3963 (0.4346)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:32:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:05:14 lr 0.000040	 wd 0.0000	time 0.1911 (0.2416)	loss 1.5882 (1.3928)	grad_norm 0.4058 (0.4362)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:32:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:04:48 lr 0.000040	 wd 0.0000	time 0.2518 (0.2400)	loss 1.5246 (1.3931)	grad_norm 0.5367 (0.4355)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:33:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:04:24 lr 0.000040	 wd 0.0000	time 0.1993 (0.2403)	loss 1.5455 (1.3926)	grad_norm 0.3936 (0.4349)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:33:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:04:00 lr 0.000040	 wd 0.0000	time 0.2336 (0.2395)	loss 1.4170 (1.3924)	grad_norm 0.4271 (0.4346)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:34:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:03:34 lr 0.000040	 wd 0.0000	time 0.2167 (0.2383)	loss 1.4875 (1.3942)	grad_norm 0.4336 (0.4354)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:34:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:03:10 lr 0.000040	 wd 0.0000	time 0.2166 (0.2372)	loss 1.3840 (1.3937)	grad_norm 0.4241 (0.4359)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:34:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:02:46 lr 0.000040	 wd 0.0000	time 0.2430 (0.2374)	loss 1.0223 (1.3930)	grad_norm 0.4032 (0.4358)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:35:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:23 lr 0.000040	 wd 0.0000	time 0.2254 (0.2381)	loss 1.4997 (1.3950)	grad_norm 0.4946 (0.4354)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:35:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:01:59 lr 0.000040	 wd 0.0000	time 0.2029 (0.2374)	loss 1.5320 (1.3959)	grad_norm 0.4343 (0.4362)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:36:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:35 lr 0.000040	 wd 0.0000	time 0.2120 (0.2365)	loss 1.5349 (1.3964)	grad_norm 0.4207 (0.4364)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:36:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:11 lr 0.000040	 wd 0.0000	time 0.2114 (0.2364)	loss 1.2766 (1.3943)	grad_norm 0.3958 (0.4363)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:36:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:47 lr 0.000040	 wd 0.0000	time 0.2066 (0.2365)	loss 0.9164 (1.3933)	grad_norm 0.3774 (0.4360)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:37:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:24 lr 0.000040	 wd 0.0000	time 0.1899 (0.2357)	loss 1.5321 (1.3933)	grad_norm 0.4507 (0.4364)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:37:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.1695 (0.2340)	loss 1.5476 (1.3931)	grad_norm 0.3990 (0.4371)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:37:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 5 training takes 0:09:49
[2024-07-09 20:38:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 32.933 (32.933)	Loss 0.4299 (0.4299)	Acc@1 91.992 (91.992)	Acc@5 98.242 (98.242)	Mem 7723MB
[2024-07-09 20:38:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.674 Acc@5 97.122
[2024-07-09 20:38:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-09 20:38:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.67%
[2024-07-09 20:38:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 20:38:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 20:38:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][0/2502]	eta 11:34:46 lr 0.000040	 wd 0.0000	time 16.6611 (16.6611)	loss 1.6015 (1.6015)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:39:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:15:05 lr 0.000040	 wd 0.0000	time 0.2026 (0.3769)	loss 1.2221 (1.3616)	grad_norm 0.4160 (0.4335)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:39:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:11:27 lr 0.000040	 wd 0.0000	time 0.2562 (0.2986)	loss 1.6331 (1.3836)	grad_norm 0.4073 (0.4386)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:39:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:11:20 lr 0.000040	 wd 0.0000	time 0.1917 (0.3091)	loss 1.4867 (1.3939)	grad_norm 0.6355 (0.4389)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:40:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:10:00 lr 0.000040	 wd 0.0000	time 0.2382 (0.2857)	loss 1.3996 (1.3872)	grad_norm 0.4024 (0.4454)	loss_scale 8192.0000 (4933.5860)	mem 7723MB
[2024-07-09 20:40:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:09:03 lr 0.000040	 wd 0.0000	time 0.2056 (0.2716)	loss 1.3818 (1.3814)	grad_norm 0.4186 (0.4421)	loss_scale 8192.0000 (5583.9681)	mem 7723MB
[2024-07-09 20:41:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:08:20 lr 0.000040	 wd 0.0000	time 0.2245 (0.2632)	loss 1.6246 (1.3855)	grad_norm 0.4076 (0.4453)	loss_scale 8192.0000 (6017.9168)	mem 7723MB
[2024-07-09 20:41:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:08:27 lr 0.000040	 wd 0.0000	time 0.2383 (0.2815)	loss 1.4348 (1.3879)	grad_norm 0.4642 (0.4431)	loss_scale 8192.0000 (6328.0571)	mem 7723MB
[2024-07-09 20:42:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:07:45 lr 0.000040	 wd 0.0000	time 0.2185 (0.2735)	loss 1.0281 (1.3867)	grad_norm 0.4514 (0.4437)	loss_scale 8192.0000 (6560.7591)	mem 7723MB
[2024-07-09 20:42:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:07:08 lr 0.000040	 wd 0.0000	time 0.2093 (0.2672)	loss 1.6742 (1.3909)	grad_norm 0.3749 (0.4428)	loss_scale 8192.0000 (6741.8069)	mem 7723MB
[2024-07-09 20:42:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:06:36 lr 0.000040	 wd 0.0000	time 0.3307 (0.2638)	loss 1.3318 (1.3861)	grad_norm 0.4352 (0.4417)	loss_scale 8192.0000 (6886.6813)	mem 7723MB
[2024-07-09 20:43:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:06:07 lr 0.000040	 wd 0.0000	time 0.2121 (0.2622)	loss 1.2259 (1.3854)	grad_norm 0.4109 (0.4434)	loss_scale 8192.0000 (7005.2389)	mem 7723MB
[2024-07-09 20:43:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:05:37 lr 0.000040	 wd 0.0000	time 0.2058 (0.2589)	loss 1.0804 (1.3870)	grad_norm 0.4569 (0.4420)	loss_scale 8192.0000 (7104.0533)	mem 7723MB
[2024-07-09 20:43:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:05:07 lr 0.000040	 wd 0.0000	time 0.2253 (0.2558)	loss 1.4189 (1.3861)	grad_norm 0.4059 (0.4424)	loss_scale 8192.0000 (7187.6772)	mem 7723MB
[2024-07-09 20:44:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:04:39 lr 0.000040	 wd 0.0000	time 0.2425 (0.2538)	loss 1.6825 (1.3846)	grad_norm 0.4232 (0.4428)	loss_scale 8192.0000 (7259.3633)	mem 7723MB
[2024-07-09 20:44:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:04:14 lr 0.000040	 wd 0.0000	time 0.2093 (0.2541)	loss 1.4814 (1.3860)	grad_norm 0.3953 (inf)	loss_scale 4096.0000 (7272.3784)	mem 7723MB
[2024-07-09 20:45:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:03:47 lr 0.000040	 wd 0.0000	time 0.2024 (0.2524)	loss 1.5607 (1.3847)	grad_norm 0.5267 (inf)	loss_scale 4096.0000 (7073.9788)	mem 7723MB
[2024-07-09 20:45:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:03:20 lr 0.000040	 wd 0.0000	time 0.2108 (0.2506)	loss 1.5355 (1.3839)	grad_norm 0.4661 (inf)	loss_scale 4096.0000 (6898.9065)	mem 7723MB
[2024-07-09 20:45:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:02:54 lr 0.000040	 wd 0.0000	time 0.2140 (0.2491)	loss 1.1914 (1.3836)	grad_norm 0.4302 (inf)	loss_scale 4096.0000 (6743.2760)	mem 7723MB
[2024-07-09 20:46:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:29 lr 0.000040	 wd 0.0000	time 0.2164 (0.2490)	loss 1.2737 (1.3830)	grad_norm 0.4050 (inf)	loss_scale 4096.0000 (6604.0189)	mem 7723MB
[2024-07-09 20:46:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:04 lr 0.000039	 wd 0.0000	time 0.2625 (0.2483)	loss 0.9975 (1.3829)	grad_norm 0.4149 (inf)	loss_scale 4096.0000 (6478.6807)	mem 7723MB
[2024-07-09 20:47:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:39 lr 0.000039	 wd 0.0000	time 0.2087 (0.2470)	loss 1.5828 (1.3824)	grad_norm 0.4129 (inf)	loss_scale 4096.0000 (6365.2737)	mem 7723MB
[2024-07-09 20:47:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:14 lr 0.000039	 wd 0.0000	time 0.2394 (0.2457)	loss 1.2902 (1.3829)	grad_norm 0.5386 (inf)	loss_scale 4096.0000 (6262.1717)	mem 7723MB
[2024-07-09 20:47:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:49 lr 0.000039	 wd 0.0000	time 0.2706 (0.2456)	loss 1.4782 (1.3836)	grad_norm 0.4162 (inf)	loss_scale 4096.0000 (6168.0313)	mem 7723MB
[2024-07-09 20:48:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:25 lr 0.000039	 wd 0.0000	time 0.2039 (0.2454)	loss 1.2166 (1.3827)	grad_norm 0.3806 (inf)	loss_scale 4096.0000 (6081.7326)	mem 7723MB
[2024-07-09 20:48:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.1886 (0.2434)	loss 1.0554 (1.3834)	grad_norm 0.4136 (inf)	loss_scale 4096.0000 (6002.3351)	mem 7723MB
[2024-07-09 20:48:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 6 training takes 0:10:13
[2024-07-09 20:48:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 20.242 (20.242)	Loss 0.4326 (0.4326)	Acc@1 91.797 (91.797)	Acc@5 98.438 (98.438)	Mem 7723MB
[2024-07-09 20:49:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.766 Acc@5 97.140
[2024-07-09 20:49:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-09 20:49:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.77%
[2024-07-09 20:49:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 20:49:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 20:49:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][0/2502]	eta 1 day, 1:01:08 lr 0.000039	 wd 0.0000	time 35.9987 (35.9987)	loss 1.6190 (1.6190)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:50:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:22:47 lr 0.000039	 wd 0.0000	time 0.2180 (0.5691)	loss 1.2540 (1.4071)	grad_norm 0.4418 (0.4442)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:50:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:15:02 lr 0.000039	 wd 0.0000	time 0.2016 (0.3920)	loss 1.4783 (1.3811)	grad_norm 0.4820 (0.4402)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:50:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:12:26 lr 0.000039	 wd 0.0000	time 0.2576 (0.3392)	loss 1.3212 (1.3793)	grad_norm 0.4047 (0.4427)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:51:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:11:23 lr 0.000039	 wd 0.0000	time 0.2057 (0.3250)	loss 1.3526 (1.3815)	grad_norm 0.4089 (0.4411)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 20:51:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:10:06 lr 0.000039	 wd 0.0000	time 0.2196 (0.3028)	loss 1.4615 (1.3767)	grad_norm 0.4162 (nan)	loss_scale 2048.0000 (3916.1357)	mem 7723MB
[2024-07-09 20:52:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:09:09 lr 0.000039	 wd 0.0000	time 0.2248 (0.2890)	loss 1.3578 (1.3805)	grad_norm 0.3915 (nan)	loss_scale 2048.0000 (3605.2978)	mem 7723MB
[2024-07-09 20:52:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:08:25 lr 0.000039	 wd 0.0000	time 0.2509 (0.2804)	loss 1.4895 (1.3843)	grad_norm 0.3987 (nan)	loss_scale 2048.0000 (3383.1441)	mem 7723MB
[2024-07-09 20:52:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:07:54 lr 0.000039	 wd 0.0000	time 0.1879 (0.2789)	loss 0.9258 (1.3866)	grad_norm 0.4245 (nan)	loss_scale 2048.0000 (3216.4594)	mem 7723MB
[2024-07-09 20:53:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:07:16 lr 0.000039	 wd 0.0000	time 0.1994 (0.2724)	loss 1.0988 (1.3885)	grad_norm 0.3939 (nan)	loss_scale 2048.0000 (3086.7747)	mem 7723MB
[2024-07-09 20:53:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:06:41 lr 0.000039	 wd 0.0000	time 0.2035 (0.2671)	loss 1.1961 (1.3842)	grad_norm 0.3973 (nan)	loss_scale 2048.0000 (2983.0010)	mem 7723MB
[2024-07-09 20:54:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:06:08 lr 0.000039	 wd 0.0000	time 0.2084 (0.2629)	loss 1.2605 (1.3827)	grad_norm 0.4799 (nan)	loss_scale 2048.0000 (2898.0781)	mem 7723MB
[2024-07-09 20:54:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:05:41 lr 0.000039	 wd 0.0000	time 0.2303 (0.2626)	loss 1.4401 (1.3814)	grad_norm 0.4097 (nan)	loss_scale 2048.0000 (2827.2973)	mem 7723MB
[2024-07-09 20:54:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:05:11 lr 0.000039	 wd 0.0000	time 0.2204 (0.2594)	loss 1.4984 (1.3813)	grad_norm 0.5056 (nan)	loss_scale 2048.0000 (2767.3974)	mem 7723MB
[2024-07-09 20:55:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:04:42 lr 0.000039	 wd 0.0000	time 0.2069 (0.2565)	loss 1.4312 (1.3820)	grad_norm 0.4149 (nan)	loss_scale 2048.0000 (2716.0485)	mem 7723MB
[2024-07-09 20:55:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:04:14 lr 0.000039	 wd 0.0000	time 0.2224 (0.2540)	loss 1.6534 (1.3811)	grad_norm 0.4428 (nan)	loss_scale 2048.0000 (2671.5416)	mem 7723MB
[2024-07-09 20:55:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:03:48 lr 0.000039	 wd 0.0000	time 0.4121 (0.2532)	loss 1.5372 (1.3829)	grad_norm 0.4711 (nan)	loss_scale 2048.0000 (2632.5946)	mem 7723MB
[2024-07-09 20:56:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:03:22 lr 0.000039	 wd 0.0000	time 0.1944 (0.2522)	loss 1.5889 (1.3832)	grad_norm 0.4138 (nan)	loss_scale 2048.0000 (2598.2269)	mem 7723MB
[2024-07-09 20:56:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:02:55 lr 0.000039	 wd 0.0000	time 0.2162 (0.2506)	loss 1.1468 (1.3827)	grad_norm 0.4444 (nan)	loss_scale 2048.0000 (2567.6757)	mem 7723MB
[2024-07-09 20:57:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:29 lr 0.000039	 wd 0.0000	time 0.2060 (0.2490)	loss 1.3455 (1.3821)	grad_norm 0.4763 (nan)	loss_scale 2048.0000 (2540.3388)	mem 7723MB
[2024-07-09 20:57:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:04 lr 0.000039	 wd 0.0000	time 0.2110 (0.2484)	loss 1.4648 (1.3832)	grad_norm 0.4128 (nan)	loss_scale 2048.0000 (2515.7341)	mem 7723MB
[2024-07-09 20:57:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:39 lr 0.000039	 wd 0.0000	time 0.2404 (0.2481)	loss 1.5165 (1.3843)	grad_norm 0.4754 (nan)	loss_scale 2048.0000 (2493.4717)	mem 7723MB
[2024-07-09 20:58:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:14 lr 0.000039	 wd 0.0000	time 0.2288 (0.2469)	loss 1.1269 (1.3824)	grad_norm 0.4148 (nan)	loss_scale 2048.0000 (2473.2322)	mem 7723MB
[2024-07-09 20:58:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:49 lr 0.000039	 wd 0.0000	time 0.2173 (0.2458)	loss 1.6169 (1.3828)	grad_norm 0.4041 (nan)	loss_scale 2048.0000 (2454.7518)	mem 7723MB
[2024-07-09 20:59:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:25 lr 0.000039	 wd 0.0000	time 0.2588 (0.2453)	loss 1.4981 (1.3823)	grad_norm 0.4911 (nan)	loss_scale 2048.0000 (2437.8109)	mem 7723MB
[2024-07-09 20:59:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.1583 (0.2434)	loss 1.3003 (1.3826)	grad_norm 0.4192 (nan)	loss_scale 2048.0000 (2422.2247)	mem 7723MB
[2024-07-09 20:59:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 7 training takes 0:10:17
[2024-07-09 20:59:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 21.429 (21.429)	Loss 0.4338 (0.4338)	Acc@1 91.992 (91.992)	Acc@5 98.438 (98.438)	Mem 7723MB
[2024-07-09 21:00:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.742 Acc@5 97.156
[2024-07-09 21:00:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-09 21:00:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.77%
[2024-07-09 21:00:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][0/2502]	eta 11:15:44 lr 0.000039	 wd 0.0000	time 16.2048 (16.2048)	loss 1.4413 (1.4413)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:00:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:16:01 lr 0.000039	 wd 0.0000	time 0.2564 (0.4001)	loss 1.4720 (1.4181)	grad_norm 0.4108 (0.4316)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:01:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:12:35 lr 0.000039	 wd 0.0000	time 0.1997 (0.3281)	loss 1.5586 (1.3930)	grad_norm 0.6066 (0.4350)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:01:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:10:40 lr 0.000038	 wd 0.0000	time 0.2155 (0.2908)	loss 1.3864 (1.3901)	grad_norm 0.4375 (0.4429)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:01:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:09:30 lr 0.000038	 wd 0.0000	time 0.2088 (0.2714)	loss 1.6517 (1.3864)	grad_norm 0.5610 (0.4461)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:02:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:08:46 lr 0.000038	 wd 0.0000	time 0.2787 (0.2632)	loss 1.4731 (1.3895)	grad_norm 0.4168 (0.4424)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:02:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:08:16 lr 0.000038	 wd 0.0000	time 0.2075 (0.2611)	loss 1.5592 (1.3943)	grad_norm 0.4118 (0.4512)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:03:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:07:39 lr 0.000038	 wd 0.0000	time 0.2202 (0.2551)	loss 1.6706 (1.3925)	grad_norm 0.4370 (0.4524)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:03:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:07:06 lr 0.000038	 wd 0.0000	time 0.2196 (0.2504)	loss 1.4857 (1.3901)	grad_norm 0.4074 (0.4499)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:03:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:06:36 lr 0.000038	 wd 0.0000	time 0.2371 (0.2473)	loss 1.6762 (1.3902)	grad_norm 0.4040 (0.4505)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:04:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:06:12 lr 0.000038	 wd 0.0000	time 0.2240 (0.2478)	loss 1.5102 (1.3864)	grad_norm 0.4001 (0.4486)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:04:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:05:44 lr 0.000038	 wd 0.0000	time 0.1930 (0.2458)	loss 1.3573 (1.3833)	grad_norm 0.4360 (0.4488)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:04:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:05:17 lr 0.000038	 wd 0.0000	time 0.2059 (0.2436)	loss 1.4758 (1.3835)	grad_norm 0.4703 (0.4478)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:05:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:04:50 lr 0.000038	 wd 0.0000	time 0.2179 (0.2415)	loss 1.1915 (1.3819)	grad_norm 0.4149 (0.4470)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:05:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:04:26 lr 0.000038	 wd 0.0000	time 0.1981 (0.2419)	loss 0.9651 (1.3836)	grad_norm 0.4750 (0.4470)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:06:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:04:01 lr 0.000038	 wd 0.0000	time 0.2139 (0.2413)	loss 0.8487 (1.3814)	grad_norm 0.4848 (0.4494)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:06:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:03:36 lr 0.000038	 wd 0.0000	time 0.2301 (0.2400)	loss 1.2437 (1.3786)	grad_norm 0.4113 (0.4484)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:06:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:03:11 lr 0.000038	 wd 0.0000	time 0.1991 (0.2388)	loss 1.1941 (1.3804)	grad_norm 0.4208 (0.4485)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:07:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:02:47 lr 0.000038	 wd 0.0000	time 0.2165 (0.2387)	loss 1.5458 (1.3814)	grad_norm 0.4237 (0.4501)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:07:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:24 lr 0.000038	 wd 0.0000	time 0.1921 (0.2397)	loss 1.3361 (1.3806)	grad_norm 0.4509 (0.4507)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:08:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:01:59 lr 0.000038	 wd 0.0000	time 0.2183 (0.2388)	loss 1.7128 (1.3825)	grad_norm 0.4240 (0.4508)	loss_scale 4096.0000 (2095.0805)	mem 7723MB
[2024-07-09 21:08:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:35 lr 0.000038	 wd 0.0000	time 0.1931 (0.2378)	loss 1.4220 (1.3816)	grad_norm 0.4220 (0.4506)	loss_scale 4096.0000 (2190.3170)	mem 7723MB
[2024-07-09 21:08:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:11 lr 0.000038	 wd 0.0000	time 0.2674 (0.2378)	loss 1.5735 (1.3802)	grad_norm 0.4685 (0.4502)	loss_scale 4096.0000 (2276.8996)	mem 7723MB
[2024-07-09 21:09:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:48 lr 0.000038	 wd 0.0000	time 0.2462 (0.2377)	loss 1.5152 (1.3808)	grad_norm 0.4397 (0.4495)	loss_scale 4096.0000 (2355.9565)	mem 7723MB
[2024-07-09 21:09:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:24 lr 0.000038	 wd 0.0000	time 0.2048 (0.2371)	loss 1.3295 (1.3824)	grad_norm 0.4955 (0.4492)	loss_scale 4096.0000 (2428.4282)	mem 7723MB
[2024-07-09 21:09:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000038	 wd 0.0000	time 0.1600 (0.2354)	loss 1.7754 (1.3838)	grad_norm 0.4063 (0.4499)	loss_scale 4096.0000 (2495.1044)	mem 7723MB
[2024-07-09 21:09:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 8 training takes 0:09:53
[2024-07-09 21:10:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 32.991 (32.991)	Loss 0.4321 (0.4321)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 7723MB
[2024-07-09 21:10:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.772 Acc@5 97.180
[2024-07-09 21:10:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-09 21:10:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.77%
[2024-07-09 21:10:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 21:10:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 21:11:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][0/2502]	eta 11:22:02 lr 0.000038	 wd 0.0000	time 16.3559 (16.3559)	loss 1.3963 (1.3963)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:11:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:15:07 lr 0.000038	 wd 0.0000	time 0.1969 (0.3776)	loss 1.2620 (1.3486)	grad_norm 0.8198 (0.4568)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:11:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:11:29 lr 0.000037	 wd 0.0000	time 0.2345 (0.2997)	loss 1.5134 (1.3606)	grad_norm 0.4328 (0.4501)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:12:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:11:19 lr 0.000037	 wd 0.0000	time 0.2222 (0.3084)	loss 1.4443 (1.3724)	grad_norm 0.4718 (0.4483)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:12:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:10:00 lr 0.000037	 wd 0.0000	time 0.2069 (0.2855)	loss 1.5931 (1.3672)	grad_norm 0.5119 (inf)	loss_scale 2048.0000 (3789.5661)	mem 7723MB
[2024-07-09 21:13:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:09:03 lr 0.000037	 wd 0.0000	time 0.2212 (0.2717)	loss 1.4050 (1.3737)	grad_norm 0.4149 (inf)	loss_scale 2048.0000 (3441.9481)	mem 7723MB
[2024-07-09 21:13:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:08:22 lr 0.000037	 wd 0.0000	time 0.2400 (0.2641)	loss 1.5377 (1.3682)	grad_norm 0.3925 (inf)	loss_scale 2048.0000 (3210.0100)	mem 7723MB
[2024-07-09 21:13:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:07:50 lr 0.000037	 wd 0.0000	time 0.2017 (0.2614)	loss 1.5522 (1.3696)	grad_norm 0.4122 (inf)	loss_scale 2048.0000 (3044.2454)	mem 7723MB
[2024-07-09 21:14:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:07:17 lr 0.000037	 wd 0.0000	time 0.2161 (0.2570)	loss 1.3978 (1.3726)	grad_norm 0.4288 (inf)	loss_scale 2048.0000 (2919.8702)	mem 7723MB
[2024-07-09 21:14:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:06:44 lr 0.000037	 wd 0.0000	time 0.1825 (0.2528)	loss 1.6206 (1.3695)	grad_norm 0.4150 (inf)	loss_scale 2048.0000 (2823.1032)	mem 7723MB
[2024-07-09 21:14:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:06:14 lr 0.000037	 wd 0.0000	time 0.2216 (0.2491)	loss 1.4991 (1.3724)	grad_norm 0.3983 (inf)	loss_scale 2048.0000 (2745.6703)	mem 7723MB
[2024-07-09 21:15:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:05:59 lr 0.000037	 wd 0.0000	time 0.2108 (0.2564)	loss 1.4124 (1.3754)	grad_norm 0.4000 (inf)	loss_scale 2048.0000 (2682.3034)	mem 7723MB
[2024-07-09 21:15:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:05:29 lr 0.000037	 wd 0.0000	time 0.2318 (0.2533)	loss 1.5542 (1.3763)	grad_norm 0.3838 (inf)	loss_scale 2048.0000 (2629.4888)	mem 7723MB
[2024-07-09 21:16:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:05:01 lr 0.000037	 wd 0.0000	time 0.2256 (0.2506)	loss 0.9839 (1.3742)	grad_norm 0.4118 (inf)	loss_scale 2048.0000 (2584.7932)	mem 7723MB
[2024-07-09 21:16:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:04:33 lr 0.000037	 wd 0.0000	time 0.1982 (0.2485)	loss 1.3636 (1.3756)	grad_norm 0.3962 (inf)	loss_scale 2048.0000 (2546.4782)	mem 7723MB
[2024-07-09 21:17:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:04:09 lr 0.000037	 wd 0.0000	time 0.1883 (0.2488)	loss 1.4884 (1.3776)	grad_norm 0.4240 (inf)	loss_scale 2048.0000 (2513.2685)	mem 7723MB
[2024-07-09 21:17:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:03:43 lr 0.000037	 wd 0.0000	time 0.2097 (0.2481)	loss 0.9538 (1.3766)	grad_norm 0.4146 (inf)	loss_scale 2048.0000 (2484.2074)	mem 7723MB
[2024-07-09 21:17:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:03:17 lr 0.000037	 wd 0.0000	time 0.2335 (0.2465)	loss 1.1187 (1.3779)	grad_norm 0.4182 (inf)	loss_scale 2048.0000 (2458.5632)	mem 7723MB
[2024-07-09 21:18:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:02:51 lr 0.000037	 wd 0.0000	time 0.2072 (0.2449)	loss 1.5531 (1.3781)	grad_norm 0.6210 (inf)	loss_scale 2048.0000 (2435.7668)	mem 7723MB
[2024-07-09 21:18:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:27 lr 0.000037	 wd 0.0000	time 0.2546 (0.2446)	loss 1.3544 (1.3768)	grad_norm 0.4488 (inf)	loss_scale 2048.0000 (2415.3688)	mem 7723MB
[2024-07-09 21:18:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:02 lr 0.000037	 wd 0.0000	time 0.1801 (0.2450)	loss 1.5039 (1.3769)	grad_norm 0.4265 (inf)	loss_scale 2048.0000 (2397.0095)	mem 7723MB
[2024-07-09 21:19:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:38 lr 0.000036	 wd 0.0000	time 0.2209 (0.2439)	loss 1.0800 (1.3770)	grad_norm 0.4910 (inf)	loss_scale 2048.0000 (2380.3979)	mem 7723MB
[2024-07-09 21:19:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:13 lr 0.000036	 wd 0.0000	time 0.2616 (0.2428)	loss 1.4955 (1.3780)	grad_norm 0.4495 (inf)	loss_scale 2048.0000 (2365.2958)	mem 7723MB
[2024-07-09 21:20:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:49 lr 0.000036	 wd 0.0000	time 0.2371 (0.2426)	loss 1.4295 (1.3786)	grad_norm 0.4298 (inf)	loss_scale 2048.0000 (2351.5063)	mem 7723MB
[2024-07-09 21:20:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:24 lr 0.000036	 wd 0.0000	time 0.1941 (0.2425)	loss 1.2799 (1.3783)	grad_norm 0.4413 (inf)	loss_scale 2048.0000 (2338.8655)	mem 7723MB
[2024-07-09 21:20:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000036	 wd 0.0000	time 0.1782 (0.2405)	loss 1.1379 (1.3794)	grad_norm 0.4535 (inf)	loss_scale 2048.0000 (2327.2355)	mem 7723MB
[2024-07-09 21:20:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 9 training takes 0:10:06
[2024-07-09 21:21:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 19.945 (19.945)	Loss 0.4243 (0.4243)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 21:21:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.734 Acc@5 97.184
[2024-07-09 21:21:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.7%
[2024-07-09 21:21:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.77%
[2024-07-09 21:21:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][0/2502]	eta 22:47:06 lr 0.000036	 wd 0.0000	time 32.7845 (32.7845)	loss 1.2978 (1.2978)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:22:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:22:07 lr 0.000036	 wd 0.0000	time 0.2094 (0.5526)	loss 1.3763 (1.3601)	grad_norm 0.3974 (0.4390)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:22:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:14:45 lr 0.000036	 wd 0.0000	time 0.2179 (0.3845)	loss 1.3204 (1.3807)	grad_norm 0.3964 (0.4341)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:23:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:12:04 lr 0.000036	 wd 0.0000	time 0.2476 (0.3290)	loss 0.9009 (1.3775)	grad_norm 0.4214 (0.4385)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:23:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:10:48 lr 0.000036	 wd 0.0000	time 0.2307 (0.3083)	loss 1.5342 (1.3807)	grad_norm 0.4380 (0.4353)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:23:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:09:40 lr 0.000036	 wd 0.0000	time 0.2342 (0.2900)	loss 1.4291 (1.3762)	grad_norm 0.4051 (0.4347)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:24:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:08:49 lr 0.000036	 wd 0.0000	time 0.2024 (0.2783)	loss 1.5283 (1.3765)	grad_norm 0.4062 (0.4370)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:24:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:08:04 lr 0.000036	 wd 0.0000	time 0.1803 (0.2690)	loss 1.5064 (1.3723)	grad_norm 0.4046 (0.4419)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:24:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:07:31 lr 0.000036	 wd 0.0000	time 0.2411 (0.2653)	loss 1.2517 (1.3715)	grad_norm 0.3829 (0.4408)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:25:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:07:01 lr 0.000036	 wd 0.0000	time 0.1988 (0.2628)	loss 1.5574 (1.3715)	grad_norm 0.4418 (0.4413)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:25:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:06:28 lr 0.000036	 wd 0.0000	time 0.1957 (0.2585)	loss 1.4950 (1.3700)	grad_norm 0.6014 (0.4410)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:26:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:05:57 lr 0.000036	 wd 0.0000	time 0.2282 (0.2550)	loss 1.3182 (1.3676)	grad_norm 0.3975 (0.4405)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:26:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:05:29 lr 0.000035	 wd 0.0000	time 0.2574 (0.2531)	loss 1.3597 (1.3682)	grad_norm 0.4247 (0.4404)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:27:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:05:07 lr 0.000035	 wd 0.0000	time 0.2232 (0.2560)	loss 1.5918 (1.3708)	grad_norm 0.4112 (0.4406)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:27:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:04:39 lr 0.000035	 wd 0.0000	time 0.2068 (0.2532)	loss 1.3491 (1.3713)	grad_norm 0.4139 (0.4410)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:27:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:04:11 lr 0.000035	 wd 0.0000	time 0.1997 (0.2511)	loss 1.2659 (1.3712)	grad_norm 0.3835 (0.4407)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:28:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:03:45 lr 0.000035	 wd 0.0000	time 0.2519 (0.2499)	loss 1.2462 (1.3726)	grad_norm 0.4473 (0.4426)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:28:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:03:20 lr 0.000035	 wd 0.0000	time 0.2036 (0.2504)	loss 1.3212 (1.3736)	grad_norm 0.4046 (0.4422)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:28:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:02:54 lr 0.000035	 wd 0.0000	time 0.2397 (0.2489)	loss 1.4100 (1.3742)	grad_norm 0.4867 (0.4431)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 21:29:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:02:28 lr 0.000035	 wd 0.0000	time 0.2290 (0.2475)	loss 1.5251 (1.3757)	grad_norm 0.5099 (0.4429)	loss_scale 4096.0000 (2114.7943)	mem 7723MB
[2024-07-09 21:29:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:03 lr 0.000035	 wd 0.0000	time 0.2680 (0.2463)	loss 1.2669 (1.3766)	grad_norm 0.4883 (0.4428)	loss_scale 4096.0000 (2213.8051)	mem 7723MB
[2024-07-09 21:30:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:39 lr 0.000035	 wd 0.0000	time 0.2112 (0.2466)	loss 1.5799 (1.3783)	grad_norm 0.4445 (0.4428)	loss_scale 4096.0000 (2303.3908)	mem 7723MB
[2024-07-09 21:30:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:14 lr 0.000035	 wd 0.0000	time 0.2134 (0.2456)	loss 0.9693 (1.3781)	grad_norm 0.4847 (0.4423)	loss_scale 4096.0000 (2384.8360)	mem 7723MB
[2024-07-09 21:30:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:49 lr 0.000035	 wd 0.0000	time 0.2382 (0.2445)	loss 1.5024 (1.3776)	grad_norm 0.4155 (0.4416)	loss_scale 4096.0000 (2459.2021)	mem 7723MB
[2024-07-09 21:31:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:24 lr 0.000035	 wd 0.0000	time 0.2348 (0.2436)	loss 1.5444 (1.3775)	grad_norm 0.4148 (0.4414)	loss_scale 4096.0000 (2527.3736)	mem 7723MB
[2024-07-09 21:31:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.1734 (0.2419)	loss 1.3493 (1.3765)	grad_norm 0.4350 (0.4416)	loss_scale 4096.0000 (2590.0936)	mem 7723MB
[2024-07-09 21:31:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 10 training takes 0:10:12
[2024-07-09 21:32:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 26.740 (26.740)	Loss 0.4316 (0.4316)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 21:32:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.840 Acc@5 97.188
[2024-07-09 21:32:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.8%
[2024-07-09 21:32:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.84%
[2024-07-09 21:32:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 21:32:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 21:32:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][0/2502]	eta 11:08:33 lr 0.000035	 wd 0.0000	time 16.0326 (16.0326)	loss 1.5127 (1.5127)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:33:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:15:38 lr 0.000035	 wd 0.0000	time 0.2983 (0.3907)	loss 1.3951 (1.3687)	grad_norm 0.4165 (0.4520)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:33:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:12:40 lr 0.000034	 wd 0.0000	time 0.2047 (0.3306)	loss 1.4720 (1.3727)	grad_norm 0.4425 (0.4444)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:33:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:10:43 lr 0.000034	 wd 0.0000	time 0.1765 (0.2921)	loss 1.3417 (1.3747)	grad_norm 0.5288 (0.4561)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:34:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:09:34 lr 0.000034	 wd 0.0000	time 0.2102 (0.2731)	loss 1.5506 (1.3744)	grad_norm 0.4345 (0.4512)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:34:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:08:45 lr 0.000034	 wd 0.0000	time 0.1954 (0.2625)	loss 1.4021 (1.3712)	grad_norm 0.3897 (0.4521)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:34:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:08:14 lr 0.000034	 wd 0.0000	time 0.2285 (0.2602)	loss 1.4821 (1.3687)	grad_norm 0.4737 (0.4508)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:35:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:07:38 lr 0.000034	 wd 0.0000	time 0.2223 (0.2543)	loss 1.4924 (1.3690)	grad_norm 0.4122 (0.4490)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:35:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:07:05 lr 0.000034	 wd 0.0000	time 0.2214 (0.2497)	loss 1.6276 (1.3715)	grad_norm 0.4195 (0.4499)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:36:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:06:34 lr 0.000034	 wd 0.0000	time 0.2758 (0.2465)	loss 1.5397 (1.3713)	grad_norm 0.4568 (0.4485)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:36:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:06:10 lr 0.000034	 wd 0.0000	time 0.3964 (0.2468)	loss 1.4549 (1.3734)	grad_norm 0.4328 (0.4479)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:36:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:05:43 lr 0.000034	 wd 0.0000	time 0.2138 (0.2449)	loss 1.5747 (1.3747)	grad_norm 0.4405 (0.4469)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:37:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:05:15 lr 0.000034	 wd 0.0000	time 0.2421 (0.2426)	loss 1.7121 (1.3746)	grad_norm 0.4238 (0.4475)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:37:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:04:49 lr 0.000034	 wd 0.0000	time 0.2079 (0.2406)	loss 0.8994 (1.3734)	grad_norm 0.3932 (0.4477)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:37:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:04:25 lr 0.000034	 wd 0.0000	time 0.1990 (0.2411)	loss 0.8970 (1.3730)	grad_norm 0.3900 (0.4472)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:38:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:04:02 lr 0.000034	 wd 0.0000	time 0.2027 (0.2416)	loss 1.4438 (1.3720)	grad_norm 0.3889 (0.4470)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:38:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:03:36 lr 0.000034	 wd 0.0000	time 0.2114 (0.2405)	loss 1.2752 (1.3729)	grad_norm 0.4470 (0.4465)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:39:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:03:11 lr 0.000033	 wd 0.0000	time 0.2057 (0.2392)	loss 1.4715 (1.3721)	grad_norm 0.3852 (0.4473)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:39:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:02:47 lr 0.000033	 wd 0.0000	time 0.2338 (0.2390)	loss 1.2970 (1.3737)	grad_norm 0.6191 (0.4467)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:39:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:24 lr 0.000033	 wd 0.0000	time 0.2696 (0.2398)	loss 1.5091 (1.3739)	grad_norm 0.3978 (0.4464)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:40:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:01:59 lr 0.000033	 wd 0.0000	time 0.2328 (0.2387)	loss 1.1182 (1.3732)	grad_norm 0.4310 (0.4463)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:40:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:35 lr 0.000033	 wd 0.0000	time 0.2023 (0.2379)	loss 1.5642 (1.3730)	grad_norm 0.4264 (0.4461)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:41:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:11 lr 0.000033	 wd 0.0000	time 0.2517 (0.2376)	loss 1.3027 (1.3738)	grad_norm 0.4078 (0.4465)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:41:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:48 lr 0.000033	 wd 0.0000	time 0.1938 (0.2376)	loss 1.5573 (1.3735)	grad_norm 0.4709 (0.4458)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:41:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:24 lr 0.000033	 wd 0.0000	time 0.1989 (0.2371)	loss 1.2788 (1.3727)	grad_norm 0.4502 (0.4457)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:42:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000033	 wd 0.0000	time 0.1730 (0.2353)	loss 1.4305 (1.3723)	grad_norm 0.4200 (0.4458)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:42:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 11 training takes 0:09:53
[2024-07-09 21:42:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 35.602 (35.602)	Loss 0.4260 (0.4260)	Acc@1 91.797 (91.797)	Acc@5 98.438 (98.438)	Mem 7723MB
[2024-07-09 21:43:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.862 Acc@5 97.214
[2024-07-09 21:43:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-07-09 21:43:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.86%
[2024-07-09 21:43:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 21:43:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 21:43:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][0/2502]	eta 11:19:19 lr 0.000033	 wd 0.0000	time 16.2906 (16.2906)	loss 1.6336 (1.6336)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:43:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:14:58 lr 0.000033	 wd 0.0000	time 0.2382 (0.3740)	loss 1.5199 (1.3789)	grad_norm 0.4782 (0.4487)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:44:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:11:24 lr 0.000033	 wd 0.0000	time 0.2510 (0.2975)	loss 0.9959 (1.3841)	grad_norm 0.6037 (0.4486)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:44:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:10:25 lr 0.000033	 wd 0.0000	time 0.2407 (0.2842)	loss 1.3112 (1.3842)	grad_norm 0.4213 (0.4510)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:44:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:09:23 lr 0.000033	 wd 0.0000	time 0.2220 (0.2678)	loss 1.2484 (1.3765)	grad_norm 0.3738 (0.4511)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:45:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:08:35 lr 0.000032	 wd 0.0000	time 0.2130 (0.2574)	loss 1.0321 (1.3772)	grad_norm 0.3959 (0.4497)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:45:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:07:56 lr 0.000032	 wd 0.0000	time 0.2147 (0.2504)	loss 1.3709 (1.3743)	grad_norm 0.4142 (0.4543)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:46:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:07:28 lr 0.000032	 wd 0.0000	time 0.2376 (0.2491)	loss 1.4765 (1.3721)	grad_norm 0.4210 (0.4515)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:46:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:07:04 lr 0.000032	 wd 0.0000	time 0.1874 (0.2493)	loss 1.4575 (1.3799)	grad_norm 0.4323 (0.4543)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:46:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:06:33 lr 0.000032	 wd 0.0000	time 0.2015 (0.2457)	loss 1.0725 (1.3803)	grad_norm 0.4393 (0.4528)	loss_scale 8192.0000 (4396.0400)	mem 7723MB
[2024-07-09 21:47:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:06:04 lr 0.000032	 wd 0.0000	time 0.2002 (0.2428)	loss 1.6384 (1.3778)	grad_norm 0.4325 (0.4512)	loss_scale 8192.0000 (4775.2567)	mem 7723MB
[2024-07-09 21:47:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:05:39 lr 0.000032	 wd 0.0000	time 0.2334 (0.2423)	loss 1.4929 (1.3818)	grad_norm 0.4136 (0.4505)	loss_scale 8192.0000 (5085.5876)	mem 7723MB
[2024-07-09 21:48:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:05:15 lr 0.000032	 wd 0.0000	time 0.2080 (0.2422)	loss 1.5225 (1.3807)	grad_norm 0.4447 (0.4491)	loss_scale 8192.0000 (5344.2398)	mem 7723MB
[2024-07-09 21:48:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:04:49 lr 0.000032	 wd 0.0000	time 0.2224 (0.2405)	loss 1.4541 (1.3811)	grad_norm 0.4183 (0.4484)	loss_scale 8192.0000 (5563.1299)	mem 7723MB
[2024-07-09 21:48:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:04:23 lr 0.000032	 wd 0.0000	time 0.1830 (0.2392)	loss 1.6242 (1.3810)	grad_norm 0.4411 (0.4504)	loss_scale 8192.0000 (5750.7723)	mem 7723MB
[2024-07-09 21:49:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:03:58 lr 0.000032	 wd 0.0000	time 0.2131 (0.2384)	loss 1.4422 (1.3816)	grad_norm 0.4163 (0.4503)	loss_scale 8192.0000 (5913.4124)	mem 7723MB
[2024-07-09 21:49:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:03:36 lr 0.000032	 wd 0.0000	time 0.2132 (0.2396)	loss 1.4430 (1.3805)	grad_norm 0.4210 (0.4492)	loss_scale 8192.0000 (6055.7352)	mem 7723MB
[2024-07-09 21:49:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:03:11 lr 0.000031	 wd 0.0000	time 0.1967 (0.2389)	loss 1.5789 (1.3816)	grad_norm 0.4076 (0.4491)	loss_scale 8192.0000 (6181.3239)	mem 7723MB
[2024-07-09 21:50:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:02:46 lr 0.000031	 wd 0.0000	time 0.2114 (0.2378)	loss 1.2875 (1.3823)	grad_norm 0.7857 (0.4495)	loss_scale 8192.0000 (6292.9661)	mem 7723MB
[2024-07-09 21:50:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:22 lr 0.000031	 wd 0.0000	time 0.2196 (0.2371)	loss 1.4280 (1.3818)	grad_norm 0.4613 (inf)	loss_scale 4096.0000 (6186.0158)	mem 7723MB
[2024-07-09 21:51:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:01:59 lr 0.000031	 wd 0.0000	time 0.1922 (0.2379)	loss 1.4107 (1.3808)	grad_norm 0.4169 (inf)	loss_scale 4096.0000 (6081.5672)	mem 7723MB
[2024-07-09 21:51:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:35 lr 0.000031	 wd 0.0000	time 0.2010 (0.2378)	loss 1.4603 (1.3820)	grad_norm 0.4046 (inf)	loss_scale 4096.0000 (5987.0614)	mem 7723MB
[2024-07-09 21:51:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:11 lr 0.000031	 wd 0.0000	time 0.1939 (0.2369)	loss 1.5714 (1.3834)	grad_norm 0.4395 (inf)	loss_scale 4096.0000 (5901.1431)	mem 7723MB
[2024-07-09 21:52:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:47 lr 0.000031	 wd 0.0000	time 0.2231 (0.2361)	loss 1.4451 (1.3833)	grad_norm 0.4327 (inf)	loss_scale 4096.0000 (5822.6927)	mem 7723MB
[2024-07-09 21:52:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:24 lr 0.000031	 wd 0.0000	time 0.2115 (0.2368)	loss 1.5144 (1.3830)	grad_norm 0.4971 (inf)	loss_scale 4096.0000 (5750.7772)	mem 7723MB
[2024-07-09 21:52:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000031	 wd 0.0000	time 0.1617 (0.2351)	loss 1.1331 (1.3818)	grad_norm 0.4254 (inf)	loss_scale 4096.0000 (5684.6126)	mem 7723MB
[2024-07-09 21:53:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 12 training takes 0:09:53
[2024-07-09 21:53:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 20.984 (20.984)	Loss 0.4265 (0.4265)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 7723MB
[2024-07-09 21:53:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.862 Acc@5 97.200
[2024-07-09 21:53:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-07-09 21:53:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.86%
[2024-07-09 21:53:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 21:53:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 21:53:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][0/2502]	eta 15:14:35 lr 0.000031	 wd 0.0000	time 21.9327 (21.9327)	loss 1.3708 (1.3708)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:54:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:18:38 lr 0.000031	 wd 0.0000	time 0.2185 (0.4658)	loss 1.4043 (1.3670)	grad_norm 0.4837 (0.4789)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:54:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:13:05 lr 0.000031	 wd 0.0000	time 0.2179 (0.3413)	loss 1.7017 (1.3689)	grad_norm 0.4100 (0.4594)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:55:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:10:59 lr 0.000031	 wd 0.0000	time 0.1888 (0.2995)	loss 1.4527 (1.3776)	grad_norm 0.4021 (0.4517)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:55:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:09:43 lr 0.000030	 wd 0.0000	time 0.2472 (0.2777)	loss 1.3559 (1.3765)	grad_norm 0.4075 (0.4479)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:55:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:09:09 lr 0.000030	 wd 0.0000	time 0.1946 (0.2743)	loss 1.5344 (1.3713)	grad_norm 0.4431 (0.4466)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:56:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:08:24 lr 0.000030	 wd 0.0000	time 0.1929 (0.2654)	loss 1.5485 (1.3777)	grad_norm 0.4139 (0.4509)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:56:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:07:46 lr 0.000030	 wd 0.0000	time 0.1976 (0.2588)	loss 1.4499 (1.3729)	grad_norm 0.4227 (0.4503)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:57:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:07:11 lr 0.000030	 wd 0.0000	time 0.2060 (0.2533)	loss 1.5501 (1.3769)	grad_norm 0.4486 (0.4488)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:57:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:06:43 lr 0.000030	 wd 0.0000	time 0.2487 (0.2521)	loss 1.5366 (1.3737)	grad_norm 0.5314 (0.4473)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:57:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:06:16 lr 0.000030	 wd 0.0000	time 0.2128 (0.2507)	loss 1.6235 (1.3793)	grad_norm 0.5103 (0.4469)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:58:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:05:47 lr 0.000030	 wd 0.0000	time 0.2218 (0.2478)	loss 1.3976 (1.3808)	grad_norm 0.4148 (0.4477)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:58:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:05:19 lr 0.000030	 wd 0.0000	time 0.2120 (0.2455)	loss 1.5667 (1.3770)	grad_norm 0.4150 (0.4485)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:58:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:04:53 lr 0.000030	 wd 0.0000	time 0.2208 (0.2442)	loss 1.4427 (1.3756)	grad_norm 0.4032 (0.4489)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:59:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:04:29 lr 0.000030	 wd 0.0000	time 0.2289 (0.2444)	loss 1.6787 (1.3761)	grad_norm 0.4126 (0.4482)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 21:59:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:04:03 lr 0.000030	 wd 0.0000	time 0.2036 (0.2428)	loss 1.6488 (1.3748)	grad_norm 0.4368 (0.4476)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:00:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:03:37 lr 0.000029	 wd 0.0000	time 0.1997 (0.2414)	loss 1.5837 (1.3745)	grad_norm 0.4263 (0.4490)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:00:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:03:12 lr 0.000029	 wd 0.0000	time 0.2504 (0.2404)	loss 0.8663 (1.3721)	grad_norm 0.4634 (0.4498)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:00:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:02:49 lr 0.000029	 wd 0.0000	time 0.2653 (0.2414)	loss 1.4485 (1.3738)	grad_norm 0.4003 (0.4497)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:01:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:25 lr 0.000029	 wd 0.0000	time 0.1996 (0.2410)	loss 1.2235 (1.3742)	grad_norm 0.4367 (0.4497)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:01:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:00 lr 0.000029	 wd 0.0000	time 0.2161 (0.2400)	loss 1.4517 (1.3752)	grad_norm 0.4161 (0.4500)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:01:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:36 lr 0.000029	 wd 0.0000	time 0.2202 (0.2391)	loss 1.1301 (1.3748)	grad_norm 0.4401 (0.4521)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:02:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:12 lr 0.000029	 wd 0.0000	time 0.2217 (0.2396)	loss 1.5634 (1.3763)	grad_norm 0.4716 (0.4525)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:02:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:48 lr 0.000029	 wd 0.0000	time 0.2221 (0.2390)	loss 1.3203 (1.3757)	grad_norm 0.4484 (0.4525)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:03:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:24 lr 0.000029	 wd 0.0000	time 0.1933 (0.2382)	loss 1.7206 (1.3769)	grad_norm 0.4169 (0.4519)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:03:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000029	 wd 0.0000	time 0.1621 (0.2362)	loss 1.5354 (1.3763)	grad_norm 0.4130 (0.4514)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:03:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 13 training takes 0:09:55
[2024-07-09 22:04:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 41.945 (41.945)	Loss 0.4214 (0.4214)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 22:04:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.926 Acc@5 97.218
[2024-07-09 22:04:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 83.9%
[2024-07-09 22:04:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.93%
[2024-07-09 22:04:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 22:04:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 22:04:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][0/2502]	eta 10:15:50 lr 0.000029	 wd 0.0000	time 14.7683 (14.7683)	loss 1.5044 (1.5044)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:05:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:14:38 lr 0.000029	 wd 0.0000	time 0.1991 (0.3658)	loss 1.5706 (1.3866)	grad_norm 0.4060 (0.4507)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:05:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:12:27 lr 0.000028	 wd 0.0000	time 0.4650 (0.3249)	loss 1.3095 (1.4058)	grad_norm 0.3838 (0.4411)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:05:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:11:02 lr 0.000028	 wd 0.0000	time 0.2215 (0.3007)	loss 1.0266 (1.3935)	grad_norm 0.4131 (0.4425)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:06:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:09:45 lr 0.000028	 wd 0.0000	time 0.2195 (0.2786)	loss 1.2314 (1.3782)	grad_norm 0.9422 (0.4503)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:06:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:08:51 lr 0.000028	 wd 0.0000	time 0.2013 (0.2655)	loss 1.3008 (1.3808)	grad_norm 0.4127 (0.4507)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:07:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:08:17 lr 0.000028	 wd 0.0000	time 0.2215 (0.2617)	loss 1.6666 (1.3864)	grad_norm 0.4508 (0.4503)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:07:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:07:46 lr 0.000028	 wd 0.0000	time 0.2014 (0.2590)	loss 1.4538 (1.3824)	grad_norm 0.4725 (0.4495)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:07:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:07:11 lr 0.000028	 wd 0.0000	time 0.2143 (0.2536)	loss 1.3879 (1.3786)	grad_norm 0.4433 (0.4503)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:08:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:06:40 lr 0.000028	 wd 0.0000	time 0.2056 (0.2499)	loss 1.4547 (1.3745)	grad_norm 0.4395 (0.4496)	loss_scale 8192.0000 (4550.6060)	mem 7723MB
[2024-07-09 22:08:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:06:12 lr 0.000028	 wd 0.0000	time 0.2588 (0.2483)	loss 1.3425 (1.3729)	grad_norm 0.4153 (0.4516)	loss_scale 8192.0000 (4914.3816)	mem 7723MB
[2024-07-09 22:09:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:05:47 lr 0.000028	 wd 0.0000	time 0.2738 (0.2477)	loss 1.5109 (1.3705)	grad_norm 0.3866 (0.4518)	loss_scale 8192.0000 (5212.0763)	mem 7723MB
[2024-07-09 22:09:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:05:19 lr 0.000028	 wd 0.0000	time 0.2000 (0.2456)	loss 1.4184 (1.3707)	grad_norm 0.5421 (0.4516)	loss_scale 8192.0000 (5460.1965)	mem 7723MB
[2024-07-09 22:09:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:04:52 lr 0.000027	 wd 0.0000	time 0.2074 (0.2435)	loss 1.2022 (1.3708)	grad_norm 0.4209 (0.4518)	loss_scale 8192.0000 (5670.1737)	mem 7723MB
[2024-07-09 22:10:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:04:26 lr 0.000027	 wd 0.0000	time 0.2323 (0.2419)	loss 1.6603 (1.3697)	grad_norm 0.4274 (0.4517)	loss_scale 8192.0000 (5850.1756)	mem 7723MB
[2024-07-09 22:10:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:04:02 lr 0.000027	 wd 0.0000	time 0.2054 (0.2420)	loss 1.3859 (1.3706)	grad_norm 0.3946 (0.4510)	loss_scale 8192.0000 (6006.1932)	mem 7723MB
[2024-07-09 22:10:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:03:37 lr 0.000027	 wd 0.0000	time 0.1984 (0.2411)	loss 1.3213 (1.3704)	grad_norm 0.4467 (0.4520)	loss_scale 8192.0000 (6142.7208)	mem 7723MB
[2024-07-09 22:11:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:03:12 lr 0.000027	 wd 0.0000	time 0.2353 (0.2398)	loss 1.2037 (1.3697)	grad_norm 0.4868 (0.4511)	loss_scale 8192.0000 (6263.1958)	mem 7723MB
[2024-07-09 22:11:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:02:47 lr 0.000027	 wd 0.0000	time 0.2219 (0.2386)	loss 1.3175 (1.3695)	grad_norm 0.4255 (0.4507)	loss_scale 8192.0000 (6370.2921)	mem 7723MB
[2024-07-09 22:12:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:23 lr 0.000027	 wd 0.0000	time 0.2355 (0.2387)	loss 1.5484 (1.3696)	grad_norm 0.5069 (0.4505)	loss_scale 8192.0000 (6466.1210)	mem 7723MB
[2024-07-09 22:12:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:00 lr 0.000027	 wd 0.0000	time 0.2328 (0.2391)	loss 1.0822 (1.3715)	grad_norm 0.4073 (0.4505)	loss_scale 8192.0000 (6552.3718)	mem 7723MB
[2024-07-09 22:12:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:35 lr 0.000027	 wd 0.0000	time 0.2237 (0.2382)	loss 1.5608 (1.3712)	grad_norm 0.4373 (0.4509)	loss_scale 8192.0000 (6630.4122)	mem 7723MB
[2024-07-09 22:13:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:11 lr 0.000027	 wd 0.0000	time 0.1967 (0.2373)	loss 1.2128 (1.3714)	grad_norm 0.4254 (0.4508)	loss_scale 8192.0000 (6701.3612)	mem 7723MB
[2024-07-09 22:13:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:47 lr 0.000027	 wd 0.0000	time 0.2370 (0.2372)	loss 1.3933 (1.3722)	grad_norm 0.4877 (inf)	loss_scale 4096.0000 (6659.3377)	mem 7723MB
[2024-07-09 22:13:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:24 lr 0.000026	 wd 0.0000	time 0.2579 (0.2373)	loss 1.1377 (1.3733)	grad_norm 0.4463 (inf)	loss_scale 4096.0000 (6552.5764)	mem 7723MB
[2024-07-09 22:14:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.1740 (0.2353)	loss 1.2713 (1.3724)	grad_norm 0.4463 (inf)	loss_scale 4096.0000 (6454.3527)	mem 7723MB
[2024-07-09 22:14:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 14 training takes 0:09:53
[2024-07-09 22:14:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 20.439 (20.439)	Loss 0.4224 (0.4224)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 22:14:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.956 Acc@5 97.200
[2024-07-09 22:14:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 22:14:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.96%
[2024-07-09 22:14:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 22:14:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 22:15:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][0/2502]	eta 20:31:08 lr 0.000026	 wd 0.0000	time 29.5236 (29.5236)	loss 1.4696 (1.4696)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:15:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:21:32 lr 0.000026	 wd 0.0000	time 0.1773 (0.5383)	loss 1.0398 (1.3592)	grad_norm 0.4031 (0.4808)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:16:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:24 lr 0.000026	 wd 0.0000	time 0.2135 (0.3755)	loss 1.4648 (1.3743)	grad_norm 0.4482 (0.4656)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:16:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:11:49 lr 0.000026	 wd 0.0000	time 0.2118 (0.3223)	loss 1.4090 (1.3609)	grad_norm 0.3903 (0.4602)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:17:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:10:52 lr 0.000026	 wd 0.0000	time 0.2128 (0.3104)	loss 1.4012 (1.3641)	grad_norm 0.6719 (0.4543)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:17:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:09:43 lr 0.000026	 wd 0.0000	time 0.1965 (0.2914)	loss 0.8739 (1.3622)	grad_norm 0.4101 (0.4509)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:17:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:08:51 lr 0.000026	 wd 0.0000	time 0.2166 (0.2792)	loss 0.8533 (1.3656)	grad_norm 0.4403 (0.4495)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:18:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:08:07 lr 0.000026	 wd 0.0000	time 0.1963 (0.2705)	loss 1.1103 (1.3683)	grad_norm 0.4255 (0.4504)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:18:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:07:35 lr 0.000026	 wd 0.0000	time 0.2383 (0.2677)	loss 1.4291 (1.3687)	grad_norm 0.4455 (0.4509)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:18:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:07:03 lr 0.000025	 wd 0.0000	time 0.2262 (0.2643)	loss 1.2383 (1.3694)	grad_norm 0.5135 (0.4512)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:19:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:06:30 lr 0.000025	 wd 0.0000	time 0.2119 (0.2597)	loss 1.4311 (1.3707)	grad_norm 0.4055 (0.4512)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:19:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:05:58 lr 0.000025	 wd 0.0000	time 0.1963 (0.2560)	loss 1.2654 (1.3678)	grad_norm 0.6285 (0.4502)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:20:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:05:31 lr 0.000025	 wd 0.0000	time 0.2290 (0.2543)	loss 1.4058 (1.3685)	grad_norm 0.5029 (0.4500)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:20:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:05:05 lr 0.000025	 wd 0.0000	time 0.2009 (0.2539)	loss 1.3827 (1.3668)	grad_norm 0.4079 (0.4509)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:20:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:04:37 lr 0.000025	 wd 0.0000	time 0.2212 (0.2514)	loss 1.4584 (1.3670)	grad_norm 0.4118 (0.4509)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:21:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:04:09 lr 0.000025	 wd 0.0000	time 0.2023 (0.2492)	loss 1.5823 (1.3667)	grad_norm 0.4499 (0.4514)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:21:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:03:43 lr 0.000025	 wd 0.0000	time 0.2123 (0.2479)	loss 1.2880 (1.3690)	grad_norm 0.4083 (0.4514)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:21:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:03:18 lr 0.000025	 wd 0.0000	time 0.2024 (0.2480)	loss 1.4435 (1.3681)	grad_norm 0.4377 (0.4515)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:22:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:02:53 lr 0.000025	 wd 0.0000	time 0.2121 (0.2466)	loss 1.0844 (1.3678)	grad_norm 0.4531 (0.4511)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:22:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:27 lr 0.000024	 wd 0.0000	time 0.2183 (0.2453)	loss 1.6255 (1.3683)	grad_norm 0.4198 (0.4517)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:23:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:02 lr 0.000024	 wd 0.0000	time 0.2506 (0.2442)	loss 1.8653 (1.3705)	grad_norm 0.4991 (0.4520)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:23:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:38 lr 0.000024	 wd 0.0000	time 0.2199 (0.2447)	loss 1.5337 (1.3713)	grad_norm 0.4177 (0.4516)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:23:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:13 lr 0.000024	 wd 0.0000	time 0.2156 (0.2442)	loss 1.0777 (1.3725)	grad_norm 0.4046 (0.4510)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:24:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:49 lr 0.000024	 wd 0.0000	time 0.2601 (0.2433)	loss 1.5926 (1.3726)	grad_norm 0.6235 (0.4513)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:24:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:24 lr 0.000024	 wd 0.0000	time 0.2111 (0.2423)	loss 1.0782 (1.3714)	grad_norm 0.4544 (nan)	loss_scale 2048.0000 (4053.3511)	mem 7723MB
[2024-07-09 22:24:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.1729 (0.2409)	loss 1.2256 (1.3715)	grad_norm 0.3987 (nan)	loss_scale 2048.0000 (3973.1691)	mem 7723MB
[2024-07-09 22:25:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 15 training takes 0:10:08
[2024-07-09 22:25:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_15.pth saving......
[2024-07-09 22:25:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_15.pth saved !!!
[2024-07-09 22:25:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 32.135 (32.135)	Loss 0.4219 (0.4219)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 22:25:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.976 Acc@5 97.222
[2024-07-09 22:25:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 22:25:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.98%
[2024-07-09 22:25:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 22:25:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 22:26:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][0/2502]	eta 10:31:07 lr 0.000024	 wd 0.0000	time 15.1349 (15.1349)	loss 1.2795 (1.2795)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:26:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:16:12 lr 0.000024	 wd 0.0000	time 0.3284 (0.4047)	loss 1.4771 (1.3649)	grad_norm 0.5046 (0.4631)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:27:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:13:32 lr 0.000024	 wd 0.0000	time 0.2283 (0.3530)	loss 1.3262 (1.3737)	grad_norm 0.4768 (0.4610)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:27:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:11:15 lr 0.000024	 wd 0.0000	time 0.2326 (0.3070)	loss 1.3680 (1.3863)	grad_norm 0.4197 (0.4581)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:27:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:09:56 lr 0.000024	 wd 0.0000	time 0.1819 (0.2835)	loss 1.2793 (1.3803)	grad_norm 0.4955 (0.4554)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:28:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:09:10 lr 0.000023	 wd 0.0000	time 0.2902 (0.2748)	loss 1.2688 (1.3907)	grad_norm 0.4150 (0.4545)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:28:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:08:35 lr 0.000023	 wd 0.0000	time 0.2106 (0.2712)	loss 1.0434 (1.3830)	grad_norm 0.4367 (0.4558)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:28:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:07:55 lr 0.000023	 wd 0.0000	time 0.1998 (0.2639)	loss 1.3416 (1.3846)	grad_norm 0.5373 (0.4561)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:29:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:07:19 lr 0.000023	 wd 0.0000	time 0.2154 (0.2585)	loss 1.3987 (1.3790)	grad_norm 0.3975 (0.4547)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:29:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:06:49 lr 0.000023	 wd 0.0000	time 0.2641 (0.2556)	loss 1.5680 (1.3767)	grad_norm 0.4205 (0.4571)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:30:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:06:23 lr 0.000023	 wd 0.0000	time 0.1894 (0.2551)	loss 1.5689 (1.3769)	grad_norm 0.4163 (0.4565)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:30:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:05:53 lr 0.000023	 wd 0.0000	time 0.1797 (0.2521)	loss 1.3519 (1.3781)	grad_norm 0.4315 (0.4561)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:30:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:05:24 lr 0.000023	 wd 0.0000	time 0.2092 (0.2495)	loss 1.4201 (1.3792)	grad_norm 0.4458 (0.4553)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:31:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:04:57 lr 0.000023	 wd 0.0000	time 0.2579 (0.2473)	loss 1.1127 (1.3798)	grad_norm 0.4431 (0.4562)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:31:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:04:32 lr 0.000023	 wd 0.0000	time 0.2329 (0.2475)	loss 1.2101 (1.3771)	grad_norm 0.4267 (0.4574)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:32:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:04:06 lr 0.000022	 wd 0.0000	time 0.2103 (0.2461)	loss 1.4032 (1.3740)	grad_norm 0.4247 (0.4579)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:32:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:03:40 lr 0.000022	 wd 0.0000	time 0.1916 (0.2446)	loss 1.3947 (1.3743)	grad_norm 0.4668 (0.4582)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:32:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:03:15 lr 0.000022	 wd 0.0000	time 0.2107 (0.2433)	loss 1.1162 (1.3723)	grad_norm 0.4870 (0.4570)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:33:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:02:51 lr 0.000022	 wd 0.0000	time 0.4043 (0.2441)	loss 1.2451 (1.3705)	grad_norm 0.5775 (0.4571)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:33:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:26 lr 0.000022	 wd 0.0000	time 0.2068 (0.2434)	loss 1.2085 (1.3694)	grad_norm 0.4276 (0.4571)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:33:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:01 lr 0.000022	 wd 0.0000	time 0.2392 (0.2424)	loss 1.4872 (1.3697)	grad_norm 0.4072 (0.4590)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:34:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:36 lr 0.000022	 wd 0.0000	time 0.2101 (0.2412)	loss 1.3589 (1.3693)	grad_norm 0.4442 (0.4589)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:34:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:12 lr 0.000022	 wd 0.0000	time 0.2292 (0.2409)	loss 1.4820 (1.3699)	grad_norm 0.4347 (0.4606)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:35:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:48 lr 0.000022	 wd 0.0000	time 0.2109 (0.2411)	loss 1.2338 (1.3699)	grad_norm 0.4430 (0.4598)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:35:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:24 lr 0.000022	 wd 0.0000	time 0.2219 (0.2403)	loss 1.5498 (1.3701)	grad_norm 0.5636 (0.4597)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:35:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.1800 (0.2383)	loss 1.3865 (1.3700)	grad_norm 0.4043 (0.4592)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:35:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 16 training takes 0:10:00
[2024-07-09 22:36:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 44.945 (44.945)	Loss 0.4216 (0.4216)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 22:36:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.966 Acc@5 97.208
[2024-07-09 22:36:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 22:36:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 83.98%
[2024-07-09 22:37:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][0/2502]	eta 10:53:33 lr 0.000021	 wd 0.0000	time 15.6728 (15.6728)	loss 1.4789 (1.4789)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:37:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:15:03 lr 0.000021	 wd 0.0000	time 0.2153 (0.3761)	loss 1.5632 (1.3427)	grad_norm 0.4297 (0.4587)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:38:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:00 lr 0.000021	 wd 0.0000	time 0.6380 (0.3652)	loss 1.5935 (1.3584)	grad_norm 0.4631 (0.4723)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:38:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:12:04 lr 0.000021	 wd 0.0000	time 0.2039 (0.3289)	loss 1.3450 (1.3551)	grad_norm 0.4607 (0.4706)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:38:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:10:30 lr 0.000021	 wd 0.0000	time 0.2036 (0.3001)	loss 1.1291 (1.3494)	grad_norm 0.4155 (0.4637)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:39:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:09:24 lr 0.000021	 wd 0.0000	time 0.2278 (0.2821)	loss 1.3725 (1.3493)	grad_norm 0.4245 (0.4623)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:39:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:09:05 lr 0.000021	 wd 0.0000	time 0.2231 (0.2870)	loss 1.1888 (1.3563)	grad_norm 0.4207 (0.4597)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:40:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:08:21 lr 0.000021	 wd 0.0000	time 0.2015 (0.2781)	loss 1.3288 (1.3621)	grad_norm 0.4220 (0.4572)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:40:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:07:40 lr 0.000021	 wd 0.0000	time 0.1936 (0.2706)	loss 1.3973 (1.3659)	grad_norm 0.4374 (0.4580)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:40:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:07:03 lr 0.000021	 wd 0.0000	time 0.2193 (0.2646)	loss 1.6129 (1.3673)	grad_norm 0.4632 (0.4565)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:41:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:06:34 lr 0.000020	 wd 0.0000	time 0.2573 (0.2629)	loss 1.5501 (1.3657)	grad_norm 0.4448 (0.4561)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:41:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:06:05 lr 0.000020	 wd 0.0000	time 0.2378 (0.2605)	loss 1.1272 (1.3661)	grad_norm 0.4298 (0.4550)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:41:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:05:34 lr 0.000020	 wd 0.0000	time 0.2155 (0.2570)	loss 1.6136 (1.3679)	grad_norm 0.4029 (0.4542)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:42:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:05:05 lr 0.000020	 wd 0.0000	time 0.2022 (0.2541)	loss 1.1716 (1.3679)	grad_norm 0.4473 (0.4548)	loss_scale 2048.0000 (2048.0000)	mem 7723MB
[2024-07-09 22:42:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:04:38 lr 0.000020	 wd 0.0000	time 0.2329 (0.2528)	loss 1.4943 (1.3687)	grad_norm 0.4695 (0.4557)	loss_scale 4096.0000 (2126.9379)	mem 7723MB
[2024-07-09 22:43:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:04:12 lr 0.000020	 wd 0.0000	time 0.1909 (0.2525)	loss 1.3934 (1.3694)	grad_norm 0.4761 (0.4564)	loss_scale 4096.0000 (2258.1213)	mem 7723MB
[2024-07-09 22:43:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:03:45 lr 0.000020	 wd 0.0000	time 0.2102 (0.2504)	loss 1.5559 (1.3706)	grad_norm 0.6849 (0.4557)	loss_scale 4096.0000 (2372.9169)	mem 7723MB
[2024-07-09 22:43:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:19 lr 0.000020	 wd 0.0000	time 0.1925 (0.2488)	loss 1.5871 (1.3726)	grad_norm 0.4399 (0.4554)	loss_scale 4096.0000 (2474.2152)	mem 7723MB
[2024-07-09 22:44:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:02:54 lr 0.000020	 wd 0.0000	time 0.2494 (0.2480)	loss 1.3245 (1.3734)	grad_norm 0.4340 (0.4554)	loss_scale 4096.0000 (2564.2643)	mem 7723MB
[2024-07-09 22:44:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:29 lr 0.000020	 wd 0.0000	time 0.2144 (0.2479)	loss 1.1624 (1.3742)	grad_norm 0.4348 (0.4550)	loss_scale 4096.0000 (2644.8396)	mem 7723MB
[2024-07-09 22:45:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:03 lr 0.000019	 wd 0.0000	time 0.2002 (0.2468)	loss 1.5709 (1.3721)	grad_norm 0.4950 (0.4538)	loss_scale 4096.0000 (2717.3613)	mem 7723MB
[2024-07-09 22:45:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:38 lr 0.000019	 wd 0.0000	time 0.2061 (0.2456)	loss 1.4532 (1.3718)	grad_norm 0.4192 (0.4547)	loss_scale 4096.0000 (2782.9795)	mem 7723MB
[2024-07-09 22:45:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:13 lr 0.000019	 wd 0.0000	time 0.2888 (0.2448)	loss 1.3389 (1.3729)	grad_norm 0.3943 (0.4546)	loss_scale 4096.0000 (2842.6352)	mem 7723MB
[2024-07-09 22:46:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:49 lr 0.000019	 wd 0.0000	time 0.1964 (0.2450)	loss 1.4772 (1.3723)	grad_norm 1.2451 (0.4549)	loss_scale 4096.0000 (2897.1056)	mem 7723MB
[2024-07-09 22:46:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:24 lr 0.000019	 wd 0.0000	time 0.1950 (0.2442)	loss 1.4681 (1.3709)	grad_norm 0.4201 (0.4550)	loss_scale 4096.0000 (2947.0387)	mem 7723MB
[2024-07-09 22:46:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0000	time 0.1750 (0.2423)	loss 1.5075 (1.3714)	grad_norm 0.4136 (0.4549)	loss_scale 4096.0000 (2992.9788)	mem 7723MB
[2024-07-09 22:46:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 17 training takes 0:10:10
[2024-07-09 22:47:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 22.763 (22.763)	Loss 0.4214 (0.4214)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 22:47:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.010 Acc@5 97.222
[2024-07-09 22:47:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 22:47:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.01%
[2024-07-09 22:47:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 22:47:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 22:48:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][0/2502]	eta 13:35:58 lr 0.000019	 wd 0.0000	time 19.5677 (19.5677)	loss 1.6115 (1.6115)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:48:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:16:35 lr 0.000019	 wd 0.0000	time 0.2388 (0.4144)	loss 1.5634 (1.4108)	grad_norm 0.4073 (0.4677)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:48:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:12:04 lr 0.000019	 wd 0.0000	time 0.1834 (0.3147)	loss 1.4870 (1.3911)	grad_norm 0.4108 (0.4578)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:49:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:10:35 lr 0.000019	 wd 0.0000	time 0.2024 (0.2887)	loss 1.7257 (1.3905)	grad_norm 0.4216 (0.4600)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:49:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:10:02 lr 0.000019	 wd 0.0000	time 0.2207 (0.2867)	loss 1.4611 (1.3839)	grad_norm 0.4483 (0.4560)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:49:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:09:04 lr 0.000018	 wd 0.0000	time 0.2106 (0.2718)	loss 1.3156 (1.3735)	grad_norm 0.4422 (0.4556)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:50:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:08:18 lr 0.000018	 wd 0.0000	time 0.2293 (0.2622)	loss 1.6393 (1.3743)	grad_norm 0.4295 (0.4519)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:50:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:07:43 lr 0.000018	 wd 0.0000	time 0.2606 (0.2574)	loss 1.4468 (1.3749)	grad_norm 0.4031 (0.4525)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:51:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:07:24 lr 0.000018	 wd 0.0000	time 0.2052 (0.2614)	loss 1.1174 (1.3722)	grad_norm 0.4275 (0.4518)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:51:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:06:50 lr 0.000018	 wd 0.0000	time 0.1956 (0.2563)	loss 1.1870 (1.3683)	grad_norm 0.4765 (0.4533)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:51:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:06:19 lr 0.000018	 wd 0.0000	time 0.2052 (0.2525)	loss 1.0509 (1.3719)	grad_norm 0.4197 (0.4544)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:52:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:05:50 lr 0.000018	 wd 0.0000	time 0.2100 (0.2501)	loss 1.2750 (1.3714)	grad_norm 0.5148 (0.4553)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:52:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:05:26 lr 0.000018	 wd 0.0000	time 0.2460 (0.2509)	loss 1.6782 (1.3718)	grad_norm 0.4220 (0.4550)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:53:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:04:59 lr 0.000018	 wd 0.0000	time 0.1974 (0.2488)	loss 1.4347 (1.3724)	grad_norm 0.4679 (0.4559)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:53:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:04:31 lr 0.000018	 wd 0.0000	time 0.2296 (0.2467)	loss 1.2376 (1.3722)	grad_norm 0.4346 (0.4568)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:53:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:04:05 lr 0.000017	 wd 0.0000	time 0.2561 (0.2450)	loss 1.5993 (1.3713)	grad_norm 0.4412 (0.4563)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:54:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:03:42 lr 0.000017	 wd 0.0000	time 0.2130 (0.2462)	loss 1.4931 (1.3707)	grad_norm 0.5056 (0.4559)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:54:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:03:16 lr 0.000017	 wd 0.0000	time 0.2183 (0.2451)	loss 1.4888 (1.3709)	grad_norm 0.4546 (0.4554)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:55:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:02:51 lr 0.000017	 wd 0.0000	time 0.2272 (0.2439)	loss 1.3325 (1.3717)	grad_norm 0.4118 (0.4563)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:55:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:26 lr 0.000017	 wd 0.0000	time 0.2553 (0.2429)	loss 1.4854 (1.3709)	grad_norm 0.4054 (0.4552)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:55:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:01 lr 0.000017	 wd 0.0000	time 0.2478 (0.2426)	loss 1.3845 (1.3731)	grad_norm 0.4077 (0.4566)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:56:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:37 lr 0.000017	 wd 0.0000	time 0.2200 (0.2426)	loss 1.4589 (1.3723)	grad_norm 0.4331 (0.4564)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:56:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:12 lr 0.000017	 wd 0.0000	time 0.2098 (0.2416)	loss 1.3603 (1.3723)	grad_norm 0.3965 (0.4566)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:56:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:48 lr 0.000017	 wd 0.0000	time 0.2304 (0.2406)	loss 1.5009 (1.3726)	grad_norm 0.4294 (0.4581)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:57:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:24 lr 0.000017	 wd 0.0000	time 0.2439 (0.2405)	loss 1.5300 (1.3740)	grad_norm 0.4276 (0.4580)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:57:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.1826 (0.2394)	loss 1.5049 (1.3735)	grad_norm 0.4224 (0.4576)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:57:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 18 training takes 0:10:05
[2024-07-09 22:58:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 20.762 (20.762)	Loss 0.4209 (0.4209)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 22:58:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.960 Acc@5 97.228
[2024-07-09 22:58:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 22:58:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.01%
[2024-07-09 22:58:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][0/2502]	eta 12:03:05 lr 0.000016	 wd 0.0000	time 17.3403 (17.3403)	loss 1.4475 (1.4475)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:59:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:08 lr 0.000016	 wd 0.0000	time 0.1937 (0.4280)	loss 1.1027 (1.3850)	grad_norm 0.6803 (0.4527)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:59:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:12:22 lr 0.000016	 wd 0.0000	time 0.1926 (0.3227)	loss 1.4495 (1.3783)	grad_norm 0.4673 (0.4520)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 22:59:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:10:31 lr 0.000016	 wd 0.0000	time 0.2498 (0.2866)	loss 1.3154 (1.3907)	grad_norm 0.4237 (0.4534)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:00:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:09:24 lr 0.000016	 wd 0.0000	time 0.1878 (0.2685)	loss 1.5213 (1.3901)	grad_norm 0.4310 (0.4507)	loss_scale 8192.0000 (4688.4389)	mem 7723MB
[2024-07-09 23:00:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:08:43 lr 0.000016	 wd 0.0000	time 0.2435 (0.2617)	loss 1.5244 (1.3853)	grad_norm 0.4570 (0.4515)	loss_scale 8192.0000 (5387.7525)	mem 7723MB
[2024-07-09 23:00:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:08:10 lr 0.000016	 wd 0.0000	time 0.2216 (0.2578)	loss 1.4771 (1.3802)	grad_norm 0.4299 (nan)	loss_scale 4096.0000 (5445.4309)	mem 7723MB
[2024-07-09 23:01:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:07:34 lr 0.000016	 wd 0.0000	time 0.1953 (0.2525)	loss 1.6541 (1.3823)	grad_norm 0.4385 (nan)	loss_scale 4096.0000 (5252.9301)	mem 7723MB
[2024-07-09 23:01:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:07:01 lr 0.000016	 wd 0.0000	time 0.2188 (0.2479)	loss 1.3704 (1.3812)	grad_norm 0.4582 (nan)	loss_scale 4096.0000 (5108.4944)	mem 7723MB
[2024-07-09 23:02:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:06:33 lr 0.000016	 wd 0.0000	time 0.2370 (0.2458)	loss 1.2509 (1.3808)	grad_norm 0.4075 (nan)	loss_scale 4096.0000 (4996.1199)	mem 7723MB
[2024-07-09 23:02:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:06:09 lr 0.000016	 wd 0.0000	time 0.2144 (0.2463)	loss 1.5977 (1.3815)	grad_norm 0.4609 (nan)	loss_scale 4096.0000 (4906.1978)	mem 7723MB
[2024-07-09 23:02:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:05:42 lr 0.000015	 wd 0.0000	time 0.2209 (0.2439)	loss 0.9301 (1.3801)	grad_norm 0.4076 (nan)	loss_scale 4096.0000 (4832.6104)	mem 7723MB
[2024-07-09 23:03:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:05:14 lr 0.000015	 wd 0.0000	time 0.2138 (0.2418)	loss 1.1284 (1.3783)	grad_norm 0.4732 (nan)	loss_scale 4096.0000 (4771.2773)	mem 7723MB
[2024-07-09 23:03:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:04:48 lr 0.000015	 wd 0.0000	time 0.2131 (0.2401)	loss 1.5767 (1.3787)	grad_norm 0.4259 (nan)	loss_scale 4096.0000 (4719.3728)	mem 7723MB
[2024-07-09 23:03:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:04:25 lr 0.000015	 wd 0.0000	time 0.2134 (0.2407)	loss 1.4690 (1.3797)	grad_norm 0.4247 (nan)	loss_scale 4096.0000 (4674.8779)	mem 7723MB
[2024-07-09 23:04:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:04:00 lr 0.000015	 wd 0.0000	time 0.2181 (0.2400)	loss 1.2260 (1.3795)	grad_norm 0.3879 (nan)	loss_scale 4096.0000 (4636.3118)	mem 7723MB
[2024-07-09 23:04:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:03:35 lr 0.000015	 wd 0.0000	time 0.2311 (0.2387)	loss 1.3205 (1.3797)	grad_norm 0.4487 (nan)	loss_scale 4096.0000 (4602.5634)	mem 7723MB
[2024-07-09 23:05:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:03:10 lr 0.000015	 wd 0.0000	time 0.2398 (0.2375)	loss 1.4107 (1.3805)	grad_norm 0.4427 (nan)	loss_scale 4096.0000 (4572.7831)	mem 7723MB
[2024-07-09 23:05:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:02:46 lr 0.000015	 wd 0.0000	time 0.2469 (0.2375)	loss 1.3405 (1.3799)	grad_norm 0.5271 (nan)	loss_scale 4096.0000 (4546.3098)	mem 7723MB
[2024-07-09 23:05:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:23 lr 0.000015	 wd 0.0000	time 0.1969 (0.2383)	loss 1.4319 (1.3803)	grad_norm 0.4027 (nan)	loss_scale 4096.0000 (4522.6218)	mem 7723MB
[2024-07-09 23:06:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:01:59 lr 0.000015	 wd 0.0000	time 0.2211 (0.2375)	loss 1.0909 (1.3798)	grad_norm 0.5697 (nan)	loss_scale 4096.0000 (4501.3013)	mem 7723MB
[2024-07-09 23:06:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:35 lr 0.000014	 wd 0.0000	time 0.2183 (0.2367)	loss 1.1974 (1.3787)	grad_norm 0.4577 (nan)	loss_scale 4096.0000 (4482.0105)	mem 7723MB
[2024-07-09 23:07:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:11 lr 0.000014	 wd 0.0000	time 0.2233 (0.2366)	loss 1.4926 (1.3765)	grad_norm 0.4826 (nan)	loss_scale 4096.0000 (4464.4725)	mem 7723MB
[2024-07-09 23:07:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:47 lr 0.000014	 wd 0.0000	time 0.2680 (0.2368)	loss 1.4121 (1.3761)	grad_norm 0.4898 (nan)	loss_scale 4096.0000 (4448.4589)	mem 7723MB
[2024-07-09 23:07:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:24 lr 0.000014	 wd 0.0000	time 0.2428 (0.2362)	loss 1.5723 (1.3776)	grad_norm 0.4639 (nan)	loss_scale 4096.0000 (4433.7793)	mem 7723MB
[2024-07-09 23:08:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.1680 (0.2343)	loss 1.2368 (1.3774)	grad_norm 0.4342 (nan)	loss_scale 4096.0000 (4420.2735)	mem 7723MB
[2024-07-09 23:08:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 19 training takes 0:09:50
[2024-07-09 23:08:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 30.372 (30.372)	Loss 0.4197 (0.4197)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 23:08:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.984 Acc@5 97.208
[2024-07-09 23:08:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 23:08:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.01%
[2024-07-09 23:09:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][0/2502]	eta 11:32:43 lr 0.000014	 wd 0.0000	time 16.6122 (16.6122)	loss 1.0144 (1.0144)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:09:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:15:12 lr 0.000014	 wd 0.0000	time 0.2037 (0.3800)	loss 1.5981 (1.3732)	grad_norm 0.4187 (0.4475)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:09:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:11:25 lr 0.000014	 wd 0.0000	time 0.2187 (0.2977)	loss 1.4478 (1.3746)	grad_norm 0.5439 (0.4467)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:10:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:10:51 lr 0.000014	 wd 0.0000	time 0.2362 (0.2957)	loss 1.4877 (1.3711)	grad_norm 0.4447 (0.4521)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:10:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:09:39 lr 0.000014	 wd 0.0000	time 0.1994 (0.2759)	loss 1.4156 (1.3803)	grad_norm 0.4174 (0.4513)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:11:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:08:47 lr 0.000014	 wd 0.0000	time 0.1918 (0.2634)	loss 1.5410 (1.3823)	grad_norm 0.4137 (0.4516)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:11:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:08:06 lr 0.000014	 wd 0.0000	time 0.2191 (0.2558)	loss 1.3367 (1.3787)	grad_norm 0.4617 (0.4523)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:11:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:07:38 lr 0.000013	 wd 0.0000	time 0.2480 (0.2546)	loss 1.4914 (1.3809)	grad_norm 0.4648 (0.4526)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:12:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:07:10 lr 0.000013	 wd 0.0000	time 0.2353 (0.2529)	loss 1.2742 (1.3780)	grad_norm 0.3933 (0.4518)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:12:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:06:39 lr 0.000013	 wd 0.0000	time 0.2086 (0.2491)	loss 1.5952 (1.3806)	grad_norm 0.4144 (0.4544)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:13:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:06:09 lr 0.000013	 wd 0.0000	time 0.2016 (0.2459)	loss 1.3053 (1.3797)	grad_norm 0.4035 (0.4541)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:13:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:05:43 lr 0.000013	 wd 0.0000	time 0.2453 (0.2452)	loss 1.3580 (1.3773)	grad_norm 0.4438 (0.4547)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:13:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:05:18 lr 0.000013	 wd 0.0000	time 0.1792 (0.2446)	loss 1.4339 (1.3742)	grad_norm 0.4189 (0.4541)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:14:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:04:51 lr 0.000013	 wd 0.0000	time 0.2240 (0.2427)	loss 1.3213 (1.3729)	grad_norm 0.4397 (0.4542)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:14:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:04:25 lr 0.000013	 wd 0.0000	time 0.2188 (0.2410)	loss 1.4127 (1.3747)	grad_norm 0.4248 (0.4553)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:14:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:04:00 lr 0.000013	 wd 0.0000	time 0.2150 (0.2401)	loss 1.4473 (1.3752)	grad_norm 0.4924 (0.4588)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:15:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:03:37 lr 0.000013	 wd 0.0000	time 0.2178 (0.2412)	loss 1.7137 (1.3755)	grad_norm 0.4703 (0.4585)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:15:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:03:12 lr 0.000012	 wd 0.0000	time 0.1918 (0.2403)	loss 1.5379 (1.3748)	grad_norm 0.4469 (0.4583)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:16:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:02:48 lr 0.000012	 wd 0.0000	time 0.1848 (0.2393)	loss 1.3057 (1.3740)	grad_norm 0.4288 (0.4579)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:16:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:23 lr 0.000012	 wd 0.0000	time 0.2250 (0.2388)	loss 1.5391 (1.3759)	grad_norm 0.3949 (0.4579)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:16:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:00 lr 0.000012	 wd 0.0000	time 0.2215 (0.2402)	loss 1.5084 (1.3755)	grad_norm 0.4409 (0.4582)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:17:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:36 lr 0.000012	 wd 0.0000	time 0.1959 (0.2395)	loss 1.3232 (1.3761)	grad_norm 0.4055 (0.4575)	loss_scale 8192.0000 (4216.8720)	mem 7723MB
[2024-07-09 23:17:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:12 lr 0.000012	 wd 0.0000	time 0.1906 (0.2386)	loss 1.2545 (1.3777)	grad_norm 0.4416 (0.4571)	loss_scale 8192.0000 (4397.4775)	mem 7723MB
[2024-07-09 23:18:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:48 lr 0.000012	 wd 0.0000	time 0.2356 (0.2379)	loss 1.4095 (1.3776)	grad_norm 0.4153 (0.4568)	loss_scale 8192.0000 (4562.3850)	mem 7723MB
[2024-07-09 23:18:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:24 lr 0.000012	 wd 0.0000	time 0.1900 (0.2379)	loss 1.3646 (1.3779)	grad_norm 0.4464 (nan)	loss_scale 4096.0000 (4641.9059)	mem 7723MB
[2024-07-09 23:18:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0000	time 0.1589 (0.2362)	loss 1.6470 (1.3786)	grad_norm 0.4806 (nan)	loss_scale 4096.0000 (4620.0784)	mem 7723MB
[2024-07-09 23:18:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 20 training takes 0:09:57
[2024-07-09 23:19:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 19.205 (19.205)	Loss 0.4204 (0.4204)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 23:19:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.950 Acc@5 97.228
[2024-07-09 23:19:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 23:19:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.01%
[2024-07-09 23:19:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][0/2502]	eta 14:30:48 lr 0.000012	 wd 0.0000	time 20.8828 (20.8828)	loss 1.5786 (1.5786)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:20:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:18:13 lr 0.000012	 wd 0.0000	time 0.1953 (0.4553)	loss 1.4723 (1.3985)	grad_norm 0.6770 (0.4560)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:20:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:12:52 lr 0.000012	 wd 0.0000	time 0.1947 (0.3356)	loss 0.8997 (1.3782)	grad_norm 0.4408 (0.4660)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:20:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:10:50 lr 0.000012	 wd 0.0000	time 0.1842 (0.2955)	loss 1.4083 (1.3711)	grad_norm 0.4632 (0.4685)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:21:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:09:39 lr 0.000011	 wd 0.0000	time 0.2276 (0.2756)	loss 1.4537 (1.3724)	grad_norm 0.4229 (0.4685)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:21:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:09:01 lr 0.000011	 wd 0.0000	time 0.2641 (0.2705)	loss 1.3642 (1.3716)	grad_norm 0.4342 (0.4632)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:22:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:08:21 lr 0.000011	 wd 0.0000	time 0.2038 (0.2635)	loss 1.2067 (1.3723)	grad_norm 0.4267 (0.4612)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:22:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:07:42 lr 0.000011	 wd 0.0000	time 0.2128 (0.2569)	loss 1.3013 (1.3668)	grad_norm 0.4027 (0.4612)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:22:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:07:08 lr 0.000011	 wd 0.0000	time 0.2009 (0.2518)	loss 1.2317 (1.3663)	grad_norm 0.4324 (0.4599)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:23:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:06:42 lr 0.000011	 wd 0.0000	time 0.2354 (0.2510)	loss 1.5555 (1.3672)	grad_norm 0.5536 (0.4600)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:23:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:06:14 lr 0.000011	 wd 0.0000	time 0.2139 (0.2495)	loss 1.0630 (1.3664)	grad_norm 0.4388 (0.4583)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:24:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:05:45 lr 0.000011	 wd 0.0000	time 0.2276 (0.2467)	loss 1.4707 (1.3664)	grad_norm 0.4371 (0.4576)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:24:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:05:18 lr 0.000011	 wd 0.0000	time 0.2061 (0.2443)	loss 1.3438 (1.3675)	grad_norm 0.4542 (0.4578)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:24:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:04:52 lr 0.000011	 wd 0.0000	time 0.2609 (0.2436)	loss 1.5297 (1.3676)	grad_norm 0.4246 (0.4578)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:25:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:04:29 lr 0.000011	 wd 0.0000	time 0.2088 (0.2448)	loss 1.4471 (1.3681)	grad_norm 0.4631 (0.4572)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:25:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:04:03 lr 0.000010	 wd 0.0000	time 0.2113 (0.2433)	loss 1.6488 (1.3677)	grad_norm 0.4169 (0.4560)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:25:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:03:38 lr 0.000010	 wd 0.0000	time 0.2505 (0.2419)	loss 1.2178 (1.3671)	grad_norm 0.5217 (0.4569)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:26:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:03:13 lr 0.000010	 wd 0.0000	time 0.2713 (0.2412)	loss 1.4843 (1.3673)	grad_norm 0.4363 (0.4563)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:26:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:02:50 lr 0.000010	 wd 0.0000	time 0.2675 (0.2425)	loss 1.0305 (1.3678)	grad_norm 0.4494 (0.4560)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:27:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:25 lr 0.000010	 wd 0.0000	time 0.2161 (0.2417)	loss 1.0063 (1.3665)	grad_norm 0.4486 (0.4560)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:27:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:00 lr 0.000010	 wd 0.0000	time 0.2369 (0.2406)	loss 1.4133 (1.3660)	grad_norm 0.5280 (0.4571)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:27:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:36 lr 0.000010	 wd 0.0000	time 0.2187 (0.2399)	loss 1.1972 (1.3663)	grad_norm 0.4724 (0.4566)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:28:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:12 lr 0.000010	 wd 0.0000	time 0.4103 (0.2403)	loss 1.5767 (1.3643)	grad_norm 0.4077 (0.4575)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:28:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:48 lr 0.000010	 wd 0.0000	time 0.2008 (0.2395)	loss 1.5818 (1.3640)	grad_norm 0.4256 (0.4572)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:29:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:24 lr 0.000010	 wd 0.0000	time 0.2154 (0.2387)	loss 1.3826 (1.3654)	grad_norm 0.4253 (0.4569)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:29:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.1747 (0.2367)	loss 1.3329 (1.3651)	grad_norm 0.4808 (0.4566)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:29:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 21 training takes 0:09:57
[2024-07-09 23:30:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 39.031 (39.031)	Loss 0.4192 (0.4192)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 23:30:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 83.986 Acc@5 97.214
[2024-07-09 23:30:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 23:30:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.01%
[2024-07-09 23:30:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][0/2502]	eta 11:40:03 lr 0.000010	 wd 0.0000	time 16.7878 (16.7878)	loss 1.3237 (1.3237)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:30:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:14:59 lr 0.000010	 wd 0.0000	time 0.2162 (0.3743)	loss 1.2766 (1.3757)	grad_norm 0.4070 (0.4595)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:31:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:14:02 lr 0.000009	 wd 0.0000	time 0.2743 (0.3658)	loss 1.2016 (1.3725)	grad_norm 0.4193 (0.4601)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:31:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:11:41 lr 0.000009	 wd 0.0000	time 0.1960 (0.3185)	loss 1.5267 (1.3766)	grad_norm 0.4669 (0.4575)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:32:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:10:14 lr 0.000009	 wd 0.0000	time 0.2121 (0.2924)	loss 1.4371 (1.3806)	grad_norm 0.4602 (0.4576)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:32:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:09:14 lr 0.000009	 wd 0.0000	time 0.2645 (0.2772)	loss 0.9086 (1.3796)	grad_norm 0.4211 (0.4577)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:33:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:08:41 lr 0.000009	 wd 0.0000	time 0.2315 (0.2740)	loss 1.3695 (1.3827)	grad_norm 0.4714 (0.4570)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:33:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:08:00 lr 0.000009	 wd 0.0000	time 0.1858 (0.2668)	loss 0.9039 (1.3781)	grad_norm 0.5256 (0.4575)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:33:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:07:24 lr 0.000009	 wd 0.0000	time 0.2046 (0.2609)	loss 1.3853 (1.3739)	grad_norm 0.4096 (0.4564)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:34:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:06:49 lr 0.000009	 wd 0.0000	time 0.1978 (0.2559)	loss 1.3626 (1.3730)	grad_norm 0.4946 (0.4547)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:34:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:06:22 lr 0.000009	 wd 0.0000	time 0.2484 (0.2547)	loss 1.5656 (1.3752)	grad_norm 0.4655 (0.4559)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:35:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:05:55 lr 0.000009	 wd 0.0000	time 0.2081 (0.2537)	loss 1.4080 (1.3728)	grad_norm 0.4869 (0.4599)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:35:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:05:26 lr 0.000009	 wd 0.0000	time 0.2051 (0.2506)	loss 1.3510 (1.3715)	grad_norm 0.4190 (0.4585)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:35:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:04:58 lr 0.000009	 wd 0.0000	time 0.2276 (0.2480)	loss 0.9330 (1.3720)	grad_norm 0.4313 (0.4575)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:36:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:04:32 lr 0.000008	 wd 0.0000	time 0.2362 (0.2473)	loss 1.5805 (1.3741)	grad_norm 0.4460 (0.4592)	loss_scale 8192.0000 (4230.4868)	mem 7723MB
[2024-07-09 23:36:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:04:07 lr 0.000008	 wd 0.0000	time 0.2163 (0.2471)	loss 1.3232 (1.3748)	grad_norm 0.4350 (0.4592)	loss_scale 8192.0000 (4494.4117)	mem 7723MB
[2024-07-09 23:36:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:03:41 lr 0.000008	 wd 0.0000	time 0.2168 (0.2454)	loss 1.5533 (1.3750)	grad_norm 0.4457 (0.4585)	loss_scale 8192.0000 (4725.3666)	mem 7723MB
[2024-07-09 23:37:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:03:15 lr 0.000008	 wd 0.0000	time 0.2054 (0.2439)	loss 1.3214 (1.3747)	grad_norm 0.4242 (0.4588)	loss_scale 8192.0000 (4929.1664)	mem 7723MB
[2024-07-09 23:37:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:02:50 lr 0.000008	 wd 0.0000	time 0.2066 (0.2430)	loss 1.5976 (1.3740)	grad_norm 0.4530 (0.4581)	loss_scale 8192.0000 (5110.3343)	mem 7723MB
[2024-07-09 23:38:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:26 lr 0.000008	 wd 0.0000	time 0.1986 (0.2436)	loss 1.5619 (1.3734)	grad_norm 0.5235 (0.4575)	loss_scale 8192.0000 (5272.4419)	mem 7723MB
[2024-07-09 23:38:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:01 lr 0.000008	 wd 0.0000	time 0.1889 (0.2427)	loss 1.5193 (1.3725)	grad_norm 0.4542 (0.4577)	loss_scale 8192.0000 (5418.3468)	mem 7723MB
[2024-07-09 23:38:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:37 lr 0.000008	 wd 0.0000	time 0.1922 (0.2417)	loss 1.4456 (1.3730)	grad_norm 0.5108 (0.4577)	loss_scale 8192.0000 (5550.3627)	mem 7723MB
[2024-07-09 23:39:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:12 lr 0.000008	 wd 0.0000	time 0.2535 (0.2410)	loss 1.4485 (1.3712)	grad_norm 0.4270 (0.4581)	loss_scale 8192.0000 (5670.3826)	mem 7723MB
[2024-07-09 23:39:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:48 lr 0.000008	 wd 0.0000	time 0.1933 (0.2409)	loss 1.5067 (1.3713)	grad_norm 0.4724 (0.4580)	loss_scale 8192.0000 (5779.9704)	mem 7723MB
[2024-07-09 23:39:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:24 lr 0.000008	 wd 0.0000	time 0.2049 (0.2403)	loss 1.2209 (1.3706)	grad_norm 0.4708 (0.4575)	loss_scale 8192.0000 (5880.4298)	mem 7723MB
[2024-07-09 23:40:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.1613 (0.2383)	loss 0.9974 (1.3697)	grad_norm 0.4394 (0.4574)	loss_scale 8192.0000 (5972.8557)	mem 7723MB
[2024-07-09 23:40:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 22 training takes 0:10:02
[2024-07-09 23:40:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 18.289 (18.289)	Loss 0.4187 (0.4187)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 23:40:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.004 Acc@5 97.212
[2024-07-09 23:40:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 23:40:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.01%
[2024-07-09 23:41:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][0/2502]	eta 1 day, 0:12:56 lr 0.000008	 wd 0.0000	time 34.8425 (34.8425)	loss 1.1717 (1.1717)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:41:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:22:44 lr 0.000008	 wd 0.0000	time 0.2123 (0.5679)	loss 1.5856 (1.4080)	grad_norm 0.4181 (0.4631)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:42:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:14:57 lr 0.000007	 wd 0.0000	time 0.1892 (0.3899)	loss 1.5557 (1.3863)	grad_norm 0.4649 (0.4603)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:42:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:13:32 lr 0.000007	 wd 0.0000	time 0.3181 (0.3689)	loss 1.5188 (1.3797)	grad_norm 0.4479 (0.4550)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:43:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:11:53 lr 0.000007	 wd 0.0000	time 0.2064 (0.3394)	loss 1.5438 (1.3711)	grad_norm 0.4125 (0.4573)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:43:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:10:30 lr 0.000007	 wd 0.0000	time 0.2051 (0.3148)	loss 1.7007 (1.3762)	grad_norm 0.4918 (0.4558)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:43:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:09:28 lr 0.000007	 wd 0.0000	time 0.2254 (0.2987)	loss 1.1522 (1.3760)	grad_norm 0.4091 (0.4549)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:44:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:08:48 lr 0.000007	 wd 0.0000	time 0.2273 (0.2930)	loss 1.4188 (1.3732)	grad_norm 0.4150 (0.4569)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:44:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:08:04 lr 0.000007	 wd 0.0000	time 0.2026 (0.2848)	loss 1.7017 (1.3720)	grad_norm 0.6868 (0.4556)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:45:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:07:24 lr 0.000007	 wd 0.0000	time 0.2026 (0.2774)	loss 1.7144 (1.3685)	grad_norm 0.5020 (0.4552)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:45:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:06:47 lr 0.000007	 wd 0.0000	time 0.1976 (0.2712)	loss 1.1582 (1.3665)	grad_norm 0.4190 (0.4547)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:45:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:06:17 lr 0.000007	 wd 0.0000	time 0.2177 (0.2690)	loss 0.8925 (1.3620)	grad_norm 0.4575 (0.4551)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:46:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:05:47 lr 0.000007	 wd 0.0000	time 0.2372 (0.2672)	loss 1.5772 (1.3638)	grad_norm 0.4450 (0.4552)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:46:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:05:16 lr 0.000007	 wd 0.0000	time 0.2169 (0.2636)	loss 1.4557 (1.3652)	grad_norm 0.4087 (0.4558)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:47:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:04:46 lr 0.000007	 wd 0.0000	time 0.1930 (0.2604)	loss 1.5948 (1.3652)	grad_norm 0.4430 (0.4561)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:47:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:04:18 lr 0.000006	 wd 0.0000	time 0.2290 (0.2583)	loss 1.2845 (1.3674)	grad_norm 0.5947 (0.4574)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:47:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:03:52 lr 0.000006	 wd 0.0000	time 0.2090 (0.2575)	loss 1.1774 (1.3667)	grad_norm 0.4383 (0.4573)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:48:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:03:24 lr 0.000006	 wd 0.0000	time 0.2044 (0.2555)	loss 1.3262 (1.3670)	grad_norm 0.4468 (0.4574)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:48:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:02:58 lr 0.000006	 wd 0.0000	time 0.2061 (0.2536)	loss 1.4463 (1.3664)	grad_norm 0.4508 (0.4594)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:48:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:02:32 lr 0.000006	 wd 0.0000	time 0.2487 (0.2525)	loss 1.2424 (1.3690)	grad_norm 0.4882 (0.4606)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:49:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:06 lr 0.000006	 wd 0.0000	time 0.2319 (0.2527)	loss 1.4617 (1.3677)	grad_norm 0.4214 (0.4610)	loss_scale 8192.0000 (8192.0000)	mem 7723MB
[2024-07-09 23:49:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:41 lr 0.000006	 wd 0.0000	time 0.2062 (0.2515)	loss 1.7139 (1.3682)	grad_norm 0.4703 (inf)	loss_scale 4096.0000 (8117.9172)	mem 7723MB
[2024-07-09 23:50:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:15 lr 0.000006	 wd 0.0000	time 0.2242 (0.2500)	loss 1.4812 (1.3661)	grad_norm 0.4460 (inf)	loss_scale 4096.0000 (7935.1858)	mem 7723MB
[2024-07-09 23:50:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:50 lr 0.000006	 wd 0.0000	time 0.2265 (0.2490)	loss 1.6020 (1.3662)	grad_norm 0.4299 (inf)	loss_scale 4096.0000 (7768.3372)	mem 7723MB
[2024-07-09 23:50:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:25 lr 0.000006	 wd 0.0000	time 0.2227 (0.2488)	loss 1.5573 (1.3668)	grad_norm 0.4205 (inf)	loss_scale 4096.0000 (7615.3869)	mem 7723MB
[2024-07-09 23:51:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0000	time 0.1671 (0.2466)	loss 1.4475 (1.3675)	grad_norm 0.4489 (inf)	loss_scale 4096.0000 (7474.6677)	mem 7723MB
[2024-07-09 23:51:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 23 training takes 0:10:26
[2024-07-09 23:51:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 19.748 (19.748)	Loss 0.4189 (0.4189)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-09 23:52:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.010 Acc@5 97.224
[2024-07-09 23:52:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-09 23:52:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.01%
[2024-07-09 23:52:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-09 23:52:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-09 23:52:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][0/2502]	eta 1 day, 0:02:09 lr 0.000006	 wd 0.0000	time 34.5841 (34.5841)	loss 1.5997 (1.5997)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:53:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:22:18 lr 0.000006	 wd 0.0000	time 0.2281 (0.5572)	loss 1.1698 (1.4068)	grad_norm 0.4821 (0.4785)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:53:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:14:49 lr 0.000006	 wd 0.0000	time 0.2070 (0.3862)	loss 1.4235 (1.3885)	grad_norm 0.5910 (0.4649)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:53:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:12:12 lr 0.000006	 wd 0.0000	time 0.2586 (0.3327)	loss 1.6346 (1.3816)	grad_norm 0.4473 (0.4571)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:54:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:11:36 lr 0.000005	 wd 0.0000	time 0.2187 (0.3313)	loss 1.4240 (1.3783)	grad_norm 0.4408 (0.4650)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:54:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:10:16 lr 0.000005	 wd 0.0000	time 0.1818 (0.3077)	loss 1.6765 (1.3797)	grad_norm 0.4329 (0.4652)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:55:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:09:16 lr 0.000005	 wd 0.0000	time 0.1787 (0.2924)	loss 1.4089 (1.3767)	grad_norm 0.4435 (0.4641)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:55:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:08:29 lr 0.000005	 wd 0.0000	time 0.2163 (0.2826)	loss 1.3502 (1.3776)	grad_norm 0.4351 (0.4641)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:55:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:07:55 lr 0.000005	 wd 0.0000	time 0.1774 (0.2796)	loss 1.4314 (1.3786)	grad_norm 0.4261 (0.4627)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:56:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:07:17 lr 0.000005	 wd 0.0000	time 0.2140 (0.2731)	loss 1.5615 (1.3754)	grad_norm 0.4432 (0.4629)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:56:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:06:42 lr 0.000005	 wd 0.0000	time 0.2221 (0.2678)	loss 1.2260 (1.3757)	grad_norm 0.4249 (0.4604)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:56:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:06:09 lr 0.000005	 wd 0.0000	time 0.2227 (0.2634)	loss 1.5134 (1.3746)	grad_norm 0.4533 (0.4597)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:57:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:05:42 lr 0.000005	 wd 0.0000	time 0.2925 (0.2627)	loss 1.2587 (1.3727)	grad_norm 0.4510 (0.4587)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:57:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:05:12 lr 0.000005	 wd 0.0000	time 0.2627 (0.2602)	loss 1.4005 (1.3720)	grad_norm 0.5030 (0.4602)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:58:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:04:43 lr 0.000005	 wd 0.0000	time 0.2078 (0.2571)	loss 1.3548 (1.3704)	grad_norm 0.4503 (0.4595)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:58:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:04:14 lr 0.000005	 wd 0.0000	time 0.1991 (0.2544)	loss 0.9771 (1.3689)	grad_norm 0.5118 (0.4587)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:58:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:03:48 lr 0.000005	 wd 0.0000	time 0.2293 (0.2534)	loss 1.5268 (1.3685)	grad_norm 0.4236 (0.4594)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:59:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:03:22 lr 0.000005	 wd 0.0000	time 0.2377 (0.2529)	loss 1.6859 (1.3698)	grad_norm 0.4143 (0.4618)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:59:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:02:56 lr 0.000005	 wd 0.0000	time 0.2185 (0.2511)	loss 1.1468 (1.3677)	grad_norm 0.4316 (0.4605)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-09 23:59:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:02:30 lr 0.000005	 wd 0.0000	time 0.2276 (0.2496)	loss 1.4101 (1.3687)	grad_norm 0.4212 (0.4601)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:00:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:04 lr 0.000004	 wd 0.0000	time 0.2007 (0.2489)	loss 1.3838 (1.3693)	grad_norm 0.5256 (0.4599)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:00:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:40 lr 0.000004	 wd 0.0000	time 0.1964 (0.2489)	loss 1.5404 (1.3684)	grad_norm 0.4844 (0.4606)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:01:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:14 lr 0.000004	 wd 0.0000	time 0.2498 (0.2475)	loss 1.4185 (1.3683)	grad_norm 0.4084 (0.4618)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:01:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:49 lr 0.000004	 wd 0.0000	time 0.2025 (0.2464)	loss 1.7401 (1.3679)	grad_norm 0.4615 (0.4610)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:01:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:25 lr 0.000004	 wd 0.0000	time 0.2609 (0.2457)	loss 1.5135 (1.3678)	grad_norm 0.3961 (0.4608)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:02:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0000	time 0.1635 (0.2439)	loss 1.2755 (1.3680)	grad_norm 0.5345 (0.4606)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:02:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 24 training takes 0:10:23
[2024-07-10 00:02:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 22.902 (22.902)	Loss 0.4192 (0.4192)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-10 00:03:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.042 Acc@5 97.226
[2024-07-10 00:03:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-10 00:03:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-10 00:03:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saving......
[2024-07-10 00:03:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth saved !!!
[2024-07-10 00:03:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][0/2502]	eta 14:03:51 lr 0.000004	 wd 0.0000	time 20.2366 (20.2366)	loss 1.2694 (1.2694)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:03:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:17:51 lr 0.000004	 wd 0.0000	time 0.1913 (0.4459)	loss 1.3937 (1.3355)	grad_norm 0.4572 (0.4966)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:04:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:12:43 lr 0.000004	 wd 0.0000	time 0.1960 (0.3318)	loss 1.5008 (1.3635)	grad_norm 0.4066 (0.4766)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:04:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:10:44 lr 0.000004	 wd 0.0000	time 0.2173 (0.2926)	loss 1.1344 (1.3538)	grad_norm 0.3991 (0.4769)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:04:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:09:34 lr 0.000004	 wd 0.0000	time 0.2307 (0.2733)	loss 1.2603 (1.3690)	grad_norm 0.4583 (0.4706)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:05:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:08:54 lr 0.000004	 wd 0.0000	time 0.2154 (0.2672)	loss 1.5597 (1.3691)	grad_norm 2.1022 (0.4777)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:05:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:08:16 lr 0.000004	 wd 0.0000	time 0.2237 (0.2609)	loss 1.1714 (1.3705)	grad_norm 0.4350 (0.4775)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:06:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:07:39 lr 0.000004	 wd 0.0000	time 0.2562 (0.2547)	loss 1.2520 (1.3710)	grad_norm 0.4710 (0.4742)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:06:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:07:05 lr 0.000004	 wd 0.0000	time 0.1945 (0.2497)	loss 1.0147 (1.3711)	grad_norm 0.5102 (0.4724)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:06:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:06:41 lr 0.000004	 wd 0.0000	time 0.3047 (0.2504)	loss 1.5285 (1.3731)	grad_norm 0.4198 (0.4707)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:07:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:06:20 lr 0.000004	 wd 0.0000	time 0.2171 (0.2534)	loss 1.2382 (1.3736)	grad_norm 0.4316 (0.4702)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:07:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:05:50 lr 0.000004	 wd 0.0000	time 0.2190 (0.2501)	loss 1.3163 (1.3717)	grad_norm 0.4569 (0.4722)	loss_scale 8192.0000 (4252.2507)	mem 7723MB
[2024-07-10 00:08:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:05:22 lr 0.000004	 wd 0.0000	time 0.2073 (0.2475)	loss 1.3474 (1.3741)	grad_norm 0.4703 (0.4712)	loss_scale 8192.0000 (4580.2898)	mem 7723MB
[2024-07-10 00:08:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:04:56 lr 0.000003	 wd 0.0000	time 0.2717 (0.2471)	loss 1.4825 (1.3757)	grad_norm 0.4690 (inf)	loss_scale 4096.0000 (4606.0323)	mem 7723MB
[2024-07-10 00:08:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:04:32 lr 0.000003	 wd 0.0000	time 0.2331 (0.2472)	loss 1.1219 (1.3751)	grad_norm 0.4290 (inf)	loss_scale 4096.0000 (4569.6274)	mem 7723MB
[2024-07-10 00:09:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:04:05 lr 0.000003	 wd 0.0000	time 0.2487 (0.2455)	loss 1.2483 (1.3752)	grad_norm 0.4275 (inf)	loss_scale 4096.0000 (4538.0733)	mem 7723MB
[2024-07-10 00:09:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:03:39 lr 0.000003	 wd 0.0000	time 0.2133 (0.2438)	loss 1.4996 (1.3746)	grad_norm 0.4956 (inf)	loss_scale 4096.0000 (4510.4610)	mem 7723MB
[2024-07-10 00:10:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:03:15 lr 0.000003	 wd 0.0000	time 0.2423 (0.2432)	loss 1.3874 (1.3739)	grad_norm 0.4162 (inf)	loss_scale 4096.0000 (4486.0952)	mem 7723MB
[2024-07-10 00:10:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:02:51 lr 0.000003	 wd 0.0000	time 0.2309 (0.2439)	loss 1.5710 (1.3737)	grad_norm 0.4483 (inf)	loss_scale 4096.0000 (4464.4353)	mem 7723MB
[2024-07-10 00:10:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:02:26 lr 0.000003	 wd 0.0000	time 0.1913 (0.2427)	loss 1.9529 (1.3735)	grad_norm 0.4207 (inf)	loss_scale 4096.0000 (4445.0542)	mem 7723MB
[2024-07-10 00:11:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:01 lr 0.000003	 wd 0.0000	time 0.2080 (0.2416)	loss 1.2749 (1.3728)	grad_norm 0.4127 (inf)	loss_scale 4096.0000 (4427.6102)	mem 7723MB
[2024-07-10 00:11:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:36 lr 0.000003	 wd 0.0000	time 0.2900 (0.2412)	loss 0.8934 (1.3728)	grad_norm 0.4769 (inf)	loss_scale 4096.0000 (4411.8267)	mem 7723MB
[2024-07-10 00:12:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:13 lr 0.000003	 wd 0.0000	time 0.2366 (0.2417)	loss 1.4446 (1.3715)	grad_norm 0.4095 (inf)	loss_scale 4096.0000 (4397.4775)	mem 7723MB
[2024-07-10 00:12:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:48 lr 0.000003	 wd 0.0000	time 0.1984 (0.2410)	loss 1.2247 (1.3702)	grad_norm 0.4026 (inf)	loss_scale 4096.0000 (4384.3755)	mem 7723MB
[2024-07-10 00:12:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:24 lr 0.000003	 wd 0.0000	time 0.2577 (0.2401)	loss 1.3997 (1.3717)	grad_norm 0.5304 (inf)	loss_scale 4096.0000 (4372.3648)	mem 7723MB
[2024-07-10 00:13:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.1629 (0.2382)	loss 1.4943 (1.3718)	grad_norm 0.5609 (inf)	loss_scale 4096.0000 (4361.3147)	mem 7723MB
[2024-07-10 00:13:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 25 training takes 0:10:04
[2024-07-10 00:13:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 44.148 (44.148)	Loss 0.4194 (0.4194)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-10 00:14:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.038 Acc@5 97.212
[2024-07-10 00:14:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-10 00:14:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-10 00:14:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][0/2502]	eta 11:05:46 lr 0.000003	 wd 0.0000	time 15.9660 (15.9660)	loss 1.3279 (1.3279)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:15:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:17:12 lr 0.000003	 wd 0.0000	time 0.4201 (0.4297)	loss 1.5176 (1.3643)	grad_norm 0.4051 (0.4510)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:15:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:13:29 lr 0.000003	 wd 0.0000	time 0.2132 (0.3518)	loss 1.4879 (1.3855)	grad_norm 0.4365 (0.4618)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:15:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:11:15 lr 0.000003	 wd 0.0000	time 0.2260 (0.3067)	loss 1.5057 (1.3771)	grad_norm 0.5749 (0.4612)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:16:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:09:56 lr 0.000003	 wd 0.0000	time 0.2068 (0.2836)	loss 1.6652 (1.3788)	grad_norm 0.4243 (0.4628)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:16:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:09:08 lr 0.000003	 wd 0.0000	time 0.2200 (0.2742)	loss 1.6273 (1.3850)	grad_norm 0.4325 (0.4587)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:17:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:08:35 lr 0.000003	 wd 0.0000	time 0.2461 (0.2710)	loss 1.4112 (1.3800)	grad_norm 0.4287 (0.4569)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:17:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:07:54 lr 0.000003	 wd 0.0000	time 0.1979 (0.2634)	loss 1.5335 (1.3759)	grad_norm 0.4230 (0.4610)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:17:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:07:18 lr 0.000002	 wd 0.0000	time 0.2078 (0.2578)	loss 1.5920 (1.3746)	grad_norm 0.4137 (0.4643)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:18:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:06:47 lr 0.000002	 wd 0.0000	time 0.2419 (0.2545)	loss 1.3897 (1.3745)	grad_norm 0.4634 (0.4630)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:18:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:06:22 lr 0.000002	 wd 0.0000	time 0.2748 (0.2549)	loss 1.4228 (1.3751)	grad_norm 0.4815 (0.4629)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:18:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:05:52 lr 0.000002	 wd 0.0000	time 0.1962 (0.2517)	loss 1.5975 (1.3768)	grad_norm 0.4088 (0.4612)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:19:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:05:24 lr 0.000002	 wd 0.0000	time 0.2209 (0.2490)	loss 1.0876 (1.3773)	grad_norm 0.4729 (0.4604)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:19:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:04:56 lr 0.000002	 wd 0.0000	time 0.2471 (0.2471)	loss 0.9635 (1.3755)	grad_norm 0.4666 (0.4605)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:20:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:04:32 lr 0.000002	 wd 0.0000	time 0.1983 (0.2475)	loss 1.6105 (1.3750)	grad_norm 0.4405 (0.4605)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:20:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:04:06 lr 0.000002	 wd 0.0000	time 0.2273 (0.2459)	loss 1.4100 (1.3737)	grad_norm 0.6097 (0.4603)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:20:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:03:40 lr 0.000002	 wd 0.0000	time 0.1949 (0.2443)	loss 1.4561 (1.3726)	grad_norm 0.4509 (0.4597)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:21:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:03:14 lr 0.000002	 wd 0.0000	time 0.2193 (0.2429)	loss 1.2861 (1.3737)	grad_norm 0.4547 (0.4597)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:21:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:02:50 lr 0.000002	 wd 0.0000	time 0.1998 (0.2431)	loss 1.5371 (1.3765)	grad_norm 0.6872 (0.4599)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:22:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:02:26 lr 0.000002	 wd 0.0000	time 0.2139 (0.2432)	loss 0.9950 (1.3750)	grad_norm 0.4080 (0.4594)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:22:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:01 lr 0.000002	 wd 0.0000	time 0.1899 (0.2421)	loss 0.9347 (1.3741)	grad_norm 0.4655 (0.4584)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:22:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:36 lr 0.000002	 wd 0.0000	time 0.2332 (0.2411)	loss 1.1245 (1.3730)	grad_norm 0.4206 (0.4577)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:23:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:12 lr 0.000002	 wd 0.0000	time 0.2451 (0.2411)	loss 0.9730 (1.3727)	grad_norm 0.4561 (0.4573)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:23:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:48 lr 0.000002	 wd 0.0000	time 0.2057 (0.2412)	loss 1.2075 (1.3716)	grad_norm 0.4513 (0.4572)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:23:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:24 lr 0.000002	 wd 0.0000	time 0.2042 (0.2403)	loss 1.1301 (1.3717)	grad_norm 0.4460 (0.4581)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:24:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.1591 (0.2384)	loss 0.9538 (1.3698)	grad_norm 0.4429 (0.4578)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:24:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 26 training takes 0:10:04
[2024-07-10 00:25:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 47.058 (47.058)	Loss 0.4187 (0.4187)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-10 00:25:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.036 Acc@5 97.214
[2024-07-10 00:25:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-10 00:25:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-10 00:25:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][0/2502]	eta 10:42:02 lr 0.000002	 wd 0.0000	time 15.3967 (15.3967)	loss 0.9411 (0.9411)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:26:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:15:36 lr 0.000002	 wd 0.0000	time 0.2585 (0.3897)	loss 1.5050 (1.3762)	grad_norm 0.4354 (0.4695)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:26:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:13:23 lr 0.000002	 wd 0.0000	time 0.2155 (0.3492)	loss 1.5827 (1.3712)	grad_norm 0.5654 (0.4633)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:27:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:11:12 lr 0.000002	 wd 0.0000	time 0.2399 (0.3052)	loss 1.3658 (1.3715)	grad_norm 0.4162 (0.4603)	loss_scale 8192.0000 (5239.0698)	mem 7723MB
[2024-07-10 00:27:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:09:54 lr 0.000002	 wd 0.0000	time 0.2009 (0.2827)	loss 1.4489 (1.3695)	grad_norm 0.4208 (0.4607)	loss_scale 8192.0000 (5975.4613)	mem 7723MB
[2024-07-10 00:27:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:09:01 lr 0.000002	 wd 0.0000	time 0.2541 (0.2706)	loss 0.9823 (1.3653)	grad_norm 0.4595 (0.4613)	loss_scale 8192.0000 (6417.8842)	mem 7723MB
[2024-07-10 00:28:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:08:30 lr 0.000002	 wd 0.0000	time 0.2511 (0.2683)	loss 1.2792 (1.3685)	grad_norm 0.5596 (0.4634)	loss_scale 8192.0000 (6713.0782)	mem 7723MB
[2024-07-10 00:28:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:07:52 lr 0.000002	 wd 0.0000	time 0.2053 (0.2621)	loss 1.4917 (1.3742)	grad_norm 0.4226 (0.4625)	loss_scale 8192.0000 (6924.0514)	mem 7723MB
[2024-07-10 00:28:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:07:16 lr 0.000002	 wd 0.0000	time 0.1927 (0.2566)	loss 1.5454 (1.3778)	grad_norm 0.4307 (0.4609)	loss_scale 8192.0000 (7082.3471)	mem 7723MB
[2024-07-10 00:29:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:06:44 lr 0.000001	 wd 0.0000	time 0.2330 (0.2524)	loss 1.3000 (1.3824)	grad_norm 0.5411 (0.4644)	loss_scale 8192.0000 (7205.5050)	mem 7723MB
[2024-07-10 00:29:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:06:18 lr 0.000001	 wd 0.0000	time 0.2181 (0.2522)	loss 1.4231 (1.3806)	grad_norm 0.4126 (0.4650)	loss_scale 8192.0000 (7304.0559)	mem 7723MB
[2024-07-10 00:30:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:05:50 lr 0.000001	 wd 0.0000	time 0.1907 (0.2502)	loss 1.5297 (1.3805)	grad_norm 0.4393 (0.4634)	loss_scale 8192.0000 (7384.7048)	mem 7723MB
[2024-07-10 00:30:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:05:22 lr 0.000001	 wd 0.0000	time 0.2207 (0.2476)	loss 1.4995 (1.3805)	grad_norm 0.4483 (0.4640)	loss_scale 8192.0000 (7451.9234)	mem 7723MB
[2024-07-10 00:30:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:04:54 lr 0.000001	 wd 0.0000	time 0.2333 (0.2451)	loss 1.5079 (1.3776)	grad_norm 0.4688 (0.4620)	loss_scale 8192.0000 (7508.8086)	mem 7723MB
[2024-07-10 00:31:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:04:29 lr 0.000001	 wd 0.0000	time 0.2137 (0.2449)	loss 1.2841 (1.3781)	grad_norm 0.4242 (0.4624)	loss_scale 8192.0000 (7557.5732)	mem 7723MB
[2024-07-10 00:31:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:04:05 lr 0.000001	 wd 0.0000	time 0.1969 (0.2449)	loss 1.5564 (1.3777)	grad_norm 0.4518 (0.4621)	loss_scale 8192.0000 (7599.8401)	mem 7723MB
[2024-07-10 00:31:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:03:39 lr 0.000001	 wd 0.0000	time 0.1976 (0.2432)	loss 1.5751 (1.3756)	grad_norm 0.4600 (0.4618)	loss_scale 8192.0000 (7636.8270)	mem 7723MB
[2024-07-10 00:32:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:03:13 lr 0.000001	 wd 0.0000	time 0.2204 (0.2418)	loss 1.4017 (1.3733)	grad_norm 0.4212 (0.4616)	loss_scale 8192.0000 (7669.4650)	mem 7723MB
[2024-07-10 00:32:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:02:49 lr 0.000001	 wd 0.0000	time 0.2278 (0.2414)	loss 1.5306 (1.3739)	grad_norm 0.4371 (0.4612)	loss_scale 8192.0000 (7698.4786)	mem 7723MB
[2024-07-10 00:33:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:02:25 lr 0.000001	 wd 0.0000	time 0.2267 (0.2424)	loss 1.0872 (1.3754)	grad_norm 0.4245 (0.4621)	loss_scale 8192.0000 (7724.4398)	mem 7723MB
[2024-07-10 00:33:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:01 lr 0.000001	 wd 0.0000	time 0.2151 (0.2413)	loss 1.0558 (1.3742)	grad_norm 0.4364 (0.4618)	loss_scale 8192.0000 (7747.8061)	mem 7723MB
[2024-07-10 00:33:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:36 lr 0.000001	 wd 0.0000	time 0.1945 (0.2404)	loss 1.7213 (1.3735)	grad_norm 0.4282 (nan)	loss_scale 4096.0000 (7663.6725)	mem 7723MB
[2024-07-10 00:34:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:12 lr 0.000001	 wd 0.0000	time 0.2493 (0.2399)	loss 1.0815 (1.3741)	grad_norm 0.4499 (nan)	loss_scale 4096.0000 (7501.5793)	mem 7723MB
[2024-07-10 00:34:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:48 lr 0.000001	 wd 0.0000	time 0.1946 (0.2403)	loss 1.4655 (1.3742)	grad_norm 0.4097 (nan)	loss_scale 4096.0000 (7353.5750)	mem 7723MB
[2024-07-10 00:35:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:24 lr 0.000001	 wd 0.0000	time 0.2038 (0.2396)	loss 1.3810 (1.3738)	grad_norm 0.4488 (nan)	loss_scale 4096.0000 (7217.8992)	mem 7723MB
[2024-07-10 00:35:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1625 (0.2377)	loss 1.5176 (1.3722)	grad_norm 0.4104 (nan)	loss_scale 4096.0000 (7093.0732)	mem 7723MB
[2024-07-10 00:35:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 27 training takes 0:10:01
[2024-07-10 00:36:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 38.574 (38.574)	Loss 0.4194 (0.4194)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-10 00:36:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.026 Acc@5 97.218
[2024-07-10 00:36:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-10 00:36:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-10 00:36:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][0/2502]	eta 11:56:32 lr 0.000001	 wd 0.0000	time 17.1833 (17.1833)	loss 1.2907 (1.2907)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:37:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:15:21 lr 0.000001	 wd 0.0000	time 0.2240 (0.3837)	loss 1.0746 (1.3530)	grad_norm 0.4314 (0.4503)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:37:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:12:00 lr 0.000001	 wd 0.0000	time 0.2949 (0.3129)	loss 1.0716 (1.3562)	grad_norm 0.4280 (0.4525)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:38:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:11:50 lr 0.000001	 wd 0.0000	time 0.2121 (0.3226)	loss 1.6509 (1.3664)	grad_norm 0.5629 (0.4620)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:38:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:10:21 lr 0.000001	 wd 0.0000	time 0.2136 (0.2955)	loss 1.3908 (1.3708)	grad_norm 0.4285 (0.4598)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:38:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:09:17 lr 0.000001	 wd 0.0000	time 0.1902 (0.2786)	loss 1.5772 (1.3731)	grad_norm 0.4106 (0.4577)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:39:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:08:38 lr 0.000001	 wd 0.0000	time 0.2002 (0.2725)	loss 1.4024 (1.3730)	grad_norm 0.7190 (0.4576)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:39:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:08:03 lr 0.000001	 wd 0.0000	time 0.2037 (0.2681)	loss 1.0376 (1.3749)	grad_norm 0.4174 (0.4571)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:40:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:07:25 lr 0.000001	 wd 0.0000	time 0.2241 (0.2619)	loss 1.6169 (1.3721)	grad_norm 0.4589 (0.4567)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:40:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:06:51 lr 0.000001	 wd 0.0000	time 0.2107 (0.2570)	loss 1.5422 (1.3728)	grad_norm 0.4653 (0.4577)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:40:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:06:22 lr 0.000001	 wd 0.0000	time 0.2206 (0.2546)	loss 1.5692 (1.3754)	grad_norm 0.4314 (0.4582)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:41:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:05:56 lr 0.000001	 wd 0.0000	time 0.2013 (0.2545)	loss 0.9524 (1.3718)	grad_norm 0.4435 (0.4580)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:41:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:05:27 lr 0.000001	 wd 0.0000	time 0.1958 (0.2517)	loss 1.2200 (1.3722)	grad_norm 0.5832 (0.4578)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:41:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:04:59 lr 0.000001	 wd 0.0000	time 0.2259 (0.2491)	loss 1.5950 (1.3690)	grad_norm 0.4632 (0.4589)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:42:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:04:32 lr 0.000001	 wd 0.0000	time 0.2716 (0.2475)	loss 0.9382 (1.3701)	grad_norm 0.4329 (0.4587)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:42:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:04:08 lr 0.000001	 wd 0.0000	time 0.2083 (0.2481)	loss 1.5007 (1.3686)	grad_norm 0.4292 (0.4589)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:43:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:03:42 lr 0.000001	 wd 0.0000	time 0.2098 (0.2467)	loss 1.1954 (1.3681)	grad_norm 0.5210 (0.4582)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:43:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:03:16 lr 0.000001	 wd 0.0000	time 0.2259 (0.2451)	loss 1.4293 (1.3695)	grad_norm 0.4685 (0.4576)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:43:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:02:51 lr 0.000001	 wd 0.0000	time 0.1896 (0.2437)	loss 1.3821 (1.3692)	grad_norm 0.4433 (0.4575)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:44:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:02:26 lr 0.000001	 wd 0.0000	time 0.3970 (0.2440)	loss 1.5459 (1.3682)	grad_norm 0.4865 (0.4584)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:44:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:02 lr 0.000001	 wd 0.0000	time 0.2504 (0.2433)	loss 0.8897 (1.3681)	grad_norm 0.4266 (0.4588)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:45:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:37 lr 0.000001	 wd 0.0000	time 0.1931 (0.2424)	loss 1.0934 (1.3683)	grad_norm 0.4316 (0.4590)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:45:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:12 lr 0.000001	 wd 0.0000	time 0.2376 (0.2414)	loss 1.5391 (1.3671)	grad_norm 0.4211 (0.4603)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:45:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:48 lr 0.000001	 wd 0.0000	time 0.2244 (0.2413)	loss 1.0953 (1.3672)	grad_norm 0.4507 (0.4615)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:46:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:24 lr 0.000001	 wd 0.0000	time 0.2439 (0.2413)	loss 1.5939 (1.3657)	grad_norm 0.4353 (0.4609)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:46:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1671 (0.2392)	loss 1.2061 (1.3649)	grad_norm 0.4326 (0.4610)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:46:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 28 training takes 0:10:05
[2024-07-10 00:46:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 18.776 (18.776)	Loss 0.4192 (0.4192)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-10 00:47:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.030 Acc@5 97.218
[2024-07-10 00:47:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-10 00:47:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-10 00:47:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][0/2502]	eta 1 day, 2:27:02 lr 0.000001	 wd 0.0000	time 38.0584 (38.0584)	loss 1.4173 (1.4173)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:48:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:23:41 lr 0.000001	 wd 0.0000	time 0.2040 (0.5920)	loss 1.4836 (1.3477)	grad_norm 0.4156 (0.4871)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:48:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:15:23 lr 0.000001	 wd 0.0000	time 0.2149 (0.4010)	loss 1.1522 (1.3623)	grad_norm 0.4806 (0.4971)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:49:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:13:36 lr 0.000001	 wd 0.0000	time 0.2126 (0.3710)	loss 1.5027 (1.3610)	grad_norm 0.4291 (0.4981)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:49:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:11:44 lr 0.000001	 wd 0.0000	time 0.2092 (0.3350)	loss 1.1878 (1.3697)	grad_norm 0.4662 (0.4885)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:49:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:10:21 lr 0.000001	 wd 0.0000	time 0.2074 (0.3105)	loss 0.9502 (1.3712)	grad_norm 0.4483 (0.4849)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:50:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:09:20 lr 0.000000	 wd 0.0000	time 0.1927 (0.2945)	loss 1.3597 (1.3719)	grad_norm 0.4725 (0.4796)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:50:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:08:42 lr 0.000000	 wd 0.0000	time 0.4228 (0.2902)	loss 1.5939 (1.3726)	grad_norm 0.4642 (0.4749)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:51:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:08:08 lr 0.000000	 wd 0.0000	time 0.2013 (0.2873)	loss 0.8037 (1.3737)	grad_norm 0.6136 (0.4737)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:51:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:07:27 lr 0.000000	 wd 0.0000	time 0.2033 (0.2793)	loss 1.4242 (1.3733)	grad_norm 0.4677 (0.4711)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:51:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:06:49 lr 0.000000	 wd 0.0000	time 0.2144 (0.2729)	loss 1.3414 (1.3730)	grad_norm 0.4494 (0.4691)	loss_scale 4096.0000 (4096.0000)	mem 7723MB
[2024-07-10 00:52:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:06:19 lr 0.000000	 wd 0.0000	time 0.2586 (0.2704)	loss 1.4707 (1.3736)	grad_norm 0.3907 (0.4684)	loss_scale 8192.0000 (4311.7748)	mem 7723MB
[2024-07-10 00:52:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:05:49 lr 0.000000	 wd 0.0000	time 0.2312 (0.2687)	loss 1.2755 (1.3706)	grad_norm 0.4470 (0.4661)	loss_scale 8192.0000 (4634.8576)	mem 7723MB
[2024-07-10 00:52:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:05:18 lr 0.000000	 wd 0.0000	time 0.2064 (0.2650)	loss 1.5623 (1.3714)	grad_norm 0.4431 (0.4647)	loss_scale 8192.0000 (4908.2736)	mem 7723MB
[2024-07-10 00:53:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:04:48 lr 0.000000	 wd 0.0000	time 0.2170 (0.2616)	loss 1.7652 (1.3703)	grad_norm 0.5099 (0.4649)	loss_scale 8192.0000 (5142.6581)	mem 7723MB
[2024-07-10 00:53:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:04:20 lr 0.000000	 wd 0.0000	time 0.2527 (0.2596)	loss 1.2946 (1.3715)	grad_norm 0.5087 (0.4645)	loss_scale 8192.0000 (5345.8121)	mem 7723MB
[2024-07-10 00:54:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:03:54 lr 0.000000	 wd 0.0000	time 0.2091 (0.2596)	loss 1.4916 (1.3729)	grad_norm 0.4639 (0.4645)	loss_scale 8192.0000 (5523.5878)	mem 7723MB
[2024-07-10 00:54:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:03:26 lr 0.000000	 wd 0.0000	time 0.1999 (0.2574)	loss 1.3162 (1.3698)	grad_norm 0.5846 (0.4645)	loss_scale 8192.0000 (5680.4609)	mem 7723MB
[2024-07-10 00:54:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:02:59 lr 0.000000	 wd 0.0000	time 0.2654 (0.2554)	loss 1.1759 (1.3714)	grad_norm 0.4676 (0.4640)	loss_scale 8192.0000 (5819.9134)	mem 7723MB
[2024-07-10 00:55:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:02:33 lr 0.000000	 wd 0.0000	time 0.2300 (0.2542)	loss 1.2324 (1.3711)	grad_norm 0.5022 (0.4631)	loss_scale 8192.0000 (5944.6944)	mem 7723MB
[2024-07-10 00:55:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:07 lr 0.000000	 wd 0.0000	time 0.2478 (0.2539)	loss 1.5098 (1.3712)	grad_norm 0.4973 (0.4634)	loss_scale 8192.0000 (6057.0035)	mem 7723MB
[2024-07-10 00:56:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:41 lr 0.000000	 wd 0.0000	time 0.2297 (0.2526)	loss 1.5071 (1.3718)	grad_norm 0.4590 (0.4636)	loss_scale 8192.0000 (6158.6216)	mem 7723MB
[2024-07-10 00:56:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:15 lr 0.000000	 wd 0.0000	time 0.2390 (0.2512)	loss 1.4309 (1.3735)	grad_norm 0.4264 (inf)	loss_scale 4096.0000 (6202.6206)	mem 7723MB
[2024-07-10 00:56:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:50 lr 0.000000	 wd 0.0000	time 0.2156 (0.2501)	loss 1.4358 (1.3747)	grad_norm 0.5041 (inf)	loss_scale 4096.0000 (6111.0682)	mem 7723MB
[2024-07-10 00:57:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:25 lr 0.000000	 wd 0.0000	time 0.2028 (0.2499)	loss 1.4122 (1.3753)	grad_norm 0.4681 (inf)	loss_scale 4096.0000 (6027.1420)	mem 7723MB
[2024-07-10 00:57:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0000	time 0.1718 (0.2476)	loss 1.5774 (1.3744)	grad_norm 0.4492 (inf)	loss_scale 4096.0000 (5949.9272)	mem 7723MB
[2024-07-10 00:57:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 249): INFO EPOCH 29 training takes 0:10:27
[2024-07-10 00:57:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_29.pth saving......
[2024-07-10 00:57:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_29.pth saved !!!
[2024-07-10 00:58:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 289): INFO Test: [0/98]	Time 19.445 (19.445)	Loss 0.4192 (0.4192)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 7723MB
[2024-07-10 00:58:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 296): INFO  * Acc@1 84.030 Acc@5 97.220
[2024-07-10 00:58:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-10 00:58:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-10 00:58:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0] (main.py 189): INFO Training time 5:25:36
