[2024-07-12 15:35:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/config.json
[2024-07-12 15:35:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: part1
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-12 15:35:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/swin/diffusion_ft_adapter_swin_base_patch4_window7_224_22kto1k_step_cross_process1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-12 15:35:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 108): INFO Creating model:adapter_swin_diffusion_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1
[2024-07-12 15:35:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 110): INFO Adapter_SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-12 15:35:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 113): INFO number of params: 2779464
[2024-07-12 15:35:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-12 15:35:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 150): INFO no checkpoint found in pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1, ignoring auto resume
[2024-07-12 15:35:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-12 15:35:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.relative_position_index', 'layers.0.blocks.1.attn_mask', 'layers.0.blocks.1.attn.relative_position_index', 'layers.1.blocks.0.attn.relative_position_index', 'layers.1.blocks.1.attn_mask', 'layers.1.blocks.1.attn.relative_position_index', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn_mask', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn_mask', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn_mask', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn_mask', 'layers.2.blocks.7.attn.relative_position_index', 'layers.2.blocks.8.attn.relative_position_index', 'layers.2.blocks.9.attn_mask', 'layers.2.blocks.9.attn.relative_position_index', 'layers.2.blocks.10.attn.relative_position_index', 'layers.2.blocks.11.attn_mask', 'layers.2.blocks.11.attn.relative_position_index', 'layers.2.blocks.12.attn.relative_position_index', 'layers.2.blocks.13.attn_mask', 'layers.2.blocks.13.attn.relative_position_index', 'layers.2.blocks.14.attn.relative_position_index', 'layers.2.blocks.15.attn_mask', 'layers.2.blocks.15.attn.relative_position_index', 'layers.2.blocks.16.attn.relative_position_index', 'layers.2.blocks.17.attn_mask', 'layers.2.blocks.17.attn.relative_position_index', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.relative_position_index'], unexpected_keys=[])
[2024-07-12 15:35:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth'
[2024-07-12 15:36:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 60.573 (60.573)	Loss 0.4192 (0.4192)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 1482MB
[2024-07-12 15:36:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.042 Acc@5 97.226
[2024-07-12 15:36:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 162): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 15:36:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 168): INFO Start training
[2024-07-12 15:37:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][0/2502]	eta 14:11:54 lr 0.000000	 wd 0.0000	time 20.4294 (20.4294)	loss 1.6051 (1.6051)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 6754MB
[2024-07-12 15:37:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:22:05 lr 0.000000	 wd 0.0000	time 0.2038 (0.5520)	loss 1.4206 (1.4051)	grad_norm 0.3718 (nan)	loss_scale 32768.0000 (38932.2772)	mem 6787MB
[2024-07-12 15:37:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:39 lr 0.000001	 wd 0.0000	time 0.2316 (0.3822)	loss 1.4345 (1.3901)	grad_norm 0.4216 (nan)	loss_scale 32768.0000 (35865.4726)	mem 6787MB
[2024-07-12 15:38:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:11:59 lr 0.000001	 wd 0.0000	time 0.2063 (0.3267)	loss 1.4429 (1.3709)	grad_norm 0.3855 (nan)	loss_scale 32768.0000 (34836.4120)	mem 6787MB
[2024-07-12 15:38:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:10:31 lr 0.000001	 wd 0.0000	time 0.2233 (0.3004)	loss 1.8418 (1.3735)	grad_norm 0.3843 (nan)	loss_scale 32768.0000 (34320.5985)	mem 6787MB
[2024-07-12 15:39:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:10:09 lr 0.000002	 wd 0.0000	time 0.2348 (0.3044)	loss 1.4951 (1.3722)	grad_norm 0.4013 (nan)	loss_scale 32768.0000 (34010.6986)	mem 6787MB
[2024-07-12 15:39:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:09:09 lr 0.000002	 wd 0.0000	time 0.2002 (0.2890)	loss 1.1683 (1.3717)	grad_norm 0.4714 (nan)	loss_scale 32768.0000 (33803.9268)	mem 6787MB
[2024-07-12 15:39:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:08:21 lr 0.000002	 wd 0.0000	time 0.2094 (0.2785)	loss 1.4227 (1.3677)	grad_norm 0.3961 (nan)	loss_scale 32768.0000 (33656.1484)	mem 6787MB
[2024-07-12 15:40:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:07:42 lr 0.000003	 wd 0.0000	time 0.2501 (0.2718)	loss 1.5372 (1.3687)	grad_norm nan (nan)	loss_scale 16384.0000 (33504.3596)	mem 6787MB
[2024-07-12 15:40:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:07:16 lr 0.000003	 wd 0.0000	time 0.1752 (0.2726)	loss 1.5623 (1.3638)	grad_norm 0.4082 (nan)	loss_scale 16384.0000 (31604.2087)	mem 6787MB
[2024-07-12 15:41:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:06:40 lr 0.000003	 wd 0.0000	time 0.2058 (0.2669)	loss 1.3696 (1.3634)	grad_norm 0.5007 (nan)	loss_scale 8192.0000 (30050.9730)	mem 6787MB
[2024-07-12 15:41:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:06:07 lr 0.000004	 wd 0.0000	time 0.2004 (0.2623)	loss 1.5212 (1.3643)	grad_norm 0.3521 (nan)	loss_scale 8192.0000 (28065.5985)	mem 6787MB
[2024-07-12 15:41:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:05:36 lr 0.000004	 wd 0.0000	time 0.2493 (0.2584)	loss 1.4726 (1.3664)	grad_norm 0.3790 (nan)	loss_scale 8192.0000 (26410.8443)	mem 6787MB
[2024-07-12 15:42:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:05:08 lr 0.000004	 wd 0.0000	time 0.3173 (0.2571)	loss 1.4435 (1.3684)	grad_norm 0.3595 (nan)	loss_scale 8192.0000 (25010.4719)	mem 6787MB
[2024-07-12 15:42:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:04:40 lr 0.000005	 wd 0.0000	time 0.2073 (0.2549)	loss 1.5563 (1.3695)	grad_norm 0.3925 (nan)	loss_scale 8192.0000 (23810.0100)	mem 6787MB
[2024-07-12 15:42:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:04:12 lr 0.000005	 wd 0.0000	time 0.2149 (0.2521)	loss 1.4812 (1.3694)	grad_norm 0.3911 (nan)	loss_scale 8192.0000 (22769.5030)	mem 6787MB
[2024-07-12 15:43:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:03:45 lr 0.000005	 wd 0.0000	time 0.1897 (0.2497)	loss 1.6006 (1.3699)	grad_norm 0.3794 (nan)	loss_scale 8192.0000 (21858.9781)	mem 6787MB
[2024-07-12 15:43:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:03:19 lr 0.000005	 wd 0.0000	time 0.2342 (0.2485)	loss 1.4913 (1.3690)	grad_norm 0.4051 (nan)	loss_scale 8192.0000 (21055.5109)	mem 6787MB
[2024-07-12 15:44:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:02:54 lr 0.000006	 wd 0.0000	time 0.2127 (0.2489)	loss 1.2246 (1.3696)	grad_norm 0.3694 (nan)	loss_scale 8192.0000 (20341.2682)	mem 6787MB
[2024-07-12 15:44:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:28 lr 0.000006	 wd 0.0000	time 0.1981 (0.2473)	loss 1.6301 (1.3690)	grad_norm 0.3687 (nan)	loss_scale 8192.0000 (19702.1694)	mem 6787MB
[2024-07-12 15:44:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:03 lr 0.000006	 wd 0.0000	time 0.2180 (0.2455)	loss 1.5032 (1.3671)	grad_norm 0.4512 (nan)	loss_scale 8192.0000 (19126.9485)	mem 6787MB
[2024-07-12 15:45:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:38 lr 0.000007	 wd 0.0000	time 0.2000 (0.2445)	loss 1.3682 (1.3676)	grad_norm 0.3756 (nan)	loss_scale 8192.0000 (18606.4845)	mem 6787MB
[2024-07-12 15:45:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:13 lr 0.000007	 wd 0.0000	time 0.2607 (0.2444)	loss 1.6198 (1.3684)	grad_norm 0.3592 (nan)	loss_scale 8192.0000 (18133.3139)	mem 6787MB
[2024-07-12 15:45:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:49 lr 0.000007	 wd 0.0000	time 0.2401 (0.2433)	loss 1.5365 (1.3672)	grad_norm 0.3789 (nan)	loss_scale 8192.0000 (17701.2708)	mem 6787MB
[2024-07-12 15:46:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:24 lr 0.000008	 wd 0.0000	time 0.2110 (0.2422)	loss 1.4014 (1.3674)	grad_norm 0.3737 (nan)	loss_scale 8192.0000 (17305.2162)	mem 6787MB
[2024-07-12 15:46:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.1549 (0.2398)	loss 1.6307 (1.3676)	grad_norm 0.3434 (nan)	loss_scale 8192.0000 (16940.8333)	mem 6787MB
[2024-07-12 15:46:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 0 training takes 0:10:04
[2024-07-12 15:46:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_0.pth saving......
[2024-07-12 15:46:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_0.pth saved !!!
[2024-07-12 15:47:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 55.974 (55.974)	Loss 0.4214 (0.4214)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 15:47:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.044 Acc@5 97.214
[2024-07-12 15:47:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 15:47:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-12 15:47:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saving......
[2024-07-12 15:47:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saved !!!
[2024-07-12 15:48:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][0/2502]	eta 10:57:12 lr 0.000008	 wd 0.0000	time 15.7602 (15.7602)	loss 1.2188 (1.2188)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:48:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:19:59 lr 0.000008	 wd 0.0000	time 0.2623 (0.4996)	loss 1.1586 (1.4058)	grad_norm 0.3797 (0.3851)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:49:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:13:47 lr 0.000009	 wd 0.0000	time 0.1955 (0.3595)	loss 1.3572 (1.4055)	grad_norm 0.3850 (0.3857)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:49:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:11:21 lr 0.000009	 wd 0.0000	time 0.2280 (0.3094)	loss 1.7007 (1.3847)	grad_norm 0.3620 (0.3885)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:49:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:09:56 lr 0.000009	 wd 0.0000	time 0.2183 (0.2836)	loss 0.9824 (1.3743)	grad_norm 0.4065 (0.3876)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:50:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:09:55 lr 0.000010	 wd 0.0000	time 0.2536 (0.2973)	loss 1.5250 (1.3726)	grad_norm 0.3723 (0.3892)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:50:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:09:03 lr 0.000010	 wd 0.0000	time 0.2016 (0.2858)	loss 1.4371 (1.3722)	grad_norm 0.3952 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:51:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:08:15 lr 0.000010	 wd 0.0000	time 0.2094 (0.2748)	loss 1.6333 (1.3709)	grad_norm 0.4012 (0.3898)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:51:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:07:33 lr 0.000011	 wd 0.0000	time 0.2122 (0.2664)	loss 1.5500 (1.3743)	grad_norm 0.3561 (0.3933)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:51:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:07:13 lr 0.000011	 wd 0.0000	time 0.2453 (0.2705)	loss 1.5978 (1.3716)	grad_norm 0.3909 (0.3939)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:52:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:06:37 lr 0.000011	 wd 0.0000	time 0.2367 (0.2650)	loss 1.6857 (1.3705)	grad_norm 0.3941 (0.3932)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:52:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:06:04 lr 0.000012	 wd 0.0000	time 0.2503 (0.2602)	loss 1.0847 (1.3696)	grad_norm 0.3970 (0.3936)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:53:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:05:33 lr 0.000012	 wd 0.0000	time 0.2070 (0.2562)	loss 1.3500 (1.3723)	grad_norm 0.3638 (0.3944)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:53:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:05:05 lr 0.000012	 wd 0.0000	time 0.2112 (0.2544)	loss 1.5498 (1.3756)	grad_norm 0.3704 (0.3939)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:53:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:04:39 lr 0.000012	 wd 0.0000	time 0.2132 (0.2537)	loss 1.4296 (1.3736)	grad_norm 0.3802 (0.3940)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:54:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:04:11 lr 0.000013	 wd 0.0000	time 0.2339 (0.2511)	loss 0.9845 (1.3720)	grad_norm 0.3873 (0.3933)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:54:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:03:44 lr 0.000013	 wd 0.0000	time 0.2208 (0.2489)	loss 0.9397 (1.3706)	grad_norm 0.3944 (0.3936)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:54:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:03:18 lr 0.000013	 wd 0.0000	time 0.2803 (0.2474)	loss 1.3524 (1.3699)	grad_norm 0.3778 (0.3933)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:55:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:02:53 lr 0.000014	 wd 0.0000	time 0.2682 (0.2478)	loss 1.4380 (1.3700)	grad_norm 0.3826 (0.3937)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:55:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:28 lr 0.000014	 wd 0.0000	time 0.2044 (0.2461)	loss 1.4578 (1.3712)	grad_norm 0.3936 (0.3935)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:56:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:02 lr 0.000014	 wd 0.0000	time 0.1928 (0.2445)	loss 1.3259 (1.3704)	grad_norm 0.3821 (0.3931)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:56:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:37 lr 0.000015	 wd 0.0000	time 0.2020 (0.2433)	loss 1.4018 (1.3714)	grad_norm 0.3721 (0.3929)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:56:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:13 lr 0.000015	 wd 0.0000	time 0.2971 (0.2433)	loss 1.2080 (1.3726)	grad_norm 0.3709 (0.3930)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:57:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:48 lr 0.000015	 wd 0.0000	time 0.2011 (0.2424)	loss 1.6379 (1.3731)	grad_norm 0.3992 (0.3931)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:57:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:24 lr 0.000016	 wd 0.0000	time 0.2199 (0.2413)	loss 1.4548 (1.3712)	grad_norm 0.3636 (0.3927)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 15:57:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.1530 (0.2389)	loss 1.1115 (1.3724)	grad_norm 0.3749 (0.3927)	loss_scale 16384.0000 (8211.6529)	mem 6787MB
[2024-07-12 15:57:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 1 training takes 0:10:02
[2024-07-12 15:58:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 40.394 (40.394)	Loss 0.4209 (0.4209)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 15:58:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.028 Acc@5 97.208
[2024-07-12 15:58:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 15:58:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-12 15:59:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][0/2502]	eta 11:48:17 lr 0.000016	 wd 0.0000	time 16.9853 (16.9853)	loss 1.6514 (1.6514)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 15:59:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:14:59 lr 0.000016	 wd 0.0000	time 0.2121 (0.3745)	loss 1.4182 (1.3448)	grad_norm 0.3960 (0.3878)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:00:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:19 lr 0.000017	 wd 0.0000	time 0.2133 (0.3734)	loss 1.4443 (1.3646)	grad_norm 0.3498 (0.3896)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:00:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:11:52 lr 0.000017	 wd 0.0000	time 0.2025 (0.3235)	loss 1.2937 (1.3755)	grad_norm 0.3980 (0.3901)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:00:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:10:18 lr 0.000017	 wd 0.0000	time 0.1948 (0.2941)	loss 1.5285 (1.3732)	grad_norm 0.3947 (0.3889)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:01:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:09:14 lr 0.000018	 wd 0.0000	time 0.2410 (0.2769)	loss 1.5335 (1.3716)	grad_norm 0.3600 (0.3882)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:01:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:08:35 lr 0.000018	 wd 0.0000	time 0.2234 (0.2713)	loss 1.2698 (1.3662)	grad_norm 0.3749 (0.3886)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:01:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:08:00 lr 0.000018	 wd 0.0000	time 0.1994 (0.2664)	loss 1.2509 (1.3683)	grad_norm 0.3843 (0.3883)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:02:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:07:21 lr 0.000019	 wd 0.0000	time 0.2165 (0.2597)	loss 1.4424 (1.3666)	grad_norm 0.3651 (0.3885)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:02:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:06:47 lr 0.000019	 wd 0.0000	time 0.2126 (0.2543)	loss 1.5328 (1.3710)	grad_norm 0.3977 (nan)	loss_scale 8192.0000 (15765.7358)	mem 6787MB
[2024-07-12 16:03:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:06:18 lr 0.000019	 wd 0.0000	time 0.2804 (0.2522)	loss 1.4541 (1.3722)	grad_norm 0.3723 (nan)	loss_scale 8192.0000 (15009.1189)	mem 6787MB
[2024-07-12 16:03:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:05:57 lr 0.000020	 wd 0.0000	time 0.1970 (0.2548)	loss 1.1816 (1.3717)	grad_norm 0.3705 (nan)	loss_scale 8192.0000 (14389.9437)	mem 6787MB
[2024-07-12 16:03:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:05:27 lr 0.000020	 wd 0.0000	time 0.1980 (0.2513)	loss 1.2563 (1.3707)	grad_norm 0.3845 (nan)	loss_scale 8192.0000 (13873.8784)	mem 6787MB
[2024-07-12 16:04:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:04:58 lr 0.000020	 wd 0.0000	time 0.2016 (0.2483)	loss 1.7111 (1.3735)	grad_norm 0.3830 (nan)	loss_scale 8192.0000 (13437.1468)	mem 6787MB
[2024-07-12 16:04:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:04:31 lr 0.000020	 wd 0.0000	time 0.2099 (0.2464)	loss 1.4447 (1.3735)	grad_norm 0.3987 (nan)	loss_scale 8192.0000 (13062.7609)	mem 6787MB
[2024-07-12 16:05:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:04:07 lr 0.000021	 wd 0.0000	time 0.2282 (0.2469)	loss 1.5045 (1.3716)	grad_norm 0.3565 (nan)	loss_scale 8192.0000 (12738.2598)	mem 6787MB
[2024-07-12 16:05:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:03:41 lr 0.000021	 wd 0.0000	time 0.2085 (0.2451)	loss 1.2827 (1.3719)	grad_norm 0.4102 (nan)	loss_scale 8192.0000 (12454.2961)	mem 6787MB
[2024-07-12 16:05:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:03:14 lr 0.000021	 wd 0.0000	time 0.1882 (0.2431)	loss 1.4565 (1.3716)	grad_norm 0.3912 (nan)	loss_scale 8192.0000 (12203.7202)	mem 6787MB
[2024-07-12 16:06:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:02:49 lr 0.000022	 wd 0.0000	time 0.2130 (0.2417)	loss 1.3547 (1.3716)	grad_norm 0.3810 (nan)	loss_scale 8192.0000 (11980.9706)	mem 6787MB
[2024-07-12 16:06:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:02:25 lr 0.000022	 wd 0.0000	time 0.2197 (0.2421)	loss 1.0849 (1.3701)	grad_norm 0.3610 (nan)	loss_scale 8192.0000 (11781.6560)	mem 6787MB
[2024-07-12 16:06:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:01 lr 0.000022	 wd 0.0000	time 0.2019 (0.2413)	loss 1.1139 (1.3683)	grad_norm 0.3636 (nan)	loss_scale 8192.0000 (11602.2629)	mem 6787MB
[2024-07-12 16:07:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:36 lr 0.000023	 wd 0.0000	time 0.2097 (0.2402)	loss 1.4507 (1.3685)	grad_norm 0.3793 (nan)	loss_scale 8192.0000 (11439.9467)	mem 6787MB
[2024-07-12 16:07:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:12 lr 0.000023	 wd 0.0000	time 0.1997 (0.2390)	loss 1.5086 (1.3684)	grad_norm 0.3869 (nan)	loss_scale 8192.0000 (11292.3798)	mem 6787MB
[2024-07-12 16:08:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:48 lr 0.000023	 wd 0.0000	time 0.2193 (0.2388)	loss 1.5752 (1.3685)	grad_norm 0.3834 (nan)	loss_scale 8192.0000 (11157.6393)	mem 6787MB
[2024-07-12 16:08:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:24 lr 0.000024	 wd 0.0000	time 0.2543 (0.2387)	loss 1.4210 (1.3673)	grad_norm 0.3588 (nan)	loss_scale 8192.0000 (11034.1224)	mem 6787MB
[2024-07-12 16:08:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.1519 (0.2365)	loss 1.4469 (1.3673)	grad_norm 0.3591 (nan)	loss_scale 8192.0000 (10920.4830)	mem 6787MB
[2024-07-12 16:08:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 2 training takes 0:09:55
[2024-07-12 16:09:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 23.200 (23.200)	Loss 0.4175 (0.4175)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 16:09:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.006 Acc@5 97.222
[2024-07-12 16:09:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 16:09:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-12 16:10:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][0/2502]	eta 1 day, 3:14:46 lr 0.000024	 wd 0.0000	time 39.2031 (39.2031)	loss 1.0950 (1.0950)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:10:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:23:55 lr 0.000024	 wd 0.0000	time 0.2230 (0.5976)	loss 1.6792 (1.3530)	grad_norm 0.3984 (0.3900)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:10:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:15:25 lr 0.000025	 wd 0.0000	time 0.1973 (0.4022)	loss 1.4974 (1.3652)	grad_norm 0.3757 (0.3859)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:11:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:12:43 lr 0.000025	 wd 0.0000	time 0.2053 (0.3468)	loss 1.4087 (1.3540)	grad_norm 0.3609 (0.3855)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:11:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:11:31 lr 0.000025	 wd 0.0000	time 0.2181 (0.3289)	loss 1.5266 (1.3604)	grad_norm 0.3799 (0.3837)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:11:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:10:09 lr 0.000026	 wd 0.0000	time 0.1952 (0.3044)	loss 1.3917 (1.3579)	grad_norm 0.3836 (0.3985)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:12:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:09:10 lr 0.000026	 wd 0.0000	time 0.2004 (0.2893)	loss 1.4670 (1.3575)	grad_norm 0.3909 (0.3965)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:12:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:08:24 lr 0.000026	 wd 0.0000	time 0.2082 (0.2799)	loss 1.4969 (1.3596)	grad_norm 0.3847 (0.3966)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:13:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:07:50 lr 0.000027	 wd 0.0000	time 0.2108 (0.2766)	loss 1.1361 (1.3602)	grad_norm 0.3645 (0.3963)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:13:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:07:12 lr 0.000027	 wd 0.0000	time 0.2181 (0.2700)	loss 1.3456 (1.3618)	grad_norm 0.3740 (0.3951)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:13:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:06:36 lr 0.000027	 wd 0.0000	time 0.1835 (0.2641)	loss 1.6100 (1.3651)	grad_norm 0.4533 (0.3939)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:14:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:06:04 lr 0.000028	 wd 0.0000	time 0.2511 (0.2597)	loss 1.4037 (1.3673)	grad_norm 0.3771 (0.3946)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:14:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:05:37 lr 0.000028	 wd 0.0000	time 0.2029 (0.2593)	loss 1.3041 (1.3662)	grad_norm 0.3316 (0.3939)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:14:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:05:08 lr 0.000028	 wd 0.0000	time 0.1991 (0.2564)	loss 1.2232 (1.3690)	grad_norm 0.5028 (0.3936)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:15:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:04:39 lr 0.000028	 wd 0.0000	time 0.1938 (0.2534)	loss 1.2259 (1.3697)	grad_norm 0.4030 (0.3926)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:15:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:04:11 lr 0.000029	 wd 0.0000	time 0.2048 (0.2505)	loss 1.3912 (1.3701)	grad_norm 0.3888 (0.3915)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:16:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:03:44 lr 0.000029	 wd 0.0000	time 0.2295 (0.2494)	loss 1.4153 (1.3688)	grad_norm 0.3639 (0.3913)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:16:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:03:20 lr 0.000029	 wd 0.0000	time 0.2124 (0.2495)	loss 1.0676 (1.3679)	grad_norm 0.3674 (0.3919)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:16:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:02:53 lr 0.000030	 wd 0.0000	time 0.2093 (0.2478)	loss 1.6356 (1.3695)	grad_norm 0.3934 (0.3916)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:17:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:02:28 lr 0.000030	 wd 0.0000	time 0.2043 (0.2461)	loss 1.5427 (1.3692)	grad_norm 0.4147 (0.3919)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:17:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:03 lr 0.000030	 wd 0.0000	time 0.2431 (0.2452)	loss 1.1634 (1.3699)	grad_norm 0.3862 (0.3919)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:17:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:38 lr 0.000031	 wd 0.0000	time 0.2068 (0.2451)	loss 1.2273 (1.3683)	grad_norm 0.3776 (0.3914)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:18:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:13 lr 0.000031	 wd 0.0000	time 0.2411 (0.2438)	loss 1.3586 (1.3694)	grad_norm 0.3841 (0.3911)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:18:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:00:48 lr 0.000031	 wd 0.0000	time 0.2113 (0.2426)	loss 1.5354 (1.3700)	grad_norm 0.4046 (0.3909)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:19:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:24 lr 0.000032	 wd 0.0000	time 0.2414 (0.2417)	loss 0.9889 (1.3693)	grad_norm 0.3552 (0.3910)	loss_scale 16384.0000 (8430.8338)	mem 6787MB
[2024-07-12 16:19:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000032	 wd 0.0000	time 0.1583 (0.2396)	loss 1.5500 (1.3698)	grad_norm 0.3831 (0.3906)	loss_scale 16384.0000 (8748.8333)	mem 6787MB
[2024-07-12 16:19:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 3 training takes 0:10:09
[2024-07-12 16:19:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 27.247 (27.247)	Loss 0.4194 (0.4194)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 16:20:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.028 Acc@5 97.226
[2024-07-12 16:20:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 16:20:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.04%
[2024-07-12 16:20:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][0/2502]	eta 11:23:06 lr 0.000032	 wd 0.0000	time 16.3813 (16.3813)	loss 1.4341 (1.4341)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:20:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:16:55 lr 0.000032	 wd 0.0000	time 0.4239 (0.4226)	loss 1.1484 (1.3807)	grad_norm 0.3914 (0.3853)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:21:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:13:02 lr 0.000033	 wd 0.0000	time 0.2318 (0.3397)	loss 1.1372 (1.3817)	grad_norm 0.3751 (0.3894)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 16:21:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:10:51 lr 0.000033	 wd 0.0000	time 0.1881 (0.2961)	loss 1.1187 (1.3815)	grad_norm 0.4446 (nan)	loss_scale 8192.0000 (14805.4751)	mem 6787MB
[2024-07-12 16:22:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:09:35 lr 0.000033	 wd 0.0000	time 0.2186 (0.2738)	loss 1.5803 (1.3772)	grad_norm 0.3599 (nan)	loss_scale 8192.0000 (13156.2294)	mem 6787MB
[2024-07-12 16:22:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:08:49 lr 0.000034	 wd 0.0000	time 0.2495 (0.2646)	loss 1.3417 (1.3708)	grad_norm 0.4040 (nan)	loss_scale 8192.0000 (12165.3653)	mem 6787MB
[2024-07-12 16:22:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:08:35 lr 0.000034	 wd 0.0000	time 0.2259 (0.2711)	loss 1.4902 (1.3666)	grad_norm 0.3872 (nan)	loss_scale 8192.0000 (11504.2396)	mem 6787MB
[2024-07-12 16:23:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:07:52 lr 0.000034	 wd 0.0000	time 0.2102 (0.2623)	loss 1.2874 (1.3665)	grad_norm 0.3747 (nan)	loss_scale 8192.0000 (11031.7375)	mem 6787MB
[2024-07-12 16:23:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:07:15 lr 0.000035	 wd 0.0000	time 0.2061 (0.2556)	loss 1.4754 (1.3659)	grad_norm 0.3705 (nan)	loss_scale 8192.0000 (10677.2135)	mem 6787MB
[2024-07-12 16:23:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:06:44 lr 0.000035	 wd 0.0000	time 0.2813 (0.2524)	loss 1.4945 (1.3672)	grad_norm 0.3651 (nan)	loss_scale 8192.0000 (10401.3851)	mem 6787MB
[2024-07-12 16:24:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:06:19 lr 0.000035	 wd 0.0000	time 0.2172 (0.2524)	loss 1.5372 (1.3678)	grad_norm 0.3560 (nan)	loss_scale 8192.0000 (10180.6673)	mem 6787MB
[2024-07-12 16:24:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:05:49 lr 0.000036	 wd 0.0000	time 0.2074 (0.2490)	loss 1.5369 (1.3687)	grad_norm 0.3758 (nan)	loss_scale 8192.0000 (10000.0436)	mem 6787MB
[2024-07-12 16:25:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:05:20 lr 0.000036	 wd 0.0000	time 0.2204 (0.2461)	loss 1.0882 (1.3672)	grad_norm 0.3774 (nan)	loss_scale 8192.0000 (9849.4988)	mem 6787MB
[2024-07-12 16:25:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:04:53 lr 0.000036	 wd 0.0000	time 0.2033 (0.2439)	loss 1.5796 (1.3665)	grad_norm 0.3888 (nan)	loss_scale 8192.0000 (9722.0968)	mem 6787MB
[2024-07-12 16:25:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:04:28 lr 0.000036	 wd 0.0000	time 0.4487 (0.2439)	loss 1.5061 (1.3666)	grad_norm 0.3896 (nan)	loss_scale 8192.0000 (9612.8822)	mem 6787MB
[2024-07-12 16:26:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:04:02 lr 0.000037	 wd 0.0000	time 0.2037 (0.2421)	loss 1.2593 (1.3690)	grad_norm 0.3524 (nan)	loss_scale 8192.0000 (9518.2199)	mem 6787MB
[2024-07-12 16:26:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:03:36 lr 0.000037	 wd 0.0000	time 0.2076 (0.2403)	loss 1.0589 (1.3686)	grad_norm 0.3679 (nan)	loss_scale 8192.0000 (9435.3829)	mem 6787MB
[2024-07-12 16:26:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:03:11 lr 0.000037	 wd 0.0000	time 0.2032 (0.2388)	loss 1.5542 (1.3680)	grad_norm 0.3626 (nan)	loss_scale 8192.0000 (9362.2857)	mem 6787MB
[2024-07-12 16:27:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:02:47 lr 0.000038	 wd 0.0000	time 0.2157 (0.2384)	loss 1.3797 (1.3678)	grad_norm 0.3521 (nan)	loss_scale 8192.0000 (9297.3059)	mem 6787MB
[2024-07-12 16:27:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:23 lr 0.000038	 wd 0.0000	time 0.1971 (0.2387)	loss 1.6497 (1.3677)	grad_norm 0.3774 (nan)	loss_scale 8192.0000 (9239.1625)	mem 6787MB
[2024-07-12 16:28:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:01:59 lr 0.000038	 wd 0.0000	time 0.2194 (0.2376)	loss 1.4727 (1.3671)	grad_norm 0.3815 (nan)	loss_scale 8192.0000 (9186.8306)	mem 6787MB
[2024-07-12 16:28:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:35 lr 0.000039	 wd 0.0000	time 0.2225 (0.2365)	loss 1.4323 (1.3676)	grad_norm nan (nan)	loss_scale 4096.0000 (9135.5812)	mem 6787MB
[2024-07-12 16:28:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:11 lr 0.000039	 wd 0.0000	time 0.2066 (0.2360)	loss 0.9887 (1.3652)	grad_norm 0.3602 (nan)	loss_scale 4096.0000 (8906.6134)	mem 6787MB
[2024-07-12 16:29:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:47 lr 0.000039	 wd 0.0000	time 0.2164 (0.2362)	loss 1.2947 (1.3654)	grad_norm 0.3533 (nan)	loss_scale 4096.0000 (8697.5472)	mem 6787MB
[2024-07-12 16:29:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:24 lr 0.000040	 wd 0.0000	time 0.1977 (0.2356)	loss 1.5835 (1.3653)	grad_norm 0.3422 (nan)	loss_scale 4096.0000 (8505.8959)	mem 6787MB
[2024-07-12 16:29:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.1604 (0.2335)	loss 0.9319 (1.3651)	grad_norm 0.3610 (nan)	loss_scale 4096.0000 (8329.5706)	mem 6787MB
[2024-07-12 16:30:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 4 training takes 0:09:48
[2024-07-12 16:30:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 34.689 (34.689)	Loss 0.4192 (0.4192)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 16:30:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.082 Acc@5 97.210
[2024-07-12 16:30:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 16:30:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.08%
[2024-07-12 16:30:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saving......
[2024-07-12 16:30:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saved !!!
[2024-07-12 16:31:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][0/2502]	eta 11:11:39 lr 0.000040	 wd 0.0000	time 16.1068 (16.1068)	loss 1.6010 (1.6010)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:31:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:14:48 lr 0.000040	 wd 0.0000	time 0.2143 (0.3699)	loss 1.2920 (1.3851)	grad_norm 0.3955 (0.3779)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:31:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:11:08 lr 0.000040	 wd 0.0000	time 0.2474 (0.2905)	loss 1.4790 (1.3889)	grad_norm 0.3796 (0.3841)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:32:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:10:50 lr 0.000040	 wd 0.0000	time 0.2035 (0.2954)	loss 1.6180 (1.3784)	grad_norm 0.3769 (0.3834)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:32:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:09:36 lr 0.000040	 wd 0.0000	time 0.2399 (0.2741)	loss 1.0055 (1.3800)	grad_norm 0.3914 (0.3841)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:33:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:08:42 lr 0.000040	 wd 0.0000	time 0.2061 (0.2612)	loss 1.5196 (1.3753)	grad_norm 0.3609 (0.3835)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:33:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:08:01 lr 0.000040	 wd 0.0000	time 0.1917 (0.2529)	loss 1.4352 (1.3761)	grad_norm 0.3929 (0.3885)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:33:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:07:32 lr 0.000040	 wd 0.0000	time 0.2510 (0.2513)	loss 1.4587 (1.3737)	grad_norm 0.3992 (0.3871)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:34:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:07:03 lr 0.000040	 wd 0.0000	time 0.2142 (0.2489)	loss 0.9394 (1.3677)	grad_norm 0.3588 (0.3864)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:34:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:06:32 lr 0.000040	 wd 0.0000	time 0.2196 (0.2449)	loss 0.9076 (1.3670)	grad_norm 0.5537 (0.3874)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:34:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:06:02 lr 0.000040	 wd 0.0000	time 0.2152 (0.2415)	loss 1.5296 (1.3702)	grad_norm 0.3824 (0.3867)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:35:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:05:37 lr 0.000040	 wd 0.0000	time 0.2234 (0.2406)	loss 1.5485 (1.3662)	grad_norm 0.3641 (0.3863)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:35:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:05:14 lr 0.000040	 wd 0.0000	time 0.2000 (0.2416)	loss 1.5533 (1.3641)	grad_norm 0.4889 (0.3895)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:36:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:04:47 lr 0.000040	 wd 0.0000	time 0.2071 (0.2394)	loss 1.5091 (1.3647)	grad_norm 0.3824 (0.3891)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:36:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:04:21 lr 0.000040	 wd 0.0000	time 0.1956 (0.2375)	loss 1.5302 (1.3642)	grad_norm 0.3626 (0.3881)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:36:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:03:56 lr 0.000040	 wd 0.0000	time 0.2750 (0.2361)	loss 1.3864 (1.3642)	grad_norm 0.3743 (0.3880)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:37:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:03:33 lr 0.000040	 wd 0.0000	time 0.2967 (0.2365)	loss 1.4527 (1.3662)	grad_norm 0.3896 (0.3872)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:37:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:03:08 lr 0.000040	 wd 0.0000	time 0.1958 (0.2355)	loss 1.3549 (1.3658)	grad_norm 0.3475 (0.3878)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:37:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:02:44 lr 0.000040	 wd 0.0000	time 0.2179 (0.2346)	loss 0.9932 (1.3652)	grad_norm 0.3564 (0.3890)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:38:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:20 lr 0.000040	 wd 0.0000	time 0.2272 (0.2337)	loss 1.4815 (1.3672)	grad_norm 0.3683 (0.3887)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:38:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:01:57 lr 0.000040	 wd 0.0000	time 0.2524 (0.2338)	loss 1.5176 (1.3681)	grad_norm 0.3690 (0.3886)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:39:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:34 lr 0.000040	 wd 0.0000	time 0.2281 (0.2342)	loss 1.5084 (1.3687)	grad_norm 0.3662 (0.3881)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:39:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:10 lr 0.000040	 wd 0.0000	time 0.2108 (0.2334)	loss 1.2410 (1.3667)	grad_norm 0.3684 (0.3877)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:39:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:46 lr 0.000040	 wd 0.0000	time 0.2018 (0.2327)	loss 0.9012 (1.3658)	grad_norm 0.3562 (0.3877)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:40:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:23 lr 0.000040	 wd 0.0000	time 0.2291 (0.2324)	loss 1.5092 (1.3659)	grad_norm 0.4156 (0.3876)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:40:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000040	 wd 0.0000	time 0.1663 (0.2314)	loss 1.5202 (1.3658)	grad_norm 0.3806 (0.3874)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:40:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 5 training takes 0:09:45
[2024-07-12 16:40:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 18.854 (18.854)	Loss 0.4155 (0.4155)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 16:41:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.002 Acc@5 97.238
[2024-07-12 16:41:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 16:41:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.08%
[2024-07-12 16:41:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][0/2502]	eta 11:54:25 lr 0.000040	 wd 0.0000	time 17.1325 (17.1325)	loss 1.5732 (1.5732)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:41:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:16:25 lr 0.000040	 wd 0.0000	time 0.2010 (0.4103)	loss 1.2092 (1.3363)	grad_norm 0.3851 (0.3821)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:42:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:12:21 lr 0.000040	 wd 0.0000	time 0.2024 (0.3223)	loss 1.5975 (1.3578)	grad_norm 0.3665 (0.3791)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:42:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:10:28 lr 0.000040	 wd 0.0000	time 0.1933 (0.2853)	loss 1.4636 (1.3684)	grad_norm 0.4303 (0.3806)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:42:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:09:19 lr 0.000040	 wd 0.0000	time 0.1825 (0.2664)	loss 1.3764 (1.3621)	grad_norm 0.3901 (0.3796)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:43:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:08:34 lr 0.000040	 wd 0.0000	time 0.2124 (0.2570)	loss 1.3595 (1.3565)	grad_norm 0.3542 (0.3911)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:43:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:08:03 lr 0.000040	 wd 0.0000	time 0.1731 (0.2540)	loss 1.5937 (1.3607)	grad_norm 0.3818 (0.3929)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:44:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:07:27 lr 0.000040	 wd 0.0000	time 0.2141 (0.2485)	loss 1.4179 (1.3631)	grad_norm 0.4065 (0.3909)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:44:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:06:55 lr 0.000040	 wd 0.0000	time 0.2018 (0.2440)	loss 1.0112 (1.3621)	grad_norm 0.4847 (0.3898)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:44:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:06:25 lr 0.000040	 wd 0.0000	time 0.2147 (0.2405)	loss 1.6531 (1.3662)	grad_norm 0.3388 (0.3887)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:45:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:06:02 lr 0.000040	 wd 0.0000	time 0.2081 (0.2410)	loss 1.3065 (1.3616)	grad_norm 0.4030 (0.3877)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 16:45:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:05:36 lr 0.000040	 wd 0.0000	time 0.1867 (0.2399)	loss 1.2006 (1.3608)	grad_norm 0.3740 (0.3872)	loss_scale 8192.0000 (4118.3215)	mem 6787MB
[2024-07-12 16:45:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:05:09 lr 0.000040	 wd 0.0000	time 0.2311 (0.2379)	loss 1.0555 (1.3624)	grad_norm 0.3711 (0.3865)	loss_scale 8192.0000 (4457.5121)	mem 6787MB
[2024-07-12 16:46:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:04:43 lr 0.000040	 wd 0.0000	time 0.2009 (0.2360)	loss 1.4076 (1.3615)	grad_norm 0.4043 (0.3857)	loss_scale 8192.0000 (4744.5596)	mem 6787MB
[2024-07-12 16:46:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:04:19 lr 0.000040	 wd 0.0000	time 0.2086 (0.2355)	loss 1.6562 (1.3602)	grad_norm 0.4413 (0.3862)	loss_scale 8192.0000 (4990.6296)	mem 6787MB
[2024-07-12 16:47:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:03:56 lr 0.000040	 wd 0.0000	time 0.1859 (0.2359)	loss 1.4617 (1.3617)	grad_norm 0.3523 (0.3855)	loss_scale 8192.0000 (5203.9121)	mem 6787MB
[2024-07-12 16:47:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:03:31 lr 0.000040	 wd 0.0000	time 0.2158 (0.2347)	loss 1.5375 (1.3604)	grad_norm 0.3742 (0.3854)	loss_scale 8192.0000 (5390.5509)	mem 6787MB
[2024-07-12 16:47:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:03:07 lr 0.000040	 wd 0.0000	time 0.1915 (0.2334)	loss 1.5061 (1.3597)	grad_norm 0.4199 (0.3853)	loss_scale 8192.0000 (5555.2451)	mem 6787MB
[2024-07-12 16:48:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:02:43 lr 0.000040	 wd 0.0000	time 0.2460 (0.2328)	loss 1.1583 (1.3595)	grad_norm 0.3764 (0.3852)	loss_scale 8192.0000 (5701.6502)	mem 6787MB
[2024-07-12 16:48:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:20 lr 0.000040	 wd 0.0000	time 0.2018 (0.2341)	loss 1.2781 (1.3590)	grad_norm 0.3781 (0.3850)	loss_scale 8192.0000 (5832.6523)	mem 6787MB
[2024-07-12 16:48:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:01:57 lr 0.000039	 wd 0.0000	time 0.1983 (0.2335)	loss 0.9886 (1.3589)	grad_norm 0.4050 (0.3853)	loss_scale 8192.0000 (5950.5607)	mem 6787MB
[2024-07-12 16:49:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:33 lr 0.000039	 wd 0.0000	time 0.1927 (0.2326)	loss 1.5668 (1.3585)	grad_norm 0.3786 (0.3858)	loss_scale 8192.0000 (6057.2451)	mem 6787MB
[2024-07-12 16:49:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:10 lr 0.000039	 wd 0.0000	time 0.2521 (0.2320)	loss 1.2627 (1.3590)	grad_norm 0.3689 (0.3863)	loss_scale 8192.0000 (6154.2353)	mem 6787MB
[2024-07-12 16:50:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:46 lr 0.000039	 wd 0.0000	time 0.2879 (0.2326)	loss 1.4582 (1.3597)	grad_norm 0.3903 (0.3859)	loss_scale 8192.0000 (6242.7953)	mem 6787MB
[2024-07-12 16:50:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:23 lr 0.000039	 wd 0.0000	time 0.1997 (0.2320)	loss 1.1993 (1.3590)	grad_norm 0.3445 (0.3856)	loss_scale 8192.0000 (6323.9783)	mem 6787MB
[2024-07-12 16:50:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.1630 (0.2300)	loss 1.0397 (1.3596)	grad_norm 0.3576 (0.3852)	loss_scale 8192.0000 (6398.6693)	mem 6787MB
[2024-07-12 16:50:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 6 training takes 0:09:39
[2024-07-12 16:51:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 19.292 (19.292)	Loss 0.4175 (0.4175)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 16:51:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.030 Acc@5 97.226
[2024-07-12 16:51:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 16:51:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.08%
[2024-07-12 16:51:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][0/2502]	eta 1 day, 1:20:33 lr 0.000039	 wd 0.0000	time 36.4644 (36.4644)	loss 1.5939 (1.5939)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:52:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:22:38 lr 0.000039	 wd 0.0000	time 0.2047 (0.5656)	loss 1.2357 (1.3853)	grad_norm 0.4036 (0.3798)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:52:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:50 lr 0.000039	 wd 0.0000	time 0.2014 (0.3867)	loss 1.4496 (1.3595)	grad_norm 0.4847 (0.3835)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:53:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:12:14 lr 0.000039	 wd 0.0000	time 0.2519 (0.3337)	loss 1.3092 (1.3571)	grad_norm 0.3758 (0.3851)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:53:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:10:49 lr 0.000039	 wd 0.0000	time 0.1815 (0.3088)	loss 1.3400 (1.3591)	grad_norm 0.4126 (0.3824)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:53:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:09:37 lr 0.000039	 wd 0.0000	time 0.2114 (0.2885)	loss 1.4332 (1.3544)	grad_norm 0.3578 (0.3843)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:54:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:08:45 lr 0.000039	 wd 0.0000	time 0.1996 (0.2761)	loss 1.3374 (1.3581)	grad_norm 0.3515 (0.3850)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:54:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:08:01 lr 0.000039	 wd 0.0000	time 0.2253 (0.2671)	loss 1.4869 (1.3619)	grad_norm 0.3681 (0.3842)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:54:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:07:31 lr 0.000039	 wd 0.0000	time 0.2048 (0.2653)	loss 0.9049 (1.3642)	grad_norm 0.3699 (0.3851)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:55:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:06:56 lr 0.000039	 wd 0.0000	time 0.2193 (0.2597)	loss 1.0869 (1.3661)	grad_norm 0.4295 (0.3850)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:55:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:06:22 lr 0.000039	 wd 0.0000	time 0.2209 (0.2549)	loss 1.1761 (1.3621)	grad_norm 0.3644 (0.3845)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:55:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:05:51 lr 0.000039	 wd 0.0000	time 0.2069 (0.2509)	loss 1.2381 (1.3606)	grad_norm 0.3534 (0.3847)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:56:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:05:24 lr 0.000039	 wd 0.0000	time 0.2217 (0.2492)	loss 1.4179 (1.3594)	grad_norm 0.3623 (0.3860)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:56:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:04:59 lr 0.000039	 wd 0.0000	time 0.2395 (0.2488)	loss 1.4751 (1.3594)	grad_norm 0.5716 (0.3874)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:57:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:04:31 lr 0.000039	 wd 0.0000	time 0.2195 (0.2462)	loss 1.4117 (1.3603)	grad_norm 0.3579 (0.3867)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:57:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:04:04 lr 0.000039	 wd 0.0000	time 0.2305 (0.2443)	loss 1.6479 (1.3596)	grad_norm 0.4001 (0.3858)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:57:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:03:38 lr 0.000039	 wd 0.0000	time 0.2000 (0.2427)	loss 1.5174 (1.3614)	grad_norm 0.3676 (0.3851)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:58:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:03:15 lr 0.000039	 wd 0.0000	time 0.1802 (0.2433)	loss 1.5745 (1.3616)	grad_norm 0.3832 (0.3844)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:58:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:02:50 lr 0.000039	 wd 0.0000	time 0.2044 (0.2424)	loss 1.1258 (1.3611)	grad_norm 0.3879 (0.3844)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:59:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:25 lr 0.000039	 wd 0.0000	time 0.2234 (0.2412)	loss 1.3200 (1.3606)	grad_norm 0.4143 (0.3845)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:59:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:00 lr 0.000039	 wd 0.0000	time 0.2580 (0.2400)	loss 1.4267 (1.3617)	grad_norm 0.3773 (0.3841)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 16:59:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:36 lr 0.000039	 wd 0.0000	time 0.2559 (0.2399)	loss 1.5065 (1.3627)	grad_norm 0.4402 (0.3844)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:00:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:12 lr 0.000039	 wd 0.0000	time 0.2397 (0.2391)	loss 1.1120 (1.3608)	grad_norm 0.3591 (0.3845)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:00:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:48 lr 0.000039	 wd 0.0000	time 0.2251 (0.2381)	loss 1.5820 (1.3613)	grad_norm 0.3833 (0.3853)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:00:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:24 lr 0.000039	 wd 0.0000	time 0.1953 (0.2371)	loss 1.4722 (1.3608)	grad_norm 0.3746 (0.3849)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:01:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000039	 wd 0.0000	time 0.1609 (0.2351)	loss 1.2805 (1.3611)	grad_norm 0.3786 (0.3845)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:01:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 7 training takes 0:09:52
[2024-07-12 17:01:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 31.967 (31.967)	Loss 0.4248 (0.4248)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 17:02:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.054 Acc@5 97.238
[2024-07-12 17:02:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 17:02:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.08%
[2024-07-12 17:02:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][0/2502]	eta 11:43:45 lr 0.000039	 wd 0.0000	time 16.8767 (16.8767)	loss 1.4251 (1.4251)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:02:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:15:11 lr 0.000039	 wd 0.0000	time 0.2285 (0.3794)	loss 1.4625 (1.3986)	grad_norm 0.3656 (0.3923)	loss_scale 16384.0000 (9003.0891)	mem 6787MB
[2024-07-12 17:03:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:12:47 lr 0.000039	 wd 0.0000	time 0.2234 (0.3335)	loss 1.5378 (1.3738)	grad_norm 0.3796 (0.3857)	loss_scale 16384.0000 (12675.1841)	mem 6787MB
[2024-07-12 17:03:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:10:43 lr 0.000038	 wd 0.0000	time 0.2030 (0.2924)	loss 1.3602 (1.3705)	grad_norm 0.3734 (0.3841)	loss_scale 16384.0000 (13907.3488)	mem 6787MB
[2024-07-12 17:03:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:09:31 lr 0.000038	 wd 0.0000	time 0.2153 (0.2719)	loss 1.6277 (1.3668)	grad_norm 0.3849 (0.3811)	loss_scale 16384.0000 (14524.9676)	mem 6787MB
[2024-07-12 17:04:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:08:39 lr 0.000038	 wd 0.0000	time 0.1877 (0.2593)	loss 1.4444 (1.3699)	grad_norm 0.3610 (0.3812)	loss_scale 16384.0000 (14896.0319)	mem 6787MB
[2024-07-12 17:04:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:08:06 lr 0.000038	 wd 0.0000	time 0.2890 (0.2556)	loss 1.5343 (1.3744)	grad_norm 0.3657 (0.3837)	loss_scale 16384.0000 (15143.6140)	mem 6787MB
[2024-07-12 17:04:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:07:34 lr 0.000038	 wd 0.0000	time 0.2677 (0.2524)	loss 1.6388 (1.3725)	grad_norm 0.3753 (0.3855)	loss_scale 16384.0000 (15320.5592)	mem 6787MB
[2024-07-12 17:05:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:07:01 lr 0.000038	 wd 0.0000	time 0.2480 (0.2474)	loss 1.4795 (1.3702)	grad_norm 0.3619 (0.3850)	loss_scale 16384.0000 (15453.3233)	mem 6787MB
[2024-07-12 17:05:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:06:30 lr 0.000038	 wd 0.0000	time 0.2034 (0.2435)	loss 1.6584 (1.3703)	grad_norm 0.5722 (0.3850)	loss_scale 16384.0000 (15556.6171)	mem 6787MB
[2024-07-12 17:06:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:06:03 lr 0.000038	 wd 0.0000	time 0.2252 (0.2419)	loss 1.4963 (1.3666)	grad_norm 0.3902 (nan)	loss_scale 8192.0000 (15361.0230)	mem 6787MB
[2024-07-12 17:06:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:05:38 lr 0.000038	 wd 0.0000	time 0.2013 (0.2412)	loss 1.3367 (1.3635)	grad_norm 0.3754 (nan)	loss_scale 8192.0000 (14709.8856)	mem 6787MB
[2024-07-12 17:06:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:05:11 lr 0.000038	 wd 0.0000	time 0.1874 (0.2389)	loss 1.4653 (1.3638)	grad_norm 0.3719 (nan)	loss_scale 8192.0000 (14167.1807)	mem 6787MB
[2024-07-12 17:07:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:04:44 lr 0.000038	 wd 0.0000	time 0.2013 (0.2369)	loss 1.1791 (1.3622)	grad_norm 0.3773 (nan)	loss_scale 8192.0000 (13707.9047)	mem 6787MB
[2024-07-12 17:07:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:04:19 lr 0.000038	 wd 0.0000	time 0.2423 (0.2354)	loss 0.9607 (1.3640)	grad_norm 0.4381 (nan)	loss_scale 8192.0000 (13314.1927)	mem 6787MB
[2024-07-12 17:07:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:03:56 lr 0.000038	 wd 0.0000	time 0.1677 (0.2362)	loss 0.8311 (1.3617)	grad_norm 0.3952 (nan)	loss_scale 8192.0000 (12972.9407)	mem 6787MB
[2024-07-12 17:08:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:03:32 lr 0.000038	 wd 0.0000	time 0.2714 (0.2356)	loss 1.2330 (1.3590)	grad_norm 0.3599 (nan)	loss_scale 8192.0000 (12674.3186)	mem 6787MB
[2024-07-12 17:08:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:03:08 lr 0.000038	 wd 0.0000	time 0.2458 (0.2344)	loss 1.1840 (1.3608)	grad_norm 0.3617 (nan)	loss_scale 8192.0000 (12410.8078)	mem 6787MB
[2024-07-12 17:09:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:02:43 lr 0.000038	 wd 0.0000	time 0.2064 (0.2333)	loss 1.5380 (1.3618)	grad_norm 0.3615 (nan)	loss_scale 8192.0000 (12176.5597)	mem 6787MB
[2024-07-12 17:09:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:20 lr 0.000038	 wd 0.0000	time 0.2183 (0.2335)	loss 1.3085 (1.3610)	grad_norm 0.3628 (nan)	loss_scale 8192.0000 (11966.9563)	mem 6787MB
[2024-07-12 17:09:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:01:57 lr 0.000038	 wd 0.0000	time 0.2360 (0.2344)	loss 1.6941 (1.3630)	grad_norm 0.3686 (nan)	loss_scale 8192.0000 (11778.3028)	mem 6787MB
[2024-07-12 17:10:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:33 lr 0.000038	 wd 0.0000	time 0.2285 (0.2335)	loss 1.4083 (1.3621)	grad_norm 0.3864 (nan)	loss_scale 8192.0000 (11607.6078)	mem 6787MB
[2024-07-12 17:10:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:10 lr 0.000038	 wd 0.0000	time 0.2053 (0.2324)	loss 1.5626 (1.3607)	grad_norm 0.3973 (nan)	loss_scale 8192.0000 (11452.4234)	mem 6787MB
[2024-07-12 17:10:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:46 lr 0.000038	 wd 0.0000	time 0.2340 (0.2322)	loss 1.4898 (1.3614)	grad_norm 0.3897 (nan)	loss_scale 8192.0000 (11310.7275)	mem 6787MB
[2024-07-12 17:11:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:23 lr 0.000038	 wd 0.0000	time 0.2040 (0.2327)	loss 1.3164 (1.3630)	grad_norm 0.3963 (nan)	loss_scale 8192.0000 (11180.8347)	mem 6787MB
[2024-07-12 17:11:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000038	 wd 0.0000	time 0.1604 (0.2307)	loss 1.7503 (1.3644)	grad_norm 0.3538 (nan)	loss_scale 8192.0000 (11061.3291)	mem 6787MB
[2024-07-12 17:11:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 8 training takes 0:09:41
[2024-07-12 17:12:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 18.595 (18.595)	Loss 0.4221 (0.4221)	Acc@1 91.797 (91.797)	Acc@5 98.438 (98.438)	Mem 6787MB
[2024-07-12 17:12:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.042 Acc@5 97.256
[2024-07-12 17:12:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 17:12:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.08%
[2024-07-12 17:12:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][0/2502]	eta 1 day, 0:46:32 lr 0.000038	 wd 0.0000	time 35.6485 (35.6485)	loss 1.3673 (1.3673)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:13:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:22:29 lr 0.000038	 wd 0.0000	time 0.2028 (0.5620)	loss 1.2399 (1.3293)	grad_norm 0.3741 (0.3827)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:13:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:14:47 lr 0.000037	 wd 0.0000	time 0.2163 (0.3857)	loss 1.4989 (1.3417)	grad_norm 0.6263 (0.3817)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:13:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:11:57 lr 0.000037	 wd 0.0000	time 0.2145 (0.3257)	loss 1.4361 (1.3534)	grad_norm 0.3775 (0.3885)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:14:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:11:09 lr 0.000037	 wd 0.0000	time 0.5657 (0.3183)	loss 1.5806 (1.3485)	grad_norm 0.3428 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:14:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:09:56 lr 0.000037	 wd 0.0000	time 0.2065 (0.2979)	loss 1.3836 (1.3551)	grad_norm 0.3686 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:15:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:08:59 lr 0.000037	 wd 0.0000	time 0.2092 (0.2835)	loss 1.5004 (1.3497)	grad_norm 0.3636 (0.3883)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:15:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:08:12 lr 0.000037	 wd 0.0000	time 0.2085 (0.2731)	loss 1.5272 (1.3513)	grad_norm 0.3612 (0.3883)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:15:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:07:44 lr 0.000037	 wd 0.0000	time 0.4747 (0.2728)	loss 1.3817 (1.3542)	grad_norm 0.3709 (0.3876)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:16:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:07:18 lr 0.000037	 wd 0.0000	time 0.1887 (0.2736)	loss 1.5979 (1.3513)	grad_norm 0.3674 (0.3879)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:16:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:06:41 lr 0.000037	 wd 0.0000	time 0.2080 (0.2672)	loss 1.4758 (1.3540)	grad_norm 0.3556 (0.3879)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:17:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:06:07 lr 0.000037	 wd 0.0000	time 0.1987 (0.2621)	loss 1.3941 (1.3570)	grad_norm 0.3558 (0.3877)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:17:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:05:38 lr 0.000037	 wd 0.0000	time 0.1999 (0.2597)	loss 1.5416 (1.3580)	grad_norm 0.3514 (0.3885)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:17:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:05:10 lr 0.000037	 wd 0.0000	time 0.2029 (0.2581)	loss 0.9693 (1.3560)	grad_norm 0.3693 (0.3888)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:18:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:04:40 lr 0.000037	 wd 0.0000	time 0.1949 (0.2548)	loss 1.3364 (1.3574)	grad_norm 0.3464 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:18:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:04:12 lr 0.000037	 wd 0.0000	time 0.2113 (0.2521)	loss 1.4635 (1.3594)	grad_norm 0.3924 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:18:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:03:45 lr 0.000037	 wd 0.0000	time 0.2292 (0.2501)	loss 0.9367 (1.3584)	grad_norm 0.3496 (0.3907)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:19:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:03:20 lr 0.000037	 wd 0.0000	time 0.1976 (0.2498)	loss 1.1052 (1.3597)	grad_norm 0.3557 (0.3904)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:19:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:02:54 lr 0.000037	 wd 0.0000	time 0.1897 (0.2484)	loss 1.5306 (1.3600)	grad_norm 0.3726 (0.3908)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:20:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:28 lr 0.000037	 wd 0.0000	time 0.2080 (0.2467)	loss 1.3349 (1.3588)	grad_norm 0.3560 (0.3902)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:20:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:03 lr 0.000037	 wd 0.0000	time 0.2130 (0.2451)	loss 1.4882 (1.3589)	grad_norm 0.3455 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:20:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:38 lr 0.000036	 wd 0.0000	time 0.1635 (0.2449)	loss 1.0717 (1.3591)	grad_norm 0.3820 (0.3898)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:21:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:13 lr 0.000036	 wd 0.0000	time 0.2484 (0.2440)	loss 1.4812 (1.3601)	grad_norm 0.4091 (nan)	loss_scale 4096.0000 (8098.9514)	mem 6787MB
[2024-07-12 17:21:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:49 lr 0.000036	 wd 0.0000	time 0.1835 (0.2426)	loss 1.4184 (1.3607)	grad_norm 0.3573 (nan)	loss_scale 4096.0000 (7924.9857)	mem 6787MB
[2024-07-12 17:21:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:24 lr 0.000036	 wd 0.0000	time 0.2037 (0.2413)	loss 1.2703 (1.3604)	grad_norm 0.3793 (nan)	loss_scale 4096.0000 (7765.5110)	mem 6787MB
[2024-07-12 17:22:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000036	 wd 0.0000	time 0.1645 (0.2392)	loss 1.1274 (1.3615)	grad_norm 0.3571 (nan)	loss_scale 4096.0000 (7618.7893)	mem 6787MB
[2024-07-12 17:22:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 9 training takes 0:10:02
[2024-07-12 17:22:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 35.958 (35.958)	Loss 0.4163 (0.4163)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 17:23:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.074 Acc@5 97.260
[2024-07-12 17:23:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 17:23:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.08%
[2024-07-12 17:23:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][0/2502]	eta 11:31:58 lr 0.000036	 wd 0.0000	time 16.5942 (16.5942)	loss 1.2864 (1.2864)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:23:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:15:21 lr 0.000036	 wd 0.0000	time 0.2510 (0.3836)	loss 1.3591 (1.3422)	grad_norm 0.4760 (0.3775)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:24:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:13:14 lr 0.000036	 wd 0.0000	time 0.2082 (0.3453)	loss 1.2985 (1.3629)	grad_norm 0.3828 (0.3785)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:24:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:10:59 lr 0.000036	 wd 0.0000	time 0.2200 (0.2996)	loss 0.8877 (1.3601)	grad_norm 0.3826 (0.3805)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:24:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:09:41 lr 0.000036	 wd 0.0000	time 0.1873 (0.2765)	loss 1.5146 (1.3633)	grad_norm 0.3950 (0.3840)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:25:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:08:46 lr 0.000036	 wd 0.0000	time 0.2211 (0.2629)	loss 1.4117 (1.3592)	grad_norm 0.3618 (0.3835)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:25:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:08:17 lr 0.000036	 wd 0.0000	time 0.2125 (0.2615)	loss 1.5165 (1.3595)	grad_norm 0.8898 (0.3858)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:26:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:07:39 lr 0.000036	 wd 0.0000	time 0.2123 (0.2549)	loss 1.4772 (1.3554)	grad_norm 0.4075 (0.3854)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:26:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:07:05 lr 0.000036	 wd 0.0000	time 0.1880 (0.2497)	loss 1.2427 (1.3547)	grad_norm 0.3589 (0.3853)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:26:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:06:32 lr 0.000036	 wd 0.0000	time 0.2211 (0.2451)	loss 1.5360 (1.3549)	grad_norm 0.3785 (0.3837)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:27:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:06:07 lr 0.000036	 wd 0.0000	time 0.2885 (0.2446)	loss 1.4870 (1.3534)	grad_norm 0.3933 (0.3833)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:27:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:05:41 lr 0.000036	 wd 0.0000	time 0.1831 (0.2438)	loss 1.3024 (1.3510)	grad_norm 0.4137 (0.3836)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:27:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:05:14 lr 0.000035	 wd 0.0000	time 0.2179 (0.2412)	loss 1.3391 (1.3516)	grad_norm 0.3800 (0.3834)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:28:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:04:47 lr 0.000035	 wd 0.0000	time 0.1946 (0.2390)	loss 1.5798 (1.3542)	grad_norm 0.4149 (0.3840)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:28:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:04:22 lr 0.000035	 wd 0.0000	time 0.2231 (0.2378)	loss 1.3351 (1.3547)	grad_norm 0.3948 (0.3838)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:29:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:03:58 lr 0.000035	 wd 0.0000	time 0.2382 (0.2382)	loss 1.2520 (1.3546)	grad_norm 0.3577 (0.3831)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:29:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:03:33 lr 0.000035	 wd 0.0000	time 0.2064 (0.2371)	loss 1.2264 (1.3561)	grad_norm 0.3910 (0.3831)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:29:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:03:09 lr 0.000035	 wd 0.0000	time 0.2002 (0.2358)	loss 1.3150 (1.3571)	grad_norm 0.3705 (0.3848)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:30:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:02:44 lr 0.000035	 wd 0.0000	time 0.1999 (0.2349)	loss 1.4027 (1.3578)	grad_norm 0.4047 (0.3846)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:30:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:02:21 lr 0.000035	 wd 0.0000	time 0.1704 (0.2352)	loss 1.5100 (1.3592)	grad_norm 0.4008 (0.3843)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:30:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:01:57 lr 0.000035	 wd 0.0000	time 0.1991 (0.2351)	loss 1.2534 (1.3603)	grad_norm 0.4023 (0.3847)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:31:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:34 lr 0.000035	 wd 0.0000	time 0.2051 (0.2341)	loss 1.5561 (1.3620)	grad_norm 0.3817 (0.3845)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:31:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:10 lr 0.000035	 wd 0.0000	time 0.1775 (0.2331)	loss 0.9607 (1.3618)	grad_norm 0.3918 (0.3851)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:32:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:47 lr 0.000035	 wd 0.0000	time 0.2426 (0.2329)	loss 1.4816 (1.3614)	grad_norm 0.3985 (0.3847)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:32:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:23 lr 0.000035	 wd 0.0000	time 0.2152 (0.2329)	loss 1.5218 (1.3613)	grad_norm 0.3616 (0.3847)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:32:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000035	 wd 0.0000	time 0.1576 (0.2309)	loss 1.3328 (1.3603)	grad_norm 0.4783 (0.3851)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:32:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 10 training takes 0:09:41
[2024-07-12 17:33:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 19.494 (19.494)	Loss 0.4185 (0.4185)	Acc@1 91.797 (91.797)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 17:33:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.022 Acc@5 97.244
[2024-07-12 17:33:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.0%
[2024-07-12 17:33:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.08%
[2024-07-12 17:33:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][0/2502]	eta 22:02:20 lr 0.000035	 wd 0.0000	time 31.7109 (31.7109)	loss 1.4992 (1.4992)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:34:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:21:03 lr 0.000035	 wd 0.0000	time 0.1989 (0.5259)	loss 1.3938 (1.3525)	grad_norm 0.4201 (0.3983)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:34:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:14:05 lr 0.000034	 wd 0.0000	time 0.1806 (0.3673)	loss 1.4653 (1.3563)	grad_norm 0.4461 (0.3911)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:34:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:11:31 lr 0.000034	 wd 0.0000	time 0.2137 (0.3140)	loss 1.3275 (1.3588)	grad_norm 0.3816 (0.3868)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:35:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:10:18 lr 0.000034	 wd 0.0000	time 0.2124 (0.2941)	loss 1.5363 (1.3583)	grad_norm 0.4232 (0.3844)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:35:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:09:17 lr 0.000034	 wd 0.0000	time 0.2034 (0.2786)	loss 1.3915 (1.3553)	grad_norm 0.3407 (0.3822)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:35:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:08:28 lr 0.000034	 wd 0.0000	time 0.1959 (0.2672)	loss 1.4715 (1.3530)	grad_norm 0.3799 (0.3830)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:36:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:07:47 lr 0.000034	 wd 0.0000	time 0.2590 (0.2592)	loss 1.4769 (1.3534)	grad_norm 0.3733 (0.3845)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:36:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:07:13 lr 0.000034	 wd 0.0000	time 0.2361 (0.2549)	loss 1.6033 (1.3559)	grad_norm 0.3689 (0.3840)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:37:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:06:47 lr 0.000034	 wd 0.0000	time 0.2292 (0.2541)	loss 1.5138 (1.3558)	grad_norm 0.3833 (0.3840)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:37:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:06:16 lr 0.000034	 wd 0.0000	time 0.2166 (0.2504)	loss 1.4444 (1.3579)	grad_norm 0.4578 (0.3839)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:37:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:05:45 lr 0.000034	 wd 0.0000	time 0.2092 (0.2468)	loss 1.5613 (1.3593)	grad_norm 0.3716 (0.3836)	loss_scale 4096.0000 (4096.0000)	mem 6787MB
[2024-07-12 17:38:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:05:17 lr 0.000034	 wd 0.0000	time 0.2056 (0.2439)	loss 1.6997 (1.3591)	grad_norm 0.4086 (0.3836)	loss_scale 8192.0000 (4280.1665)	mem 6787MB
[2024-07-12 17:38:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:04:53 lr 0.000034	 wd 0.0000	time 0.2155 (0.2442)	loss 0.8942 (1.3580)	grad_norm 0.3413 (0.3842)	loss_scale 8192.0000 (4580.8455)	mem 6787MB
[2024-07-12 17:38:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:04:26 lr 0.000034	 wd 0.0000	time 0.1986 (0.2422)	loss 0.8833 (1.3576)	grad_norm 0.3463 (0.3839)	loss_scale 8192.0000 (4838.6010)	mem 6787MB
[2024-07-12 17:39:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:04:00 lr 0.000034	 wd 0.0000	time 0.2270 (0.2401)	loss 1.4315 (1.3566)	grad_norm 0.3477 (0.3832)	loss_scale 8192.0000 (5062.0120)	mem 6787MB
[2024-07-12 17:39:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:03:34 lr 0.000034	 wd 0.0000	time 0.2126 (0.2383)	loss 1.2651 (1.3576)	grad_norm 0.4090 (0.3833)	loss_scale 8192.0000 (5257.5141)	mem 6787MB
[2024-07-12 17:40:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:03:10 lr 0.000033	 wd 0.0000	time 0.2386 (0.2376)	loss 1.4519 (1.3568)	grad_norm 0.4130 (0.3830)	loss_scale 8192.0000 (5430.0294)	mem 6787MB
[2024-07-12 17:40:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:02:47 lr 0.000033	 wd 0.0000	time 0.2045 (0.2383)	loss 1.2847 (1.3584)	grad_norm 0.3532 (0.3829)	loss_scale 8192.0000 (5583.3870)	mem 6787MB
[2024-07-12 17:40:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:22 lr 0.000033	 wd 0.0000	time 0.2093 (0.2371)	loss 1.5014 (1.3586)	grad_norm 0.3670 (0.3827)	loss_scale 8192.0000 (5720.6102)	mem 6787MB
[2024-07-12 17:41:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:01:58 lr 0.000033	 wd 0.0000	time 0.2032 (0.2360)	loss 1.1123 (1.3580)	grad_norm 0.3559 (0.3829)	loss_scale 8192.0000 (5844.1179)	mem 6787MB
[2024-07-12 17:41:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:34 lr 0.000033	 wd 0.0000	time 0.2122 (0.2352)	loss 1.5397 (1.3578)	grad_norm 0.3705 (0.3831)	loss_scale 8192.0000 (5955.8686)	mem 6787MB
[2024-07-12 17:41:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:11 lr 0.000033	 wd 0.0000	time 0.2018 (0.2355)	loss 1.2828 (1.3587)	grad_norm 0.3555 (0.3830)	loss_scale 8192.0000 (6057.4648)	mem 6787MB
[2024-07-12 17:42:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:47 lr 0.000033	 wd 0.0000	time 0.2511 (0.2347)	loss 1.5262 (1.3583)	grad_norm 0.4307 (0.3833)	loss_scale 8192.0000 (6150.2303)	mem 6787MB
[2024-07-12 17:42:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:23 lr 0.000033	 wd 0.0000	time 0.2015 (0.2338)	loss 1.2662 (1.3575)	grad_norm 0.3696 (0.3850)	loss_scale 8192.0000 (6235.2686)	mem 6787MB
[2024-07-12 17:42:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000033	 wd 0.0000	time 0.1562 (0.2316)	loss 1.4041 (1.3572)	grad_norm 0.3584 (0.3846)	loss_scale 8192.0000 (6313.5066)	mem 6787MB
[2024-07-12 17:43:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 11 training takes 0:09:43
[2024-07-12 17:43:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 38.553 (38.553)	Loss 0.4189 (0.4189)	Acc@1 91.797 (91.797)	Acc@5 98.438 (98.438)	Mem 6787MB
[2024-07-12 17:43:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.142 Acc@5 97.240
[2024-07-12 17:43:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 17:43:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.14%
[2024-07-12 17:43:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saving......
[2024-07-12 17:43:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saved !!!
[2024-07-12 17:44:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][0/2502]	eta 10:05:25 lr 0.000033	 wd 0.0000	time 14.5187 (14.5187)	loss 1.6210 (1.6210)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:44:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:14:19 lr 0.000033	 wd 0.0000	time 0.2213 (0.3579)	loss 1.4962 (1.3634)	grad_norm 0.4008 (0.3828)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:44:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:12:18 lr 0.000033	 wd 0.0000	time 0.4807 (0.3209)	loss 0.9833 (1.3693)	grad_norm 0.3511 (0.3971)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:45:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:11:06 lr 0.000033	 wd 0.0000	time 0.2070 (0.3028)	loss 1.2997 (1.3695)	grad_norm 0.3649 (0.3946)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:45:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:09:47 lr 0.000033	 wd 0.0000	time 0.2005 (0.2795)	loss 1.2297 (1.3620)	grad_norm 0.3410 (0.3975)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:46:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:08:49 lr 0.000032	 wd 0.0000	time 0.2110 (0.2645)	loss 1.0193 (1.3627)	grad_norm 0.3524 (0.3931)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:46:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:08:14 lr 0.000032	 wd 0.0000	time 0.3299 (0.2600)	loss 1.3536 (1.3599)	grad_norm 0.3783 (0.3924)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:47:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:07:58 lr 0.000032	 wd 0.0000	time 0.1915 (0.2658)	loss 1.4627 (1.3576)	grad_norm 0.3849 (0.3907)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:47:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:07:20 lr 0.000032	 wd 0.0000	time 0.2171 (0.2588)	loss 1.4479 (1.3654)	grad_norm 0.3694 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:47:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:06:46 lr 0.000032	 wd 0.0000	time 0.1917 (0.2535)	loss 1.0590 (1.3658)	grad_norm 0.3531 (0.3888)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:48:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:06:19 lr 0.000032	 wd 0.0000	time 0.3263 (0.2523)	loss 1.6310 (1.3634)	grad_norm 0.3577 (0.3876)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:48:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:05:59 lr 0.000032	 wd 0.0000	time 0.2016 (0.2563)	loss 1.4822 (1.3673)	grad_norm 0.3608 (0.3868)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:48:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:05:28 lr 0.000032	 wd 0.0000	time 0.2116 (0.2523)	loss 1.5128 (1.3663)	grad_norm 0.4179 (0.3860)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:49:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:04:59 lr 0.000032	 wd 0.0000	time 0.2201 (0.2494)	loss 1.4487 (1.3667)	grad_norm 0.3801 (0.3862)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:49:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:04:32 lr 0.000032	 wd 0.0000	time 0.2177 (0.2477)	loss 1.6031 (1.3667)	grad_norm 0.3717 (0.3865)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:50:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:04:07 lr 0.000032	 wd 0.0000	time 0.1896 (0.2473)	loss 1.4358 (1.3673)	grad_norm 0.6524 (0.3869)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:50:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:03:41 lr 0.000032	 wd 0.0000	time 0.1901 (0.2453)	loss 1.4204 (1.3661)	grad_norm 0.3573 (0.3863)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:50:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:03:15 lr 0.000031	 wd 0.0000	time 0.1975 (0.2433)	loss 1.5648 (1.3672)	grad_norm 0.3735 (0.3864)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:51:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:02:49 lr 0.000031	 wd 0.0000	time 0.2090 (0.2417)	loss 1.2785 (1.3680)	grad_norm 0.3921 (0.3861)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:51:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:26 lr 0.000031	 wd 0.0000	time 0.2105 (0.2426)	loss 1.4077 (1.3675)	grad_norm 0.4129 (0.3863)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:51:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:01 lr 0.000031	 wd 0.0000	time 0.2004 (0.2414)	loss 1.3952 (1.3665)	grad_norm 0.3888 (0.3865)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:52:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:36 lr 0.000031	 wd 0.0000	time 0.2159 (0.2402)	loss 1.4406 (1.3677)	grad_norm 0.5052 (0.3869)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:52:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:12 lr 0.000031	 wd 0.0000	time 0.1986 (0.2388)	loss 1.5580 (1.3691)	grad_norm 0.3626 (0.3866)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:53:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:48 lr 0.000031	 wd 0.0000	time 0.2556 (0.2384)	loss 1.4340 (1.3691)	grad_norm 0.3832 (0.3864)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:53:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:24 lr 0.000031	 wd 0.0000	time 0.2017 (0.2381)	loss 1.5005 (1.3688)	grad_norm 0.4228 (0.3865)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:53:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000031	 wd 0.0000	time 0.1515 (0.2358)	loss 1.1243 (1.3676)	grad_norm 0.3715 (0.3865)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:53:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 12 training takes 0:09:54
[2024-07-12 17:54:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 19.192 (19.192)	Loss 0.4182 (0.4182)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 17:54:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.116 Acc@5 97.242
[2024-07-12 17:54:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 17:54:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.14%
[2024-07-12 17:54:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][0/2502]	eta 1 day, 1:00:26 lr 0.000031	 wd 0.0000	time 35.9817 (35.9817)	loss 1.3650 (1.3650)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:55:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:22:25 lr 0.000031	 wd 0.0000	time 0.1900 (0.5600)	loss 1.3908 (1.3548)	grad_norm 0.3679 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 17:55:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:14:46 lr 0.000031	 wd 0.0000	time 0.1872 (0.3851)	loss 1.6820 (1.3560)	grad_norm 0.3757 (0.3856)	loss_scale 16384.0000 (10555.8607)	mem 6787MB
[2024-07-12 17:55:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:12:00 lr 0.000031	 wd 0.0000	time 0.2009 (0.3270)	loss 1.4470 (1.3642)	grad_norm 0.3875 (0.3838)	loss_scale 16384.0000 (12492.1196)	mem 6787MB
[2024-07-12 17:56:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:11:19 lr 0.000030	 wd 0.0000	time 0.2118 (0.3232)	loss 1.3332 (1.3632)	grad_norm 0.3725 (0.3813)	loss_scale 16384.0000 (13462.6633)	mem 6787MB
[2024-07-12 17:56:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:10:01 lr 0.000030	 wd 0.0000	time 0.1986 (0.3005)	loss 1.5301 (1.3579)	grad_norm 0.4094 (inf)	loss_scale 8192.0000 (12901.1737)	mem 6787MB
[2024-07-12 17:57:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:09:03 lr 0.000030	 wd 0.0000	time 0.1759 (0.2855)	loss 1.5322 (1.3642)	grad_norm 0.3608 (inf)	loss_scale 8192.0000 (12117.6173)	mem 6787MB
[2024-07-12 17:57:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:08:15 lr 0.000030	 wd 0.0000	time 0.2225 (0.2750)	loss 1.4413 (1.3596)	grad_norm 0.3948 (inf)	loss_scale 8192.0000 (11557.6148)	mem 6787MB
[2024-07-12 17:57:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:07:41 lr 0.000030	 wd 0.0000	time 0.2201 (0.2709)	loss 1.5359 (1.3634)	grad_norm 0.3418 (inf)	loss_scale 8192.0000 (11137.4382)	mem 6787MB
[2024-07-12 17:58:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:07:06 lr 0.000030	 wd 0.0000	time 0.2501 (0.2665)	loss 1.5304 (1.3603)	grad_norm 0.3814 (inf)	loss_scale 8192.0000 (10810.5305)	mem 6787MB
[2024-07-12 17:58:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:06:32 lr 0.000030	 wd 0.0000	time 0.2084 (0.2614)	loss 1.6069 (1.3659)	grad_norm 0.3643 (inf)	loss_scale 8192.0000 (10548.9391)	mem 6787MB
[2024-07-12 17:59:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:06:00 lr 0.000030	 wd 0.0000	time 0.2086 (0.2569)	loss 1.3855 (1.3673)	grad_norm 0.3658 (inf)	loss_scale 8192.0000 (10334.8665)	mem 6787MB
[2024-07-12 17:59:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:05:31 lr 0.000030	 wd 0.0000	time 0.2376 (0.2544)	loss 1.5544 (1.3636)	grad_norm 0.3795 (inf)	loss_scale 8192.0000 (10156.4430)	mem 6787MB
[2024-07-12 17:59:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:05:04 lr 0.000030	 wd 0.0000	time 0.2056 (0.2530)	loss 1.4238 (1.3622)	grad_norm 0.3612 (inf)	loss_scale 8192.0000 (10005.4481)	mem 6787MB
[2024-07-12 18:00:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:04:35 lr 0.000030	 wd 0.0000	time 0.2051 (0.2502)	loss 1.6579 (1.3628)	grad_norm 0.3745 (inf)	loss_scale 8192.0000 (9876.0086)	mem 6787MB
[2024-07-12 18:00:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:04:08 lr 0.000030	 wd 0.0000	time 0.1898 (0.2476)	loss 1.6294 (1.3616)	grad_norm 0.3787 (inf)	loss_scale 8192.0000 (9763.8161)	mem 6787MB
[2024-07-12 18:00:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:03:41 lr 0.000029	 wd 0.0000	time 0.2157 (0.2456)	loss 1.5712 (1.3612)	grad_norm 0.3825 (inf)	loss_scale 8192.0000 (9665.6390)	mem 6787MB
[2024-07-12 18:01:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:03:17 lr 0.000029	 wd 0.0000	time 0.2432 (0.2460)	loss 0.8620 (1.3589)	grad_norm 0.3690 (inf)	loss_scale 8192.0000 (9579.0053)	mem 6787MB
[2024-07-12 18:01:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:02:51 lr 0.000029	 wd 0.0000	time 0.1972 (0.2448)	loss 1.4287 (1.3605)	grad_norm 0.3798 (inf)	loss_scale 8192.0000 (9501.9922)	mem 6787MB
[2024-07-12 18:02:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:26 lr 0.000029	 wd 0.0000	time 0.2089 (0.2431)	loss 1.2119 (1.3609)	grad_norm 0.3756 (inf)	loss_scale 8192.0000 (9433.0815)	mem 6787MB
[2024-07-12 18:02:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:01 lr 0.000029	 wd 0.0000	time 0.2326 (0.2415)	loss 1.4336 (1.3619)	grad_norm 0.3715 (inf)	loss_scale 8192.0000 (9371.0585)	mem 6787MB
[2024-07-12 18:02:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:36 lr 0.000029	 wd 0.0000	time 0.2488 (0.2408)	loss 1.1092 (1.3616)	grad_norm 0.3985 (inf)	loss_scale 8192.0000 (9314.9396)	mem 6787MB
[2024-07-12 18:03:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:12 lr 0.000029	 wd 0.0000	time 0.2253 (0.2406)	loss 1.5423 (1.3631)	grad_norm 0.3737 (inf)	loss_scale 8192.0000 (9263.9200)	mem 6787MB
[2024-07-12 18:03:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:48 lr 0.000029	 wd 0.0000	time 0.2282 (0.2395)	loss 1.3072 (1.3625)	grad_norm 0.3824 (inf)	loss_scale 8192.0000 (9217.3351)	mem 6787MB
[2024-07-12 18:03:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:24 lr 0.000029	 wd 0.0000	time 0.1862 (0.2383)	loss 1.6970 (1.3637)	grad_norm 1.3536 (inf)	loss_scale 8192.0000 (9174.6306)	mem 6787MB
[2024-07-12 18:04:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000029	 wd 0.0000	time 0.1676 (0.2362)	loss 1.5220 (1.3631)	grad_norm 0.3530 (inf)	loss_scale 8192.0000 (9135.3411)	mem 6787MB
[2024-07-12 18:04:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 13 training takes 0:09:55
[2024-07-12 18:04:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 34.548 (34.548)	Loss 0.4172 (0.4172)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 18:05:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.104 Acc@5 97.264
[2024-07-12 18:05:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 18:05:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.14%
[2024-07-12 18:05:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][0/2502]	eta 10:54:21 lr 0.000029	 wd 0.0000	time 15.6921 (15.6921)	loss 1.4871 (1.4871)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:05:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:14:55 lr 0.000029	 wd 0.0000	time 0.2280 (0.3728)	loss 1.5443 (1.3732)	grad_norm 0.3511 (0.3885)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:06:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:13:10 lr 0.000028	 wd 0.0000	time 0.1888 (0.3435)	loss 1.3062 (1.3924)	grad_norm 0.3472 (0.3890)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:06:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:10:57 lr 0.000028	 wd 0.0000	time 0.1838 (0.2984)	loss 1.0163 (1.3802)	grad_norm 0.3557 (0.3931)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:06:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:09:40 lr 0.000028	 wd 0.0000	time 0.2061 (0.2763)	loss 1.2191 (1.3653)	grad_norm 0.3736 (0.3910)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:07:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:08:46 lr 0.000028	 wd 0.0000	time 0.2351 (0.2630)	loss 1.2943 (1.3681)	grad_norm 0.4553 (0.3905)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:07:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:08:43 lr 0.000028	 wd 0.0000	time 0.2445 (0.2750)	loss 1.6546 (1.3736)	grad_norm 0.4258 (0.3914)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:08:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:08:00 lr 0.000028	 wd 0.0000	time 0.2342 (0.2665)	loss 1.4399 (1.3695)	grad_norm 0.3836 (0.3908)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:08:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:07:21 lr 0.000028	 wd 0.0000	time 0.2535 (0.2593)	loss 1.3814 (1.3658)	grad_norm 0.3709 (0.3929)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:08:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:06:46 lr 0.000028	 wd 0.0000	time 0.2125 (0.2538)	loss 1.4404 (1.3618)	grad_norm 0.3583 (0.3913)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:09:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:06:19 lr 0.000028	 wd 0.0000	time 0.2522 (0.2526)	loss 1.3210 (1.3603)	grad_norm 0.3847 (0.3905)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:09:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:05:51 lr 0.000028	 wd 0.0000	time 0.1972 (0.2506)	loss 1.4926 (1.3579)	grad_norm 0.3424 (0.3902)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:09:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:05:21 lr 0.000028	 wd 0.0000	time 0.2364 (0.2472)	loss 1.4151 (1.3581)	grad_norm 0.3848 (0.3896)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:10:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:04:53 lr 0.000027	 wd 0.0000	time 0.2001 (0.2444)	loss 1.1823 (1.3583)	grad_norm 0.7973 (0.3898)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:10:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:04:27 lr 0.000027	 wd 0.0000	time 0.2068 (0.2429)	loss 1.6530 (1.3572)	grad_norm 0.3740 (0.3898)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:11:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:04:03 lr 0.000027	 wd 0.0000	time 0.2034 (0.2426)	loss 1.3732 (1.3580)	grad_norm 0.3746 (0.3891)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:11:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:03:37 lr 0.000027	 wd 0.0000	time 0.1901 (0.2408)	loss 1.3132 (1.3579)	grad_norm 0.3816 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:11:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:03:11 lr 0.000027	 wd 0.0000	time 0.2205 (0.2392)	loss 1.1820 (1.3572)	grad_norm 0.3832 (0.3889)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:12:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:02:47 lr 0.000027	 wd 0.0000	time 0.2344 (0.2379)	loss 1.3047 (1.3571)	grad_norm 0.3662 (0.3898)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:12:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:23 lr 0.000027	 wd 0.0000	time 0.2255 (0.2380)	loss 1.5357 (1.3572)	grad_norm 0.3785 (0.3900)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:12:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:01:59 lr 0.000027	 wd 0.0000	time 0.1838 (0.2373)	loss 1.0745 (1.3590)	grad_norm 0.3769 (0.3901)	loss_scale 16384.0000 (8486.7646)	mem 6787MB
[2024-07-12 18:13:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:34 lr 0.000027	 wd 0.0000	time 0.2147 (0.2362)	loss 1.5498 (1.3588)	grad_norm 0.3901 (0.3899)	loss_scale 16384.0000 (8862.6445)	mem 6787MB
[2024-07-12 18:13:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:11 lr 0.000027	 wd 0.0000	time 0.2046 (0.2351)	loss 1.2140 (1.3589)	grad_norm 0.3759 (0.3896)	loss_scale 16384.0000 (9204.3689)	mem 6787MB
[2024-07-12 18:14:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:47 lr 0.000027	 wd 0.0000	time 0.2051 (0.2347)	loss 1.3827 (1.3598)	grad_norm 0.3830 (0.3890)	loss_scale 16384.0000 (9516.3911)	mem 6787MB
[2024-07-12 18:14:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:23 lr 0.000026	 wd 0.0000	time 0.2194 (0.2348)	loss 1.1241 (1.3608)	grad_norm 0.4240 (0.3886)	loss_scale 16384.0000 (9802.4223)	mem 6787MB
[2024-07-12 18:14:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000026	 wd 0.0000	time 0.1687 (0.2328)	loss 1.2560 (1.3600)	grad_norm 0.3956 (0.3888)	loss_scale 16384.0000 (10065.5802)	mem 6787MB
[2024-07-12 18:14:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 14 training takes 0:09:47
[2024-07-12 18:15:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 19.397 (19.397)	Loss 0.4155 (0.4155)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 18:15:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.108 Acc@5 97.260
[2024-07-12 18:15:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 18:15:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.14%
[2024-07-12 18:15:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][0/2502]	eta 23:36:27 lr 0.000026	 wd 0.0000	time 33.9678 (33.9678)	loss 1.4676 (1.4676)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:16:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:22:23 lr 0.000026	 wd 0.0000	time 0.1813 (0.5593)	loss 1.0239 (1.3473)	grad_norm 0.3564 (0.3933)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:16:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:48 lr 0.000026	 wd 0.0000	time 0.2037 (0.3858)	loss 1.4469 (1.3620)	grad_norm 0.4071 (0.3889)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:16:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:11:59 lr 0.000026	 wd 0.0000	time 0.2123 (0.3267)	loss 1.3991 (1.3488)	grad_norm 0.3529 (0.3872)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:17:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:10:44 lr 0.000026	 wd 0.0000	time 0.2118 (0.3064)	loss 1.3924 (1.3524)	grad_norm 0.3916 (0.3864)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:17:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:09:34 lr 0.000026	 wd 0.0000	time 0.1666 (0.2867)	loss 0.8661 (1.3504)	grad_norm 0.3560 (0.3862)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:18:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:08:41 lr 0.000026	 wd 0.0000	time 0.2222 (0.2744)	loss 0.8432 (1.3538)	grad_norm 0.3735 (0.3856)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:18:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:07:57 lr 0.000026	 wd 0.0000	time 0.1978 (0.2652)	loss 1.1045 (1.3564)	grad_norm 0.3942 (0.3858)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:18:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:07:23 lr 0.000026	 wd 0.0000	time 0.2298 (0.2608)	loss 1.4138 (1.3568)	grad_norm 0.3539 (0.3851)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:19:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:06:55 lr 0.000025	 wd 0.0000	time 0.2003 (0.2596)	loss 1.2305 (1.3576)	grad_norm 0.3677 (0.3848)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:19:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:06:22 lr 0.000025	 wd 0.0000	time 0.1933 (0.2547)	loss 1.4239 (1.3590)	grad_norm 0.3565 (0.3844)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:19:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:05:51 lr 0.000025	 wd 0.0000	time 0.1813 (0.2507)	loss 1.2490 (1.3562)	grad_norm 0.3628 (0.3842)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:20:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:05:23 lr 0.000025	 wd 0.0000	time 0.2442 (0.2481)	loss 1.3920 (1.3568)	grad_norm 0.3511 (0.3846)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:20:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:04:57 lr 0.000025	 wd 0.0000	time 0.2337 (0.2477)	loss 1.3673 (1.3552)	grad_norm 0.3656 (0.3852)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:21:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:04:30 lr 0.000025	 wd 0.0000	time 0.2109 (0.2452)	loss 1.4478 (1.3554)	grad_norm 0.3747 (0.3856)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:21:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:04:03 lr 0.000025	 wd 0.0000	time 0.2011 (0.2431)	loss 1.5719 (1.3551)	grad_norm 0.4064 (0.3857)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:21:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:03:37 lr 0.000025	 wd 0.0000	time 0.2068 (0.2411)	loss 1.2809 (1.3575)	grad_norm 0.3555 (0.3858)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:22:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:03:12 lr 0.000025	 wd 0.0000	time 0.1826 (0.2406)	loss 1.4318 (1.3565)	grad_norm 0.3742 (0.3857)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:22:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:02:49 lr 0.000025	 wd 0.0000	time 0.2043 (0.2413)	loss 1.0684 (1.3562)	grad_norm 0.3835 (0.3854)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:22:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:24 lr 0.000024	 wd 0.0000	time 0.2194 (0.2399)	loss 1.6079 (1.3567)	grad_norm 0.3612 (0.3854)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:23:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:01:59 lr 0.000024	 wd 0.0000	time 0.1927 (0.2385)	loss 1.8358 (1.3589)	grad_norm 0.3751 (0.3855)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:23:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:35 lr 0.000024	 wd 0.0000	time 0.2330 (0.2379)	loss 1.5101 (1.3597)	grad_norm 0.3878 (0.3852)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:24:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:11 lr 0.000024	 wd 0.0000	time 0.2285 (0.2375)	loss 1.0671 (1.3608)	grad_norm 0.3740 (0.3856)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:24:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:47 lr 0.000024	 wd 0.0000	time 0.1837 (0.2364)	loss 1.5784 (1.3610)	grad_norm 0.4084 (0.3863)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:24:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:24 lr 0.000024	 wd 0.0000	time 0.1832 (0.2355)	loss 1.0656 (1.3598)	grad_norm 0.3677 (0.3863)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:25:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000024	 wd 0.0000	time 0.1581 (0.2333)	loss 1.2145 (1.3599)	grad_norm 0.3475 (0.3859)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:25:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 15 training takes 0:09:48
[2024-07-12 18:25:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_15.pth saving......
[2024-07-12 18:25:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_15.pth saved !!!
[2024-07-12 18:25:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 40.096 (40.096)	Loss 0.4138 (0.4138)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 18:26:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.116 Acc@5 97.268
[2024-07-12 18:26:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 18:26:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.14%
[2024-07-12 18:26:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][0/2502]	eta 11:31:26 lr 0.000024	 wd 0.0000	time 16.5812 (16.5812)	loss 1.2701 (1.2701)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:26:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:15:00 lr 0.000024	 wd 0.0000	time 0.2218 (0.3750)	loss 1.4675 (1.3536)	grad_norm 0.3815 (0.3822)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:27:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:13:03 lr 0.000024	 wd 0.0000	time 0.1950 (0.3403)	loss 1.3142 (1.3626)	grad_norm 0.3854 (0.3900)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:27:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:10:55 lr 0.000024	 wd 0.0000	time 0.2085 (0.2979)	loss 1.3595 (1.3750)	grad_norm 0.3548 (0.3909)	loss_scale 16384.0000 (16384.0000)	mem 6787MB
[2024-07-12 18:27:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:09:40 lr 0.000024	 wd 0.0000	time 0.2036 (0.2763)	loss 1.2711 (1.3689)	grad_norm 0.3598 (nan)	loss_scale 8192.0000 (14504.5387)	mem 6787MB
[2024-07-12 18:28:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:08:46 lr 0.000023	 wd 0.0000	time 0.2228 (0.2627)	loss 1.2566 (1.3791)	grad_norm 0.3487 (nan)	loss_scale 8192.0000 (13244.5509)	mem 6787MB
[2024-07-12 18:28:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:08:11 lr 0.000023	 wd 0.0000	time 0.2169 (0.2583)	loss 1.0389 (1.3714)	grad_norm 0.3708 (nan)	loss_scale 8192.0000 (12403.8602)	mem 6787MB
[2024-07-12 18:29:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:07:37 lr 0.000023	 wd 0.0000	time 0.2061 (0.2542)	loss 1.3275 (1.3731)	grad_norm 0.3655 (nan)	loss_scale 8192.0000 (11803.0243)	mem 6787MB
[2024-07-12 18:29:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:07:03 lr 0.000023	 wd 0.0000	time 0.2149 (0.2487)	loss 1.3924 (1.3677)	grad_norm 0.3569 (nan)	loss_scale 8192.0000 (11352.2097)	mem 6787MB
[2024-07-12 18:29:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:06:31 lr 0.000023	 wd 0.0000	time 0.2036 (0.2445)	loss 1.5553 (1.3655)	grad_norm 0.3662 (nan)	loss_scale 8192.0000 (11001.4650)	mem 6787MB
[2024-07-12 18:30:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:06:04 lr 0.000023	 wd 0.0000	time 0.2275 (0.2428)	loss 1.5545 (1.3658)	grad_norm 0.3884 (nan)	loss_scale 8192.0000 (10720.7992)	mem 6787MB
[2024-07-12 18:30:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:05:40 lr 0.000023	 wd 0.0000	time 0.2793 (0.2431)	loss 1.3329 (1.3669)	grad_norm 0.3773 (nan)	loss_scale 8192.0000 (10491.1172)	mem 6787MB
[2024-07-12 18:30:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:05:13 lr 0.000023	 wd 0.0000	time 0.1897 (0.2406)	loss 1.4075 (1.3681)	grad_norm 0.4645 (nan)	loss_scale 8192.0000 (10299.6836)	mem 6787MB
[2024-07-12 18:31:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:04:46 lr 0.000023	 wd 0.0000	time 0.2104 (0.2383)	loss 1.0988 (1.3686)	grad_norm 0.3739 (nan)	loss_scale 8192.0000 (10137.6787)	mem 6787MB
[2024-07-12 18:31:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:04:20 lr 0.000023	 wd 0.0000	time 0.2054 (0.2367)	loss 1.2036 (1.3659)	grad_norm 0.3770 (nan)	loss_scale 8192.0000 (9998.8009)	mem 6787MB
[2024-07-12 18:32:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:03:58 lr 0.000022	 wd 0.0000	time 0.1946 (0.2381)	loss 1.4031 (1.3628)	grad_norm 0.4017 (nan)	loss_scale 8192.0000 (9878.4277)	mem 6787MB
[2024-07-12 18:32:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:03:33 lr 0.000022	 wd 0.0000	time 0.1905 (0.2366)	loss 1.3895 (1.3632)	grad_norm 0.3748 (nan)	loss_scale 8192.0000 (9773.0918)	mem 6787MB
[2024-07-12 18:32:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:03:08 lr 0.000022	 wd 0.0000	time 0.2083 (0.2353)	loss 1.1105 (1.3612)	grad_norm 0.3862 (nan)	loss_scale 8192.0000 (9680.1411)	mem 6787MB
[2024-07-12 18:33:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:02:44 lr 0.000022	 wd 0.0000	time 0.2342 (0.2341)	loss 1.2404 (1.3595)	grad_norm 0.3576 (nan)	loss_scale 8192.0000 (9597.5125)	mem 6787MB
[2024-07-12 18:33:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:20 lr 0.000022	 wd 0.0000	time 0.2205 (0.2339)	loss 1.1942 (1.3584)	grad_norm 0.3609 (nan)	loss_scale 8192.0000 (9523.5771)	mem 6787MB
[2024-07-12 18:33:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:01:57 lr 0.000022	 wd 0.0000	time 0.2758 (0.2344)	loss 1.4686 (1.3587)	grad_norm 0.3641 (nan)	loss_scale 8192.0000 (9457.0315)	mem 6787MB
[2024-07-12 18:34:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:33 lr 0.000022	 wd 0.0000	time 0.2184 (0.2333)	loss 1.3506 (1.3582)	grad_norm 0.3782 (nan)	loss_scale 8192.0000 (9396.8206)	mem 6787MB
[2024-07-12 18:34:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:10 lr 0.000022	 wd 0.0000	time 0.2116 (0.2324)	loss 1.4695 (1.3589)	grad_norm 0.3629 (nan)	loss_scale 8192.0000 (9342.0809)	mem 6787MB
[2024-07-12 18:34:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:46 lr 0.000022	 wd 0.0000	time 0.2335 (0.2321)	loss 1.2239 (1.3589)	grad_norm 0.3982 (nan)	loss_scale 8192.0000 (9292.0991)	mem 6787MB
[2024-07-12 18:35:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:23 lr 0.000022	 wd 0.0000	time 0.2238 (0.2320)	loss 1.5411 (1.3591)	grad_norm 0.3696 (nan)	loss_scale 8192.0000 (9246.2807)	mem 6787MB
[2024-07-12 18:35:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000021	 wd 0.0000	time 0.1522 (0.2302)	loss 1.3661 (1.3590)	grad_norm 0.3691 (nan)	loss_scale 8192.0000 (9204.1263)	mem 6787MB
[2024-07-12 18:35:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 16 training takes 0:09:40
[2024-07-12 18:36:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 19.364 (19.364)	Loss 0.4146 (0.4146)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 6787MB
[2024-07-12 18:36:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.148 Acc@5 97.258
[2024-07-12 18:36:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 18:36:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 18:36:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 160): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saving......
[2024-07-12 18:36:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 162): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth saved !!!
[2024-07-12 18:36:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][0/2502]	eta 21:49:39 lr 0.000021	 wd 0.0000	time 31.4066 (31.4066)	loss 1.4556 (1.4556)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:37:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:21:00 lr 0.000021	 wd 0.0000	time 0.1867 (0.5246)	loss 1.5584 (1.3324)	grad_norm 0.4253 (0.3843)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:37:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:02 lr 0.000021	 wd 0.0000	time 0.1781 (0.3660)	loss 1.5886 (1.3479)	grad_norm 0.3974 (0.3820)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:37:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:11:29 lr 0.000021	 wd 0.0000	time 0.1901 (0.3130)	loss 1.3350 (1.3445)	grad_norm 0.3840 (0.3823)	loss_scale 8192.0000 (8192.0000)	mem 6787MB
[2024-07-12 18:38:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:10:14 lr 0.000021	 wd 0.0000	time 0.2421 (0.2922)	loss 1.1222 (1.3388)	grad_norm 0.3751 (nan)	loss_scale 4096.0000 (8130.7132)	mem 6787MB
[2024-07-12 18:38:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:09:15 lr 0.000021	 wd 0.0000	time 0.2577 (0.2774)	loss 1.3647 (1.3388)	grad_norm 0.3706 (nan)	loss_scale 4096.0000 (7325.3812)	mem 6787MB
[2024-07-12 18:38:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:08:27 lr 0.000021	 wd 0.0000	time 0.2665 (0.2668)	loss 1.1844 (1.3458)	grad_norm 0.3752 (nan)	loss_scale 4096.0000 (6788.0466)	mem 6787MB
[2024-07-12 18:39:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:07:46 lr 0.000021	 wd 0.0000	time 0.1757 (0.2590)	loss 1.3276 (1.3515)	grad_norm 0.3779 (nan)	loss_scale 4096.0000 (6404.0171)	mem 6787MB
[2024-07-12 18:39:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:07:11 lr 0.000021	 wd 0.0000	time 0.2434 (0.2537)	loss 1.3751 (1.3553)	grad_norm 0.3756 (nan)	loss_scale 4096.0000 (6115.8752)	mem 6787MB
[2024-07-12 18:40:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:06:44 lr 0.000021	 wd 0.0000	time 0.1965 (0.2524)	loss 1.6004 (1.3567)	grad_norm 0.3644 (nan)	loss_scale 4096.0000 (5891.6937)	mem 6787MB
[2024-07-12 18:40:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:06:13 lr 0.000020	 wd 0.0000	time 0.1881 (0.2489)	loss 1.5342 (1.3551)	grad_norm 0.4532 (nan)	loss_scale 4096.0000 (5712.3037)	mem 6787MB
[2024-07-12 18:40:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:05:44 lr 0.000020	 wd 0.0000	time 0.2178 (0.2454)	loss 1.1113 (1.3556)	grad_norm 0.3638 (nan)	loss_scale 4096.0000 (5565.5005)	mem 6787MB
[2024-07-12 18:41:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:05:15 lr 0.000020	 wd 0.0000	time 0.2227 (0.2425)	loss 1.5901 (1.3574)	grad_norm 0.3524 (nan)	loss_scale 4096.0000 (5443.1440)	mem 6787MB
[2024-07-12 18:41:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:04:50 lr 0.000020	 wd 0.0000	time 0.2011 (0.2415)	loss 1.1639 (1.3574)	grad_norm 0.4003 (nan)	loss_scale 4096.0000 (5339.5972)	mem 6787MB
[2024-07-12 18:41:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:04:25 lr 0.000020	 wd 0.0000	time 0.1990 (0.2412)	loss 1.4920 (1.3582)	grad_norm 0.3874 (nan)	loss_scale 4096.0000 (5250.8323)	mem 6787MB
[2024-07-12 18:42:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:03:59 lr 0.000020	 wd 0.0000	time 0.2101 (0.2392)	loss 1.3901 (1.3589)	grad_norm 0.3949 (nan)	loss_scale 4096.0000 (5173.8947)	mem 6787MB
[2024-07-12 18:42:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:03:34 lr 0.000020	 wd 0.0000	time 0.1864 (0.2374)	loss 1.5501 (1.3602)	grad_norm 0.4008 (nan)	loss_scale 4096.0000 (5106.5684)	mem 6787MB
[2024-07-12 18:42:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:09 lr 0.000020	 wd 0.0000	time 0.2307 (0.2365)	loss 1.5757 (1.3622)	grad_norm 0.3752 (nan)	loss_scale 4096.0000 (5047.1581)	mem 6787MB
[2024-07-12 19:22:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/config.json
[2024-07-12 19:22:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: part1
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-12 19:22:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/swin/diffusion_ft_adapter_swin_base_patch4_window7_224_22kto1k_step_cross_process1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-12 19:22:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 108): INFO Creating model:adapter_swin_diffusion_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1
[2024-07-12 19:22:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 110): INFO Adapter_SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-12 19:22:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 113): INFO number of params: 2779464
[2024-07-12 19:22:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-12 19:22:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 148): INFO auto resuming from pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth
[2024-07-12 19:22:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 19): INFO ==============> Resuming form pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth....................
[2024-07-12 19:22:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 26): INFO <All keys matched successfully>
[2024-07-12 19:22:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 36): INFO => loaded successfully 'pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth' (epoch 16)
[2024-07-12 19:24:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 61.219 (61.219)	Loss 0.4146 (0.4146)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 1503MB
[2024-07-12 19:24:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.148 Acc@5 97.258
[2024-07-12 19:24:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 155): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 19:24:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 168): INFO Start training
[2024-07-12 19:24:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][0/2502]	eta 20:36:40 lr 0.000021	 wd 0.0000	time 29.6564 (29.6564)	loss 1.5420 (1.5420)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6777MB
[2024-07-12 19:25:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:31:25 lr 0.000021	 wd 0.0000	time 0.1992 (0.7850)	loss 1.5881 (1.3917)	grad_norm 0.3692 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:25:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:18:52 lr 0.000021	 wd 0.0000	time 0.2096 (0.4919)	loss 1.4938 (1.3823)	grad_norm 0.3969 (0.4044)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:26:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:14:31 lr 0.000021	 wd 0.0000	time 0.1848 (0.3958)	loss 1.2749 (1.3559)	grad_norm 0.3874 (0.3994)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:27:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:14:45 lr 0.000021	 wd 0.0000	time 0.1968 (0.4214)	loss 1.5325 (1.3549)	grad_norm 0.3780 (0.4000)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:27:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:12:40 lr 0.000021	 wd 0.0000	time 0.1944 (0.3797)	loss 1.3823 (1.3550)	grad_norm 0.3737 (0.3957)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:27:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:11:05 lr 0.000021	 wd 0.0000	time 0.1942 (0.3499)	loss 0.9640 (1.3592)	grad_norm 0.3821 (0.3925)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:28:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:10:31 lr 0.000021	 wd 0.0000	time 0.5077 (0.3502)	loss 1.4430 (1.3585)	grad_norm 0.3830 (0.3906)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:28:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:26 lr 0.000021	 wd 0.0000	time 0.1933 (0.3328)	loss 1.4758 (1.3583)	grad_norm 0.3916 (0.3903)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:29:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:08:29 lr 0.000021	 wd 0.0000	time 0.1931 (0.3178)	loss 1.4369 (1.3530)	grad_norm 0.3711 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:29:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:07:39 lr 0.000020	 wd 0.0000	time 0.1919 (0.3057)	loss 1.2618 (1.3525)	grad_norm 0.4022 (0.3891)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:30:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:31 lr 0.000020	 wd 0.0000	time 0.2866 (0.3219)	loss 1.4766 (1.3531)	grad_norm 0.3811 (0.3895)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:30:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:06:49 lr 0.000020	 wd 0.0000	time 0.1902 (0.3146)	loss 1.4092 (1.3568)	grad_norm 0.3680 (0.3895)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:30:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:08 lr 0.000020	 wd 0.0000	time 0.1842 (0.3069)	loss 1.5224 (1.3581)	grad_norm 0.3667 (0.3889)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:31:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:05:35 lr 0.000020	 wd 0.0000	time 0.3842 (0.3044)	loss 1.4156 (1.3601)	grad_norm 0.3910 (0.3879)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:31:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:03 lr 0.000020	 wd 0.0000	time 0.2088 (0.3032)	loss 1.3305 (1.3604)	grad_norm 0.3786 (0.3877)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:32:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:28 lr 0.000020	 wd 0.0000	time 0.2290 (0.2976)	loss 1.3637 (1.3605)	grad_norm 0.3833 (0.3877)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:32:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:54 lr 0.000020	 wd 0.0000	time 0.2182 (0.2927)	loss 1.6081 (1.3606)	grad_norm 0.3944 (0.3876)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:32:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:23 lr 0.000020	 wd 0.0000	time 0.2143 (0.2896)	loss 1.2758 (1.3610)	grad_norm 0.3433 (0.3876)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:50:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 366): INFO Full config saved to pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/config.json
[2024-07-12 19:50:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.2
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_swin_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 2
    - 2
    - 8
    - 1
    EMBED_DIMS:
    - 64
    - 128
    - 256
    - 512
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    HEAD_CONV: 3
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: false
  VCNU_SWIN:
    AB_NORM_ATTN: false
    AB_NORM_LTM: false
    APE: false
    DEPTHS:
    - 2
    - 2
    - 18
    - 2
    EMBED_DIM: 128
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: part1
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 4
    - 8
    - 16
    - 32
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 1.0e-08

[2024-07-12 19:50:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/swin/diffusion_ft_adapter_swin_base_patch4_window7_224_22kto1k_step_cross_process1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_sequence_crosslayer_process0/diffusion_ft_adapter_swin_b_22kto1k_sequence_crosslayer0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/vcnu_finetune", "tag": "diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-12 19:50:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 108): INFO Creating model:adapter_swin_diffusion_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1
[2024-07-12 19:50:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 110): INFO Adapter_SwinTransformer_Diffusion_Finetune(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=128, input_resolution=(56, 56), num_heads=4, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=128, window_size=(7, 7), num_heads=4
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=128, emb_dim=32, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=128, out_features=32, bias=True)
            (up): Linear(in_features=32, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=256, input_resolution=(28, 28), num_heads=8, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=256, window_size=(7, 7), num_heads=8
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=256, emb_dim=64, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=256, out_features=64, bias=True)
            (up): Linear(in_features=64, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): VCNU_SwinTransformerBlock(
          dim=512, input_resolution=(14, 14), num_heads=16, window_size=7, shift_size=3, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=512, window_size=(7, 7), num_heads=16
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=512, emb_dim=128, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=512, out_features=128, bias=True)
            (up): Linear(in_features=128, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): VCNU_SwinTransformerBlock(
          dim=1024, input_resolution=(7, 7), num_heads=32, window_size=7, shift_size=0, mlp_ratio=4.0, training_mode=tfs
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=1024, window_size=(7, 7), num_heads=32
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (adapter): Adapter(
            dim=1024, emb_dim=256, model_style=trans, 
            (activation): GELU()
            (down): Linear(in_features=1024, out_features=256, bias=True)
            (up): Linear(in_features=256, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=1000, bias=True)
)
[2024-07-12 19:50:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 113): INFO number of params: 2779464
[2024-07-12 19:50:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 116): INFO number of GFLOPs: 15.438473216
[2024-07-12 19:50:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 148): INFO auto resuming from pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth
[2024-07-12 19:50:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 19): INFO ==============> Resuming form pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth....................
[2024-07-12 19:50:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 26): INFO <All keys matched successfully>
[2024-07-12 19:50:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 36): INFO => loaded successfully 'pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_best.pth' (epoch 16)
[2024-07-12 19:51:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 54.508 (54.508)	Loss 0.4146 (0.4146)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 1503MB
[2024-07-12 19:52:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.148 Acc@5 97.258
[2024-07-12 19:52:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 155): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 19:52:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 168): INFO Start training
[2024-07-12 19:52:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][0/2502]	eta 12:51:13 lr 0.000021	 wd 0.0000	time 18.4947 (18.4947)	loss 1.5420 (1.5420)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6777MB
[2024-07-12 19:52:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:15:31 lr 0.000021	 wd 0.0000	time 0.1909 (0.3878)	loss 1.5881 (1.3917)	grad_norm 0.3692 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:53:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:12:10 lr 0.000021	 wd 0.0000	time 0.4320 (0.3172)	loss 1.4938 (1.3823)	grad_norm 0.3969 (0.4044)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:53:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:11:19 lr 0.000021	 wd 0.0000	time 0.1855 (0.3087)	loss 1.2749 (1.3559)	grad_norm 0.3874 (0.3994)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:54:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:09:52 lr 0.000021	 wd 0.0000	time 0.2087 (0.2820)	loss 1.5325 (1.3549)	grad_norm 0.3780 (0.4000)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:54:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:08:49 lr 0.000021	 wd 0.0000	time 0.1817 (0.2644)	loss 1.3823 (1.3550)	grad_norm 0.3737 (0.3957)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:54:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:08:19 lr 0.000021	 wd 0.0000	time 0.6145 (0.2627)	loss 0.9640 (1.3592)	grad_norm 0.3821 (0.3925)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:55:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:08:48 lr 0.000021	 wd 0.0000	time 0.2585 (0.2932)	loss 1.4430 (1.3585)	grad_norm 0.3830 (0.3906)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:56:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:08:01 lr 0.000021	 wd 0.0000	time 0.1984 (0.2828)	loss 1.4758 (1.3583)	grad_norm 0.3916 (0.3903)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:56:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:07:23 lr 0.000021	 wd 0.0000	time 0.2445 (0.2767)	loss 1.4369 (1.3530)	grad_norm 0.3711 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:57:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:07:01 lr 0.000020	 wd 0.0000	time 0.2210 (0.2809)	loss 1.2618 (1.3525)	grad_norm 0.4022 (0.3891)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:57:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:06:24 lr 0.000020	 wd 0.0000	time 0.2034 (0.2744)	loss 1.4766 (1.3531)	grad_norm 0.3811 (0.3895)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:57:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:05:49 lr 0.000020	 wd 0.0000	time 0.1925 (0.2682)	loss 1.4092 (1.3568)	grad_norm 0.3680 (0.3895)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:58:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:05:17 lr 0.000020	 wd 0.0000	time 0.3729 (0.2645)	loss 1.5224 (1.3581)	grad_norm 0.3667 (0.3889)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:58:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:05:07 lr 0.000020	 wd 0.0000	time 0.2341 (0.2786)	loss 1.4156 (1.3601)	grad_norm 0.3910 (0.3879)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:59:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:04:34 lr 0.000020	 wd 0.0000	time 0.1904 (0.2737)	loss 1.3305 (1.3604)	grad_norm 0.3786 (0.3877)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:59:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:02 lr 0.000020	 wd 0.0000	time 0.1853 (0.2688)	loss 1.3637 (1.3605)	grad_norm 0.3833 (0.3877)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 19:59:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:34 lr 0.000020	 wd 0.0000	time 0.2270 (0.2671)	loss 1.6081 (1.3606)	grad_norm 0.3944 (0.3876)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:00:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:07 lr 0.000020	 wd 0.0000	time 0.2632 (0.2665)	loss 1.2758 (1.3610)	grad_norm 0.3433 (0.3876)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:00:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:38 lr 0.000020	 wd 0.0000	time 0.2159 (0.2637)	loss 1.4045 (1.3609)	grad_norm 0.3893 (0.3879)	loss_scale 16384.0000 (8597.0752)	mem 6789MB
[2024-07-12 20:01:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:11 lr 0.000019	 wd 0.0000	time 0.1805 (0.2613)	loss 1.4729 (1.3581)	grad_norm 0.3517 (0.3879)	loss_scale 16384.0000 (8986.2269)	mem 6789MB
[2024-07-12 20:01:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:44 lr 0.000019	 wd 0.0000	time 0.2478 (0.2598)	loss 1.3431 (1.3592)	grad_norm 0.3855 (0.3878)	loss_scale 16384.0000 (9338.3341)	mem 6789MB
[2024-07-12 20:01:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:18 lr 0.000019	 wd 0.0000	time 0.1919 (0.2591)	loss 1.6532 (1.3604)	grad_norm 0.3509 (0.3877)	loss_scale 16384.0000 (9658.4462)	mem 6789MB
[2024-07-12 20:02:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:51 lr 0.000019	 wd 0.0000	time 0.2186 (0.2571)	loss 1.4065 (1.3592)	grad_norm 0.3821 (0.3872)	loss_scale 16384.0000 (9950.7345)	mem 6789MB
[2024-07-12 20:02:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:26 lr 0.000019	 wd 0.0000	time 0.1783 (0.2554)	loss 1.4316 (1.3591)	grad_norm 0.3930 (0.3875)	loss_scale 16384.0000 (10218.6756)	mem 6789MB
[2024-07-12 20:02:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0000	time 0.1619 (0.2525)	loss 1.4523 (1.3593)	grad_norm 0.3636 (0.3873)	loss_scale 16384.0000 (10465.1899)	mem 6789MB
[2024-07-12 20:02:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 17 training takes 0:10:36
[2024-07-12 20:03:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 40.124 (40.124)	Loss 0.4150 (0.4150)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 20:03:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.114 Acc@5 97.266
[2024-07-12 20:03:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 20:03:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 20:04:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][0/2502]	eta 10:54:28 lr 0.000019	 wd 0.0000	time 15.6948 (15.6948)	loss 1.3655 (1.3655)	grad_norm 0.0000 (0.0000)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:04:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:14:59 lr 0.000019	 wd 0.0000	time 0.2357 (0.3744)	loss 1.3932 (1.4017)	grad_norm 0.4025 (0.3942)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:04:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:13:13 lr 0.000019	 wd 0.0000	time 0.2324 (0.3447)	loss 1.5101 (1.4008)	grad_norm 0.4717 (0.3926)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:05:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:10:58 lr 0.000019	 wd 0.0000	time 0.1858 (0.2992)	loss 1.7430 (1.3873)	grad_norm 0.3606 (0.3920)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:05:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:09:41 lr 0.000019	 wd 0.0000	time 0.2162 (0.2767)	loss 0.9648 (1.3762)	grad_norm 0.4726 (0.3912)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:05:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:08:46 lr 0.000018	 wd 0.0000	time 0.2732 (0.2629)	loss 1.4446 (1.3722)	grad_norm 0.3608 (0.3892)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:06:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:08:45 lr 0.000018	 wd 0.0000	time 0.5012 (0.2764)	loss 1.2930 (1.3689)	grad_norm 0.3941 (0.3903)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:06:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:08:07 lr 0.000018	 wd 0.0000	time 0.2041 (0.2706)	loss 1.5538 (1.3661)	grad_norm 0.4066 (0.3890)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:07:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:07:27 lr 0.000018	 wd 0.0000	time 0.2306 (0.2632)	loss 1.4204 (1.3657)	grad_norm 0.3493 (0.3881)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:07:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:06:52 lr 0.000018	 wd 0.0000	time 0.2176 (0.2576)	loss 1.4493 (1.3650)	grad_norm 0.3820 (0.3883)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:08:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:06:39 lr 0.000018	 wd 0.0000	time 0.2399 (0.2657)	loss 1.3618 (1.3632)	grad_norm 0.3700 (0.3880)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:08:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:06:05 lr 0.000018	 wd 0.0000	time 0.1920 (0.2608)	loss 0.8772 (1.3625)	grad_norm 0.3749 (0.3880)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:08:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:05:34 lr 0.000018	 wd 0.0000	time 0.2131 (0.2568)	loss 1.4474 (1.3652)	grad_norm 0.3821 (0.3876)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:09:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:05:04 lr 0.000018	 wd 0.0000	time 0.2242 (0.2537)	loss 1.6082 (1.3682)	grad_norm 0.3843 (0.3881)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:09:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:04:38 lr 0.000018	 wd 0.0000	time 0.2467 (0.2529)	loss 1.5417 (1.3647)	grad_norm 0.3628 (0.3878)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:10:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:04:11 lr 0.000017	 wd 0.0000	time 0.2251 (0.2506)	loss 0.9257 (1.3640)	grad_norm 0.3811 (0.3876)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:10:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:03:44 lr 0.000017	 wd 0.0000	time 0.2135 (0.2484)	loss 1.0133 (1.3625)	grad_norm 0.3614 (0.3875)	loss_scale 16384.0000 (16384.0000)	mem 6789MB
[2024-07-12 20:10:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:03:17 lr 0.000017	 wd 0.0000	time 0.1929 (0.2463)	loss 1.6051 (1.3628)	grad_norm 0.3853 (nan)	loss_scale 8192.0000 (16085.4086)	mem 6789MB
[2024-07-12 20:11:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:02:52 lr 0.000017	 wd 0.0000	time 0.2183 (0.2451)	loss 1.1518 (1.3621)	grad_norm 0.3976 (nan)	loss_scale 8192.0000 (15647.1294)	mem 6789MB
[2024-07-12 20:11:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:27 lr 0.000017	 wd 0.0000	time 0.2797 (0.2449)	loss 1.5044 (1.3636)	grad_norm 0.4130 (nan)	loss_scale 8192.0000 (15254.9605)	mem 6789MB
[2024-07-12 20:11:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:02 lr 0.000017	 wd 0.0000	time 0.2062 (0.2437)	loss 1.4476 (1.3639)	grad_norm 0.3870 (nan)	loss_scale 8192.0000 (14901.9890)	mem 6789MB
[2024-07-12 20:12:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:37 lr 0.000017	 wd 0.0000	time 0.2112 (0.2423)	loss 1.3755 (1.3653)	grad_norm 0.3779 (nan)	loss_scale 8192.0000 (14582.6178)	mem 6789MB
[2024-07-12 20:12:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:12 lr 0.000017	 wd 0.0000	time 0.1923 (0.2415)	loss 1.2143 (1.3665)	grad_norm 0.3781 (nan)	loss_scale 8192.0000 (14292.2672)	mem 6789MB
[2024-07-12 20:13:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:48 lr 0.000017	 wd 0.0000	time 0.2149 (0.2414)	loss 1.2129 (1.3662)	grad_norm 0.3746 (nan)	loss_scale 4096.0000 (13891.8661)	mem 6789MB
[2024-07-12 20:13:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:24 lr 0.000017	 wd 0.0000	time 0.2167 (0.2405)	loss 1.5978 (1.3650)	grad_norm 0.3802 (nan)	loss_scale 4096.0000 (13483.8751)	mem 6789MB
[2024-07-12 20:13:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0000	time 0.1435 (0.2381)	loss 1.0240 (1.3660)	grad_norm 0.3450 (nan)	loss_scale 4096.0000 (13108.5102)	mem 6789MB
[2024-07-12 20:13:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 18 training takes 0:09:59
[2024-07-12 20:14:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 30.463 (30.463)	Loss 0.4143 (0.4143)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 20:14:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.096 Acc@5 97.262
[2024-07-12 20:14:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 20:14:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 20:14:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][0/2502]	eta 12:33:02 lr 0.000016	 wd 0.0000	time 18.0585 (18.0585)	loss 1.4394 (1.4394)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:15:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:15:24 lr 0.000016	 wd 0.0000	time 0.1892 (0.3850)	loss 1.6785 (1.3423)	grad_norm 0.3861 (0.3807)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:15:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:11:22 lr 0.000016	 wd 0.0000	time 0.2076 (0.2964)	loss 1.4137 (1.3542)	grad_norm 0.3965 (0.3825)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:16:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:11:19 lr 0.000016	 wd 0.0000	time 0.2132 (0.3085)	loss 1.2117 (1.3656)	grad_norm 0.3665 (0.3852)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:16:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:09:59 lr 0.000016	 wd 0.0000	time 0.2089 (0.2852)	loss 1.6354 (1.3617)	grad_norm 0.3749 (0.3866)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:16:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:08:59 lr 0.000016	 wd 0.0000	time 0.2057 (0.2696)	loss 1.4800 (1.3604)	grad_norm 0.3695 (0.3857)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:17:14 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:08:13 lr 0.000016	 wd 0.0000	time 0.1916 (0.2595)	loss 1.1327 (1.3554)	grad_norm 0.3787 (0.3867)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:17:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:07:45 lr 0.000016	 wd 0.0000	time 0.3013 (0.2585)	loss 1.2669 (1.3575)	grad_norm 0.3792 (0.3874)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:18:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:07:20 lr 0.000016	 wd 0.0000	time 0.1909 (0.2586)	loss 1.4710 (1.3552)	grad_norm 0.3629 (0.3879)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:18:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:06:46 lr 0.000016	 wd 0.0000	time 0.2209 (0.2535)	loss 1.4931 (1.3594)	grad_norm 0.3851 (0.3872)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:18:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:06:14 lr 0.000016	 wd 0.0000	time 0.2036 (0.2492)	loss 1.5718 (1.3639)	grad_norm 0.3702 (0.3865)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:19:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:05:46 lr 0.000015	 wd 0.0000	time 0.2379 (0.2472)	loss 1.0807 (1.3654)	grad_norm 0.3891 (0.3863)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:19:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:05:20 lr 0.000015	 wd 0.0000	time 0.1786 (0.2463)	loss 1.1540 (1.3669)	grad_norm 0.3599 (0.3858)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:19:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:04:52 lr 0.000015	 wd 0.0000	time 0.2075 (0.2437)	loss 1.5125 (1.3686)	grad_norm 0.3844 (0.3868)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:20:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:04:26 lr 0.000015	 wd 0.0000	time 0.2046 (0.2414)	loss 1.3761 (1.3684)	grad_norm 0.3894 (0.3867)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:20:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:04:00 lr 0.000015	 wd 0.0000	time 0.2010 (0.2396)	loss 1.5562 (1.3661)	grad_norm 0.3571 (0.3865)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:21:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:03:36 lr 0.000015	 wd 0.0000	time 0.3593 (0.2401)	loss 1.1801 (1.3661)	grad_norm 0.3677 (0.3863)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:21:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:03:11 lr 0.000015	 wd 0.0000	time 0.1977 (0.2390)	loss 1.5490 (1.3655)	grad_norm 0.3911 (0.3859)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:21:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:02:46 lr 0.000015	 wd 0.0000	time 0.2200 (0.2377)	loss 1.3901 (1.3662)	grad_norm 0.3808 (0.3862)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:22:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:22 lr 0.000015	 wd 0.0000	time 0.2059 (0.2362)	loss 1.0462 (1.3641)	grad_norm 0.3512 (0.3861)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:22:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:01:58 lr 0.000015	 wd 0.0000	time 0.2030 (0.2359)	loss 1.3348 (1.3623)	grad_norm 0.4493 (0.3858)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:22:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:34 lr 0.000014	 wd 0.0000	time 0.2576 (0.2355)	loss 1.4935 (1.3617)	grad_norm 0.3564 (0.3859)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:23:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:10 lr 0.000014	 wd 0.0000	time 0.2490 (0.2349)	loss 1.5207 (1.3612)	grad_norm 0.4203 (0.3860)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:23:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:47 lr 0.000014	 wd 0.0000	time 0.1940 (0.2340)	loss 1.6029 (1.3617)	grad_norm 0.5031 (0.3876)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:23:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:23 lr 0.000014	 wd 0.0000	time 0.2139 (0.2335)	loss 1.5067 (1.3606)	grad_norm 0.3577 (0.3876)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:24:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0000	time 0.1722 (0.2319)	loss 1.4030 (1.3607)	grad_norm 0.3725 (0.3876)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:24:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 19 training takes 0:09:49
[2024-07-12 20:24:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 22.033 (22.033)	Loss 0.4180 (0.4180)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 20:25:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.136 Acc@5 97.260
[2024-07-12 20:25:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 20:25:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 20:25:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][0/2502]	eta 10:32:26 lr 0.000014	 wd 0.0000	time 15.1665 (15.1665)	loss 1.0798 (1.0798)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:25:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:19:18 lr 0.000014	 wd 0.0000	time 0.1958 (0.4824)	loss 1.6293 (1.3479)	grad_norm 0.3948 (0.3913)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:26:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:13:24 lr 0.000014	 wd 0.0000	time 0.1986 (0.3496)	loss 1.5048 (1.3592)	grad_norm 0.3557 (0.3927)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:26:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:11:07 lr 0.000014	 wd 0.0000	time 0.2006 (0.3030)	loss 1.2360 (1.3487)	grad_norm 0.3710 (0.3905)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:26:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:09:45 lr 0.000014	 wd 0.0000	time 0.2122 (0.2785)	loss 1.4134 (1.3506)	grad_norm 0.3774 (0.3904)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:27:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:08:58 lr 0.000014	 wd 0.0000	time 0.2399 (0.2688)	loss 1.3702 (1.3516)	grad_norm 0.3857 (0.3880)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:27:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:08:23 lr 0.000014	 wd 0.0000	time 0.1969 (0.2648)	loss 1.7490 (1.3505)	grad_norm 0.3848 (0.3875)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:28:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:07:43 lr 0.000013	 wd 0.0000	time 0.2210 (0.2574)	loss 1.4194 (1.3522)	grad_norm 0.4195 (0.3888)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:28:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:07:08 lr 0.000013	 wd 0.0000	time 0.2178 (0.2517)	loss 1.0619 (1.3537)	grad_norm 0.3628 (0.3887)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:28:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:06:38 lr 0.000013	 wd 0.0000	time 0.3375 (0.2488)	loss 1.3141 (1.3541)	grad_norm 0.3500 (0.3888)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:29:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:06:14 lr 0.000013	 wd 0.0000	time 0.2171 (0.2493)	loss 1.4333 (1.3561)	grad_norm 0.3907 (0.3892)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:29:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:05:45 lr 0.000013	 wd 0.0000	time 0.2072 (0.2463)	loss 1.4750 (1.3575)	grad_norm 0.3649 (0.3900)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:29:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:05:17 lr 0.000013	 wd 0.0000	time 0.1976 (0.2437)	loss 1.1335 (1.3569)	grad_norm 0.3399 (0.3893)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 20:30:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:04:50 lr 0.000013	 wd 0.0000	time 0.2312 (0.2417)	loss 1.3244 (1.3587)	grad_norm 0.3812 (0.3897)	loss_scale 8192.0000 (4347.8678)	mem 6789MB
[2024-07-12 20:30:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:04:26 lr 0.000013	 wd 0.0000	time 0.2088 (0.2417)	loss 1.2739 (1.3595)	grad_norm 0.3944 (0.3900)	loss_scale 8192.0000 (4622.2527)	mem 6789MB
[2024-07-12 20:31:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:04:01 lr 0.000013	 wd 0.0000	time 0.2101 (0.2407)	loss 1.5621 (1.3598)	grad_norm 0.3884 (0.3898)	loss_scale 8192.0000 (4860.0773)	mem 6789MB
[2024-07-12 20:31:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:03:35 lr 0.000013	 wd 0.0000	time 0.1889 (0.2391)	loss 1.6683 (1.3590)	grad_norm 0.3581 (0.3905)	loss_scale 8192.0000 (5068.1924)	mem 6789MB
[2024-07-12 20:31:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:03:10 lr 0.000012	 wd 0.0000	time 0.2041 (0.2376)	loss 1.1265 (1.3588)	grad_norm 0.3594 (0.3916)	loss_scale 8192.0000 (5251.8377)	mem 6789MB
[2024-07-12 20:32:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:02:46 lr 0.000012	 wd 0.0000	time 0.2278 (0.2371)	loss 1.5771 (1.3591)	grad_norm 0.3668 (0.3911)	loss_scale 8192.0000 (5415.0894)	mem 6789MB
[2024-07-12 20:32:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:23 lr 0.000012	 wd 0.0000	time 0.1784 (0.2377)	loss 1.5704 (1.3593)	grad_norm 0.4129 (0.3919)	loss_scale 8192.0000 (5561.1657)	mem 6789MB
[2024-07-12 20:32:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:01:58 lr 0.000012	 wd 0.0000	time 0.2055 (0.2366)	loss 1.1688 (1.3602)	grad_norm 0.3929 (0.3919)	loss_scale 8192.0000 (5692.6417)	mem 6789MB
[2024-07-12 20:33:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:34 lr 0.000012	 wd 0.0000	time 0.2023 (0.2356)	loss 1.4104 (1.3597)	grad_norm 0.3646 (0.3911)	loss_scale 8192.0000 (5811.6021)	mem 6789MB
[2024-07-12 20:33:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:10 lr 0.000012	 wd 0.0000	time 0.2363 (0.2351)	loss 1.3844 (1.3612)	grad_norm 0.3902 (0.3911)	loss_scale 8192.0000 (5919.7528)	mem 6789MB
[2024-07-12 20:34:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:47 lr 0.000012	 wd 0.0000	time 0.1982 (0.2353)	loss 1.5126 (1.3619)	grad_norm 0.3944 (0.3909)	loss_scale 8192.0000 (6018.5033)	mem 6789MB
[2024-07-12 20:34:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:23 lr 0.000012	 wd 0.0000	time 0.1974 (0.2347)	loss 0.8872 (1.3615)	grad_norm 0.3589 (0.3909)	loss_scale 8192.0000 (6109.0279)	mem 6789MB
[2024-07-12 20:34:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0000	time 0.1641 (0.2326)	loss 1.2978 (1.3616)	grad_norm 0.3817 (0.3910)	loss_scale 8192.0000 (6192.3135)	mem 6789MB
[2024-07-12 20:34:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 20 training takes 0:09:46
[2024-07-12 20:35:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 24.443 (24.443)	Loss 0.4167 (0.4167)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 20:35:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.112 Acc@5 97.264
[2024-07-12 20:35:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 20:35:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 20:35:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][0/2502]	eta 13:52:28 lr 0.000012	 wd 0.0000	time 19.9633 (19.9633)	loss 1.4535 (1.4535)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:36:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:16:13 lr 0.000012	 wd 0.0000	time 0.2019 (0.4055)	loss 1.1984 (1.4016)	grad_norm 0.3642 (0.3920)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:36:41 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:11:45 lr 0.000012	 wd 0.0000	time 0.2336 (0.3064)	loss 1.0171 (1.3862)	grad_norm 0.3901 (0.3868)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:37:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:11:19 lr 0.000012	 wd 0.0000	time 0.2265 (0.3086)	loss 1.0594 (1.3845)	grad_norm 0.3912 (0.3855)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:37:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:09:59 lr 0.000011	 wd 0.0000	time 0.1943 (0.2852)	loss 1.5333 (1.3726)	grad_norm 0.3550 (0.3856)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:37:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:09:00 lr 0.000011	 wd 0.0000	time 0.2089 (0.2699)	loss 1.4002 (1.3708)	grad_norm 0.3902 (0.3866)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:38:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:08:14 lr 0.000011	 wd 0.0000	time 0.1888 (0.2599)	loss 1.5522 (1.3648)	grad_norm 0.4604 (0.3877)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:38:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:07:43 lr 0.000011	 wd 0.0000	time 0.3030 (0.2570)	loss 1.2731 (1.3681)	grad_norm 0.3632 (0.3873)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:39:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:07:12 lr 0.000011	 wd 0.0000	time 0.2015 (0.2541)	loss 1.5910 (1.3663)	grad_norm 0.3718 (0.3875)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:39:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:06:39 lr 0.000011	 wd 0.0000	time 0.1956 (0.2495)	loss 1.3210 (1.3656)	grad_norm 0.3587 (0.3877)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:39:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:06:09 lr 0.000011	 wd 0.0000	time 0.2173 (0.2457)	loss 1.5140 (1.3655)	grad_norm 0.3664 (0.3871)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:40:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:05:41 lr 0.000011	 wd 0.0000	time 0.2283 (0.2439)	loss 1.4223 (1.3658)	grad_norm 0.3695 (0.3880)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:40:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:05:18 lr 0.000011	 wd 0.0000	time 0.1827 (0.2445)	loss 1.1312 (1.3645)	grad_norm 0.3664 (0.3878)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:40:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:04:51 lr 0.000011	 wd 0.0000	time 0.2067 (0.2423)	loss 1.5610 (1.3639)	grad_norm 0.3931 (0.3883)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:41:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:04:24 lr 0.000011	 wd 0.0000	time 0.2248 (0.2403)	loss 1.3992 (1.3651)	grad_norm 0.3842 (0.3888)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:41:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:03:59 lr 0.000010	 wd 0.0000	time 0.1835 (0.2387)	loss 1.3053 (1.3678)	grad_norm 0.3734 (0.3893)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:42:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:03:36 lr 0.000010	 wd 0.0000	time 0.2052 (0.2395)	loss 1.1475 (1.3664)	grad_norm 0.3951 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:42:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:03:11 lr 0.000010	 wd 0.0000	time 0.1677 (0.2388)	loss 1.4580 (1.3663)	grad_norm 0.3969 (0.3897)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:42:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:02:46 lr 0.000010	 wd 0.0000	time 0.2117 (0.2378)	loss 1.3275 (1.3655)	grad_norm 0.3771 (0.3890)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:43:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:22 lr 0.000010	 wd 0.0000	time 0.2427 (0.2365)	loss 1.3986 (1.3647)	grad_norm 0.3563 (0.3887)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:43:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:01:58 lr 0.000010	 wd 0.0000	time 0.1980 (0.2362)	loss 1.4106 (1.3647)	grad_norm 0.3770 (0.3893)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:43:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:34 lr 0.000010	 wd 0.0000	time 0.2078 (0.2363)	loss 1.3216 (1.3638)	grad_norm 0.3764 (0.3894)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:44:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:11 lr 0.000010	 wd 0.0000	time 0.2099 (0.2353)	loss 0.8607 (1.3617)	grad_norm 0.3654 (0.3893)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:44:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:47 lr 0.000010	 wd 0.0000	time 0.2264 (0.2344)	loss 1.2934 (1.3621)	grad_norm 0.3960 (0.3889)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:45:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:23 lr 0.000010	 wd 0.0000	time 0.2417 (0.2340)	loss 1.5721 (1.3613)	grad_norm 0.3456 (0.3888)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:45:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000010	 wd 0.0000	time 0.1517 (0.2324)	loss 0.9110 (1.3604)	grad_norm 0.3590 (0.3886)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:45:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 21 training takes 0:09:50
[2024-07-12 20:45:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 25.047 (25.047)	Loss 0.4172 (0.4172)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 20:46:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.076 Acc@5 97.258
[2024-07-12 20:46:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 20:46:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 20:46:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][0/2502]	eta 11:47:18 lr 0.000010	 wd 0.0000	time 16.9620 (16.9620)	loss 1.5803 (1.5803)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:46:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:19:35 lr 0.000010	 wd 0.0000	time 0.2193 (0.4894)	loss 1.3332 (1.3810)	grad_norm 0.3939 (0.3824)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:47:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:13:30 lr 0.000009	 wd 0.0000	time 0.1976 (0.3519)	loss 1.4108 (1.3813)	grad_norm 0.3648 (0.3900)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:47:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:11:10 lr 0.000009	 wd 0.0000	time 0.1989 (0.3045)	loss 1.5553 (1.3732)	grad_norm 0.3716 (0.3902)	loss_scale 16384.0000 (10478.1395)	mem 6789MB
[2024-07-12 20:47:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:09:48 lr 0.000009	 wd 0.0000	time 0.2307 (0.2800)	loss 0.9327 (1.3724)	grad_norm 0.3862 (0.3906)	loss_scale 16384.0000 (11950.9227)	mem 6789MB
[2024-07-12 20:48:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:09:30 lr 0.000009	 wd 0.0000	time 0.2064 (0.2848)	loss 1.5725 (1.3692)	grad_norm 0.3879 (0.3897)	loss_scale 16384.0000 (12835.7685)	mem 6789MB
[2024-07-12 20:48:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:08:41 lr 0.000009	 wd 0.0000	time 0.2035 (0.2741)	loss 1.4736 (1.3701)	grad_norm 0.3463 (0.3910)	loss_scale 16384.0000 (13426.1564)	mem 6789MB
[2024-07-12 20:49:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:07:57 lr 0.000009	 wd 0.0000	time 0.1915 (0.2648)	loss 1.5269 (1.3704)	grad_norm 0.4100 (0.3913)	loss_scale 16384.0000 (13848.1027)	mem 6789MB
[2024-07-12 20:49:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:07:19 lr 0.000009	 wd 0.0000	time 0.2140 (0.2580)	loss 0.9476 (1.3662)	grad_norm 0.3680 (0.3915)	loss_scale 16384.0000 (14164.6941)	mem 6789MB
[2024-07-12 20:49:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:06:49 lr 0.000009	 wd 0.0000	time 0.1984 (0.2558)	loss 0.8470 (1.3644)	grad_norm 0.3832 (0.3910)	loss_scale 16384.0000 (14411.0100)	mem 6789MB
[2024-07-12 20:50:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:06:21 lr 0.000009	 wd 0.0000	time 0.2062 (0.2543)	loss 1.4373 (1.3672)	grad_norm 0.3825 (nan)	loss_scale 8192.0000 (14182.5534)	mem 6789MB
[2024-07-12 20:50:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:05:50 lr 0.000009	 wd 0.0000	time 0.1939 (0.2503)	loss 1.4625 (1.3630)	grad_norm 0.4411 (nan)	loss_scale 8192.0000 (13638.4523)	mem 6789MB
[2024-07-12 20:51:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:05:21 lr 0.000009	 wd 0.0000	time 0.2020 (0.2470)	loss 1.4675 (1.3617)	grad_norm 0.3705 (nan)	loss_scale 8192.0000 (13184.9592)	mem 6789MB
[2024-07-12 20:51:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:04:54 lr 0.000009	 wd 0.0000	time 0.2696 (0.2451)	loss 1.5724 (1.3621)	grad_norm 0.3738 (nan)	loss_scale 8192.0000 (12801.1806)	mem 6789MB
[2024-07-12 20:51:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:04:30 lr 0.000008	 wd 0.0000	time 0.1721 (0.2455)	loss 1.5245 (1.3620)	grad_norm 0.3528 (nan)	loss_scale 8192.0000 (12472.1884)	mem 6789MB
[2024-07-12 20:52:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:04:04 lr 0.000008	 wd 0.0000	time 0.1976 (0.2436)	loss 1.4694 (1.3631)	grad_norm 0.3665 (nan)	loss_scale 8192.0000 (12187.0326)	mem 6789MB
[2024-07-12 20:52:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:03:38 lr 0.000008	 wd 0.0000	time 0.1981 (0.2418)	loss 1.4796 (1.3645)	grad_norm 0.3778 (nan)	loss_scale 8192.0000 (11937.4991)	mem 6789MB
[2024-07-12 20:52:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:03:12 lr 0.000008	 wd 0.0000	time 0.2171 (0.2403)	loss 1.3070 (1.3638)	grad_norm 0.9453 (nan)	loss_scale 8192.0000 (11717.3051)	mem 6789MB
[2024-07-12 20:53:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:02:48 lr 0.000008	 wd 0.0000	time 0.4681 (0.2404)	loss 1.2307 (1.3630)	grad_norm 0.3792 (nan)	loss_scale 8192.0000 (11521.5636)	mem 6789MB
[2024-07-12 20:53:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:24 lr 0.000008	 wd 0.0000	time 0.2077 (0.2400)	loss 1.3890 (1.3650)	grad_norm 0.3979 (nan)	loss_scale 8192.0000 (11346.4156)	mem 6789MB
[2024-07-12 20:54:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:01:59 lr 0.000008	 wd 0.0000	time 0.2246 (0.2388)	loss 1.5640 (1.3656)	grad_norm 0.3911 (nan)	loss_scale 8192.0000 (11188.7736)	mem 6789MB
[2024-07-12 20:54:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:35 lr 0.000008	 wd 0.0000	time 0.2085 (0.2375)	loss 1.4925 (1.3664)	grad_norm 0.3745 (nan)	loss_scale 8192.0000 (11046.1380)	mem 6789MB
[2024-07-12 20:54:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:11 lr 0.000008	 wd 0.0000	time 0.2341 (0.2371)	loss 1.2698 (1.3642)	grad_norm 0.3980 (nan)	loss_scale 8192.0000 (10916.4634)	mem 6789MB
[2024-07-12 20:55:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:47 lr 0.000008	 wd 0.0000	time 0.1831 (0.2368)	loss 1.1830 (1.3635)	grad_norm 0.3570 (nan)	loss_scale 8192.0000 (10798.0600)	mem 6789MB
[2024-07-12 20:55:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:24 lr 0.000008	 wd 0.0000	time 0.2310 (0.2358)	loss 1.5126 (1.3641)	grad_norm 0.4265 (nan)	loss_scale 8192.0000 (10689.5194)	mem 6789MB
[2024-07-12 20:55:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0000	time 0.1521 (0.2339)	loss 1.6416 (1.3643)	grad_norm 0.3696 (nan)	loss_scale 8192.0000 (10589.6585)	mem 6789MB
[2024-07-12 20:55:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 22 training takes 0:09:49
[2024-07-12 20:56:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 41.285 (41.285)	Loss 0.4163 (0.4163)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 20:56:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.122 Acc@5 97.252
[2024-07-12 20:56:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 20:56:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 20:57:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][0/2502]	eta 11:07:47 lr 0.000008	 wd 0.0000	time 16.0141 (16.0141)	loss 1.4909 (1.4909)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:57:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:15:01 lr 0.000008	 wd 0.0000	time 0.2016 (0.3755)	loss 1.2385 (1.3396)	grad_norm 0.3746 (0.3948)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:57:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:11:25 lr 0.000007	 wd 0.0000	time 0.2397 (0.2979)	loss 1.5818 (1.3512)	grad_norm 0.3942 (0.3903)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:58:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:11:13 lr 0.000007	 wd 0.0000	time 0.2032 (0.3059)	loss 1.5734 (1.3615)	grad_norm 0.3949 (0.3914)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:58:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:09:50 lr 0.000007	 wd 0.0000	time 0.2071 (0.2810)	loss 1.3859 (1.3574)	grad_norm 0.3926 (0.3921)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:59:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:08:53 lr 0.000007	 wd 0.0000	time 0.1999 (0.2666)	loss 1.4746 (1.3601)	grad_norm 0.3580 (0.3919)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:59:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:08:11 lr 0.000007	 wd 0.0000	time 0.2501 (0.2582)	loss 1.4338 (1.3611)	grad_norm 0.3680 (0.3918)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 20:59:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:07:45 lr 0.000007	 wd 0.0000	time 0.2072 (0.2583)	loss 1.5365 (1.3643)	grad_norm 0.3988 (0.3912)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:00:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:07:10 lr 0.000007	 wd 0.0000	time 0.1848 (0.2527)	loss 1.2433 (1.3614)	grad_norm 0.4025 (0.3923)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:00:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:06:37 lr 0.000007	 wd 0.0000	time 0.1934 (0.2482)	loss 1.6600 (1.3643)	grad_norm 0.3641 (0.3914)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:00:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:06:06 lr 0.000007	 wd 0.0000	time 0.1994 (0.2443)	loss 1.3088 (1.3600)	grad_norm 0.3721 (0.3907)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:01:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:05:41 lr 0.000007	 wd 0.0000	time 0.2398 (0.2436)	loss 1.2505 (1.3575)	grad_norm 0.3733 (0.3930)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:01:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:05:15 lr 0.000007	 wd 0.0000	time 0.2240 (0.2426)	loss 1.1623 (1.3599)	grad_norm 0.4164 (0.3937)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:02:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:04:48 lr 0.000007	 wd 0.0000	time 0.2105 (0.2403)	loss 1.5615 (1.3576)	grad_norm 0.3969 (0.3954)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:02:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:04:22 lr 0.000007	 wd 0.0000	time 0.2038 (0.2383)	loss 1.5312 (1.3568)	grad_norm 0.4025 (0.3969)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:02:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:03:57 lr 0.000006	 wd 0.0000	time 0.2085 (0.2371)	loss 1.4995 (1.3581)	grad_norm 0.3874 (0.3967)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:03:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:03:34 lr 0.000006	 wd 0.0000	time 0.1761 (0.2375)	loss 1.3927 (1.3569)	grad_norm 0.3782 (0.3963)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:03:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:03:09 lr 0.000006	 wd 0.0000	time 0.1888 (0.2362)	loss 1.4643 (1.3573)	grad_norm 0.3718 (0.3967)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:03:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:02:44 lr 0.000006	 wd 0.0000	time 0.2644 (0.2350)	loss 1.1888 (1.3567)	grad_norm 0.4329 (0.3960)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:04:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:02:20 lr 0.000006	 wd 0.0000	time 0.2868 (0.2341)	loss 1.0354 (1.3566)	grad_norm 0.3788 (0.3955)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:04:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:01:57 lr 0.000006	 wd 0.0000	time 0.3321 (0.2347)	loss 0.9944 (1.3560)	grad_norm 0.4193 (0.3950)	loss_scale 8192.0000 (8192.0000)	mem 6789MB
[2024-07-12 21:05:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:34 lr 0.000006	 wd 0.0000	time 0.2077 (0.2341)	loss 1.5956 (1.3556)	grad_norm 0.3793 (nan)	loss_scale 4096.0000 (8164.7063)	mem 6789MB
[2024-07-12 21:05:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:10 lr 0.000006	 wd 0.0000	time 0.2302 (0.2331)	loss 1.3278 (1.3564)	grad_norm 0.3699 (nan)	loss_scale 4096.0000 (7979.8492)	mem 6789MB
[2024-07-12 21:05:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:46 lr 0.000006	 wd 0.0000	time 0.1984 (0.2321)	loss 1.5649 (1.3572)	grad_norm 0.3640 (nan)	loss_scale 4096.0000 (7811.0595)	mem 6789MB
[2024-07-12 21:06:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:23 lr 0.000006	 wd 0.0000	time 0.2060 (0.2320)	loss 1.3872 (1.3573)	grad_norm 0.3578 (nan)	loss_scale 4096.0000 (7656.3299)	mem 6789MB
[2024-07-12 21:06:28 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0000	time 0.1608 (0.2306)	loss 1.0599 (1.3573)	grad_norm 0.3930 (nan)	loss_scale 4096.0000 (7513.9736)	mem 6789MB
[2024-07-12 21:06:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 23 training takes 0:09:43
[2024-07-12 21:06:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 21.879 (21.879)	Loss 0.4155 (0.4155)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 21:07:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.104 Acc@5 97.270
[2024-07-12 21:07:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 21:07:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 21:07:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][0/2502]	eta 11:15:00 lr 0.000006	 wd 0.0000	time 16.1874 (16.1874)	loss 1.5096 (1.5096)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:07:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:18:06 lr 0.000006	 wd 0.0000	time 0.2059 (0.4523)	loss 1.2267 (1.3832)	grad_norm 0.3798 (0.3816)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:08:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:12:44 lr 0.000006	 wd 0.0000	time 0.1833 (0.3321)	loss 1.5123 (1.3601)	grad_norm 0.3854 (0.3825)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:08:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:10:39 lr 0.000006	 wd 0.0000	time 0.2280 (0.2906)	loss 1.3783 (1.3630)	grad_norm 0.4034 (0.3844)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:08:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:09:25 lr 0.000005	 wd 0.0000	time 0.1999 (0.2691)	loss 1.3466 (1.3586)	grad_norm 0.4159 (0.3868)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:09:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:08:48 lr 0.000005	 wd 0.0000	time 0.3292 (0.2640)	loss 1.5790 (1.3591)	grad_norm 0.3888 (0.3872)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:09:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:08:26 lr 0.000005	 wd 0.0000	time 0.2109 (0.2664)	loss 1.4926 (1.3591)	grad_norm 0.3686 (0.3881)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:10:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:07:45 lr 0.000005	 wd 0.0000	time 0.1866 (0.2581)	loss 1.2578 (1.3625)	grad_norm 0.3503 (0.3880)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:10:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:07:08 lr 0.000005	 wd 0.0000	time 0.2035 (0.2520)	loss 0.8203 (1.3653)	grad_norm 0.3671 (0.3888)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:10:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:06:39 lr 0.000005	 wd 0.0000	time 0.2454 (0.2497)	loss 1.1948 (1.3676)	grad_norm 0.4581 (0.3885)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:11:18 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:06:13 lr 0.000005	 wd 0.0000	time 0.2135 (0.2489)	loss 1.2321 (1.3659)	grad_norm 0.3818 (0.3891)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:11:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:05:44 lr 0.000005	 wd 0.0000	time 0.2120 (0.2455)	loss 1.4819 (1.3650)	grad_norm 0.3679 (0.3887)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:12:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:05:15 lr 0.000005	 wd 0.0000	time 0.1956 (0.2426)	loss 1.2211 (1.3617)	grad_norm 0.4299 (0.3881)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:12:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:04:49 lr 0.000005	 wd 0.0000	time 0.1894 (0.2405)	loss 1.3155 (1.3612)	grad_norm 0.3733 (0.3876)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:12:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:04:25 lr 0.000005	 wd 0.0000	time 0.2248 (0.2410)	loss 1.4191 (1.3614)	grad_norm 0.3564 (0.3871)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:13:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:03:59 lr 0.000005	 wd 0.0000	time 0.2038 (0.2395)	loss 1.3283 (1.3604)	grad_norm 0.4083 (0.3888)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:13:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:03:34 lr 0.000005	 wd 0.0000	time 0.2125 (0.2378)	loss 1.6121 (1.3623)	grad_norm 0.3914 (0.3886)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:13:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:03:09 lr 0.000005	 wd 0.0000	time 0.2045 (0.2362)	loss 1.6174 (1.3624)	grad_norm 0.4056 (0.3892)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:14:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:02:45 lr 0.000005	 wd 0.0000	time 0.2264 (0.2359)	loss 1.2494 (1.3627)	grad_norm 0.3978 (0.3900)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:14:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:02:22 lr 0.000005	 wd 0.0000	time 0.2236 (0.2368)	loss 1.2039 (1.3616)	grad_norm 0.3879 (0.3905)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:15:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:01:58 lr 0.000004	 wd 0.0000	time 0.2005 (0.2357)	loss 1.4480 (1.3629)	grad_norm 0.3773 (0.3917)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:15:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:34 lr 0.000004	 wd 0.0000	time 0.1892 (0.2345)	loss 1.5316 (1.3637)	grad_norm 0.4108 (0.3924)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:15:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:10 lr 0.000004	 wd 0.0000	time 0.1867 (0.2340)	loss 1.2030 (1.3629)	grad_norm 0.3588 (0.3928)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:16:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:47 lr 0.000004	 wd 0.0000	time 0.1989 (0.2342)	loss 1.5090 (1.3630)	grad_norm 0.3787 (0.3923)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:16:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:23 lr 0.000004	 wd 0.0000	time 0.1910 (0.2334)	loss 1.6796 (1.3630)	grad_norm 0.3724 (0.3918)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:16:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0000	time 0.1600 (0.2315)	loss 1.3211 (1.3630)	grad_norm 0.3738 (0.3925)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:16:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 24 training takes 0:09:43
[2024-07-12 21:17:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 36.944 (36.944)	Loss 0.4150 (0.4150)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 21:17:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.114 Acc@5 97.276
[2024-07-12 21:17:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 21:17:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 21:18:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][0/2502]	eta 12:08:13 lr 0.000004	 wd 0.0000	time 17.4634 (17.4634)	loss 1.4447 (1.4447)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:18:27 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:15:08 lr 0.000004	 wd 0.0000	time 0.2300 (0.3782)	loss 1.2642 (1.3990)	grad_norm 0.3743 (0.3890)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:18:49 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:11:25 lr 0.000004	 wd 0.0000	time 0.3150 (0.2979)	loss 1.4896 (1.3729)	grad_norm 0.3742 (0.3894)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:19:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:11:13 lr 0.000004	 wd 0.0000	time 0.2239 (0.3058)	loss 1.2246 (1.3666)	grad_norm 0.3766 (0.3957)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:19:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:09:51 lr 0.000004	 wd 0.0000	time 0.1960 (0.2816)	loss 1.5139 (1.3616)	grad_norm 0.3895 (0.3918)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:20:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:08:52 lr 0.000004	 wd 0.0000	time 0.1936 (0.2662)	loss 1.4418 (1.3662)	grad_norm 0.3940 (0.3924)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:20:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:08:08 lr 0.000004	 wd 0.0000	time 0.2558 (0.2568)	loss 1.5444 (1.3697)	grad_norm 0.3920 (0.3924)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:20:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:07:41 lr 0.000004	 wd 0.0000	time 0.2257 (0.2564)	loss 1.4126 (1.3706)	grad_norm 0.3797 (0.3918)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:21:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:07:07 lr 0.000004	 wd 0.0000	time 0.2077 (0.2513)	loss 1.5016 (1.3688)	grad_norm 0.3742 (0.3946)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:21:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:06:35 lr 0.000004	 wd 0.0000	time 0.2277 (0.2471)	loss 1.4933 (1.3691)	grad_norm 0.3743 (0.3945)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:21:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:06:05 lr 0.000004	 wd 0.0000	time 0.2004 (0.2435)	loss 1.6857 (1.3674)	grad_norm 0.3602 (0.3941)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:22:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:05:39 lr 0.000004	 wd 0.0000	time 0.2148 (0.2423)	loss 1.5387 (1.3643)	grad_norm 0.4960 (0.3959)	loss_scale 8192.0000 (4162.9646)	mem 6789MB
[2024-07-12 21:22:39 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:05:15 lr 0.000004	 wd 0.0000	time 0.2604 (0.2420)	loss 1.5337 (1.3648)	grad_norm 0.3666 (0.3949)	loss_scale 8192.0000 (4498.4380)	mem 6789MB
[2024-07-12 21:23:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:04:48 lr 0.000003	 wd 0.0000	time 0.2262 (0.2396)	loss 1.2423 (1.3614)	grad_norm 0.3648 (0.3946)	loss_scale 8192.0000 (4782.3397)	mem 6789MB
[2024-07-12 21:23:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:04:21 lr 0.000003	 wd 0.0000	time 0.2277 (0.2375)	loss 0.8554 (1.3635)	grad_norm 0.3816 (0.3944)	loss_scale 8192.0000 (5025.7131)	mem 6789MB
[2024-07-12 21:23:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:03:56 lr 0.000003	 wd 0.0000	time 0.2411 (0.2362)	loss 0.9718 (1.3619)	grad_norm 0.4007 (0.3956)	loss_scale 8192.0000 (5236.6582)	mem 6789MB
[2024-07-12 21:24:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:03:33 lr 0.000003	 wd 0.0000	time 0.2729 (0.2364)	loss 1.1369 (1.3592)	grad_norm 0.3717 (0.3953)	loss_scale 8192.0000 (5421.2517)	mem 6789MB
[2024-07-12 21:24:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:03:08 lr 0.000003	 wd 0.0000	time 0.2005 (0.2354)	loss 1.1588 (1.3602)	grad_norm 0.3820 (0.3950)	loss_scale 8192.0000 (5584.1411)	mem 6789MB
[2024-07-12 21:24:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:02:44 lr 0.000003	 wd 0.0000	time 0.2031 (0.2343)	loss 1.5334 (1.3613)	grad_norm 0.3624 (0.3942)	loss_scale 8192.0000 (5728.9417)	mem 6789MB
[2024-07-12 21:25:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:02:20 lr 0.000003	 wd 0.0000	time 0.1988 (0.2331)	loss 1.5997 (1.3610)	grad_norm 0.4439 (0.3939)	loss_scale 8192.0000 (5858.5082)	mem 6789MB
[2024-07-12 21:25:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:01:56 lr 0.000003	 wd 0.0000	time 0.1984 (0.2330)	loss 1.7538 (1.3616)	grad_norm 0.3570 (0.3936)	loss_scale 8192.0000 (5975.1244)	mem 6789MB
[2024-07-12 21:25:59 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:33 lr 0.000003	 wd 0.0000	time 0.2107 (0.2335)	loss 1.4429 (1.3600)	grad_norm 0.4011 (nan)	loss_scale 4096.0000 (6049.4469)	mem 6789MB
[2024-07-12 21:26:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:10 lr 0.000003	 wd 0.0000	time 0.2238 (0.2326)	loss 1.4722 (1.3585)	grad_norm 0.3812 (nan)	loss_scale 4096.0000 (5960.6942)	mem 6789MB
[2024-07-12 21:26:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:46 lr 0.000003	 wd 0.0000	time 0.2084 (0.2317)	loss 1.4677 (1.3601)	grad_norm 0.3842 (nan)	loss_scale 4096.0000 (5879.6558)	mem 6789MB
[2024-07-12 21:27:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:23 lr 0.000003	 wd 0.0000	time 0.2463 (0.2314)	loss 1.4237 (1.3612)	grad_norm 0.3744 (nan)	loss_scale 4096.0000 (5805.3678)	mem 6789MB
[2024-07-12 21:27:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0000	time 0.1496 (0.2298)	loss 1.6116 (1.3623)	grad_norm 0.3830 (nan)	loss_scale 4096.0000 (5737.0204)	mem 6789MB
[2024-07-12 21:27:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 25 training takes 0:09:42
[2024-07-12 21:27:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 19.297 (19.297)	Loss 0.4148 (0.4148)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 21:28:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.110 Acc@5 97.274
[2024-07-12 21:28:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 21:28:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 21:28:20 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][0/2502]	eta 11:12:33 lr 0.000003	 wd 0.0000	time 16.1287 (16.1287)	loss 1.3919 (1.3919)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:28:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:16:38 lr 0.000003	 wd 0.0000	time 0.3634 (0.4158)	loss 1.2342 (1.3364)	grad_norm 0.3629 (0.3864)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:29:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:12:29 lr 0.000003	 wd 0.0000	time 0.1763 (0.3258)	loss 1.5683 (1.3497)	grad_norm 0.3776 (0.3817)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:29:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:10:31 lr 0.000003	 wd 0.0000	time 0.1876 (0.2870)	loss 1.4113 (1.3569)	grad_norm 0.3746 (0.3845)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:29:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:09:22 lr 0.000003	 wd 0.0000	time 0.1970 (0.2674)	loss 1.6922 (1.3534)	grad_norm 0.3515 (0.3848)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:30:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:08:36 lr 0.000003	 wd 0.0000	time 0.2220 (0.2580)	loss 1.5224 (1.3567)	grad_norm 0.3787 (0.3875)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:30:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:08:21 lr 0.000003	 wd 0.0000	time 0.2019 (0.2635)	loss 1.4739 (1.3501)	grad_norm 0.3909 (0.3899)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:31:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:07:40 lr 0.000003	 wd 0.0000	time 0.1815 (0.2555)	loss 1.5067 (1.3502)	grad_norm 0.3724 (0.3888)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:31:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:07:04 lr 0.000002	 wd 0.0000	time 0.1922 (0.2497)	loss 1.4155 (1.3532)	grad_norm 0.4035 (0.3889)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:31:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:06:34 lr 0.000002	 wd 0.0000	time 0.2337 (0.2460)	loss 1.4358 (1.3486)	grad_norm 0.3957 (0.3927)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:32:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:06:08 lr 0.000002	 wd 0.0000	time 0.2403 (0.2455)	loss 1.4044 (1.3512)	grad_norm 0.3576 (0.3926)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:32:31 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:05:40 lr 0.000002	 wd 0.0000	time 0.1907 (0.2429)	loss 1.5158 (1.3544)	grad_norm 0.3545 (0.3922)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:32:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:05:12 lr 0.000002	 wd 0.0000	time 0.1965 (0.2402)	loss 1.4247 (1.3554)	grad_norm 0.4028 (0.3915)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:33:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:04:45 lr 0.000002	 wd 0.0000	time 0.1933 (0.2376)	loss 0.8452 (1.3542)	grad_norm 0.4193 (0.3928)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:33:36 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:04:21 lr 0.000002	 wd 0.0000	time 0.2303 (0.2371)	loss 1.4110 (1.3550)	grad_norm 0.3801 (0.3941)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:34:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:03:57 lr 0.000002	 wd 0.0000	time 0.2080 (0.2371)	loss 1.6642 (1.3561)	grad_norm 0.3750 (0.3940)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:34:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:03:32 lr 0.000002	 wd 0.0000	time 0.2105 (0.2356)	loss 0.8186 (1.3552)	grad_norm 0.3754 (0.3941)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:34:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:03:07 lr 0.000002	 wd 0.0000	time 0.1811 (0.2342)	loss 1.0040 (1.3561)	grad_norm 0.3667 (0.3938)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:35:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:02:43 lr 0.000002	 wd 0.0000	time 0.2331 (0.2335)	loss 1.5797 (1.3575)	grad_norm 0.3727 (0.3937)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 21:35:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:02:21 lr 0.000002	 wd 0.0000	time 0.2005 (0.2342)	loss 1.3106 (1.3562)	grad_norm 0.3721 (nan)	loss_scale 2048.0000 (4087.3814)	mem 6789MB
[2024-07-12 21:35:51 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:01:57 lr 0.000002	 wd 0.0000	time 0.1901 (0.2333)	loss 1.5976 (1.3559)	grad_norm 0.4026 (nan)	loss_scale 2048.0000 (3985.4633)	mem 6789MB
[2024-07-12 21:36:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:33 lr 0.000002	 wd 0.0000	time 0.1871 (0.2324)	loss 1.1264 (1.3565)	grad_norm 0.4552 (nan)	loss_scale 2048.0000 (3893.2470)	mem 6789MB
[2024-07-12 21:36:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:09 lr 0.000002	 wd 0.0000	time 0.2056 (0.2316)	loss 1.3808 (1.3574)	grad_norm 0.4658 (nan)	loss_scale 2048.0000 (3809.4103)	mem 6789MB
[2024-07-12 21:36:57 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:46 lr 0.000002	 wd 0.0000	time 0.2307 (0.2319)	loss 1.5189 (1.3575)	grad_norm 0.3735 (nan)	loss_scale 2048.0000 (3732.8605)	mem 6789MB
[2024-07-12 21:37:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:23 lr 0.000002	 wd 0.0000	time 0.2134 (0.2314)	loss 1.2863 (1.3579)	grad_norm 0.3784 (nan)	loss_scale 2048.0000 (3662.6872)	mem 6789MB
[2024-07-12 21:37:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0000	time 0.1629 (0.2294)	loss 1.0216 (1.3589)	grad_norm 0.3589 (nan)	loss_scale 2048.0000 (3598.1255)	mem 6789MB
[2024-07-12 21:37:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 26 training takes 0:09:38
[2024-07-12 21:38:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 20.392 (20.392)	Loss 0.4148 (0.4148)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 21:38:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.108 Acc@5 97.276
[2024-07-12 21:38:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 21:38:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 21:38:52 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][0/2502]	eta 1 day, 1:28:54 lr 0.000002	 wd 0.0000	time 36.6644 (36.6644)	loss 1.2840 (1.2840)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:39:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:22:45 lr 0.000002	 wd 0.0000	time 0.2243 (0.5684)	loss 1.2414 (1.3306)	grad_norm 0.3483 (0.3794)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:39:33 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:14:54 lr 0.000002	 wd 0.0000	time 0.2151 (0.3885)	loss 1.3772 (1.3584)	grad_norm 0.3784 (0.3803)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:39:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:12:18 lr 0.000002	 wd 0.0000	time 0.2235 (0.3352)	loss 0.8629 (1.3574)	grad_norm 0.3894 (0.3824)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:40:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:10:59 lr 0.000002	 wd 0.0000	time 0.2075 (0.3139)	loss 1.3493 (1.3609)	grad_norm 0.3986 (0.3858)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:40:42 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:09:46 lr 0.000002	 wd 0.0000	time 0.1836 (0.2928)	loss 1.5090 (1.3577)	grad_norm 0.3713 (0.3868)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:41:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:08:51 lr 0.000002	 wd 0.0000	time 0.2015 (0.2793)	loss 1.5597 (1.3587)	grad_norm 0.3699 (0.3855)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:41:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:08:06 lr 0.000002	 wd 0.0000	time 0.2382 (0.2698)	loss 1.4978 (1.3559)	grad_norm 0.3887 (0.3865)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:41:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:07:35 lr 0.000002	 wd 0.0000	time 0.2043 (0.2678)	loss 1.3320 (1.3561)	grad_norm 0.3579 (0.3872)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:42:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:07:00 lr 0.000001	 wd 0.0000	time 0.2093 (0.2622)	loss 1.4250 (1.3560)	grad_norm 0.3922 (0.3878)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:42:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:06:25 lr 0.000001	 wd 0.0000	time 0.1939 (0.2569)	loss 1.5524 (1.3538)	grad_norm 0.4308 (0.3893)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:42:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:05:54 lr 0.000001	 wd 0.0000	time 0.2215 (0.2526)	loss 1.5606 (1.3516)	grad_norm 0.3602 (0.3888)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:43:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:05:26 lr 0.000001	 wd 0.0000	time 0.2125 (0.2509)	loss 1.2637 (1.3512)	grad_norm 0.3832 (0.3891)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:43:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:05:00 lr 0.000001	 wd 0.0000	time 0.1848 (0.2501)	loss 1.5572 (1.3527)	grad_norm 0.3685 (0.3884)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:44:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:04:32 lr 0.000001	 wd 0.0000	time 0.1927 (0.2473)	loss 1.4280 (1.3547)	grad_norm 0.3985 (0.3877)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:44:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:04:05 lr 0.000001	 wd 0.0000	time 0.1949 (0.2450)	loss 1.2967 (1.3549)	grad_norm 0.3454 (0.3903)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:44:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:03:39 lr 0.000001	 wd 0.0000	time 0.2632 (0.2433)	loss 1.3055 (1.3568)	grad_norm 0.3746 (0.3906)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:45:10 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:03:15 lr 0.000001	 wd 0.0000	time 0.1874 (0.2439)	loss 1.5273 (1.3573)	grad_norm 0.3537 (0.3903)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:45:32 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:02:50 lr 0.000001	 wd 0.0000	time 0.2407 (0.2426)	loss 1.4836 (1.3581)	grad_norm 0.4023 (0.3906)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:45:54 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:02:25 lr 0.000001	 wd 0.0000	time 0.1789 (0.2412)	loss 1.4566 (1.3588)	grad_norm 0.3764 (0.3904)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:46:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:00 lr 0.000001	 wd 0.0000	time 0.2111 (0.2400)	loss 1.1152 (1.3583)	grad_norm 0.3898 (0.3902)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:46:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:36 lr 0.000001	 wd 0.0000	time 0.2351 (0.2402)	loss 1.8865 (1.3612)	grad_norm 0.3686 (0.3910)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:47:02 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:12 lr 0.000001	 wd 0.0000	time 0.2198 (0.2394)	loss 0.8972 (1.3609)	grad_norm 0.3895 (0.3910)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:47:23 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:48 lr 0.000001	 wd 0.0000	time 0.1975 (0.2382)	loss 1.3722 (1.3602)	grad_norm 0.3931 (0.3909)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:47:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:24 lr 0.000001	 wd 0.0000	time 0.1925 (0.2371)	loss 1.4721 (1.3600)	grad_norm 0.3674 (0.3908)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:48:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1565 (0.2350)	loss 1.5191 (1.3596)	grad_norm 0.3846 (0.3917)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:48:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 27 training takes 0:09:52
[2024-07-12 21:48:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 35.363 (35.363)	Loss 0.4148 (0.4148)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 21:48:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.104 Acc@5 97.274
[2024-07-12 21:48:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 21:48:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 21:49:12 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][0/2502]	eta 11:07:34 lr 0.000001	 wd 0.0000	time 16.0091 (16.0091)	loss 1.5922 (1.5922)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:49:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:15:16 lr 0.000001	 wd 0.0000	time 0.2337 (0.3815)	loss 1.3425 (1.3500)	grad_norm 0.3782 (0.3907)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:50:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:12:57 lr 0.000001	 wd 0.0000	time 0.1933 (0.3377)	loss 1.4790 (1.3568)	grad_norm 0.4777 (0.3912)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:50:24 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:10:50 lr 0.000001	 wd 0.0000	time 0.2396 (0.2954)	loss 1.3497 (1.3614)	grad_norm 0.3903 (0.3943)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:50:45 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:09:35 lr 0.000001	 wd 0.0000	time 0.2133 (0.2740)	loss 1.5349 (1.3628)	grad_norm 0.4683 (0.3934)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:51:06 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:08:42 lr 0.000001	 wd 0.0000	time 0.1971 (0.2612)	loss 1.3409 (1.3657)	grad_norm 0.3653 (0.3945)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:51:44 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:08:54 lr 0.000001	 wd 0.0000	time 0.2059 (0.2810)	loss 1.4896 (1.3623)	grad_norm 0.4070 (0.3950)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:52:05 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:08:07 lr 0.000001	 wd 0.0000	time 0.2069 (0.2708)	loss 1.4067 (1.3603)	grad_norm 0.3702 (0.3937)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:52:26 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:07:27 lr 0.000001	 wd 0.0000	time 0.1918 (0.2631)	loss 1.4947 (1.3614)	grad_norm 0.3758 (0.3931)	loss_scale 2048.0000 (2048.0000)	mem 6789MB
[2024-07-12 21:52:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:06:53 lr 0.000001	 wd 0.0000	time 0.2515 (0.2580)	loss 1.4523 (1.3596)	grad_norm 0.4718 (0.3930)	loss_scale 4096.0000 (2075.2764)	mem 6789MB
[2024-07-12 21:53:13 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:06:26 lr 0.000001	 wd 0.0000	time 0.1851 (0.2572)	loss 1.5662 (1.3605)	grad_norm 0.3905 (0.3931)	loss_scale 4096.0000 (2277.1469)	mem 6789MB
[2024-07-12 21:53:35 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:05:55 lr 0.000001	 wd 0.0000	time 0.2014 (0.2535)	loss 1.4511 (1.3613)	grad_norm 0.3885 (0.3928)	loss_scale 4096.0000 (2442.3470)	mem 6789MB
[2024-07-12 21:53:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:05:25 lr 0.000001	 wd 0.0000	time 0.2211 (0.2500)	loss 1.5706 (1.3619)	grad_norm 0.3922 (0.3924)	loss_scale 4096.0000 (2580.0366)	mem 6789MB
[2024-07-12 21:54:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:04:57 lr 0.000001	 wd 0.0000	time 0.2297 (0.2472)	loss 0.9587 (1.3607)	grad_norm 0.4050 (0.3923)	loss_scale 4096.0000 (2696.5596)	mem 6789MB
[2024-07-12 21:54:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:04:31 lr 0.000001	 wd 0.0000	time 0.2838 (0.2462)	loss 0.8607 (1.3603)	grad_norm 0.3599 (0.3920)	loss_scale 4096.0000 (2796.4483)	mem 6789MB
[2024-07-12 21:55:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:04:05 lr 0.000001	 wd 0.0000	time 0.2118 (0.2454)	loss 1.3831 (1.3596)	grad_norm 0.5140 (0.3923)	loss_scale 4096.0000 (2883.0273)	mem 6789MB
[2024-07-12 21:55:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:03:39 lr 0.000001	 wd 0.0000	time 0.2246 (0.2433)	loss 1.4751 (1.3609)	grad_norm 0.4002 (0.3925)	loss_scale 4096.0000 (2958.7908)	mem 6789MB
[2024-07-12 21:55:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:03:13 lr 0.000001	 wd 0.0000	time 0.1972 (0.2415)	loss 1.4719 (1.3604)	grad_norm 0.3769 (0.3925)	loss_scale 4096.0000 (3025.6461)	mem 6789MB
[2024-07-12 21:56:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:02:48 lr 0.000001	 wd 0.0000	time 0.2450 (0.2405)	loss 1.2017 (1.3611)	grad_norm 0.3849 (0.3929)	loss_scale 4096.0000 (3085.0772)	mem 6789MB
[2024-07-12 21:56:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:02:25 lr 0.000001	 wd 0.0000	time 0.2023 (0.2409)	loss 1.6165 (1.3615)	grad_norm 0.3969 (0.3939)	loss_scale 4096.0000 (3138.2557)	mem 6789MB
[2024-07-12 21:56:55 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:00 lr 0.000001	 wd 0.0000	time 0.2035 (0.2396)	loss 1.2594 (1.3608)	grad_norm 0.3495 (0.3939)	loss_scale 4096.0000 (3186.1189)	mem 6789MB
[2024-07-12 21:57:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:35 lr 0.000001	 wd 0.0000	time 0.1981 (0.2384)	loss 1.3706 (1.3610)	grad_norm 0.3737 (0.3942)	loss_scale 4096.0000 (3229.4260)	mem 6789MB
[2024-07-12 21:57:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:11 lr 0.000001	 wd 0.0000	time 0.1949 (0.2374)	loss 1.3726 (1.3622)	grad_norm 0.3570 (0.3938)	loss_scale 4096.0000 (3268.7978)	mem 6789MB
[2024-07-12 21:58:03 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:48 lr 0.000001	 wd 0.0000	time 0.2050 (0.2377)	loss 1.6334 (1.3627)	grad_norm 0.3701 (0.3939)	loss_scale 4096.0000 (3304.7475)	mem 6789MB
[2024-07-12 21:58:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:24 lr 0.000001	 wd 0.0000	time 0.2273 (0.2370)	loss 1.4624 (1.3614)	grad_norm 0.3858 (0.3935)	loss_scale 4096.0000 (3337.7026)	mem 6789MB
[2024-07-12 21:58:43 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0000	time 0.1492 (0.2348)	loss 1.3159 (1.3623)	grad_norm 0.3823 (0.3932)	loss_scale 4096.0000 (3368.0224)	mem 6789MB
[2024-07-12 21:58:47 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 28 training takes 0:09:51
[2024-07-12 21:59:07 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 20.084 (20.084)	Loss 0.4148 (0.4148)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 21:59:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.112 Acc@5 97.270
[2024-07-12 21:59:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 21:59:21 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 21:59:56 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][0/2502]	eta 1 day, 0:32:55 lr 0.000001	 wd 0.0000	time 35.3220 (35.3220)	loss 1.3997 (1.3997)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:00:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:22:18 lr 0.000001	 wd 0.0000	time 0.2279 (0.5572)	loss 1.6012 (1.3735)	grad_norm 0.3999 (0.4138)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:00:37 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:14:39 lr 0.000001	 wd 0.0000	time 0.1927 (0.3819)	loss 0.9666 (1.3770)	grad_norm 0.3751 (0.4049)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:01:04 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:12:32 lr 0.000001	 wd 0.0000	time 0.3948 (0.3419)	loss 1.3715 (1.3734)	grad_norm 0.3784 (0.3966)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:01:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:11:16 lr 0.000001	 wd 0.0000	time 0.2012 (0.3218)	loss 1.3938 (1.3662)	grad_norm 0.3515 (0.3939)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:01:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:09:57 lr 0.000001	 wd 0.0000	time 0.2040 (0.2986)	loss 0.9758 (1.3655)	grad_norm 0.3576 (0.3931)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:02:11 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:09:00 lr 0.000000	 wd 0.0000	time 0.1984 (0.2840)	loss 1.2433 (1.3625)	grad_norm 0.3891 (0.3916)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:02:34 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:08:17 lr 0.000000	 wd 0.0000	time 0.2351 (0.2759)	loss 1.3447 (1.3595)	grad_norm 0.3744 (0.3913)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:02:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:07:40 lr 0.000000	 wd 0.0000	time 0.1817 (0.2707)	loss 1.4512 (1.3684)	grad_norm 0.3959 (0.3914)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:03:19 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:07:03 lr 0.000000	 wd 0.0000	time 0.1831 (0.2646)	loss 1.0304 (1.3690)	grad_norm 0.3537 (0.3903)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:03:40 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:06:29 lr 0.000000	 wd 0.0000	time 0.1832 (0.2590)	loss 1.4335 (1.3672)	grad_norm 0.3884 (0.3912)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:04:01 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:05:57 lr 0.000000	 wd 0.0000	time 0.2030 (0.2547)	loss 1.5418 (1.3714)	grad_norm 0.3816 (0.3914)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:04:25 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:05:29 lr 0.000000	 wd 0.0000	time 0.3236 (0.2531)	loss 1.4321 (1.3703)	grad_norm 0.3889 (0.3921)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:04:48 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:05:02 lr 0.000000	 wd 0.0000	time 0.1997 (0.2516)	loss 1.5971 (1.3713)	grad_norm 0.3724 (0.3920)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:05:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:04:33 lr 0.000000	 wd 0.0000	time 0.2015 (0.2486)	loss 1.6199 (1.3717)	grad_norm 0.3860 (0.3919)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:05:30 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:04:06 lr 0.000000	 wd 0.0000	time 0.1826 (0.2458)	loss 1.4764 (1.3714)	grad_norm 0.3695 (0.3922)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:05:53 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:03:40 lr 0.000000	 wd 0.0000	time 0.2579 (0.2448)	loss 1.3730 (1.3695)	grad_norm 0.3655 (0.3916)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:06:17 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:03:16 lr 0.000000	 wd 0.0000	time 0.2106 (0.2448)	loss 1.5315 (1.3706)	grad_norm 0.3886 (0.3915)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:06:38 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:02:50 lr 0.000000	 wd 0.0000	time 0.1954 (0.2431)	loss 1.4423 (1.3708)	grad_norm 0.3640 (0.3915)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:07:00 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:02:25 lr 0.000000	 wd 0.0000	time 0.1888 (0.2415)	loss 1.3009 (1.3701)	grad_norm 0.9078 (0.3915)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:07:22 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:00 lr 0.000000	 wd 0.0000	time 0.2268 (0.2404)	loss 1.4668 (1.3687)	grad_norm 0.3652 (0.3909)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:07:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:36 lr 0.000000	 wd 0.0000	time 0.1965 (0.2404)	loss 1.4978 (1.3695)	grad_norm 0.3649 (0.3908)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:08:08 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:12 lr 0.000000	 wd 0.0000	time 0.1918 (0.2395)	loss 1.5212 (1.3704)	grad_norm 0.3677 (0.3909)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:08:29 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:48 lr 0.000000	 wd 0.0000	time 0.1999 (0.2384)	loss 1.5421 (1.3704)	grad_norm 0.3949 (0.3912)	loss_scale 4096.0000 (4096.0000)	mem 6789MB
[2024-07-12 22:08:50 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:24 lr 0.000000	 wd 0.0000	time 0.2161 (0.2373)	loss 1.3284 (1.3704)	grad_norm 0.4087 (0.3907)	loss_scale 8192.0000 (4119.8834)	mem 6789MB
[2024-07-12 22:09:09 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0000	time 0.1570 (0.2354)	loss 1.1616 (1.3691)	grad_norm 0.3756 (0.3906)	loss_scale 8192.0000 (4282.7029)	mem 6789MB
[2024-07-12 22:09:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 249): INFO EPOCH 29 training takes 0:09:54
[2024-07-12 22:09:15 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 145): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_29.pth saving......
[2024-07-12 22:09:16 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (utils.py 147): INFO pretrain/vcnu_finetune/adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1/diffusion_ft_adapter_swin_b_22kto1k_step_crosslayer1/ckpt_epoch_29.pth saved !!!
[2024-07-12 22:09:46 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 289): INFO Test: [0/98]	Time 29.225 (29.225)	Loss 0.4155 (0.4155)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 6789MB
[2024-07-12 22:09:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 296): INFO  * Acc@1 84.110 Acc@5 97.268
[2024-07-12 22:09:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.1%
[2024-07-12 22:09:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 182): INFO Max accuracy: 84.15%
[2024-07-12 22:09:58 adapter_swin_diffusion_finetune_base_patch4_window7_224_22kto1k_finetune_step_crosslayer_process1] (main.py 189): INFO Training time 2:17:39
