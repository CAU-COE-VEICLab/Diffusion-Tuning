[2024-08-01 08:57:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/config.json
[2024-08-01 08:57:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_stage2
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: stage2
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_step_stage2
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-01 08:57:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_step_stage2.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_step_stage2", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-01 08:57:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_stage2
[2024-08-01 08:57:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-01 08:57:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 113): INFO number of params: 56035048
[2024-08-01 08:57:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2, ignoring auto resume
[2024-08-01 08:57:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth for fine-tuning......
[2024-08-01 08:57:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-01 08:57:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth'
[2024-08-01 08:57:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 13.992 (13.992)	Loss 0.5010 (0.5010)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 2625MB
[2024-08-01 08:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.894 Acc@5 97.758
[2024-08-01 08:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 08:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 168): INFO Start training
[2024-08-01 08:58:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:03:03 lr 0.000000	 wd 0.0500	time 13.0232 (13.0232)	loss 1.5455 (1.5455)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 19785MB
[2024-08-01 08:59:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:30:38 lr 0.000000	 wd 0.0500	time 0.6047 (0.7652)	loss 1.3226 (1.2085)	grad_norm 1.5793 (nan)	loss_scale 4096.0000 (21169.4257)	mem 19785MB
[2024-08-01 09:00:25 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:27:05 lr 0.000000	 wd 0.0500	time 0.7353 (0.7061)	loss 1.0626 (1.2021)	grad_norm 1.8224 (nan)	loss_scale 2048.0000 (12451.0249)	mem 19785MB
[2024-08-01 09:01:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:25:09 lr 0.000000	 wd 0.0500	time 0.6223 (0.6855)	loss 0.8846 (1.1620)	grad_norm 2.2765 (nan)	loss_scale 2048.0000 (8994.8704)	mem 19785MB
[2024-08-01 09:02:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:23:39 lr 0.000001	 wd 0.0500	time 0.6126 (0.6755)	loss 1.1092 (1.1698)	grad_norm 1.9353 (nan)	loss_scale 2048.0000 (7262.4838)	mem 19785MB
[2024-08-01 09:03:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:22:20 lr 0.000001	 wd 0.0500	time 0.6151 (0.6694)	loss 1.1885 (1.1719)	grad_norm 1.5001 (nan)	loss_scale 2048.0000 (6221.6687)	mem 19785MB
[2024-08-01 09:04:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:21:04 lr 0.000001	 wd 0.0500	time 0.5999 (0.6651)	loss 1.3015 (1.1701)	grad_norm 2.4234 (nan)	loss_scale 2048.0000 (5527.2146)	mem 19785MB
[2024-08-01 09:05:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:19:52 lr 0.000001	 wd 0.0500	time 0.6160 (0.6619)	loss 1.3118 (1.1727)	grad_norm 2.0824 (nan)	loss_scale 2048.0000 (5030.8930)	mem 19785MB
[2024-08-01 09:06:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:18:42 lr 0.000001	 wd 0.0500	time 0.5959 (0.6596)	loss 1.1013 (1.1710)	grad_norm 1.7302 (nan)	loss_scale 2048.0000 (4658.4969)	mem 19785MB
[2024-08-01 09:07:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:17:34 lr 0.000001	 wd 0.0500	time 0.6143 (0.6581)	loss 1.3432 (1.1713)	grad_norm 5.6831 (nan)	loss_scale 2048.0000 (4368.7636)	mem 19785MB
[2024-08-01 09:09:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:16:26 lr 0.000002	 wd 0.0500	time 0.6196 (0.6566)	loss 1.4454 (1.1689)	grad_norm 1.5825 (nan)	loss_scale 2048.0000 (4136.9191)	mem 19785MB
[2024-08-01 09:10:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:15:19 lr 0.000002	 wd 0.0500	time 0.6114 (0.6555)	loss 1.3471 (1.1704)	grad_norm 1.8892 (nan)	loss_scale 2048.0000 (3947.1898)	mem 19785MB
[2024-08-01 09:11:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:14:12 lr 0.000002	 wd 0.0500	time 0.6127 (0.6550)	loss 1.0144 (1.1699)	grad_norm 2.2222 (nan)	loss_scale 2048.0000 (3789.0558)	mem 19785MB
[2024-08-01 09:12:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:13:06 lr 0.000002	 wd 0.0500	time 0.6142 (0.6543)	loss 1.2119 (1.1715)	grad_norm 1.9796 (nan)	loss_scale 2048.0000 (3655.2314)	mem 19785MB
[2024-08-01 09:13:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:12:00 lr 0.000002	 wd 0.0500	time 0.6340 (0.6537)	loss 1.4520 (1.1725)	grad_norm 1.9184 (nan)	loss_scale 2048.0000 (3540.5111)	mem 19785MB
[2024-08-01 09:14:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:10:54 lr 0.000002	 wd 0.0500	time 0.6112 (0.6531)	loss 1.0358 (1.1757)	grad_norm 1.7913 (nan)	loss_scale 2048.0000 (3441.0766)	mem 19785MB
[2024-08-01 09:15:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:09:48 lr 0.000003	 wd 0.0500	time 0.6193 (0.6526)	loss 1.2389 (1.1730)	grad_norm 1.6305 (nan)	loss_scale 2048.0000 (3354.0637)	mem 19785MB
[2024-08-01 09:16:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:08:42 lr 0.000003	 wd 0.0500	time 0.6220 (0.6521)	loss 1.0104 (1.1725)	grad_norm 2.0044 (nan)	loss_scale 2048.0000 (3277.2816)	mem 19785MB
[2024-08-01 09:17:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:07:37 lr 0.000003	 wd 0.0500	time 0.6130 (0.6517)	loss 1.0903 (1.1723)	grad_norm 3.2965 (nan)	loss_scale 2048.0000 (3209.0261)	mem 19785MB
[2024-08-01 09:18:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:06:32 lr 0.000003	 wd 0.0500	time 0.6153 (0.6513)	loss 1.5139 (1.1720)	grad_norm 1.6894 (nan)	loss_scale 2048.0000 (3147.9516)	mem 19785MB
[2024-08-01 09:19:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:05:26 lr 0.000003	 wd 0.0500	time 0.5800 (0.6510)	loss 0.8428 (1.1710)	grad_norm 1.8805 (nan)	loss_scale 2048.0000 (3092.9815)	mem 19785MB
[2024-08-01 09:20:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:04:21 lr 0.000003	 wd 0.0500	time 0.6237 (0.6507)	loss 1.0332 (1.1698)	grad_norm 1.9268 (nan)	loss_scale 2048.0000 (3043.2442)	mem 19785MB
[2024-08-01 09:21:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:03:16 lr 0.000004	 wd 0.0500	time 0.6092 (0.6505)	loss 1.5329 (1.1689)	grad_norm 1.4488 (nan)	loss_scale 2048.0000 (2998.0264)	mem 19785MB
[2024-08-01 09:23:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:02:11 lr 0.000004	 wd 0.0500	time 0.6340 (0.6503)	loss 1.4057 (1.1688)	grad_norm 1.7217 (nan)	loss_scale 2048.0000 (2956.7388)	mem 19785MB
[2024-08-01 09:24:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:06 lr 0.000004	 wd 0.0500	time 0.6314 (0.6501)	loss 1.3507 (1.1685)	grad_norm 1.8064 (nan)	loss_scale 2048.0000 (2918.8905)	mem 19785MB
[2024-08-01 09:25:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.6111 (0.6497)	loss 1.0129 (1.1668)	grad_norm 1.7834 (nan)	loss_scale 2048.0000 (2884.0688)	mem 19785MB
[2024-08-01 09:25:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 0 training takes 0:27:08
[2024-08-01 09:25:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_0.pth saving......
[2024-08-01 09:25:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_0.pth saved !!!
[2024-08-01 09:25:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 10.407 (10.407)	Loss 0.5171 (0.5171)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 09:25:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.052 Acc@5 97.858
[2024-08-01 09:25:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 09:25:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.05%
[2024-08-01 09:25:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 09:25:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 09:25:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:48:39 lr 0.000004	 wd 0.0500	time 11.2388 (11.2388)	loss 1.3377 (1.3377)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:27:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:30:03 lr 0.000004	 wd 0.0500	time 0.6176 (0.7507)	loss 1.2685 (1.1514)	grad_norm 2.4365 (3.0514)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:28:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:26:47 lr 0.000004	 wd 0.0500	time 0.6038 (0.6982)	loss 0.8083 (1.1533)	grad_norm 2.5980 (2.7874)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:29:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:24:57 lr 0.000004	 wd 0.0500	time 0.5992 (0.6801)	loss 0.9127 (1.1579)	grad_norm 2.0683 (2.7018)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:30:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:23:31 lr 0.000005	 wd 0.0500	time 0.5949 (0.6713)	loss 0.9328 (1.1733)	grad_norm 1.7397 (2.6332)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:31:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:22:14 lr 0.000005	 wd 0.0500	time 0.6363 (0.6665)	loss 1.5310 (1.1760)	grad_norm 2.0720 (2.6941)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:32:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:21:01 lr 0.000005	 wd 0.0500	time 0.6154 (0.6632)	loss 1.4541 (1.1755)	grad_norm 2.2410 (2.6293)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:33:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:19:50 lr 0.000005	 wd 0.0500	time 0.6056 (0.6607)	loss 0.8574 (1.1720)	grad_norm 2.9891 (2.5423)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:34:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:18:40 lr 0.000005	 wd 0.0500	time 0.6141 (0.6586)	loss 1.5707 (1.1715)	grad_norm 1.9115 (2.5038)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:35:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:17:32 lr 0.000005	 wd 0.0500	time 0.6151 (0.6569)	loss 1.1598 (1.1695)	grad_norm 2.5085 (2.4616)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:36:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:16:24 lr 0.000006	 wd 0.0500	time 0.6149 (0.6558)	loss 1.4625 (1.1661)	grad_norm 2.0159 (2.4331)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:37:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:15:18 lr 0.000006	 wd 0.0500	time 0.6174 (0.6548)	loss 1.5181 (1.1647)	grad_norm 1.6535 (2.4239)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:38:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:14:11 lr 0.000006	 wd 0.0500	time 0.6450 (0.6539)	loss 0.9230 (1.1630)	grad_norm 1.8234 (2.4329)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:39:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:13:05 lr 0.000006	 wd 0.0500	time 0.6132 (0.6532)	loss 1.2380 (1.1642)	grad_norm 4.8818 (2.4406)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:41:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:11:59 lr 0.000006	 wd 0.0500	time 0.6000 (0.6526)	loss 0.8092 (1.1641)	grad_norm 2.0213 (2.4328)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:42:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:10:53 lr 0.000006	 wd 0.0500	time 0.6139 (0.6523)	loss 1.4668 (1.1642)	grad_norm 1.6386 (2.4229)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:43:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:09:47 lr 0.000007	 wd 0.0500	time 0.6122 (0.6518)	loss 1.3516 (1.1652)	grad_norm 2.2077 (2.4108)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 09:44:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:08:42 lr 0.000007	 wd 0.0500	time 0.6130 (0.6514)	loss 0.8375 (1.1659)	grad_norm 2.2711 (2.3912)	loss_scale 4096.0000 (2076.8959)	mem 19785MB
[2024-08-01 09:45:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:07:37 lr 0.000007	 wd 0.0500	time 0.6017 (0.6510)	loss 1.6464 (1.1651)	grad_norm 1.7681 (2.3836)	loss_scale 4096.0000 (2189.0061)	mem 19785MB
[2024-08-01 09:46:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:06:31 lr 0.000007	 wd 0.0500	time 0.6130 (0.6507)	loss 1.4830 (1.1669)	grad_norm 1.6747 (2.4205)	loss_scale 4096.0000 (2289.3214)	mem 19785MB
[2024-08-01 09:47:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:05:26 lr 0.000007	 wd 0.0500	time 0.6118 (0.6505)	loss 1.2720 (1.1674)	grad_norm 2.8688 (2.4277)	loss_scale 4096.0000 (2379.6102)	mem 19785MB
[2024-08-01 09:48:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:04:21 lr 0.000007	 wd 0.0500	time 0.6190 (0.6502)	loss 1.3929 (1.1695)	grad_norm 2.7753 (2.4414)	loss_scale 4096.0000 (2461.3041)	mem 19785MB
[2024-08-01 09:49:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:03:16 lr 0.000008	 wd 0.0500	time 0.6485 (0.6499)	loss 1.1391 (1.1699)	grad_norm 2.6533 (2.4322)	loss_scale 4096.0000 (2535.5747)	mem 19785MB
[2024-08-01 09:50:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:02:11 lr 0.000008	 wd 0.0500	time 0.6408 (0.6497)	loss 0.8182 (1.1714)	grad_norm 1.7395 (2.4233)	loss_scale 4096.0000 (2603.3898)	mem 19785MB
[2024-08-01 09:51:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:06 lr 0.000008	 wd 0.0500	time 0.6122 (0.6495)	loss 1.2177 (1.1713)	grad_norm 1.8271 (2.4111)	loss_scale 4096.0000 (2665.5560)	mem 19785MB
[2024-08-01 09:52:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.6090 (0.6493)	loss 0.9304 (1.1712)	grad_norm 1.5359 (2.4025)	loss_scale 4096.0000 (2722.7509)	mem 19785MB
[2024-08-01 09:52:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 1 training takes 0:27:06
[2024-08-01 09:53:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.137 (12.137)	Loss 0.5142 (0.5142)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 09:53:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.112 Acc@5 97.884
[2024-08-01 09:53:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 09:53:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.11%
[2024-08-01 09:53:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 09:53:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 09:53:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:46:08 lr 0.000008	 wd 0.0500	time 11.1784 (11.1784)	loss 1.1487 (1.1487)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 09:54:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:29:55 lr 0.000008	 wd 0.0500	time 0.6191 (0.7475)	loss 1.4693 (1.1715)	grad_norm 3.4444 (2.2584)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 09:55:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:26:42 lr 0.000008	 wd 0.0500	time 0.6173 (0.6963)	loss 1.2796 (1.1618)	grad_norm 1.7701 (nan)	loss_scale 2048.0000 (3668.0597)	mem 19785MB
[2024-08-01 09:56:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:24:55 lr 0.000008	 wd 0.0500	time 0.6301 (0.6793)	loss 1.2403 (1.1508)	grad_norm 2.0119 (nan)	loss_scale 2048.0000 (3129.8339)	mem 19785MB
[2024-08-01 09:57:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:23:30 lr 0.000009	 wd 0.0500	time 0.6183 (0.6709)	loss 1.3831 (1.1550)	grad_norm 2.1320 (nan)	loss_scale 2048.0000 (2860.0499)	mem 19785MB
[2024-08-01 09:59:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:22:12 lr 0.000009	 wd 0.0500	time 0.6155 (0.6657)	loss 1.3454 (1.1531)	grad_norm 1.7670 (nan)	loss_scale 2048.0000 (2697.9641)	mem 19785MB
[2024-08-01 10:00:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:20:59 lr 0.000009	 wd 0.0500	time 0.6108 (0.6623)	loss 1.2531 (1.1522)	grad_norm 1.9744 (nan)	loss_scale 2048.0000 (2589.8170)	mem 19785MB
[2024-08-01 10:01:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:19:49 lr 0.000009	 wd 0.0500	time 0.6309 (0.6599)	loss 1.3371 (1.1565)	grad_norm 2.4237 (nan)	loss_scale 2048.0000 (2512.5250)	mem 19785MB
[2024-08-01 10:02:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:18:39 lr 0.000009	 wd 0.0500	time 0.6110 (0.6580)	loss 1.2323 (1.1595)	grad_norm 1.5332 (nan)	loss_scale 2048.0000 (2454.5318)	mem 19785MB
[2024-08-01 10:03:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:17:32 lr 0.000009	 wd 0.0500	time 0.6117 (0.6571)	loss 0.8395 (1.1634)	grad_norm 2.8072 (nan)	loss_scale 2048.0000 (2409.4118)	mem 19785MB
[2024-08-01 10:04:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:16:25 lr 0.000010	 wd 0.0500	time 0.6148 (0.6560)	loss 0.8787 (1.1609)	grad_norm 1.6868 (nan)	loss_scale 2048.0000 (2373.3067)	mem 19785MB
[2024-08-01 10:05:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:15:18 lr 0.000010	 wd 0.0500	time 0.6171 (0.6550)	loss 1.3982 (1.1597)	grad_norm 6.7014 (nan)	loss_scale 2048.0000 (2343.7602)	mem 19785MB
[2024-08-01 10:06:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:14:11 lr 0.000010	 wd 0.0500	time 0.6129 (0.6543)	loss 1.0061 (1.1603)	grad_norm 2.1147 (nan)	loss_scale 2048.0000 (2319.1341)	mem 19785MB
[2024-08-01 10:07:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:13:05 lr 0.000010	 wd 0.0500	time 0.6125 (0.6536)	loss 1.4283 (1.1609)	grad_norm 2.4277 (nan)	loss_scale 2048.0000 (2298.2936)	mem 19785MB
[2024-08-01 10:08:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:11:59 lr 0.000010	 wd 0.0500	time 0.6109 (0.6531)	loss 1.2058 (1.1621)	grad_norm 1.8516 (nan)	loss_scale 2048.0000 (2280.4283)	mem 19785MB
[2024-08-01 10:09:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:10:53 lr 0.000010	 wd 0.0500	time 0.6166 (0.6525)	loss 1.3926 (1.1627)	grad_norm 2.6298 (nan)	loss_scale 2048.0000 (2264.9434)	mem 19785MB
[2024-08-01 10:10:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:09:48 lr 0.000011	 wd 0.0500	time 0.6031 (0.6519)	loss 0.9504 (1.1628)	grad_norm 1.5116 (nan)	loss_scale 2048.0000 (2251.3929)	mem 19785MB
[2024-08-01 10:11:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:08:42 lr 0.000011	 wd 0.0500	time 0.6077 (0.6515)	loss 1.2353 (1.1628)	grad_norm 1.9295 (nan)	loss_scale 2048.0000 (2239.4356)	mem 19785MB
[2024-08-01 10:13:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:07:37 lr 0.000011	 wd 0.0500	time 0.6609 (0.6512)	loss 1.1431 (1.1639)	grad_norm 1.7527 (nan)	loss_scale 2048.0000 (2228.8062)	mem 19785MB
[2024-08-01 10:14:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:06:31 lr 0.000011	 wd 0.0500	time 0.6141 (0.6510)	loss 1.3530 (1.1635)	grad_norm 1.9838 (nan)	loss_scale 2048.0000 (2219.2951)	mem 19785MB
[2024-08-01 10:15:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:05:26 lr 0.000011	 wd 0.0500	time 0.6066 (0.6506)	loss 0.9967 (1.1638)	grad_norm 2.1482 (nan)	loss_scale 2048.0000 (2210.7346)	mem 19785MB
[2024-08-01 10:16:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:04:21 lr 0.000011	 wd 0.0500	time 0.6079 (0.6504)	loss 1.0045 (1.1630)	grad_norm 1.7648 (nan)	loss_scale 2048.0000 (2202.9891)	mem 19785MB
[2024-08-01 10:17:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:03:16 lr 0.000012	 wd 0.0500	time 0.6112 (0.6502)	loss 1.2840 (1.1634)	grad_norm 1.6032 (nan)	loss_scale 2048.0000 (2195.9473)	mem 19785MB
[2024-08-01 10:18:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:02:11 lr 0.000012	 wd 0.0500	time 0.6400 (0.6499)	loss 1.1110 (1.1632)	grad_norm 1.6764 (nan)	loss_scale 2048.0000 (2189.5176)	mem 19785MB
[2024-08-01 10:19:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:06 lr 0.000012	 wd 0.0500	time 0.6021 (0.6497)	loss 0.8002 (1.1638)	grad_norm 2.1860 (nan)	loss_scale 2048.0000 (2183.6235)	mem 19785MB
[2024-08-01 10:20:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.6176 (0.6495)	loss 1.2094 (1.1636)	grad_norm 1.9488 (nan)	loss_scale 2048.0000 (2178.2007)	mem 19785MB
[2024-08-01 10:20:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 2 training takes 0:27:07
[2024-08-01 10:20:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.887 (11.887)	Loss 0.4941 (0.4941)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 10:21:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.144 Acc@5 97.916
[2024-08-01 10:21:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 10:21:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.14%
[2024-08-01 10:21:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 10:21:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 10:21:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:37:14 lr 0.000012	 wd 0.0500	time 10.9649 (10.9649)	loss 0.6861 (0.6861)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:22:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:29:49 lr 0.000012	 wd 0.0500	time 0.6130 (0.7449)	loss 1.2829 (1.2009)	grad_norm 2.0280 (2.2697)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:23:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:26:43 lr 0.000012	 wd 0.0500	time 0.6826 (0.6966)	loss 1.4878 (1.1751)	grad_norm 1.7185 (2.2523)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:24:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:24:58 lr 0.000012	 wd 0.0500	time 0.6119 (0.6804)	loss 1.4457 (1.1750)	grad_norm 1.8457 (2.2275)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:25:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:23:32 lr 0.000013	 wd 0.0500	time 0.6357 (0.6718)	loss 1.4031 (1.1661)	grad_norm 2.1942 (2.2346)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:26:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:22:14 lr 0.000013	 wd 0.0500	time 0.6184 (0.6666)	loss 0.8106 (1.1660)	grad_norm 2.0009 (2.2144)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:27:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:21:00 lr 0.000013	 wd 0.0500	time 0.6059 (0.6628)	loss 1.0737 (1.1584)	grad_norm 2.1413 (2.2374)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:28:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:19:49 lr 0.000013	 wd 0.0500	time 0.6166 (0.6603)	loss 1.3840 (1.1564)	grad_norm 2.0963 (2.2275)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:29:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:18:40 lr 0.000013	 wd 0.0500	time 0.6347 (0.6585)	loss 0.9842 (1.1513)	grad_norm 1.8189 (2.2974)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:31:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:17:32 lr 0.000013	 wd 0.0500	time 0.6117 (0.6570)	loss 1.5812 (1.1539)	grad_norm 2.0821 (2.2791)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:32:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:16:25 lr 0.000014	 wd 0.0500	time 0.6070 (0.6559)	loss 1.2773 (1.1539)	grad_norm 1.5722 (2.2708)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:33:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:15:18 lr 0.000014	 wd 0.0500	time 0.6033 (0.6550)	loss 0.8154 (1.1549)	grad_norm 2.0955 (2.2543)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:34:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:14:11 lr 0.000014	 wd 0.0500	time 0.6161 (0.6544)	loss 1.1685 (1.1529)	grad_norm 1.7475 (2.2525)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:35:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:13:05 lr 0.000014	 wd 0.0500	time 0.6144 (0.6537)	loss 1.2550 (1.1527)	grad_norm 5.9037 (2.2644)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:36:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:11:59 lr 0.000014	 wd 0.0500	time 0.6025 (0.6530)	loss 1.1924 (1.1534)	grad_norm 1.4276 (2.3029)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:37:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:10:53 lr 0.000014	 wd 0.0500	time 0.6130 (0.6524)	loss 1.4159 (1.1519)	grad_norm 1.7235 (2.3108)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:38:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:09:48 lr 0.000015	 wd 0.0500	time 0.6182 (0.6519)	loss 0.7148 (1.1493)	grad_norm 1.9747 (2.3056)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 10:39:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:08:42 lr 0.000015	 wd 0.0500	time 0.6221 (0.6515)	loss 1.0011 (1.1520)	grad_norm 1.7613 (2.3190)	loss_scale 4096.0000 (2100.9759)	mem 19785MB
[2024-08-01 10:40:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:07:37 lr 0.000015	 wd 0.0500	time 0.6123 (0.6511)	loss 1.2544 (1.1533)	grad_norm 1.6173 (2.3081)	loss_scale 4096.0000 (2211.7490)	mem 19785MB
[2024-08-01 10:41:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:06:31 lr 0.000015	 wd 0.0500	time 0.6177 (0.6507)	loss 1.3741 (1.1519)	grad_norm 1.6273 (2.3062)	loss_scale 4096.0000 (2310.8680)	mem 19785MB
[2024-08-01 10:42:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:05:26 lr 0.000015	 wd 0.0500	time 0.6136 (0.6504)	loss 1.2285 (1.1515)	grad_norm 1.7519 (2.3173)	loss_scale 4096.0000 (2400.0800)	mem 19785MB
[2024-08-01 10:43:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:04:21 lr 0.000015	 wd 0.0500	time 0.6156 (0.6501)	loss 0.8194 (1.1515)	grad_norm 2.4566 (2.3070)	loss_scale 4096.0000 (2480.7996)	mem 19785MB
[2024-08-01 10:45:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:03:16 lr 0.000016	 wd 0.0500	time 0.6046 (0.6501)	loss 0.7958 (1.1507)	grad_norm 1.5419 (2.2972)	loss_scale 4096.0000 (2554.1845)	mem 19785MB
[2024-08-01 10:46:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:02:11 lr 0.000016	 wd 0.0500	time 0.6163 (0.6499)	loss 0.9120 (1.1512)	grad_norm 3.0661 (2.2961)	loss_scale 4096.0000 (2621.1908)	mem 19785MB
[2024-08-01 10:47:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:06 lr 0.000016	 wd 0.0500	time 0.6557 (0.6498)	loss 1.2670 (1.1524)	grad_norm 16.8648 (2.3029)	loss_scale 4096.0000 (2682.6156)	mem 19785MB
[2024-08-01 10:48:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.6340 (0.6495)	loss 1.2283 (1.1525)	grad_norm 1.7178 (2.3036)	loss_scale 4096.0000 (2739.1283)	mem 19785MB
[2024-08-01 10:48:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 3 training takes 0:27:07
[2024-08-01 10:48:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.570 (12.570)	Loss 0.5024 (0.5024)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 10:48:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.210 Acc@5 97.920
[2024-08-01 10:48:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 10:48:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.21%
[2024-08-01 10:48:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 10:48:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 10:49:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:46:52 lr 0.000016	 wd 0.0500	time 11.1958 (11.1958)	loss 1.2476 (1.2476)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:50:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:29:58 lr 0.000016	 wd 0.0500	time 0.6143 (0.7487)	loss 0.9316 (1.1647)	grad_norm 2.5427 (2.1008)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:51:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:26:42 lr 0.000016	 wd 0.0500	time 0.6076 (0.6961)	loss 1.1082 (1.1550)	grad_norm 1.7878 (2.1756)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:52:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:24:55 lr 0.000016	 wd 0.0500	time 0.6163 (0.6793)	loss 0.7649 (1.1524)	grad_norm 3.6082 (2.2444)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:53:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:23:30 lr 0.000017	 wd 0.0500	time 0.6133 (0.6711)	loss 1.3667 (1.1499)	grad_norm 1.5714 (2.2340)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:54:25 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:22:13 lr 0.000017	 wd 0.0500	time 0.6217 (0.6659)	loss 1.2024 (1.1499)	grad_norm 2.5471 (2.2421)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:55:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:21:01 lr 0.000017	 wd 0.0500	time 0.5789 (0.6632)	loss 0.9882 (1.1510)	grad_norm 2.2783 (2.2690)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:56:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:19:50 lr 0.000017	 wd 0.0500	time 0.5989 (0.6605)	loss 0.9867 (1.1517)	grad_norm 3.3063 (2.2796)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:57:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:18:40 lr 0.000017	 wd 0.0500	time 0.6334 (0.6585)	loss 0.7483 (1.1537)	grad_norm 1.7769 (2.2673)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:58:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:17:32 lr 0.000017	 wd 0.0500	time 0.6138 (0.6569)	loss 0.8599 (1.1544)	grad_norm 1.9349 (2.2654)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 10:59:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:16:24 lr 0.000018	 wd 0.0500	time 0.6129 (0.6555)	loss 1.4486 (1.1553)	grad_norm 2.5944 (2.2566)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 11:00:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:15:17 lr 0.000018	 wd 0.0500	time 0.5972 (0.6544)	loss 1.4386 (1.1566)	grad_norm 1.7354 (2.2863)	loss_scale 4096.0000 (4096.0000)	mem 19785MB
[2024-08-01 11:01:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:14:10 lr 0.000018	 wd 0.0500	time 0.6117 (0.6534)	loss 1.2642 (1.1546)	grad_norm nan (nan)	loss_scale 2048.0000 (4092.5895)	mem 19785MB
[2024-08-01 11:03:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:13:04 lr 0.000018	 wd 0.0500	time 0.6153 (0.6528)	loss 0.9393 (1.1539)	grad_norm 2.0142 (nan)	loss_scale 2048.0000 (3935.4343)	mem 19785MB
[2024-08-01 11:04:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:11:58 lr 0.000018	 wd 0.0500	time 0.6121 (0.6522)	loss 1.3960 (1.1542)	grad_norm 2.3557 (nan)	loss_scale 2048.0000 (3800.7138)	mem 19785MB
[2024-08-01 11:05:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:10:53 lr 0.000018	 wd 0.0500	time 0.6139 (0.6518)	loss 1.1652 (1.1541)	grad_norm 1.8575 (nan)	loss_scale 2048.0000 (3683.9440)	mem 19785MB
[2024-08-01 11:06:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:09:47 lr 0.000019	 wd 0.0500	time 0.6163 (0.6515)	loss 1.3240 (1.1524)	grad_norm 1.5691 (nan)	loss_scale 2048.0000 (3581.7614)	mem 19785MB
[2024-08-01 11:07:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:08:42 lr 0.000019	 wd 0.0500	time 0.6462 (0.6511)	loss 1.1394 (1.1525)	grad_norm 2.4156 (nan)	loss_scale 2048.0000 (3491.5932)	mem 19785MB
[2024-08-01 11:08:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:07:36 lr 0.000019	 wd 0.0500	time 0.6144 (0.6508)	loss 1.4979 (1.1541)	grad_norm 2.1634 (nan)	loss_scale 2048.0000 (3411.4381)	mem 19785MB
[2024-08-01 11:09:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:06:31 lr 0.000019	 wd 0.0500	time 0.6100 (0.6505)	loss 1.3443 (1.1542)	grad_norm 2.4013 (nan)	loss_scale 2048.0000 (3339.7159)	mem 19785MB
[2024-08-01 11:10:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:05:26 lr 0.000019	 wd 0.0500	time 0.6117 (0.6502)	loss 0.7948 (1.1533)	grad_norm 1.7059 (nan)	loss_scale 2048.0000 (3275.1624)	mem 19785MB
[2024-08-01 11:11:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:04:21 lr 0.000019	 wd 0.0500	time 0.6286 (0.6500)	loss 0.9179 (1.1520)	grad_norm 1.5825 (nan)	loss_scale 2048.0000 (3216.7539)	mem 19785MB
[2024-08-01 11:12:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:03:16 lr 0.000020	 wd 0.0500	time 0.6115 (0.6498)	loss 0.9028 (1.1522)	grad_norm 2.3226 (nan)	loss_scale 2048.0000 (3163.6529)	mem 19785MB
[2024-08-01 11:13:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:02:11 lr 0.000020	 wd 0.0500	time 0.6226 (0.6495)	loss 0.7986 (1.1524)	grad_norm 1.9387 (nan)	loss_scale 1024.0000 (3071.5550)	mem 19785MB
[2024-08-01 11:14:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:06 lr 0.000020	 wd 0.0500	time 0.6131 (0.6494)	loss 0.7534 (1.1517)	grad_norm 1.8673 (nan)	loss_scale 1024.0000 (2986.2757)	mem 19785MB
[2024-08-01 11:15:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.6190 (0.6493)	loss 1.0995 (1.1521)	grad_norm 2.3852 (nan)	loss_scale 1024.0000 (2907.8161)	mem 19785MB
[2024-08-01 11:15:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 4 training takes 0:27:06
[2024-08-01 11:16:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.170 (12.170)	Loss 0.4941 (0.4941)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 11:16:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.392 Acc@5 97.952
[2024-08-01 11:16:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-08-01 11:16:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.39%
[2024-08-01 11:16:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 11:16:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 11:16:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:45:55 lr 0.000020	 wd 0.0500	time 11.1733 (11.1733)	loss 1.3712 (1.3712)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:17:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:29:55 lr 0.000020	 wd 0.0500	time 0.6118 (0.7474)	loss 0.9896 (1.1774)	grad_norm 4.3164 (2.8716)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:18:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:26:42 lr 0.000020	 wd 0.0500	time 0.6128 (0.6960)	loss 1.1836 (1.1443)	grad_norm 1.7270 (2.6223)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:19:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:24:54 lr 0.000020	 wd 0.0500	time 0.6118 (0.6788)	loss 0.8202 (1.1387)	grad_norm 2.0433 (2.4485)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:21:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:23:28 lr 0.000020	 wd 0.0500	time 0.6063 (0.6703)	loss 1.4856 (1.1464)	grad_norm 1.9260 (2.4067)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:22:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:22:11 lr 0.000020	 wd 0.0500	time 0.6181 (0.6650)	loss 0.9963 (1.1444)	grad_norm 1.9624 (2.4594)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:23:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:20:58 lr 0.000020	 wd 0.0500	time 0.6033 (0.6615)	loss 0.8859 (1.1452)	grad_norm 3.6034 (2.4112)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:24:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:19:47 lr 0.000020	 wd 0.0500	time 0.6032 (0.6588)	loss 0.9086 (1.1453)	grad_norm 1.9047 (2.3734)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:25:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:18:38 lr 0.000020	 wd 0.0500	time 0.6168 (0.6571)	loss 1.4670 (1.1433)	grad_norm 2.2141 (2.3256)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:26:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:17:31 lr 0.000020	 wd 0.0500	time 0.6032 (0.6563)	loss 1.0070 (1.1424)	grad_norm 1.8054 (2.3023)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:27:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:16:23 lr 0.000020	 wd 0.0500	time 0.6114 (0.6551)	loss 1.1236 (1.1413)	grad_norm 2.3229 (2.2935)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:28:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:15:16 lr 0.000020	 wd 0.0500	time 0.6117 (0.6540)	loss 1.1818 (1.1429)	grad_norm 2.2111 (2.2933)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:29:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:14:10 lr 0.000020	 wd 0.0500	time 0.6063 (0.6532)	loss 1.4727 (1.1420)	grad_norm 3.9379 (2.3370)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:30:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:13:04 lr 0.000020	 wd 0.0500	time 0.6162 (0.6526)	loss 0.8052 (1.1431)	grad_norm 2.2018 (2.3296)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:31:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:11:58 lr 0.000020	 wd 0.0500	time 0.6111 (0.6520)	loss 1.2754 (1.1467)	grad_norm 1.9948 (2.3347)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:32:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:10:52 lr 0.000020	 wd 0.0500	time 0.6147 (0.6515)	loss 0.7687 (1.1484)	grad_norm 1.9433 (2.3365)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:33:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:09:47 lr 0.000020	 wd 0.0500	time 0.6179 (0.6512)	loss 1.3395 (1.1515)	grad_norm 2.1180 (2.3844)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:35:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:08:41 lr 0.000020	 wd 0.0500	time 0.6062 (0.6509)	loss 0.8045 (1.1518)	grad_norm 1.6680 (2.3772)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:36:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:07:36 lr 0.000020	 wd 0.0500	time 0.6151 (0.6505)	loss 0.8262 (1.1493)	grad_norm 1.6560 (2.4514)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:37:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:06:31 lr 0.000020	 wd 0.0500	time 0.6356 (0.6503)	loss 1.1339 (1.1488)	grad_norm 1.8596 (2.4553)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:38:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:05:26 lr 0.000020	 wd 0.0500	time 0.5997 (0.6499)	loss 1.5375 (1.1498)	grad_norm 2.3464 (2.4584)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:39:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:04:21 lr 0.000020	 wd 0.0500	time 0.6159 (0.6496)	loss 1.5337 (1.1498)	grad_norm 1.9561 (2.4557)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:40:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:03:16 lr 0.000020	 wd 0.0500	time 0.6139 (0.6494)	loss 1.2681 (1.1502)	grad_norm 1.6540 (2.4406)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:41:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:02:11 lr 0.000020	 wd 0.0500	time 0.5817 (0.6492)	loss 1.1531 (1.1490)	grad_norm 2.1829 (2.4291)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:42:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:06 lr 0.000020	 wd 0.0500	time 0.5764 (0.6490)	loss 0.9062 (1.1485)	grad_norm 1.5390 (2.4213)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:43:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.6207 (0.6488)	loss 1.3662 (1.1494)	grad_norm 2.7438 (2.4173)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:43:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 5 training takes 0:27:05
[2024-08-01 11:43:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.344 (12.344)	Loss 0.4978 (0.4978)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 11:44:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.364 Acc@5 97.942
[2024-08-01 11:44:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-08-01 11:44:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.39%
[2024-08-01 11:44:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:04:59 lr 0.000020	 wd 0.0500	time 11.6307 (11.6307)	loss 1.2130 (1.2130)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:45:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:30:07 lr 0.000020	 wd 0.0500	time 0.6018 (0.7526)	loss 0.9605 (1.1757)	grad_norm 1.6736 (2.4540)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:46:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:26:49 lr 0.000020	 wd 0.0500	time 0.6531 (0.6992)	loss 0.8965 (1.1581)	grad_norm 3.5923 (2.4186)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:47:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:25:01 lr 0.000020	 wd 0.0500	time 0.6251 (0.6817)	loss 0.9267 (1.1545)	grad_norm 1.7467 (2.3936)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:48:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:23:33 lr 0.000020	 wd 0.0500	time 0.6137 (0.6724)	loss 0.6868 (1.1529)	grad_norm 1.8644 (2.3295)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:49:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:22:15 lr 0.000020	 wd 0.0500	time 0.6042 (0.6670)	loss 1.2564 (1.1538)	grad_norm 1.4859 (2.3704)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:50:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:21:01 lr 0.000020	 wd 0.0500	time 0.6176 (0.6633)	loss 0.8791 (1.1456)	grad_norm 1.7289 (2.4654)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:51:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:19:50 lr 0.000020	 wd 0.0500	time 0.5949 (0.6608)	loss 1.3772 (1.1478)	grad_norm 1.6321 (2.4257)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:52:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:18:41 lr 0.000020	 wd 0.0500	time 0.6142 (0.6589)	loss 0.7568 (1.1529)	grad_norm 1.7098 (2.3828)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:54:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:17:33 lr 0.000020	 wd 0.0500	time 0.6417 (0.6575)	loss 1.5894 (1.1486)	grad_norm 1.8958 (2.4619)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:55:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:16:25 lr 0.000020	 wd 0.0500	time 0.6398 (0.6563)	loss 1.0117 (1.1485)	grad_norm 1.6951 (2.4421)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:56:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:15:18 lr 0.000020	 wd 0.0500	time 0.6444 (0.6553)	loss 0.7791 (1.1439)	grad_norm 2.1799 (2.4329)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 11:57:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:14:12 lr 0.000020	 wd 0.0500	time 0.6149 (0.6547)	loss 1.2847 (1.1408)	grad_norm 1.7861 (2.4603)	loss_scale 2048.0000 (1025.7052)	mem 19785MB
[2024-08-01 11:58:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:13:06 lr 0.000020	 wd 0.0500	time 0.6164 (0.6540)	loss 1.1125 (1.1398)	grad_norm 1.6487 (2.4356)	loss_scale 2048.0000 (1104.2829)	mem 19785MB
[2024-08-01 11:59:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:11:59 lr 0.000020	 wd 0.0500	time 0.6108 (0.6533)	loss 1.0125 (1.1391)	grad_norm 2.5824 (2.4333)	loss_scale 2048.0000 (1171.6431)	mem 19785MB
[2024-08-01 12:00:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:10:54 lr 0.000020	 wd 0.0500	time 0.6036 (0.6527)	loss 0.6879 (1.1390)	grad_norm 1.7739 (2.4176)	loss_scale 2048.0000 (1230.0280)	mem 19785MB
[2024-08-01 12:01:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:09:48 lr 0.000020	 wd 0.0500	time 0.6087 (0.6522)	loss 1.3384 (1.1369)	grad_norm 1.7317 (2.4122)	loss_scale 2048.0000 (1281.1193)	mem 19785MB
[2024-08-01 12:02:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:08:42 lr 0.000020	 wd 0.0500	time 0.6080 (0.6519)	loss 1.3468 (1.1350)	grad_norm 1.8191 (2.4220)	loss_scale 2048.0000 (1326.2034)	mem 19785MB
[2024-08-01 12:03:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:07:37 lr 0.000020	 wd 0.0500	time 0.5997 (0.6516)	loss 1.1963 (1.1353)	grad_norm 1.5625 (2.4105)	loss_scale 2048.0000 (1366.2810)	mem 19785MB
[2024-08-01 12:04:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:06:32 lr 0.000020	 wd 0.0500	time 0.6376 (0.6512)	loss 1.3880 (1.1374)	grad_norm 2.1656 (2.4153)	loss_scale 2048.0000 (1402.1420)	mem 19785MB
[2024-08-01 12:05:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:05:26 lr 0.000020	 wd 0.0500	time 0.6082 (0.6508)	loss 0.9047 (1.1382)	grad_norm 2.2175 (2.4322)	loss_scale 2048.0000 (1434.4188)	mem 19785MB
[2024-08-01 12:06:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:04:21 lr 0.000020	 wd 0.0500	time 0.6634 (0.6506)	loss 1.0999 (1.1386)	grad_norm 1.9834 (2.4230)	loss_scale 2048.0000 (1463.6230)	mem 19785MB
[2024-08-01 12:08:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:03:16 lr 0.000020	 wd 0.0500	time 0.6198 (0.6505)	loss 0.8027 (1.1381)	grad_norm 2.1264 (2.4367)	loss_scale 2048.0000 (1490.1736)	mem 19785MB
[2024-08-01 12:09:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:02:11 lr 0.000020	 wd 0.0500	time 0.5999 (0.6502)	loss 0.9416 (1.1401)	grad_norm 2.0322 (2.4554)	loss_scale 2048.0000 (1514.4163)	mem 19785MB
[2024-08-01 12:10:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:06 lr 0.000020	 wd 0.0500	time 0.6075 (0.6500)	loss 1.3970 (1.1391)	grad_norm 1.8952 (2.4519)	loss_scale 2048.0000 (1536.6397)	mem 19785MB
[2024-08-01 12:11:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.6199 (0.6498)	loss 1.4113 (1.1385)	grad_norm 1.7063 (2.4681)	loss_scale 2048.0000 (1557.0860)	mem 19785MB
[2024-08-01 12:11:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 6 training takes 0:27:08
[2024-08-01 12:11:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.619 (12.619)	Loss 0.5015 (0.5015)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 12:11:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.356 Acc@5 97.950
[2024-08-01 12:11:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-08-01 12:11:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.39%
[2024-08-01 12:12:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:50:12 lr 0.000020	 wd 0.0500	time 12.7148 (12.7148)	loss 0.9077 (0.9077)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:13:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:30:31 lr 0.000020	 wd 0.0500	time 0.6023 (0.7626)	loss 1.1942 (1.1208)	grad_norm 1.7294 (2.6656)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:14:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:27:00 lr 0.000020	 wd 0.0500	time 0.6123 (0.7038)	loss 1.0886 (1.1396)	grad_norm 1.9725 (2.5674)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:15:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:25:06 lr 0.000020	 wd 0.0500	time 0.6163 (0.6839)	loss 1.4657 (1.1396)	grad_norm 1.8179 (2.5024)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:16:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:23:37 lr 0.000020	 wd 0.0500	time 0.6016 (0.6744)	loss 0.8226 (1.1445)	grad_norm 5.1727 (2.4324)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:17:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:22:18 lr 0.000020	 wd 0.0500	time 0.6101 (0.6687)	loss 1.3786 (1.1424)	grad_norm 1.5236 (2.4572)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:18:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:21:05 lr 0.000020	 wd 0.0500	time 0.6129 (0.6653)	loss 1.2458 (1.1470)	grad_norm 2.5993 (2.5179)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:19:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:19:53 lr 0.000020	 wd 0.0500	time 0.5789 (0.6623)	loss 1.0395 (1.1414)	grad_norm 1.4899 (2.5135)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:20:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:18:43 lr 0.000020	 wd 0.0500	time 0.6276 (0.6602)	loss 0.9132 (1.1389)	grad_norm 2.3020 (2.5380)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:21:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:17:34 lr 0.000020	 wd 0.0500	time 0.6172 (0.6583)	loss 1.4828 (1.1419)	grad_norm 1.8042 (2.5311)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:22:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:16:26 lr 0.000020	 wd 0.0500	time 0.6139 (0.6570)	loss 0.8395 (1.1420)	grad_norm 1.8848 (2.5680)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:23:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:15:19 lr 0.000020	 wd 0.0500	time 0.6220 (0.6557)	loss 1.2087 (1.1401)	grad_norm 2.6756 (2.5414)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:25:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:14:12 lr 0.000020	 wd 0.0500	time 0.6148 (0.6549)	loss 1.2759 (1.1368)	grad_norm 1.8422 (2.5186)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:26:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:13:06 lr 0.000020	 wd 0.0500	time 0.6192 (0.6540)	loss 1.2898 (1.1367)	grad_norm 2.5631 (2.5183)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:27:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:11:59 lr 0.000019	 wd 0.0500	time 0.6167 (0.6533)	loss 1.2031 (1.1381)	grad_norm 9.3237 (2.5051)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:28:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:10:54 lr 0.000019	 wd 0.0500	time 0.6075 (0.6529)	loss 0.8138 (1.1390)	grad_norm 1.9698 (2.4752)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:29:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:09:48 lr 0.000019	 wd 0.0500	time 0.5823 (0.6525)	loss 1.1910 (1.1396)	grad_norm 2.0575 (2.4671)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:30:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:08:42 lr 0.000019	 wd 0.0500	time 0.6007 (0.6519)	loss 1.2299 (1.1393)	grad_norm 1.5974 (2.4500)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:31:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:07:37 lr 0.000019	 wd 0.0500	time 0.6053 (0.6515)	loss 0.7469 (1.1414)	grad_norm 2.9050 (2.4447)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:32:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:06:31 lr 0.000019	 wd 0.0500	time 0.6147 (0.6511)	loss 1.1606 (1.1419)	grad_norm 1.7232 (2.4270)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:33:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:05:26 lr 0.000019	 wd 0.0500	time 0.6069 (0.6508)	loss 1.2475 (1.1425)	grad_norm 2.1102 (2.4203)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:34:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:04:21 lr 0.000019	 wd 0.0500	time 0.6149 (0.6504)	loss 1.5034 (1.1449)	grad_norm 2.9878 (2.4177)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:35:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:03:16 lr 0.000019	 wd 0.0500	time 0.6200 (0.6501)	loss 1.2391 (1.1429)	grad_norm 2.2073 (2.4195)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:36:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:02:11 lr 0.000019	 wd 0.0500	time 0.6083 (0.6499)	loss 1.4320 (1.1423)	grad_norm 1.9220 (2.4051)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:37:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:06 lr 0.000019	 wd 0.0500	time 0.6068 (0.6497)	loss 1.3134 (1.1423)	grad_norm 1.6525 (2.4108)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:38:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.6088 (0.6496)	loss 1.4891 (1.1416)	grad_norm 1.5739 (2.4041)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:39:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 7 training takes 0:27:07
[2024-08-01 12:39:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.728 (12.728)	Loss 0.4788 (0.4788)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 12:39:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.362 Acc@5 97.994
[2024-08-01 12:39:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-08-01 12:39:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.39%
[2024-08-01 12:39:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][0/2502]	eta 9:04:06 lr 0.000019	 wd 0.0500	time 13.0480 (13.0480)	loss 1.2324 (1.2324)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:40:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:30:42 lr 0.000019	 wd 0.0500	time 0.6154 (0.7671)	loss 0.8924 (1.1967)	grad_norm 2.1202 (2.2212)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 12:41:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:27:04 lr 0.000019	 wd 0.0500	time 0.6181 (0.7058)	loss 0.8297 (1.1715)	grad_norm 2.0120 (2.1493)	loss_scale 4096.0000 (2109.1343)	mem 19785MB
[2024-08-01 12:43:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:25:08 lr 0.000019	 wd 0.0500	time 0.6114 (0.6850)	loss 1.4438 (1.1561)	grad_norm 2.9497 (nan)	loss_scale 2048.0000 (2401.8073)	mem 19785MB
[2024-08-01 12:44:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:23:38 lr 0.000019	 wd 0.0500	time 0.5791 (0.6748)	loss 0.8560 (1.1534)	grad_norm 2.2086 (nan)	loss_scale 2048.0000 (2313.5761)	mem 19785MB
[2024-08-01 12:45:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:22:19 lr 0.000019	 wd 0.0500	time 0.6063 (0.6689)	loss 0.7996 (1.1440)	grad_norm 2.5525 (nan)	loss_scale 2048.0000 (2260.5669)	mem 19785MB
[2024-08-01 12:46:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:21:04 lr 0.000019	 wd 0.0500	time 0.6096 (0.6650)	loss 0.8279 (1.1438)	grad_norm 1.9729 (nan)	loss_scale 2048.0000 (2225.1980)	mem 19785MB
[2024-08-01 12:47:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:19:52 lr 0.000019	 wd 0.0500	time 0.6432 (0.6620)	loss 1.2707 (1.1436)	grad_norm 2.2469 (nan)	loss_scale 2048.0000 (2199.9201)	mem 19785MB
[2024-08-01 12:48:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:18:43 lr 0.000019	 wd 0.0500	time 0.6082 (0.6599)	loss 1.2996 (1.1456)	grad_norm 1.7693 (nan)	loss_scale 2048.0000 (2180.9538)	mem 19785MB
[2024-08-01 12:49:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:17:34 lr 0.000019	 wd 0.0500	time 0.6187 (0.6585)	loss 0.9149 (1.1420)	grad_norm 2.7323 (nan)	loss_scale 2048.0000 (2166.1976)	mem 19785MB
[2024-08-01 12:50:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:16:27 lr 0.000019	 wd 0.0500	time 0.6403 (0.6572)	loss 1.3163 (1.1440)	grad_norm 2.9162 (nan)	loss_scale 2048.0000 (2154.3896)	mem 19785MB
[2024-08-01 12:51:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:15:19 lr 0.000019	 wd 0.0500	time 0.6081 (0.6559)	loss 1.2649 (1.1412)	grad_norm 1.7845 (nan)	loss_scale 2048.0000 (2144.7266)	mem 19785MB
[2024-08-01 12:52:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:14:12 lr 0.000019	 wd 0.0500	time 0.6025 (0.6549)	loss 1.0506 (1.1419)	grad_norm 2.4422 (nan)	loss_scale 2048.0000 (2136.6728)	mem 19785MB
[2024-08-01 12:53:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:13:06 lr 0.000019	 wd 0.0500	time 0.6086 (0.6541)	loss 0.8102 (1.1418)	grad_norm 2.9442 (nan)	loss_scale 2048.0000 (2129.8570)	mem 19785MB
[2024-08-01 12:54:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:12:00 lr 0.000019	 wd 0.0500	time 0.6145 (0.6535)	loss 1.2898 (1.1430)	grad_norm 1.8189 (nan)	loss_scale 2048.0000 (2124.0143)	mem 19785MB
[2024-08-01 12:55:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:10:54 lr 0.000019	 wd 0.0500	time 0.6151 (0.6530)	loss 1.0549 (1.1438)	grad_norm 2.3166 (nan)	loss_scale 2048.0000 (2118.9500)	mem 19785MB
[2024-08-01 12:56:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:09:48 lr 0.000019	 wd 0.0500	time 0.6087 (0.6525)	loss 1.3673 (1.1455)	grad_norm 2.5511 (nan)	loss_scale 2048.0000 (2114.5184)	mem 19785MB
[2024-08-01 12:58:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:08:43 lr 0.000019	 wd 0.0500	time 0.6202 (0.6522)	loss 0.7695 (1.1477)	grad_norm 1.7346 (nan)	loss_scale 2048.0000 (2110.6079)	mem 19785MB
[2024-08-01 12:59:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:07:37 lr 0.000019	 wd 0.0500	time 0.6049 (0.6519)	loss 0.7771 (1.1460)	grad_norm 2.1390 (nan)	loss_scale 2048.0000 (2107.1316)	mem 19785MB
[2024-08-01 13:00:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:06:32 lr 0.000019	 wd 0.0500	time 0.6068 (0.6516)	loss 1.2389 (1.1477)	grad_norm 2.5714 (nan)	loss_scale 2048.0000 (2104.0210)	mem 19785MB
[2024-08-01 13:01:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:05:26 lr 0.000019	 wd 0.0500	time 0.6171 (0.6513)	loss 0.7543 (1.1465)	grad_norm 2.1746 (nan)	loss_scale 2048.0000 (2101.2214)	mem 19785MB
[2024-08-01 13:02:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:04:21 lr 0.000019	 wd 0.0500	time 0.6125 (0.6509)	loss 0.8946 (1.1473)	grad_norm 2.4234 (nan)	loss_scale 2048.0000 (2098.6882)	mem 19785MB
[2024-08-01 13:03:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:03:16 lr 0.000019	 wd 0.0500	time 0.6182 (0.6506)	loss 1.2050 (1.1460)	grad_norm 2.5305 (nan)	loss_scale 2048.0000 (2096.3853)	mem 19785MB
[2024-08-01 13:04:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:02:11 lr 0.000019	 wd 0.0500	time 0.6142 (0.6504)	loss 1.2486 (1.1444)	grad_norm 1.9364 (nan)	loss_scale 2048.0000 (2094.2825)	mem 19785MB
[2024-08-01 13:05:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:06 lr 0.000019	 wd 0.0500	time 0.6120 (0.6501)	loss 1.0659 (1.1444)	grad_norm 2.7713 (nan)	loss_scale 2048.0000 (2092.3549)	mem 19785MB
[2024-08-01 13:06:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.6285 (0.6499)	loss 0.8640 (1.1440)	grad_norm 2.2344 (nan)	loss_scale 2048.0000 (2090.5814)	mem 19785MB
[2024-08-01 13:06:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 8 training takes 0:27:08
[2024-08-01 13:06:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.427 (12.427)	Loss 0.4817 (0.4817)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 13:07:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.514 Acc@5 98.022
[2024-08-01 13:07:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.5%
[2024-08-01 13:07:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.51%
[2024-08-01 13:07:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 13:07:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 13:07:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:56:33 lr 0.000019	 wd 0.0500	time 11.4283 (11.4283)	loss 1.3128 (1.3128)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:08:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:30:02 lr 0.000019	 wd 0.0500	time 0.6333 (0.7505)	loss 1.2299 (1.1305)	grad_norm 1.8468 (3.0338)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:09:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:26:47 lr 0.000019	 wd 0.0500	time 0.6387 (0.6982)	loss 0.9356 (1.1521)	grad_norm 2.6279 (2.6206)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:10:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:25:00 lr 0.000019	 wd 0.0500	time 0.6415 (0.6814)	loss 1.3969 (1.1570)	grad_norm 14.9618 (2.6593)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:11:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:23:32 lr 0.000019	 wd 0.0500	time 0.6129 (0.6720)	loss 0.9129 (1.1374)	grad_norm 1.8852 (2.6146)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:12:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:22:14 lr 0.000019	 wd 0.0500	time 0.6137 (0.6665)	loss 0.6799 (1.1326)	grad_norm 2.5554 (2.5452)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:13:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:21:00 lr 0.000019	 wd 0.0500	time 0.6140 (0.6628)	loss 0.7794 (1.1355)	grad_norm 1.8091 (2.5281)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:15:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:19:49 lr 0.000019	 wd 0.0500	time 0.6030 (0.6600)	loss 1.4111 (1.1383)	grad_norm 1.5612 (2.5393)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:16:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:18:40 lr 0.000019	 wd 0.0500	time 0.6156 (0.6581)	loss 1.4003 (1.1400)	grad_norm 1.9583 (2.5357)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:17:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:17:31 lr 0.000019	 wd 0.0500	time 0.6021 (0.6566)	loss 1.3378 (1.1402)	grad_norm 2.2001 (2.5045)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:18:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:16:24 lr 0.000019	 wd 0.0500	time 0.6102 (0.6553)	loss 1.2707 (1.1407)	grad_norm 2.8073 (2.4999)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:19:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:15:17 lr 0.000018	 wd 0.0500	time 0.6157 (0.6544)	loss 1.3621 (1.1367)	grad_norm 2.0117 (2.4860)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:20:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:14:11 lr 0.000018	 wd 0.0500	time 0.5776 (0.6539)	loss 0.9697 (1.1370)	grad_norm 4.7694 (2.4813)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:21:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:13:05 lr 0.000018	 wd 0.0500	time 0.6041 (0.6532)	loss 1.3139 (1.1379)	grad_norm 3.7357 (2.4870)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:22:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:11:59 lr 0.000018	 wd 0.0500	time 0.6159 (0.6526)	loss 0.9146 (1.1359)	grad_norm 1.7101 (2.4983)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 13:23:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:10:53 lr 0.000018	 wd 0.0500	time 0.6178 (0.6520)	loss 0.8722 (1.1385)	grad_norm 1.8321 (nan)	loss_scale 1024.0000 (1987.9654)	mem 19785MB
[2024-08-01 13:24:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:09:47 lr 0.000018	 wd 0.0500	time 0.6091 (0.6514)	loss 0.8982 (1.1365)	grad_norm 2.0558 (nan)	loss_scale 1024.0000 (1927.7552)	mem 19785MB
[2024-08-01 13:25:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:08:42 lr 0.000018	 wd 0.0500	time 0.6114 (0.6509)	loss 1.3871 (1.1368)	grad_norm 2.6654 (nan)	loss_scale 1024.0000 (1874.6243)	mem 19785MB
[2024-08-01 13:26:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:07:36 lr 0.000018	 wd 0.0500	time 0.6385 (0.6506)	loss 1.2562 (1.1361)	grad_norm 2.0214 (nan)	loss_scale 1024.0000 (1827.3937)	mem 19785MB
[2024-08-01 13:27:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:06:31 lr 0.000018	 wd 0.0500	time 0.6080 (0.6503)	loss 1.3688 (1.1350)	grad_norm 1.8266 (nan)	loss_scale 1024.0000 (1785.1320)	mem 19785MB
[2024-08-01 13:28:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:05:26 lr 0.000018	 wd 0.0500	time 0.6148 (0.6500)	loss 1.1587 (1.1355)	grad_norm 3.7108 (nan)	loss_scale 1024.0000 (1747.0945)	mem 19785MB
[2024-08-01 13:30:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:04:21 lr 0.000018	 wd 0.0500	time 0.6125 (0.6498)	loss 1.3482 (1.1366)	grad_norm 2.2232 (nan)	loss_scale 1024.0000 (1712.6778)	mem 19785MB
[2024-08-01 13:31:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:03:16 lr 0.000018	 wd 0.0500	time 0.6196 (0.6496)	loss 1.1997 (1.1375)	grad_norm 1.9554 (nan)	loss_scale 1024.0000 (1681.3885)	mem 19785MB
[2024-08-01 13:32:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:02:11 lr 0.000018	 wd 0.0500	time 0.6425 (0.6494)	loss 0.8198 (1.1375)	grad_norm 3.0118 (nan)	loss_scale 1024.0000 (1652.8188)	mem 19785MB
[2024-08-01 13:33:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:06 lr 0.000018	 wd 0.0500	time 0.6208 (0.6492)	loss 1.1122 (1.1379)	grad_norm 2.6679 (nan)	loss_scale 1024.0000 (1626.6289)	mem 19785MB
[2024-08-01 13:34:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.6132 (0.6489)	loss 1.3646 (1.1374)	grad_norm 5.3191 (nan)	loss_scale 1024.0000 (1602.5334)	mem 19785MB
[2024-08-01 13:34:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 9 training takes 0:27:05
[2024-08-01 13:34:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.358 (12.358)	Loss 0.4839 (0.4839)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 19785MB
[2024-08-01 13:34:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.592 Acc@5 98.018
[2024-08-01 13:34:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-01 13:34:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.59%
[2024-08-01 13:34:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 13:34:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 13:35:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:55:27 lr 0.000018	 wd 0.0500	time 11.4017 (11.4017)	loss 1.3894 (1.3894)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:36:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:30:02 lr 0.000018	 wd 0.0500	time 0.6087 (0.7505)	loss 0.8054 (1.1430)	grad_norm 2.5473 (2.5426)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:37:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:26:43 lr 0.000018	 wd 0.0500	time 0.6691 (0.6966)	loss 1.3466 (1.1338)	grad_norm 2.0040 (2.5096)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:38:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:24:56 lr 0.000018	 wd 0.0500	time 0.6098 (0.6794)	loss 1.3193 (1.1337)	grad_norm 2.3566 (2.5100)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:39:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:23:28 lr 0.000018	 wd 0.0500	time 0.6288 (0.6702)	loss 0.7536 (1.1412)	grad_norm 1.9932 (2.4262)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:40:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:22:11 lr 0.000018	 wd 0.0500	time 0.6138 (0.6653)	loss 0.7173 (1.1387)	grad_norm 1.9521 (2.3689)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:41:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:20:59 lr 0.000018	 wd 0.0500	time 0.6123 (0.6622)	loss 1.2122 (1.1449)	grad_norm 1.7857 (2.3267)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:42:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:19:48 lr 0.000018	 wd 0.0500	time 0.6296 (0.6596)	loss 1.2622 (1.1423)	grad_norm 2.0934 (2.3331)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:43:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:18:39 lr 0.000018	 wd 0.0500	time 0.6112 (0.6578)	loss 1.2452 (1.1448)	grad_norm 1.7480 (2.3413)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:44:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:17:31 lr 0.000018	 wd 0.0500	time 0.6017 (0.6564)	loss 1.2963 (1.1431)	grad_norm 1.6860 (2.3409)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:45:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:16:23 lr 0.000018	 wd 0.0500	time 0.6420 (0.6551)	loss 1.2868 (1.1440)	grad_norm 4.5945 (2.3483)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:46:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:15:17 lr 0.000018	 wd 0.0500	time 0.6129 (0.6541)	loss 0.9949 (1.1511)	grad_norm 1.9818 (2.3480)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:48:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:14:10 lr 0.000018	 wd 0.0500	time 0.6111 (0.6532)	loss 1.4301 (1.1480)	grad_norm 2.4089 (2.3340)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:49:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:13:04 lr 0.000018	 wd 0.0500	time 0.6055 (0.6525)	loss 1.1849 (1.1461)	grad_norm 3.1529 (2.3405)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:50:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:11:58 lr 0.000018	 wd 0.0500	time 0.6156 (0.6518)	loss 1.4058 (1.1447)	grad_norm 2.1019 (2.3501)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:51:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:10:52 lr 0.000018	 wd 0.0500	time 0.6266 (0.6514)	loss 1.1539 (1.1429)	grad_norm 2.1049 (2.3394)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:52:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:09:47 lr 0.000018	 wd 0.0500	time 0.6099 (0.6510)	loss 1.1274 (1.1410)	grad_norm 2.5261 (2.3367)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:53:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:08:41 lr 0.000018	 wd 0.0500	time 0.6520 (0.6506)	loss 1.2499 (1.1429)	grad_norm 2.4255 (2.3386)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:54:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:07:36 lr 0.000018	 wd 0.0500	time 0.6168 (0.6502)	loss 0.9307 (1.1412)	grad_norm 3.3309 (2.3340)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:55:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:06:31 lr 0.000018	 wd 0.0500	time 0.6067 (0.6499)	loss 1.0525 (1.1400)	grad_norm 1.9575 (2.3354)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:56:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:05:26 lr 0.000017	 wd 0.0500	time 0.6034 (0.6495)	loss 1.2620 (1.1430)	grad_norm 2.1869 (2.3316)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:57:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:04:21 lr 0.000017	 wd 0.0500	time 0.6130 (0.6493)	loss 1.0416 (1.1446)	grad_norm 2.4239 (2.3872)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:58:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:03:16 lr 0.000017	 wd 0.0500	time 0.6097 (0.6490)	loss 1.2761 (1.1455)	grad_norm 1.6944 (2.3806)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 13:59:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:02:11 lr 0.000017	 wd 0.0500	time 0.6114 (0.6488)	loss 1.2481 (1.1434)	grad_norm 2.2186 (2.3865)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:00:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:06 lr 0.000017	 wd 0.0500	time 0.6183 (0.6485)	loss 1.4223 (1.1423)	grad_norm 2.7274 (2.3849)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:02:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.6085 (0.6484)	loss 0.8144 (1.1413)	grad_norm 2.6655 (2.3849)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:02:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 10 training takes 0:27:04
[2024-08-01 14:02:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.597 (12.597)	Loss 0.4839 (0.4839)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 14:02:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.652 Acc@5 98.062
[2024-08-01 14:02:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-01 14:02:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.65%
[2024-08-01 14:02:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 14:02:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 14:02:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:15:59 lr 0.000017	 wd 0.0500	time 11.8943 (11.8943)	loss 0.9401 (0.9401)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:03:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:30:09 lr 0.000017	 wd 0.0500	time 0.6465 (0.7533)	loss 1.3208 (1.1266)	grad_norm 2.6248 (2.5324)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:05:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:26:46 lr 0.000017	 wd 0.0500	time 0.6070 (0.6977)	loss 1.2425 (1.1375)	grad_norm 3.0876 (2.4785)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:06:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:24:55 lr 0.000017	 wd 0.0500	time 0.6152 (0.6792)	loss 0.8876 (1.1311)	grad_norm 2.0607 (2.4735)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:07:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:23:28 lr 0.000017	 wd 0.0500	time 0.6085 (0.6700)	loss 1.3879 (1.1339)	grad_norm 1.8927 (2.4136)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:08:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:22:10 lr 0.000017	 wd 0.0500	time 0.6141 (0.6647)	loss 0.7566 (1.1325)	grad_norm 1.9812 (2.3697)	loss_scale 2048.0000 (1212.0399)	mem 19785MB
[2024-08-01 14:09:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:20:57 lr 0.000017	 wd 0.0500	time 0.6013 (0.6612)	loss 1.5055 (1.1344)	grad_norm 3.1132 (2.3681)	loss_scale 2048.0000 (1351.1348)	mem 19785MB
[2024-08-01 14:10:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:19:47 lr 0.000017	 wd 0.0500	time 0.6151 (0.6589)	loss 0.9104 (1.1316)	grad_norm 9.4931 (2.3867)	loss_scale 2048.0000 (1450.5449)	mem 19785MB
[2024-08-01 14:11:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:18:38 lr 0.000017	 wd 0.0500	time 0.6065 (0.6570)	loss 1.4316 (1.1301)	grad_norm 1.5449 (2.3792)	loss_scale 2048.0000 (1525.1336)	mem 19785MB
[2024-08-01 14:12:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:17:30 lr 0.000017	 wd 0.0500	time 0.6078 (0.6560)	loss 0.8711 (1.1321)	grad_norm 1.8294 (2.3740)	loss_scale 2048.0000 (1583.1654)	mem 19785MB
[2024-08-01 14:13:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:16:23 lr 0.000017	 wd 0.0500	time 0.5998 (0.6548)	loss 1.2155 (1.1329)	grad_norm 2.4036 (2.3549)	loss_scale 2048.0000 (1629.6024)	mem 19785MB
[2024-08-01 14:14:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:15:16 lr 0.000017	 wd 0.0500	time 0.6269 (0.6538)	loss 0.8974 (1.1312)	grad_norm 2.5684 (2.3323)	loss_scale 2048.0000 (1667.6040)	mem 19785MB
[2024-08-01 14:15:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:14:10 lr 0.000017	 wd 0.0500	time 0.6131 (0.6531)	loss 0.9593 (1.1275)	grad_norm 3.1465 (2.3221)	loss_scale 2048.0000 (1699.2773)	mem 19785MB
[2024-08-01 14:16:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:13:04 lr 0.000017	 wd 0.0500	time 0.5980 (0.6524)	loss 1.2252 (1.1280)	grad_norm 2.0608 (2.3774)	loss_scale 2048.0000 (1726.0815)	mem 19785MB
[2024-08-01 14:17:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:11:58 lr 0.000017	 wd 0.0500	time 0.6180 (0.6518)	loss 0.8518 (1.1290)	grad_norm 1.9366 (2.3773)	loss_scale 2048.0000 (1749.0592)	mem 19785MB
[2024-08-01 14:18:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:10:52 lr 0.000017	 wd 0.0500	time 0.6079 (0.6512)	loss 1.3699 (1.1286)	grad_norm 2.6635 (2.4041)	loss_scale 2048.0000 (1768.9753)	mem 19785MB
[2024-08-01 14:20:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:09:47 lr 0.000017	 wd 0.0500	time 0.6174 (0.6508)	loss 1.2681 (1.1299)	grad_norm 1.7298 (2.3872)	loss_scale 2048.0000 (1786.4035)	mem 19785MB
[2024-08-01 14:21:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:08:41 lr 0.000017	 wd 0.0500	time 0.6122 (0.6504)	loss 1.0055 (1.1306)	grad_norm 2.0443 (2.3784)	loss_scale 2048.0000 (1801.7825)	mem 19785MB
[2024-08-01 14:22:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:07:36 lr 0.000017	 wd 0.0500	time 0.6113 (0.6501)	loss 0.8413 (1.1296)	grad_norm 1.9853 (2.3761)	loss_scale 2048.0000 (1815.4536)	mem 19785MB
[2024-08-01 14:23:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:06:31 lr 0.000017	 wd 0.0500	time 0.6111 (0.6499)	loss 1.0342 (1.1282)	grad_norm 13.3138 (2.3843)	loss_scale 2048.0000 (1827.6865)	mem 19785MB
[2024-08-01 14:24:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:05:26 lr 0.000017	 wd 0.0500	time 0.6105 (0.6496)	loss 1.1892 (1.1286)	grad_norm 1.6255 (2.3738)	loss_scale 2048.0000 (1838.6967)	mem 19785MB
[2024-08-01 14:25:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:04:21 lr 0.000017	 wd 0.0500	time 0.6115 (0.6493)	loss 1.3737 (1.1282)	grad_norm 1.8712 (2.3729)	loss_scale 2048.0000 (1848.6587)	mem 19785MB
[2024-08-01 14:26:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:03:16 lr 0.000017	 wd 0.0500	time 0.6056 (0.6490)	loss 0.8687 (1.1296)	grad_norm 2.1481 (2.3705)	loss_scale 2048.0000 (1857.7156)	mem 19785MB
[2024-08-01 14:27:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:02:11 lr 0.000016	 wd 0.0500	time 0.6152 (0.6488)	loss 1.1404 (1.1281)	grad_norm 1.7797 (2.3773)	loss_scale 2048.0000 (1865.9852)	mem 19785MB
[2024-08-01 14:28:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:06 lr 0.000016	 wd 0.0500	time 0.6098 (0.6487)	loss 1.2507 (1.1293)	grad_norm 3.7006 (2.3965)	loss_scale 2048.0000 (1873.5660)	mem 19785MB
[2024-08-01 14:29:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.6082 (0.6484)	loss 0.8154 (1.1274)	grad_norm 3.0823 (2.4192)	loss_scale 2048.0000 (1880.5406)	mem 19785MB
[2024-08-01 14:29:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 11 training takes 0:27:04
[2024-08-01 14:29:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.464 (11.464)	Loss 0.4778 (0.4778)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 14:30:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.614 Acc@5 98.034
[2024-08-01 14:30:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-01 14:30:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.65%
[2024-08-01 14:30:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:04:51 lr 0.000016	 wd 0.0500	time 11.6274 (11.6274)	loss 0.9128 (0.9128)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 14:31:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:30:20 lr 0.000016	 wd 0.0500	time 0.6079 (0.7580)	loss 1.2503 (1.1229)	grad_norm 1.9941 (2.5057)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 14:32:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:26:55 lr 0.000016	 wd 0.0500	time 0.7277 (0.7019)	loss 1.1700 (1.1222)	grad_norm 2.4946 (2.4053)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 14:33:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:25:04 lr 0.000016	 wd 0.0500	time 0.6175 (0.6834)	loss 0.9070 (1.1318)	grad_norm 2.4192 (2.5511)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 14:34:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:23:35 lr 0.000016	 wd 0.0500	time 0.5762 (0.6733)	loss 1.1905 (1.1267)	grad_norm 2.4947 (nan)	loss_scale 1024.0000 (1838.6035)	mem 19785MB
[2024-08-01 14:35:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:22:15 lr 0.000016	 wd 0.0500	time 0.6100 (0.6672)	loss 1.3667 (1.1246)	grad_norm 3.0274 (nan)	loss_scale 1024.0000 (1676.0080)	mem 19785MB
[2024-08-01 14:36:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:21:01 lr 0.000016	 wd 0.0500	time 0.6081 (0.6633)	loss 1.3817 (1.1222)	grad_norm 2.6376 (nan)	loss_scale 1024.0000 (1567.5208)	mem 19785MB
[2024-08-01 14:38:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:19:50 lr 0.000016	 wd 0.0500	time 0.6482 (0.6608)	loss 1.2204 (1.1199)	grad_norm 2.8803 (nan)	loss_scale 1024.0000 (1489.9857)	mem 19785MB
[2024-08-01 14:39:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:18:41 lr 0.000016	 wd 0.0500	time 0.6156 (0.6587)	loss 1.3276 (1.1212)	grad_norm 1.9491 (nan)	loss_scale 1024.0000 (1431.8102)	mem 19785MB
[2024-08-01 14:40:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:17:32 lr 0.000016	 wd 0.0500	time 0.6203 (0.6571)	loss 1.3417 (1.1186)	grad_norm 3.5024 (nan)	loss_scale 1024.0000 (1386.5483)	mem 19785MB
[2024-08-01 14:41:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:16:24 lr 0.000016	 wd 0.0500	time 0.6166 (0.6558)	loss 1.2609 (1.1227)	grad_norm 1.6716 (nan)	loss_scale 1024.0000 (1350.3297)	mem 19785MB
[2024-08-01 14:42:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:15:17 lr 0.000016	 wd 0.0500	time 0.6177 (0.6547)	loss 0.8838 (1.1247)	grad_norm 3.6588 (nan)	loss_scale 1024.0000 (1320.6903)	mem 19785MB
[2024-08-01 14:43:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:14:11 lr 0.000016	 wd 0.0500	time 0.6090 (0.6541)	loss 1.2984 (1.1236)	grad_norm 2.4286 (nan)	loss_scale 1024.0000 (1295.9867)	mem 19785MB
[2024-08-01 14:44:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:13:05 lr 0.000016	 wd 0.0500	time 0.6173 (0.6532)	loss 1.0251 (1.1251)	grad_norm 2.1909 (nan)	loss_scale 1024.0000 (1275.0807)	mem 19785MB
[2024-08-01 14:45:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:11:59 lr 0.000016	 wd 0.0500	time 0.6072 (0.6525)	loss 1.3554 (1.1259)	grad_norm 2.4998 (nan)	loss_scale 1024.0000 (1257.1592)	mem 19785MB
[2024-08-01 14:46:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:10:53 lr 0.000016	 wd 0.0500	time 0.6027 (0.6519)	loss 0.7932 (1.1226)	grad_norm 3.5467 (nan)	loss_scale 1024.0000 (1241.6256)	mem 19785MB
[2024-08-01 14:47:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:09:47 lr 0.000016	 wd 0.0500	time 0.6022 (0.6513)	loss 1.3782 (1.1200)	grad_norm 1.6125 (nan)	loss_scale 1024.0000 (1228.0325)	mem 19785MB
[2024-08-01 14:48:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:08:42 lr 0.000016	 wd 0.0500	time 0.6102 (0.6509)	loss 1.1734 (1.1225)	grad_norm 2.6926 (nan)	loss_scale 1024.0000 (1216.0376)	mem 19785MB
[2024-08-01 14:49:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:07:36 lr 0.000016	 wd 0.0500	time 0.6113 (0.6506)	loss 0.8001 (1.1214)	grad_norm 2.4339 (nan)	loss_scale 1024.0000 (1205.3748)	mem 19785MB
[2024-08-01 14:50:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:06:31 lr 0.000016	 wd 0.0500	time 0.6293 (0.6501)	loss 1.1774 (1.1219)	grad_norm 1.6597 (nan)	loss_scale 1024.0000 (1195.8338)	mem 19785MB
[2024-08-01 14:51:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:05:26 lr 0.000016	 wd 0.0500	time 0.5998 (0.6498)	loss 0.9936 (1.1230)	grad_norm 2.1134 (nan)	loss_scale 1024.0000 (1187.2464)	mem 19785MB
[2024-08-01 14:53:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:04:21 lr 0.000016	 wd 0.0500	time 0.6055 (0.6495)	loss 0.8083 (1.1222)	grad_norm 2.0091 (nan)	loss_scale 1024.0000 (1179.4764)	mem 19785MB
[2024-08-01 14:54:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:03:16 lr 0.000016	 wd 0.0500	time 0.6121 (0.6494)	loss 0.7615 (1.1235)	grad_norm 2.9449 (nan)	loss_scale 1024.0000 (1172.4125)	mem 19785MB
[2024-08-01 14:55:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:02:11 lr 0.000015	 wd 0.0500	time 0.6072 (0.6491)	loss 1.0538 (1.1238)	grad_norm 2.0807 (nan)	loss_scale 1024.0000 (1165.9626)	mem 19785MB
[2024-08-01 14:56:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:06 lr 0.000015	 wd 0.0500	time 0.6136 (0.6489)	loss 1.3494 (1.1244)	grad_norm 1.9120 (nan)	loss_scale 1024.0000 (1160.0500)	mem 19785MB
[2024-08-01 14:57:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.6095 (0.6486)	loss 0.7620 (1.1236)	grad_norm 1.7705 (nan)	loss_scale 1024.0000 (1154.6102)	mem 19785MB
[2024-08-01 14:57:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 12 training takes 0:27:05
[2024-08-01 14:57:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.948 (12.948)	Loss 0.4932 (0.4932)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 14:57:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.638 Acc@5 98.054
[2024-08-01 14:57:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-01 14:57:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.65%
[2024-08-01 14:58:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:34:18 lr 0.000015	 wd 0.0500	time 12.3334 (12.3334)	loss 1.2901 (1.2901)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 14:59:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:30:32 lr 0.000015	 wd 0.0500	time 0.6095 (0.7628)	loss 1.3595 (1.1831)	grad_norm 1.4583 (2.3517)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:00:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:26:58 lr 0.000015	 wd 0.0500	time 0.6050 (0.7030)	loss 1.3655 (1.1670)	grad_norm 1.7053 (2.4973)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:01:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:25:04 lr 0.000015	 wd 0.0500	time 0.6175 (0.6833)	loss 1.3872 (1.1666)	grad_norm 1.8312 (2.4335)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:02:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:23:34 lr 0.000015	 wd 0.0500	time 0.6161 (0.6731)	loss 1.4168 (1.1506)	grad_norm 1.9607 (2.3930)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:03:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:22:16 lr 0.000015	 wd 0.0500	time 0.7096 (0.6674)	loss 1.3587 (1.1438)	grad_norm 2.2339 (2.3257)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:04:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:21:02 lr 0.000015	 wd 0.0500	time 0.6080 (0.6639)	loss 1.2084 (1.1373)	grad_norm 1.6255 (2.3575)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:05:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:19:51 lr 0.000015	 wd 0.0500	time 0.6110 (0.6612)	loss 1.0592 (1.1307)	grad_norm 2.2110 (2.3347)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:06:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:18:41 lr 0.000015	 wd 0.0500	time 0.6108 (0.6590)	loss 1.3985 (1.1305)	grad_norm 2.4130 (2.3401)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:07:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:17:32 lr 0.000015	 wd 0.0500	time 0.6075 (0.6572)	loss 1.1183 (1.1298)	grad_norm 2.1267 (2.3375)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:08:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:16:25 lr 0.000015	 wd 0.0500	time 0.6125 (0.6559)	loss 1.2834 (1.1291)	grad_norm 2.0566 (2.3489)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:09:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:15:17 lr 0.000015	 wd 0.0500	time 0.6144 (0.6547)	loss 1.2874 (1.1259)	grad_norm 4.4009 (2.3826)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:11:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:14:11 lr 0.000015	 wd 0.0500	time 0.6109 (0.6537)	loss 1.2235 (1.1299)	grad_norm 2.1855 (2.4179)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:12:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:13:04 lr 0.000015	 wd 0.0500	time 0.6151 (0.6529)	loss 0.8484 (1.1296)	grad_norm 1.8721 (2.3990)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:13:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:11:58 lr 0.000015	 wd 0.0500	time 0.6382 (0.6522)	loss 1.3662 (1.1284)	grad_norm 3.5820 (2.4192)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:14:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:10:53 lr 0.000015	 wd 0.0500	time 0.6079 (0.6518)	loss 1.1632 (1.1287)	grad_norm 3.6359 (2.4066)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:15:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:09:47 lr 0.000015	 wd 0.0500	time 0.6143 (0.6513)	loss 0.9779 (1.1283)	grad_norm 1.8241 (2.3963)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:16:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:08:41 lr 0.000015	 wd 0.0500	time 0.6196 (0.6508)	loss 1.0550 (1.1291)	grad_norm 2.8924 (2.3894)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:17:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:07:36 lr 0.000015	 wd 0.0500	time 0.6126 (0.6504)	loss 1.3704 (1.1293)	grad_norm 2.2037 (2.3944)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 15:18:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:06:31 lr 0.000015	 wd 0.0500	time 0.6059 (0.6501)	loss 1.0890 (1.1304)	grad_norm 1.8735 (2.4003)	loss_scale 2048.0000 (1069.2478)	mem 19785MB
[2024-08-01 15:19:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:05:26 lr 0.000015	 wd 0.0500	time 0.6131 (0.6497)	loss 1.3151 (1.1303)	grad_norm 2.3028 (2.3937)	loss_scale 2048.0000 (1118.1609)	mem 19785MB
[2024-08-01 15:20:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:04:21 lr 0.000014	 wd 0.0500	time 0.6063 (0.6494)	loss 1.3971 (1.1307)	grad_norm 2.5757 (2.3902)	loss_scale 2048.0000 (1162.4179)	mem 19785MB
[2024-08-01 15:21:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:03:16 lr 0.000014	 wd 0.0500	time 0.6108 (0.6491)	loss 1.1579 (1.1291)	grad_norm 2.1801 (2.3854)	loss_scale 2048.0000 (1202.6533)	mem 19785MB
[2024-08-01 15:22:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:02:11 lr 0.000014	 wd 0.0500	time 0.6114 (0.6489)	loss 1.3373 (1.1288)	grad_norm 1.6946 (2.3901)	loss_scale 2048.0000 (1239.3916)	mem 19785MB
[2024-08-01 15:23:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:06 lr 0.000014	 wd 0.0500	time 0.6150 (0.6487)	loss 0.8466 (1.1316)	grad_norm 2.7417 (2.3878)	loss_scale 2048.0000 (1273.0696)	mem 19785MB
[2024-08-01 15:24:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.6136 (0.6486)	loss 0.8127 (1.1299)	grad_norm 1.8494 (2.3847)	loss_scale 2048.0000 (1304.0544)	mem 19785MB
[2024-08-01 15:25:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 13 training takes 0:27:05
[2024-08-01 15:25:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 13.341 (13.341)	Loss 0.4668 (0.4668)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 15:25:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.654 Acc@5 98.030
[2024-08-01 15:25:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-01 15:25:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.65%
[2024-08-01 15:25:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 15:25:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 15:25:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:56:38 lr 0.000014	 wd 0.0500	time 11.4301 (11.4301)	loss 1.3017 (1.3017)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:26:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:30:02 lr 0.000014	 wd 0.0500	time 0.6027 (0.7503)	loss 1.0394 (1.1310)	grad_norm 2.8529 (3.3319)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:27:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:26:44 lr 0.000014	 wd 0.0500	time 0.5994 (0.6970)	loss 1.3632 (1.1371)	grad_norm 2.3774 (3.1581)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:29:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:24:54 lr 0.000014	 wd 0.0500	time 0.6061 (0.6788)	loss 0.9286 (1.1398)	grad_norm 1.8487 (2.8270)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:30:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:23:28 lr 0.000014	 wd 0.0500	time 0.6123 (0.6699)	loss 1.5471 (1.1417)	grad_norm 2.3336 (2.6567)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:31:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:22:10 lr 0.000014	 wd 0.0500	time 0.6166 (0.6644)	loss 0.9363 (1.1375)	grad_norm 2.2307 (2.6754)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:32:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:20:57 lr 0.000014	 wd 0.0500	time 0.5926 (0.6610)	loss 0.9556 (1.1347)	grad_norm 2.4918 (2.7818)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:33:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:19:46 lr 0.000014	 wd 0.0500	time 0.6209 (0.6584)	loss 1.1817 (1.1363)	grad_norm 2.3080 (2.7737)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:34:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:18:37 lr 0.000014	 wd 0.0500	time 0.6092 (0.6565)	loss 0.7660 (1.1313)	grad_norm 3.1040 (2.7039)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:35:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:17:30 lr 0.000014	 wd 0.0500	time 0.6116 (0.6555)	loss 0.7889 (1.1297)	grad_norm 2.0948 (2.6863)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:36:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:16:22 lr 0.000014	 wd 0.0500	time 0.6115 (0.6544)	loss 0.9567 (1.1277)	grad_norm 3.3533 (2.6535)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:37:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:15:15 lr 0.000014	 wd 0.0500	time 0.6168 (0.6533)	loss 1.3574 (1.1286)	grad_norm 1.7596 (2.6317)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:38:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:14:09 lr 0.000014	 wd 0.0500	time 0.6244 (0.6525)	loss 1.4795 (1.1284)	grad_norm 2.3919 (2.6051)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:39:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:13:03 lr 0.000014	 wd 0.0500	time 0.6061 (0.6518)	loss 1.1685 (1.1317)	grad_norm 2.5288 (2.5819)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:40:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:11:57 lr 0.000014	 wd 0.0500	time 0.6252 (0.6512)	loss 1.2669 (1.1321)	grad_norm 2.5189 (2.5582)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:41:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:10:52 lr 0.000014	 wd 0.0500	time 0.6068 (0.6507)	loss 1.4385 (1.1308)	grad_norm 1.8487 (2.5247)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:42:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:09:46 lr 0.000014	 wd 0.0500	time 0.6176 (0.6503)	loss 0.8871 (1.1298)	grad_norm 1.5522 (2.5024)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:44:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:08:41 lr 0.000014	 wd 0.0500	time 0.6122 (0.6499)	loss 0.8471 (1.1293)	grad_norm 1.8462 (2.5000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:45:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:07:36 lr 0.000013	 wd 0.0500	time 0.6047 (0.6496)	loss 1.0108 (1.1304)	grad_norm 2.2967 (2.4903)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:46:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:06:30 lr 0.000013	 wd 0.0500	time 0.6036 (0.6494)	loss 1.2126 (1.1311)	grad_norm 2.2826 (2.4733)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:47:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:05:25 lr 0.000013	 wd 0.0500	time 0.6177 (0.6491)	loss 1.3875 (1.1297)	grad_norm 3.3945 (2.4772)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:48:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:04:20 lr 0.000013	 wd 0.0500	time 0.6142 (0.6489)	loss 1.2462 (1.1289)	grad_norm 1.7894 (2.4716)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:49:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:03:15 lr 0.000013	 wd 0.0500	time 0.6104 (0.6486)	loss 1.2044 (1.1282)	grad_norm 2.0613 (2.4600)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:50:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:02:10 lr 0.000013	 wd 0.0500	time 0.6083 (0.6484)	loss 0.8225 (1.1276)	grad_norm 1.9338 (2.4524)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:51:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:06 lr 0.000013	 wd 0.0500	time 0.6100 (0.6482)	loss 0.7855 (1.1271)	grad_norm 2.5156 (2.4386)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:52:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.6074 (0.6480)	loss 1.2745 (1.1263)	grad_norm 2.8490 (2.4451)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:52:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 14 training takes 0:27:03
[2024-08-01 15:52:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 13.328 (13.328)	Loss 0.4624 (0.4624)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 15:53:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.648 Acc@5 98.040
[2024-08-01 15:53:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-01 15:53:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.65%
[2024-08-01 15:53:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][0/2502]	eta 8:39:07 lr 0.000013	 wd 0.0500	time 12.4492 (12.4492)	loss 1.2230 (1.2230)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:54:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:30:35 lr 0.000013	 wd 0.0500	time 0.6192 (0.7642)	loss 0.7623 (1.1242)	grad_norm 2.5642 (2.3894)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:55:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:27:04 lr 0.000013	 wd 0.0500	time 0.6162 (0.7056)	loss 1.5159 (1.1133)	grad_norm 1.9999 (2.2574)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:56:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:25:08 lr 0.000013	 wd 0.0500	time 0.6108 (0.6853)	loss 0.8694 (1.1118)	grad_norm 1.7000 (2.2494)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:57:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:23:38 lr 0.000013	 wd 0.0500	time 0.6058 (0.6748)	loss 0.7453 (1.1100)	grad_norm 1.7434 (2.5122)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:58:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:22:17 lr 0.000013	 wd 0.0500	time 0.6245 (0.6683)	loss 0.7193 (1.1195)	grad_norm 1.6870 (2.5118)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 15:59:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:21:02 lr 0.000013	 wd 0.0500	time 0.6023 (0.6639)	loss 1.2510 (1.1249)	grad_norm 2.2135 (2.5360)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 16:01:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:19:51 lr 0.000013	 wd 0.0500	time 0.6196 (0.6610)	loss 1.3035 (1.1273)	grad_norm 2.4503 (2.5733)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 16:02:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:18:41 lr 0.000013	 wd 0.0500	time 0.6076 (0.6588)	loss 1.1518 (1.1280)	grad_norm 1.9857 (2.5508)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 16:03:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:17:32 lr 0.000013	 wd 0.0500	time 0.6185 (0.6573)	loss 1.3273 (1.1291)	grad_norm 1.9339 (2.5127)	loss_scale 4096.0000 (2248.0266)	mem 19785MB
[2024-08-01 16:04:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:16:25 lr 0.000013	 wd 0.0500	time 0.6089 (0.6560)	loss 0.7751 (1.1289)	grad_norm 1.8424 (2.4741)	loss_scale 4096.0000 (2432.6394)	mem 19785MB
[2024-08-01 16:05:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:15:18 lr 0.000013	 wd 0.0500	time 0.6188 (0.6548)	loss 0.8397 (1.1297)	grad_norm 2.2329 (2.4699)	loss_scale 4096.0000 (2583.7166)	mem 19785MB
[2024-08-01 16:06:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:14:11 lr 0.000013	 wd 0.0500	time 0.6122 (0.6543)	loss 0.8605 (1.1300)	grad_norm 1.8496 (2.4907)	loss_scale 4096.0000 (2709.6353)	mem 19785MB
[2024-08-01 16:07:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:13:05 lr 0.000013	 wd 0.0500	time 0.6165 (0.6535)	loss 0.9032 (1.1312)	grad_norm 2.0348 (2.5003)	loss_scale 4096.0000 (2816.1968)	mem 19785MB
[2024-08-01 16:08:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:11:59 lr 0.000012	 wd 0.0500	time 0.6130 (0.6530)	loss 0.7798 (1.1310)	grad_norm 1.6913 (2.4907)	loss_scale 4096.0000 (2907.5460)	mem 19785MB
[2024-08-01 16:09:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:10:53 lr 0.000012	 wd 0.0500	time 0.6173 (0.6524)	loss 1.0497 (1.1270)	grad_norm 2.4535 (2.4845)	loss_scale 4096.0000 (2986.7235)	mem 19785MB
[2024-08-01 16:10:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:09:47 lr 0.000012	 wd 0.0500	time 0.6131 (0.6519)	loss 0.9144 (1.1280)	grad_norm 2.8276 (2.4769)	loss_scale 4096.0000 (3056.0100)	mem 19785MB
[2024-08-01 16:11:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:08:42 lr 0.000012	 wd 0.0500	time 0.6138 (0.6513)	loss 0.7731 (1.1283)	grad_norm 2.8892 (2.4961)	loss_scale 4096.0000 (3117.1499)	mem 19785MB
[2024-08-01 16:12:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:07:36 lr 0.000012	 wd 0.0500	time 0.6032 (0.6509)	loss 1.6043 (1.1269)	grad_norm 1.6971 (2.4757)	loss_scale 4096.0000 (3171.5003)	mem 19785MB
[2024-08-01 16:13:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:06:31 lr 0.000012	 wd 0.0500	time 0.6134 (0.6505)	loss 0.8916 (1.1260)	grad_norm 2.2246 (2.4777)	loss_scale 4096.0000 (3220.1326)	mem 19785MB
[2024-08-01 16:14:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:05:26 lr 0.000012	 wd 0.0500	time 0.6149 (0.6501)	loss 0.8992 (1.1271)	grad_norm 2.6508 (2.4845)	loss_scale 4096.0000 (3263.9040)	mem 19785MB
[2024-08-01 16:16:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:04:21 lr 0.000012	 wd 0.0500	time 0.6098 (0.6499)	loss 0.7088 (1.1256)	grad_norm 2.0348 (2.4770)	loss_scale 4096.0000 (3303.5088)	mem 19785MB
[2024-08-01 16:17:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:03:16 lr 0.000012	 wd 0.0500	time 0.5806 (0.6497)	loss 0.8865 (1.1251)	grad_norm 1.9200 (2.4650)	loss_scale 4096.0000 (3339.5148)	mem 19785MB
[2024-08-01 16:18:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:02:11 lr 0.000012	 wd 0.0500	time 0.6413 (0.6495)	loss 0.9717 (1.1256)	grad_norm 2.5281 (2.4657)	loss_scale 4096.0000 (3372.3911)	mem 19785MB
[2024-08-01 16:19:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:06 lr 0.000012	 wd 0.0500	time 0.6022 (0.6492)	loss 1.3143 (1.1259)	grad_norm 1.5523 (nan)	loss_scale 2048.0000 (3322.3490)	mem 19785MB
[2024-08-01 16:20:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.6118 (0.6489)	loss 1.4187 (1.1267)	grad_norm 1.8487 (nan)	loss_scale 2048.0000 (3271.3954)	mem 19785MB
[2024-08-01 16:20:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 15 training takes 0:27:06
[2024-08-01 16:20:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_15.pth saving......
[2024-08-01 16:20:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_15.pth saved !!!
[2024-08-01 16:20:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.465 (12.465)	Loss 0.5020 (0.5020)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 16:20:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.708 Acc@5 97.980
[2024-08-01 16:20:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-01 16:20:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.71%
[2024-08-01 16:20:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 16:20:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 16:21:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:15:48 lr 0.000012	 wd 0.0500	time 11.8899 (11.8899)	loss 1.1922 (1.1922)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 16:22:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:30:08 lr 0.000012	 wd 0.0500	time 0.6113 (0.7531)	loss 1.0841 (1.1359)	grad_norm 2.3309 (2.7952)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 16:23:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:26:45 lr 0.000012	 wd 0.0500	time 0.6150 (0.6976)	loss 1.1841 (1.1238)	grad_norm 1.6419 (2.7299)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 16:24:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:24:55 lr 0.000012	 wd 0.0500	time 0.5985 (0.6792)	loss 1.0188 (1.1177)	grad_norm 2.9065 (nan)	loss_scale 1024.0000 (1864.2924)	mem 19785MB
[2024-08-01 16:25:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:23:29 lr 0.000012	 wd 0.0500	time 0.6026 (0.6705)	loss 0.8211 (1.1075)	grad_norm 2.0866 (nan)	loss_scale 1024.0000 (1654.7431)	mem 19785MB
[2024-08-01 16:26:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:22:12 lr 0.000012	 wd 0.0500	time 0.6062 (0.6657)	loss 1.3844 (1.1118)	grad_norm 2.0119 (nan)	loss_scale 1024.0000 (1528.8463)	mem 19785MB
[2024-08-01 16:27:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:20:59 lr 0.000012	 wd 0.0500	time 0.6223 (0.6621)	loss 1.3658 (1.1107)	grad_norm 1.8295 (nan)	loss_scale 1024.0000 (1444.8453)	mem 19785MB
[2024-08-01 16:28:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:19:48 lr 0.000012	 wd 0.0500	time 0.6144 (0.6594)	loss 1.0003 (1.1128)	grad_norm 2.2321 (nan)	loss_scale 1024.0000 (1384.8103)	mem 19785MB
[2024-08-01 16:29:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:18:38 lr 0.000012	 wd 0.0500	time 0.6071 (0.6574)	loss 1.2973 (1.1143)	grad_norm 1.6988 (nan)	loss_scale 1024.0000 (1339.7653)	mem 19785MB
[2024-08-01 16:30:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:17:30 lr 0.000012	 wd 0.0500	time 0.6184 (0.6557)	loss 1.0331 (1.1126)	grad_norm 1.5143 (nan)	loss_scale 1024.0000 (1304.7192)	mem 19785MB
[2024-08-01 16:31:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:16:22 lr 0.000011	 wd 0.0500	time 0.6028 (0.6544)	loss 1.2873 (1.1137)	grad_norm 1.9669 (nan)	loss_scale 1024.0000 (1276.6753)	mem 19785MB
[2024-08-01 16:32:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:15:16 lr 0.000011	 wd 0.0500	time 0.6148 (0.6534)	loss 0.8824 (1.1190)	grad_norm 1.5688 (nan)	loss_scale 1024.0000 (1253.7257)	mem 19785MB
[2024-08-01 16:34:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:14:09 lr 0.000011	 wd 0.0500	time 0.6148 (0.6526)	loss 0.8737 (1.1184)	grad_norm 2.1529 (nan)	loss_scale 1024.0000 (1234.5978)	mem 19785MB
[2024-08-01 16:35:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:13:03 lr 0.000011	 wd 0.0500	time 0.6085 (0.6519)	loss 1.2388 (1.1215)	grad_norm 1.5923 (nan)	loss_scale 1024.0000 (1218.4105)	mem 19785MB
[2024-08-01 16:36:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:11:57 lr 0.000011	 wd 0.0500	time 0.6140 (0.6513)	loss 1.1911 (1.1221)	grad_norm 2.3137 (nan)	loss_scale 1024.0000 (1204.5339)	mem 19785MB
[2024-08-01 16:37:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:10:52 lr 0.000011	 wd 0.0500	time 0.6131 (0.6510)	loss 1.1640 (1.1240)	grad_norm 1.8848 (nan)	loss_scale 1024.0000 (1192.5063)	mem 19785MB
[2024-08-01 16:38:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:09:46 lr 0.000011	 wd 0.0500	time 0.6123 (0.6505)	loss 1.2161 (1.1243)	grad_norm 3.5873 (nan)	loss_scale 1024.0000 (1181.9813)	mem 19785MB
[2024-08-01 16:39:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:08:41 lr 0.000011	 wd 0.0500	time 0.6144 (0.6502)	loss 1.0254 (1.1238)	grad_norm 1.9280 (nan)	loss_scale 1024.0000 (1172.6937)	mem 19785MB
[2024-08-01 16:40:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:07:36 lr 0.000011	 wd 0.0500	time 0.6055 (0.6499)	loss 0.8864 (1.1227)	grad_norm 1.6043 (nan)	loss_scale 1024.0000 (1164.4375)	mem 19785MB
[2024-08-01 16:41:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:06:31 lr 0.000011	 wd 0.0500	time 0.6146 (0.6496)	loss 0.8395 (1.1234)	grad_norm 2.2960 (nan)	loss_scale 1024.0000 (1157.0500)	mem 19785MB
[2024-08-01 16:42:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:05:25 lr 0.000011	 wd 0.0500	time 0.6230 (0.6493)	loss 1.2879 (1.1239)	grad_norm 1.8780 (nan)	loss_scale 1024.0000 (1150.4008)	mem 19785MB
[2024-08-01 16:43:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:04:20 lr 0.000011	 wd 0.0500	time 0.6133 (0.6491)	loss 1.2170 (1.1246)	grad_norm 2.0530 (nan)	loss_scale 1024.0000 (1144.3846)	mem 19785MB
[2024-08-01 16:44:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:03:15 lr 0.000011	 wd 0.0500	time 0.6150 (0.6488)	loss 0.8947 (1.1244)	grad_norm 2.1296 (nan)	loss_scale 1024.0000 (1138.9150)	mem 19785MB
[2024-08-01 16:45:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:02:11 lr 0.000011	 wd 0.0500	time 0.6064 (0.6487)	loss 0.7846 (1.1227)	grad_norm 5.5456 (nan)	loss_scale 1024.0000 (1133.9209)	mem 19785MB
[2024-08-01 16:46:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:06 lr 0.000011	 wd 0.0500	time 0.6167 (0.6484)	loss 1.1954 (1.1221)	grad_norm 1.5642 (nan)	loss_scale 1024.0000 (1129.3428)	mem 19785MB
[2024-08-01 16:47:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.6116 (0.6482)	loss 0.8033 (1.1218)	grad_norm 2.5469 (nan)	loss_scale 1024.0000 (1125.1307)	mem 19785MB
[2024-08-01 16:48:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 16 training takes 0:27:04
[2024-08-01 16:48:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.651 (11.651)	Loss 0.4766 (0.4766)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 16:48:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.738 Acc@5 98.006
[2024-08-01 16:48:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-01 16:48:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.74%
[2024-08-01 16:48:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 16:48:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 16:48:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:11:41 lr 0.000011	 wd 0.0500	time 11.7913 (11.7913)	loss 1.2267 (1.2267)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:49:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:30:05 lr 0.000011	 wd 0.0500	time 0.6151 (0.7518)	loss 1.2151 (1.0990)	grad_norm 1.8498 (2.1659)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:50:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:26:46 lr 0.000011	 wd 0.0500	time 0.6024 (0.6979)	loss 1.3624 (1.1039)	grad_norm 1.6514 (2.1792)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:52:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:24:56 lr 0.000011	 wd 0.0500	time 0.6119 (0.6794)	loss 1.3917 (1.1179)	grad_norm 2.3301 (2.2120)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:53:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:23:29 lr 0.000011	 wd 0.0500	time 0.6163 (0.6706)	loss 1.5002 (1.1135)	grad_norm 2.0538 (2.4426)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:54:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:22:12 lr 0.000010	 wd 0.0500	time 0.6110 (0.6654)	loss 1.3168 (1.1197)	grad_norm 2.3929 (2.4264)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:55:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:20:58 lr 0.000010	 wd 0.0500	time 0.6067 (0.6618)	loss 0.8580 (1.1211)	grad_norm 1.6688 (2.4285)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:56:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:19:47 lr 0.000010	 wd 0.0500	time 0.5996 (0.6592)	loss 1.3074 (1.1184)	grad_norm 2.5091 (2.4498)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:57:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:18:38 lr 0.000010	 wd 0.0500	time 0.6177 (0.6572)	loss 1.1838 (1.1182)	grad_norm 2.1122 (2.4474)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:58:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:17:31 lr 0.000010	 wd 0.0500	time 0.6228 (0.6561)	loss 1.2810 (1.1200)	grad_norm 2.0998 (2.4342)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 16:59:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:16:23 lr 0.000010	 wd 0.0500	time 0.6318 (0.6549)	loss 0.9527 (1.1187)	grad_norm 1.5909 (2.4415)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:00:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:15:16 lr 0.000010	 wd 0.0500	time 0.6138 (0.6538)	loss 0.9575 (1.1232)	grad_norm 2.6059 (2.4332)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:01:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:14:10 lr 0.000010	 wd 0.0500	time 0.6081 (0.6530)	loss 0.8363 (1.1233)	grad_norm 3.2936 (2.4571)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:02:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:13:04 lr 0.000010	 wd 0.0500	time 0.6071 (0.6523)	loss 1.4565 (1.1220)	grad_norm 1.7258 (2.4268)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:03:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:11:58 lr 0.000010	 wd 0.0500	time 0.6020 (0.6517)	loss 1.3899 (1.1220)	grad_norm 1.6964 (2.4123)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:04:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:10:52 lr 0.000010	 wd 0.0500	time 0.6158 (0.6511)	loss 1.2659 (1.1222)	grad_norm 2.0703 (2.3979)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:05:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:09:46 lr 0.000010	 wd 0.0500	time 0.6138 (0.6507)	loss 1.3002 (1.1206)	grad_norm 2.3366 (2.3957)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:07:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:08:41 lr 0.000010	 wd 0.0500	time 0.6092 (0.6503)	loss 1.4802 (1.1225)	grad_norm 1.8469 (2.3817)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 17:08:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:07:36 lr 0.000010	 wd 0.0500	time 0.6185 (0.6500)	loss 0.9525 (1.1225)	grad_norm 3.7090 (2.3871)	loss_scale 2048.0000 (1055.8401)	mem 19785MB
[2024-08-01 17:09:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:06:31 lr 0.000010	 wd 0.0500	time 0.6167 (0.6498)	loss 1.1840 (1.1215)	grad_norm 2.2140 (2.3807)	loss_scale 2048.0000 (1108.0316)	mem 19785MB
[2024-08-01 17:10:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:05:26 lr 0.000010	 wd 0.0500	time 0.6193 (0.6495)	loss 0.9619 (1.1224)	grad_norm 2.4069 (2.3835)	loss_scale 2048.0000 (1155.0065)	mem 19785MB
[2024-08-01 17:11:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:04:21 lr 0.000010	 wd 0.0500	time 0.6038 (0.6493)	loss 1.4106 (1.1224)	grad_norm 2.2333 (2.3815)	loss_scale 2048.0000 (1197.5098)	mem 19785MB
[2024-08-01 17:12:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:03:15 lr 0.000010	 wd 0.0500	time 0.6065 (0.6490)	loss 1.4570 (1.1230)	grad_norm 1.6115 (2.3877)	loss_scale 2048.0000 (1236.1508)	mem 19785MB
[2024-08-01 17:13:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:02:11 lr 0.000010	 wd 0.0500	time 0.6306 (0.6487)	loss 0.8211 (1.1226)	grad_norm 2.3707 (2.3815)	loss_scale 2048.0000 (1271.4333)	mem 19785MB
[2024-08-01 17:14:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:06 lr 0.000010	 wd 0.0500	time 0.6147 (0.6485)	loss 0.8459 (1.1212)	grad_norm 1.6977 (2.3677)	loss_scale 2048.0000 (1303.7768)	mem 19785MB
[2024-08-01 17:15:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.6094 (0.6483)	loss 1.4579 (1.1212)	grad_norm 2.1909 (2.3615)	loss_scale 2048.0000 (1333.5338)	mem 19785MB
[2024-08-01 17:15:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 17 training takes 0:27:04
[2024-08-01 17:15:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.216 (12.216)	Loss 0.4668 (0.4668)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 17:16:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.736 Acc@5 98.036
[2024-08-01 17:16:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-01 17:16:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.74%
[2024-08-01 17:16:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][0/2502]	eta 9:04:46 lr 0.000009	 wd 0.0500	time 13.0643 (13.0643)	loss 1.4558 (1.4558)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:17:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:30:40 lr 0.000009	 wd 0.0500	time 0.6147 (0.7661)	loss 1.4469 (1.1308)	grad_norm 1.5805 (2.0577)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:18:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:27:05 lr 0.000009	 wd 0.0500	time 0.6119 (0.7062)	loss 0.9521 (1.1331)	grad_norm 2.5441 (2.0769)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:19:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:25:11 lr 0.000009	 wd 0.0500	time 0.6279 (0.6862)	loss 1.5635 (1.1342)	grad_norm 2.0531 (2.1952)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:20:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:23:40 lr 0.000009	 wd 0.0500	time 0.5968 (0.6756)	loss 0.7221 (1.1344)	grad_norm 1.7306 (2.1959)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:21:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:22:19 lr 0.000009	 wd 0.0500	time 0.6142 (0.6692)	loss 1.3337 (1.1308)	grad_norm 1.9198 (2.2399)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:22:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:21:04 lr 0.000009	 wd 0.0500	time 0.6103 (0.6649)	loss 1.5023 (1.1248)	grad_norm 1.6968 (2.2130)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:24:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:19:52 lr 0.000009	 wd 0.0500	time 0.6091 (0.6618)	loss 1.3555 (1.1276)	grad_norm 1.6832 (2.2264)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:25:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:18:42 lr 0.000009	 wd 0.0500	time 0.6123 (0.6596)	loss 1.3277 (1.1317)	grad_norm 4.4879 (2.3152)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:26:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:17:33 lr 0.000009	 wd 0.0500	time 0.6094 (0.6578)	loss 0.9394 (1.1298)	grad_norm 1.5806 (2.3245)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:27:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:16:25 lr 0.000009	 wd 0.0500	time 0.6151 (0.6564)	loss 1.3673 (1.1278)	grad_norm 2.3990 (2.3313)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:28:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:15:18 lr 0.000009	 wd 0.0500	time 0.6067 (0.6551)	loss 0.9776 (1.1292)	grad_norm 2.0299 (2.3367)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:29:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:14:12 lr 0.000009	 wd 0.0500	time 0.6068 (0.6544)	loss 0.9557 (1.1239)	grad_norm 2.2090 (2.4303)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:30:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:13:05 lr 0.000009	 wd 0.0500	time 0.6406 (0.6535)	loss 1.4452 (1.1237)	grad_norm 4.8907 (2.4402)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:31:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:11:59 lr 0.000009	 wd 0.0500	time 0.6030 (0.6527)	loss 1.2450 (1.1244)	grad_norm 2.2642 (2.4168)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:32:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:10:53 lr 0.000009	 wd 0.0500	time 0.6114 (0.6520)	loss 1.3407 (1.1245)	grad_norm 2.0709 (2.4446)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:33:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:09:47 lr 0.000009	 wd 0.0500	time 0.6109 (0.6514)	loss 1.2786 (1.1260)	grad_norm 7.4705 (2.4514)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:34:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:08:42 lr 0.000009	 wd 0.0500	time 0.6225 (0.6509)	loss 1.2408 (1.1274)	grad_norm 1.6331 (2.4536)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:35:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:07:36 lr 0.000009	 wd 0.0500	time 0.6135 (0.6505)	loss 0.7448 (1.1277)	grad_norm 2.2696 (2.4339)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:36:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:06:31 lr 0.000009	 wd 0.0500	time 0.6159 (0.6501)	loss 1.4753 (1.1277)	grad_norm 1.8351 (2.4366)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:37:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:05:26 lr 0.000008	 wd 0.0500	time 0.6149 (0.6498)	loss 0.9713 (1.1270)	grad_norm 2.1651 (2.4742)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:39:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:04:21 lr 0.000008	 wd 0.0500	time 0.6299 (0.6495)	loss 1.2608 (1.1267)	grad_norm 2.5786 (2.4751)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:40:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:03:16 lr 0.000008	 wd 0.0500	time 0.6193 (0.6493)	loss 1.0463 (1.1268)	grad_norm 1.8518 (2.4681)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:41:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:02:11 lr 0.000008	 wd 0.0500	time 0.6147 (0.6491)	loss 1.3258 (1.1278)	grad_norm 3.8009 (2.4621)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:42:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:06 lr 0.000008	 wd 0.0500	time 0.6151 (0.6489)	loss 1.3722 (1.1281)	grad_norm 2.4402 (2.4887)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:43:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.6139 (0.6486)	loss 1.1257 (1.1283)	grad_norm 2.0427 (2.4790)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:43:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 18 training takes 0:27:05
[2024-08-01 17:43:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 13.111 (13.111)	Loss 0.4795 (0.4795)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 17:43:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.756 Acc@5 98.004
[2024-08-01 17:43:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 17:43:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.76%
[2024-08-01 17:43:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 17:43:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 17:44:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:01:54 lr 0.000008	 wd 0.0500	time 11.5567 (11.5567)	loss 0.8396 (0.8396)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:45:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:30:02 lr 0.000008	 wd 0.0500	time 0.6092 (0.7505)	loss 1.5322 (1.1676)	grad_norm 3.5104 (2.2806)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:46:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:26:44 lr 0.000008	 wd 0.0500	time 0.6146 (0.6970)	loss 0.8663 (1.1381)	grad_norm 2.0362 (2.3395)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:47:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:24:55 lr 0.000008	 wd 0.0500	time 0.6090 (0.6792)	loss 1.3777 (1.1274)	grad_norm 3.3734 (2.2825)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:48:25 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:23:29 lr 0.000008	 wd 0.0500	time 0.6178 (0.6705)	loss 0.9741 (1.1187)	grad_norm 2.1221 (2.3207)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:49:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:22:12 lr 0.000008	 wd 0.0500	time 0.6268 (0.6656)	loss 1.3615 (1.1264)	grad_norm 2.4083 (2.3123)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:50:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:21:00 lr 0.000008	 wd 0.0500	time 0.6159 (0.6626)	loss 0.8606 (1.1187)	grad_norm 2.1555 (2.3397)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:51:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:19:49 lr 0.000008	 wd 0.0500	time 0.6307 (0.6600)	loss 1.0979 (1.1177)	grad_norm 1.6999 (2.3481)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 17:52:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:18:39 lr 0.000008	 wd 0.0500	time 0.6160 (0.6580)	loss 1.3573 (1.1188)	grad_norm 2.2431 (2.3566)	loss_scale 4096.0000 (2201.4082)	mem 19785MB
[2024-08-01 17:53:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:17:31 lr 0.000008	 wd 0.0500	time 0.6106 (0.6564)	loss 1.1558 (1.1198)	grad_norm 2.2222 (2.3856)	loss_scale 4096.0000 (2411.6848)	mem 19785MB
[2024-08-01 17:54:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:16:23 lr 0.000008	 wd 0.0500	time 0.6288 (0.6551)	loss 1.3306 (1.1202)	grad_norm 2.0073 (2.4149)	loss_scale 4096.0000 (2579.9481)	mem 19785MB
[2024-08-01 17:55:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:15:17 lr 0.000008	 wd 0.0500	time 0.6455 (0.6541)	loss 1.1028 (1.1184)	grad_norm 1.6410 (2.4493)	loss_scale 4096.0000 (2717.6458)	mem 19785MB
[2024-08-01 17:57:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:14:10 lr 0.000008	 wd 0.0500	time 0.5906 (0.6533)	loss 1.3138 (1.1169)	grad_norm 1.9459 (2.4758)	loss_scale 4096.0000 (2832.4130)	mem 19785MB
[2024-08-01 17:58:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:13:04 lr 0.000008	 wd 0.0500	time 0.6136 (0.6526)	loss 0.8929 (1.1165)	grad_norm 2.3454 (2.4505)	loss_scale 4096.0000 (2929.5373)	mem 19785MB
[2024-08-01 17:59:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:11:58 lr 0.000008	 wd 0.0500	time 0.6283 (0.6520)	loss 0.8685 (1.1164)	grad_norm 2.1679 (2.4564)	loss_scale 4096.0000 (3012.7966)	mem 19785MB
[2024-08-01 18:00:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:10:52 lr 0.000008	 wd 0.0500	time 0.6154 (0.6516)	loss 1.4406 (1.1173)	grad_norm 2.1318 (2.4605)	loss_scale 4096.0000 (3084.9620)	mem 19785MB
[2024-08-01 18:01:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:09:47 lr 0.000007	 wd 0.0500	time 0.5806 (0.6511)	loss 1.1819 (1.1189)	grad_norm 1.7223 (2.4524)	loss_scale 4096.0000 (3148.1124)	mem 19785MB
[2024-08-01 18:02:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:08:41 lr 0.000007	 wd 0.0500	time 0.6016 (0.6506)	loss 0.7676 (1.1193)	grad_norm 1.7265 (2.4435)	loss_scale 4096.0000 (3203.8377)	mem 19785MB
[2024-08-01 18:03:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:07:36 lr 0.000007	 wd 0.0500	time 0.6160 (0.6503)	loss 1.2329 (1.1212)	grad_norm 2.1223 (2.4545)	loss_scale 4096.0000 (3253.3748)	mem 19785MB
[2024-08-01 18:04:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:06:31 lr 0.000007	 wd 0.0500	time 0.6077 (0.6499)	loss 1.1226 (1.1198)	grad_norm 1.9607 (inf)	loss_scale 2048.0000 (3194.2767)	mem 19785MB
[2024-08-01 18:05:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:05:26 lr 0.000007	 wd 0.0500	time 0.6075 (0.6496)	loss 1.2584 (1.1208)	grad_norm 2.0517 (inf)	loss_scale 2048.0000 (3136.9915)	mem 19785MB
[2024-08-01 18:06:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:04:21 lr 0.000007	 wd 0.0500	time 0.6135 (0.6493)	loss 1.5810 (1.1228)	grad_norm 1.6642 (inf)	loss_scale 2048.0000 (3085.1594)	mem 19785MB
[2024-08-01 18:07:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:03:16 lr 0.000007	 wd 0.0500	time 0.6173 (0.6491)	loss 1.2182 (1.1220)	grad_norm 2.0597 (inf)	loss_scale 2048.0000 (3038.0373)	mem 19785MB
[2024-08-01 18:08:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:02:11 lr 0.000007	 wd 0.0500	time 0.6240 (0.6488)	loss 0.9625 (1.1217)	grad_norm 2.3591 (inf)	loss_scale 2048.0000 (2995.0109)	mem 19785MB
[2024-08-01 18:09:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:06 lr 0.000007	 wd 0.0500	time 0.6121 (0.6486)	loss 1.0692 (1.1210)	grad_norm 2.8152 (inf)	loss_scale 2048.0000 (2955.5685)	mem 19785MB
[2024-08-01 18:10:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.6139 (0.6485)	loss 0.8000 (1.1209)	grad_norm 1.8829 (inf)	loss_scale 2048.0000 (2919.2803)	mem 19785MB
[2024-08-01 18:11:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 19 training takes 0:27:05
[2024-08-01 18:11:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.061 (12.061)	Loss 0.4580 (0.4580)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 18:11:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.770 Acc@5 98.046
[2024-08-01 18:11:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 18:11:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.77%
[2024-08-01 18:11:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 18:11:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 18:11:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:59:14 lr 0.000007	 wd 0.0500	time 11.4925 (11.4925)	loss 1.3201 (1.3201)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:12:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:29:57 lr 0.000007	 wd 0.0500	time 0.6159 (0.7485)	loss 0.9177 (1.1277)	grad_norm 2.9209 (2.4686)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:13:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:26:41 lr 0.000007	 wd 0.0500	time 0.6057 (0.6955)	loss 1.2664 (1.1167)	grad_norm 11.1507 (2.7402)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:15:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:24:53 lr 0.000007	 wd 0.0500	time 0.6029 (0.6784)	loss 0.7059 (1.1268)	grad_norm 2.2912 (2.6249)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:16:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:23:27 lr 0.000007	 wd 0.0500	time 0.6247 (0.6695)	loss 1.2566 (1.1239)	grad_norm 1.9673 (2.6169)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:17:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:22:09 lr 0.000007	 wd 0.0500	time 0.6163 (0.6639)	loss 1.2552 (1.1230)	grad_norm 1.9793 (2.6561)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:18:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:20:56 lr 0.000007	 wd 0.0500	time 0.5876 (0.6608)	loss 1.3398 (1.1271)	grad_norm 3.8477 (2.5976)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:19:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:19:45 lr 0.000007	 wd 0.0500	time 0.5932 (0.6581)	loss 1.2458 (1.1255)	grad_norm 1.8507 (2.5529)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:20:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:18:37 lr 0.000007	 wd 0.0500	time 0.6171 (0.6564)	loss 1.1261 (1.1272)	grad_norm 1.5305 (2.5309)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:21:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:17:29 lr 0.000007	 wd 0.0500	time 0.6120 (0.6554)	loss 1.3263 (1.1255)	grad_norm 1.6890 (2.5520)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:22:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:16:22 lr 0.000007	 wd 0.0500	time 0.6104 (0.6543)	loss 1.2116 (1.1272)	grad_norm 1.7193 (2.5333)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:23:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:15:16 lr 0.000007	 wd 0.0500	time 0.6008 (0.6535)	loss 0.8946 (1.1251)	grad_norm 2.7674 (2.5039)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:24:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:14:09 lr 0.000006	 wd 0.0500	time 0.6403 (0.6527)	loss 1.2118 (1.1221)	grad_norm 1.6840 (2.4759)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:25:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:13:03 lr 0.000006	 wd 0.0500	time 0.6199 (0.6520)	loss 1.3493 (1.1220)	grad_norm 2.6173 (2.4701)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:26:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:11:57 lr 0.000006	 wd 0.0500	time 0.6200 (0.6513)	loss 1.0393 (1.1239)	grad_norm 2.9188 (2.4502)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:27:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:10:52 lr 0.000006	 wd 0.0500	time 0.6078 (0.6508)	loss 1.3440 (1.1254)	grad_norm 3.3017 (2.5098)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:28:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:09:46 lr 0.000006	 wd 0.0500	time 0.6090 (0.6504)	loss 1.2055 (1.1272)	grad_norm 2.4194 (2.5021)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:30:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:08:41 lr 0.000006	 wd 0.0500	time 0.6125 (0.6500)	loss 1.4882 (1.1278)	grad_norm 1.9584 (2.4994)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:31:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:07:36 lr 0.000006	 wd 0.0500	time 0.6212 (0.6499)	loss 0.7828 (1.1258)	grad_norm 1.9168 (2.4828)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:32:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:06:31 lr 0.000006	 wd 0.0500	time 0.5992 (0.6497)	loss 0.7932 (1.1263)	grad_norm 4.3328 (2.4850)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:33:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:05:25 lr 0.000006	 wd 0.0500	time 0.6102 (0.6494)	loss 1.1010 (1.1258)	grad_norm 2.9999 (2.4863)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:34:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:04:20 lr 0.000006	 wd 0.0500	time 0.6037 (0.6492)	loss 1.2858 (1.1262)	grad_norm 1.6212 (2.4689)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:35:25 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:03:16 lr 0.000006	 wd 0.0500	time 0.6162 (0.6491)	loss 1.3242 (1.1275)	grad_norm 2.0490 (2.4745)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:36:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:02:11 lr 0.000006	 wd 0.0500	time 0.6125 (0.6489)	loss 0.8008 (1.1289)	grad_norm 1.7702 (2.4626)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:37:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:06 lr 0.000006	 wd 0.0500	time 0.6226 (0.6487)	loss 0.7293 (1.1284)	grad_norm 1.7863 (2.4562)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:38:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.6277 (0.6485)	loss 1.0924 (1.1302)	grad_norm 4.3446 (2.4468)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:38:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 20 training takes 0:27:06
[2024-08-01 18:38:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.876 (11.876)	Loss 0.4712 (0.4712)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 19785MB
[2024-08-01 18:39:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.756 Acc@5 98.056
[2024-08-01 18:39:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 18:39:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.77%
[2024-08-01 18:39:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:51:49 lr 0.000006	 wd 0.0500	time 12.7537 (12.7537)	loss 0.8865 (0.8865)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:40:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:30:32 lr 0.000006	 wd 0.0500	time 0.6170 (0.7629)	loss 1.3104 (1.1304)	grad_norm 1.7692 (2.0780)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:41:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:27:04 lr 0.000006	 wd 0.0500	time 0.6209 (0.7058)	loss 1.2694 (1.1337)	grad_norm 1.8763 (2.4405)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:42:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:25:10 lr 0.000006	 wd 0.0500	time 0.6155 (0.6860)	loss 1.0787 (1.1293)	grad_norm 2.3547 (2.4150)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:43:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:23:40 lr 0.000006	 wd 0.0500	time 0.6185 (0.6756)	loss 1.4282 (1.1198)	grad_norm 2.1583 (2.4446)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:44:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:22:19 lr 0.000006	 wd 0.0500	time 0.6089 (0.6692)	loss 0.7969 (1.1154)	grad_norm 4.2224 (2.4312)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 18:45:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:21:04 lr 0.000006	 wd 0.0500	time 0.6426 (0.6650)	loss 1.2748 (1.1113)	grad_norm 2.8943 (nan)	loss_scale 1024.0000 (1891.2479)	mem 19785MB
[2024-08-01 18:47:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:19:52 lr 0.000006	 wd 0.0500	time 0.6322 (0.6620)	loss 1.2263 (1.1137)	grad_norm 1.9705 (nan)	loss_scale 1024.0000 (1767.5321)	mem 19785MB
[2024-08-01 18:48:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:18:43 lr 0.000006	 wd 0.0500	time 0.6093 (0.6599)	loss 1.2726 (1.1127)	grad_norm 2.0033 (nan)	loss_scale 1024.0000 (1674.7066)	mem 19785MB
[2024-08-01 18:49:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:17:34 lr 0.000005	 wd 0.0500	time 0.6176 (0.6582)	loss 1.3352 (1.1166)	grad_norm 2.5830 (nan)	loss_scale 1024.0000 (1602.4861)	mem 19785MB
[2024-08-01 18:50:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:16:26 lr 0.000005	 wd 0.0500	time 0.6192 (0.6569)	loss 1.3375 (1.1202)	grad_norm 1.5596 (nan)	loss_scale 1024.0000 (1544.6953)	mem 19785MB
[2024-08-01 18:51:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:15:19 lr 0.000005	 wd 0.0500	time 0.6161 (0.6558)	loss 0.8011 (1.1237)	grad_norm 2.2124 (nan)	loss_scale 1024.0000 (1497.4024)	mem 19785MB
[2024-08-01 18:52:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:14:13 lr 0.000005	 wd 0.0500	time 0.5863 (0.6552)	loss 1.2664 (1.1203)	grad_norm 2.0033 (nan)	loss_scale 1024.0000 (1457.9850)	mem 19785MB
[2024-08-01 18:53:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:13:06 lr 0.000005	 wd 0.0500	time 0.6108 (0.6543)	loss 1.2826 (1.1204)	grad_norm 1.6013 (nan)	loss_scale 1024.0000 (1424.6272)	mem 19785MB
[2024-08-01 18:54:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:12:00 lr 0.000005	 wd 0.0500	time 0.6395 (0.6535)	loss 0.7770 (1.1183)	grad_norm 2.6125 (nan)	loss_scale 1024.0000 (1396.0314)	mem 19785MB
[2024-08-01 18:55:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:10:54 lr 0.000005	 wd 0.0500	time 0.6082 (0.6530)	loss 0.7860 (1.1197)	grad_norm 3.7916 (nan)	loss_scale 1024.0000 (1371.2458)	mem 19785MB
[2024-08-01 18:56:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:09:48 lr 0.000005	 wd 0.0500	time 0.6149 (0.6525)	loss 1.1062 (1.1165)	grad_norm 6.3536 (nan)	loss_scale 1024.0000 (1349.5565)	mem 19785MB
[2024-08-01 18:57:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:08:42 lr 0.000005	 wd 0.0500	time 0.6079 (0.6520)	loss 1.3300 (1.1164)	grad_norm 1.7857 (nan)	loss_scale 1024.0000 (1330.4174)	mem 19785MB
[2024-08-01 18:58:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:07:37 lr 0.000005	 wd 0.0500	time 0.5797 (0.6516)	loss 0.8308 (1.1167)	grad_norm 2.6025 (nan)	loss_scale 1024.0000 (1313.4037)	mem 19785MB
[2024-08-01 18:59:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:06:32 lr 0.000005	 wd 0.0500	time 0.6271 (0.6513)	loss 0.7126 (1.1182)	grad_norm 1.7664 (nan)	loss_scale 1024.0000 (1298.1799)	mem 19785MB
[2024-08-01 19:00:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:05:26 lr 0.000005	 wd 0.0500	time 0.5997 (0.6509)	loss 1.1029 (1.1177)	grad_norm 2.5078 (nan)	loss_scale 1024.0000 (1284.4778)	mem 19785MB
[2024-08-01 19:02:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:04:21 lr 0.000005	 wd 0.0500	time 0.6330 (0.6507)	loss 1.2081 (1.1173)	grad_norm 6.5632 (nan)	loss_scale 1024.0000 (1272.0800)	mem 19785MB
[2024-08-01 19:03:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:03:16 lr 0.000005	 wd 0.0500	time 0.6233 (0.6505)	loss 1.3813 (1.1157)	grad_norm 1.9450 (nan)	loss_scale 1024.0000 (1260.8087)	mem 19785MB
[2024-08-01 19:04:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:02:11 lr 0.000005	 wd 0.0500	time 0.6121 (0.6503)	loss 1.3744 (1.1178)	grad_norm 2.1303 (nan)	loss_scale 1024.0000 (1250.5172)	mem 19785MB
[2024-08-01 19:05:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:06 lr 0.000005	 wd 0.0500	time 0.6202 (0.6501)	loss 1.0742 (1.1188)	grad_norm 3.3779 (nan)	loss_scale 1024.0000 (1241.0829)	mem 19785MB
[2024-08-01 19:06:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.5765 (0.6499)	loss 1.3951 (1.1183)	grad_norm 8.9105 (nan)	loss_scale 1024.0000 (1232.4030)	mem 19785MB
[2024-08-01 19:06:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 21 training takes 0:27:10
[2024-08-01 19:06:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.332 (12.332)	Loss 0.4873 (0.4873)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 19:07:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.706 Acc@5 98.058
[2024-08-01 19:07:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-01 19:07:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.77%
[2024-08-01 19:07:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:28:18 lr 0.000005	 wd 0.0500	time 10.7508 (10.7508)	loss 0.9946 (0.9946)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:08:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:30:11 lr 0.000005	 wd 0.0500	time 0.6122 (0.7541)	loss 1.2850 (1.1277)	grad_norm 1.8692 (2.4990)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:09:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:26:48 lr 0.000005	 wd 0.0500	time 0.6134 (0.6989)	loss 0.6570 (1.1392)	grad_norm 2.1699 (2.4695)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:10:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:24:59 lr 0.000005	 wd 0.0500	time 0.6170 (0.6811)	loss 1.4881 (1.1407)	grad_norm 2.2604 (2.5414)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:11:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:23:34 lr 0.000005	 wd 0.0500	time 0.6205 (0.6727)	loss 0.7520 (1.1409)	grad_norm 1.7634 (2.6085)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:12:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:22:16 lr 0.000005	 wd 0.0500	time 0.5881 (0.6674)	loss 1.0643 (1.1370)	grad_norm 3.2703 (2.5990)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:13:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:21:02 lr 0.000005	 wd 0.0500	time 0.6152 (0.6637)	loss 0.8552 (1.1366)	grad_norm 1.7644 (2.5526)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:14:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:19:50 lr 0.000005	 wd 0.0500	time 0.6123 (0.6609)	loss 1.3783 (1.1307)	grad_norm 2.0837 (2.5232)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:15:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:18:41 lr 0.000004	 wd 0.0500	time 0.6072 (0.6587)	loss 1.0183 (1.1306)	grad_norm 1.8184 (2.5343)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:16:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:17:32 lr 0.000004	 wd 0.0500	time 0.6203 (0.6571)	loss 1.1038 (1.1306)	grad_norm 2.2251 (2.5012)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:17:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:16:24 lr 0.000004	 wd 0.0500	time 0.6102 (0.6556)	loss 1.3359 (1.1311)	grad_norm 2.2906 (2.4992)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:19:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:15:17 lr 0.000004	 wd 0.0500	time 0.6071 (0.6546)	loss 0.7597 (1.1271)	grad_norm 1.5844 (2.4954)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:20:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:14:11 lr 0.000004	 wd 0.0500	time 0.6066 (0.6536)	loss 1.2722 (1.1279)	grad_norm 2.1429 (2.4605)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:21:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:13:04 lr 0.000004	 wd 0.0500	time 0.6118 (0.6530)	loss 0.8075 (1.1220)	grad_norm 2.0917 (2.4538)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:22:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:11:58 lr 0.000004	 wd 0.0500	time 0.6110 (0.6523)	loss 1.0007 (1.1171)	grad_norm 8.1666 (2.4529)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:23:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:10:53 lr 0.000004	 wd 0.0500	time 0.6200 (0.6520)	loss 1.0996 (1.1166)	grad_norm 2.3444 (2.4464)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:24:25 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:09:47 lr 0.000004	 wd 0.0500	time 0.6177 (0.6515)	loss 1.3316 (1.1161)	grad_norm 1.5891 (2.4198)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:25:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:08:42 lr 0.000004	 wd 0.0500	time 0.6112 (0.6511)	loss 0.9133 (1.1142)	grad_norm 2.1169 (2.4144)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:26:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:07:36 lr 0.000004	 wd 0.0500	time 0.6067 (0.6506)	loss 1.1601 (1.1128)	grad_norm 2.9872 (2.4348)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:27:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:06:31 lr 0.000004	 wd 0.0500	time 0.6039 (0.6503)	loss 0.9430 (1.1137)	grad_norm 1.6587 (2.4250)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:28:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:05:26 lr 0.000004	 wd 0.0500	time 0.6100 (0.6500)	loss 0.8979 (1.1150)	grad_norm 1.6114 (2.4104)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 19:29:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:04:21 lr 0.000004	 wd 0.0500	time 0.6115 (0.6497)	loss 1.2401 (1.1162)	grad_norm 2.3501 (2.4050)	loss_scale 2048.0000 (1069.8144)	mem 19785MB
[2024-08-01 19:30:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:03:16 lr 0.000004	 wd 0.0500	time 0.6112 (0.6494)	loss 1.0634 (1.1160)	grad_norm 2.3112 (2.4020)	loss_scale 2048.0000 (1114.2572)	mem 19785MB
[2024-08-01 19:31:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:02:11 lr 0.000004	 wd 0.0500	time 0.5782 (0.6492)	loss 1.1948 (1.1164)	grad_norm 1.8085 (2.3964)	loss_scale 2048.0000 (1154.8370)	mem 19785MB
[2024-08-01 19:33:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:06 lr 0.000004	 wd 0.0500	time 0.6187 (0.6490)	loss 1.4097 (1.1180)	grad_norm 1.7637 (2.3847)	loss_scale 2048.0000 (1192.0367)	mem 19785MB
[2024-08-01 19:34:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.6204 (0.6488)	loss 0.8820 (1.1183)	grad_norm 2.2022 (2.3735)	loss_scale 2048.0000 (1226.2615)	mem 19785MB
[2024-08-01 19:34:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 22 training takes 0:27:08
[2024-08-01 19:34:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.618 (12.618)	Loss 0.4700 (0.4700)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 19785MB
[2024-08-01 19:34:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.866 Acc@5 98.070
[2024-08-01 19:34:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-01 19:34:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 19:34:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 19:34:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 19:35:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:30:36 lr 0.000004	 wd 0.0500	time 10.8058 (10.8058)	loss 0.7654 (0.7654)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:36:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:29:46 lr 0.000004	 wd 0.0500	time 0.6140 (0.7436)	loss 1.3505 (1.1053)	grad_norm 5.3269 (2.3767)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:37:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:26:35 lr 0.000004	 wd 0.0500	time 0.6114 (0.6932)	loss 0.8988 (1.1144)	grad_norm 3.5193 (2.5220)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:38:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:24:49 lr 0.000004	 wd 0.0500	time 0.6167 (0.6763)	loss 1.3085 (1.1036)	grad_norm 1.7636 (2.4577)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:39:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:23:23 lr 0.000004	 wd 0.0500	time 0.6063 (0.6679)	loss 0.8970 (1.1107)	grad_norm 1.8605 (2.3773)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:40:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:22:07 lr 0.000004	 wd 0.0500	time 0.6161 (0.6631)	loss 0.8741 (1.1107)	grad_norm 1.9349 (2.3351)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:41:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:20:55 lr 0.000004	 wd 0.0500	time 0.6174 (0.6598)	loss 1.3286 (1.1097)	grad_norm 2.3686 (2.5022)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:42:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:19:45 lr 0.000004	 wd 0.0500	time 0.6549 (0.6577)	loss 1.2300 (1.1123)	grad_norm 3.2413 (2.5517)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:43:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:18:36 lr 0.000003	 wd 0.0500	time 0.6101 (0.6562)	loss 1.3250 (1.1150)	grad_norm 2.8998 (2.5326)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:44:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:17:29 lr 0.000003	 wd 0.0500	time 0.5803 (0.6550)	loss 1.1142 (1.1159)	grad_norm 1.5759 (2.4912)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:45:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:16:21 lr 0.000003	 wd 0.0500	time 0.6162 (0.6538)	loss 1.1484 (1.1122)	grad_norm 1.5552 (2.4779)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:46:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:15:15 lr 0.000003	 wd 0.0500	time 0.6411 (0.6528)	loss 0.9910 (1.1142)	grad_norm 2.1953 (2.4509)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:47:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:14:09 lr 0.000003	 wd 0.0500	time 0.6147 (0.6522)	loss 1.3167 (1.1166)	grad_norm 2.1158 (2.4387)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:48:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:13:03 lr 0.000003	 wd 0.0500	time 0.6059 (0.6515)	loss 1.1993 (1.1176)	grad_norm 1.9120 (2.4178)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:50:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:11:57 lr 0.000003	 wd 0.0500	time 0.6146 (0.6511)	loss 1.3492 (1.1176)	grad_norm 2.3890 (2.4366)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:51:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:10:51 lr 0.000003	 wd 0.0500	time 0.6162 (0.6505)	loss 1.3387 (1.1160)	grad_norm 1.9936 (2.4461)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:52:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:09:46 lr 0.000003	 wd 0.0500	time 0.6125 (0.6501)	loss 1.1544 (1.1188)	grad_norm 2.0900 (2.4293)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:53:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:08:41 lr 0.000003	 wd 0.0500	time 0.6146 (0.6497)	loss 0.6975 (1.1162)	grad_norm 2.4141 (2.4201)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:54:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:07:36 lr 0.000003	 wd 0.0500	time 0.6068 (0.6496)	loss 1.3723 (1.1157)	grad_norm 2.2182 (2.4159)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:55:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:06:30 lr 0.000003	 wd 0.0500	time 0.6108 (0.6493)	loss 1.3094 (1.1152)	grad_norm 1.9445 (2.4210)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:56:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:05:25 lr 0.000003	 wd 0.0500	time 0.6153 (0.6490)	loss 0.8162 (1.1143)	grad_norm 2.2628 (2.4139)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:57:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:04:20 lr 0.000003	 wd 0.0500	time 0.6256 (0.6488)	loss 1.2282 (1.1150)	grad_norm 2.9540 (2.4094)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:58:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:03:15 lr 0.000003	 wd 0.0500	time 0.6070 (0.6485)	loss 1.2805 (1.1140)	grad_norm 1.8650 (2.4078)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 19:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:02:10 lr 0.000003	 wd 0.0500	time 0.6173 (0.6483)	loss 1.4042 (1.1135)	grad_norm 3.3981 (2.4287)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:00:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:06 lr 0.000003	 wd 0.0500	time 0.6171 (0.6482)	loss 0.9513 (1.1133)	grad_norm 2.3910 (2.4256)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:01:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.6175 (0.6480)	loss 0.8386 (1.1122)	grad_norm 2.1924 (2.4192)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:01:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 23 training takes 0:27:06
[2024-08-01 20:02:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.897 (12.897)	Loss 0.4863 (0.4863)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 20:02:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.794 Acc@5 98.060
[2024-08-01 20:02:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 20:02:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 20:02:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:55:25 lr 0.000003	 wd 0.0500	time 12.8398 (12.8398)	loss 1.2357 (1.2357)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:03:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:30:33 lr 0.000003	 wd 0.0500	time 0.6170 (0.7635)	loss 0.9335 (1.1256)	grad_norm 1.9919 (2.7813)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:04:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:27:03 lr 0.000003	 wd 0.0500	time 0.6174 (0.7051)	loss 1.1289 (1.1172)	grad_norm 2.2096 (2.8829)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:06:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:25:07 lr 0.000003	 wd 0.0500	time 0.6336 (0.6844)	loss 1.3192 (1.1286)	grad_norm 2.1244 (2.6889)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:07:04 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:23:37 lr 0.000003	 wd 0.0500	time 0.6111 (0.6744)	loss 0.6683 (1.1261)	grad_norm 1.6416 (2.5916)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:08:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:22:17 lr 0.000003	 wd 0.0500	time 0.6129 (0.6683)	loss 1.2748 (1.1266)	grad_norm 1.9737 (2.5145)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:09:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:21:03 lr 0.000003	 wd 0.0500	time 0.6093 (0.6642)	loss 0.8374 (1.1229)	grad_norm 1.9690 (2.4573)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:10:17 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:19:51 lr 0.000003	 wd 0.0500	time 0.6021 (0.6612)	loss 1.2594 (1.1211)	grad_norm 1.8571 (2.4117)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:11:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:18:42 lr 0.000003	 wd 0.0500	time 0.6063 (0.6592)	loss 1.2438 (1.1222)	grad_norm 1.7432 (2.4037)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:12:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:17:33 lr 0.000003	 wd 0.0500	time 0.6101 (0.6575)	loss 0.8956 (1.1266)	grad_norm 1.6957 (2.4002)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:13:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:16:25 lr 0.000003	 wd 0.0500	time 0.6222 (0.6561)	loss 0.8506 (1.1240)	grad_norm 1.6462 (2.4301)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:14:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:15:18 lr 0.000003	 wd 0.0500	time 0.6164 (0.6550)	loss 0.8596 (1.1225)	grad_norm 1.8783 (2.4425)	loss_scale 4096.0000 (2230.2925)	mem 19785MB
[2024-08-01 20:15:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:14:11 lr 0.000002	 wd 0.0500	time 0.6193 (0.6543)	loss 0.9390 (1.1200)	grad_norm 2.3628 (2.4250)	loss_scale 4096.0000 (2385.6386)	mem 19785MB
[2024-08-01 20:16:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:13:05 lr 0.000002	 wd 0.0500	time 0.6173 (0.6536)	loss 0.8496 (1.1179)	grad_norm 2.1526 (2.4157)	loss_scale 4096.0000 (2517.1038)	mem 19785MB
[2024-08-01 20:17:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:11:59 lr 0.000002	 wd 0.0500	time 0.6099 (0.6529)	loss 1.4340 (1.1192)	grad_norm 1.7805 (nan)	loss_scale 2048.0000 (2577.1763)	mem 19785MB
[2024-08-01 20:18:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:10:53 lr 0.000002	 wd 0.0500	time 0.6148 (0.6524)	loss 1.2051 (1.1157)	grad_norm 1.8566 (nan)	loss_scale 2048.0000 (2541.9214)	mem 19785MB
[2024-08-01 20:19:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:09:47 lr 0.000002	 wd 0.0500	time 0.6155 (0.6519)	loss 1.2575 (1.1146)	grad_norm 2.1017 (nan)	loss_scale 2048.0000 (2511.0706)	mem 19785MB
[2024-08-01 20:21:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:08:42 lr 0.000002	 wd 0.0500	time 0.6374 (0.6515)	loss 1.5526 (1.1159)	grad_norm 2.3774 (nan)	loss_scale 2048.0000 (2483.8471)	mem 19785MB
[2024-08-01 20:22:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:07:37 lr 0.000002	 wd 0.0500	time 0.6120 (0.6510)	loss 1.1518 (1.1163)	grad_norm 36.8481 (nan)	loss_scale 2048.0000 (2459.6469)	mem 19785MB
[2024-08-01 20:23:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:06:31 lr 0.000002	 wd 0.0500	time 0.6132 (0.6507)	loss 0.8164 (1.1170)	grad_norm 1.6776 (nan)	loss_scale 2048.0000 (2437.9926)	mem 19785MB
[2024-08-01 20:24:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:05:26 lr 0.000002	 wd 0.0500	time 0.6098 (0.6503)	loss 1.2344 (1.1167)	grad_norm 2.1516 (nan)	loss_scale 2048.0000 (2418.5027)	mem 19785MB
[2024-08-01 20:25:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:04:21 lr 0.000002	 wd 0.0500	time 0.6310 (0.6501)	loss 0.9835 (1.1164)	grad_norm 1.9203 (nan)	loss_scale 2048.0000 (2400.8682)	mem 19785MB
[2024-08-01 20:26:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:03:16 lr 0.000002	 wd 0.0500	time 0.6142 (0.6499)	loss 0.9096 (1.1180)	grad_norm 2.4208 (nan)	loss_scale 2048.0000 (2384.8360)	mem 19785MB
[2024-08-01 20:27:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:02:11 lr 0.000002	 wd 0.0500	time 0.6131 (0.6497)	loss 1.2752 (1.1191)	grad_norm 2.6610 (nan)	loss_scale 2048.0000 (2370.1973)	mem 19785MB
[2024-08-01 20:28:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:06 lr 0.000002	 wd 0.0500	time 0.6179 (0.6495)	loss 1.3797 (1.1168)	grad_norm 1.7574 (nan)	loss_scale 2048.0000 (2356.7780)	mem 19785MB
[2024-08-01 20:29:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.6085 (0.6493)	loss 0.7626 (1.1171)	grad_norm 2.7298 (nan)	loss_scale 2048.0000 (2344.4318)	mem 19785MB
[2024-08-01 20:29:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 24 training takes 0:27:11
[2024-08-01 20:29:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.618 (11.618)	Loss 0.4695 (0.4695)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 20:30:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.822 Acc@5 98.080
[2024-08-01 20:30:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 20:30:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 20:30:35 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:39:36 lr 0.000002	 wd 0.0500	time 12.4608 (12.4608)	loss 1.2944 (1.2944)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:31:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:30:26 lr 0.000002	 wd 0.0500	time 0.6089 (0.7606)	loss 1.1822 (1.1810)	grad_norm 2.2735 (2.0249)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:32:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:26:56 lr 0.000002	 wd 0.0500	time 0.6089 (0.7023)	loss 1.4251 (1.1490)	grad_norm 1.6192 (2.1540)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:33:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:25:03 lr 0.000002	 wd 0.0500	time 0.6188 (0.6827)	loss 1.1715 (1.1325)	grad_norm 1.8429 (2.2095)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:34:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:23:34 lr 0.000002	 wd 0.0500	time 0.6066 (0.6729)	loss 1.1588 (1.1292)	grad_norm 1.7148 (2.2762)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:35:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:22:15 lr 0.000002	 wd 0.0500	time 0.6176 (0.6672)	loss 1.2779 (1.1267)	grad_norm 2.3569 (2.2938)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 20:37:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:21:02 lr 0.000002	 wd 0.0500	time 0.6042 (0.6636)	loss 0.8918 (1.1180)	grad_norm 2.0414 (nan)	loss_scale 1024.0000 (1921.9168)	mem 19785MB
[2024-08-01 20:38:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:19:50 lr 0.000002	 wd 0.0500	time 0.6137 (0.6608)	loss 0.7681 (1.1196)	grad_norm 2.1268 (nan)	loss_scale 1024.0000 (1793.8260)	mem 19785MB
[2024-08-01 20:39:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:18:41 lr 0.000002	 wd 0.0500	time 0.6125 (0.6587)	loss 1.3403 (1.1176)	grad_norm 2.2599 (nan)	loss_scale 1024.0000 (1697.7179)	mem 19785MB
[2024-08-01 20:40:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:17:32 lr 0.000002	 wd 0.0500	time 0.6141 (0.6570)	loss 1.2350 (1.1176)	grad_norm 1.8066 (nan)	loss_scale 1024.0000 (1622.9434)	mem 19785MB
[2024-08-01 20:41:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:16:25 lr 0.000002	 wd 0.0500	time 0.6164 (0.6559)	loss 1.5520 (1.1182)	grad_norm 3.0275 (nan)	loss_scale 1024.0000 (1563.1089)	mem 19785MB
[2024-08-01 20:42:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:15:18 lr 0.000002	 wd 0.0500	time 0.6051 (0.6548)	loss 1.4020 (1.1203)	grad_norm 2.9861 (nan)	loss_scale 1024.0000 (1514.1435)	mem 19785MB
[2024-08-01 20:43:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:14:11 lr 0.000002	 wd 0.0500	time 0.6112 (0.6538)	loss 0.9621 (1.1168)	grad_norm 2.2103 (nan)	loss_scale 1024.0000 (1473.3322)	mem 19785MB
[2024-08-01 20:44:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:13:04 lr 0.000002	 wd 0.0500	time 0.6151 (0.6531)	loss 0.9149 (1.1163)	grad_norm 3.3734 (nan)	loss_scale 1024.0000 (1438.7948)	mem 19785MB
[2024-08-01 20:45:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:11:59 lr 0.000002	 wd 0.0500	time 0.6003 (0.6525)	loss 0.9748 (1.1163)	grad_norm 4.4614 (nan)	loss_scale 1024.0000 (1409.1877)	mem 19785MB
[2024-08-01 20:46:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:10:53 lr 0.000002	 wd 0.0500	time 0.6021 (0.6521)	loss 0.7724 (1.1158)	grad_norm 2.1512 (nan)	loss_scale 1024.0000 (1383.5256)	mem 19785MB
[2024-08-01 20:47:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:09:47 lr 0.000002	 wd 0.0500	time 0.6109 (0.6515)	loss 0.8350 (1.1150)	grad_norm 2.9528 (nan)	loss_scale 1024.0000 (1361.0693)	mem 19785MB
[2024-08-01 20:48:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:08:42 lr 0.000002	 wd 0.0500	time 0.6128 (0.6511)	loss 0.7278 (1.1127)	grad_norm 1.5382 (nan)	loss_scale 1024.0000 (1341.2534)	mem 19785MB
[2024-08-01 20:49:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:07:36 lr 0.000002	 wd 0.0500	time 0.6148 (0.6508)	loss 0.8080 (1.1144)	grad_norm 1.7298 (nan)	loss_scale 1024.0000 (1323.6380)	mem 19785MB
[2024-08-01 20:50:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:06:31 lr 0.000002	 wd 0.0500	time 0.6170 (0.6505)	loss 1.4332 (1.1163)	grad_norm 2.3868 (nan)	loss_scale 1024.0000 (1307.8759)	mem 19785MB
[2024-08-01 20:52:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:05:26 lr 0.000002	 wd 0.0500	time 0.6168 (0.6501)	loss 0.9136 (1.1164)	grad_norm 1.8340 (nan)	loss_scale 1024.0000 (1293.6892)	mem 19785MB
[2024-08-01 20:53:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:04:21 lr 0.000002	 wd 0.0500	time 0.5969 (0.6498)	loss 1.2580 (1.1172)	grad_norm 2.5961 (nan)	loss_scale 1024.0000 (1280.8529)	mem 19785MB
[2024-08-01 20:54:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:03:16 lr 0.000001	 wd 0.0500	time 0.5769 (0.6495)	loss 1.0709 (1.1176)	grad_norm 2.0206 (nan)	loss_scale 1024.0000 (1269.1831)	mem 19785MB
[2024-08-01 20:55:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:02:11 lr 0.000001	 wd 0.0500	time 0.6149 (0.6493)	loss 1.3204 (1.1175)	grad_norm 1.6551 (nan)	loss_scale 1024.0000 (1258.5276)	mem 19785MB
[2024-08-01 20:56:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:01:06 lr 0.000001	 wd 0.0500	time 0.6127 (0.6491)	loss 1.4679 (1.1170)	grad_norm 2.1444 (nan)	loss_scale 1024.0000 (1248.7597)	mem 19785MB
[2024-08-01 20:57:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.6117 (0.6490)	loss 1.0074 (1.1157)	grad_norm 1.7312 (nan)	loss_scale 1024.0000 (1239.7729)	mem 19785MB
[2024-08-01 20:57:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 25 training takes 0:27:08
[2024-08-01 20:57:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.463 (12.463)	Loss 0.4905 (0.4905)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 20:58:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.770 Acc@5 98.050
[2024-08-01 20:58:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 20:58:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 20:58:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:47:37 lr 0.000001	 wd 0.0500	time 12.6527 (12.6527)	loss 1.0546 (1.0546)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 20:59:26 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:30:29 lr 0.000001	 wd 0.0500	time 0.6065 (0.7616)	loss 1.2752 (1.1248)	grad_norm 1.7032 (2.8257)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:00:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:26:58 lr 0.000001	 wd 0.0500	time 0.6144 (0.7031)	loss 1.0151 (1.1199)	grad_norm 1.9445 (2.5896)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:01:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:25:03 lr 0.000001	 wd 0.0500	time 0.6087 (0.6827)	loss 0.8824 (1.1080)	grad_norm 2.0647 (2.4633)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:02:39 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:23:34 lr 0.000001	 wd 0.0500	time 0.6257 (0.6731)	loss 1.4229 (1.1081)	grad_norm 3.2503 (2.3886)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:03:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:22:15 lr 0.000001	 wd 0.0500	time 0.6133 (0.6671)	loss 1.3057 (1.1165)	grad_norm 2.0132 (2.3630)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:04:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:21:01 lr 0.000001	 wd 0.0500	time 0.6143 (0.6633)	loss 0.7668 (1.1190)	grad_norm 1.4938 (2.4049)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:05:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:19:50 lr 0.000001	 wd 0.0500	time 0.6143 (0.6606)	loss 1.1923 (1.1150)	grad_norm 1.5855 (2.4204)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:06:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:18:41 lr 0.000001	 wd 0.0500	time 0.6238 (0.6588)	loss 1.2887 (1.1195)	grad_norm 1.4850 (2.4346)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:08:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:17:32 lr 0.000001	 wd 0.0500	time 0.6091 (0.6572)	loss 1.1641 (1.1198)	grad_norm 1.6042 (2.4521)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:09:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:16:25 lr 0.000001	 wd 0.0500	time 0.6174 (0.6559)	loss 0.9973 (1.1189)	grad_norm 6.6349 (2.4681)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:10:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:15:18 lr 0.000001	 wd 0.0500	time 0.6132 (0.6549)	loss 0.8325 (1.1177)	grad_norm 1.9576 (2.4785)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:11:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:14:11 lr 0.000001	 wd 0.0500	time 0.6106 (0.6540)	loss 0.9192 (1.1181)	grad_norm 1.8109 (2.4660)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:12:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:13:05 lr 0.000001	 wd 0.0500	time 0.6408 (0.6532)	loss 1.2290 (1.1201)	grad_norm 2.3151 (2.4606)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:13:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:11:59 lr 0.000001	 wd 0.0500	time 0.6044 (0.6526)	loss 1.3839 (1.1220)	grad_norm 1.6017 (2.4507)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:14:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:10:53 lr 0.000001	 wd 0.0500	time 0.6173 (0.6520)	loss 1.3200 (1.1237)	grad_norm 1.6269 (2.4278)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:15:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:09:47 lr 0.000001	 wd 0.0500	time 0.6086 (0.6515)	loss 1.1275 (1.1224)	grad_norm 2.1092 (2.4245)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:16:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:08:42 lr 0.000001	 wd 0.0500	time 0.6062 (0.6511)	loss 1.1122 (1.1192)	grad_norm 1.9583 (2.4113)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:17:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:07:36 lr 0.000001	 wd 0.0500	time 0.6137 (0.6508)	loss 0.7524 (1.1184)	grad_norm 5.4299 (2.4081)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:18:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:06:31 lr 0.000001	 wd 0.0500	time 0.6092 (0.6505)	loss 1.3144 (1.1200)	grad_norm 1.8905 (2.3922)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:19:50 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:05:26 lr 0.000001	 wd 0.0500	time 0.6225 (0.6502)	loss 1.1425 (1.1188)	grad_norm 1.9101 (2.3991)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:20:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:04:21 lr 0.000001	 wd 0.0500	time 0.6395 (0.6499)	loss 1.3125 (1.1182)	grad_norm 2.4959 (2.3916)	loss_scale 2048.0000 (1061.0414)	mem 19785MB
[2024-08-01 21:21:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:03:16 lr 0.000001	 wd 0.0500	time 0.6179 (0.6497)	loss 1.2334 (1.1166)	grad_norm 1.8689 (2.3948)	loss_scale 2048.0000 (1105.8828)	mem 19785MB
[2024-08-01 21:23:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:02:11 lr 0.000001	 wd 0.0500	time 0.6044 (0.6494)	loss 1.3488 (1.1168)	grad_norm 1.6344 (2.3879)	loss_scale 2048.0000 (1146.8266)	mem 19785MB
[2024-08-01 21:24:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:01:06 lr 0.000001	 wd 0.0500	time 0.6162 (0.6491)	loss 0.7360 (1.1173)	grad_norm 1.7689 (2.3938)	loss_scale 2048.0000 (1184.3599)	mem 19785MB
[2024-08-01 21:25:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.6078 (0.6489)	loss 0.8113 (1.1174)	grad_norm 2.5869 (2.3878)	loss_scale 2048.0000 (1218.8916)	mem 19785MB
[2024-08-01 21:25:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 26 training takes 0:27:09
[2024-08-01 21:25:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 10.683 (10.683)	Loss 0.4761 (0.4761)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 21:25:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.834 Acc@5 98.070
[2024-08-01 21:25:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 21:25:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 21:26:08 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:53:06 lr 0.000001	 wd 0.0500	time 11.3455 (11.3455)	loss 1.3449 (1.3449)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:27:13 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:30:16 lr 0.000001	 wd 0.0500	time 0.6078 (0.7563)	loss 1.2141 (1.1252)	grad_norm 2.0005 (4.3116)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:28:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:26:55 lr 0.000001	 wd 0.0500	time 0.5788 (0.7020)	loss 1.1870 (1.1308)	grad_norm 3.8892 (3.3377)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:29:22 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:25:03 lr 0.000001	 wd 0.0500	time 0.6393 (0.6827)	loss 1.2653 (1.1276)	grad_norm 2.0328 (2.9528)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:30:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:23:35 lr 0.000001	 wd 0.0500	time 0.6131 (0.6734)	loss 0.9267 (1.1267)	grad_norm 3.5698 (2.8867)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:31:31 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:22:16 lr 0.000001	 wd 0.0500	time 0.6155 (0.6674)	loss 0.7322 (1.1274)	grad_norm 2.2883 (2.7478)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:32:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:21:02 lr 0.000001	 wd 0.0500	time 0.6438 (0.6637)	loss 1.3402 (1.1268)	grad_norm 2.1255 (2.9197)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:33:40 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:19:50 lr 0.000001	 wd 0.0500	time 0.6192 (0.6609)	loss 0.7696 (1.1269)	grad_norm 6.7859 (2.9310)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:34:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:18:41 lr 0.000001	 wd 0.0500	time 0.6159 (0.6589)	loss 1.0827 (1.1266)	grad_norm 2.9868 (2.8332)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:35:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:17:32 lr 0.000001	 wd 0.0500	time 0.6121 (0.6572)	loss 1.2905 (1.1251)	grad_norm 1.5235 (2.7820)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:36:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:16:25 lr 0.000001	 wd 0.0500	time 0.6050 (0.6558)	loss 0.8972 (1.1210)	grad_norm 3.6898 (2.7427)	loss_scale 2048.0000 (2048.0000)	mem 19785MB
[2024-08-01 21:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:15:17 lr 0.000001	 wd 0.0500	time 0.6200 (0.6547)	loss 1.1300 (1.1226)	grad_norm 1.6041 (nan)	loss_scale 1024.0000 (2021.9582)	mem 19785MB
[2024-08-01 21:39:02 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:14:11 lr 0.000001	 wd 0.0500	time 0.6096 (0.6540)	loss 0.8067 (1.1223)	grad_norm 1.9852 (nan)	loss_scale 1024.0000 (1938.8643)	mem 19785MB
[2024-08-01 21:40:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:13:05 lr 0.000001	 wd 0.0500	time 0.6137 (0.6532)	loss 0.7801 (1.1180)	grad_norm 2.0624 (nan)	loss_scale 1024.0000 (1868.5442)	mem 19785MB
[2024-08-01 21:41:11 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:11:59 lr 0.000001	 wd 0.0500	time 0.6176 (0.6525)	loss 0.8634 (1.1195)	grad_norm 2.2719 (nan)	loss_scale 1024.0000 (1808.2627)	mem 19785MB
[2024-08-01 21:42:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:10:53 lr 0.000001	 wd 0.0500	time 0.6093 (0.6520)	loss 1.4092 (1.1208)	grad_norm 2.4048 (nan)	loss_scale 1024.0000 (1756.0133)	mem 19785MB
[2024-08-01 21:43:20 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:09:47 lr 0.000001	 wd 0.0500	time 0.6171 (0.6514)	loss 1.4743 (1.1222)	grad_norm 2.6499 (nan)	loss_scale 1024.0000 (1710.2911)	mem 19785MB
[2024-08-01 21:44:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:08:42 lr 0.000001	 wd 0.0500	time 0.6249 (0.6509)	loss 1.3222 (1.1232)	grad_norm 1.7212 (nan)	loss_scale 1024.0000 (1669.9447)	mem 19785MB
[2024-08-01 21:45:29 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:07:36 lr 0.000001	 wd 0.0500	time 0.6141 (0.6506)	loss 1.3033 (1.1223)	grad_norm 2.2161 (nan)	loss_scale 1024.0000 (1634.0788)	mem 19785MB
[2024-08-01 21:46:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:06:31 lr 0.000001	 wd 0.0500	time 0.6135 (0.6502)	loss 1.3169 (1.1229)	grad_norm 2.1026 (nan)	loss_scale 1024.0000 (1601.9863)	mem 19785MB
[2024-08-01 21:47:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:05:26 lr 0.000001	 wd 0.0500	time 0.6153 (0.6499)	loss 1.3479 (1.1234)	grad_norm 2.8638 (nan)	loss_scale 1024.0000 (1573.1014)	mem 19785MB
[2024-08-01 21:48:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:04:21 lr 0.000001	 wd 0.0500	time 0.6181 (0.6497)	loss 1.4857 (1.1231)	grad_norm 2.4683 (nan)	loss_scale 1024.0000 (1546.9662)	mem 19785MB
[2024-08-01 21:49:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:03:16 lr 0.000001	 wd 0.0500	time 0.6124 (0.6495)	loss 1.2056 (1.1238)	grad_norm 9.7245 (nan)	loss_scale 1024.0000 (1523.2058)	mem 19785MB
[2024-08-01 21:50:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:02:11 lr 0.000001	 wd 0.0500	time 0.6202 (0.6492)	loss 1.2754 (1.1238)	grad_norm 1.6467 (nan)	loss_scale 1024.0000 (1501.5106)	mem 19785MB
[2024-08-01 21:51:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:01:06 lr 0.000001	 wd 0.0500	time 0.6083 (0.6490)	loss 1.3242 (1.1251)	grad_norm 7.8744 (nan)	loss_scale 1024.0000 (1481.6227)	mem 19785MB
[2024-08-01 21:52:59 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.6105 (0.6487)	loss 1.1334 (1.1243)	grad_norm 2.0007 (nan)	loss_scale 1024.0000 (1463.3251)	mem 19785MB
[2024-08-01 21:53:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 27 training takes 0:27:08
[2024-08-01 21:53:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.273 (12.273)	Loss 0.4644 (0.4644)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 21:53:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.832 Acc@5 98.066
[2024-08-01 21:53:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 21:53:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 21:53:56 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:56:57 lr 0.000001	 wd 0.0500	time 11.4377 (11.4377)	loss 0.7463 (0.7463)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:55:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:30:23 lr 0.000000	 wd 0.0500	time 0.6033 (0.7592)	loss 0.7401 (1.1447)	grad_norm 2.2371 (2.3102)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:56:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:26:53 lr 0.000000	 wd 0.0500	time 0.6118 (0.7010)	loss 1.3404 (1.1410)	grad_norm 1.9354 (2.3016)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:57:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:25:00 lr 0.000000	 wd 0.0500	time 0.6147 (0.6813)	loss 1.3677 (1.1328)	grad_norm 3.1148 (2.3046)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:58:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:23:32 lr 0.000000	 wd 0.0500	time 0.6315 (0.6718)	loss 0.7327 (1.1245)	grad_norm 1.7331 (2.2725)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 21:59:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:22:14 lr 0.000000	 wd 0.0500	time 0.6110 (0.6665)	loss 0.7924 (1.1297)	grad_norm 1.7514 (2.4261)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:00:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:21:01 lr 0.000000	 wd 0.0500	time 0.6273 (0.6632)	loss 0.8657 (1.1224)	grad_norm 2.0020 (2.4064)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:01:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:19:50 lr 0.000000	 wd 0.0500	time 0.6388 (0.6606)	loss 1.1957 (1.1193)	grad_norm 6.9365 (2.4142)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:02:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:18:40 lr 0.000000	 wd 0.0500	time 0.6135 (0.6586)	loss 0.7979 (1.1206)	grad_norm 2.2955 (2.4071)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:03:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:17:32 lr 0.000000	 wd 0.0500	time 0.6132 (0.6569)	loss 0.7488 (1.1175)	grad_norm 3.3130 (2.4100)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:04:41 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:16:24 lr 0.000000	 wd 0.0500	time 0.6454 (0.6557)	loss 0.9984 (1.1186)	grad_norm 1.8823 (2.3833)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:05:45 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:15:17 lr 0.000000	 wd 0.0500	time 0.6052 (0.6545)	loss 1.1418 (1.1197)	grad_norm 1.6677 (2.3537)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:06:49 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:14:10 lr 0.000000	 wd 0.0500	time 0.6106 (0.6536)	loss 1.3354 (1.1203)	grad_norm 2.1537 (2.3520)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:07:54 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:13:04 lr 0.000000	 wd 0.0500	time 0.6125 (0.6529)	loss 1.3386 (1.1222)	grad_norm 1.5194 (2.3481)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:08:58 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:11:58 lr 0.000000	 wd 0.0500	time 0.6147 (0.6522)	loss 0.8688 (1.1209)	grad_norm 1.4992 (2.3500)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:10:03 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:10:53 lr 0.000000	 wd 0.0500	time 0.6242 (0.6519)	loss 1.1928 (1.1207)	grad_norm 1.7130 (2.4289)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:11:07 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:09:47 lr 0.000000	 wd 0.0500	time 0.6070 (0.6514)	loss 1.3929 (1.1230)	grad_norm 2.5827 (2.4431)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:12:12 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:08:42 lr 0.000000	 wd 0.0500	time 0.6246 (0.6510)	loss 1.3185 (1.1228)	grad_norm 2.4797 (2.4624)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:13:16 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:07:36 lr 0.000000	 wd 0.0500	time 0.6040 (0.6506)	loss 0.7264 (1.1224)	grad_norm 2.3031 (2.4472)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:14:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:06:31 lr 0.000000	 wd 0.0500	time 0.6085 (0.6503)	loss 0.9788 (1.1199)	grad_norm 2.3160 (2.4408)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:15:25 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:05:26 lr 0.000000	 wd 0.0500	time 0.6129 (0.6500)	loss 1.1266 (1.1198)	grad_norm 2.9264 (2.4399)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:16:30 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:04:21 lr 0.000000	 wd 0.0500	time 0.6112 (0.6498)	loss 1.0224 (1.1203)	grad_norm 2.4486 (2.4296)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:17:34 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:03:16 lr 0.000000	 wd 0.0500	time 0.6021 (0.6495)	loss 1.2778 (1.1196)	grad_norm 2.1408 (2.4189)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:18:38 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:02:11 lr 0.000000	 wd 0.0500	time 0.6221 (0.6493)	loss 1.2661 (1.1195)	grad_norm 1.7061 (2.4190)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:19:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:01:06 lr 0.000000	 wd 0.0500	time 0.6111 (0.6492)	loss 1.5067 (1.1186)	grad_norm 1.7795 (2.4158)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:20:47 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.6195 (0.6490)	loss 1.0402 (1.1178)	grad_norm 1.5253 (2.4142)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:20:53 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 28 training takes 0:27:08
[2024-08-01 22:21:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.721 (12.721)	Loss 0.4722 (0.4722)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 22:21:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.758 Acc@5 98.090
[2024-08-01 22:21:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 22:21:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 22:21:43 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:15:41 lr 0.000000	 wd 0.0500	time 11.8870 (11.8870)	loss 1.3221 (1.3221)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19785MB
[2024-08-01 22:22:48 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:30:15 lr 0.000000	 wd 0.0500	time 0.6153 (0.7558)	loss 1.3257 (1.1330)	grad_norm 2.0159 (2.2883)	loss_scale 2048.0000 (1348.4356)	mem 19785MB
[2024-08-01 22:23:52 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:26:50 lr 0.000000	 wd 0.0500	time 0.6423 (0.6998)	loss 0.7965 (1.1342)	grad_norm 2.1614 (2.6074)	loss_scale 2048.0000 (1696.4776)	mem 19785MB
[2024-08-01 22:24:57 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:25:00 lr 0.000000	 wd 0.0500	time 0.6015 (0.6813)	loss 0.6845 (1.1228)	grad_norm 1.7191 (2.4196)	loss_scale 2048.0000 (1813.2625)	mem 19785MB
[2024-08-01 22:26:01 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:23:32 lr 0.000000	 wd 0.0500	time 0.6084 (0.6721)	loss 1.0218 (1.1188)	grad_norm 1.6099 (2.3741)	loss_scale 2048.0000 (1871.8005)	mem 19785MB
[2024-08-01 22:27:06 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:22:14 lr 0.000000	 wd 0.0500	time 0.6391 (0.6666)	loss 1.1621 (1.1141)	grad_norm 2.5947 (2.4746)	loss_scale 2048.0000 (1906.9701)	mem 19785MB
[2024-08-01 22:28:10 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:21:00 lr 0.000000	 wd 0.0500	time 0.6060 (0.6630)	loss 0.6947 (1.1162)	grad_norm 2.2579 (2.5023)	loss_scale 2048.0000 (1930.4359)	mem 19785MB
[2024-08-01 22:29:15 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:19:49 lr 0.000000	 wd 0.0500	time 0.6132 (0.6604)	loss 1.1566 (1.1160)	grad_norm 2.0824 (2.4751)	loss_scale 2048.0000 (1947.2068)	mem 19785MB
[2024-08-01 22:30:19 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:18:40 lr 0.000000	 wd 0.0500	time 0.6374 (0.6585)	loss 0.8394 (1.1175)	grad_norm 2.0288 (2.4640)	loss_scale 2048.0000 (1959.7903)	mem 19785MB
[2024-08-01 22:31:24 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:17:33 lr 0.000000	 wd 0.0500	time 0.6269 (0.6575)	loss 1.4016 (1.1181)	grad_norm 4.7537 (2.4510)	loss_scale 2048.0000 (1969.5805)	mem 19785MB
[2024-08-01 22:32:28 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:16:25 lr 0.000000	 wd 0.0500	time 0.6305 (0.6561)	loss 1.2522 (1.1186)	grad_norm 4.6041 (2.4374)	loss_scale 2048.0000 (1977.4146)	mem 19785MB
[2024-08-01 22:33:33 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:15:18 lr 0.000000	 wd 0.0500	time 0.6050 (0.6550)	loss 1.2496 (1.1166)	grad_norm 1.9031 (2.4580)	loss_scale 2048.0000 (1983.8256)	mem 19785MB
[2024-08-01 22:34:37 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:14:11 lr 0.000000	 wd 0.0500	time 0.6134 (0.6540)	loss 0.9445 (1.1156)	grad_norm 2.5205 (2.4454)	loss_scale 2048.0000 (1989.1690)	mem 19785MB
[2024-08-01 22:35:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:13:05 lr 0.000000	 wd 0.0500	time 0.6116 (0.6534)	loss 1.1112 (1.1192)	grad_norm 2.1799 (2.4172)	loss_scale 2048.0000 (1993.6910)	mem 19785MB
[2024-08-01 22:36:46 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:11:59 lr 0.000000	 wd 0.0500	time 0.6118 (0.6528)	loss 1.0911 (1.1143)	grad_norm 2.4204 (2.3972)	loss_scale 2048.0000 (1997.5675)	mem 19785MB
[2024-08-01 22:37:51 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:10:53 lr 0.000000	 wd 0.0500	time 0.6168 (0.6523)	loss 0.9701 (1.1136)	grad_norm 2.3359 (2.4000)	loss_scale 2048.0000 (2000.9274)	mem 19785MB
[2024-08-01 22:38:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:09:47 lr 0.000000	 wd 0.0500	time 0.6132 (0.6518)	loss 0.7716 (1.1163)	grad_norm 2.3660 (2.3781)	loss_scale 2048.0000 (2003.8676)	mem 19785MB
[2024-08-01 22:40:00 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:08:42 lr 0.000000	 wd 0.0500	time 0.6098 (0.6514)	loss 1.1808 (1.1183)	grad_norm 2.0199 (2.3703)	loss_scale 2048.0000 (2006.4621)	mem 19785MB
[2024-08-01 22:41:05 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:07:37 lr 0.000000	 wd 0.0500	time 0.6120 (0.6513)	loss 1.2419 (1.1175)	grad_norm 2.9470 (2.3800)	loss_scale 2048.0000 (2008.7685)	mem 19785MB
[2024-08-01 22:42:09 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:06:31 lr 0.000000	 wd 0.0500	time 0.6183 (0.6510)	loss 0.8626 (1.1177)	grad_norm 2.2622 (2.3747)	loss_scale 2048.0000 (2010.8322)	mem 19785MB
[2024-08-01 22:43:14 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:05:26 lr 0.000000	 wd 0.0500	time 0.6001 (0.6507)	loss 0.8905 (1.1196)	grad_norm 1.8042 (2.3791)	loss_scale 2048.0000 (2012.6897)	mem 19785MB
[2024-08-01 22:44:18 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:04:21 lr 0.000000	 wd 0.0500	time 0.5790 (0.6505)	loss 0.8171 (1.1208)	grad_norm 1.6109 (2.3949)	loss_scale 2048.0000 (2014.3703)	mem 19785MB
[2024-08-01 22:45:23 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:03:16 lr 0.000000	 wd 0.0500	time 0.5902 (0.6502)	loss 0.8270 (1.1214)	grad_norm 4.5628 (2.3837)	loss_scale 2048.0000 (2015.8982)	mem 19785MB
[2024-08-01 22:46:27 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:02:11 lr 0.000000	 wd 0.0500	time 0.6130 (0.6499)	loss 0.6998 (1.1185)	grad_norm 2.2076 (2.3739)	loss_scale 2048.0000 (2017.2934)	mem 19785MB
[2024-08-01 22:47:32 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:01:06 lr 0.000000	 wd 0.0500	time 0.6087 (0.6497)	loss 1.2182 (1.1187)	grad_norm 2.6948 (2.3676)	loss_scale 2048.0000 (2018.5723)	mem 19785MB
[2024-08-01 22:48:36 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.6120 (0.6494)	loss 1.2094 (1.1181)	grad_norm 2.1298 (2.3574)	loss_scale 2048.0000 (2019.7489)	mem 19785MB
[2024-08-01 22:48:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 29 training takes 0:27:10
[2024-08-01 22:48:42 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_29.pth saving......
[2024-08-01 22:48:44 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_29.pth saved !!!
[2024-08-01 22:48:55 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.762 (11.762)	Loss 0.4622 (0.4622)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 19785MB
[2024-08-01 22:49:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.820 Acc@5 98.096
[2024-08-01 22:49:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-01 22:49:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-01 22:49:21 smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 189): INFO Training time 13:51:17
