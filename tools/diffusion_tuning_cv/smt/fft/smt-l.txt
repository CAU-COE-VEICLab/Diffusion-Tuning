[2024-05-28 17:30:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 355): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 7
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_mam_v2_large_224_22kto1k_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SMT:
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    HEAD_CONV: 7
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 12
    - 24
    - 48
    - 96
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    NUM_STAGES: 4
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 48
    - 96
    - 192
    - 384
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: smt_mam_v2
OUTPUT: pretrain_weights/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune/smt_mam_v2_large_22kto1k
PRINT_FREQ: 100
SAVE_FREQ: 30
SEED: 0
TAG: smt_mam_v2_large_22kto1k
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-05-28 17:30:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 356): INFO {"cfg": "/root/mam_classifier/configs/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain_weights/smt_mam_v2", "tag": "smt_mam_v2_large_22kto1k", "eval": false, "throughput": false, "local_rank": 7, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-05-28 17:30:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 93): INFO Creating model:smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune
[2024-05-28 17:30:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 95): INFO MemorySMTV2(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Identity()
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Identity()
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Identity()
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Identity()
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-05-28 17:30:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 98): INFO number of params: 81551608
[2024-05-28 17:30:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 136): INFO no checkpoint found in pretrain_weights/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune/smt_mam_v2_large_22kto1k, ignoring auto resume
[2024-05-28 17:30:31 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 46): INFO ==============> Loading weight /root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth for fine-tuning......
[2024-05-28 17:30:32 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-05-28 17:30:33 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['block3.1.attn.proj_feature_to_latent_space.weight', 'block3.1.attn.proj_feature_to_latent_space.bias', 'block3.3.attn.proj_feature_to_latent_space.weight', 'block3.3.attn.proj_feature_to_latent_space.bias', 'block3.3.attn.proj_latent_space_to_feature.weight', 'block3.3.attn.proj_latent_space_to_feature.bias', 'block3.3.attn.ltm_norm.weight', 'block3.3.attn.ltm_norm.bias', 'block3.5.attn.proj_feature_to_latent_space.weight', 'block3.5.attn.proj_feature_to_latent_space.bias', 'block3.5.attn.proj_latent_space_to_feature.weight', 'block3.5.attn.proj_latent_space_to_feature.bias', 'block3.5.attn.ltm_norm.weight', 'block3.5.attn.ltm_norm.bias', 'block3.7.attn.proj_feature_to_latent_space.weight', 'block3.7.attn.proj_feature_to_latent_space.bias', 'block3.7.attn.proj_latent_space_to_feature.weight', 'block3.7.attn.proj_latent_space_to_feature.bias', 'block3.7.attn.ltm_norm.weight', 'block3.7.attn.ltm_norm.bias', 'block3.9.attn.proj_feature_to_latent_space.weight', 'block3.9.attn.proj_feature_to_latent_space.bias', 'block3.9.attn.proj_latent_space_to_feature.weight', 'block3.9.attn.proj_latent_space_to_feature.bias', 'block3.9.attn.ltm_norm.weight', 'block3.9.attn.ltm_norm.bias', 'block3.11.attn.proj_feature_to_latent_space.weight', 'block3.11.attn.proj_feature_to_latent_space.bias', 'block3.11.attn.proj_latent_space_to_feature.weight', 'block3.11.attn.proj_latent_space_to_feature.bias', 'block3.11.attn.ltm_norm.weight', 'block3.11.attn.ltm_norm.bias', 'block3.13.attn.proj_feature_to_latent_space.weight', 'block3.13.attn.proj_feature_to_latent_space.bias', 'block3.13.attn.proj_latent_space_to_feature.weight', 'block3.13.attn.proj_latent_space_to_feature.bias', 'block3.13.attn.ltm_norm.weight', 'block3.13.attn.ltm_norm.bias', 'block3.15.attn.proj_feature_to_latent_space.weight', 'block3.15.attn.proj_feature_to_latent_space.bias', 'block3.15.attn.proj_latent_space_to_feature.weight', 'block3.15.attn.proj_latent_space_to_feature.bias', 'block3.15.attn.ltm_norm.weight', 'block3.15.attn.ltm_norm.bias', 'block3.17.attn.proj_feature_to_latent_space.weight', 'block3.17.attn.proj_feature_to_latent_space.bias', 'block3.17.attn.proj_latent_space_to_feature.weight', 'block3.17.attn.proj_latent_space_to_feature.bias', 'block3.17.attn.ltm_norm.weight', 'block3.17.attn.ltm_norm.bias', 'block3.19.attn.proj_feature_to_latent_space.weight', 'block3.19.attn.proj_feature_to_latent_space.bias', 'block3.19.attn.proj_latent_space_to_feature.weight', 'block3.19.attn.proj_latent_space_to_feature.bias', 'block3.19.attn.ltm_norm.weight', 'block3.19.attn.ltm_norm.bias', 'block3.21.attn.proj_feature_to_latent_space.weight', 'block3.21.attn.proj_feature_to_latent_space.bias', 'block3.21.attn.proj_latent_space_to_feature.weight', 'block3.21.attn.proj_latent_space_to_feature.bias', 'block3.21.attn.ltm_norm.weight', 'block3.21.attn.ltm_norm.bias', 'block3.23.attn.proj_feature_to_latent_space.weight', 'block3.23.attn.proj_feature_to_latent_space.bias', 'block3.23.attn.proj_latent_space_to_feature.weight', 'block3.23.attn.proj_latent_space_to_feature.bias', 'block3.23.attn.ltm_norm.weight', 'block3.23.attn.ltm_norm.bias', 'block3.25.attn.proj_feature_to_latent_space.weight', 'block3.25.attn.proj_feature_to_latent_space.bias', 'block3.25.attn.proj_latent_space_to_feature.weight', 'block3.25.attn.proj_latent_space_to_feature.bias', 'block3.25.attn.ltm_norm.weight', 'block3.25.attn.ltm_norm.bias', 'block3.27.attn.proj_latent_space_to_feature.weight', 'block3.27.attn.proj_latent_space_to_feature.bias', 'block4.0.attn.proj_feature_to_latent_space.weight', 'block4.0.attn.proj_feature_to_latent_space.bias', 'block4.1.attn.proj_feature_to_latent_space.weight', 'block4.1.attn.proj_feature_to_latent_space.bias', 'block4.1.attn.proj_latent_space_to_feature.weight', 'block4.1.attn.proj_latent_space_to_feature.bias', 'block4.1.attn.ltm_norm.weight', 'block4.1.attn.ltm_norm.bias', 'block4.2.attn.proj_feature_to_latent_space.weight', 'block4.2.attn.proj_feature_to_latent_space.bias', 'block4.2.attn.proj_latent_space_to_feature.weight', 'block4.2.attn.proj_latent_space_to_feature.bias', 'block4.2.attn.ltm_norm.weight', 'block4.2.attn.ltm_norm.bias', 'block4.3.attn.proj_latent_space_to_feature.weight', 'block4.3.attn.proj_latent_space_to_feature.bias'], unexpected_keys=[])
[2024-05-28 17:30:33 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 129): INFO => loaded successfully '/root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth'
[2024-05-28 17:30:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/98]	Time 14.552 (14.552)	Loss 0.4092 (0.4092)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 2743MB
[2024-05-28 17:31:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 84.410 Acc@5 97.080
[2024-05-28 17:31:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 148): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-05-28 17:31:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 154): INFO Start training
[2024-05-28 17:33:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 355): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 7
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_mam_v2_large_224_22kto1k_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SMT:
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    HEAD_CONV: 7
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 12
    - 24
    - 48
    - 96
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    NUM_STAGES: 4
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 48
    - 96
    - 192
    - 384
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: smt_mam_v2
OUTPUT: pretrain_weights/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune/smt_mam_v2_large_22kto1k
PRINT_FREQ: 100
SAVE_FREQ: 30
SEED: 0
TAG: smt_mam_v2_large_22kto1k
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 4
  AUTO_RESUME: true
  BASE_LR: 4.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 4.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 4.0e-08
  WEIGHT_DECAY: 0.05

[2024-05-28 17:33:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 356): INFO {"cfg": "/root/mam_classifier/configs/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 4, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain_weights/smt_mam_v2", "tag": "smt_mam_v2_large_22kto1k", "eval": false, "throughput": false, "local_rank": 7, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-05-28 17:33:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 93): INFO Creating model:smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune
[2024-05-28 17:33:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 95): INFO MemorySMTV2(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Identity()
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Identity()
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Identity()
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Identity()
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-05-28 17:33:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 98): INFO number of params: 81551608
[2024-05-28 17:33:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 136): INFO no checkpoint found in pretrain_weights/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune/smt_mam_v2_large_22kto1k, ignoring auto resume
[2024-05-28 17:33:29 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 46): INFO ==============> Loading weight /root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth for fine-tuning......
[2024-05-28 17:33:30 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-05-28 17:33:30 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['block3.1.attn.proj_feature_to_latent_space.weight', 'block3.1.attn.proj_feature_to_latent_space.bias', 'block3.3.attn.proj_feature_to_latent_space.weight', 'block3.3.attn.proj_feature_to_latent_space.bias', 'block3.3.attn.proj_latent_space_to_feature.weight', 'block3.3.attn.proj_latent_space_to_feature.bias', 'block3.3.attn.ltm_norm.weight', 'block3.3.attn.ltm_norm.bias', 'block3.5.attn.proj_feature_to_latent_space.weight', 'block3.5.attn.proj_feature_to_latent_space.bias', 'block3.5.attn.proj_latent_space_to_feature.weight', 'block3.5.attn.proj_latent_space_to_feature.bias', 'block3.5.attn.ltm_norm.weight', 'block3.5.attn.ltm_norm.bias', 'block3.7.attn.proj_feature_to_latent_space.weight', 'block3.7.attn.proj_feature_to_latent_space.bias', 'block3.7.attn.proj_latent_space_to_feature.weight', 'block3.7.attn.proj_latent_space_to_feature.bias', 'block3.7.attn.ltm_norm.weight', 'block3.7.attn.ltm_norm.bias', 'block3.9.attn.proj_feature_to_latent_space.weight', 'block3.9.attn.proj_feature_to_latent_space.bias', 'block3.9.attn.proj_latent_space_to_feature.weight', 'block3.9.attn.proj_latent_space_to_feature.bias', 'block3.9.attn.ltm_norm.weight', 'block3.9.attn.ltm_norm.bias', 'block3.11.attn.proj_feature_to_latent_space.weight', 'block3.11.attn.proj_feature_to_latent_space.bias', 'block3.11.attn.proj_latent_space_to_feature.weight', 'block3.11.attn.proj_latent_space_to_feature.bias', 'block3.11.attn.ltm_norm.weight', 'block3.11.attn.ltm_norm.bias', 'block3.13.attn.proj_feature_to_latent_space.weight', 'block3.13.attn.proj_feature_to_latent_space.bias', 'block3.13.attn.proj_latent_space_to_feature.weight', 'block3.13.attn.proj_latent_space_to_feature.bias', 'block3.13.attn.ltm_norm.weight', 'block3.13.attn.ltm_norm.bias', 'block3.15.attn.proj_feature_to_latent_space.weight', 'block3.15.attn.proj_feature_to_latent_space.bias', 'block3.15.attn.proj_latent_space_to_feature.weight', 'block3.15.attn.proj_latent_space_to_feature.bias', 'block3.15.attn.ltm_norm.weight', 'block3.15.attn.ltm_norm.bias', 'block3.17.attn.proj_feature_to_latent_space.weight', 'block3.17.attn.proj_feature_to_latent_space.bias', 'block3.17.attn.proj_latent_space_to_feature.weight', 'block3.17.attn.proj_latent_space_to_feature.bias', 'block3.17.attn.ltm_norm.weight', 'block3.17.attn.ltm_norm.bias', 'block3.19.attn.proj_feature_to_latent_space.weight', 'block3.19.attn.proj_feature_to_latent_space.bias', 'block3.19.attn.proj_latent_space_to_feature.weight', 'block3.19.attn.proj_latent_space_to_feature.bias', 'block3.19.attn.ltm_norm.weight', 'block3.19.attn.ltm_norm.bias', 'block3.21.attn.proj_feature_to_latent_space.weight', 'block3.21.attn.proj_feature_to_latent_space.bias', 'block3.21.attn.proj_latent_space_to_feature.weight', 'block3.21.attn.proj_latent_space_to_feature.bias', 'block3.21.attn.ltm_norm.weight', 'block3.21.attn.ltm_norm.bias', 'block3.23.attn.proj_feature_to_latent_space.weight', 'block3.23.attn.proj_feature_to_latent_space.bias', 'block3.23.attn.proj_latent_space_to_feature.weight', 'block3.23.attn.proj_latent_space_to_feature.bias', 'block3.23.attn.ltm_norm.weight', 'block3.23.attn.ltm_norm.bias', 'block3.25.attn.proj_feature_to_latent_space.weight', 'block3.25.attn.proj_feature_to_latent_space.bias', 'block3.25.attn.proj_latent_space_to_feature.weight', 'block3.25.attn.proj_latent_space_to_feature.bias', 'block3.25.attn.ltm_norm.weight', 'block3.25.attn.ltm_norm.bias', 'block3.27.attn.proj_latent_space_to_feature.weight', 'block3.27.attn.proj_latent_space_to_feature.bias', 'block4.0.attn.proj_feature_to_latent_space.weight', 'block4.0.attn.proj_feature_to_latent_space.bias', 'block4.1.attn.proj_feature_to_latent_space.weight', 'block4.1.attn.proj_feature_to_latent_space.bias', 'block4.1.attn.proj_latent_space_to_feature.weight', 'block4.1.attn.proj_latent_space_to_feature.bias', 'block4.1.attn.ltm_norm.weight', 'block4.1.attn.ltm_norm.bias', 'block4.2.attn.proj_feature_to_latent_space.weight', 'block4.2.attn.proj_feature_to_latent_space.bias', 'block4.2.attn.proj_latent_space_to_feature.weight', 'block4.2.attn.proj_latent_space_to_feature.bias', 'block4.2.attn.ltm_norm.weight', 'block4.2.attn.ltm_norm.bias', 'block4.3.attn.proj_latent_space_to_feature.weight', 'block4.3.attn.proj_latent_space_to_feature.bias'], unexpected_keys=[])
[2024-05-28 17:33:30 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 129): INFO => loaded successfully '/root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth'
[2024-05-28 17:33:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/98]	Time 15.804 (15.804)	Loss 0.4092 (0.4092)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 2743MB
[2024-05-28 17:34:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 84.410 Acc@5 97.080
[2024-05-28 17:34:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 148): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-05-28 17:34:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 154): INFO Start training
[2024-05-28 17:35:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 355): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 7
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_mam_v2_large_224_22kto1k_finetune
  NUM_CLASSES: 1000
  PRETRAINED: /root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SMT:
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    HEAD_CONV: 7
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 12
    - 24
    - 48
    - 96
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    NUM_STAGES: 4
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    LATENTSPACE_DIM:
    - 48
    - 96
    - 192
    - 384
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: smt_mam_v2
OUTPUT: pretrain_weights/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune/smt_mam_v2_large_22kto1k
PRINT_FREQ: 100
SAVE_FREQ: 30
SEED: 0
TAG: smt_mam_v2_large_22kto1k
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 1.0e-05
  CLIP_GRAD: 5.0
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 1.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 1.0e-08
  WEIGHT_DECAY: 0.05

[2024-05-28 17:35:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 356): INFO {"cfg": "/root/mam_classifier/configs/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune.yaml", "opts": null, "batch_size": 32, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain_weights/smt_mam_v2", "tag": "smt_mam_v2_large_22kto1k", "eval": false, "throughput": false, "local_rank": 7, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-05-28 17:35:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 93): INFO Creating model:smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune
[2024-05-28 17:35:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 95): INFO MemorySMTV2(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Identity()
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Linear(in_features=384, out_features=48, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): LayerNorm((48,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (proj_feature_to_latent_space): Identity()
        (proj_latent_space_to_feature): Linear(in_features=48, out_features=384, bias=True)
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Identity()
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Linear(in_features=768, out_features=96, bias=True)
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): SAMorMemoryAttention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        (proj_feature_to_latent_space): Identity()
        (proj_latent_space_to_feature): Linear(in_features=96, out_features=768, bias=True)
        (ltm_norm): Identity()
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-05-28 17:35:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 98): INFO number of params: 81551608
[2024-05-28 17:35:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 136): INFO no checkpoint found in pretrain_weights/smt_mam_v2/smt_mam_v2_large_224_22kto1k_finetune/smt_mam_v2_large_22kto1k, ignoring auto resume
[2024-05-28 17:35:17 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 46): INFO ==============> Loading weight /root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth for fine-tuning......
[2024-05-28 17:35:18 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-05-28 17:35:18 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 127): WARNING _IncompatibleKeys(missing_keys=['block3.1.attn.proj_feature_to_latent_space.weight', 'block3.1.attn.proj_feature_to_latent_space.bias', 'block3.3.attn.proj_feature_to_latent_space.weight', 'block3.3.attn.proj_feature_to_latent_space.bias', 'block3.3.attn.proj_latent_space_to_feature.weight', 'block3.3.attn.proj_latent_space_to_feature.bias', 'block3.3.attn.ltm_norm.weight', 'block3.3.attn.ltm_norm.bias', 'block3.5.attn.proj_feature_to_latent_space.weight', 'block3.5.attn.proj_feature_to_latent_space.bias', 'block3.5.attn.proj_latent_space_to_feature.weight', 'block3.5.attn.proj_latent_space_to_feature.bias', 'block3.5.attn.ltm_norm.weight', 'block3.5.attn.ltm_norm.bias', 'block3.7.attn.proj_feature_to_latent_space.weight', 'block3.7.attn.proj_feature_to_latent_space.bias', 'block3.7.attn.proj_latent_space_to_feature.weight', 'block3.7.attn.proj_latent_space_to_feature.bias', 'block3.7.attn.ltm_norm.weight', 'block3.7.attn.ltm_norm.bias', 'block3.9.attn.proj_feature_to_latent_space.weight', 'block3.9.attn.proj_feature_to_latent_space.bias', 'block3.9.attn.proj_latent_space_to_feature.weight', 'block3.9.attn.proj_latent_space_to_feature.bias', 'block3.9.attn.ltm_norm.weight', 'block3.9.attn.ltm_norm.bias', 'block3.11.attn.proj_feature_to_latent_space.weight', 'block3.11.attn.proj_feature_to_latent_space.bias', 'block3.11.attn.proj_latent_space_to_feature.weight', 'block3.11.attn.proj_latent_space_to_feature.bias', 'block3.11.attn.ltm_norm.weight', 'block3.11.attn.ltm_norm.bias', 'block3.13.attn.proj_feature_to_latent_space.weight', 'block3.13.attn.proj_feature_to_latent_space.bias', 'block3.13.attn.proj_latent_space_to_feature.weight', 'block3.13.attn.proj_latent_space_to_feature.bias', 'block3.13.attn.ltm_norm.weight', 'block3.13.attn.ltm_norm.bias', 'block3.15.attn.proj_feature_to_latent_space.weight', 'block3.15.attn.proj_feature_to_latent_space.bias', 'block3.15.attn.proj_latent_space_to_feature.weight', 'block3.15.attn.proj_latent_space_to_feature.bias', 'block3.15.attn.ltm_norm.weight', 'block3.15.attn.ltm_norm.bias', 'block3.17.attn.proj_feature_to_latent_space.weight', 'block3.17.attn.proj_feature_to_latent_space.bias', 'block3.17.attn.proj_latent_space_to_feature.weight', 'block3.17.attn.proj_latent_space_to_feature.bias', 'block3.17.attn.ltm_norm.weight', 'block3.17.attn.ltm_norm.bias', 'block3.19.attn.proj_feature_to_latent_space.weight', 'block3.19.attn.proj_feature_to_latent_space.bias', 'block3.19.attn.proj_latent_space_to_feature.weight', 'block3.19.attn.proj_latent_space_to_feature.bias', 'block3.19.attn.ltm_norm.weight', 'block3.19.attn.ltm_norm.bias', 'block3.21.attn.proj_feature_to_latent_space.weight', 'block3.21.attn.proj_feature_to_latent_space.bias', 'block3.21.attn.proj_latent_space_to_feature.weight', 'block3.21.attn.proj_latent_space_to_feature.bias', 'block3.21.attn.ltm_norm.weight', 'block3.21.attn.ltm_norm.bias', 'block3.23.attn.proj_feature_to_latent_space.weight', 'block3.23.attn.proj_feature_to_latent_space.bias', 'block3.23.attn.proj_latent_space_to_feature.weight', 'block3.23.attn.proj_latent_space_to_feature.bias', 'block3.23.attn.ltm_norm.weight', 'block3.23.attn.ltm_norm.bias', 'block3.25.attn.proj_feature_to_latent_space.weight', 'block3.25.attn.proj_feature_to_latent_space.bias', 'block3.25.attn.proj_latent_space_to_feature.weight', 'block3.25.attn.proj_latent_space_to_feature.bias', 'block3.25.attn.ltm_norm.weight', 'block3.25.attn.ltm_norm.bias', 'block3.27.attn.proj_latent_space_to_feature.weight', 'block3.27.attn.proj_latent_space_to_feature.bias', 'block4.0.attn.proj_feature_to_latent_space.weight', 'block4.0.attn.proj_feature_to_latent_space.bias', 'block4.1.attn.proj_feature_to_latent_space.weight', 'block4.1.attn.proj_feature_to_latent_space.bias', 'block4.1.attn.proj_latent_space_to_feature.weight', 'block4.1.attn.proj_latent_space_to_feature.bias', 'block4.1.attn.ltm_norm.weight', 'block4.1.attn.ltm_norm.bias', 'block4.2.attn.proj_feature_to_latent_space.weight', 'block4.2.attn.proj_feature_to_latent_space.bias', 'block4.2.attn.proj_latent_space_to_feature.weight', 'block4.2.attn.proj_latent_space_to_feature.bias', 'block4.2.attn.ltm_norm.weight', 'block4.2.attn.ltm_norm.bias', 'block4.3.attn.proj_latent_space_to_feature.weight', 'block4.3.attn.proj_latent_space_to_feature.bias'], unexpected_keys=[])
[2024-05-28 17:35:18 smt_mam_v2_large_224_22kto1k_finetune] (utils.py 129): INFO => loaded successfully '/root/mam_classifier/pretrain_weights/smt/smt_large_224_22k.pth'
[2024-05-28 17:35:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 10.752 (10.752)	Loss 0.3376 (0.3376)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 2361MB
[2024-05-28 17:35:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.107 (0.241)	Loss 0.6982 (0.6018)	Acc@1 83.984 (85.543)	Acc@5 96.094 (97.575)	Mem 2361MB
[2024-05-28 17:35:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 84.412 Acc@5 97.080
[2024-05-28 17:35:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 148): INFO Accuracy of the network on the 50000 test images: 84.4%
[2024-05-28 17:35:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 154): INFO Start training
[2024-05-28 17:36:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][0/5004]	eta 12:49:43 lr 0.000000	 wd 0.0500	time 9.2293 (9.2293)	loss 1.3801 (1.3801)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 12860MB
[2024-05-28 17:36:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][100/5004]	eta 0:49:47 lr 0.000000	 wd 0.0500	time 0.4804 (0.6091)	loss 1.1211 (1.3082)	grad_norm 5.3296 (nan)	loss_scale 8192.0000 (13950.7327)	mem 12860MB
[2024-05-28 17:37:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][200/5004]	eta 0:47:04 lr 0.000000	 wd 0.0500	time 0.4963 (0.5879)	loss 1.3382 (1.3003)	grad_norm 4.0966 (nan)	loss_scale 8192.0000 (11085.6915)	mem 12860MB
[2024-05-28 17:38:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][300/5004]	eta 0:45:46 lr 0.000000	 wd 0.0500	time 0.3866 (0.5840)	loss 1.5266 (1.2974)	grad_norm 4.7044 (nan)	loss_scale 4096.0000 (9362.2857)	mem 12860MB
[2024-05-28 17:39:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][400/5004]	eta 0:43:40 lr 0.000000	 wd 0.0500	time 0.3821 (0.5693)	loss 1.4089 (1.2814)	grad_norm 5.2649 (nan)	loss_scale 2048.0000 (7538.2743)	mem 12860MB
[2024-05-28 17:40:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][500/5004]	eta 0:42:18 lr 0.000000	 wd 0.0500	time 0.4251 (0.5636)	loss 1.5282 (1.2948)	grad_norm 4.6545 (nan)	loss_scale 2048.0000 (6442.4112)	mem 12860MB
[2024-05-28 17:41:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][600/5004]	eta 0:41:14 lr 0.000000	 wd 0.0500	time 0.4459 (0.5619)	loss 1.5196 (1.2918)	grad_norm 5.9975 (nan)	loss_scale 2048.0000 (5711.2280)	mem 12860MB
[2024-05-28 17:42:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][700/5004]	eta 0:40:14 lr 0.000000	 wd 0.0500	time 0.4915 (0.5610)	loss 0.9439 (1.2882)	grad_norm 4.5361 (nan)	loss_scale 1024.0000 (5103.9315)	mem 12860MB
[2024-05-28 17:43:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][800/5004]	eta 0:39:12 lr 0.000000	 wd 0.0500	time 0.4226 (0.5595)	loss 1.4656 (1.2892)	grad_norm 4.3868 (nan)	loss_scale 1024.0000 (4594.5768)	mem 12860MB
[2024-05-28 17:44:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][900/5004]	eta 0:38:04 lr 0.000000	 wd 0.0500	time 0.4266 (0.5568)	loss 1.0666 (1.2882)	grad_norm 4.7255 (nan)	loss_scale 1024.0000 (4198.2863)	mem 12860MB
[2024-05-28 17:45:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1000/5004]	eta 0:36:59 lr 0.000000	 wd 0.0500	time 0.3643 (0.5544)	loss 1.4607 (1.2848)	grad_norm 6.2979 (nan)	loss_scale 1024.0000 (3881.1748)	mem 12860MB
[2024-05-28 17:46:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1100/5004]	eta 0:35:49 lr 0.000000	 wd 0.0500	time 0.3968 (0.5505)	loss 1.0931 (1.2792)	grad_norm 3.9396 (nan)	loss_scale 1024.0000 (3621.6676)	mem 12860MB
[2024-05-28 17:46:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1200/5004]	eta 0:34:48 lr 0.000000	 wd 0.0500	time 0.4518 (0.5491)	loss 1.0420 (1.2720)	grad_norm 5.0592 (nan)	loss_scale 1024.0000 (3405.3755)	mem 12860MB
[2024-05-28 17:47:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1300/5004]	eta 0:33:56 lr 0.000001	 wd 0.0500	time 0.5432 (0.5499)	loss 0.9047 (1.2658)	grad_norm 3.5915 (nan)	loss_scale 1024.0000 (3222.3336)	mem 12860MB
[2024-05-28 17:48:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1400/5004]	eta 0:33:02 lr 0.000001	 wd 0.0500	time 0.4416 (0.5501)	loss 1.6425 (1.2629)	grad_norm 2.8720 (nan)	loss_scale 1024.0000 (3065.4218)	mem 12860MB
[2024-05-28 17:49:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1500/5004]	eta 0:32:04 lr 0.000001	 wd 0.0500	time 0.5192 (0.5493)	loss 0.8818 (1.2597)	grad_norm 3.6491 (nan)	loss_scale 1024.0000 (2929.4177)	mem 12860MB
[2024-05-28 17:50:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1600/5004]	eta 0:31:06 lr 0.000001	 wd 0.0500	time 0.4068 (0.5483)	loss 1.3579 (1.2584)	grad_norm 2.9187 (nan)	loss_scale 1024.0000 (2810.4035)	mem 12860MB
[2024-05-28 17:51:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1700/5004]	eta 0:30:07 lr 0.000001	 wd 0.0500	time 0.4288 (0.5470)	loss 0.9488 (1.2592)	grad_norm 4.2084 (nan)	loss_scale 1024.0000 (2705.3827)	mem 12860MB
[2024-05-28 17:52:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1800/5004]	eta 0:29:11 lr 0.000001	 wd 0.0500	time 0.3885 (0.5468)	loss 1.0847 (1.2572)	grad_norm 3.2437 (nan)	loss_scale 1024.0000 (2612.0244)	mem 12860MB
[2024-05-28 17:53:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][1900/5004]	eta 0:28:16 lr 0.000001	 wd 0.0500	time 0.4480 (0.5464)	loss 1.1465 (1.2548)	grad_norm 4.4217 (nan)	loss_scale 1024.0000 (2528.4882)	mem 12860MB
[2024-05-28 17:54:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2000/5004]	eta 0:27:18 lr 0.000001	 wd 0.0500	time 0.4329 (0.5453)	loss 0.8927 (1.2517)	grad_norm 2.8903 (nan)	loss_scale 1024.0000 (2453.3013)	mem 12860MB
[2024-05-28 17:54:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2100/5004]	eta 0:26:23 lr 0.000001	 wd 0.0500	time 0.4564 (0.5452)	loss 1.5909 (1.2515)	grad_norm 4.0885 (nan)	loss_scale 1024.0000 (2385.2718)	mem 12860MB
[2024-05-28 17:55:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2200/5004]	eta 0:25:28 lr 0.000001	 wd 0.0500	time 0.4808 (0.5452)	loss 1.2895 (1.2511)	grad_norm 4.0214 (nan)	loss_scale 1024.0000 (2323.4239)	mem 12860MB
[2024-05-28 17:56:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2300/5004]	eta 0:24:33 lr 0.000001	 wd 0.0500	time 0.5272 (0.5450)	loss 1.2942 (1.2497)	grad_norm 3.1044 (nan)	loss_scale 1024.0000 (2266.9518)	mem 12860MB
[2024-05-28 17:57:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2400/5004]	eta 0:23:42 lr 0.000001	 wd 0.0500	time 0.5152 (0.5461)	loss 0.8242 (1.2471)	grad_norm 3.9047 (nan)	loss_scale 1024.0000 (2215.1837)	mem 12860MB
[2024-05-28 17:58:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2500/5004]	eta 0:22:47 lr 0.000001	 wd 0.0500	time 0.4422 (0.5463)	loss 1.5807 (1.2469)	grad_norm 4.8244 (nan)	loss_scale 1024.0000 (2167.5554)	mem 12860MB
[2024-05-28 17:59:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2600/5004]	eta 0:21:56 lr 0.000001	 wd 0.0500	time 1.1942 (0.5476)	loss 1.3084 (1.2440)	grad_norm 3.3437 (nan)	loss_scale 1024.0000 (2123.5894)	mem 12860MB
[2024-05-28 18:00:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2700/5004]	eta 0:21:06 lr 0.000001	 wd 0.0500	time 0.3870 (0.5496)	loss 1.5837 (1.2419)	grad_norm 4.0152 (nan)	loss_scale 1024.0000 (2082.8789)	mem 12860MB
[2024-05-28 18:01:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2800/5004]	eta 0:20:13 lr 0.000001	 wd 0.0500	time 0.4180 (0.5504)	loss 1.2476 (1.2429)	grad_norm 3.9702 (nan)	loss_scale 1024.0000 (2045.0753)	mem 12860MB
[2024-05-28 18:02:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][2900/5004]	eta 0:19:17 lr 0.000001	 wd 0.0500	time 0.4322 (0.5503)	loss 1.3633 (1.2423)	grad_norm 2.7101 (nan)	loss_scale 1024.0000 (2009.8780)	mem 12860MB
[2024-05-28 18:03:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3000/5004]	eta 0:18:21 lr 0.000001	 wd 0.0500	time 0.4670 (0.5496)	loss 1.1876 (1.2400)	grad_norm 2.9867 (nan)	loss_scale 1024.0000 (1977.0263)	mem 12860MB
[2024-05-28 18:04:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3100/5004]	eta 0:17:26 lr 0.000001	 wd 0.0500	time 0.3702 (0.5495)	loss 1.2636 (1.2385)	grad_norm 7.1865 (nan)	loss_scale 1024.0000 (1946.2935)	mem 12860MB
[2024-05-28 18:05:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3200/5004]	eta 0:16:30 lr 0.000001	 wd 0.0500	time 0.4896 (0.5493)	loss 0.6445 (1.2376)	grad_norm 3.7601 (nan)	loss_scale 1024.0000 (1917.4808)	mem 12860MB
[2024-05-28 18:06:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3300/5004]	eta 0:15:35 lr 0.000001	 wd 0.0500	time 0.4211 (0.5492)	loss 0.9972 (1.2358)	grad_norm 4.9500 (nan)	loss_scale 1024.0000 (1890.4138)	mem 12860MB
[2024-05-28 18:06:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3400/5004]	eta 0:14:39 lr 0.000001	 wd 0.0500	time 0.5055 (0.5484)	loss 0.8886 (1.2354)	grad_norm 3.3838 (nan)	loss_scale 1024.0000 (1864.9385)	mem 12860MB
[2024-05-28 18:07:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3500/5004]	eta 0:13:45 lr 0.000001	 wd 0.0500	time 0.4694 (0.5486)	loss 0.9887 (1.2361)	grad_norm 3.4746 (nan)	loss_scale 1024.0000 (1840.9186)	mem 12860MB
[2024-05-28 18:08:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3600/5004]	eta 0:12:49 lr 0.000001	 wd 0.0500	time 0.3960 (0.5479)	loss 1.7770 (1.2352)	grad_norm 5.3943 (nan)	loss_scale 1024.0000 (1818.2327)	mem 12860MB
[2024-05-28 18:09:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3700/5004]	eta 0:11:53 lr 0.000001	 wd 0.0500	time 0.4321 (0.5474)	loss 1.3041 (1.2342)	grad_norm 3.4256 (nan)	loss_scale 1024.0000 (1796.7728)	mem 12860MB
[2024-05-28 18:10:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3800/5004]	eta 0:10:58 lr 0.000002	 wd 0.0500	time 0.4969 (0.5467)	loss 1.2364 (1.2337)	grad_norm 2.5917 (nan)	loss_scale 1024.0000 (1776.4420)	mem 12860MB
[2024-05-28 18:11:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][3900/5004]	eta 0:10:03 lr 0.000002	 wd 0.0500	time 0.4230 (0.5467)	loss 1.4070 (1.2341)	grad_norm 2.9646 (nan)	loss_scale 1024.0000 (1757.1536)	mem 12860MB
[2024-05-28 18:12:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4000/5004]	eta 0:09:08 lr 0.000002	 wd 0.0500	time 0.4196 (0.5464)	loss 0.8790 (1.2338)	grad_norm 2.9887 (nan)	loss_scale 1024.0000 (1738.8293)	mem 12860MB
[2024-05-28 18:13:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4100/5004]	eta 0:08:13 lr 0.000002	 wd 0.0500	time 0.4679 (0.5461)	loss 0.9967 (1.2330)	grad_norm 3.9825 (nan)	loss_scale 1024.0000 (1721.3987)	mem 12860MB
[2024-05-28 18:14:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4200/5004]	eta 0:07:18 lr 0.000002	 wd 0.0500	time 0.4639 (0.5459)	loss 1.3365 (1.2329)	grad_norm 2.5168 (nan)	loss_scale 1024.0000 (1704.7979)	mem 12860MB
[2024-05-28 18:15:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4300/5004]	eta 0:06:24 lr 0.000002	 wd 0.0500	time 0.4331 (0.5455)	loss 0.8294 (1.2313)	grad_norm 4.3846 (nan)	loss_scale 1024.0000 (1688.9691)	mem 12860MB
[2024-05-28 18:15:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4400/5004]	eta 0:05:29 lr 0.000002	 wd 0.0500	time 0.5599 (0.5449)	loss 1.4417 (1.2308)	grad_norm 3.1392 (nan)	loss_scale 1024.0000 (1673.8596)	mem 12860MB
[2024-05-28 18:16:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4500/5004]	eta 0:04:34 lr 0.000002	 wd 0.0500	time 0.3537 (0.5443)	loss 1.4964 (1.2304)	grad_norm 2.3452 (nan)	loss_scale 1024.0000 (1659.4215)	mem 12860MB
[2024-05-28 18:17:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4600/5004]	eta 0:03:39 lr 0.000002	 wd 0.0500	time 0.4353 (0.5438)	loss 0.7526 (1.2294)	grad_norm 3.3766 (nan)	loss_scale 1024.0000 (1645.6110)	mem 12860MB
[2024-05-28 18:18:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4700/5004]	eta 0:02:45 lr 0.000002	 wd 0.0500	time 0.4395 (0.5436)	loss 1.4289 (1.2289)	grad_norm 3.0974 (nan)	loss_scale 2048.0000 (1645.0219)	mem 12860MB
[2024-05-28 18:19:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4800/5004]	eta 0:01:50 lr 0.000002	 wd 0.0500	time 0.4619 (0.5433)	loss 1.3366 (1.2278)	grad_norm 3.1526 (nan)	loss_scale 2048.0000 (1653.4155)	mem 12860MB
[2024-05-28 18:20:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][4900/5004]	eta 0:00:56 lr 0.000002	 wd 0.0500	time 0.4274 (0.5432)	loss 0.8506 (1.2276)	grad_norm 3.7249 (nan)	loss_scale 2048.0000 (1661.4666)	mem 12860MB
[2024-05-28 18:21:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [0/30][5000/5004]	eta 0:00:02 lr 0.000002	 wd 0.0500	time 0.3481 (0.5427)	loss 0.7861 (1.2277)	grad_norm 2.6884 (nan)	loss_scale 2048.0000 (1669.1958)	mem 12860MB
[2024-05-28 18:21:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 0 training takes 0:45:17
[2024-05-28 18:21:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.702 (8.702)	Loss 0.4609 (0.4609)	Acc@1 94.141 (94.141)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-28 18:21:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.098 (0.202)	Loss 0.8262 (0.7036)	Acc@1 82.031 (86.552)	Acc@5 96.484 (97.853)	Mem 12860MB
[2024-05-28 18:21:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 85.316 Acc@5 97.424
[2024-05-28 18:21:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-05-28 18:21:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 85.32%
[2024-05-28 18:21:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][0/5004]	eta 11:32:41 lr 0.000002	 wd 0.0500	time 8.3056 (8.3056)	loss 1.3485 (1.3485)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:22:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][100/5004]	eta 0:48:40 lr 0.000002	 wd 0.0500	time 0.4510 (0.5956)	loss 0.9651 (1.1642)	grad_norm 2.1705 (3.6433)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:23:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][200/5004]	eta 0:45:12 lr 0.000002	 wd 0.0500	time 0.4557 (0.5645)	loss 1.3931 (1.1628)	grad_norm 2.8638 (3.6884)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:24:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][300/5004]	eta 0:43:00 lr 0.000002	 wd 0.0500	time 0.4140 (0.5486)	loss 1.5344 (1.1791)	grad_norm 5.5661 (3.9779)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:25:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][400/5004]	eta 0:41:21 lr 0.000002	 wd 0.0500	time 0.4328 (0.5391)	loss 1.4788 (1.1889)	grad_norm 2.7409 (4.0363)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:26:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][500/5004]	eta 0:40:07 lr 0.000002	 wd 0.0500	time 0.4194 (0.5346)	loss 1.3759 (1.1924)	grad_norm 4.1883 (4.0594)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:27:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][600/5004]	eta 0:39:09 lr 0.000002	 wd 0.0500	time 0.5378 (0.5334)	loss 1.5873 (1.1936)	grad_norm 2.6210 (3.9921)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:27:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][700/5004]	eta 0:38:16 lr 0.000002	 wd 0.0500	time 0.4258 (0.5337)	loss 1.3501 (1.1943)	grad_norm 3.0955 (3.9227)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:28:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][800/5004]	eta 0:37:29 lr 0.000002	 wd 0.0500	time 0.3969 (0.5350)	loss 1.2074 (1.1980)	grad_norm 3.1112 (3.9796)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:29:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][900/5004]	eta 0:36:32 lr 0.000002	 wd 0.0500	time 0.4539 (0.5341)	loss 1.6849 (1.1974)	grad_norm 7.0104 (3.9637)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:30:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1000/5004]	eta 0:35:45 lr 0.000002	 wd 0.0500	time 0.5887 (0.5359)	loss 1.1240 (1.1974)	grad_norm 3.0472 (3.9010)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:31:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1100/5004]	eta 0:34:46 lr 0.000002	 wd 0.0500	time 0.3580 (0.5344)	loss 0.9875 (1.1996)	grad_norm 4.9355 (3.8646)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:32:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1200/5004]	eta 0:33:49 lr 0.000002	 wd 0.0500	time 0.4037 (0.5336)	loss 1.3449 (1.1979)	grad_norm 4.2553 (3.8778)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:33:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1300/5004]	eta 0:32:57 lr 0.000003	 wd 0.0500	time 0.4163 (0.5338)	loss 1.5557 (1.1975)	grad_norm 2.8679 (3.8997)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:34:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1400/5004]	eta 0:32:03 lr 0.000003	 wd 0.0500	time 0.4317 (0.5337)	loss 1.2153 (1.1957)	grad_norm 7.8740 (3.8835)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:35:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1500/5004]	eta 0:31:07 lr 0.000003	 wd 0.0500	time 0.4355 (0.5329)	loss 1.3095 (1.1942)	grad_norm 6.9181 (3.8661)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:35:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1600/5004]	eta 0:30:10 lr 0.000003	 wd 0.0500	time 0.4354 (0.5319)	loss 1.3625 (1.1932)	grad_norm 2.2890 (3.8736)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:36:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1700/5004]	eta 0:29:15 lr 0.000003	 wd 0.0500	time 0.3750 (0.5313)	loss 1.4334 (1.1933)	grad_norm 3.2719 (3.8588)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:37:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1800/5004]	eta 0:28:18 lr 0.000003	 wd 0.0500	time 0.4648 (0.5302)	loss 1.3668 (1.1935)	grad_norm 3.3421 (3.8605)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:38:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][1900/5004]	eta 0:27:22 lr 0.000003	 wd 0.0500	time 0.4502 (0.5292)	loss 0.9918 (1.1919)	grad_norm 3.2692 (3.8600)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:39:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2000/5004]	eta 0:26:27 lr 0.000003	 wd 0.0500	time 0.4404 (0.5284)	loss 1.0080 (1.1935)	grad_norm 2.6528 (3.8513)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:40:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2100/5004]	eta 0:25:33 lr 0.000003	 wd 0.0500	time 0.4423 (0.5279)	loss 1.2510 (1.1942)	grad_norm 2.8013 (3.8559)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:41:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2200/5004]	eta 0:24:42 lr 0.000003	 wd 0.0500	time 0.4288 (0.5286)	loss 0.9991 (1.1941)	grad_norm 4.7353 (3.8665)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:42:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2300/5004]	eta 0:23:58 lr 0.000003	 wd 0.0500	time 0.4610 (0.5321)	loss 1.1464 (1.1948)	grad_norm 2.9247 (3.8722)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:43:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2400/5004]	eta 0:23:14 lr 0.000003	 wd 0.0500	time 0.5013 (0.5354)	loss 1.2576 (1.1958)	grad_norm 3.0937 (3.8608)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:44:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2500/5004]	eta 0:22:20 lr 0.000003	 wd 0.0500	time 0.4064 (0.5353)	loss 0.8709 (1.1949)	grad_norm 5.8604 (3.8520)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:44:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2600/5004]	eta 0:21:24 lr 0.000003	 wd 0.0500	time 0.4183 (0.5345)	loss 1.3714 (1.1932)	grad_norm 3.5322 (3.8392)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:45:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2700/5004]	eta 0:20:29 lr 0.000003	 wd 0.0500	time 0.4233 (0.5338)	loss 1.1180 (1.1938)	grad_norm 2.9492 (3.8849)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:46:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2800/5004]	eta 0:19:36 lr 0.000003	 wd 0.0500	time 0.4732 (0.5337)	loss 0.8080 (1.1925)	grad_norm 5.0954 (3.9023)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:47:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][2900/5004]	eta 0:18:42 lr 0.000003	 wd 0.0500	time 0.4012 (0.5333)	loss 1.1091 (1.1919)	grad_norm 3.3936 (3.9082)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 18:48:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3000/5004]	eta 0:17:47 lr 0.000003	 wd 0.0500	time 0.5096 (0.5329)	loss 0.8326 (1.1915)	grad_norm 4.0807 (inf)	loss_scale 1024.0000 (2015.2429)	mem 12860MB
[2024-05-28 18:49:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3100/5004]	eta 0:16:54 lr 0.000003	 wd 0.0500	time 0.5232 (0.5327)	loss 1.5323 (1.1908)	grad_norm 3.0602 (inf)	loss_scale 1024.0000 (1983.2777)	mem 12860MB
[2024-05-28 18:50:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3200/5004]	eta 0:16:00 lr 0.000003	 wd 0.0500	time 0.4413 (0.5324)	loss 1.0366 (1.1893)	grad_norm 2.9077 (inf)	loss_scale 1024.0000 (1953.3096)	mem 12860MB
[2024-05-28 18:50:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3300/5004]	eta 0:15:06 lr 0.000003	 wd 0.0500	time 0.4388 (0.5321)	loss 0.6826 (1.1899)	grad_norm 4.3435 (inf)	loss_scale 1024.0000 (1925.1572)	mem 12860MB
[2024-05-28 18:51:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3400/5004]	eta 0:14:13 lr 0.000003	 wd 0.0500	time 0.5429 (0.5323)	loss 1.8217 (1.1885)	grad_norm 3.3120 (inf)	loss_scale 1024.0000 (1898.6604)	mem 12860MB
[2024-05-28 18:52:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3500/5004]	eta 0:13:23 lr 0.000003	 wd 0.0500	time 0.5377 (0.5340)	loss 1.1305 (1.1890)	grad_norm 4.2469 (inf)	loss_scale 1024.0000 (1873.6772)	mem 12860MB
[2024-05-28 18:53:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3600/5004]	eta 0:12:33 lr 0.000003	 wd 0.0500	time 0.4118 (0.5365)	loss 0.8011 (1.1895)	grad_norm 2.5527 (inf)	loss_scale 1024.0000 (1850.0816)	mem 12860MB
[2024-05-28 18:54:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3700/5004]	eta 0:11:39 lr 0.000003	 wd 0.0500	time 0.4745 (0.5362)	loss 1.5407 (1.1900)	grad_norm 6.2006 (nan)	loss_scale 512.0000 (1815.5871)	mem 12860MB
[2024-05-28 18:55:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3800/5004]	eta 0:10:47 lr 0.000004	 wd 0.0500	time 0.6330 (0.5376)	loss 1.2900 (1.1897)	grad_norm 4.6487 (nan)	loss_scale 512.0000 (1781.2912)	mem 12860MB
[2024-05-28 18:56:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][3900/5004]	eta 0:09:54 lr 0.000004	 wd 0.0500	time 0.4057 (0.5383)	loss 1.6123 (1.1901)	grad_norm 3.9072 (nan)	loss_scale 512.0000 (1748.7537)	mem 12860MB
[2024-05-28 18:57:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4000/5004]	eta 0:09:00 lr 0.000004	 wd 0.0500	time 0.4606 (0.5379)	loss 1.0466 (1.1900)	grad_norm 38.4618 (nan)	loss_scale 512.0000 (1717.8425)	mem 12860MB
[2024-05-28 18:58:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4100/5004]	eta 0:08:05 lr 0.000004	 wd 0.0500	time 0.4332 (0.5374)	loss 1.5573 (1.1903)	grad_norm 3.0836 (nan)	loss_scale 512.0000 (1688.4389)	mem 12860MB
[2024-05-28 18:59:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4200/5004]	eta 0:07:11 lr 0.000004	 wd 0.0500	time 0.4068 (0.5371)	loss 1.4109 (1.1908)	grad_norm 3.1283 (nan)	loss_scale 512.0000 (1660.4351)	mem 12860MB
[2024-05-28 19:00:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4300/5004]	eta 0:06:17 lr 0.000004	 wd 0.0500	time 0.4076 (0.5369)	loss 1.4765 (1.1913)	grad_norm 2.7326 (nan)	loss_scale 512.0000 (1633.7336)	mem 12860MB
[2024-05-28 19:01:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4400/5004]	eta 0:05:24 lr 0.000004	 wd 0.0500	time 0.4311 (0.5366)	loss 1.1099 (1.1918)	grad_norm 3.5013 (nan)	loss_scale 512.0000 (1608.2454)	mem 12860MB
[2024-05-28 19:01:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4500/5004]	eta 0:04:30 lr 0.000004	 wd 0.0500	time 0.4809 (0.5367)	loss 0.9863 (1.1921)	grad_norm 2.8764 (nan)	loss_scale 512.0000 (1583.8898)	mem 12860MB
[2024-05-28 19:02:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4600/5004]	eta 0:03:37 lr 0.000004	 wd 0.0500	time 0.4304 (0.5374)	loss 1.3247 (1.1918)	grad_norm 3.4957 (nan)	loss_scale 512.0000 (1560.5929)	mem 12860MB
[2024-05-28 19:03:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4700/5004]	eta 0:02:43 lr 0.000004	 wd 0.0500	time 0.4309 (0.5373)	loss 0.9732 (1.1925)	grad_norm 2.9339 (nan)	loss_scale 512.0000 (1538.2872)	mem 12860MB
[2024-05-28 19:04:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4800/5004]	eta 0:01:49 lr 0.000004	 wd 0.0500	time 0.4351 (0.5370)	loss 0.9068 (1.1915)	grad_norm 2.7580 (nan)	loss_scale 512.0000 (1516.9106)	mem 12860MB
[2024-05-28 19:05:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][4900/5004]	eta 0:00:55 lr 0.000004	 wd 0.0500	time 0.3980 (0.5369)	loss 1.4580 (1.1903)	grad_norm 2.8788 (nan)	loss_scale 512.0000 (1496.4064)	mem 12860MB
[2024-05-28 19:06:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [1/30][5000/5004]	eta 0:00:02 lr 0.000004	 wd 0.0500	time 0.3643 (0.5357)	loss 1.4366 (1.1911)	grad_norm 3.8829 (nan)	loss_scale 512.0000 (1476.7223)	mem 12860MB
[2024-05-28 19:06:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 1 training takes 0:44:43
[2024-05-28 19:06:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.121 (8.121)	Loss 0.4612 (0.4612)	Acc@1 93.750 (93.750)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-28 19:06:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.107 (0.209)	Loss 0.8105 (0.6854)	Acc@1 80.078 (86.877)	Acc@5 96.484 (97.942)	Mem 12860MB
[2024-05-28 19:06:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 85.662 Acc@5 97.628
[2024-05-28 19:06:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-05-28 19:06:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 85.66%
[2024-05-28 19:07:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][0/5004]	eta 13:49:16 lr 0.000004	 wd 0.0500	time 9.9433 (9.9433)	loss 1.2422 (1.2422)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:07:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][100/5004]	eta 0:48:51 lr 0.000004	 wd 0.0500	time 0.3792 (0.5977)	loss 0.7862 (1.1954)	grad_norm 3.3339 (3.6636)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:08:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][200/5004]	eta 0:44:16 lr 0.000004	 wd 0.0500	time 0.3831 (0.5529)	loss 1.3495 (1.1782)	grad_norm 3.7886 (3.7295)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:09:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][300/5004]	eta 0:42:24 lr 0.000004	 wd 0.0500	time 0.3761 (0.5410)	loss 1.1092 (1.1765)	grad_norm 7.4538 (3.7493)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:10:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][400/5004]	eta 0:40:51 lr 0.000004	 wd 0.0500	time 0.4777 (0.5325)	loss 1.0103 (1.1803)	grad_norm 3.4666 (3.7473)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:11:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][500/5004]	eta 0:39:35 lr 0.000004	 wd 0.0500	time 0.4008 (0.5274)	loss 1.3488 (1.1792)	grad_norm 2.9427 (3.7639)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:12:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][600/5004]	eta 0:38:27 lr 0.000004	 wd 0.0500	time 0.3609 (0.5239)	loss 1.1043 (1.1837)	grad_norm 3.6254 (3.7406)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:13:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][700/5004]	eta 0:37:24 lr 0.000004	 wd 0.0500	time 0.3701 (0.5214)	loss 1.4708 (1.1902)	grad_norm 4.0444 (3.7657)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:13:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][800/5004]	eta 0:36:27 lr 0.000004	 wd 0.0500	time 0.4329 (0.5204)	loss 1.4373 (1.1865)	grad_norm 3.0511 (3.7404)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:14:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][900/5004]	eta 0:35:30 lr 0.000004	 wd 0.0500	time 0.4015 (0.5191)	loss 1.8347 (1.1876)	grad_norm 3.2224 (3.8182)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:15:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1000/5004]	eta 0:34:35 lr 0.000004	 wd 0.0500	time 0.4356 (0.5184)	loss 0.9222 (1.1868)	grad_norm 10.6748 (3.9093)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:16:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1100/5004]	eta 0:33:38 lr 0.000004	 wd 0.0500	time 0.3776 (0.5171)	loss 0.9110 (1.1876)	grad_norm 2.4597 (3.8958)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:17:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1200/5004]	eta 0:32:45 lr 0.000004	 wd 0.0500	time 0.4052 (0.5167)	loss 0.8245 (1.1899)	grad_norm 2.8323 (3.8604)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:18:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1300/5004]	eta 0:31:55 lr 0.000005	 wd 0.0500	time 0.4267 (0.5171)	loss 0.7774 (1.1894)	grad_norm 2.7006 (3.9069)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:19:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1400/5004]	eta 0:31:05 lr 0.000005	 wd 0.0500	time 0.4201 (0.5175)	loss 1.4618 (1.1873)	grad_norm 3.6008 (3.8894)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:19:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1500/5004]	eta 0:30:12 lr 0.000005	 wd 0.0500	time 0.3871 (0.5171)	loss 1.2023 (1.1891)	grad_norm 3.4030 (3.9102)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:20:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1600/5004]	eta 0:29:20 lr 0.000005	 wd 0.0500	time 0.3814 (0.5170)	loss 1.2122 (1.1844)	grad_norm 3.0571 (3.9113)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:21:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1700/5004]	eta 0:28:28 lr 0.000005	 wd 0.0500	time 0.5961 (0.5170)	loss 1.1122 (1.1839)	grad_norm 3.9189 (3.9725)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:22:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1800/5004]	eta 0:27:36 lr 0.000005	 wd 0.0500	time 0.4579 (0.5169)	loss 0.7798 (1.1834)	grad_norm 2.7169 (3.9729)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:23:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][1900/5004]	eta 0:26:44 lr 0.000005	 wd 0.0500	time 0.4392 (0.5169)	loss 0.9808 (1.1830)	grad_norm 3.5906 (3.9704)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:24:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2000/5004]	eta 0:25:53 lr 0.000005	 wd 0.0500	time 0.4446 (0.5172)	loss 1.3936 (1.1851)	grad_norm 2.6763 (3.9514)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:25:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2100/5004]	eta 0:25:05 lr 0.000005	 wd 0.0500	time 0.4996 (0.5183)	loss 1.0963 (1.1851)	grad_norm 2.9744 (3.9569)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:26:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2200/5004]	eta 0:24:16 lr 0.000005	 wd 0.0500	time 0.4039 (0.5194)	loss 1.1763 (1.1844)	grad_norm 3.8445 (3.9488)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:26:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2300/5004]	eta 0:23:24 lr 0.000005	 wd 0.0500	time 0.4144 (0.5195)	loss 1.4806 (1.1859)	grad_norm 3.2434 (3.9299)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:27:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2400/5004]	eta 0:22:32 lr 0.000005	 wd 0.0500	time 0.4173 (0.5193)	loss 0.9706 (1.1831)	grad_norm 5.9069 (3.9199)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:28:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2500/5004]	eta 0:21:40 lr 0.000005	 wd 0.0500	time 0.4964 (0.5193)	loss 1.4445 (1.1825)	grad_norm 4.2804 (3.9148)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:29:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2600/5004]	eta 0:20:48 lr 0.000005	 wd 0.0500	time 0.4600 (0.5194)	loss 1.3508 (1.1838)	grad_norm 2.8490 (3.9209)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 19:30:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2700/5004]	eta 0:20:00 lr 0.000005	 wd 0.0500	time 0.5003 (0.5211)	loss 1.4487 (1.1844)	grad_norm 2.5303 (3.9182)	loss_scale 1024.0000 (529.4395)	mem 12860MB
[2024-05-28 19:31:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2800/5004]	eta 0:19:08 lr 0.000005	 wd 0.0500	time 0.3980 (0.5212)	loss 1.3073 (1.1833)	grad_norm 3.3802 (3.9240)	loss_scale 1024.0000 (547.0960)	mem 12860MB
[2024-05-28 19:32:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][2900/5004]	eta 0:18:16 lr 0.000005	 wd 0.0500	time 0.4195 (0.5209)	loss 1.1756 (1.1827)	grad_norm 2.8443 (3.9369)	loss_scale 1024.0000 (563.5353)	mem 12860MB
[2024-05-28 19:33:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3000/5004]	eta 0:17:23 lr 0.000005	 wd 0.0500	time 0.4295 (0.5208)	loss 0.8517 (1.1829)	grad_norm 2.2810 (3.9384)	loss_scale 1024.0000 (578.8790)	mem 12860MB
[2024-05-28 19:33:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3100/5004]	eta 0:16:31 lr 0.000005	 wd 0.0500	time 0.4344 (0.5206)	loss 1.0279 (1.1826)	grad_norm 3.6366 (3.9386)	loss_scale 1024.0000 (593.2332)	mem 12860MB
[2024-05-28 19:34:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3200/5004]	eta 0:15:38 lr 0.000005	 wd 0.0500	time 0.3699 (0.5205)	loss 0.9462 (1.1820)	grad_norm 2.8372 (3.9352)	loss_scale 1024.0000 (606.6904)	mem 12860MB
[2024-05-28 19:35:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3300/5004]	eta 0:14:46 lr 0.000005	 wd 0.0500	time 0.3638 (0.5205)	loss 1.5658 (1.1815)	grad_norm 2.7759 (3.9485)	loss_scale 1024.0000 (619.3323)	mem 12860MB
[2024-05-28 19:36:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3400/5004]	eta 0:13:54 lr 0.000005	 wd 0.0500	time 0.4216 (0.5205)	loss 1.0873 (1.1807)	grad_norm 3.0809 (3.9500)	loss_scale 1024.0000 (631.2308)	mem 12860MB
[2024-05-28 19:37:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3500/5004]	eta 0:13:02 lr 0.000005	 wd 0.0500	time 0.3591 (0.5204)	loss 1.2998 (1.1794)	grad_norm 4.2597 (3.9852)	loss_scale 1024.0000 (642.4496)	mem 12860MB
[2024-05-28 19:38:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3600/5004]	eta 0:12:10 lr 0.000005	 wd 0.0500	time 0.4303 (0.5204)	loss 1.0740 (1.1791)	grad_norm 3.1464 (3.9740)	loss_scale 1024.0000 (653.0453)	mem 12860MB
[2024-05-28 19:39:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3700/5004]	eta 0:11:18 lr 0.000005	 wd 0.0500	time 0.4019 (0.5204)	loss 1.3040 (1.1791)	grad_norm 6.0034 (3.9744)	loss_scale 1024.0000 (663.0684)	mem 12860MB
[2024-05-28 19:40:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3800/5004]	eta 0:10:28 lr 0.000006	 wd 0.0500	time 0.5330 (0.5217)	loss 1.4679 (1.1794)	grad_norm 5.0768 (3.9717)	loss_scale 1024.0000 (672.5641)	mem 12860MB
[2024-05-28 19:41:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][3900/5004]	eta 0:09:38 lr 0.000006	 wd 0.0500	time 0.4302 (0.5236)	loss 1.0671 (1.1783)	grad_norm 2.9863 (3.9866)	loss_scale 1024.0000 (681.5729)	mem 12860MB
[2024-05-28 19:41:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4000/5004]	eta 0:08:45 lr 0.000006	 wd 0.0500	time 0.3886 (0.5235)	loss 1.3673 (1.1783)	grad_norm 2.6803 (3.9887)	loss_scale 1024.0000 (690.1315)	mem 12860MB
[2024-05-28 19:42:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4100/5004]	eta 0:07:53 lr 0.000006	 wd 0.0500	time 0.5245 (0.5236)	loss 1.1973 (1.1778)	grad_norm 3.8246 (3.9908)	loss_scale 1024.0000 (698.2726)	mem 12860MB
[2024-05-28 19:43:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4200/5004]	eta 0:07:01 lr 0.000006	 wd 0.0500	time 0.4474 (0.5240)	loss 1.4230 (1.1769)	grad_norm 3.4013 (3.9825)	loss_scale 1024.0000 (706.0262)	mem 12860MB
[2024-05-28 19:44:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4300/5004]	eta 0:06:08 lr 0.000006	 wd 0.0500	time 0.5275 (0.5241)	loss 1.4085 (1.1783)	grad_norm 2.9093 (3.9836)	loss_scale 1024.0000 (713.4192)	mem 12860MB
[2024-05-28 19:45:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4400/5004]	eta 0:05:17 lr 0.000006	 wd 0.0500	time 0.6042 (0.5260)	loss 0.8285 (1.1784)	grad_norm 6.7575 (3.9753)	loss_scale 1024.0000 (720.4763)	mem 12860MB
[2024-05-28 19:46:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4500/5004]	eta 0:04:26 lr 0.000006	 wd 0.0500	time 0.5519 (0.5278)	loss 1.5971 (1.1785)	grad_norm 3.6490 (3.9869)	loss_scale 1024.0000 (727.2197)	mem 12860MB
[2024-05-28 19:47:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4600/5004]	eta 0:03:33 lr 0.000006	 wd 0.0500	time 0.4126 (0.5279)	loss 1.0824 (1.1784)	grad_norm 4.2996 (3.9961)	loss_scale 1024.0000 (733.6701)	mem 12860MB
[2024-05-28 19:48:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4700/5004]	eta 0:02:40 lr 0.000006	 wd 0.0500	time 0.5803 (0.5283)	loss 1.0828 (1.1785)	grad_norm 3.5706 (4.0116)	loss_scale 1024.0000 (739.8460)	mem 12860MB
[2024-05-28 19:49:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4800/5004]	eta 0:01:48 lr 0.000006	 wd 0.0500	time 0.5709 (0.5301)	loss 1.0224 (1.1777)	grad_norm 8.4190 (4.0096)	loss_scale 1024.0000 (745.7646)	mem 12860MB
[2024-05-28 19:50:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][4900/5004]	eta 0:00:55 lr 0.000006	 wd 0.0500	time 0.4682 (0.5311)	loss 1.0984 (1.1781)	grad_norm 7.7741 (4.0203)	loss_scale 1024.0000 (751.4417)	mem 12860MB
[2024-05-28 19:51:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [2/30][5000/5004]	eta 0:00:02 lr 0.000006	 wd 0.0500	time 0.3539 (0.5309)	loss 0.9675 (1.1785)	grad_norm 3.3547 (4.0135)	loss_scale 1024.0000 (756.8918)	mem 12860MB
[2024-05-28 19:51:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 2 training takes 0:44:19
[2024-05-28 19:51:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.222 (8.222)	Loss 0.4512 (0.4512)	Acc@1 94.141 (94.141)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-28 19:51:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.105 (0.206)	Loss 0.8057 (0.6722)	Acc@1 80.078 (87.063)	Acc@5 96.484 (97.981)	Mem 12860MB
[2024-05-28 19:51:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 85.912 Acc@5 97.740
[2024-05-28 19:51:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-05-28 19:51:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 85.91%
[2024-05-28 19:51:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][0/5004]	eta 13:49:38 lr 0.000006	 wd 0.0500	time 9.9478 (9.9478)	loss 0.9169 (0.9169)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:52:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][100/5004]	eta 0:48:34 lr 0.000006	 wd 0.0500	time 0.3863 (0.5944)	loss 1.3037 (1.1638)	grad_norm 2.4441 (3.5857)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:53:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][200/5004]	eta 0:43:57 lr 0.000006	 wd 0.0500	time 0.8605 (0.5489)	loss 1.2431 (1.1691)	grad_norm 2.6761 (3.6772)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:54:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][300/5004]	eta 0:41:37 lr 0.000006	 wd 0.0500	time 0.3962 (0.5310)	loss 1.2286 (1.1777)	grad_norm 2.7479 (3.7090)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:55:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][400/5004]	eta 0:40:09 lr 0.000006	 wd 0.0500	time 0.3961 (0.5233)	loss 1.2392 (1.1795)	grad_norm 3.2164 (3.7421)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:56:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][500/5004]	eta 0:39:00 lr 0.000006	 wd 0.0500	time 0.3653 (0.5196)	loss 1.3312 (1.1777)	grad_norm 3.5118 (3.8633)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:56:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][600/5004]	eta 0:37:52 lr 0.000006	 wd 0.0500	time 0.3594 (0.5160)	loss 0.9352 (1.1715)	grad_norm 3.3745 (3.9475)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:57:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][700/5004]	eta 0:36:52 lr 0.000006	 wd 0.0500	time 0.4321 (0.5140)	loss 0.7921 (1.1687)	grad_norm 2.6597 (3.9045)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:58:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][800/5004]	eta 0:35:57 lr 0.000006	 wd 0.0500	time 0.4100 (0.5132)	loss 1.1205 (1.1698)	grad_norm 4.3499 (3.8705)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 19:59:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][900/5004]	eta 0:35:01 lr 0.000006	 wd 0.0500	time 0.5030 (0.5120)	loss 0.8828 (1.1676)	grad_norm 2.9626 (3.8766)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:00:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1000/5004]	eta 0:34:10 lr 0.000006	 wd 0.0500	time 0.3945 (0.5120)	loss 1.1012 (1.1700)	grad_norm 3.3127 (3.8671)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:01:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1100/5004]	eta 0:33:16 lr 0.000006	 wd 0.0500	time 0.3857 (0.5113)	loss 1.4749 (1.1705)	grad_norm 3.4176 (3.8951)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:02:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1200/5004]	eta 0:32:24 lr 0.000006	 wd 0.0500	time 0.3605 (0.5112)	loss 0.9604 (1.1705)	grad_norm 3.4438 (3.9250)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:02:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1300/5004]	eta 0:31:41 lr 0.000007	 wd 0.0500	time 0.4302 (0.5135)	loss 1.3399 (1.1682)	grad_norm 2.9987 (3.9003)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:03:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1400/5004]	eta 0:30:49 lr 0.000007	 wd 0.0500	time 0.4096 (0.5131)	loss 1.0078 (1.1700)	grad_norm 3.1559 (3.8855)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:04:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1500/5004]	eta 0:29:58 lr 0.000007	 wd 0.0500	time 0.4125 (0.5132)	loss 1.2559 (1.1704)	grad_norm 6.3556 (3.8662)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:05:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1600/5004]	eta 0:29:08 lr 0.000007	 wd 0.0500	time 0.4223 (0.5136)	loss 1.3070 (1.1723)	grad_norm 4.0370 (3.8517)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 20:06:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1700/5004]	eta 0:28:17 lr 0.000007	 wd 0.0500	time 0.4046 (0.5136)	loss 1.3495 (1.1727)	grad_norm 2.7088 (3.8537)	loss_scale 2048.0000 (1081.7919)	mem 12860MB
[2024-05-28 20:07:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1800/5004]	eta 0:27:25 lr 0.000007	 wd 0.0500	time 0.3922 (0.5135)	loss 1.2885 (1.1697)	grad_norm 3.7375 (3.8818)	loss_scale 2048.0000 (1135.4403)	mem 12860MB
[2024-05-28 20:08:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][1900/5004]	eta 0:26:32 lr 0.000007	 wd 0.0500	time 0.4271 (0.5131)	loss 1.1103 (1.1708)	grad_norm 3.5778 (3.8583)	loss_scale 2048.0000 (1183.4445)	mem 12860MB
[2024-05-28 20:08:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2000/5004]	eta 0:25:40 lr 0.000007	 wd 0.0500	time 0.4183 (0.5129)	loss 1.0980 (1.1707)	grad_norm 2.8812 (3.8592)	loss_scale 2048.0000 (1226.6507)	mem 12860MB
[2024-05-28 20:09:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2100/5004]	eta 0:24:49 lr 0.000007	 wd 0.0500	time 0.3944 (0.5129)	loss 0.9689 (1.1708)	grad_norm 4.5384 (3.8521)	loss_scale 2048.0000 (1265.7439)	mem 12860MB
[2024-05-28 20:10:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2200/5004]	eta 0:23:58 lr 0.000007	 wd 0.0500	time 0.4289 (0.5129)	loss 1.3230 (1.1703)	grad_norm 4.1962 (3.8546)	loss_scale 2048.0000 (1301.2849)	mem 12860MB
[2024-05-28 20:11:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2300/5004]	eta 0:23:06 lr 0.000007	 wd 0.0500	time 0.4210 (0.5129)	loss 1.3212 (1.1712)	grad_norm 3.2372 (3.8757)	loss_scale 2048.0000 (1333.7366)	mem 12860MB
[2024-05-28 20:12:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2400/5004]	eta 0:22:17 lr 0.000007	 wd 0.0500	time 0.4428 (0.5137)	loss 1.2902 (1.1702)	grad_norm 3.1277 (3.8882)	loss_scale 2048.0000 (1363.4852)	mem 12860MB
[2024-05-28 20:13:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2500/5004]	eta 0:21:26 lr 0.000007	 wd 0.0500	time 0.4242 (0.5138)	loss 1.4544 (1.1699)	grad_norm 3.3268 (3.8950)	loss_scale 2048.0000 (1390.8549)	mem 12860MB
[2024-05-28 20:14:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2600/5004]	eta 0:20:34 lr 0.000007	 wd 0.0500	time 0.4674 (0.5137)	loss 0.7922 (1.1686)	grad_norm 4.2290 (3.8924)	loss_scale 2048.0000 (1416.1200)	mem 12860MB
[2024-05-28 20:14:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2700/5004]	eta 0:19:43 lr 0.000007	 wd 0.0500	time 0.4680 (0.5139)	loss 0.7453 (1.1694)	grad_norm 3.1078 (3.8917)	loss_scale 2048.0000 (1439.5143)	mem 12860MB
[2024-05-28 20:15:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2800/5004]	eta 0:18:52 lr 0.000007	 wd 0.0500	time 0.4312 (0.5138)	loss 1.2821 (1.1692)	grad_norm 3.1508 (3.8844)	loss_scale 2048.0000 (1461.2381)	mem 12860MB
[2024-05-28 20:16:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][2900/5004]	eta 0:18:00 lr 0.000007	 wd 0.0500	time 0.3828 (0.5136)	loss 1.1294 (1.1706)	grad_norm 2.8290 (3.8813)	loss_scale 2048.0000 (1481.4643)	mem 12860MB
[2024-05-28 20:17:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3000/5004]	eta 0:17:09 lr 0.000007	 wd 0.0500	time 0.4195 (0.5139)	loss 1.1454 (1.1708)	grad_norm 3.4522 (3.8721)	loss_scale 2048.0000 (1500.3426)	mem 12860MB
[2024-05-28 20:18:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3100/5004]	eta 0:16:18 lr 0.000007	 wd 0.0500	time 0.4359 (0.5140)	loss 0.7318 (1.1716)	grad_norm 3.1451 (3.8648)	loss_scale 2048.0000 (1518.0032)	mem 12860MB
[2024-05-28 20:19:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3200/5004]	eta 0:15:27 lr 0.000007	 wd 0.0500	time 0.3658 (0.5141)	loss 0.7899 (1.1717)	grad_norm 4.3331 (3.8674)	loss_scale 2048.0000 (1534.5604)	mem 12860MB
[2024-05-28 20:20:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3300/5004]	eta 0:14:37 lr 0.000007	 wd 0.0500	time 0.4611 (0.5148)	loss 1.0066 (1.1713)	grad_norm 3.0009 (3.8669)	loss_scale 2048.0000 (1550.1145)	mem 12860MB
[2024-05-28 20:20:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3400/5004]	eta 0:13:45 lr 0.000007	 wd 0.0500	time 0.4513 (0.5149)	loss 1.2716 (1.1714)	grad_norm 3.1248 (3.8798)	loss_scale 2048.0000 (1564.7539)	mem 12860MB
[2024-05-28 20:21:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3500/5004]	eta 0:12:55 lr 0.000007	 wd 0.0500	time 0.3882 (0.5159)	loss 1.3894 (1.1714)	grad_norm 2.7510 (3.8720)	loss_scale 2048.0000 (1578.5570)	mem 12860MB
[2024-05-28 20:22:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3600/5004]	eta 0:12:04 lr 0.000007	 wd 0.0500	time 0.4933 (0.5163)	loss 1.3885 (1.1703)	grad_norm 3.6060 (3.8975)	loss_scale 2048.0000 (1591.5934)	mem 12860MB
[2024-05-28 20:23:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3700/5004]	eta 0:11:13 lr 0.000007	 wd 0.0500	time 0.4601 (0.5165)	loss 1.4260 (1.1699)	grad_norm 12.0053 (3.8965)	loss_scale 2048.0000 (1603.9254)	mem 12860MB
[2024-05-28 20:24:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3800/5004]	eta 0:10:22 lr 0.000008	 wd 0.0500	time 0.6052 (0.5167)	loss 1.0692 (1.1695)	grad_norm 5.5303 (3.9365)	loss_scale 2048.0000 (1615.6085)	mem 12860MB
[2024-05-28 20:25:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][3900/5004]	eta 0:09:31 lr 0.000008	 wd 0.0500	time 0.4308 (0.5178)	loss 1.3891 (1.1689)	grad_norm 3.1243 (3.9443)	loss_scale 2048.0000 (1626.6926)	mem 12860MB
[2024-05-28 20:26:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4000/5004]	eta 0:08:40 lr 0.000008	 wd 0.0500	time 0.4922 (0.5182)	loss 1.2979 (1.1691)	grad_norm 2.4688 (3.9336)	loss_scale 2048.0000 (1637.2227)	mem 12860MB
[2024-05-28 20:27:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4100/5004]	eta 0:07:48 lr 0.000008	 wd 0.0500	time 0.4159 (0.5183)	loss 1.2332 (1.1682)	grad_norm 2.7236 (3.9290)	loss_scale 2048.0000 (1647.2392)	mem 12860MB
[2024-05-28 20:28:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4200/5004]	eta 0:06:56 lr 0.000008	 wd 0.0500	time 0.4502 (0.5186)	loss 1.2901 (1.1685)	grad_norm 3.6766 (3.9187)	loss_scale 2048.0000 (1656.7789)	mem 12860MB
[2024-05-28 20:28:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4300/5004]	eta 0:06:05 lr 0.000008	 wd 0.0500	time 0.4115 (0.5188)	loss 1.3575 (1.1688)	grad_norm 3.4785 (3.9135)	loss_scale 2048.0000 (1665.8749)	mem 12860MB
[2024-05-28 20:29:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4400/5004]	eta 0:05:13 lr 0.000008	 wd 0.0500	time 0.4830 (0.5187)	loss 0.7294 (1.1681)	grad_norm 3.7363 (3.9047)	loss_scale 2048.0000 (1674.5576)	mem 12860MB
[2024-05-28 20:30:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4500/5004]	eta 0:04:21 lr 0.000008	 wd 0.0500	time 0.4673 (0.5189)	loss 1.0330 (1.1680)	grad_norm 4.8128 (3.9158)	loss_scale 2048.0000 (1682.8545)	mem 12860MB
[2024-05-28 20:31:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4600/5004]	eta 0:03:29 lr 0.000008	 wd 0.0500	time 0.5034 (0.5190)	loss 1.0786 (1.1678)	grad_norm 3.3228 (3.9140)	loss_scale 2048.0000 (1690.7907)	mem 12860MB
[2024-05-28 20:32:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4700/5004]	eta 0:02:37 lr 0.000008	 wd 0.0500	time 0.4773 (0.5192)	loss 1.0034 (1.1678)	grad_norm 2.6590 (3.9281)	loss_scale 2048.0000 (1698.3893)	mem 12860MB
[2024-05-28 20:33:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4800/5004]	eta 0:01:45 lr 0.000008	 wd 0.0500	time 0.4460 (0.5196)	loss 1.1674 (1.1685)	grad_norm 2.8265 (3.9244)	loss_scale 2048.0000 (1705.6713)	mem 12860MB
[2024-05-28 20:34:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][4900/5004]	eta 0:00:54 lr 0.000008	 wd 0.0500	time 0.4650 (0.5202)	loss 0.9694 (1.1687)	grad_norm 3.1972 (3.9221)	loss_scale 2048.0000 (1712.6562)	mem 12860MB
[2024-05-28 20:35:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [3/30][5000/5004]	eta 0:00:02 lr 0.000008	 wd 0.0500	time 0.3955 (0.5201)	loss 0.7953 (1.1684)	grad_norm 3.3209 (3.9313)	loss_scale 2048.0000 (1719.3617)	mem 12860MB
[2024-05-28 20:35:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 3 training takes 0:43:24
[2024-05-28 20:35:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.791 (7.791)	Loss 0.4243 (0.4243)	Acc@1 93.750 (93.750)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-28 20:35:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.110 (0.198)	Loss 0.7715 (0.6554)	Acc@1 80.859 (87.063)	Acc@5 96.484 (98.078)	Mem 12860MB
[2024-05-28 20:35:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 85.938 Acc@5 97.824
[2024-05-28 20:35:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-05-28 20:35:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 85.94%
[2024-05-28 20:35:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][0/5004]	eta 14:14:39 lr 0.000008	 wd 0.0500	time 10.2477 (10.2477)	loss 1.1512 (1.1512)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 20:36:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][100/5004]	eta 0:49:35 lr 0.000008	 wd 0.0500	time 0.5187 (0.6067)	loss 0.9599 (1.1802)	grad_norm 3.3726 (4.0766)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 20:37:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][200/5004]	eta 0:44:41 lr 0.000008	 wd 0.0500	time 0.3668 (0.5582)	loss 1.0812 (1.1492)	grad_norm 3.7813 (3.9172)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-28 20:38:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][300/5004]	eta 0:42:43 lr 0.000008	 wd 0.0500	time 0.4273 (0.5449)	loss 1.2972 (1.1512)	grad_norm 2.6241 (inf)	loss_scale 1024.0000 (1959.5482)	mem 12860MB
[2024-05-28 20:39:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][400/5004]	eta 0:41:16 lr 0.000008	 wd 0.0500	time 0.4139 (0.5380)	loss 1.2720 (1.1502)	grad_norm 4.3219 (inf)	loss_scale 1024.0000 (1726.2444)	mem 12860MB
[2024-05-28 20:40:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][500/5004]	eta 0:40:01 lr 0.000008	 wd 0.0500	time 0.4362 (0.5332)	loss 1.0445 (1.1541)	grad_norm 2.7056 (inf)	loss_scale 1024.0000 (1586.0758)	mem 12860MB
[2024-05-28 20:41:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][600/5004]	eta 0:38:48 lr 0.000008	 wd 0.0500	time 0.3936 (0.5286)	loss 1.0091 (1.1572)	grad_norm 2.9658 (inf)	loss_scale 1024.0000 (1492.5524)	mem 12860MB
[2024-05-28 20:41:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][700/5004]	eta 0:37:41 lr 0.000008	 wd 0.0500	time 0.3919 (0.5254)	loss 1.2715 (1.1543)	grad_norm 2.7111 (inf)	loss_scale 1024.0000 (1425.7118)	mem 12860MB
[2024-05-28 20:42:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][800/5004]	eta 0:36:39 lr 0.000008	 wd 0.0500	time 0.3643 (0.5232)	loss 1.0297 (1.1571)	grad_norm 5.4116 (inf)	loss_scale 1024.0000 (1375.5605)	mem 12860MB
[2024-05-28 20:43:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][900/5004]	eta 0:35:44 lr 0.000008	 wd 0.0500	time 0.4811 (0.5226)	loss 1.2996 (1.1566)	grad_norm 4.4527 (inf)	loss_scale 1024.0000 (1336.5416)	mem 12860MB
[2024-05-28 20:44:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1000/5004]	eta 0:34:45 lr 0.000008	 wd 0.0500	time 0.3854 (0.5208)	loss 1.2507 (1.1609)	grad_norm 3.7839 (inf)	loss_scale 1024.0000 (1305.3187)	mem 12860MB
[2024-05-28 20:45:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1100/5004]	eta 0:33:48 lr 0.000008	 wd 0.0500	time 0.3724 (0.5197)	loss 1.2879 (1.1654)	grad_norm 3.0988 (inf)	loss_scale 1024.0000 (1279.7675)	mem 12860MB
[2024-05-28 20:46:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1200/5004]	eta 0:32:51 lr 0.000008	 wd 0.0500	time 0.3948 (0.5182)	loss 0.7958 (1.1649)	grad_norm 6.4564 (inf)	loss_scale 1024.0000 (1258.4713)	mem 12860MB
[2024-05-28 20:46:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1300/5004]	eta 0:31:57 lr 0.000009	 wd 0.0500	time 0.4276 (0.5176)	loss 0.7659 (1.1655)	grad_norm 2.8090 (inf)	loss_scale 1024.0000 (1240.4489)	mem 12860MB
[2024-05-28 20:47:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1400/5004]	eta 0:31:04 lr 0.000009	 wd 0.0500	time 0.4332 (0.5174)	loss 1.1675 (1.1651)	grad_norm 2.5783 (inf)	loss_scale 1024.0000 (1224.9993)	mem 12860MB
[2024-05-28 20:48:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1500/5004]	eta 0:30:11 lr 0.000009	 wd 0.0500	time 0.3897 (0.5169)	loss 1.3402 (1.1647)	grad_norm 3.6873 (inf)	loss_scale 1024.0000 (1211.6083)	mem 12860MB
[2024-05-28 20:49:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1600/5004]	eta 0:29:16 lr 0.000009	 wd 0.0500	time 0.3759 (0.5160)	loss 1.0599 (1.1652)	grad_norm 4.1470 (inf)	loss_scale 1024.0000 (1199.8901)	mem 12860MB
[2024-05-28 20:50:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1700/5004]	eta 0:28:29 lr 0.000009	 wd 0.0500	time 0.4617 (0.5173)	loss 1.0582 (1.1641)	grad_norm 3.7955 (inf)	loss_scale 1024.0000 (1189.5497)	mem 12860MB
[2024-05-28 20:51:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1800/5004]	eta 0:27:42 lr 0.000009	 wd 0.0500	time 0.3986 (0.5188)	loss 1.3272 (1.1658)	grad_norm 3.6500 (inf)	loss_scale 1024.0000 (1180.3576)	mem 12860MB
[2024-05-28 20:52:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][1900/5004]	eta 0:26:52 lr 0.000009	 wd 0.0500	time 0.5189 (0.5193)	loss 1.4402 (1.1665)	grad_norm 3.5108 (inf)	loss_scale 1024.0000 (1172.1326)	mem 12860MB
[2024-05-28 20:53:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2000/5004]	eta 0:25:59 lr 0.000009	 wd 0.0500	time 0.4044 (0.5193)	loss 1.1724 (1.1664)	grad_norm 3.3154 (inf)	loss_scale 1024.0000 (1164.7296)	mem 12860MB
[2024-05-28 20:53:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2100/5004]	eta 0:25:05 lr 0.000009	 wd 0.0500	time 0.4148 (0.5186)	loss 1.0058 (1.1673)	grad_norm 3.5064 (inf)	loss_scale 1024.0000 (1158.0314)	mem 12860MB
[2024-05-28 20:54:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2200/5004]	eta 0:24:13 lr 0.000009	 wd 0.0500	time 0.4222 (0.5183)	loss 1.2386 (1.1671)	grad_norm 3.7607 (inf)	loss_scale 1024.0000 (1151.9418)	mem 12860MB
[2024-05-28 20:55:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2300/5004]	eta 0:23:22 lr 0.000009	 wd 0.0500	time 0.4032 (0.5187)	loss 1.4608 (1.1663)	grad_norm 3.0877 (inf)	loss_scale 1024.0000 (1146.3816)	mem 12860MB
[2024-05-28 20:56:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2400/5004]	eta 0:22:29 lr 0.000009	 wd 0.0500	time 0.3914 (0.5183)	loss 1.1227 (1.1653)	grad_norm 5.0477 (inf)	loss_scale 1024.0000 (1141.2845)	mem 12860MB
[2024-05-28 20:57:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2500/5004]	eta 0:21:37 lr 0.000009	 wd 0.0500	time 0.4314 (0.5180)	loss 0.9987 (1.1654)	grad_norm 3.3988 (inf)	loss_scale 1024.0000 (1136.5950)	mem 12860MB
[2024-05-28 20:58:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2600/5004]	eta 0:20:45 lr 0.000009	 wd 0.0500	time 0.3998 (0.5180)	loss 1.6010 (1.1650)	grad_norm 3.2926 (inf)	loss_scale 1024.0000 (1132.2661)	mem 12860MB
[2024-05-28 20:59:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2700/5004]	eta 0:19:52 lr 0.000009	 wd 0.0500	time 0.4272 (0.5177)	loss 1.0209 (1.1640)	grad_norm 3.2491 (inf)	loss_scale 1024.0000 (1128.2577)	mem 12860MB
[2024-05-28 20:59:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2800/5004]	eta 0:19:02 lr 0.000009	 wd 0.0500	time 0.3964 (0.5183)	loss 1.3585 (1.1644)	grad_norm 2.9096 (inf)	loss_scale 1024.0000 (1124.5355)	mem 12860MB
[2024-05-28 21:00:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][2900/5004]	eta 0:18:10 lr 0.000009	 wd 0.0500	time 0.4471 (0.5185)	loss 0.8883 (1.1643)	grad_norm 2.5988 (inf)	loss_scale 1024.0000 (1121.0700)	mem 12860MB
[2024-05-28 21:01:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3000/5004]	eta 0:17:18 lr 0.000009	 wd 0.0500	time 0.4481 (0.5184)	loss 1.2431 (1.1652)	grad_norm 3.9304 (inf)	loss_scale 1024.0000 (1117.8354)	mem 12860MB
[2024-05-28 21:02:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3100/5004]	eta 0:16:26 lr 0.000009	 wd 0.0500	time 0.3772 (0.5182)	loss 1.3876 (1.1656)	grad_norm 3.0470 (inf)	loss_scale 1024.0000 (1114.8094)	mem 12860MB
[2024-05-28 21:03:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3200/5004]	eta 0:15:34 lr 0.000009	 wd 0.0500	time 0.3892 (0.5178)	loss 1.4437 (1.1661)	grad_norm 3.5947 (inf)	loss_scale 1024.0000 (1111.9725)	mem 12860MB
[2024-05-28 21:04:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3300/5004]	eta 0:14:42 lr 0.000009	 wd 0.0500	time 0.4400 (0.5178)	loss 1.0049 (1.1650)	grad_norm 2.8142 (inf)	loss_scale 1024.0000 (1109.3075)	mem 12860MB
[2024-05-28 21:05:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3400/5004]	eta 0:13:51 lr 0.000009	 wd 0.0500	time 0.4384 (0.5187)	loss 1.3939 (1.1634)	grad_norm 3.6235 (inf)	loss_scale 1024.0000 (1106.7992)	mem 12860MB
[2024-05-28 21:06:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3500/5004]	eta 0:13:01 lr 0.000009	 wd 0.0500	time 0.4232 (0.5196)	loss 0.8941 (1.1617)	grad_norm 3.5849 (inf)	loss_scale 1024.0000 (1104.4342)	mem 12860MB
[2024-05-28 21:06:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3600/5004]	eta 0:12:09 lr 0.000009	 wd 0.0500	time 0.4930 (0.5195)	loss 1.6523 (1.1613)	grad_norm 2.7322 (inf)	loss_scale 1024.0000 (1102.2005)	mem 12860MB
[2024-05-28 21:07:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3700/5004]	eta 0:11:17 lr 0.000009	 wd 0.0500	time 0.4192 (0.5193)	loss 1.6925 (1.1607)	grad_norm 8.3857 (inf)	loss_scale 1024.0000 (1100.0875)	mem 12860MB
[2024-05-28 21:08:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3800/5004]	eta 0:10:25 lr 0.000010	 wd 0.0500	time 0.3720 (0.5195)	loss 1.3207 (1.1611)	grad_norm 3.4599 (inf)	loss_scale 1024.0000 (1098.0858)	mem 12860MB
[2024-05-28 21:09:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][3900/5004]	eta 0:09:33 lr 0.000010	 wd 0.0500	time 0.4738 (0.5198)	loss 0.9120 (1.1612)	grad_norm 2.4898 (inf)	loss_scale 1024.0000 (1096.1866)	mem 12860MB
[2024-05-28 21:10:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4000/5004]	eta 0:08:42 lr 0.000010	 wd 0.0500	time 0.3942 (0.5201)	loss 1.3219 (1.1608)	grad_norm 3.3132 (inf)	loss_scale 1024.0000 (1094.3824)	mem 12860MB
[2024-05-28 21:11:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4100/5004]	eta 0:07:50 lr 0.000010	 wd 0.0500	time 0.4715 (0.5200)	loss 1.4138 (1.1620)	grad_norm 3.5157 (inf)	loss_scale 1024.0000 (1092.6662)	mem 12860MB
[2024-05-28 21:12:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4200/5004]	eta 0:06:58 lr 0.000010	 wd 0.0500	time 0.5773 (0.5203)	loss 0.6846 (1.1608)	grad_norm 2.9984 (inf)	loss_scale 1024.0000 (1091.0317)	mem 12860MB
[2024-05-28 21:13:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4300/5004]	eta 0:06:06 lr 0.000010	 wd 0.0500	time 0.4521 (0.5204)	loss 0.9260 (1.1599)	grad_norm 5.9129 (inf)	loss_scale 2048.0000 (1095.6633)	mem 12860MB
[2024-05-28 21:13:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4400/5004]	eta 0:05:14 lr 0.000010	 wd 0.0500	time 0.4209 (0.5205)	loss 0.9575 (1.1587)	grad_norm 2.7694 (inf)	loss_scale 1024.0000 (1110.7875)	mem 12860MB
[2024-05-28 21:14:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4500/5004]	eta 0:04:22 lr 0.000010	 wd 0.0500	time 0.5949 (0.5218)	loss 0.7023 (1.1593)	grad_norm 2.9620 (inf)	loss_scale 1024.0000 (1108.8594)	mem 12860MB
[2024-05-28 21:15:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4600/5004]	eta 0:03:31 lr 0.000010	 wd 0.0500	time 0.4547 (0.5240)	loss 1.1744 (1.1591)	grad_norm 3.2313 (inf)	loss_scale 1024.0000 (1107.0150)	mem 12860MB
[2024-05-28 21:16:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4700/5004]	eta 0:02:39 lr 0.000010	 wd 0.0500	time 0.4507 (0.5243)	loss 1.2931 (1.1598)	grad_norm 2.9941 (inf)	loss_scale 1024.0000 (1105.2491)	mem 12860MB
[2024-05-28 21:17:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4800/5004]	eta 0:01:46 lr 0.000010	 wd 0.0500	time 0.3594 (0.5243)	loss 1.2704 (1.1594)	grad_norm 3.0691 (inf)	loss_scale 1024.0000 (1103.5568)	mem 12860MB
[2024-05-28 21:18:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][4900/5004]	eta 0:00:54 lr 0.000010	 wd 0.0500	time 0.4181 (0.5240)	loss 0.7473 (1.1594)	grad_norm 3.4969 (inf)	loss_scale 1024.0000 (1101.9335)	mem 12860MB
[2024-05-28 21:19:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [4/30][5000/5004]	eta 0:00:02 lr 0.000010	 wd 0.0500	time 0.3662 (0.5233)	loss 1.0105 (1.1594)	grad_norm 2.6953 (inf)	loss_scale 1024.0000 (1100.3751)	mem 12860MB
[2024-05-28 21:19:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 4 training takes 0:43:40
[2024-05-28 21:19:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.394 (8.394)	Loss 0.4385 (0.4385)	Acc@1 94.141 (94.141)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-28 21:19:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.108 (0.211)	Loss 0.7837 (0.6658)	Acc@1 80.469 (87.113)	Acc@5 96.875 (98.101)	Mem 12860MB
[2024-05-28 21:19:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.060 Acc@5 97.898
[2024-05-28 21:19:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-05-28 21:19:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.06%
[2024-05-28 21:20:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][0/5004]	eta 12:34:53 lr 0.000010	 wd 0.0500	time 9.0515 (9.0515)	loss 1.0447 (1.0447)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:20:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][100/5004]	eta 0:48:21 lr 0.000010	 wd 0.0500	time 0.3955 (0.5916)	loss 1.1673 (1.1573)	grad_norm 4.4400 (4.6236)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:21:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][200/5004]	eta 0:43:36 lr 0.000010	 wd 0.0500	time 0.4184 (0.5447)	loss 1.3076 (1.1551)	grad_norm 4.0636 (4.3230)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:22:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][300/5004]	eta 0:41:38 lr 0.000010	 wd 0.0500	time 0.4196 (0.5312)	loss 1.3986 (1.1537)	grad_norm 4.6591 (4.2673)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:23:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][400/5004]	eta 0:40:19 lr 0.000010	 wd 0.0500	time 0.4732 (0.5255)	loss 1.5125 (1.1636)	grad_norm 3.9665 (4.3132)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:24:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][500/5004]	eta 0:39:14 lr 0.000010	 wd 0.0500	time 0.4094 (0.5228)	loss 1.2504 (1.1605)	grad_norm 2.8917 (4.2531)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:25:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][600/5004]	eta 0:38:11 lr 0.000010	 wd 0.0500	time 0.4542 (0.5204)	loss 1.4530 (1.1594)	grad_norm 4.7432 (4.1482)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:26:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][700/5004]	eta 0:37:17 lr 0.000010	 wd 0.0500	time 0.4714 (0.5199)	loss 1.3519 (1.1626)	grad_norm 2.8518 (4.0839)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:26:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][800/5004]	eta 0:36:17 lr 0.000010	 wd 0.0500	time 0.4285 (0.5179)	loss 0.9431 (1.1587)	grad_norm 3.1503 (4.1683)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:27:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][900/5004]	eta 0:35:23 lr 0.000010	 wd 0.0500	time 0.5081 (0.5175)	loss 1.4057 (1.1564)	grad_norm 3.3940 (4.1646)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:28:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1000/5004]	eta 0:34:29 lr 0.000010	 wd 0.0500	time 0.3623 (0.5168)	loss 0.9548 (1.1547)	grad_norm 4.4573 (4.1423)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:29:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1100/5004]	eta 0:33:33 lr 0.000010	 wd 0.0500	time 0.3638 (0.5157)	loss 1.2199 (1.1556)	grad_norm 3.6547 (4.1226)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:30:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1200/5004]	eta 0:32:51 lr 0.000010	 wd 0.0500	time 0.4184 (0.5183)	loss 0.9634 (1.1548)	grad_norm 3.4464 (4.1170)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:31:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1300/5004]	eta 0:31:59 lr 0.000010	 wd 0.0500	time 0.5062 (0.5183)	loss 1.0662 (1.1528)	grad_norm 3.6683 (4.0794)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:32:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1400/5004]	eta 0:31:06 lr 0.000010	 wd 0.0500	time 0.4631 (0.5179)	loss 0.7457 (1.1506)	grad_norm 2.7977 (4.0867)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:32:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1500/5004]	eta 0:30:17 lr 0.000010	 wd 0.0500	time 0.3926 (0.5187)	loss 1.3572 (1.1473)	grad_norm 6.3192 (4.0948)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:33:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1600/5004]	eta 0:29:23 lr 0.000010	 wd 0.0500	time 0.3964 (0.5181)	loss 0.8213 (1.1459)	grad_norm 5.0893 (4.1315)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:34:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1700/5004]	eta 0:28:32 lr 0.000010	 wd 0.0500	time 0.5678 (0.5182)	loss 0.9478 (1.1475)	grad_norm 9.6858 (4.1158)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:35:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1800/5004]	eta 0:27:41 lr 0.000010	 wd 0.0500	time 0.3898 (0.5185)	loss 1.3135 (1.1470)	grad_norm 3.1258 (4.1516)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:36:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][1900/5004]	eta 0:26:48 lr 0.000010	 wd 0.0500	time 0.4374 (0.5182)	loss 0.8306 (1.1490)	grad_norm 2.9911 (4.1555)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:37:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2000/5004]	eta 0:25:59 lr 0.000010	 wd 0.0500	time 0.4352 (0.5191)	loss 1.3767 (1.1484)	grad_norm 4.0640 (4.1387)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:38:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2100/5004]	eta 0:25:06 lr 0.000010	 wd 0.0500	time 0.3766 (0.5188)	loss 1.2845 (1.1495)	grad_norm 4.1828 (4.1648)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:38:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2200/5004]	eta 0:24:14 lr 0.000010	 wd 0.0500	time 0.4080 (0.5186)	loss 0.7951 (1.1508)	grad_norm 5.4283 (4.1467)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:39:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2300/5004]	eta 0:23:26 lr 0.000010	 wd 0.0500	time 0.4688 (0.5202)	loss 1.0927 (1.1512)	grad_norm 3.2024 (4.1727)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:40:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2400/5004]	eta 0:22:35 lr 0.000010	 wd 0.0500	time 0.3972 (0.5204)	loss 0.7703 (1.1530)	grad_norm 2.9834 (4.1951)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:41:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2500/5004]	eta 0:21:41 lr 0.000010	 wd 0.0500	time 0.4237 (0.5197)	loss 1.0233 (1.1539)	grad_norm 3.1921 (4.1821)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:42:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2600/5004]	eta 0:20:49 lr 0.000010	 wd 0.0500	time 0.4206 (0.5199)	loss 0.8556 (1.1539)	grad_norm 3.7609 (4.1806)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:43:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2700/5004]	eta 0:19:56 lr 0.000010	 wd 0.0500	time 0.4239 (0.5192)	loss 0.8175 (1.1545)	grad_norm 4.2454 (4.1922)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:44:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2800/5004]	eta 0:19:03 lr 0.000010	 wd 0.0500	time 0.4211 (0.5189)	loss 0.8501 (1.1543)	grad_norm 2.9151 (4.1798)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:45:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][2900/5004]	eta 0:18:13 lr 0.000010	 wd 0.0500	time 0.4180 (0.5195)	loss 1.1500 (1.1528)	grad_norm 3.0381 (4.1815)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:45:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3000/5004]	eta 0:17:21 lr 0.000010	 wd 0.0500	time 0.4561 (0.5195)	loss 1.2441 (1.1531)	grad_norm 2.9773 (4.1674)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-28 21:46:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3100/5004]	eta 0:16:29 lr 0.000010	 wd 0.0500	time 0.4105 (0.5197)	loss 1.0475 (1.1529)	grad_norm 3.7681 (nan)	loss_scale 512.0000 (1021.3583)	mem 12860MB
[2024-05-28 21:47:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3200/5004]	eta 0:15:37 lr 0.000010	 wd 0.0500	time 0.3994 (0.5194)	loss 1.3083 (1.1537)	grad_norm 2.6485 (nan)	loss_scale 512.0000 (1005.4458)	mem 12860MB
[2024-05-28 21:48:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3300/5004]	eta 0:14:44 lr 0.000010	 wd 0.0500	time 0.3918 (0.5192)	loss 0.9669 (1.1526)	grad_norm 3.2583 (nan)	loss_scale 512.0000 (990.4974)	mem 12860MB
[2024-05-28 21:49:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3400/5004]	eta 0:13:53 lr 0.000010	 wd 0.0500	time 0.4337 (0.5194)	loss 0.7185 (1.1521)	grad_norm 3.7858 (nan)	loss_scale 512.0000 (976.4281)	mem 12860MB
[2024-05-28 21:50:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3500/5004]	eta 0:13:02 lr 0.000010	 wd 0.0500	time 0.4850 (0.5202)	loss 1.4607 (1.1521)	grad_norm 2.7426 (nan)	loss_scale 512.0000 (963.1625)	mem 12860MB
[2024-05-28 21:51:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3600/5004]	eta 0:12:12 lr 0.000010	 wd 0.0500	time 0.4487 (0.5214)	loss 1.1643 (1.1526)	grad_norm 3.5127 (nan)	loss_scale 512.0000 (950.6337)	mem 12860MB
[2024-05-28 21:52:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3700/5004]	eta 0:11:20 lr 0.000010	 wd 0.0500	time 0.3709 (0.5221)	loss 0.9751 (1.1533)	grad_norm 3.6230 (nan)	loss_scale 512.0000 (938.7820)	mem 12860MB
[2024-05-28 21:53:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3800/5004]	eta 0:10:28 lr 0.000010	 wd 0.0500	time 0.3766 (0.5224)	loss 1.3260 (1.1531)	grad_norm 2.8313 (nan)	loss_scale 512.0000 (927.5538)	mem 12860MB
[2024-05-28 21:53:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][3900/5004]	eta 0:09:36 lr 0.000010	 wd 0.0500	time 0.4149 (0.5223)	loss 1.3873 (1.1530)	grad_norm 11.5788 (nan)	loss_scale 512.0000 (916.9013)	mem 12860MB
[2024-05-28 21:54:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4000/5004]	eta 0:08:44 lr 0.000010	 wd 0.0500	time 0.3890 (0.5223)	loss 0.7617 (1.1536)	grad_norm 3.1298 (nan)	loss_scale 512.0000 (906.7813)	mem 12860MB
[2024-05-28 21:55:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4100/5004]	eta 0:07:52 lr 0.000010	 wd 0.0500	time 0.4350 (0.5229)	loss 1.1630 (1.1537)	grad_norm 6.7387 (nan)	loss_scale 512.0000 (897.1548)	mem 12860MB
[2024-05-28 21:56:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4200/5004]	eta 0:07:00 lr 0.000010	 wd 0.0500	time 0.4118 (0.5229)	loss 1.3399 (1.1530)	grad_norm 3.8005 (nan)	loss_scale 512.0000 (887.9867)	mem 12860MB
[2024-05-28 21:57:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4300/5004]	eta 0:06:08 lr 0.000010	 wd 0.0500	time 0.4227 (0.5228)	loss 1.2673 (1.1531)	grad_norm 3.2548 (nan)	loss_scale 512.0000 (879.2448)	mem 12860MB
[2024-05-28 21:58:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4400/5004]	eta 0:05:15 lr 0.000010	 wd 0.0500	time 0.5037 (0.5231)	loss 1.0293 (1.1534)	grad_norm 3.6259 (nan)	loss_scale 512.0000 (870.9002)	mem 12860MB
[2024-05-28 21:59:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4500/5004]	eta 0:04:23 lr 0.000010	 wd 0.0500	time 0.5017 (0.5231)	loss 1.0336 (1.1534)	grad_norm 3.4908 (nan)	loss_scale 512.0000 (862.9265)	mem 12860MB
[2024-05-28 22:00:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4600/5004]	eta 0:03:31 lr 0.000010	 wd 0.0500	time 0.4737 (0.5234)	loss 0.8447 (1.1535)	grad_norm 3.5529 (nan)	loss_scale 512.0000 (855.2993)	mem 12860MB
[2024-05-28 22:01:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4700/5004]	eta 0:02:39 lr 0.000010	 wd 0.0500	time 0.5795 (0.5245)	loss 1.0824 (1.1531)	grad_norm 13.9714 (nan)	loss_scale 512.0000 (847.9966)	mem 12860MB
[2024-05-28 22:01:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4800/5004]	eta 0:01:47 lr 0.000010	 wd 0.0500	time 0.3928 (0.5248)	loss 1.1104 (1.1525)	grad_norm 4.4688 (nan)	loss_scale 512.0000 (840.9981)	mem 12860MB
[2024-05-28 22:02:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][4900/5004]	eta 0:00:54 lr 0.000010	 wd 0.0500	time 0.3895 (0.5250)	loss 1.4531 (1.1520)	grad_norm 3.5438 (nan)	loss_scale 512.0000 (834.2852)	mem 12860MB
[2024-05-28 22:03:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [5/30][5000/5004]	eta 0:00:02 lr 0.000010	 wd 0.0500	time 0.3475 (0.5242)	loss 1.1467 (1.1523)	grad_norm 2.8417 (nan)	loss_scale 512.0000 (827.8408)	mem 12860MB
[2024-05-28 22:03:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 5 training takes 0:43:45
[2024-05-28 22:03:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.142 (8.142)	Loss 0.4282 (0.4282)	Acc@1 93.750 (93.750)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-28 22:04:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.112 (0.201)	Loss 0.7817 (0.6505)	Acc@1 80.469 (87.252)	Acc@5 97.656 (98.078)	Mem 12860MB
[2024-05-28 22:04:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.204 Acc@5 97.880
[2024-05-28 22:04:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-05-28 22:04:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.20%
[2024-05-28 22:04:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][0/5004]	eta 13:35:07 lr 0.000010	 wd 0.0500	time 9.7737 (9.7737)	loss 1.2934 (1.2934)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:05:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][100/5004]	eta 0:49:06 lr 0.000010	 wd 0.0500	time 0.5150 (0.6009)	loss 0.7029 (1.1237)	grad_norm 2.6248 (7.3002)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:06:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][200/5004]	eta 0:44:50 lr 0.000010	 wd 0.0500	time 0.4113 (0.5600)	loss 1.1553 (1.1316)	grad_norm 3.9135 (5.4790)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:06:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][300/5004]	eta 0:42:32 lr 0.000010	 wd 0.0500	time 0.4616 (0.5427)	loss 1.5310 (1.1329)	grad_norm 3.6321 (4.9635)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:07:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][400/5004]	eta 0:40:52 lr 0.000010	 wd 0.0500	time 0.3807 (0.5326)	loss 1.0097 (1.1422)	grad_norm 3.3662 (4.6982)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:08:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][500/5004]	eta 0:39:36 lr 0.000010	 wd 0.0500	time 0.4190 (0.5277)	loss 1.2171 (1.1417)	grad_norm 2.7333 (4.8459)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:09:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][600/5004]	eta 0:38:32 lr 0.000010	 wd 0.0500	time 0.3857 (0.5250)	loss 1.4566 (1.1415)	grad_norm 4.3920 (4.6733)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:10:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][700/5004]	eta 0:37:51 lr 0.000010	 wd 0.0500	time 0.4390 (0.5277)	loss 1.0648 (1.1442)	grad_norm 4.0734 (4.6216)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:11:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][800/5004]	eta 0:36:57 lr 0.000010	 wd 0.0500	time 0.3925 (0.5275)	loss 1.0884 (1.1450)	grad_norm 3.8097 (4.5514)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:12:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][900/5004]	eta 0:35:59 lr 0.000010	 wd 0.0500	time 0.4659 (0.5262)	loss 0.8838 (1.1443)	grad_norm 3.4783 (4.4829)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:12:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1000/5004]	eta 0:35:01 lr 0.000010	 wd 0.0500	time 0.4010 (0.5248)	loss 0.6660 (1.1412)	grad_norm 3.0345 (4.4747)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:13:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1100/5004]	eta 0:34:07 lr 0.000010	 wd 0.0500	time 0.4263 (0.5245)	loss 0.7673 (1.1423)	grad_norm 3.2816 (4.3954)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:14:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1200/5004]	eta 0:33:10 lr 0.000010	 wd 0.0500	time 0.4530 (0.5233)	loss 0.9297 (1.1448)	grad_norm 3.5802 (4.3266)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:15:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1300/5004]	eta 0:32:20 lr 0.000010	 wd 0.0500	time 0.4089 (0.5239)	loss 1.2525 (1.1419)	grad_norm 3.2113 (4.2986)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:16:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1400/5004]	eta 0:31:34 lr 0.000010	 wd 0.0500	time 0.4522 (0.5257)	loss 1.2570 (1.1394)	grad_norm 3.1227 (4.2663)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:17:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1500/5004]	eta 0:30:42 lr 0.000010	 wd 0.0500	time 0.4276 (0.5260)	loss 1.1448 (1.1389)	grad_norm 3.6934 (4.2404)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:18:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1600/5004]	eta 0:29:51 lr 0.000010	 wd 0.0500	time 0.4282 (0.5262)	loss 1.2317 (1.1360)	grad_norm 4.3184 (4.1917)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:19:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1700/5004]	eta 0:28:56 lr 0.000010	 wd 0.0500	time 0.3919 (0.5257)	loss 1.2985 (1.1366)	grad_norm 3.1460 (4.1842)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:19:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1800/5004]	eta 0:28:02 lr 0.000010	 wd 0.0500	time 0.4779 (0.5251)	loss 0.6380 (1.1384)	grad_norm 3.2234 (4.1939)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:20:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][1900/5004]	eta 0:27:15 lr 0.000010	 wd 0.0500	time 0.5283 (0.5270)	loss 1.0763 (1.1386)	grad_norm 2.9490 (4.1753)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:21:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2000/5004]	eta 0:26:22 lr 0.000010	 wd 0.0500	time 0.4727 (0.5266)	loss 0.8174 (1.1376)	grad_norm 6.0420 (4.1842)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:22:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2100/5004]	eta 0:25:28 lr 0.000010	 wd 0.0500	time 0.4485 (0.5263)	loss 1.1317 (1.1387)	grad_norm 3.2012 (4.1749)	loss_scale 1024.0000 (516.8739)	mem 12860MB
[2024-05-28 22:23:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2200/5004]	eta 0:24:33 lr 0.000010	 wd 0.0500	time 0.4329 (0.5255)	loss 1.3911 (1.1409)	grad_norm 4.2639 (4.1557)	loss_scale 1024.0000 (539.9146)	mem 12860MB
[2024-05-28 22:24:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2300/5004]	eta 0:23:42 lr 0.000010	 wd 0.0500	time 0.4640 (0.5260)	loss 1.2145 (1.1407)	grad_norm 3.3737 (4.1428)	loss_scale 1024.0000 (560.9526)	mem 12860MB
[2024-05-28 22:25:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2400/5004]	eta 0:22:48 lr 0.000010	 wd 0.0500	time 0.4820 (0.5256)	loss 1.1903 (1.1391)	grad_norm 3.1448 (4.1287)	loss_scale 1024.0000 (580.2382)	mem 12860MB
[2024-05-28 22:26:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2500/5004]	eta 0:21:54 lr 0.000010	 wd 0.0500	time 0.4322 (0.5250)	loss 1.1032 (1.1394)	grad_norm 3.8217 (inf)	loss_scale 512.0000 (589.3834)	mem 12860MB
[2024-05-28 22:26:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2600/5004]	eta 0:21:01 lr 0.000010	 wd 0.0500	time 0.4106 (0.5248)	loss 0.8843 (1.1411)	grad_norm 2.9044 (inf)	loss_scale 512.0000 (586.4083)	mem 12860MB
[2024-05-28 22:27:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2700/5004]	eta 0:20:08 lr 0.000010	 wd 0.0500	time 0.4273 (0.5244)	loss 0.7319 (1.1406)	grad_norm 4.4224 (inf)	loss_scale 512.0000 (583.6535)	mem 12860MB
[2024-05-28 22:28:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2800/5004]	eta 0:19:14 lr 0.000010	 wd 0.0500	time 0.4271 (0.5238)	loss 1.1891 (1.1411)	grad_norm 3.7350 (inf)	loss_scale 512.0000 (581.0953)	mem 12860MB
[2024-05-28 22:29:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][2900/5004]	eta 0:18:22 lr 0.000010	 wd 0.0500	time 0.3948 (0.5238)	loss 0.8417 (1.1407)	grad_norm 2.8644 (inf)	loss_scale 512.0000 (578.7135)	mem 12860MB
[2024-05-28 22:30:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3000/5004]	eta 0:17:31 lr 0.000010	 wd 0.0500	time 0.4824 (0.5246)	loss 0.9809 (1.1409)	grad_norm 2.5966 (inf)	loss_scale 512.0000 (576.4905)	mem 12860MB
[2024-05-28 22:31:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3100/5004]	eta 0:16:40 lr 0.000010	 wd 0.0500	time 0.4438 (0.5256)	loss 0.6003 (1.1424)	grad_norm 3.1769 (inf)	loss_scale 512.0000 (574.4108)	mem 12860MB
[2024-05-28 22:32:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3200/5004]	eta 0:15:47 lr 0.000010	 wd 0.0500	time 0.3713 (0.5250)	loss 0.8588 (1.1437)	grad_norm 2.6172 (inf)	loss_scale 512.0000 (572.4611)	mem 12860MB
[2024-05-28 22:33:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3300/5004]	eta 0:14:54 lr 0.000010	 wd 0.0500	time 0.5105 (0.5249)	loss 1.3072 (1.1441)	grad_norm 3.7798 (inf)	loss_scale 512.0000 (570.6295)	mem 12860MB
[2024-05-28 22:33:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3400/5004]	eta 0:14:01 lr 0.000010	 wd 0.0500	time 0.4381 (0.5247)	loss 1.6586 (1.1443)	grad_norm 3.5287 (inf)	loss_scale 512.0000 (568.9056)	mem 12860MB
[2024-05-28 22:34:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3500/5004]	eta 0:13:08 lr 0.000010	 wd 0.0500	time 0.3938 (0.5245)	loss 0.9454 (1.1439)	grad_norm 3.3413 (inf)	loss_scale 512.0000 (567.2802)	mem 12860MB
[2024-05-28 22:35:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3600/5004]	eta 0:12:16 lr 0.000010	 wd 0.0500	time 0.5153 (0.5248)	loss 0.8675 (1.1448)	grad_norm 3.5056 (inf)	loss_scale 512.0000 (565.7451)	mem 12860MB
[2024-05-28 22:36:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3700/5004]	eta 0:11:24 lr 0.000010	 wd 0.0500	time 0.4629 (0.5246)	loss 1.5015 (1.1454)	grad_norm 2.8469 (inf)	loss_scale 512.0000 (564.2929)	mem 12860MB
[2024-05-28 22:37:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3800/5004]	eta 0:10:31 lr 0.000010	 wd 0.0500	time 0.4205 (0.5247)	loss 1.3289 (1.1456)	grad_norm 3.0912 (inf)	loss_scale 512.0000 (562.9171)	mem 12860MB
[2024-05-28 22:38:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][3900/5004]	eta 0:09:39 lr 0.000010	 wd 0.0500	time 0.4371 (0.5247)	loss 0.8601 (1.1461)	grad_norm 4.4131 (inf)	loss_scale 512.0000 (561.6119)	mem 12860MB
[2024-05-28 22:39:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4000/5004]	eta 0:08:46 lr 0.000010	 wd 0.0500	time 0.4398 (0.5246)	loss 1.2870 (1.1477)	grad_norm 3.7616 (inf)	loss_scale 512.0000 (560.3719)	mem 12860MB
[2024-05-28 22:40:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4100/5004]	eta 0:07:55 lr 0.000010	 wd 0.0500	time 0.5113 (0.5257)	loss 1.2207 (1.1488)	grad_norm 4.2686 (inf)	loss_scale 512.0000 (559.1924)	mem 12860MB
[2024-05-28 22:41:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4200/5004]	eta 0:07:03 lr 0.000010	 wd 0.0500	time 0.4824 (0.5267)	loss 1.3247 (1.1483)	grad_norm 3.4909 (inf)	loss_scale 512.0000 (558.0690)	mem 12860MB
[2024-05-28 22:42:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4300/5004]	eta 0:06:10 lr 0.000010	 wd 0.0500	time 0.5561 (0.5270)	loss 1.3824 (1.1474)	grad_norm 2.7793 (inf)	loss_scale 512.0000 (556.9979)	mem 12860MB
[2024-05-28 22:42:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4400/5004]	eta 0:05:18 lr 0.000010	 wd 0.0500	time 0.4769 (0.5272)	loss 0.8409 (1.1477)	grad_norm 4.1869 (inf)	loss_scale 512.0000 (555.9755)	mem 12860MB
[2024-05-28 22:43:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4500/5004]	eta 0:04:25 lr 0.000010	 wd 0.0500	time 0.5097 (0.5271)	loss 0.8171 (1.1473)	grad_norm 3.5625 (inf)	loss_scale 512.0000 (554.9984)	mem 12860MB
[2024-05-28 22:44:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4600/5004]	eta 0:03:32 lr 0.000010	 wd 0.0500	time 0.4767 (0.5272)	loss 1.3184 (1.1479)	grad_norm 3.4614 (inf)	loss_scale 512.0000 (554.0639)	mem 12860MB
[2024-05-28 22:45:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4700/5004]	eta 0:02:40 lr 0.000010	 wd 0.0500	time 0.4606 (0.5276)	loss 1.0299 (1.1487)	grad_norm 3.1135 (inf)	loss_scale 512.0000 (553.1691)	mem 12860MB
[2024-05-28 22:46:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4800/5004]	eta 0:01:47 lr 0.000010	 wd 0.0500	time 0.4647 (0.5279)	loss 1.0610 (1.1486)	grad_norm 2.8227 (inf)	loss_scale 512.0000 (552.3116)	mem 12860MB
[2024-05-28 22:47:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][4900/5004]	eta 0:00:54 lr 0.000010	 wd 0.0500	time 0.4818 (0.5288)	loss 1.3731 (1.1491)	grad_norm 3.6023 (inf)	loss_scale 512.0000 (551.4891)	mem 12860MB
[2024-05-28 22:48:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [6/30][5000/5004]	eta 0:00:02 lr 0.000010	 wd 0.0500	time 0.3546 (0.5281)	loss 1.2255 (1.1492)	grad_norm 2.7819 (inf)	loss_scale 512.0000 (550.6995)	mem 12860MB
[2024-05-28 22:48:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 6 training takes 0:44:04
[2024-05-28 22:48:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.267 (8.267)	Loss 0.3889 (0.3889)	Acc@1 94.141 (94.141)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-28 22:48:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.118 (0.226)	Loss 0.7217 (0.6256)	Acc@1 82.422 (87.299)	Acc@5 97.656 (98.140)	Mem 12860MB
[2024-05-28 22:48:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.272 Acc@5 97.906
[2024-05-28 22:48:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-05-28 22:48:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.27%
[2024-05-28 22:49:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][0/5004]	eta 12:52:19 lr 0.000010	 wd 0.0500	time 9.2605 (9.2605)	loss 1.2877 (1.2877)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:49:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][100/5004]	eta 0:48:53 lr 0.000010	 wd 0.0500	time 0.3745 (0.5982)	loss 1.1578 (1.1410)	grad_norm 2.6035 (3.5610)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:50:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][200/5004]	eta 0:44:11 lr 0.000010	 wd 0.0500	time 0.4045 (0.5519)	loss 1.0953 (1.1615)	grad_norm 3.1836 (3.8469)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:51:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][300/5004]	eta 0:42:36 lr 0.000010	 wd 0.0500	time 0.4790 (0.5435)	loss 1.5620 (1.1529)	grad_norm 4.3120 (3.8908)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:52:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][400/5004]	eta 0:41:36 lr 0.000010	 wd 0.0500	time 0.4397 (0.5422)	loss 1.4191 (1.1518)	grad_norm 3.3044 (4.2299)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:53:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][500/5004]	eta 0:40:24 lr 0.000010	 wd 0.0500	time 0.4214 (0.5383)	loss 1.0100 (1.1440)	grad_norm 5.0104 (4.1948)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:54:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][600/5004]	eta 0:39:25 lr 0.000010	 wd 0.0500	time 0.4393 (0.5371)	loss 1.5438 (1.1493)	grad_norm 2.9222 (4.1099)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:55:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][700/5004]	eta 0:38:21 lr 0.000010	 wd 0.0500	time 0.4302 (0.5348)	loss 1.5564 (1.1530)	grad_norm 2.6489 (4.0807)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:55:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][800/5004]	eta 0:37:20 lr 0.000010	 wd 0.0500	time 0.4318 (0.5330)	loss 0.9732 (1.1554)	grad_norm 3.1739 (4.1130)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:56:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][900/5004]	eta 0:36:18 lr 0.000010	 wd 0.0500	time 0.4194 (0.5308)	loss 0.9489 (1.1481)	grad_norm 2.4524 (4.2156)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:57:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1000/5004]	eta 0:35:20 lr 0.000010	 wd 0.0500	time 0.3871 (0.5296)	loss 0.9303 (1.1484)	grad_norm 3.2169 (4.1759)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:58:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1100/5004]	eta 0:34:22 lr 0.000010	 wd 0.0500	time 0.5701 (0.5284)	loss 1.0588 (1.1510)	grad_norm 10.7603 (4.2285)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 22:59:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1200/5004]	eta 0:33:23 lr 0.000010	 wd 0.0500	time 0.4762 (0.5267)	loss 1.2902 (1.1531)	grad_norm 3.2882 (4.2423)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:00:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1300/5004]	eta 0:32:28 lr 0.000010	 wd 0.0500	time 0.3794 (0.5261)	loss 1.0085 (1.1508)	grad_norm 4.9337 (4.2295)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:01:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1400/5004]	eta 0:31:32 lr 0.000010	 wd 0.0500	time 0.4107 (0.5252)	loss 1.4783 (1.1510)	grad_norm 4.5465 (4.1967)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:01:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1500/5004]	eta 0:30:36 lr 0.000010	 wd 0.0500	time 0.3697 (0.5240)	loss 0.8045 (1.1498)	grad_norm 3.1805 (4.1780)	loss_scale 1024.0000 (527.6909)	mem 12860MB
[2024-05-28 23:02:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1600/5004]	eta 0:29:47 lr 0.000010	 wd 0.0500	time 0.4318 (0.5252)	loss 1.3874 (1.1502)	grad_norm 3.2536 (4.1430)	loss_scale 1024.0000 (558.6908)	mem 12860MB
[2024-05-28 23:03:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1700/5004]	eta 0:28:51 lr 0.000010	 wd 0.0500	time 0.4464 (0.5240)	loss 1.3012 (1.1511)	grad_norm 2.8323 (4.1623)	loss_scale 1024.0000 (586.0459)	mem 12860MB
[2024-05-28 23:04:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1800/5004]	eta 0:27:55 lr 0.000010	 wd 0.0500	time 0.4728 (0.5230)	loss 0.7788 (1.1497)	grad_norm 3.3521 (4.1494)	loss_scale 1024.0000 (610.3631)	mem 12860MB
[2024-05-28 23:05:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][1900/5004]	eta 0:27:04 lr 0.000010	 wd 0.0500	time 0.3717 (0.5234)	loss 1.4054 (1.1480)	grad_norm 4.3095 (4.1302)	loss_scale 1024.0000 (632.1220)	mem 12860MB
[2024-05-28 23:06:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2000/5004]	eta 0:26:11 lr 0.000010	 wd 0.0500	time 0.3637 (0.5230)	loss 0.7819 (1.1451)	grad_norm 2.9435 (4.1169)	loss_scale 1024.0000 (651.7061)	mem 12860MB
[2024-05-28 23:07:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2100/5004]	eta 0:25:16 lr 0.000010	 wd 0.0500	time 0.4654 (0.5222)	loss 0.9455 (1.1451)	grad_norm 2.5910 (4.0844)	loss_scale 1024.0000 (669.4260)	mem 12860MB
[2024-05-28 23:08:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2200/5004]	eta 0:24:23 lr 0.000010	 wd 0.0500	time 0.4180 (0.5220)	loss 0.6791 (1.1445)	grad_norm 2.8877 (4.0757)	loss_scale 1024.0000 (685.5357)	mem 12860MB
[2024-05-28 23:08:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2300/5004]	eta 0:23:28 lr 0.000010	 wd 0.0500	time 0.3972 (0.5210)	loss 0.7380 (1.1420)	grad_norm 4.2099 (4.0626)	loss_scale 1024.0000 (700.2451)	mem 12860MB
[2024-05-28 23:09:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2400/5004]	eta 0:22:35 lr 0.000010	 wd 0.0500	time 0.3849 (0.5207)	loss 1.0996 (1.1410)	grad_norm 4.0279 (4.0965)	loss_scale 1024.0000 (713.7293)	mem 12860MB
[2024-05-28 23:10:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2500/5004]	eta 0:21:44 lr 0.000010	 wd 0.0500	time 0.4748 (0.5208)	loss 1.0193 (1.1412)	grad_norm 5.6914 (4.0746)	loss_scale 1024.0000 (726.1351)	mem 12860MB
[2024-05-28 23:11:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2600/5004]	eta 0:20:50 lr 0.000010	 wd 0.0500	time 0.3969 (0.5204)	loss 1.2600 (1.1407)	grad_norm 3.0775 (4.0614)	loss_scale 1024.0000 (737.5871)	mem 12860MB
[2024-05-28 23:12:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2700/5004]	eta 0:19:59 lr 0.000010	 wd 0.0500	time 0.3958 (0.5206)	loss 1.2716 (1.1421)	grad_norm 3.3687 (4.0514)	loss_scale 1024.0000 (748.1910)	mem 12860MB
[2024-05-28 23:13:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2800/5004]	eta 0:19:06 lr 0.000010	 wd 0.0500	time 0.4214 (0.5204)	loss 0.9138 (1.1413)	grad_norm 2.6642 (4.0331)	loss_scale 1024.0000 (758.0378)	mem 12860MB
[2024-05-28 23:14:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][2900/5004]	eta 0:18:14 lr 0.000010	 wd 0.0500	time 0.4300 (0.5203)	loss 1.3166 (1.1410)	grad_norm 3.8760 (4.0607)	loss_scale 1024.0000 (767.2058)	mem 12860MB
[2024-05-28 23:14:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3000/5004]	eta 0:17:21 lr 0.000010	 wd 0.0500	time 0.4116 (0.5197)	loss 1.4187 (1.1413)	grad_norm inf (inf)	loss_scale 512.0000 (775.4215)	mem 12860MB
[2024-05-28 23:15:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3100/5004]	eta 0:16:29 lr 0.000010	 wd 0.0500	time 0.3820 (0.5197)	loss 1.2753 (1.1401)	grad_norm 5.5458 (inf)	loss_scale 512.0000 (766.9268)	mem 12860MB
[2024-05-28 23:16:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3200/5004]	eta 0:15:37 lr 0.000010	 wd 0.0500	time 0.4651 (0.5198)	loss 1.2728 (1.1396)	grad_norm 3.6161 (inf)	loss_scale 512.0000 (758.9628)	mem 12860MB
[2024-05-28 23:17:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3300/5004]	eta 0:14:46 lr 0.000010	 wd 0.0500	time 0.4216 (0.5203)	loss 1.5539 (1.1394)	grad_norm 3.0371 (inf)	loss_scale 512.0000 (751.4814)	mem 12860MB
[2024-05-28 23:18:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3400/5004]	eta 0:13:54 lr 0.000010	 wd 0.0500	time 0.4524 (0.5205)	loss 1.5621 (1.1398)	grad_norm 3.5196 (inf)	loss_scale 512.0000 (744.4399)	mem 12860MB
[2024-05-28 23:19:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3500/5004]	eta 0:13:03 lr 0.000010	 wd 0.0500	time 0.4093 (0.5206)	loss 1.1335 (1.1405)	grad_norm 2.9405 (inf)	loss_scale 512.0000 (737.8006)	mem 12860MB
[2024-05-28 23:20:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3600/5004]	eta 0:12:11 lr 0.000010	 wd 0.0500	time 0.3801 (0.5209)	loss 0.8961 (1.1408)	grad_norm 6.2195 (inf)	loss_scale 512.0000 (731.5301)	mem 12860MB
[2024-05-28 23:21:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3700/5004]	eta 0:11:19 lr 0.000010	 wd 0.0500	time 0.4634 (0.5209)	loss 1.1948 (1.1403)	grad_norm 2.8724 (inf)	loss_scale 512.0000 (725.5985)	mem 12860MB
[2024-05-28 23:21:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3800/5004]	eta 0:10:27 lr 0.000010	 wd 0.0500	time 0.5185 (0.5212)	loss 0.7479 (1.1403)	grad_norm 3.6815 (inf)	loss_scale 512.0000 (719.9790)	mem 12860MB
[2024-05-28 23:22:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][3900/5004]	eta 0:09:35 lr 0.000010	 wd 0.0500	time 0.4152 (0.5215)	loss 1.5919 (1.1410)	grad_norm 2.6673 (inf)	loss_scale 512.0000 (714.6475)	mem 12860MB
[2024-05-28 23:23:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4000/5004]	eta 0:08:43 lr 0.000010	 wd 0.0500	time 0.3713 (0.5214)	loss 1.3289 (1.1413)	grad_norm 3.7528 (inf)	loss_scale 512.0000 (709.5826)	mem 12860MB
[2024-05-28 23:24:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4100/5004]	eta 0:07:51 lr 0.000010	 wd 0.0500	time 0.3818 (0.5215)	loss 1.3628 (1.1422)	grad_norm 3.0629 (inf)	loss_scale 512.0000 (704.7647)	mem 12860MB
[2024-05-28 23:25:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4200/5004]	eta 0:06:59 lr 0.000010	 wd 0.0500	time 0.3984 (0.5215)	loss 0.8111 (1.1429)	grad_norm 3.0839 (inf)	loss_scale 512.0000 (700.1761)	mem 12860MB
[2024-05-28 23:26:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4300/5004]	eta 0:06:07 lr 0.000010	 wd 0.0500	time 0.4460 (0.5223)	loss 1.1716 (1.1434)	grad_norm 3.2189 (inf)	loss_scale 512.0000 (695.8010)	mem 12860MB
[2024-05-28 23:27:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4400/5004]	eta 0:05:15 lr 0.000010	 wd 0.0500	time 0.4389 (0.5229)	loss 1.1893 (1.1430)	grad_norm 3.1616 (inf)	loss_scale 512.0000 (691.6246)	mem 12860MB
[2024-05-28 23:28:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4500/5004]	eta 0:04:23 lr 0.000010	 wd 0.0500	time 0.4933 (0.5230)	loss 0.7212 (1.1423)	grad_norm 3.7717 (inf)	loss_scale 512.0000 (687.6339)	mem 12860MB
[2024-05-28 23:29:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4600/5004]	eta 0:03:31 lr 0.000010	 wd 0.0500	time 0.4712 (0.5236)	loss 0.8050 (1.1422)	grad_norm 3.4111 (inf)	loss_scale 512.0000 (683.8166)	mem 12860MB
[2024-05-28 23:29:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4700/5004]	eta 0:02:39 lr 0.000010	 wd 0.0500	time 0.4292 (0.5241)	loss 0.9121 (1.1418)	grad_norm 2.9375 (inf)	loss_scale 512.0000 (680.1617)	mem 12860MB
[2024-05-28 23:30:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4800/5004]	eta 0:01:46 lr 0.000010	 wd 0.0500	time 0.4304 (0.5244)	loss 1.1392 (1.1415)	grad_norm 2.5948 (inf)	loss_scale 512.0000 (676.6590)	mem 12860MB
[2024-05-28 23:31:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][4900/5004]	eta 0:00:54 lr 0.000010	 wd 0.0500	time 0.3869 (0.5246)	loss 1.1060 (1.1413)	grad_norm 4.0538 (inf)	loss_scale 512.0000 (673.2993)	mem 12860MB
[2024-05-28 23:32:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [7/30][5000/5004]	eta 0:00:02 lr 0.000010	 wd 0.0500	time 0.3940 (0.5244)	loss 0.7355 (1.1421)	grad_norm 3.4120 (inf)	loss_scale 512.0000 (670.0740)	mem 12860MB
[2024-05-28 23:32:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 7 training takes 0:43:46
[2024-05-28 23:32:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.174 (8.174)	Loss 0.4253 (0.4253)	Acc@1 94.141 (94.141)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-28 23:33:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.145 (0.217)	Loss 0.7793 (0.6592)	Acc@1 81.250 (87.392)	Acc@5 97.656 (98.163)	Mem 12860MB
[2024-05-28 23:33:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.318 Acc@5 97.934
[2024-05-28 23:33:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-05-28 23:33:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.32%
[2024-05-28 23:33:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][0/5004]	eta 13:03:11 lr 0.000010	 wd 0.0500	time 9.3909 (9.3909)	loss 0.9870 (0.9870)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:34:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][100/5004]	eta 0:47:37 lr 0.000010	 wd 0.0500	time 0.4907 (0.5827)	loss 0.8644 (1.1655)	grad_norm 3.7092 (4.1096)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:35:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][200/5004]	eta 0:43:46 lr 0.000010	 wd 0.0500	time 0.4589 (0.5468)	loss 0.9985 (1.1533)	grad_norm 3.1937 (3.9273)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:35:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][300/5004]	eta 0:41:45 lr 0.000010	 wd 0.0500	time 0.4463 (0.5325)	loss 1.0496 (1.1525)	grad_norm 7.2225 (3.9397)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:36:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][400/5004]	eta 0:40:18 lr 0.000010	 wd 0.0500	time 0.4035 (0.5253)	loss 1.2605 (1.1567)	grad_norm 3.5552 (3.9303)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:37:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][500/5004]	eta 0:39:04 lr 0.000010	 wd 0.0500	time 0.3585 (0.5205)	loss 1.3594 (1.1614)	grad_norm 10.5194 (3.8653)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:38:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][600/5004]	eta 0:38:09 lr 0.000010	 wd 0.0500	time 0.4241 (0.5198)	loss 1.1777 (1.1667)	grad_norm 12.7756 (4.1558)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:39:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][700/5004]	eta 0:37:10 lr 0.000010	 wd 0.0500	time 0.4742 (0.5182)	loss 1.3096 (1.1651)	grad_norm 2.9220 (4.1491)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:40:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][800/5004]	eta 0:36:12 lr 0.000010	 wd 0.0500	time 0.4132 (0.5167)	loss 1.5191 (1.1644)	grad_norm 3.2122 (4.0974)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:40:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][900/5004]	eta 0:35:28 lr 0.000010	 wd 0.0500	time 0.4175 (0.5186)	loss 1.3284 (1.1600)	grad_norm 3.7441 (4.0825)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:41:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1000/5004]	eta 0:34:29 lr 0.000010	 wd 0.0500	time 0.4865 (0.5170)	loss 0.6760 (1.1617)	grad_norm 2.9112 (4.0498)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:42:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1100/5004]	eta 0:33:34 lr 0.000010	 wd 0.0500	time 0.4397 (0.5161)	loss 1.1787 (1.1605)	grad_norm 2.7571 (4.0457)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:43:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1200/5004]	eta 0:32:38 lr 0.000010	 wd 0.0500	time 0.4092 (0.5147)	loss 1.4918 (1.1625)	grad_norm 3.3995 (4.0060)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:44:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1300/5004]	eta 0:31:43 lr 0.000010	 wd 0.0500	time 0.4061 (0.5138)	loss 0.8171 (1.1593)	grad_norm 3.4834 (3.9719)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:45:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1400/5004]	eta 0:30:50 lr 0.000010	 wd 0.0500	time 0.3652 (0.5135)	loss 1.3852 (1.1577)	grad_norm 3.6564 (3.9439)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:46:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1500/5004]	eta 0:30:08 lr 0.000010	 wd 0.0500	time 0.4302 (0.5162)	loss 1.2481 (1.1579)	grad_norm 2.7074 (3.9504)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:46:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1600/5004]	eta 0:29:19 lr 0.000010	 wd 0.0500	time 0.5574 (0.5170)	loss 1.0715 (1.1552)	grad_norm 3.9203 (3.9725)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:47:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1700/5004]	eta 0:28:31 lr 0.000010	 wd 0.0500	time 0.4392 (0.5179)	loss 1.3310 (1.1534)	grad_norm 3.1759 (3.9591)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:48:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1800/5004]	eta 0:27:41 lr 0.000010	 wd 0.0500	time 0.4102 (0.5186)	loss 1.4240 (1.1518)	grad_norm 3.4018 (3.9464)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:49:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][1900/5004]	eta 0:26:53 lr 0.000010	 wd 0.0500	time 0.5179 (0.5198)	loss 1.4628 (1.1511)	grad_norm 3.1951 (3.9394)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-28 23:50:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2000/5004]	eta 0:26:05 lr 0.000010	 wd 0.0500	time 0.4584 (0.5211)	loss 1.4524 (1.1502)	grad_norm 2.9761 (3.9439)	loss_scale 1024.0000 (513.5352)	mem 12860MB
[2024-05-28 23:51:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2100/5004]	eta 0:25:15 lr 0.000010	 wd 0.0500	time 0.4738 (0.5217)	loss 1.6486 (1.1474)	grad_norm 2.9916 (3.9419)	loss_scale 1024.0000 (537.8315)	mem 12860MB
[2024-05-28 23:52:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2200/5004]	eta 0:24:23 lr 0.000010	 wd 0.0500	time 0.4185 (0.5221)	loss 1.3305 (1.1461)	grad_norm 3.0956 (3.9369)	loss_scale 1024.0000 (559.9200)	mem 12860MB
[2024-05-28 23:53:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2300/5004]	eta 0:23:33 lr 0.000010	 wd 0.0500	time 0.4305 (0.5227)	loss 1.4939 (1.1468)	grad_norm 3.5357 (3.9321)	loss_scale 1024.0000 (580.0887)	mem 12860MB
[2024-05-28 23:54:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2400/5004]	eta 0:22:42 lr 0.000010	 wd 0.0500	time 0.4210 (0.5234)	loss 1.0145 (1.1467)	grad_norm 2.9004 (3.9270)	loss_scale 1024.0000 (598.5773)	mem 12860MB
[2024-05-28 23:55:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2500/5004]	eta 0:21:51 lr 0.000010	 wd 0.0500	time 0.4198 (0.5238)	loss 1.0370 (1.1469)	grad_norm 3.3198 (3.9142)	loss_scale 1024.0000 (615.5874)	mem 12860MB
[2024-05-28 23:55:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2600/5004]	eta 0:20:57 lr 0.000010	 wd 0.0500	time 0.3949 (0.5230)	loss 1.2150 (1.1464)	grad_norm 3.2113 (3.9645)	loss_scale 1024.0000 (631.2895)	mem 12860MB
[2024-05-28 23:56:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2700/5004]	eta 0:20:03 lr 0.000010	 wd 0.0500	time 0.3901 (0.5222)	loss 1.2906 (1.1442)	grad_norm 30.7136 (3.9935)	loss_scale 1024.0000 (645.8290)	mem 12860MB
[2024-05-28 23:57:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2800/5004]	eta 0:19:09 lr 0.000010	 wd 0.0500	time 0.4307 (0.5215)	loss 1.2024 (1.1447)	grad_norm 3.1256 (3.9916)	loss_scale 1024.0000 (659.3302)	mem 12860MB
[2024-05-28 23:58:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][2900/5004]	eta 0:18:15 lr 0.000010	 wd 0.0500	time 0.4823 (0.5208)	loss 1.2725 (1.1436)	grad_norm 2.8248 (3.9937)	loss_scale 1024.0000 (671.9007)	mem 12860MB
[2024-05-28 23:59:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3000/5004]	eta 0:17:23 lr 0.000010	 wd 0.0500	time 0.4075 (0.5205)	loss 1.4000 (1.1437)	grad_norm 2.8414 (3.9814)	loss_scale 1024.0000 (683.6335)	mem 12860MB
[2024-05-29 00:00:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3100/5004]	eta 0:16:30 lr 0.000009	 wd 0.0500	time 0.3894 (0.5202)	loss 1.3131 (1.1426)	grad_norm 4.6832 (3.9891)	loss_scale 1024.0000 (694.6095)	mem 12860MB
[2024-05-29 00:00:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3200/5004]	eta 0:15:37 lr 0.000009	 wd 0.0500	time 0.4353 (0.5198)	loss 0.7822 (1.1428)	grad_norm 3.6095 (3.9814)	loss_scale 1024.0000 (704.8997)	mem 12860MB
[2024-05-29 00:01:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3300/5004]	eta 0:14:45 lr 0.000009	 wd 0.0500	time 0.4536 (0.5196)	loss 1.2565 (1.1430)	grad_norm 3.4224 (3.9764)	loss_scale 1024.0000 (714.5665)	mem 12860MB
[2024-05-29 00:02:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3400/5004]	eta 0:13:52 lr 0.000009	 wd 0.0500	time 0.3606 (0.5192)	loss 0.7690 (1.1422)	grad_norm 3.6212 (3.9637)	loss_scale 1024.0000 (723.6648)	mem 12860MB
[2024-05-29 00:03:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3500/5004]	eta 0:13:00 lr 0.000009	 wd 0.0500	time 0.4852 (0.5192)	loss 0.8898 (1.1429)	grad_norm 3.4031 (3.9642)	loss_scale 1024.0000 (732.2434)	mem 12860MB
[2024-05-29 00:04:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3600/5004]	eta 0:12:08 lr 0.000009	 wd 0.0500	time 0.4251 (0.5190)	loss 0.9664 (1.1434)	grad_norm 3.6447 (3.9602)	loss_scale 1024.0000 (740.3455)	mem 12860MB
[2024-05-29 00:05:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3700/5004]	eta 0:11:16 lr 0.000009	 wd 0.0500	time 0.4617 (0.5190)	loss 1.3337 (1.1431)	grad_norm 3.3479 (3.9732)	loss_scale 1024.0000 (748.0097)	mem 12860MB
[2024-05-29 00:06:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3800/5004]	eta 0:10:25 lr 0.000009	 wd 0.0500	time 0.4207 (0.5196)	loss 1.3687 (1.1426)	grad_norm 3.5372 (3.9633)	loss_scale 1024.0000 (755.2707)	mem 12860MB
[2024-05-29 00:06:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][3900/5004]	eta 0:09:33 lr 0.000009	 wd 0.0500	time 0.3886 (0.5195)	loss 1.4679 (1.1429)	grad_norm 3.8854 (3.9567)	loss_scale 1024.0000 (762.1594)	mem 12860MB
[2024-05-29 00:07:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4000/5004]	eta 0:08:41 lr 0.000009	 wd 0.0500	time 0.4361 (0.5194)	loss 1.0258 (1.1422)	grad_norm 3.8027 (3.9400)	loss_scale 1024.0000 (768.7038)	mem 12860MB
[2024-05-29 00:08:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4100/5004]	eta 0:07:49 lr 0.000009	 wd 0.0500	time 0.4700 (0.5197)	loss 1.6352 (1.1431)	grad_norm 3.2253 (3.9357)	loss_scale 1024.0000 (774.9290)	mem 12860MB
[2024-05-29 00:09:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4200/5004]	eta 0:06:57 lr 0.000009	 wd 0.0500	time 0.4970 (0.5197)	loss 1.1597 (1.1431)	grad_norm 4.8905 (3.9305)	loss_scale 1024.0000 (780.8579)	mem 12860MB
[2024-05-29 00:10:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4300/5004]	eta 0:06:06 lr 0.000009	 wd 0.0500	time 0.7034 (0.5209)	loss 1.3868 (1.1432)	grad_norm 3.0833 (3.9322)	loss_scale 1024.0000 (786.5110)	mem 12860MB
[2024-05-29 00:11:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4400/5004]	eta 0:05:16 lr 0.000009	 wd 0.0500	time 0.5013 (0.5236)	loss 1.2956 (1.1420)	grad_norm 3.2619 (3.9255)	loss_scale 1024.0000 (791.9073)	mem 12860MB
[2024-05-29 00:12:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4500/5004]	eta 0:04:24 lr 0.000009	 wd 0.0500	time 0.4486 (0.5257)	loss 1.0118 (1.1421)	grad_norm 2.7610 (3.9289)	loss_scale 1024.0000 (797.0638)	mem 12860MB
[2024-05-29 00:13:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4600/5004]	eta 0:03:32 lr 0.000009	 wd 0.0500	time 0.4553 (0.5259)	loss 1.4340 (1.1419)	grad_norm 3.1041 (3.9509)	loss_scale 1024.0000 (801.9961)	mem 12860MB
[2024-05-29 00:14:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4700/5004]	eta 0:02:39 lr 0.000009	 wd 0.0500	time 0.4103 (0.5261)	loss 1.2728 (1.1420)	grad_norm 6.5109 (3.9518)	loss_scale 1024.0000 (806.7186)	mem 12860MB
[2024-05-29 00:15:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4800/5004]	eta 0:01:47 lr 0.000009	 wd 0.0500	time 0.4255 (0.5271)	loss 0.8074 (1.1413)	grad_norm 3.9492 (3.9498)	loss_scale 1024.0000 (811.2443)	mem 12860MB
[2024-05-29 00:16:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][4900/5004]	eta 0:00:54 lr 0.000009	 wd 0.0500	time 0.4365 (0.5278)	loss 1.2005 (1.1410)	grad_norm 8.3764 (3.9461)	loss_scale 1024.0000 (815.5854)	mem 12860MB
[2024-05-29 00:17:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [8/30][5000/5004]	eta 0:00:02 lr 0.000009	 wd 0.0500	time 0.3439 (0.5268)	loss 0.8658 (1.1407)	grad_norm 3.6277 (3.9405)	loss_scale 1024.0000 (819.7528)	mem 12860MB
[2024-05-29 00:17:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 8 training takes 0:43:58
[2024-05-29 00:17:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.180 (7.180)	Loss 0.3938 (0.3938)	Acc@1 94.531 (94.531)	Acc@5 98.828 (98.828)	Mem 12860MB
[2024-05-29 00:17:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.100 (0.190)	Loss 0.7236 (0.6270)	Acc@1 83.594 (87.504)	Acc@5 97.656 (98.167)	Mem 12860MB
[2024-05-29 00:17:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.430 Acc@5 97.992
[2024-05-29 00:17:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-05-29 00:17:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.43%
[2024-05-29 00:17:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][0/5004]	eta 13:34:45 lr 0.000009	 wd 0.0500	time 9.7692 (9.7692)	loss 1.2951 (1.2951)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:18:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][100/5004]	eta 0:48:04 lr 0.000009	 wd 0.0500	time 0.4289 (0.5881)	loss 1.2924 (1.1519)	grad_norm 3.3116 (3.9738)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:19:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][200/5004]	eta 0:43:39 lr 0.000009	 wd 0.0500	time 0.4870 (0.5453)	loss 1.4818 (1.1649)	grad_norm 3.2840 (3.8420)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:20:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][300/5004]	eta 0:41:44 lr 0.000009	 wd 0.0500	time 0.3774 (0.5323)	loss 0.9484 (1.1526)	grad_norm 4.1656 (3.8944)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:21:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][400/5004]	eta 0:40:54 lr 0.000009	 wd 0.0500	time 0.4643 (0.5330)	loss 0.6727 (1.1534)	grad_norm 3.4909 (3.8247)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:22:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][500/5004]	eta 0:39:53 lr 0.000009	 wd 0.0500	time 0.3833 (0.5314)	loss 1.2148 (1.1543)	grad_norm 3.8686 (3.9253)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:22:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][600/5004]	eta 0:38:47 lr 0.000009	 wd 0.0500	time 0.4146 (0.5286)	loss 0.8282 (1.1561)	grad_norm 3.0750 (3.9434)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:23:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][700/5004]	eta 0:37:36 lr 0.000009	 wd 0.0500	time 0.4011 (0.5242)	loss 1.2249 (1.1519)	grad_norm 2.6581 (3.8813)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:24:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][800/5004]	eta 0:36:38 lr 0.000009	 wd 0.0500	time 0.4122 (0.5229)	loss 1.1357 (1.1507)	grad_norm 3.3858 (3.9222)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:25:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][900/5004]	eta 0:35:36 lr 0.000009	 wd 0.0500	time 0.4364 (0.5205)	loss 1.5584 (1.1499)	grad_norm 3.3820 (4.0796)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 00:26:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1000/5004]	eta 0:34:44 lr 0.000009	 wd 0.0500	time 0.3685 (0.5205)	loss 1.3654 (1.1440)	grad_norm 3.6845 (4.0275)	loss_scale 2048.0000 (1034.2298)	mem 12860MB
[2024-05-29 00:27:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1100/5004]	eta 0:33:51 lr 0.000009	 wd 0.0500	time 0.3939 (0.5203)	loss 1.4276 (1.1425)	grad_norm 3.1527 (4.0387)	loss_scale 2048.0000 (1126.3070)	mem 12860MB
[2024-05-29 00:28:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1200/5004]	eta 0:32:59 lr 0.000009	 wd 0.0500	time 0.4311 (0.5204)	loss 1.0199 (1.1417)	grad_norm 3.1310 (4.0777)	loss_scale 2048.0000 (1203.0508)	mem 12860MB
[2024-05-29 00:28:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1300/5004]	eta 0:32:00 lr 0.000009	 wd 0.0500	time 0.4015 (0.5186)	loss 1.1390 (1.1406)	grad_norm 4.6586 (4.1140)	loss_scale 2048.0000 (1267.9969)	mem 12860MB
[2024-05-29 00:29:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1400/5004]	eta 0:31:06 lr 0.000009	 wd 0.0500	time 0.3995 (0.5179)	loss 1.2296 (1.1397)	grad_norm 3.6368 (4.0901)	loss_scale 2048.0000 (1323.6717)	mem 12860MB
[2024-05-29 00:30:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1500/5004]	eta 0:30:15 lr 0.000009	 wd 0.0500	time 0.4880 (0.5181)	loss 1.2777 (1.1403)	grad_norm 3.1992 (4.0680)	loss_scale 2048.0000 (1371.9280)	mem 12860MB
[2024-05-29 00:31:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1600/5004]	eta 0:29:23 lr 0.000009	 wd 0.0500	time 0.4260 (0.5179)	loss 1.4666 (1.1413)	grad_norm 12.0538 (4.0953)	loss_scale 2048.0000 (1414.1562)	mem 12860MB
[2024-05-29 00:32:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1700/5004]	eta 0:28:30 lr 0.000009	 wd 0.0500	time 0.6799 (0.5176)	loss 0.8060 (1.1410)	grad_norm 3.4044 (4.0684)	loss_scale 2048.0000 (1451.4192)	mem 12860MB
[2024-05-29 00:33:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1800/5004]	eta 0:27:39 lr 0.000009	 wd 0.0500	time 0.4862 (0.5179)	loss 1.1189 (1.1413)	grad_norm 3.4312 (4.0442)	loss_scale 2048.0000 (1484.5441)	mem 12860MB
[2024-05-29 00:34:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][1900/5004]	eta 0:26:49 lr 0.000009	 wd 0.0500	time 0.4370 (0.5186)	loss 1.3505 (1.1405)	grad_norm 2.9118 (4.0298)	loss_scale 2048.0000 (1514.1841)	mem 12860MB
[2024-05-29 00:35:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2000/5004]	eta 0:26:01 lr 0.000009	 wd 0.0500	time 0.4883 (0.5200)	loss 1.2573 (1.1402)	grad_norm 8.1591 (4.0578)	loss_scale 2048.0000 (1540.8616)	mem 12860MB
[2024-05-29 00:35:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2100/5004]	eta 0:25:14 lr 0.000009	 wd 0.0500	time 0.4045 (0.5214)	loss 1.0193 (1.1422)	grad_norm 4.3311 (4.0848)	loss_scale 2048.0000 (1564.9995)	mem 12860MB
[2024-05-29 00:36:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2200/5004]	eta 0:24:20 lr 0.000009	 wd 0.0500	time 0.3852 (0.5209)	loss 1.4080 (1.1422)	grad_norm 3.5539 (4.0999)	loss_scale 2048.0000 (1586.9441)	mem 12860MB
[2024-05-29 00:37:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2300/5004]	eta 0:23:27 lr 0.000009	 wd 0.0500	time 0.4073 (0.5206)	loss 1.0992 (1.1427)	grad_norm 3.1548 (4.1132)	loss_scale 2048.0000 (1606.9813)	mem 12860MB
[2024-05-29 00:38:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2400/5004]	eta 0:22:34 lr 0.000009	 wd 0.0500	time 0.4231 (0.5201)	loss 0.7536 (1.1424)	grad_norm 3.3807 (4.1082)	loss_scale 2048.0000 (1625.3494)	mem 12860MB
[2024-05-29 00:39:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2500/5004]	eta 0:21:41 lr 0.000009	 wd 0.0500	time 0.4234 (0.5197)	loss 1.1861 (1.1427)	grad_norm 2.9558 (4.0900)	loss_scale 2048.0000 (1642.2487)	mem 12860MB
[2024-05-29 00:40:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2600/5004]	eta 0:20:48 lr 0.000009	 wd 0.0500	time 0.4425 (0.5193)	loss 0.9505 (1.1415)	grad_norm 2.8354 (inf)	loss_scale 1024.0000 (1640.5260)	mem 12860MB
[2024-05-29 00:41:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2700/5004]	eta 0:19:56 lr 0.000009	 wd 0.0500	time 0.4046 (0.5192)	loss 0.7878 (1.1400)	grad_norm 2.9532 (inf)	loss_scale 1024.0000 (1617.7001)	mem 12860MB
[2024-05-29 00:41:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2800/5004]	eta 0:19:04 lr 0.000009	 wd 0.0500	time 0.4208 (0.5193)	loss 1.4874 (1.1398)	grad_norm 3.4529 (inf)	loss_scale 1024.0000 (1596.5041)	mem 12860MB
[2024-05-29 00:42:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][2900/5004]	eta 0:18:12 lr 0.000009	 wd 0.0500	time 0.4282 (0.5191)	loss 1.4779 (1.1386)	grad_norm 5.6134 (inf)	loss_scale 1024.0000 (1576.7694)	mem 12860MB
[2024-05-29 00:43:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3000/5004]	eta 0:17:20 lr 0.000009	 wd 0.0500	time 0.4203 (0.5190)	loss 1.3954 (1.1389)	grad_norm 3.6562 (inf)	loss_scale 1024.0000 (1558.3499)	mem 12860MB
[2024-05-29 00:44:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3100/5004]	eta 0:16:27 lr 0.000009	 wd 0.0500	time 0.4829 (0.5188)	loss 0.8641 (1.1404)	grad_norm 4.8700 (inf)	loss_scale 1024.0000 (1541.1183)	mem 12860MB
[2024-05-29 00:45:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3200/5004]	eta 0:15:39 lr 0.000009	 wd 0.0500	time 0.4849 (0.5210)	loss 0.8173 (1.1406)	grad_norm 3.1742 (inf)	loss_scale 1024.0000 (1524.9634)	mem 12860MB
[2024-05-29 00:46:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3300/5004]	eta 0:14:51 lr 0.000009	 wd 0.0500	time 0.5020 (0.5232)	loss 1.4728 (1.1404)	grad_norm 3.0468 (inf)	loss_scale 1024.0000 (1509.7873)	mem 12860MB
[2024-05-29 00:47:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3400/5004]	eta 0:14:00 lr 0.000009	 wd 0.0500	time 0.3916 (0.5237)	loss 1.4532 (1.1429)	grad_norm 2.5460 (inf)	loss_scale 1024.0000 (1495.5037)	mem 12860MB
[2024-05-29 00:48:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3500/5004]	eta 0:13:07 lr 0.000009	 wd 0.0500	time 0.4302 (0.5239)	loss 0.9956 (1.1427)	grad_norm 2.8887 (inf)	loss_scale 1024.0000 (1482.0360)	mem 12860MB
[2024-05-29 00:49:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3600/5004]	eta 0:12:15 lr 0.000009	 wd 0.0500	time 0.4953 (0.5237)	loss 1.2429 (1.1426)	grad_norm 4.8192 (inf)	loss_scale 1024.0000 (1469.3163)	mem 12860MB
[2024-05-29 00:49:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3700/5004]	eta 0:11:22 lr 0.000009	 wd 0.0500	time 0.4459 (0.5234)	loss 1.2020 (1.1416)	grad_norm 3.1505 (inf)	loss_scale 1024.0000 (1457.2840)	mem 12860MB
[2024-05-29 00:50:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3800/5004]	eta 0:10:30 lr 0.000009	 wd 0.0500	time 0.3838 (0.5235)	loss 1.1235 (1.1409)	grad_norm 3.1433 (inf)	loss_scale 1024.0000 (1445.8848)	mem 12860MB
[2024-05-29 00:51:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][3900/5004]	eta 0:09:37 lr 0.000009	 wd 0.0500	time 0.4913 (0.5233)	loss 0.9727 (1.1408)	grad_norm 2.9759 (inf)	loss_scale 1024.0000 (1435.0700)	mem 12860MB
[2024-05-29 00:52:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4000/5004]	eta 0:08:45 lr 0.000009	 wd 0.0500	time 0.5578 (0.5235)	loss 0.7930 (1.1411)	grad_norm 4.8358 (inf)	loss_scale 1024.0000 (1424.7958)	mem 12860MB
[2024-05-29 00:53:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4100/5004]	eta 0:07:53 lr 0.000009	 wd 0.0500	time 0.4199 (0.5234)	loss 0.7531 (1.1409)	grad_norm 2.8351 (inf)	loss_scale 1024.0000 (1415.0227)	mem 12860MB
[2024-05-29 00:54:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4200/5004]	eta 0:07:00 lr 0.000009	 wd 0.0500	time 0.4528 (0.5234)	loss 0.9013 (1.1414)	grad_norm 3.1383 (inf)	loss_scale 1024.0000 (1405.7148)	mem 12860MB
[2024-05-29 00:55:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4300/5004]	eta 0:06:08 lr 0.000009	 wd 0.0500	time 0.3664 (0.5235)	loss 0.8674 (1.1415)	grad_norm 2.8234 (inf)	loss_scale 1024.0000 (1396.8398)	mem 12860MB
[2024-05-29 00:56:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4400/5004]	eta 0:05:16 lr 0.000009	 wd 0.0500	time 0.4206 (0.5236)	loss 1.3026 (1.1414)	grad_norm 12.0525 (inf)	loss_scale 1024.0000 (1388.3681)	mem 12860MB
[2024-05-29 00:56:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4500/5004]	eta 0:04:24 lr 0.000009	 wd 0.0500	time 0.4212 (0.5241)	loss 0.9638 (1.1397)	grad_norm 4.3584 (inf)	loss_scale 1024.0000 (1380.2728)	mem 12860MB
[2024-05-29 00:57:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4600/5004]	eta 0:03:32 lr 0.000009	 wd 0.0500	time 0.5742 (0.5254)	loss 0.9884 (1.1388)	grad_norm 4.2640 (inf)	loss_scale 1024.0000 (1372.5295)	mem 12860MB
[2024-05-29 00:59:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4700/5004]	eta 0:02:40 lr 0.000009	 wd 0.0500	time 0.5542 (0.5276)	loss 1.1877 (1.1385)	grad_norm 2.7554 (inf)	loss_scale 1024.0000 (1365.1155)	mem 12860MB
[2024-05-29 00:59:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4800/5004]	eta 0:01:47 lr 0.000009	 wd 0.0500	time 0.4317 (0.5282)	loss 1.3565 (1.1379)	grad_norm 2.8407 (inf)	loss_scale 1024.0000 (1358.0104)	mem 12860MB
[2024-05-29 01:00:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][4900/5004]	eta 0:00:55 lr 0.000009	 wd 0.0500	time 0.4573 (0.5297)	loss 1.4572 (1.1388)	grad_norm 3.4595 (inf)	loss_scale 1024.0000 (1351.1953)	mem 12860MB
[2024-05-29 01:01:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [9/30][5000/5004]	eta 0:00:02 lr 0.000009	 wd 0.0500	time 0.3548 (0.5290)	loss 1.1192 (1.1381)	grad_norm 4.0444 (inf)	loss_scale 1024.0000 (1344.6527)	mem 12860MB
[2024-05-29 01:01:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 9 training takes 0:44:09
[2024-05-29 01:01:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.209 (7.209)	Loss 0.3809 (0.3809)	Acc@1 94.141 (94.141)	Acc@5 98.828 (98.828)	Mem 12860MB
[2024-05-29 01:02:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.121 (0.188)	Loss 0.7256 (0.6222)	Acc@1 82.031 (87.612)	Acc@5 97.656 (98.194)	Mem 12860MB
[2024-05-29 01:02:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.540 Acc@5 97.974
[2024-05-29 01:02:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.5%
[2024-05-29 01:02:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.54%
[2024-05-29 01:02:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][0/5004]	eta 13:46:35 lr 0.000009	 wd 0.0500	time 9.9111 (9.9111)	loss 1.2147 (1.2147)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:03:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][100/5004]	eta 0:49:16 lr 0.000009	 wd 0.0500	time 0.4254 (0.6028)	loss 1.3310 (1.1416)	grad_norm 3.9979 (5.8832)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:04:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][200/5004]	eta 0:44:40 lr 0.000009	 wd 0.0500	time 0.4358 (0.5581)	loss 1.1124 (1.1490)	grad_norm 4.8655 (4.7875)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:05:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][300/5004]	eta 0:42:56 lr 0.000009	 wd 0.0500	time 0.4868 (0.5478)	loss 1.4675 (1.1316)	grad_norm 3.6615 (4.4184)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:05:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][400/5004]	eta 0:41:21 lr 0.000009	 wd 0.0500	time 0.4499 (0.5391)	loss 1.1783 (1.1381)	grad_norm 3.4582 (4.3898)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:06:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][500/5004]	eta 0:40:13 lr 0.000009	 wd 0.0500	time 0.3914 (0.5358)	loss 1.2473 (1.1433)	grad_norm 2.6964 (4.2498)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:07:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][600/5004]	eta 0:39:16 lr 0.000009	 wd 0.0500	time 0.3626 (0.5350)	loss 1.6098 (1.1383)	grad_norm 3.4689 (4.2616)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:08:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][700/5004]	eta 0:38:25 lr 0.000009	 wd 0.0500	time 0.4444 (0.5356)	loss 1.4161 (1.1307)	grad_norm 2.8981 (4.2025)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:09:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][800/5004]	eta 0:37:23 lr 0.000009	 wd 0.0500	time 0.4494 (0.5336)	loss 0.7999 (1.1255)	grad_norm 2.9397 (4.1628)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:10:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][900/5004]	eta 0:36:38 lr 0.000009	 wd 0.0500	time 0.3724 (0.5357)	loss 1.0160 (1.1191)	grad_norm 2.6529 (4.1965)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:11:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1000/5004]	eta 0:35:49 lr 0.000009	 wd 0.0500	time 0.4103 (0.5369)	loss 1.4763 (1.1179)	grad_norm 7.3873 (4.1961)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:12:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1100/5004]	eta 0:34:52 lr 0.000009	 wd 0.0500	time 0.4379 (0.5360)	loss 0.8067 (1.1180)	grad_norm 9.0872 (4.2153)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:13:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1200/5004]	eta 0:33:58 lr 0.000009	 wd 0.0500	time 0.4206 (0.5358)	loss 0.8229 (1.1209)	grad_norm 5.3150 (4.2420)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:13:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1300/5004]	eta 0:32:59 lr 0.000009	 wd 0.0500	time 0.4261 (0.5343)	loss 1.5390 (1.1217)	grad_norm 3.3643 (4.1862)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:14:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1400/5004]	eta 0:32:00 lr 0.000009	 wd 0.0500	time 0.4448 (0.5328)	loss 1.5995 (1.1192)	grad_norm 3.2677 (4.1482)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:15:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1500/5004]	eta 0:31:03 lr 0.000009	 wd 0.0500	time 0.3924 (0.5318)	loss 0.7600 (1.1224)	grad_norm 3.2031 (4.1464)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:16:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1600/5004]	eta 0:30:06 lr 0.000009	 wd 0.0500	time 0.4015 (0.5307)	loss 1.2878 (1.1227)	grad_norm 3.5258 (4.1172)	loss_scale 2048.0000 (1054.7008)	mem 12860MB
[2024-05-29 01:17:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1700/5004]	eta 0:29:10 lr 0.000009	 wd 0.0500	time 0.4044 (0.5297)	loss 1.4072 (1.1242)	grad_norm 5.8498 (4.0994)	loss_scale 2048.0000 (1113.0958)	mem 12860MB
[2024-05-29 01:18:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1800/5004]	eta 0:28:18 lr 0.000009	 wd 0.0500	time 0.4291 (0.5300)	loss 0.9768 (1.1251)	grad_norm 2.8534 (4.0720)	loss_scale 2048.0000 (1165.0061)	mem 12860MB
[2024-05-29 01:19:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][1900/5004]	eta 0:27:21 lr 0.000009	 wd 0.0500	time 0.3810 (0.5289)	loss 0.8320 (1.1251)	grad_norm 2.6742 (4.0566)	loss_scale 2048.0000 (1211.4550)	mem 12860MB
[2024-05-29 01:19:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2000/5004]	eta 0:26:26 lr 0.000009	 wd 0.0500	time 0.5368 (0.5281)	loss 1.3354 (1.1274)	grad_norm 2.9869 (4.0366)	loss_scale 2048.0000 (1253.2614)	mem 12860MB
[2024-05-29 01:20:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2100/5004]	eta 0:25:31 lr 0.000009	 wd 0.0500	time 0.4339 (0.5272)	loss 1.2799 (1.1274)	grad_norm 2.9375 (4.0202)	loss_scale 2048.0000 (1291.0881)	mem 12860MB
[2024-05-29 01:21:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2200/5004]	eta 0:24:37 lr 0.000009	 wd 0.0500	time 0.4044 (0.5270)	loss 1.5315 (1.1278)	grad_norm 3.3382 (4.0106)	loss_scale 2048.0000 (1325.4775)	mem 12860MB
[2024-05-29 01:22:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2300/5004]	eta 0:23:43 lr 0.000009	 wd 0.0500	time 0.4215 (0.5265)	loss 1.3963 (1.1259)	grad_norm 4.0133 (4.0230)	loss_scale 2048.0000 (1356.8779)	mem 12860MB
[2024-05-29 01:23:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2400/5004]	eta 0:22:50 lr 0.000009	 wd 0.0500	time 0.4293 (0.5263)	loss 1.3176 (1.1268)	grad_norm 2.8627 (4.0013)	loss_scale 2048.0000 (1385.6626)	mem 12860MB
[2024-05-29 01:24:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2500/5004]	eta 0:21:56 lr 0.000009	 wd 0.0500	time 0.4016 (0.5259)	loss 1.2450 (1.1261)	grad_norm 4.3473 (4.0527)	loss_scale 2048.0000 (1412.1455)	mem 12860MB
[2024-05-29 01:25:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2600/5004]	eta 0:21:06 lr 0.000009	 wd 0.0500	time 0.5141 (0.5267)	loss 1.3058 (1.1266)	grad_norm 3.3966 (4.0430)	loss_scale 2048.0000 (1436.5921)	mem 12860MB
[2024-05-29 01:26:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2700/5004]	eta 0:20:15 lr 0.000009	 wd 0.0500	time 0.4601 (0.5275)	loss 1.2053 (1.1266)	grad_norm 3.2892 (4.0289)	loss_scale 2048.0000 (1459.2284)	mem 12860MB
[2024-05-29 01:26:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2800/5004]	eta 0:19:23 lr 0.000009	 wd 0.0500	time 0.4724 (0.5277)	loss 0.6239 (1.1269)	grad_norm 3.0115 (4.0153)	loss_scale 2048.0000 (1480.2485)	mem 12860MB
[2024-05-29 01:27:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][2900/5004]	eta 0:18:29 lr 0.000009	 wd 0.0500	time 0.3868 (0.5273)	loss 1.0043 (1.1278)	grad_norm 2.6932 (4.0140)	loss_scale 2048.0000 (1499.8194)	mem 12860MB
[2024-05-29 01:28:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3000/5004]	eta 0:17:36 lr 0.000009	 wd 0.0500	time 0.3766 (0.5273)	loss 1.3831 (1.1282)	grad_norm 3.9588 (4.0103)	loss_scale 2048.0000 (1518.0860)	mem 12860MB
[2024-05-29 01:29:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3100/5004]	eta 0:16:43 lr 0.000009	 wd 0.0500	time 0.3805 (0.5271)	loss 0.7268 (1.1272)	grad_norm 2.8328 (4.0015)	loss_scale 2048.0000 (1535.1745)	mem 12860MB
[2024-05-29 01:30:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3200/5004]	eta 0:15:50 lr 0.000009	 wd 0.0500	time 0.4011 (0.5270)	loss 1.1489 (1.1274)	grad_norm 10.4632 (4.0092)	loss_scale 2048.0000 (1551.1953)	mem 12860MB
[2024-05-29 01:31:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3300/5004]	eta 0:14:57 lr 0.000009	 wd 0.0500	time 0.4248 (0.5269)	loss 1.2000 (1.1276)	grad_norm 3.4861 (4.0021)	loss_scale 2048.0000 (1566.2454)	mem 12860MB
[2024-05-29 01:32:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3400/5004]	eta 0:14:05 lr 0.000009	 wd 0.0500	time 0.4981 (0.5270)	loss 1.3233 (1.1285)	grad_norm 4.2228 (3.9875)	loss_scale 2048.0000 (1580.4105)	mem 12860MB
[2024-05-29 01:33:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3500/5004]	eta 0:13:13 lr 0.000009	 wd 0.0500	time 0.4998 (0.5278)	loss 1.0582 (1.1295)	grad_norm 6.3724 (3.9966)	loss_scale 2048.0000 (1593.7664)	mem 12860MB
[2024-05-29 01:33:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3600/5004]	eta 0:12:21 lr 0.000009	 wd 0.0500	time 0.4478 (0.5279)	loss 0.8142 (1.1301)	grad_norm 3.2226 (3.9826)	loss_scale 2048.0000 (1606.3804)	mem 12860MB
[2024-05-29 01:34:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3700/5004]	eta 0:11:28 lr 0.000009	 wd 0.0500	time 0.4833 (0.5279)	loss 1.3303 (1.1298)	grad_norm 6.8696 (3.9809)	loss_scale 2048.0000 (1618.3129)	mem 12860MB
[2024-05-29 01:35:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3800/5004]	eta 0:10:36 lr 0.000009	 wd 0.0500	time 0.4837 (0.5283)	loss 1.4202 (1.1313)	grad_norm 2.9919 (4.0162)	loss_scale 2048.0000 (1629.6175)	mem 12860MB
[2024-05-29 01:36:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][3900/5004]	eta 0:09:43 lr 0.000009	 wd 0.0500	time 0.3903 (0.5282)	loss 1.2984 (1.1320)	grad_norm 3.8044 (4.0177)	loss_scale 2048.0000 (1640.3425)	mem 12860MB
[2024-05-29 01:37:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4000/5004]	eta 0:08:50 lr 0.000009	 wd 0.0500	time 0.4328 (0.5286)	loss 1.2311 (1.1320)	grad_norm 12.4353 (inf)	loss_scale 1024.0000 (1642.3414)	mem 12860MB
[2024-05-29 01:38:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4100/5004]	eta 0:07:58 lr 0.000009	 wd 0.0500	time 0.4300 (0.5289)	loss 1.4514 (1.1318)	grad_norm 6.3217 (inf)	loss_scale 1024.0000 (1627.2636)	mem 12860MB
[2024-05-29 01:39:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4200/5004]	eta 0:07:05 lr 0.000009	 wd 0.0500	time 0.4570 (0.5290)	loss 0.7412 (1.1317)	grad_norm 3.4803 (inf)	loss_scale 1024.0000 (1612.9036)	mem 12860MB
[2024-05-29 01:40:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4300/5004]	eta 0:06:12 lr 0.000009	 wd 0.0500	time 0.4306 (0.5296)	loss 1.3960 (1.1320)	grad_norm 5.5660 (inf)	loss_scale 1024.0000 (1599.2113)	mem 12860MB
[2024-05-29 01:41:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4400/5004]	eta 0:05:19 lr 0.000009	 wd 0.0500	time 0.4160 (0.5297)	loss 1.5825 (1.1317)	grad_norm 4.2761 (inf)	loss_scale 1024.0000 (1586.1413)	mem 12860MB
[2024-05-29 01:42:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4500/5004]	eta 0:04:26 lr 0.000009	 wd 0.0500	time 0.3828 (0.5295)	loss 1.2235 (1.1320)	grad_norm 17.5399 (inf)	loss_scale 1024.0000 (1573.6521)	mem 12860MB
[2024-05-29 01:42:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4600/5004]	eta 0:03:33 lr 0.000009	 wd 0.0500	time 0.4112 (0.5295)	loss 1.3300 (1.1314)	grad_norm 3.4656 (inf)	loss_scale 1024.0000 (1561.7057)	mem 12860MB
[2024-05-29 01:43:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4700/5004]	eta 0:02:41 lr 0.000009	 wd 0.0500	time 0.5456 (0.5297)	loss 1.3554 (1.1316)	grad_norm 18.5552 (inf)	loss_scale 1024.0000 (1550.2676)	mem 12860MB
[2024-05-29 01:44:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4800/5004]	eta 0:01:48 lr 0.000009	 wd 0.0500	time 0.3934 (0.5297)	loss 1.3591 (1.1312)	grad_norm 3.8928 (inf)	loss_scale 1024.0000 (1539.3060)	mem 12860MB
[2024-05-29 01:45:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][4900/5004]	eta 0:00:55 lr 0.000009	 wd 0.0500	time 0.4296 (0.5296)	loss 1.0670 (1.1315)	grad_norm 5.7150 (inf)	loss_scale 1024.0000 (1528.7917)	mem 12860MB
[2024-05-29 01:46:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [10/30][5000/5004]	eta 0:00:02 lr 0.000009	 wd 0.0500	time 0.3709 (0.5287)	loss 1.0552 (1.1313)	grad_norm 4.1160 (inf)	loss_scale 1024.0000 (1518.6979)	mem 12860MB
[2024-05-29 01:46:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 10 training takes 0:44:07
[2024-05-29 01:46:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.237 (8.237)	Loss 0.4148 (0.4148)	Acc@1 94.922 (94.922)	Acc@5 98.828 (98.828)	Mem 12860MB
[2024-05-29 01:46:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.144 (0.198)	Loss 0.7617 (0.6441)	Acc@1 82.422 (87.566)	Acc@5 97.656 (98.194)	Mem 12860MB
[2024-05-29 01:46:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.444 Acc@5 98.000
[2024-05-29 01:46:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-05-29 01:46:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.54%
[2024-05-29 01:47:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][0/5004]	eta 12:08:40 lr 0.000009	 wd 0.0500	time 8.7372 (8.7372)	loss 1.3571 (1.3571)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:47:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][100/5004]	eta 0:47:40 lr 0.000009	 wd 0.0500	time 0.4004 (0.5833)	loss 1.2203 (1.1471)	grad_norm 2.8094 (3.8341)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:48:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][200/5004]	eta 0:43:32 lr 0.000009	 wd 0.0500	time 0.4581 (0.5439)	loss 1.2257 (1.1797)	grad_norm 9.7886 (4.4853)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:49:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][300/5004]	eta 0:41:46 lr 0.000009	 wd 0.0500	time 0.4036 (0.5327)	loss 0.8413 (1.1683)	grad_norm 3.1838 (4.3282)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:50:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][400/5004]	eta 0:40:33 lr 0.000009	 wd 0.0500	time 0.4151 (0.5286)	loss 1.4540 (1.1622)	grad_norm 2.9247 (4.2796)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:51:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][500/5004]	eta 0:39:39 lr 0.000009	 wd 0.0500	time 0.4288 (0.5284)	loss 1.2301 (1.1593)	grad_norm 3.2490 (4.2048)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:52:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][600/5004]	eta 0:38:38 lr 0.000009	 wd 0.0500	time 0.4226 (0.5265)	loss 1.2974 (1.1578)	grad_norm 3.3251 (4.0707)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:53:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][700/5004]	eta 0:37:32 lr 0.000009	 wd 0.0500	time 0.3856 (0.5235)	loss 0.9586 (1.1535)	grad_norm 3.5258 (4.0641)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:53:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][800/5004]	eta 0:36:31 lr 0.000009	 wd 0.0500	time 0.4218 (0.5213)	loss 0.8734 (1.1473)	grad_norm 22.1146 (4.1057)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:54:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][900/5004]	eta 0:35:51 lr 0.000009	 wd 0.0500	time 0.4358 (0.5242)	loss 1.4250 (1.1478)	grad_norm 4.6651 (4.0775)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:55:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1000/5004]	eta 0:34:52 lr 0.000009	 wd 0.0500	time 0.4390 (0.5226)	loss 1.2665 (1.1448)	grad_norm 2.8680 (4.0590)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:56:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1100/5004]	eta 0:33:54 lr 0.000009	 wd 0.0500	time 0.4425 (0.5212)	loss 1.5263 (1.1419)	grad_norm 4.2710 (3.9971)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:57:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1200/5004]	eta 0:32:58 lr 0.000009	 wd 0.0500	time 0.4553 (0.5202)	loss 1.3965 (1.1437)	grad_norm 3.2732 (3.9874)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:58:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1300/5004]	eta 0:32:03 lr 0.000009	 wd 0.0500	time 0.4634 (0.5192)	loss 0.7927 (1.1407)	grad_norm 3.5023 (3.9561)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:59:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1400/5004]	eta 0:31:08 lr 0.000009	 wd 0.0500	time 0.4147 (0.5184)	loss 1.2442 (1.1379)	grad_norm 2.9068 (3.9721)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 01:59:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1500/5004]	eta 0:30:14 lr 0.000009	 wd 0.0500	time 0.3892 (0.5179)	loss 1.6478 (1.1375)	grad_norm 3.4619 (3.9534)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:00:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1600/5004]	eta 0:29:20 lr 0.000009	 wd 0.0500	time 0.4115 (0.5173)	loss 0.6576 (1.1384)	grad_norm 3.1801 (3.9274)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:01:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1700/5004]	eta 0:28:26 lr 0.000009	 wd 0.0500	time 0.3709 (0.5165)	loss 1.2445 (1.1393)	grad_norm 3.0222 (3.9399)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:02:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1800/5004]	eta 0:27:38 lr 0.000009	 wd 0.0500	time 0.3967 (0.5176)	loss 0.6095 (1.1409)	grad_norm 2.7050 (3.9367)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:03:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][1900/5004]	eta 0:26:50 lr 0.000008	 wd 0.0500	time 0.4485 (0.5188)	loss 1.3122 (1.1409)	grad_norm 3.3889 (3.9564)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:04:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2000/5004]	eta 0:25:57 lr 0.000008	 wd 0.0500	time 0.3974 (0.5185)	loss 1.2625 (1.1413)	grad_norm 3.2291 (3.9928)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:05:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2100/5004]	eta 0:25:10 lr 0.000008	 wd 0.0500	time 0.4578 (0.5201)	loss 1.1279 (1.1406)	grad_norm 2.8825 (3.9996)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:06:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2200/5004]	eta 0:24:23 lr 0.000008	 wd 0.0500	time 0.3844 (0.5218)	loss 0.9671 (1.1396)	grad_norm 3.4866 (3.9989)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:06:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2300/5004]	eta 0:23:30 lr 0.000008	 wd 0.0500	time 0.4339 (0.5215)	loss 0.9300 (1.1393)	grad_norm 2.5440 (3.9806)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:07:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2400/5004]	eta 0:22:36 lr 0.000008	 wd 0.0500	time 0.4519 (0.5211)	loss 1.6338 (1.1397)	grad_norm 2.8651 (3.9705)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:08:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2500/5004]	eta 0:21:43 lr 0.000008	 wd 0.0500	time 0.4172 (0.5204)	loss 1.2884 (1.1395)	grad_norm 2.9034 (3.9743)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:09:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2600/5004]	eta 0:20:50 lr 0.000008	 wd 0.0500	time 0.4948 (0.5202)	loss 1.2057 (1.1395)	grad_norm 3.4763 (3.9587)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:10:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2700/5004]	eta 0:20:02 lr 0.000008	 wd 0.0500	time 0.4509 (0.5218)	loss 0.8140 (1.1411)	grad_norm 4.2684 (3.9842)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:11:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2800/5004]	eta 0:19:10 lr 0.000008	 wd 0.0500	time 0.4372 (0.5220)	loss 1.2308 (1.1408)	grad_norm 3.7013 (3.9817)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:12:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][2900/5004]	eta 0:18:17 lr 0.000008	 wd 0.0500	time 0.4243 (0.5217)	loss 1.2836 (1.1412)	grad_norm 3.5453 (3.9601)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:13:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3000/5004]	eta 0:17:25 lr 0.000008	 wd 0.0500	time 0.3617 (0.5217)	loss 0.7303 (1.1412)	grad_norm 4.2004 (3.9879)	loss_scale 2048.0000 (1036.2839)	mem 12860MB
[2024-05-29 02:13:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3100/5004]	eta 0:16:32 lr 0.000008	 wd 0.0500	time 0.3831 (0.5214)	loss 1.1838 (1.1407)	grad_norm 4.6743 (3.9800)	loss_scale 2048.0000 (1068.9094)	mem 12860MB
[2024-05-29 02:14:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3200/5004]	eta 0:15:40 lr 0.000008	 wd 0.0500	time 0.4277 (0.5215)	loss 1.4359 (1.1405)	grad_norm 4.2233 (3.9698)	loss_scale 2048.0000 (1099.4964)	mem 12860MB
[2024-05-29 02:15:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3300/5004]	eta 0:14:49 lr 0.000008	 wd 0.0500	time 0.4613 (0.5219)	loss 1.2732 (1.1399)	grad_norm 3.2040 (3.9689)	loss_scale 2048.0000 (1128.2302)	mem 12860MB
[2024-05-29 02:16:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3400/5004]	eta 0:13:58 lr 0.000008	 wd 0.0500	time 0.4307 (0.5226)	loss 1.1295 (1.1385)	grad_norm 4.1148 (3.9702)	loss_scale 2048.0000 (1155.2743)	mem 12860MB
[2024-05-29 02:17:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3500/5004]	eta 0:13:06 lr 0.000008	 wd 0.0500	time 0.4234 (0.5227)	loss 1.3264 (1.1391)	grad_norm 6.6526 (nan)	loss_scale 1024.0000 (1171.4139)	mem 12860MB
[2024-05-29 02:18:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3600/5004]	eta 0:12:13 lr 0.000008	 wd 0.0500	time 0.4208 (0.5227)	loss 1.0853 (1.1388)	grad_norm 2.3903 (nan)	loss_scale 1024.0000 (1167.3202)	mem 12860MB
[2024-05-29 02:19:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3700/5004]	eta 0:11:21 lr 0.000008	 wd 0.0500	time 0.4187 (0.5226)	loss 1.6150 (1.1395)	grad_norm 2.9904 (nan)	loss_scale 1024.0000 (1163.4477)	mem 12860MB
[2024-05-29 02:20:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3800/5004]	eta 0:10:29 lr 0.000008	 wd 0.0500	time 0.3919 (0.5227)	loss 0.9106 (1.1386)	grad_norm 3.5843 (nan)	loss_scale 1024.0000 (1159.7790)	mem 12860MB
[2024-05-29 02:20:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][3900/5004]	eta 0:09:36 lr 0.000008	 wd 0.0500	time 0.4027 (0.5226)	loss 1.5432 (1.1382)	grad_norm 2.6805 (nan)	loss_scale 1024.0000 (1156.2984)	mem 12860MB
[2024-05-29 02:21:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4000/5004]	eta 0:08:44 lr 0.000008	 wd 0.0500	time 0.4038 (0.5225)	loss 1.3862 (1.1379)	grad_norm 4.1465 (nan)	loss_scale 1024.0000 (1152.9918)	mem 12860MB
[2024-05-29 02:22:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4100/5004]	eta 0:07:52 lr 0.000008	 wd 0.0500	time 0.4529 (0.5226)	loss 1.3832 (1.1382)	grad_norm 3.7813 (nan)	loss_scale 1024.0000 (1149.8464)	mem 12860MB
[2024-05-29 02:23:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4200/5004]	eta 0:07:00 lr 0.000008	 wd 0.0500	time 0.4958 (0.5226)	loss 0.8105 (1.1377)	grad_norm 3.5847 (nan)	loss_scale 1024.0000 (1146.8507)	mem 12860MB
[2024-05-29 02:24:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4300/5004]	eta 0:06:07 lr 0.000008	 wd 0.0500	time 0.4226 (0.5226)	loss 1.3370 (1.1378)	grad_norm 4.6887 (nan)	loss_scale 1024.0000 (1143.9944)	mem 12860MB
[2024-05-29 02:25:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4400/5004]	eta 0:05:16 lr 0.000008	 wd 0.0500	time 0.5566 (0.5239)	loss 1.3166 (1.1378)	grad_norm 4.0547 (nan)	loss_scale 1024.0000 (1141.2679)	mem 12860MB
[2024-05-29 02:26:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4500/5004]	eta 0:04:24 lr 0.000008	 wd 0.0500	time 0.6362 (0.5255)	loss 0.7731 (1.1379)	grad_norm 2.7147 (nan)	loss_scale 1024.0000 (1138.6625)	mem 12860MB
[2024-05-29 02:27:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4600/5004]	eta 0:03:32 lr 0.000008	 wd 0.0500	time 0.3781 (0.5254)	loss 1.3604 (1.1372)	grad_norm 2.5259 (nan)	loss_scale 1024.0000 (1136.1704)	mem 12860MB
[2024-05-29 02:28:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4700/5004]	eta 0:02:39 lr 0.000008	 wd 0.0500	time 0.4291 (0.5255)	loss 1.4400 (1.1371)	grad_norm 3.4412 (nan)	loss_scale 1024.0000 (1133.7843)	mem 12860MB
[2024-05-29 02:28:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4800/5004]	eta 0:01:47 lr 0.000008	 wd 0.0500	time 0.5076 (0.5255)	loss 1.0318 (1.1372)	grad_norm 3.0226 (nan)	loss_scale 1024.0000 (1131.4976)	mem 12860MB
[2024-05-29 02:29:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][4900/5004]	eta 0:00:54 lr 0.000008	 wd 0.0500	time 0.4677 (0.5257)	loss 1.2405 (1.1379)	grad_norm 3.9649 (nan)	loss_scale 1024.0000 (1129.3042)	mem 12860MB
[2024-05-29 02:30:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [11/30][5000/5004]	eta 0:00:02 lr 0.000008	 wd 0.0500	time 0.3455 (0.5248)	loss 1.0842 (1.1369)	grad_norm 4.8978 (nan)	loss_scale 1024.0000 (1127.1986)	mem 12860MB
[2024-05-29 02:30:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 11 training takes 0:43:48
[2024-05-29 02:30:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.503 (8.503)	Loss 0.4106 (0.4106)	Acc@1 94.531 (94.531)	Acc@5 98.828 (98.828)	Mem 12860MB
[2024-05-29 02:31:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.191 (0.218)	Loss 0.7476 (0.6447)	Acc@1 82.422 (87.527)	Acc@5 97.656 (98.194)	Mem 12860MB
[2024-05-29 02:31:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.514 Acc@5 98.006
[2024-05-29 02:31:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.5%
[2024-05-29 02:31:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.54%
[2024-05-29 02:31:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][0/5004]	eta 11:26:28 lr 0.000008	 wd 0.0500	time 8.2312 (8.2312)	loss 0.8868 (0.8868)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:32:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][100/5004]	eta 0:47:02 lr 0.000008	 wd 0.0500	time 0.3737 (0.5756)	loss 1.3598 (1.1293)	grad_norm 6.7346 (3.7300)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:33:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][200/5004]	eta 0:43:08 lr 0.000008	 wd 0.0500	time 0.3738 (0.5387)	loss 1.4167 (1.1363)	grad_norm 2.8443 (3.6489)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:33:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][300/5004]	eta 0:41:20 lr 0.000008	 wd 0.0500	time 0.4255 (0.5273)	loss 1.4083 (1.1392)	grad_norm 3.6892 (3.7067)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:34:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][400/5004]	eta 0:39:55 lr 0.000008	 wd 0.0500	time 0.3834 (0.5204)	loss 0.7855 (1.1427)	grad_norm 3.9113 (3.6793)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:35:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][500/5004]	eta 0:38:49 lr 0.000008	 wd 0.0500	time 0.4238 (0.5173)	loss 1.4459 (1.1319)	grad_norm 3.4575 (3.6864)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:36:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][600/5004]	eta 0:37:51 lr 0.000008	 wd 0.0500	time 0.4066 (0.5157)	loss 0.9742 (1.1356)	grad_norm 3.2983 (3.7496)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:37:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][700/5004]	eta 0:36:46 lr 0.000008	 wd 0.0500	time 0.4308 (0.5127)	loss 1.0244 (1.1356)	grad_norm 3.0331 (3.8257)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:38:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][800/5004]	eta 0:35:51 lr 0.000008	 wd 0.0500	time 0.3768 (0.5118)	loss 1.1729 (1.1342)	grad_norm 3.5785 (3.7845)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:38:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][900/5004]	eta 0:34:55 lr 0.000008	 wd 0.0500	time 0.3613 (0.5107)	loss 1.2287 (1.1351)	grad_norm 3.4929 (3.8774)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:39:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1000/5004]	eta 0:34:11 lr 0.000008	 wd 0.0500	time 0.4722 (0.5123)	loss 1.3289 (1.1363)	grad_norm 5.2600 (3.9565)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:40:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1100/5004]	eta 0:33:18 lr 0.000008	 wd 0.0500	time 0.4689 (0.5120)	loss 1.1492 (1.1368)	grad_norm 3.0408 (3.9556)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:41:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1200/5004]	eta 0:32:28 lr 0.000008	 wd 0.0500	time 0.4379 (0.5121)	loss 1.2455 (1.1371)	grad_norm 4.8292 (3.9317)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 02:42:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1300/5004]	eta 0:31:43 lr 0.000008	 wd 0.0500	time 0.4514 (0.5140)	loss 1.0675 (1.1384)	grad_norm 3.5734 (nan)	loss_scale 512.0000 (992.5165)	mem 12860MB
[2024-05-29 02:43:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1400/5004]	eta 0:30:48 lr 0.000008	 wd 0.0500	time 0.4194 (0.5130)	loss 1.1595 (1.1404)	grad_norm 3.3432 (nan)	loss_scale 512.0000 (958.2184)	mem 12860MB
[2024-05-29 02:44:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1500/5004]	eta 0:29:56 lr 0.000008	 wd 0.0500	time 0.4214 (0.5126)	loss 1.0660 (1.1384)	grad_norm 3.0222 (nan)	loss_scale 512.0000 (928.4903)	mem 12860MB
[2024-05-29 02:44:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1600/5004]	eta 0:29:04 lr 0.000008	 wd 0.0500	time 0.4945 (0.5124)	loss 0.6787 (1.1386)	grad_norm 6.4978 (nan)	loss_scale 512.0000 (902.4760)	mem 12860MB
[2024-05-29 02:45:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1700/5004]	eta 0:28:16 lr 0.000008	 wd 0.0500	time 0.4558 (0.5136)	loss 1.0273 (1.1373)	grad_norm 4.2995 (nan)	loss_scale 512.0000 (879.5203)	mem 12860MB
[2024-05-29 02:46:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1800/5004]	eta 0:27:23 lr 0.000008	 wd 0.0500	time 0.4201 (0.5130)	loss 0.6840 (1.1381)	grad_norm 3.1300 (nan)	loss_scale 512.0000 (859.1138)	mem 12860MB
[2024-05-29 02:47:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][1900/5004]	eta 0:26:31 lr 0.000008	 wd 0.0500	time 0.4143 (0.5127)	loss 1.2035 (1.1395)	grad_norm 3.4614 (nan)	loss_scale 512.0000 (840.8543)	mem 12860MB
[2024-05-29 02:48:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2000/5004]	eta 0:25:40 lr 0.000008	 wd 0.0500	time 0.3903 (0.5129)	loss 0.9962 (1.1382)	grad_norm 3.5148 (nan)	loss_scale 512.0000 (824.4198)	mem 12860MB
[2024-05-29 02:49:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2100/5004]	eta 0:24:48 lr 0.000008	 wd 0.0500	time 0.4144 (0.5125)	loss 0.8752 (1.1373)	grad_norm 3.6289 (nan)	loss_scale 512.0000 (809.5497)	mem 12860MB
[2024-05-29 02:50:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2200/5004]	eta 0:23:56 lr 0.000008	 wd 0.0500	time 0.3851 (0.5123)	loss 1.0708 (1.1380)	grad_norm 3.3406 (nan)	loss_scale 512.0000 (796.0309)	mem 12860MB
[2024-05-29 02:50:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2300/5004]	eta 0:23:04 lr 0.000008	 wd 0.0500	time 0.4601 (0.5120)	loss 1.1734 (1.1389)	grad_norm 4.3824 (nan)	loss_scale 512.0000 (783.6871)	mem 12860MB
[2024-05-29 02:51:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2400/5004]	eta 0:22:16 lr 0.000008	 wd 0.0500	time 0.4036 (0.5132)	loss 1.0103 (1.1397)	grad_norm 3.9432 (nan)	loss_scale 512.0000 (772.3715)	mem 12860MB
[2024-05-29 02:52:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2500/5004]	eta 0:21:28 lr 0.000008	 wd 0.0500	time 0.4405 (0.5146)	loss 1.2275 (1.1402)	grad_norm 3.0327 (nan)	loss_scale 512.0000 (761.9608)	mem 12860MB
[2024-05-29 02:53:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2600/5004]	eta 0:20:41 lr 0.000008	 wd 0.0500	time 0.4391 (0.5163)	loss 1.2303 (1.1403)	grad_norm 2.8046 (nan)	loss_scale 512.0000 (752.3506)	mem 12860MB
[2024-05-29 02:54:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2700/5004]	eta 0:19:48 lr 0.000008	 wd 0.0500	time 0.4019 (0.5160)	loss 0.7615 (1.1399)	grad_norm 3.7741 (nan)	loss_scale 512.0000 (743.4521)	mem 12860MB
[2024-05-29 02:55:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2800/5004]	eta 0:18:57 lr 0.000008	 wd 0.0500	time 0.4324 (0.5160)	loss 1.1384 (1.1383)	grad_norm 3.6908 (nan)	loss_scale 512.0000 (735.1889)	mem 12860MB
[2024-05-29 02:56:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][2900/5004]	eta 0:18:05 lr 0.000008	 wd 0.0500	time 0.4170 (0.5160)	loss 1.1636 (1.1359)	grad_norm 3.5272 (nan)	loss_scale 512.0000 (727.4953)	mem 12860MB
[2024-05-29 02:57:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3000/5004]	eta 0:17:13 lr 0.000008	 wd 0.0500	time 0.4325 (0.5158)	loss 1.2324 (1.1354)	grad_norm 2.5396 (nan)	loss_scale 512.0000 (720.3146)	mem 12860MB
[2024-05-29 02:57:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3100/5004]	eta 0:16:22 lr 0.000008	 wd 0.0500	time 0.5051 (0.5158)	loss 1.2537 (1.1356)	grad_norm 3.9597 (nan)	loss_scale 512.0000 (713.5969)	mem 12860MB
[2024-05-29 02:58:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3200/5004]	eta 0:15:30 lr 0.000008	 wd 0.0500	time 0.4747 (0.5158)	loss 1.2785 (1.1357)	grad_norm 3.4854 (nan)	loss_scale 512.0000 (707.2990)	mem 12860MB
[2024-05-29 02:59:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3300/5004]	eta 0:14:39 lr 0.000008	 wd 0.0500	time 0.4214 (0.5163)	loss 1.6081 (1.1358)	grad_norm 3.5385 (nan)	loss_scale 512.0000 (701.3826)	mem 12860MB
[2024-05-29 03:00:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3400/5004]	eta 0:13:48 lr 0.000008	 wd 0.0500	time 0.3830 (0.5165)	loss 0.8353 (1.1358)	grad_norm 3.5647 (nan)	loss_scale 512.0000 (695.8142)	mem 12860MB
[2024-05-29 03:01:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3500/5004]	eta 0:12:57 lr 0.000008	 wd 0.0500	time 0.3984 (0.5169)	loss 0.9431 (1.1363)	grad_norm 3.1591 (nan)	loss_scale 512.0000 (690.5638)	mem 12860MB
[2024-05-29 03:02:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3600/5004]	eta 0:12:05 lr 0.000008	 wd 0.0500	time 0.4564 (0.5169)	loss 1.3813 (1.1357)	grad_norm 3.9645 (nan)	loss_scale 512.0000 (685.6051)	mem 12860MB
[2024-05-29 03:03:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3700/5004]	eta 0:11:14 lr 0.000008	 wd 0.0500	time 0.4356 (0.5173)	loss 0.7101 (1.1354)	grad_norm 3.8963 (nan)	loss_scale 512.0000 (680.9143)	mem 12860MB
[2024-05-29 03:04:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3800/5004]	eta 0:10:23 lr 0.000008	 wd 0.0500	time 0.4205 (0.5177)	loss 1.5627 (1.1350)	grad_norm 4.0137 (nan)	loss_scale 512.0000 (676.4704)	mem 12860MB
[2024-05-29 03:04:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][3900/5004]	eta 0:09:31 lr 0.000008	 wd 0.0500	time 0.4557 (0.5178)	loss 1.3624 (1.1356)	grad_norm 4.7020 (nan)	loss_scale 512.0000 (672.2543)	mem 12860MB
[2024-05-29 03:05:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4000/5004]	eta 0:08:39 lr 0.000008	 wd 0.0500	time 0.3930 (0.5178)	loss 1.1229 (1.1351)	grad_norm 4.3989 (nan)	loss_scale 512.0000 (668.2489)	mem 12860MB
[2024-05-29 03:06:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4100/5004]	eta 0:07:48 lr 0.000008	 wd 0.0500	time 0.4096 (0.5179)	loss 0.9928 (1.1359)	grad_norm 2.8858 (nan)	loss_scale 512.0000 (664.4389)	mem 12860MB
[2024-05-29 03:07:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4200/5004]	eta 0:06:56 lr 0.000008	 wd 0.0500	time 0.3848 (0.5181)	loss 1.3860 (1.1368)	grad_norm 2.4642 (nan)	loss_scale 512.0000 (660.8103)	mem 12860MB
[2024-05-29 03:08:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4300/5004]	eta 0:06:04 lr 0.000008	 wd 0.0500	time 0.3852 (0.5180)	loss 0.9253 (1.1367)	grad_norm 4.0109 (nan)	loss_scale 512.0000 (657.3504)	mem 12860MB
[2024-05-29 03:09:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4400/5004]	eta 0:05:12 lr 0.000008	 wd 0.0500	time 0.4439 (0.5181)	loss 0.6913 (1.1362)	grad_norm 5.1546 (nan)	loss_scale 512.0000 (654.0477)	mem 12860MB
[2024-05-29 03:10:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4500/5004]	eta 0:04:21 lr 0.000008	 wd 0.0500	time 0.4542 (0.5195)	loss 0.9634 (1.1365)	grad_norm 3.2059 (nan)	loss_scale 512.0000 (650.8918)	mem 12860MB
[2024-05-29 03:11:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4600/5004]	eta 0:03:30 lr 0.000008	 wd 0.0500	time 0.5528 (0.5215)	loss 1.0383 (1.1368)	grad_norm 4.5206 (nan)	loss_scale 512.0000 (647.8731)	mem 12860MB
[2024-05-29 03:12:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4700/5004]	eta 0:02:39 lr 0.000008	 wd 0.0500	time 0.5282 (0.5238)	loss 1.5401 (1.1368)	grad_norm 7.9125 (nan)	loss_scale 512.0000 (644.9828)	mem 12860MB
[2024-05-29 03:13:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4800/5004]	eta 0:01:47 lr 0.000008	 wd 0.0500	time 0.4745 (0.5249)	loss 1.2061 (1.1367)	grad_norm 3.5215 (nan)	loss_scale 512.0000 (642.2129)	mem 12860MB
[2024-05-29 03:14:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][4900/5004]	eta 0:00:54 lr 0.000008	 wd 0.0500	time 0.4792 (0.5250)	loss 0.8744 (1.1356)	grad_norm 2.6577 (nan)	loss_scale 512.0000 (639.5560)	mem 12860MB
[2024-05-29 03:14:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [12/30][5000/5004]	eta 0:00:02 lr 0.000008	 wd 0.0500	time 0.3838 (0.5242)	loss 1.0924 (1.1356)	grad_norm 3.5247 (nan)	loss_scale 512.0000 (637.0054)	mem 12860MB
[2024-05-29 03:15:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 12 training takes 0:43:45
[2024-05-29 03:15:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.840 (7.840)	Loss 0.4111 (0.4111)	Acc@1 94.922 (94.922)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-29 03:15:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.110 (0.195)	Loss 0.7412 (0.6446)	Acc@1 82.812 (87.720)	Acc@5 97.656 (98.209)	Mem 12860MB
[2024-05-29 03:15:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.614 Acc@5 97.996
[2024-05-29 03:15:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-05-29 03:15:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.61%
[2024-05-29 03:15:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][0/5004]	eta 12:59:36 lr 0.000008	 wd 0.0500	time 9.3479 (9.3479)	loss 0.9439 (0.9439)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 03:16:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][100/5004]	eta 0:48:02 lr 0.000008	 wd 0.0500	time 0.4190 (0.5878)	loss 1.5737 (1.1236)	grad_norm 4.8915 (4.2705)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 03:17:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][200/5004]	eta 0:43:19 lr 0.000008	 wd 0.0500	time 0.4906 (0.5411)	loss 0.8114 (1.1317)	grad_norm 2.9973 (4.0640)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 03:18:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][300/5004]	eta 0:41:13 lr 0.000008	 wd 0.0500	time 0.3978 (0.5259)	loss 1.3314 (1.1334)	grad_norm 4.8417 (4.1471)	loss_scale 1024.0000 (654.8837)	mem 12860MB
[2024-05-29 03:19:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][400/5004]	eta 0:40:10 lr 0.000008	 wd 0.0500	time 0.3621 (0.5236)	loss 1.1441 (1.1377)	grad_norm 3.6489 (4.1547)	loss_scale 1024.0000 (746.9327)	mem 12860MB
[2024-05-29 03:19:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][500/5004]	eta 0:38:56 lr 0.000008	 wd 0.0500	time 0.3899 (0.5188)	loss 0.8299 (1.1316)	grad_norm 3.2969 (4.1010)	loss_scale 1024.0000 (802.2355)	mem 12860MB
[2024-05-29 03:20:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][600/5004]	eta 0:37:51 lr 0.000008	 wd 0.0500	time 0.3986 (0.5157)	loss 1.2698 (1.1336)	grad_norm 2.9804 (4.0160)	loss_scale 1024.0000 (839.1348)	mem 12860MB
[2024-05-29 03:21:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][700/5004]	eta 0:36:57 lr 0.000008	 wd 0.0500	time 0.4217 (0.5152)	loss 1.4096 (1.1361)	grad_norm 3.0740 (3.9869)	loss_scale 1024.0000 (865.5064)	mem 12860MB
[2024-05-29 03:22:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][800/5004]	eta 0:36:01 lr 0.000008	 wd 0.0500	time 0.4495 (0.5141)	loss 0.7912 (1.1351)	grad_norm 2.8160 (4.1106)	loss_scale 1024.0000 (885.2934)	mem 12860MB
[2024-05-29 03:23:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][900/5004]	eta 0:35:13 lr 0.000008	 wd 0.0500	time 0.4631 (0.5150)	loss 1.2447 (1.1366)	grad_norm 2.6615 (4.1150)	loss_scale 1024.0000 (900.6881)	mem 12860MB
[2024-05-29 03:24:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1000/5004]	eta 0:34:19 lr 0.000008	 wd 0.0500	time 0.4326 (0.5143)	loss 1.5630 (1.1364)	grad_norm 3.3771 (4.1545)	loss_scale 1024.0000 (913.0070)	mem 12860MB
[2024-05-29 03:24:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1100/5004]	eta 0:33:27 lr 0.000008	 wd 0.0500	time 0.4320 (0.5143)	loss 0.8631 (1.1335)	grad_norm 2.6749 (4.1114)	loss_scale 1024.0000 (923.0881)	mem 12860MB
[2024-05-29 03:25:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1200/5004]	eta 0:32:36 lr 0.000008	 wd 0.0500	time 0.4463 (0.5143)	loss 1.5627 (1.1332)	grad_norm 2.7090 (4.1977)	loss_scale 1024.0000 (931.4904)	mem 12860MB
[2024-05-29 03:26:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1300/5004]	eta 0:31:43 lr 0.000008	 wd 0.0500	time 0.3674 (0.5140)	loss 1.1100 (1.1341)	grad_norm 4.0075 (4.1524)	loss_scale 1024.0000 (938.6011)	mem 12860MB
[2024-05-29 03:27:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1400/5004]	eta 0:30:53 lr 0.000008	 wd 0.0500	time 0.4783 (0.5144)	loss 1.2779 (1.1337)	grad_norm 3.1271 (4.1392)	loss_scale 1024.0000 (944.6966)	mem 12860MB
[2024-05-29 03:28:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1500/5004]	eta 0:30:03 lr 0.000008	 wd 0.0500	time 0.4134 (0.5146)	loss 0.9637 (1.1320)	grad_norm 2.6877 (4.1402)	loss_scale 1024.0000 (949.9800)	mem 12860MB
[2024-05-29 03:29:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1600/5004]	eta 0:29:10 lr 0.000008	 wd 0.0500	time 0.4409 (0.5144)	loss 0.7965 (1.1320)	grad_norm 4.1270 (4.1245)	loss_scale 1024.0000 (954.6034)	mem 12860MB
[2024-05-29 03:30:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1700/5004]	eta 0:28:21 lr 0.000008	 wd 0.0500	time 0.4516 (0.5148)	loss 1.3677 (1.1321)	grad_norm 3.5111 (4.1501)	loss_scale 1024.0000 (958.6831)	mem 12860MB
[2024-05-29 03:31:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1800/5004]	eta 0:27:30 lr 0.000008	 wd 0.0500	time 0.3852 (0.5153)	loss 1.3594 (1.1330)	grad_norm 3.5920 (4.1584)	loss_scale 1024.0000 (962.3098)	mem 12860MB
[2024-05-29 03:31:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][1900/5004]	eta 0:26:39 lr 0.000008	 wd 0.0500	time 0.4273 (0.5152)	loss 1.1441 (1.1327)	grad_norm 2.8715 (4.1347)	loss_scale 1024.0000 (965.5550)	mem 12860MB
[2024-05-29 03:32:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2000/5004]	eta 0:25:46 lr 0.000007	 wd 0.0500	time 0.4590 (0.5149)	loss 0.8787 (1.1312)	grad_norm 3.6538 (4.1237)	loss_scale 1024.0000 (968.4758)	mem 12860MB
[2024-05-29 03:33:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2100/5004]	eta 0:24:54 lr 0.000007	 wd 0.0500	time 0.3816 (0.5146)	loss 0.9665 (1.1308)	grad_norm 3.4155 (4.1084)	loss_scale 1024.0000 (971.1185)	mem 12860MB
[2024-05-29 03:34:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2200/5004]	eta 0:24:02 lr 0.000007	 wd 0.0500	time 0.3958 (0.5144)	loss 1.2214 (1.1320)	grad_norm 3.0380 (4.0882)	loss_scale 1024.0000 (973.5211)	mem 12860MB
[2024-05-29 03:35:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2300/5004]	eta 0:23:10 lr 0.000007	 wd 0.0500	time 0.4547 (0.5141)	loss 1.1981 (1.1318)	grad_norm 2.6421 (4.0729)	loss_scale 1024.0000 (975.7149)	mem 12860MB
[2024-05-29 03:36:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2400/5004]	eta 0:22:19 lr 0.000007	 wd 0.0500	time 0.4258 (0.5144)	loss 1.1934 (1.1301)	grad_norm 3.0780 (4.0656)	loss_scale 1024.0000 (977.7259)	mem 12860MB
[2024-05-29 03:36:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2500/5004]	eta 0:21:28 lr 0.000007	 wd 0.0500	time 0.3925 (0.5145)	loss 1.2318 (1.1289)	grad_norm 2.9226 (4.0608)	loss_scale 1024.0000 (979.5762)	mem 12860MB
[2024-05-29 03:37:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2600/5004]	eta 0:20:37 lr 0.000007	 wd 0.0500	time 0.3861 (0.5147)	loss 0.7778 (1.1283)	grad_norm 3.2122 (4.0591)	loss_scale 1024.0000 (981.2841)	mem 12860MB
[2024-05-29 03:38:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2700/5004]	eta 0:19:45 lr 0.000007	 wd 0.0500	time 0.4492 (0.5145)	loss 1.2124 (1.1264)	grad_norm 3.6641 (4.0574)	loss_scale 1024.0000 (982.8656)	mem 12860MB
[2024-05-29 03:39:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2800/5004]	eta 0:18:54 lr 0.000007	 wd 0.0500	time 0.4173 (0.5146)	loss 0.6454 (1.1254)	grad_norm 4.5505 (4.0619)	loss_scale 1024.0000 (984.3342)	mem 12860MB
[2024-05-29 03:40:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][2900/5004]	eta 0:18:04 lr 0.000007	 wd 0.0500	time 0.4135 (0.5155)	loss 1.1892 (1.1251)	grad_norm 6.5009 (4.0571)	loss_scale 1024.0000 (985.7015)	mem 12860MB
[2024-05-29 03:41:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3000/5004]	eta 0:17:13 lr 0.000007	 wd 0.0500	time 0.4099 (0.5157)	loss 0.6543 (1.1238)	grad_norm 3.3427 (4.0625)	loss_scale 1024.0000 (986.9777)	mem 12860MB
[2024-05-29 03:42:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3100/5004]	eta 0:16:24 lr 0.000007	 wd 0.0500	time 0.4197 (0.5170)	loss 0.9027 (1.1262)	grad_norm 3.2015 (4.0601)	loss_scale 1024.0000 (988.1716)	mem 12860MB
[2024-05-29 03:43:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3200/5004]	eta 0:15:33 lr 0.000007	 wd 0.0500	time 0.4355 (0.5177)	loss 1.4157 (1.1263)	grad_norm 3.1609 (4.0628)	loss_scale 1024.0000 (989.2908)	mem 12860MB
[2024-05-29 03:44:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3300/5004]	eta 0:14:41 lr 0.000007	 wd 0.0500	time 0.4329 (0.5175)	loss 1.5542 (1.1287)	grad_norm 20.8346 (4.0830)	loss_scale 1024.0000 (990.3423)	mem 12860MB
[2024-05-29 03:44:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3400/5004]	eta 0:13:50 lr 0.000007	 wd 0.0500	time 0.4254 (0.5178)	loss 0.9506 (1.1279)	grad_norm 3.6267 (4.0647)	loss_scale 1024.0000 (991.3320)	mem 12860MB
[2024-05-29 03:45:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3500/5004]	eta 0:12:59 lr 0.000007	 wd 0.0500	time 0.4972 (0.5180)	loss 1.5514 (1.1285)	grad_norm 3.7843 (4.0590)	loss_scale 1024.0000 (992.2651)	mem 12860MB
[2024-05-29 03:46:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3600/5004]	eta 0:12:07 lr 0.000007	 wd 0.0500	time 0.4207 (0.5183)	loss 1.2699 (1.1292)	grad_norm 3.3266 (4.0477)	loss_scale 1024.0000 (993.1463)	mem 12860MB
[2024-05-29 03:47:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3700/5004]	eta 0:11:15 lr 0.000007	 wd 0.0500	time 0.4949 (0.5182)	loss 0.8685 (1.1284)	grad_norm 3.7937 (4.0554)	loss_scale 1024.0000 (993.9800)	mem 12860MB
[2024-05-29 03:48:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3800/5004]	eta 0:10:24 lr 0.000007	 wd 0.0500	time 0.4211 (0.5185)	loss 1.5300 (1.1292)	grad_norm 2.7453 (4.0561)	loss_scale 1024.0000 (994.7698)	mem 12860MB
[2024-05-29 03:49:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][3900/5004]	eta 0:09:32 lr 0.000007	 wd 0.0500	time 0.4475 (0.5186)	loss 1.3426 (1.1297)	grad_norm 3.5251 (4.0532)	loss_scale 1024.0000 (995.5191)	mem 12860MB
[2024-05-29 03:50:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4000/5004]	eta 0:08:40 lr 0.000007	 wd 0.0500	time 0.4356 (0.5186)	loss 1.1253 (1.1299)	grad_norm 3.1797 (4.0466)	loss_scale 1024.0000 (996.2309)	mem 12860MB
[2024-05-29 03:51:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4100/5004]	eta 0:07:49 lr 0.000007	 wd 0.0500	time 0.4155 (0.5189)	loss 1.1749 (1.1293)	grad_norm 4.8581 (4.0383)	loss_scale 1024.0000 (996.9081)	mem 12860MB
[2024-05-29 03:51:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4200/5004]	eta 0:06:57 lr 0.000007	 wd 0.0500	time 0.4471 (0.5193)	loss 1.4465 (1.1281)	grad_norm 4.2260 (4.0522)	loss_scale 1024.0000 (997.5530)	mem 12860MB
[2024-05-29 03:52:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4300/5004]	eta 0:06:05 lr 0.000007	 wd 0.0500	time 0.3715 (0.5194)	loss 0.9975 (1.1284)	grad_norm 2.9973 (4.0447)	loss_scale 2048.0000 (1018.1669)	mem 12860MB
[2024-05-29 03:53:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4400/5004]	eta 0:05:13 lr 0.000007	 wd 0.0500	time 0.4018 (0.5198)	loss 1.0996 (1.1281)	grad_norm 4.3059 (4.0364)	loss_scale 2048.0000 (1041.5669)	mem 12860MB
[2024-05-29 03:54:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4500/5004]	eta 0:04:22 lr 0.000007	 wd 0.0500	time 0.4925 (0.5198)	loss 1.1788 (1.1277)	grad_norm 4.2336 (4.0451)	loss_scale 2048.0000 (1063.9271)	mem 12860MB
[2024-05-29 03:55:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4600/5004]	eta 0:03:30 lr 0.000007	 wd 0.0500	time 0.5319 (0.5207)	loss 1.1380 (1.1285)	grad_norm 3.4206 (4.0535)	loss_scale 2048.0000 (1085.3154)	mem 12860MB
[2024-05-29 03:56:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4700/5004]	eta 0:02:38 lr 0.000007	 wd 0.0500	time 0.4933 (0.5216)	loss 1.3546 (1.1276)	grad_norm 3.9020 (4.0560)	loss_scale 2048.0000 (1105.7937)	mem 12860MB
[2024-05-29 03:57:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4800/5004]	eta 0:01:46 lr 0.000007	 wd 0.0500	time 0.4738 (0.5229)	loss 0.6135 (1.1282)	grad_norm 3.6001 (4.0794)	loss_scale 2048.0000 (1125.4189)	mem 12860MB
[2024-05-29 03:58:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][4900/5004]	eta 0:00:54 lr 0.000007	 wd 0.0500	time 0.5033 (0.5239)	loss 1.2201 (1.1289)	grad_norm 2.8939 (4.0998)	loss_scale 2048.0000 (1144.2432)	mem 12860MB
[2024-05-29 03:59:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [13/30][5000/5004]	eta 0:00:02 lr 0.000007	 wd 0.0500	time 0.3526 (0.5234)	loss 1.3827 (1.1285)	grad_norm 3.4848 (4.1042)	loss_scale 2048.0000 (1162.3147)	mem 12860MB
[2024-05-29 03:59:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 13 training takes 0:43:41
[2024-05-29 03:59:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.498 (7.498)	Loss 0.3867 (0.3867)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 03:59:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.118 (0.221)	Loss 0.7236 (0.6230)	Acc@1 83.203 (87.647)	Acc@5 97.656 (98.190)	Mem 12860MB
[2024-05-29 03:59:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.676 Acc@5 97.984
[2024-05-29 03:59:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 03:59:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.68%
[2024-05-29 03:59:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][0/5004]	eta 12:52:31 lr 0.000007	 wd 0.0500	time 9.2629 (9.2629)	loss 0.6242 (0.6242)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-29 04:00:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][100/5004]	eta 0:48:19 lr 0.000007	 wd 0.0500	time 0.3948 (0.5912)	loss 0.7905 (1.1318)	grad_norm 4.0316 (3.9090)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-29 04:01:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][200/5004]	eta 0:43:43 lr 0.000007	 wd 0.0500	time 0.4484 (0.5460)	loss 0.6079 (1.1247)	grad_norm 3.4273 (3.9452)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-29 04:02:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][300/5004]	eta 0:41:42 lr 0.000007	 wd 0.0500	time 0.4598 (0.5320)	loss 0.9015 (1.1109)	grad_norm 3.3805 (3.8490)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-29 04:03:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][400/5004]	eta 0:40:20 lr 0.000007	 wd 0.0500	time 0.3839 (0.5257)	loss 0.9107 (1.1174)	grad_norm 3.1787 (3.8493)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-29 04:04:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][500/5004]	eta 0:39:09 lr 0.000007	 wd 0.0500	time 0.4007 (0.5217)	loss 1.4795 (1.1197)	grad_norm 7.3856 (nan)	loss_scale 1024.0000 (1896.7505)	mem 12860MB
[2024-05-29 04:05:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][600/5004]	eta 0:38:08 lr 0.000007	 wd 0.0500	time 0.4283 (0.5196)	loss 1.4239 (1.1212)	grad_norm 3.3579 (nan)	loss_scale 1024.0000 (1751.5341)	mem 12860MB
[2024-05-29 04:05:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][700/5004]	eta 0:37:05 lr 0.000007	 wd 0.0500	time 0.4296 (0.5172)	loss 1.5030 (1.1185)	grad_norm 3.5436 (nan)	loss_scale 1024.0000 (1647.7489)	mem 12860MB
[2024-05-29 04:06:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][800/5004]	eta 0:36:10 lr 0.000007	 wd 0.0500	time 0.3738 (0.5164)	loss 1.1467 (1.1116)	grad_norm 3.2420 (nan)	loss_scale 1024.0000 (1569.8777)	mem 12860MB
[2024-05-29 04:07:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][900/5004]	eta 0:35:11 lr 0.000007	 wd 0.0500	time 0.3800 (0.5146)	loss 1.0224 (1.1162)	grad_norm 4.0394 (nan)	loss_scale 1024.0000 (1509.2919)	mem 12860MB
[2024-05-29 04:08:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1000/5004]	eta 0:34:14 lr 0.000007	 wd 0.0500	time 0.3634 (0.5132)	loss 0.8239 (1.1149)	grad_norm 4.0196 (nan)	loss_scale 1024.0000 (1460.8112)	mem 12860MB
[2024-05-29 04:09:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1100/5004]	eta 0:33:20 lr 0.000007	 wd 0.0500	time 0.4361 (0.5124)	loss 1.1693 (1.1177)	grad_norm 2.7075 (nan)	loss_scale 1024.0000 (1421.1371)	mem 12860MB
[2024-05-29 04:10:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1200/5004]	eta 0:32:39 lr 0.000007	 wd 0.0500	time 0.4511 (0.5151)	loss 0.8778 (1.1157)	grad_norm 3.4978 (nan)	loss_scale 1024.0000 (1388.0699)	mem 12860MB
[2024-05-29 04:11:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1300/5004]	eta 0:31:55 lr 0.000007	 wd 0.0500	time 0.3565 (0.5170)	loss 0.8429 (1.1142)	grad_norm 2.8451 (nan)	loss_scale 1024.0000 (1360.0861)	mem 12860MB
[2024-05-29 04:11:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1400/5004]	eta 0:31:00 lr 0.000007	 wd 0.0500	time 0.4584 (0.5161)	loss 1.2317 (1.1145)	grad_norm 2.5784 (nan)	loss_scale 1024.0000 (1336.0971)	mem 12860MB
[2024-05-29 04:12:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1500/5004]	eta 0:30:04 lr 0.000007	 wd 0.0500	time 0.4099 (0.5150)	loss 0.8832 (1.1137)	grad_norm 3.1279 (nan)	loss_scale 1024.0000 (1315.3045)	mem 12860MB
[2024-05-29 04:13:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1600/5004]	eta 0:29:10 lr 0.000007	 wd 0.0500	time 0.4751 (0.5142)	loss 0.9944 (1.1129)	grad_norm 3.4373 (nan)	loss_scale 1024.0000 (1297.1093)	mem 12860MB
[2024-05-29 04:14:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1700/5004]	eta 0:28:15 lr 0.000007	 wd 0.0500	time 0.4078 (0.5133)	loss 1.1347 (1.1121)	grad_norm 3.1821 (nan)	loss_scale 1024.0000 (1281.0535)	mem 12860MB
[2024-05-29 04:15:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1800/5004]	eta 0:27:24 lr 0.000007	 wd 0.0500	time 0.4559 (0.5132)	loss 0.9863 (1.1123)	grad_norm 3.4560 (nan)	loss_scale 1024.0000 (1266.7807)	mem 12860MB
[2024-05-29 04:16:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][1900/5004]	eta 0:26:34 lr 0.000007	 wd 0.0500	time 0.4140 (0.5136)	loss 1.1307 (1.1119)	grad_norm 3.0559 (nan)	loss_scale 1024.0000 (1254.0095)	mem 12860MB
[2024-05-29 04:16:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2000/5004]	eta 0:25:44 lr 0.000007	 wd 0.0500	time 0.4771 (0.5143)	loss 1.5622 (1.1112)	grad_norm 19.4221 (nan)	loss_scale 1024.0000 (1242.5147)	mem 12860MB
[2024-05-29 04:17:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2100/5004]	eta 0:24:54 lr 0.000007	 wd 0.0500	time 0.3907 (0.5146)	loss 0.7923 (1.1110)	grad_norm 4.1095 (nan)	loss_scale 1024.0000 (1232.1142)	mem 12860MB
[2024-05-29 04:18:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2200/5004]	eta 0:24:03 lr 0.000007	 wd 0.0500	time 0.4388 (0.5147)	loss 0.7279 (1.1111)	grad_norm 3.4574 (nan)	loss_scale 1024.0000 (1222.6588)	mem 12860MB
[2024-05-29 04:19:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2300/5004]	eta 0:23:11 lr 0.000007	 wd 0.0500	time 0.4391 (0.5148)	loss 0.7993 (1.1119)	grad_norm 2.9758 (nan)	loss_scale 1024.0000 (1214.0252)	mem 12860MB
[2024-05-29 04:20:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2400/5004]	eta 0:22:22 lr 0.000007	 wd 0.0500	time 0.4508 (0.5154)	loss 1.2194 (1.1128)	grad_norm 2.9885 (nan)	loss_scale 1024.0000 (1206.1108)	mem 12860MB
[2024-05-29 04:21:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2500/5004]	eta 0:21:31 lr 0.000007	 wd 0.0500	time 0.4343 (0.5157)	loss 1.1214 (1.1112)	grad_norm 3.0349 (nan)	loss_scale 1024.0000 (1198.8293)	mem 12860MB
[2024-05-29 04:22:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2600/5004]	eta 0:20:44 lr 0.000007	 wd 0.0500	time 0.4516 (0.5177)	loss 0.8271 (1.1126)	grad_norm 4.0932 (nan)	loss_scale 1024.0000 (1192.1077)	mem 12860MB
[2024-05-29 04:23:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2700/5004]	eta 0:19:54 lr 0.000007	 wd 0.0500	time 0.4709 (0.5183)	loss 1.4934 (1.1136)	grad_norm 3.9480 (nan)	loss_scale 1024.0000 (1185.8837)	mem 12860MB
[2024-05-29 04:24:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2800/5004]	eta 0:19:06 lr 0.000007	 wd 0.0500	time 0.4085 (0.5200)	loss 0.9950 (1.1134)	grad_norm 7.4107 (nan)	loss_scale 1024.0000 (1180.1042)	mem 12860MB
[2024-05-29 04:25:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][2900/5004]	eta 0:18:18 lr 0.000007	 wd 0.0500	time 0.4627 (0.5222)	loss 1.3759 (1.1137)	grad_norm 3.5553 (nan)	loss_scale 1024.0000 (1174.7232)	mem 12860MB
[2024-05-29 04:25:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3000/5004]	eta 0:17:28 lr 0.000007	 wd 0.0500	time 0.5269 (0.5234)	loss 0.7310 (1.1136)	grad_norm 9.1482 (nan)	loss_scale 1024.0000 (1169.7008)	mem 12860MB
[2024-05-29 04:26:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3100/5004]	eta 0:16:38 lr 0.000007	 wd 0.0500	time 0.3695 (0.5245)	loss 0.7655 (1.1124)	grad_norm 4.7402 (nan)	loss_scale 1024.0000 (1165.0023)	mem 12860MB
[2024-05-29 04:27:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3200/5004]	eta 0:15:45 lr 0.000007	 wd 0.0500	time 0.4167 (0.5241)	loss 1.1105 (1.1127)	grad_norm 3.5124 (nan)	loss_scale 1024.0000 (1160.5973)	mem 12860MB
[2024-05-29 04:28:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3300/5004]	eta 0:14:52 lr 0.000007	 wd 0.0500	time 0.3666 (0.5235)	loss 1.4678 (1.1134)	grad_norm 3.0866 (nan)	loss_scale 1024.0000 (1156.4593)	mem 12860MB
[2024-05-29 04:29:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3400/5004]	eta 0:13:59 lr 0.000007	 wd 0.0500	time 0.4426 (0.5231)	loss 0.8839 (1.1137)	grad_norm 2.7731 (nan)	loss_scale 1024.0000 (1152.5645)	mem 12860MB
[2024-05-29 04:30:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3500/5004]	eta 0:13:06 lr 0.000007	 wd 0.0500	time 0.4337 (0.5230)	loss 1.1526 (1.1147)	grad_norm 4.6045 (nan)	loss_scale 1024.0000 (1148.8923)	mem 12860MB
[2024-05-29 04:31:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3600/5004]	eta 0:12:13 lr 0.000007	 wd 0.0500	time 0.3920 (0.5227)	loss 1.3256 (1.1148)	grad_norm 3.1265 (nan)	loss_scale 1024.0000 (1145.4240)	mem 12860MB
[2024-05-29 04:32:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3700/5004]	eta 0:11:21 lr 0.000007	 wd 0.0500	time 0.4278 (0.5225)	loss 1.1022 (1.1145)	grad_norm 3.7576 (nan)	loss_scale 1024.0000 (1142.1432)	mem 12860MB
[2024-05-29 04:32:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3800/5004]	eta 0:10:29 lr 0.000007	 wd 0.0500	time 0.4169 (0.5225)	loss 1.3090 (1.1138)	grad_norm 3.8053 (nan)	loss_scale 1024.0000 (1139.0350)	mem 12860MB
[2024-05-29 04:33:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][3900/5004]	eta 0:09:36 lr 0.000007	 wd 0.0500	time 0.4404 (0.5225)	loss 1.2087 (1.1139)	grad_norm 3.6674 (nan)	loss_scale 1024.0000 (1136.0861)	mem 12860MB
[2024-05-29 04:34:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4000/5004]	eta 0:08:44 lr 0.000007	 wd 0.0500	time 0.3821 (0.5227)	loss 0.7283 (1.1144)	grad_norm 3.4491 (nan)	loss_scale 1024.0000 (1133.2847)	mem 12860MB
[2024-05-29 04:35:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4100/5004]	eta 0:07:53 lr 0.000007	 wd 0.0500	time 0.4889 (0.5238)	loss 1.4172 (1.1153)	grad_norm 3.7797 (nan)	loss_scale 1024.0000 (1130.6198)	mem 12860MB
[2024-05-29 04:36:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4200/5004]	eta 0:07:01 lr 0.000007	 wd 0.0500	time 0.5893 (0.5236)	loss 0.9122 (1.1150)	grad_norm 3.1778 (nan)	loss_scale 1024.0000 (1128.0819)	mem 12860MB
[2024-05-29 04:37:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4300/5004]	eta 0:06:08 lr 0.000007	 wd 0.0500	time 0.4464 (0.5239)	loss 1.1019 (1.1159)	grad_norm 3.6096 (nan)	loss_scale 1024.0000 (1125.6619)	mem 12860MB
[2024-05-29 04:38:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4400/5004]	eta 0:05:16 lr 0.000007	 wd 0.0500	time 0.4722 (0.5241)	loss 1.3864 (1.1159)	grad_norm 3.0288 (nan)	loss_scale 1024.0000 (1123.3520)	mem 12860MB
[2024-05-29 04:39:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4500/5004]	eta 0:04:24 lr 0.000007	 wd 0.0500	time 0.4350 (0.5243)	loss 1.1891 (1.1162)	grad_norm 2.8344 (nan)	loss_scale 2048.0000 (1137.9800)	mem 12860MB
[2024-05-29 04:40:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4600/5004]	eta 0:03:31 lr 0.000007	 wd 0.0500	time 0.3969 (0.5244)	loss 0.9040 (1.1160)	grad_norm 3.4267 (nan)	loss_scale 2048.0000 (1157.7587)	mem 12860MB
[2024-05-29 04:40:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4700/5004]	eta 0:02:39 lr 0.000007	 wd 0.0500	time 0.4998 (0.5245)	loss 1.5038 (1.1161)	grad_norm 3.7910 (nan)	loss_scale 2048.0000 (1176.6960)	mem 12860MB
[2024-05-29 04:41:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4800/5004]	eta 0:01:46 lr 0.000007	 wd 0.0500	time 0.4147 (0.5243)	loss 0.7994 (1.1149)	grad_norm 4.0850 (nan)	loss_scale 2048.0000 (1194.8444)	mem 12860MB
[2024-05-29 04:42:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][4900/5004]	eta 0:00:54 lr 0.000007	 wd 0.0500	time 0.4138 (0.5246)	loss 1.0195 (1.1150)	grad_norm 3.4495 (nan)	loss_scale 2048.0000 (1212.2522)	mem 12860MB
[2024-05-29 04:43:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [14/30][5000/5004]	eta 0:00:02 lr 0.000007	 wd 0.0500	time 0.3576 (0.5239)	loss 0.7885 (1.1159)	grad_norm 3.4899 (nan)	loss_scale 1024.0000 (1226.9162)	mem 12860MB
[2024-05-29 04:43:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 14 training takes 0:43:44
[2024-05-29 04:43:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.326 (7.326)	Loss 0.3933 (0.3933)	Acc@1 94.141 (94.141)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-29 04:43:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.124 (0.200)	Loss 0.7310 (0.6174)	Acc@1 82.812 (87.798)	Acc@5 97.656 (98.194)	Mem 12860MB
[2024-05-29 04:44:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.676 Acc@5 97.978
[2024-05-29 04:44:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 04:44:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.68%
[2024-05-29 04:44:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][0/5004]	eta 13:25:41 lr 0.000007	 wd 0.0500	time 9.6606 (9.6606)	loss 1.3173 (1.3173)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:45:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][100/5004]	eta 0:49:01 lr 0.000007	 wd 0.0500	time 0.4239 (0.5998)	loss 0.7897 (1.0749)	grad_norm 3.4407 (3.6004)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:45:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][200/5004]	eta 0:45:15 lr 0.000007	 wd 0.0500	time 0.4534 (0.5653)	loss 1.1984 (1.0752)	grad_norm 3.0182 (3.6257)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:46:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][300/5004]	eta 0:43:02 lr 0.000007	 wd 0.0500	time 0.4501 (0.5489)	loss 1.3906 (1.0817)	grad_norm 2.9620 (3.6093)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:47:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][400/5004]	eta 0:41:32 lr 0.000007	 wd 0.0500	time 0.4137 (0.5413)	loss 1.5298 (1.0988)	grad_norm 4.7910 (3.6354)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:48:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][500/5004]	eta 0:40:09 lr 0.000007	 wd 0.0500	time 0.4626 (0.5350)	loss 1.3174 (1.1051)	grad_norm 3.0698 (3.6359)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:49:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][600/5004]	eta 0:39:00 lr 0.000007	 wd 0.0500	time 0.4360 (0.5315)	loss 1.0631 (1.1062)	grad_norm 3.3677 (3.6268)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:50:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][700/5004]	eta 0:37:58 lr 0.000006	 wd 0.0500	time 0.3839 (0.5294)	loss 1.4767 (1.1088)	grad_norm 4.5245 (3.6305)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:51:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][800/5004]	eta 0:36:58 lr 0.000006	 wd 0.0500	time 0.4144 (0.5276)	loss 1.2503 (1.1127)	grad_norm 3.8347 (3.8501)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:51:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][900/5004]	eta 0:35:53 lr 0.000006	 wd 0.0500	time 0.4342 (0.5248)	loss 0.9016 (1.1185)	grad_norm 4.6710 (3.9707)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:52:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1000/5004]	eta 0:34:54 lr 0.000006	 wd 0.0500	time 0.4323 (0.5230)	loss 0.7636 (1.1180)	grad_norm 3.1358 (3.9798)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:53:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1100/5004]	eta 0:33:57 lr 0.000006	 wd 0.0500	time 0.4052 (0.5218)	loss 0.9476 (1.1148)	grad_norm 3.4935 (4.0801)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:54:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1200/5004]	eta 0:32:59 lr 0.000006	 wd 0.0500	time 0.3725 (0.5204)	loss 1.3005 (1.1145)	grad_norm 4.3332 (4.0667)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:55:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1300/5004]	eta 0:32:02 lr 0.000006	 wd 0.0500	time 0.4348 (0.5190)	loss 1.1930 (1.1119)	grad_norm 3.4060 (4.0790)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:56:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1400/5004]	eta 0:31:12 lr 0.000006	 wd 0.0500	time 0.3751 (0.5194)	loss 1.2365 (1.1150)	grad_norm 3.4097 (4.0796)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:57:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1500/5004]	eta 0:30:16 lr 0.000006	 wd 0.0500	time 0.3904 (0.5183)	loss 1.2923 (1.1164)	grad_norm 3.9774 (4.0857)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:57:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1600/5004]	eta 0:29:21 lr 0.000006	 wd 0.0500	time 0.3909 (0.5175)	loss 0.7330 (1.1181)	grad_norm 4.2477 (4.0611)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:58:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1700/5004]	eta 0:28:27 lr 0.000006	 wd 0.0500	time 0.3997 (0.5169)	loss 1.2760 (1.1205)	grad_norm 4.3316 (4.0278)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 04:59:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1800/5004]	eta 0:27:35 lr 0.000006	 wd 0.0500	time 0.4631 (0.5166)	loss 1.1553 (1.1198)	grad_norm 4.8200 (4.0489)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:00:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][1900/5004]	eta 0:26:42 lr 0.000006	 wd 0.0500	time 0.5669 (0.5162)	loss 1.3290 (1.1197)	grad_norm 3.2592 (4.0386)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:01:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2000/5004]	eta 0:25:48 lr 0.000006	 wd 0.0500	time 0.4048 (0.5155)	loss 1.2906 (1.1210)	grad_norm 8.6004 (4.0474)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:02:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2100/5004]	eta 0:24:56 lr 0.000006	 wd 0.0500	time 0.3761 (0.5152)	loss 0.8038 (1.1214)	grad_norm 3.2104 (4.0782)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:02:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2200/5004]	eta 0:24:06 lr 0.000006	 wd 0.0500	time 0.4222 (0.5160)	loss 0.8164 (1.1223)	grad_norm 3.3666 (4.0850)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:03:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2300/5004]	eta 0:23:14 lr 0.000006	 wd 0.0500	time 0.4368 (0.5157)	loss 1.1839 (1.1223)	grad_norm 2.6257 (4.0866)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:04:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2400/5004]	eta 0:22:23 lr 0.000006	 wd 0.0500	time 0.4744 (0.5159)	loss 1.0209 (1.1221)	grad_norm 3.6724 (4.0790)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:05:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2500/5004]	eta 0:21:32 lr 0.000006	 wd 0.0500	time 0.3944 (0.5161)	loss 1.5674 (1.1230)	grad_norm 5.5438 (4.0701)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:06:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2600/5004]	eta 0:20:41 lr 0.000006	 wd 0.0500	time 0.4800 (0.5163)	loss 1.0275 (1.1239)	grad_norm 3.5259 (4.0876)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:07:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2700/5004]	eta 0:19:50 lr 0.000006	 wd 0.0500	time 0.3646 (0.5168)	loss 1.2114 (1.1244)	grad_norm 3.2302 (4.1062)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:08:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2800/5004]	eta 0:19:00 lr 0.000006	 wd 0.0500	time 0.4668 (0.5175)	loss 1.2474 (1.1241)	grad_norm 2.6195 (4.0956)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:09:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][2900/5004]	eta 0:18:11 lr 0.000006	 wd 0.0500	time 0.5282 (0.5190)	loss 1.3214 (1.1244)	grad_norm 3.0158 (4.0989)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:10:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3000/5004]	eta 0:17:21 lr 0.000006	 wd 0.0500	time 0.4529 (0.5199)	loss 0.7671 (1.1242)	grad_norm 3.5355 (4.0890)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:10:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3100/5004]	eta 0:16:29 lr 0.000006	 wd 0.0500	time 0.4089 (0.5199)	loss 1.3097 (1.1235)	grad_norm 3.3577 (4.0936)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:11:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3200/5004]	eta 0:15:38 lr 0.000006	 wd 0.0500	time 0.4543 (0.5202)	loss 1.1560 (1.1243)	grad_norm 4.0357 (4.0988)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:12:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3300/5004]	eta 0:14:46 lr 0.000006	 wd 0.0500	time 0.4213 (0.5202)	loss 1.0338 (1.1244)	grad_norm 3.0938 (4.1270)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:13:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3400/5004]	eta 0:13:54 lr 0.000006	 wd 0.0500	time 0.4446 (0.5205)	loss 1.1135 (1.1249)	grad_norm 8.4011 (4.1192)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:14:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3500/5004]	eta 0:13:02 lr 0.000006	 wd 0.0500	time 0.4714 (0.5206)	loss 1.1458 (1.1246)	grad_norm 3.0388 (4.1018)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:15:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3600/5004]	eta 0:12:11 lr 0.000006	 wd 0.0500	time 0.4835 (0.5208)	loss 0.9194 (1.1236)	grad_norm 3.5184 (4.0934)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:16:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3700/5004]	eta 0:11:19 lr 0.000006	 wd 0.0500	time 0.5127 (0.5210)	loss 1.1804 (1.1245)	grad_norm 3.6329 (4.0850)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:17:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3800/5004]	eta 0:10:27 lr 0.000006	 wd 0.0500	time 0.4750 (0.5211)	loss 1.1187 (1.1242)	grad_norm 3.1079 (4.0972)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:17:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][3900/5004]	eta 0:09:35 lr 0.000006	 wd 0.0500	time 0.4133 (0.5211)	loss 0.8166 (1.1241)	grad_norm 2.8128 (4.0969)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:18:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4000/5004]	eta 0:08:43 lr 0.000006	 wd 0.0500	time 0.4068 (0.5213)	loss 1.1120 (1.1237)	grad_norm 3.0454 (4.0803)	loss_scale 2048.0000 (1027.5831)	mem 12860MB
[2024-05-29 05:19:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4100/5004]	eta 0:07:51 lr 0.000006	 wd 0.0500	time 0.5422 (0.5216)	loss 0.9535 (1.1241)	grad_norm 14.3153 (4.0760)	loss_scale 2048.0000 (1052.4653)	mem 12860MB
[2024-05-29 05:20:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4200/5004]	eta 0:07:00 lr 0.000006	 wd 0.0500	time 0.5046 (0.5232)	loss 0.8660 (1.1231)	grad_norm 3.0995 (4.0701)	loss_scale 2048.0000 (1076.1628)	mem 12860MB
[2024-05-29 05:21:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4300/5004]	eta 0:06:08 lr 0.000006	 wd 0.0500	time 0.4769 (0.5234)	loss 1.2755 (1.1234)	grad_norm 3.5222 (inf)	loss_scale 1024.0000 (1083.0449)	mem 12860MB
[2024-05-29 05:22:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4400/5004]	eta 0:05:16 lr 0.000006	 wd 0.0500	time 0.4620 (0.5236)	loss 1.0417 (1.1235)	grad_norm 7.9179 (inf)	loss_scale 1024.0000 (1081.7032)	mem 12860MB
[2024-05-29 05:23:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4500/5004]	eta 0:04:23 lr 0.000006	 wd 0.0500	time 0.4093 (0.5236)	loss 0.8538 (1.1225)	grad_norm 4.0569 (inf)	loss_scale 1024.0000 (1080.4212)	mem 12860MB
[2024-05-29 05:24:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4600/5004]	eta 0:03:31 lr 0.000006	 wd 0.0500	time 0.4245 (0.5237)	loss 0.8116 (1.1227)	grad_norm 5.2269 (inf)	loss_scale 1024.0000 (1079.1950)	mem 12860MB
[2024-05-29 05:25:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4700/5004]	eta 0:02:39 lr 0.000006	 wd 0.0500	time 0.6046 (0.5240)	loss 1.1276 (1.1229)	grad_norm 5.7338 (inf)	loss_scale 1024.0000 (1078.0208)	mem 12860MB
[2024-05-29 05:26:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4800/5004]	eta 0:01:47 lr 0.000006	 wd 0.0500	time 0.3886 (0.5246)	loss 0.9153 (1.1226)	grad_norm 3.1411 (inf)	loss_scale 1024.0000 (1076.8956)	mem 12860MB
[2024-05-29 05:26:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][4900/5004]	eta 0:00:54 lr 0.000006	 wd 0.0500	time 0.5810 (0.5247)	loss 0.9271 (1.1229)	grad_norm 3.0856 (inf)	loss_scale 1024.0000 (1075.8164)	mem 12860MB
[2024-05-29 05:27:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [15/30][5000/5004]	eta 0:00:02 lr 0.000006	 wd 0.0500	time 0.3564 (0.5239)	loss 0.7465 (1.1225)	grad_norm 3.5506 (inf)	loss_scale 1024.0000 (1074.7802)	mem 12860MB
[2024-05-29 05:27:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 15 training takes 0:43:43
[2024-05-29 05:27:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.176 (8.176)	Loss 0.3772 (0.3772)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 05:28:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.209 (0.217)	Loss 0.7329 (0.6170)	Acc@1 81.250 (87.755)	Acc@5 97.656 (98.236)	Mem 12860MB
[2024-05-29 05:28:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.670 Acc@5 98.018
[2024-05-29 05:28:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 05:28:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.68%
[2024-05-29 05:28:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][0/5004]	eta 11:48:45 lr 0.000006	 wd 0.0500	time 8.4983 (8.4983)	loss 1.2348 (1.2348)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:29:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][100/5004]	eta 0:47:11 lr 0.000006	 wd 0.0500	time 0.3669 (0.5773)	loss 0.7369 (1.0968)	grad_norm 2.7516 (4.2048)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 05:30:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][200/5004]	eta 0:43:20 lr 0.000006	 wd 0.0500	time 0.3862 (0.5414)	loss 0.7840 (1.1112)	grad_norm 5.8974 (inf)	loss_scale 512.0000 (810.0299)	mem 12860MB
[2024-05-29 05:31:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][300/5004]	eta 0:41:31 lr 0.000006	 wd 0.0500	time 0.4351 (0.5297)	loss 1.6128 (1.1094)	grad_norm 3.2118 (inf)	loss_scale 512.0000 (711.0166)	mem 12860MB
[2024-05-29 05:31:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][400/5004]	eta 0:40:17 lr 0.000006	 wd 0.0500	time 0.4366 (0.5250)	loss 1.1280 (1.1170)	grad_norm 3.3503 (inf)	loss_scale 512.0000 (661.3865)	mem 12860MB
[2024-05-29 05:32:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][500/5004]	eta 0:39:14 lr 0.000006	 wd 0.0500	time 0.4614 (0.5228)	loss 1.3337 (1.1236)	grad_norm 3.1746 (inf)	loss_scale 512.0000 (631.5689)	mem 12860MB
[2024-05-29 05:33:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][600/5004]	eta 0:38:09 lr 0.000006	 wd 0.0500	time 0.3764 (0.5198)	loss 0.8109 (1.1291)	grad_norm 3.5997 (inf)	loss_scale 512.0000 (611.6739)	mem 12860MB
[2024-05-29 05:34:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][700/5004]	eta 0:37:17 lr 0.000006	 wd 0.0500	time 0.3655 (0.5198)	loss 0.9009 (1.1306)	grad_norm 2.8172 (inf)	loss_scale 512.0000 (597.4551)	mem 12860MB
[2024-05-29 05:35:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][800/5004]	eta 0:36:16 lr 0.000006	 wd 0.0500	time 0.4067 (0.5176)	loss 1.4356 (1.1309)	grad_norm 3.2225 (inf)	loss_scale 512.0000 (586.7865)	mem 12860MB
[2024-05-29 05:36:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][900/5004]	eta 0:35:16 lr 0.000006	 wd 0.0500	time 0.3929 (0.5157)	loss 1.2032 (1.1291)	grad_norm 2.8717 (inf)	loss_scale 512.0000 (578.4861)	mem 12860MB
[2024-05-29 05:36:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1000/5004]	eta 0:34:20 lr 0.000006	 wd 0.0500	time 0.3825 (0.5147)	loss 1.2311 (1.1343)	grad_norm 3.9238 (inf)	loss_scale 512.0000 (571.8442)	mem 12860MB
[2024-05-29 05:37:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1100/5004]	eta 0:33:31 lr 0.000006	 wd 0.0500	time 0.4977 (0.5154)	loss 1.1696 (1.1342)	grad_norm 2.9767 (inf)	loss_scale 512.0000 (566.4087)	mem 12860MB
[2024-05-29 05:38:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1200/5004]	eta 0:32:38 lr 0.000006	 wd 0.0500	time 0.3611 (0.5147)	loss 1.3423 (1.1353)	grad_norm 2.8552 (inf)	loss_scale 512.0000 (561.8784)	mem 12860MB
[2024-05-29 05:39:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1300/5004]	eta 0:31:44 lr 0.000006	 wd 0.0500	time 0.4409 (0.5141)	loss 1.4666 (1.1351)	grad_norm 2.7675 (inf)	loss_scale 512.0000 (558.0446)	mem 12860MB
[2024-05-29 05:40:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1400/5004]	eta 0:30:53 lr 0.000006	 wd 0.0500	time 0.4298 (0.5143)	loss 0.8072 (1.1320)	grad_norm 3.3778 (inf)	loss_scale 512.0000 (554.7580)	mem 12860MB
[2024-05-29 05:41:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1500/5004]	eta 0:30:01 lr 0.000006	 wd 0.0500	time 0.4414 (0.5141)	loss 1.1710 (1.1280)	grad_norm 3.4391 (inf)	loss_scale 512.0000 (551.9094)	mem 12860MB
[2024-05-29 05:42:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1600/5004]	eta 0:29:13 lr 0.000006	 wd 0.0500	time 0.4554 (0.5151)	loss 0.7291 (1.1268)	grad_norm 3.2142 (inf)	loss_scale 512.0000 (549.4166)	mem 12860MB
[2024-05-29 05:42:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1700/5004]	eta 0:28:27 lr 0.000006	 wd 0.0500	time 0.4221 (0.5169)	loss 1.2223 (1.1257)	grad_norm 2.8618 (inf)	loss_scale 512.0000 (547.2169)	mem 12860MB
[2024-05-29 05:43:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1800/5004]	eta 0:27:33 lr 0.000006	 wd 0.0500	time 0.4827 (0.5162)	loss 1.4080 (1.1280)	grad_norm 2.7593 (inf)	loss_scale 512.0000 (545.2615)	mem 12860MB
[2024-05-29 05:44:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][1900/5004]	eta 0:26:42 lr 0.000006	 wd 0.0500	time 0.4024 (0.5161)	loss 0.9635 (1.1276)	grad_norm 3.1950 (inf)	loss_scale 512.0000 (543.5118)	mem 12860MB
[2024-05-29 05:45:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2000/5004]	eta 0:25:55 lr 0.000006	 wd 0.0500	time 0.3713 (0.5178)	loss 1.5455 (1.1277)	grad_norm 5.7177 (inf)	loss_scale 512.0000 (541.9370)	mem 12860MB
[2024-05-29 05:46:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2100/5004]	eta 0:25:03 lr 0.000006	 wd 0.0500	time 0.5055 (0.5176)	loss 0.7670 (1.1269)	grad_norm 3.5150 (inf)	loss_scale 512.0000 (540.5121)	mem 12860MB
[2024-05-29 05:47:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2200/5004]	eta 0:24:09 lr 0.000006	 wd 0.0500	time 0.4370 (0.5171)	loss 1.0612 (1.1286)	grad_norm 3.1863 (inf)	loss_scale 512.0000 (539.2167)	mem 12860MB
[2024-05-29 05:48:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2300/5004]	eta 0:23:17 lr 0.000006	 wd 0.0500	time 0.4382 (0.5169)	loss 1.2131 (1.1282)	grad_norm 3.7878 (inf)	loss_scale 512.0000 (538.0339)	mem 12860MB
[2024-05-29 05:49:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2400/5004]	eta 0:22:24 lr 0.000006	 wd 0.0500	time 0.3973 (0.5164)	loss 0.6644 (1.1286)	grad_norm 3.8965 (inf)	loss_scale 512.0000 (536.9496)	mem 12860MB
[2024-05-29 05:49:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2500/5004]	eta 0:21:31 lr 0.000006	 wd 0.0500	time 0.3947 (0.5159)	loss 1.3256 (1.1283)	grad_norm 3.3073 (inf)	loss_scale 512.0000 (535.9520)	mem 12860MB
[2024-05-29 05:50:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2600/5004]	eta 0:20:40 lr 0.000006	 wd 0.0500	time 0.4350 (0.5162)	loss 1.2277 (1.1274)	grad_norm 2.9157 (inf)	loss_scale 512.0000 (535.0311)	mem 12860MB
[2024-05-29 05:51:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2700/5004]	eta 0:19:48 lr 0.000006	 wd 0.0500	time 0.3775 (0.5159)	loss 1.0016 (1.1267)	grad_norm 24.1386 (inf)	loss_scale 512.0000 (534.1785)	mem 12860MB
[2024-05-29 05:52:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2800/5004]	eta 0:18:56 lr 0.000006	 wd 0.0500	time 0.4006 (0.5157)	loss 0.7960 (1.1259)	grad_norm 3.6612 (nan)	loss_scale 256.0000 (532.8383)	mem 12860MB
[2024-05-29 05:53:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][2900/5004]	eta 0:18:06 lr 0.000006	 wd 0.0500	time 0.4648 (0.5165)	loss 1.1464 (1.1248)	grad_norm 3.0792 (nan)	loss_scale 256.0000 (523.2954)	mem 12860MB
[2024-05-29 05:54:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3000/5004]	eta 0:17:15 lr 0.000006	 wd 0.0500	time 0.3964 (0.5167)	loss 0.7731 (1.1261)	grad_norm 2.8673 (nan)	loss_scale 256.0000 (514.3885)	mem 12860MB
[2024-05-29 05:55:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3100/5004]	eta 0:16:24 lr 0.000006	 wd 0.0500	time 0.4292 (0.5173)	loss 1.4508 (1.1252)	grad_norm 3.5937 (nan)	loss_scale 256.0000 (506.0561)	mem 12860MB
[2024-05-29 05:55:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3200/5004]	eta 0:15:34 lr 0.000006	 wd 0.0500	time 0.4094 (0.5178)	loss 0.9576 (1.1247)	grad_norm 2.7775 (nan)	loss_scale 256.0000 (498.2443)	mem 12860MB
[2024-05-29 05:56:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3300/5004]	eta 0:14:42 lr 0.000006	 wd 0.0500	time 0.4692 (0.5179)	loss 1.2793 (1.1246)	grad_norm 2.6277 (nan)	loss_scale 256.0000 (490.9058)	mem 12860MB
[2024-05-29 05:57:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3400/5004]	eta 0:13:54 lr 0.000006	 wd 0.0500	time 0.4031 (0.5200)	loss 1.1738 (1.1243)	grad_norm 3.6475 (nan)	loss_scale 256.0000 (483.9988)	mem 12860MB
[2024-05-29 05:58:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3500/5004]	eta 0:13:03 lr 0.000006	 wd 0.0500	time 0.3874 (0.5208)	loss 1.1971 (1.1239)	grad_norm 2.9628 (nan)	loss_scale 256.0000 (477.4864)	mem 12860MB
[2024-05-29 05:59:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3600/5004]	eta 0:12:11 lr 0.000006	 wd 0.0500	time 0.3943 (0.5209)	loss 1.0564 (1.1238)	grad_norm 3.2493 (nan)	loss_scale 256.0000 (471.3357)	mem 12860MB
[2024-05-29 06:00:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3700/5004]	eta 0:11:19 lr 0.000006	 wd 0.0500	time 0.3819 (0.5213)	loss 1.2665 (1.1227)	grad_norm 4.6869 (nan)	loss_scale 256.0000 (465.5174)	mem 12860MB
[2024-05-29 06:01:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3800/5004]	eta 0:10:27 lr 0.000006	 wd 0.0500	time 0.4122 (0.5215)	loss 0.9810 (1.1226)	grad_norm 2.7218 (nan)	loss_scale 256.0000 (460.0053)	mem 12860MB
[2024-05-29 06:02:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][3900/5004]	eta 0:09:35 lr 0.000005	 wd 0.0500	time 0.4514 (0.5216)	loss 1.3841 (1.1228)	grad_norm 3.3484 (nan)	loss_scale 256.0000 (454.7757)	mem 12860MB
[2024-05-29 06:03:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4000/5004]	eta 0:08:44 lr 0.000005	 wd 0.0500	time 0.4218 (0.5224)	loss 1.2598 (1.1222)	grad_norm 3.1218 (nan)	loss_scale 256.0000 (449.8075)	mem 12860MB
[2024-05-29 06:04:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4100/5004]	eta 0:07:53 lr 0.000005	 wd 0.0500	time 0.4808 (0.5237)	loss 1.4273 (1.1221)	grad_norm 3.2798 (nan)	loss_scale 256.0000 (445.0817)	mem 12860MB
[2024-05-29 06:05:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4200/5004]	eta 0:07:02 lr 0.000005	 wd 0.0500	time 0.4513 (0.5250)	loss 1.3067 (1.1221)	grad_norm 3.7042 (nan)	loss_scale 256.0000 (440.5808)	mem 12860MB
[2024-05-29 06:06:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4300/5004]	eta 0:06:11 lr 0.000005	 wd 0.0500	time 0.4867 (0.5271)	loss 1.1929 (1.1229)	grad_norm 3.0598 (nan)	loss_scale 256.0000 (436.2892)	mem 12860MB
[2024-05-29 06:07:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4400/5004]	eta 0:05:18 lr 0.000005	 wd 0.0500	time 0.5106 (0.5278)	loss 0.9222 (1.1232)	grad_norm 3.9521 (nan)	loss_scale 256.0000 (432.1927)	mem 12860MB
[2024-05-29 06:07:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4500/5004]	eta 0:04:25 lr 0.000005	 wd 0.0500	time 0.4136 (0.5276)	loss 0.7205 (1.1231)	grad_norm 2.7715 (nan)	loss_scale 256.0000 (428.2782)	mem 12860MB
[2024-05-29 06:08:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4600/5004]	eta 0:03:33 lr 0.000005	 wd 0.0500	time 0.4883 (0.5281)	loss 0.8114 (1.1229)	grad_norm 4.1248 (nan)	loss_scale 256.0000 (424.5338)	mem 12860MB
[2024-05-29 06:09:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4700/5004]	eta 0:02:40 lr 0.000005	 wd 0.0500	time 0.3977 (0.5285)	loss 1.2142 (1.1232)	grad_norm 3.2147 (nan)	loss_scale 256.0000 (420.9487)	mem 12860MB
[2024-05-29 06:10:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4800/5004]	eta 0:01:48 lr 0.000005	 wd 0.0500	time 0.5271 (0.5301)	loss 1.2448 (1.1226)	grad_norm 2.9892 (nan)	loss_scale 256.0000 (417.5130)	mem 12860MB
[2024-05-29 06:11:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][4900/5004]	eta 0:00:55 lr 0.000005	 wd 0.0500	time 0.3594 (0.5302)	loss 1.4058 (1.1222)	grad_norm 3.0926 (nan)	loss_scale 256.0000 (414.2175)	mem 12860MB
[2024-05-29 06:12:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [16/30][5000/5004]	eta 0:00:02 lr 0.000005	 wd 0.0500	time 0.3453 (0.5298)	loss 0.7996 (1.1226)	grad_norm 2.7215 (nan)	loss_scale 256.0000 (411.0538)	mem 12860MB
[2024-05-29 06:12:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 16 training takes 0:44:14
[2024-05-29 06:12:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.470 (8.470)	Loss 0.3892 (0.3892)	Acc@1 95.312 (95.312)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-29 06:12:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.134 (0.234)	Loss 0.7422 (0.6273)	Acc@1 82.812 (87.806)	Acc@5 97.656 (98.194)	Mem 12860MB
[2024-05-29 06:13:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.742 Acc@5 97.990
[2024-05-29 06:13:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 06:13:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.74%
[2024-05-29 06:13:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][0/5004]	eta 14:09:07 lr 0.000005	 wd 0.0500	time 10.1814 (10.1814)	loss 1.1941 (1.1941)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:14:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][100/5004]	eta 0:49:16 lr 0.000005	 wd 0.0500	time 0.4418 (0.6030)	loss 0.7746 (1.0863)	grad_norm 3.0276 (3.6895)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:15:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][200/5004]	eta 0:44:55 lr 0.000005	 wd 0.0500	time 0.5263 (0.5611)	loss 0.8465 (1.0904)	grad_norm 2.8129 (3.9369)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:15:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][300/5004]	eta 0:43:05 lr 0.000005	 wd 0.0500	time 0.4037 (0.5496)	loss 1.1937 (1.0936)	grad_norm 3.0556 (4.0178)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:16:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][400/5004]	eta 0:41:46 lr 0.000005	 wd 0.0500	time 0.4347 (0.5445)	loss 1.0693 (1.1004)	grad_norm 4.8207 (3.9992)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:17:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][500/5004]	eta 0:40:54 lr 0.000005	 wd 0.0500	time 0.3905 (0.5450)	loss 0.9247 (1.1039)	grad_norm 3.1046 (3.9278)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:18:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][600/5004]	eta 0:39:37 lr 0.000005	 wd 0.0500	time 0.4277 (0.5399)	loss 0.8512 (1.1065)	grad_norm 5.3617 (3.8993)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:19:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][700/5004]	eta 0:38:27 lr 0.000005	 wd 0.0500	time 0.4277 (0.5360)	loss 1.2017 (1.1066)	grad_norm 4.6819 (3.8658)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:20:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][800/5004]	eta 0:37:20 lr 0.000005	 wd 0.0500	time 0.4063 (0.5329)	loss 1.2854 (1.1086)	grad_norm 4.1920 (4.0331)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:21:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][900/5004]	eta 0:36:17 lr 0.000005	 wd 0.0500	time 0.4789 (0.5306)	loss 1.3041 (1.1136)	grad_norm 3.2334 (4.0307)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:22:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1000/5004]	eta 0:35:14 lr 0.000005	 wd 0.0500	time 0.4044 (0.5282)	loss 0.9746 (1.1157)	grad_norm 4.0309 (4.1262)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:22:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1100/5004]	eta 0:34:14 lr 0.000005	 wd 0.0500	time 0.4139 (0.5263)	loss 1.2319 (1.1162)	grad_norm 4.3071 (4.1215)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:23:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1200/5004]	eta 0:33:18 lr 0.000005	 wd 0.0500	time 0.4522 (0.5255)	loss 1.4805 (1.1184)	grad_norm 3.7810 (4.0884)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:24:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1300/5004]	eta 0:32:25 lr 0.000005	 wd 0.0500	time 0.4334 (0.5252)	loss 0.8196 (1.1216)	grad_norm 3.2120 (4.2312)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:25:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1400/5004]	eta 0:31:33 lr 0.000005	 wd 0.0500	time 0.4404 (0.5254)	loss 1.1530 (1.1204)	grad_norm 3.8968 (4.1976)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:26:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1500/5004]	eta 0:30:41 lr 0.000005	 wd 0.0500	time 0.4170 (0.5254)	loss 0.8721 (1.1206)	grad_norm 3.7095 (4.1778)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:27:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1600/5004]	eta 0:29:45 lr 0.000005	 wd 0.0500	time 0.4560 (0.5246)	loss 1.1691 (1.1201)	grad_norm 3.8849 (4.1759)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:28:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1700/5004]	eta 0:28:52 lr 0.000005	 wd 0.0500	time 0.4370 (0.5243)	loss 1.3136 (1.1209)	grad_norm 3.1302 (4.1515)	loss_scale 256.0000 (256.0000)	mem 12860MB
[2024-05-29 06:28:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1800/5004]	eta 0:27:59 lr 0.000005	 wd 0.0500	time 0.4539 (0.5241)	loss 1.3060 (1.1198)	grad_norm 3.0968 (4.2069)	loss_scale 512.0000 (257.4214)	mem 12860MB
[2024-05-29 06:29:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][1900/5004]	eta 0:27:03 lr 0.000005	 wd 0.0500	time 0.3976 (0.5231)	loss 0.9132 (1.1200)	grad_norm 5.6685 (4.2146)	loss_scale 512.0000 (270.8133)	mem 12860MB
[2024-05-29 06:30:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2000/5004]	eta 0:26:09 lr 0.000005	 wd 0.0500	time 0.4068 (0.5224)	loss 1.1981 (1.1204)	grad_norm 4.9220 (4.2173)	loss_scale 512.0000 (282.8666)	mem 12860MB
[2024-05-29 06:31:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2100/5004]	eta 0:25:16 lr 0.000005	 wd 0.0500	time 0.3739 (0.5223)	loss 0.8540 (1.1207)	grad_norm 3.4508 (4.2006)	loss_scale 512.0000 (293.7725)	mem 12860MB
[2024-05-29 06:32:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2200/5004]	eta 0:24:22 lr 0.000005	 wd 0.0500	time 0.5289 (0.5217)	loss 0.9616 (1.1219)	grad_norm 2.9423 (4.2285)	loss_scale 512.0000 (303.6874)	mem 12860MB
[2024-05-29 06:33:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2300/5004]	eta 0:23:29 lr 0.000005	 wd 0.0500	time 0.4332 (0.5212)	loss 1.3558 (1.1208)	grad_norm 3.2617 (4.2260)	loss_scale 512.0000 (312.7405)	mem 12860MB
[2024-05-29 06:34:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2400/5004]	eta 0:22:36 lr 0.000005	 wd 0.0500	time 0.4673 (0.5211)	loss 0.8507 (1.1214)	grad_norm 4.1365 (4.2011)	loss_scale 512.0000 (321.0396)	mem 12860MB
[2024-05-29 06:34:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2500/5004]	eta 0:21:44 lr 0.000005	 wd 0.0500	time 0.5022 (0.5210)	loss 1.2159 (1.1216)	grad_norm 3.2751 (4.1980)	loss_scale 512.0000 (328.6749)	mem 12860MB
[2024-05-29 06:35:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2600/5004]	eta 0:20:53 lr 0.000005	 wd 0.0500	time 0.5429 (0.5212)	loss 0.7357 (1.1203)	grad_norm 2.6026 (4.2438)	loss_scale 512.0000 (335.7232)	mem 12860MB
[2024-05-29 06:36:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2700/5004]	eta 0:20:00 lr 0.000005	 wd 0.0500	time 0.4375 (0.5210)	loss 1.4373 (1.1192)	grad_norm 3.6851 (4.2429)	loss_scale 512.0000 (342.2495)	mem 12860MB
[2024-05-29 06:37:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2800/5004]	eta 0:19:07 lr 0.000005	 wd 0.0500	time 0.4529 (0.5205)	loss 1.4548 (1.1200)	grad_norm 3.3980 (4.2253)	loss_scale 512.0000 (348.3099)	mem 12860MB
[2024-05-29 06:38:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][2900/5004]	eta 0:18:15 lr 0.000005	 wd 0.0500	time 0.4476 (0.5205)	loss 1.0830 (1.1210)	grad_norm 3.5042 (4.1996)	loss_scale 512.0000 (353.9524)	mem 12860MB
[2024-05-29 06:39:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3000/5004]	eta 0:17:22 lr 0.000005	 wd 0.0500	time 0.4294 (0.5203)	loss 0.9479 (1.1198)	grad_norm 2.8609 (4.2020)	loss_scale 512.0000 (359.2189)	mem 12860MB
[2024-05-29 06:40:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3100/5004]	eta 0:16:30 lr 0.000005	 wd 0.0500	time 0.4333 (0.5201)	loss 0.6497 (1.1191)	grad_norm 3.5422 (4.1951)	loss_scale 512.0000 (364.1458)	mem 12860MB
[2024-05-29 06:40:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3200/5004]	eta 0:15:38 lr 0.000005	 wd 0.0500	time 0.4457 (0.5202)	loss 1.1925 (1.1202)	grad_norm 4.0707 (4.1807)	loss_scale 512.0000 (368.7648)	mem 12860MB
[2024-05-29 06:41:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3300/5004]	eta 0:14:46 lr 0.000005	 wd 0.0500	time 0.4306 (0.5203)	loss 1.3427 (1.1205)	grad_norm 7.1359 (4.1785)	loss_scale 512.0000 (373.1039)	mem 12860MB
[2024-05-29 06:42:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3400/5004]	eta 0:13:54 lr 0.000005	 wd 0.0500	time 0.4517 (0.5203)	loss 1.2921 (1.1213)	grad_norm 4.4448 (4.2224)	loss_scale 512.0000 (377.1879)	mem 12860MB
[2024-05-29 06:43:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3500/5004]	eta 0:13:02 lr 0.000005	 wd 0.0500	time 0.4784 (0.5202)	loss 1.2231 (1.1200)	grad_norm 3.8524 (4.2090)	loss_scale 512.0000 (381.0386)	mem 12860MB
[2024-05-29 06:44:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3600/5004]	eta 0:12:10 lr 0.000005	 wd 0.0500	time 0.4050 (0.5203)	loss 1.3358 (1.1195)	grad_norm 2.6935 (4.1952)	loss_scale 512.0000 (384.6754)	mem 12860MB
[2024-05-29 06:45:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3700/5004]	eta 0:11:19 lr 0.000005	 wd 0.0500	time 0.5438 (0.5207)	loss 0.7635 (1.1202)	grad_norm 11.2264 (4.1996)	loss_scale 512.0000 (388.1156)	mem 12860MB
[2024-05-29 06:46:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3800/5004]	eta 0:10:27 lr 0.000005	 wd 0.0500	time 0.4096 (0.5208)	loss 0.8552 (1.1187)	grad_norm 5.8031 (4.1894)	loss_scale 512.0000 (391.3749)	mem 12860MB
[2024-05-29 06:47:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][3900/5004]	eta 0:09:35 lr 0.000005	 wd 0.0500	time 0.4019 (0.5209)	loss 1.1547 (1.1186)	grad_norm 5.4508 (4.1751)	loss_scale 512.0000 (394.4671)	mem 12860MB
[2024-05-29 06:47:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4000/5004]	eta 0:08:43 lr 0.000005	 wd 0.0500	time 0.5347 (0.5210)	loss 0.8021 (1.1187)	grad_norm 3.3954 (4.1743)	loss_scale 512.0000 (397.4046)	mem 12860MB
[2024-05-29 06:48:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4100/5004]	eta 0:07:51 lr 0.000005	 wd 0.0500	time 0.5347 (0.5212)	loss 0.7119 (1.1189)	grad_norm 3.4179 (4.2187)	loss_scale 512.0000 (400.1990)	mem 12860MB
[2024-05-29 06:49:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4200/5004]	eta 0:06:59 lr 0.000005	 wd 0.0500	time 0.6348 (0.5220)	loss 0.9112 (1.1189)	grad_norm 3.3789 (4.2204)	loss_scale 512.0000 (402.8603)	mem 12860MB
[2024-05-29 06:50:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4300/5004]	eta 0:06:09 lr 0.000005	 wd 0.0500	time 0.4624 (0.5248)	loss 0.7333 (1.1182)	grad_norm 3.2471 (4.2217)	loss_scale 512.0000 (405.3978)	mem 12860MB
[2024-05-29 06:51:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4400/5004]	eta 0:05:17 lr 0.000005	 wd 0.0500	time 0.4506 (0.5254)	loss 1.4185 (1.1179)	grad_norm 4.3551 (4.2108)	loss_scale 512.0000 (407.8200)	mem 12860MB
[2024-05-29 06:52:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4500/5004]	eta 0:04:25 lr 0.000005	 wd 0.0500	time 0.4539 (0.5259)	loss 1.1916 (1.1176)	grad_norm 3.0862 (4.2004)	loss_scale 512.0000 (410.1346)	mem 12860MB
[2024-05-29 06:53:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4600/5004]	eta 0:03:32 lr 0.000005	 wd 0.0500	time 0.4091 (0.5268)	loss 1.4187 (1.1179)	grad_norm 3.6851 (4.1903)	loss_scale 512.0000 (412.3486)	mem 12860MB
[2024-05-29 06:54:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4700/5004]	eta 0:02:40 lr 0.000005	 wd 0.0500	time 0.5603 (0.5272)	loss 1.1102 (1.1172)	grad_norm 3.3120 (4.1821)	loss_scale 512.0000 (414.4684)	mem 12860MB
[2024-05-29 06:55:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4800/5004]	eta 0:01:47 lr 0.000005	 wd 0.0500	time 0.4365 (0.5277)	loss 1.3572 (1.1172)	grad_norm 3.3685 (4.1747)	loss_scale 512.0000 (416.4999)	mem 12860MB
[2024-05-29 06:56:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][4900/5004]	eta 0:00:54 lr 0.000005	 wd 0.0500	time 0.4433 (0.5285)	loss 0.7279 (1.1174)	grad_norm 5.3237 (4.1643)	loss_scale 512.0000 (418.4485)	mem 12860MB
[2024-05-29 06:57:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [17/30][5000/5004]	eta 0:00:02 lr 0.000005	 wd 0.0500	time 0.3413 (0.5277)	loss 1.3558 (1.1174)	grad_norm 3.5721 (4.1609)	loss_scale 512.0000 (420.3191)	mem 12860MB
[2024-05-29 06:57:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 17 training takes 0:44:03
[2024-05-29 06:57:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.609 (7.609)	Loss 0.3875 (0.3875)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 06:57:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.135 (0.214)	Loss 0.7266 (0.6254)	Acc@1 82.812 (87.771)	Acc@5 97.656 (98.256)	Mem 12860MB
[2024-05-29 06:57:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.684 Acc@5 98.030
[2024-05-29 06:57:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 06:57:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.74%
[2024-05-29 06:58:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][0/5004]	eta 11:30:14 lr 0.000005	 wd 0.0500	time 8.2762 (8.2762)	loss 1.8563 (1.8563)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 06:58:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][100/5004]	eta 0:47:02 lr 0.000005	 wd 0.0500	time 0.5154 (0.5756)	loss 1.3021 (1.1227)	grad_norm 2.9545 (4.1517)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 06:59:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][200/5004]	eta 0:43:07 lr 0.000005	 wd 0.0500	time 0.3637 (0.5386)	loss 1.3280 (1.1534)	grad_norm 3.5756 (3.9769)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 07:00:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][300/5004]	eta 0:41:34 lr 0.000005	 wd 0.0500	time 0.4822 (0.5303)	loss 1.1431 (1.1435)	grad_norm 3.1576 (4.0067)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 07:01:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][400/5004]	eta 0:40:06 lr 0.000005	 wd 0.0500	time 0.4672 (0.5228)	loss 0.8986 (1.1456)	grad_norm 2.8344 (3.8843)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 07:02:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][500/5004]	eta 0:38:53 lr 0.000005	 wd 0.0500	time 0.4747 (0.5181)	loss 0.8361 (1.1442)	grad_norm 4.4407 (3.8630)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 07:03:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][600/5004]	eta 0:38:05 lr 0.000005	 wd 0.0500	time 0.4069 (0.5190)	loss 1.2281 (1.1418)	grad_norm 2.7604 (3.9252)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 07:03:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][700/5004]	eta 0:37:01 lr 0.000005	 wd 0.0500	time 0.3969 (0.5162)	loss 0.7567 (1.1357)	grad_norm 3.0228 (4.0621)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 07:04:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][800/5004]	eta 0:36:01 lr 0.000005	 wd 0.0500	time 0.3989 (0.5141)	loss 1.3962 (1.1308)	grad_norm 4.7829 (4.1570)	loss_scale 1024.0000 (520.9488)	mem 12860MB
[2024-05-29 07:05:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][900/5004]	eta 0:35:05 lr 0.000005	 wd 0.0500	time 0.4837 (0.5130)	loss 1.1641 (1.1231)	grad_norm 2.7627 (4.1800)	loss_scale 1024.0000 (576.7814)	mem 12860MB
[2024-05-29 07:06:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1000/5004]	eta 0:34:09 lr 0.000005	 wd 0.0500	time 0.4134 (0.5118)	loss 1.1768 (1.1256)	grad_norm 4.7565 (4.1399)	loss_scale 1024.0000 (621.4585)	mem 12860MB
[2024-05-29 07:07:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1100/5004]	eta 0:33:14 lr 0.000005	 wd 0.0500	time 0.3825 (0.5108)	loss 0.8592 (1.1237)	grad_norm 2.8693 (4.1865)	loss_scale 1024.0000 (658.0200)	mem 12860MB
[2024-05-29 07:08:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1200/5004]	eta 0:32:21 lr 0.000005	 wd 0.0500	time 0.3750 (0.5103)	loss 1.2023 (1.1241)	grad_norm 2.9696 (4.1592)	loss_scale 1024.0000 (688.4929)	mem 12860MB
[2024-05-29 07:08:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1300/5004]	eta 0:31:32 lr 0.000005	 wd 0.0500	time 0.3864 (0.5110)	loss 1.5731 (1.1235)	grad_norm 2.8596 (4.1386)	loss_scale 1024.0000 (714.2813)	mem 12860MB
[2024-05-29 07:09:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1400/5004]	eta 0:30:39 lr 0.000005	 wd 0.0500	time 0.4143 (0.5105)	loss 1.4759 (1.1234)	grad_norm 3.7409 (4.1132)	loss_scale 1024.0000 (736.3883)	mem 12860MB
[2024-05-29 07:10:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1500/5004]	eta 0:29:47 lr 0.000005	 wd 0.0500	time 0.4587 (0.5103)	loss 1.0659 (1.1233)	grad_norm 2.9011 (4.0900)	loss_scale 1024.0000 (755.5496)	mem 12860MB
[2024-05-29 07:11:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1600/5004]	eta 0:28:58 lr 0.000005	 wd 0.0500	time 0.5136 (0.5106)	loss 0.7374 (1.1237)	grad_norm 3.5864 (4.2709)	loss_scale 1024.0000 (772.3173)	mem 12860MB
[2024-05-29 07:12:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1700/5004]	eta 0:28:08 lr 0.000005	 wd 0.0500	time 0.4375 (0.5111)	loss 1.4479 (1.1223)	grad_norm 3.3421 (4.2261)	loss_scale 1024.0000 (787.1135)	mem 12860MB
[2024-05-29 07:13:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1800/5004]	eta 0:27:18 lr 0.000005	 wd 0.0500	time 0.3638 (0.5113)	loss 1.1852 (1.1220)	grad_norm 3.0696 (4.2062)	loss_scale 1024.0000 (800.2665)	mem 12860MB
[2024-05-29 07:14:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][1900/5004]	eta 0:26:26 lr 0.000005	 wd 0.0500	time 0.4107 (0.5112)	loss 1.2472 (1.1243)	grad_norm 4.7596 (4.1771)	loss_scale 1024.0000 (812.0358)	mem 12860MB
[2024-05-29 07:14:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2000/5004]	eta 0:25:35 lr 0.000004	 wd 0.0500	time 0.4431 (0.5112)	loss 1.3496 (1.1241)	grad_norm 3.4132 (4.1480)	loss_scale 1024.0000 (822.6287)	mem 12860MB
[2024-05-29 07:15:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2100/5004]	eta 0:24:44 lr 0.000004	 wd 0.0500	time 0.4194 (0.5111)	loss 1.0729 (1.1237)	grad_norm 3.6388 (4.1551)	loss_scale 1024.0000 (832.2132)	mem 12860MB
[2024-05-29 07:16:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2200/5004]	eta 0:23:54 lr 0.000004	 wd 0.0500	time 0.4368 (0.5115)	loss 0.7634 (1.1232)	grad_norm 3.3643 (4.1385)	loss_scale 1024.0000 (840.9269)	mem 12860MB
[2024-05-29 07:17:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2300/5004]	eta 0:23:03 lr 0.000004	 wd 0.0500	time 0.4331 (0.5116)	loss 1.2505 (1.1227)	grad_norm 2.8714 (4.1224)	loss_scale 1024.0000 (848.8831)	mem 12860MB
[2024-05-29 07:18:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2400/5004]	eta 0:22:11 lr 0.000004	 wd 0.0500	time 0.4870 (0.5114)	loss 0.7119 (1.1239)	grad_norm 3.1133 (4.1210)	loss_scale 1024.0000 (856.1766)	mem 12860MB
[2024-05-29 07:19:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2500/5004]	eta 0:21:21 lr 0.000004	 wd 0.0500	time 0.5218 (0.5117)	loss 0.8096 (1.1217)	grad_norm 14.5713 (4.1138)	loss_scale 1024.0000 (862.8868)	mem 12860MB
[2024-05-29 07:20:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2600/5004]	eta 0:20:31 lr 0.000004	 wd 0.0500	time 0.3831 (0.5121)	loss 0.8165 (1.1227)	grad_norm 4.1083 (4.1065)	loss_scale 1024.0000 (869.0811)	mem 12860MB
[2024-05-29 07:20:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2700/5004]	eta 0:19:40 lr 0.000004	 wd 0.0500	time 0.4516 (0.5123)	loss 0.8422 (1.1219)	grad_norm 5.0903 (4.0976)	loss_scale 1024.0000 (874.8167)	mem 12860MB
[2024-05-29 07:21:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2800/5004]	eta 0:18:50 lr 0.000004	 wd 0.0500	time 0.4197 (0.5129)	loss 1.2277 (1.1202)	grad_norm 2.4227 (4.0802)	loss_scale 1024.0000 (880.1428)	mem 12860MB
[2024-05-29 07:22:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][2900/5004]	eta 0:18:01 lr 0.000004	 wd 0.0500	time 0.4140 (0.5142)	loss 0.8548 (1.1199)	grad_norm 3.7588 (4.0715)	loss_scale 1024.0000 (885.1017)	mem 12860MB
[2024-05-29 07:23:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3000/5004]	eta 0:17:10 lr 0.000004	 wd 0.0500	time 0.4297 (0.5142)	loss 0.9952 (1.1197)	grad_norm 3.2330 (4.0773)	loss_scale 1024.0000 (889.7301)	mem 12860MB
[2024-05-29 07:24:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3100/5004]	eta 0:16:19 lr 0.000004	 wd 0.0500	time 0.4443 (0.5142)	loss 1.1898 (1.1199)	grad_norm 3.6922 (4.0624)	loss_scale 1024.0000 (894.0600)	mem 12860MB
[2024-05-29 07:25:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3200/5004]	eta 0:15:28 lr 0.000004	 wd 0.0500	time 0.4537 (0.5147)	loss 1.2791 (1.1195)	grad_norm 3.3379 (4.0627)	loss_scale 1024.0000 (898.1193)	mem 12860MB
[2024-05-29 07:26:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3300/5004]	eta 0:14:37 lr 0.000004	 wd 0.0500	time 0.4344 (0.5150)	loss 1.1363 (1.1199)	grad_norm 3.3563 (4.0605)	loss_scale 1024.0000 (901.9327)	mem 12860MB
[2024-05-29 07:27:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3400/5004]	eta 0:13:46 lr 0.000004	 wd 0.0500	time 0.4501 (0.5154)	loss 1.2811 (1.1188)	grad_norm 2.8192 (4.0484)	loss_scale 1024.0000 (905.5219)	mem 12860MB
[2024-05-29 07:27:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3500/5004]	eta 0:12:55 lr 0.000004	 wd 0.0500	time 0.3612 (0.5154)	loss 1.5698 (1.1188)	grad_norm 3.6541 (4.0487)	loss_scale 1024.0000 (908.9060)	mem 12860MB
[2024-05-29 07:28:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3600/5004]	eta 0:12:03 lr 0.000004	 wd 0.0500	time 0.4740 (0.5154)	loss 0.9528 (1.1190)	grad_norm 4.0001 (4.0498)	loss_scale 1024.0000 (912.1022)	mem 12860MB
[2024-05-29 07:29:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3700/5004]	eta 0:11:12 lr 0.000004	 wd 0.0500	time 0.4575 (0.5156)	loss 1.3144 (1.1190)	grad_norm 11.4005 (4.0413)	loss_scale 1024.0000 (915.1256)	mem 12860MB
[2024-05-29 07:30:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3800/5004]	eta 0:10:21 lr 0.000004	 wd 0.0500	time 0.4642 (0.5160)	loss 1.0998 (1.1191)	grad_norm 5.1051 (4.0364)	loss_scale 1024.0000 (917.9900)	mem 12860MB
[2024-05-29 07:31:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][3900/5004]	eta 0:09:30 lr 0.000004	 wd 0.0500	time 0.4611 (0.5165)	loss 1.1766 (1.1185)	grad_norm 3.1301 (4.0591)	loss_scale 1024.0000 (920.7075)	mem 12860MB
[2024-05-29 07:32:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4000/5004]	eta 0:08:38 lr 0.000004	 wd 0.0500	time 0.4698 (0.5167)	loss 0.6895 (1.1176)	grad_norm 3.9138 (4.0559)	loss_scale 1024.0000 (923.2892)	mem 12860MB
[2024-05-29 07:33:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4100/5004]	eta 0:07:47 lr 0.000004	 wd 0.0500	time 0.5354 (0.5170)	loss 1.1745 (1.1174)	grad_norm 2.5242 (4.0629)	loss_scale 1024.0000 (925.7449)	mem 12860MB
[2024-05-29 07:34:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4200/5004]	eta 0:06:56 lr 0.000004	 wd 0.0500	time 0.3621 (0.5185)	loss 0.7376 (1.1175)	grad_norm 2.5467 (4.0606)	loss_scale 1024.0000 (928.0838)	mem 12860MB
[2024-05-29 07:35:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4300/5004]	eta 0:06:05 lr 0.000004	 wd 0.0500	time 0.4600 (0.5192)	loss 1.3851 (1.1173)	grad_norm 4.1066 (4.0674)	loss_scale 1024.0000 (930.3139)	mem 12860MB
[2024-05-29 07:36:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4400/5004]	eta 0:05:14 lr 0.000004	 wd 0.0500	time 0.4435 (0.5201)	loss 1.2495 (1.1171)	grad_norm 3.2389 (4.0704)	loss_scale 1024.0000 (932.4426)	mem 12860MB
[2024-05-29 07:36:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4500/5004]	eta 0:04:22 lr 0.000004	 wd 0.0500	time 0.4232 (0.5209)	loss 1.4463 (1.1164)	grad_norm 3.9809 (4.0628)	loss_scale 1024.0000 (934.4768)	mem 12860MB
[2024-05-29 07:37:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4600/5004]	eta 0:03:30 lr 0.000004	 wd 0.0500	time 0.5372 (0.5211)	loss 0.8005 (1.1163)	grad_norm 2.6116 (4.0840)	loss_scale 1024.0000 (936.4225)	mem 12860MB
[2024-05-29 07:38:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4700/5004]	eta 0:02:38 lr 0.000004	 wd 0.0500	time 0.4788 (0.5215)	loss 1.0913 (1.1164)	grad_norm 3.3061 (4.0755)	loss_scale 1024.0000 (938.2855)	mem 12860MB
[2024-05-29 07:39:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4800/5004]	eta 0:01:46 lr 0.000004	 wd 0.0500	time 0.6759 (0.5218)	loss 1.2978 (1.1163)	grad_norm 3.5301 (4.0833)	loss_scale 2048.0000 (943.0569)	mem 12860MB
[2024-05-29 07:40:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][4900/5004]	eta 0:00:54 lr 0.000004	 wd 0.0500	time 0.4709 (0.5222)	loss 1.3297 (1.1161)	grad_norm 4.5744 (4.0868)	loss_scale 2048.0000 (965.6021)	mem 12860MB
[2024-05-29 07:41:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [18/30][5000/5004]	eta 0:00:02 lr 0.000004	 wd 0.0500	time 0.4188 (0.5219)	loss 0.8788 (1.1159)	grad_norm 4.6238 (4.0856)	loss_scale 2048.0000 (987.2458)	mem 12860MB
[2024-05-29 07:41:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 18 training takes 0:43:35
[2024-05-29 07:41:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.508 (8.508)	Loss 0.3848 (0.3848)	Acc@1 95.312 (95.312)	Acc@5 99.219 (99.219)	Mem 12860MB
[2024-05-29 07:41:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.119 (0.202)	Loss 0.7314 (0.6140)	Acc@1 82.422 (87.891)	Acc@5 97.656 (98.256)	Mem 12860MB
[2024-05-29 07:42:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.730 Acc@5 98.040
[2024-05-29 07:42:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 07:42:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.74%
[2024-05-29 07:42:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][0/5004]	eta 11:35:39 lr 0.000004	 wd 0.0500	time 8.3412 (8.3412)	loss 1.1791 (1.1791)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-29 07:42:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][100/5004]	eta 0:46:51 lr 0.000004	 wd 0.0500	time 0.4666 (0.5734)	loss 1.1423 (1.0825)	grad_norm 4.0697 (3.9808)	loss_scale 2048.0000 (2048.0000)	mem 12860MB
[2024-05-29 07:43:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][200/5004]	eta 0:43:42 lr 0.000004	 wd 0.0500	time 0.4230 (0.5459)	loss 1.3765 (1.0820)	grad_norm 3.2611 (nan)	loss_scale 1024.0000 (1997.0547)	mem 12860MB
[2024-05-29 07:44:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][300/5004]	eta 0:41:42 lr 0.000004	 wd 0.0500	time 0.4214 (0.5319)	loss 1.1120 (1.0982)	grad_norm 3.4669 (nan)	loss_scale 1024.0000 (1673.7807)	mem 12860MB
[2024-05-29 07:45:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][400/5004]	eta 0:40:11 lr 0.000004	 wd 0.0500	time 0.3639 (0.5237)	loss 1.0325 (1.1091)	grad_norm 3.4015 (nan)	loss_scale 1024.0000 (1511.7406)	mem 12860MB
[2024-05-29 07:46:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][500/5004]	eta 0:39:00 lr 0.000004	 wd 0.0500	time 0.4035 (0.5197)	loss 1.3790 (1.1165)	grad_norm 5.4737 (nan)	loss_scale 1024.0000 (1414.3872)	mem 12860MB
[2024-05-29 07:47:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][600/5004]	eta 0:38:08 lr 0.000004	 wd 0.0500	time 0.4467 (0.5196)	loss 1.1764 (1.1171)	grad_norm 3.1044 (nan)	loss_scale 1024.0000 (1349.4309)	mem 12860MB
[2024-05-29 07:48:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][700/5004]	eta 0:37:04 lr 0.000004	 wd 0.0500	time 0.4339 (0.5168)	loss 1.0645 (1.1218)	grad_norm 35.0953 (nan)	loss_scale 1024.0000 (1303.0071)	mem 12860MB
[2024-05-29 07:48:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][800/5004]	eta 0:36:06 lr 0.000004	 wd 0.0500	time 0.3604 (0.5155)	loss 1.3498 (1.1225)	grad_norm 2.8369 (nan)	loss_scale 1024.0000 (1268.1748)	mem 12860MB
[2024-05-29 07:49:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][900/5004]	eta 0:35:09 lr 0.000004	 wd 0.0500	time 0.4090 (0.5141)	loss 0.6938 (1.1204)	grad_norm 3.5861 (nan)	loss_scale 1024.0000 (1241.0744)	mem 12860MB
[2024-05-29 07:50:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1000/5004]	eta 0:34:16 lr 0.000004	 wd 0.0500	time 0.3940 (0.5135)	loss 1.2611 (1.1195)	grad_norm 5.3860 (nan)	loss_scale 1024.0000 (1219.3886)	mem 12860MB
[2024-05-29 07:51:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1100/5004]	eta 0:33:23 lr 0.000004	 wd 0.0500	time 0.4549 (0.5131)	loss 1.4492 (1.1182)	grad_norm 3.5582 (nan)	loss_scale 1024.0000 (1201.6421)	mem 12860MB
[2024-05-29 07:52:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1200/5004]	eta 0:32:31 lr 0.000004	 wd 0.0500	time 0.3825 (0.5130)	loss 0.8368 (1.1204)	grad_norm 4.2525 (nan)	loss_scale 1024.0000 (1186.8510)	mem 12860MB
[2024-05-29 07:53:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1300/5004]	eta 0:31:37 lr 0.000004	 wd 0.0500	time 0.4225 (0.5123)	loss 0.7695 (1.1196)	grad_norm 3.6235 (nan)	loss_scale 1024.0000 (1174.3336)	mem 12860MB
[2024-05-29 07:53:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1400/5004]	eta 0:30:44 lr 0.000004	 wd 0.0500	time 0.4242 (0.5118)	loss 1.3345 (1.1193)	grad_norm 3.5835 (nan)	loss_scale 1024.0000 (1163.6031)	mem 12860MB
[2024-05-29 07:54:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1500/5004]	eta 0:29:54 lr 0.000004	 wd 0.0500	time 0.5586 (0.5120)	loss 1.2059 (1.1194)	grad_norm 3.3286 (nan)	loss_scale 1024.0000 (1154.3025)	mem 12860MB
[2024-05-29 07:55:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1600/5004]	eta 0:29:05 lr 0.000004	 wd 0.0500	time 0.4158 (0.5129)	loss 1.0535 (1.1187)	grad_norm 3.0955 (nan)	loss_scale 1024.0000 (1146.1636)	mem 12860MB
[2024-05-29 07:56:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1700/5004]	eta 0:28:16 lr 0.000004	 wd 0.0500	time 0.4051 (0.5134)	loss 0.8402 (1.1191)	grad_norm 3.4235 (nan)	loss_scale 1024.0000 (1138.9818)	mem 12860MB
[2024-05-29 07:57:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1800/5004]	eta 0:27:25 lr 0.000004	 wd 0.0500	time 0.3853 (0.5136)	loss 1.2269 (1.1183)	grad_norm 12.2476 (nan)	loss_scale 1024.0000 (1132.5974)	mem 12860MB
[2024-05-29 07:58:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][1900/5004]	eta 0:26:33 lr 0.000004	 wd 0.0500	time 0.4386 (0.5135)	loss 0.9287 (1.1190)	grad_norm 3.4897 (nan)	loss_scale 1024.0000 (1126.8848)	mem 12860MB
[2024-05-29 07:59:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2000/5004]	eta 0:25:43 lr 0.000004	 wd 0.0500	time 0.4094 (0.5138)	loss 1.4144 (1.1187)	grad_norm 2.9035 (nan)	loss_scale 1024.0000 (1121.7431)	mem 12860MB
[2024-05-29 08:00:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2100/5004]	eta 0:24:51 lr 0.000004	 wd 0.0500	time 0.4130 (0.5137)	loss 1.0184 (1.1174)	grad_norm 3.0246 (nan)	loss_scale 1024.0000 (1117.0909)	mem 12860MB
[2024-05-29 08:00:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2200/5004]	eta 0:24:00 lr 0.000004	 wd 0.0500	time 0.4060 (0.5137)	loss 1.1871 (1.1151)	grad_norm 3.4784 (nan)	loss_scale 1024.0000 (1112.8614)	mem 12860MB
[2024-05-29 08:01:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2300/5004]	eta 0:23:09 lr 0.000004	 wd 0.0500	time 0.3977 (0.5140)	loss 1.3788 (1.1157)	grad_norm 13.4967 (nan)	loss_scale 1024.0000 (1108.9996)	mem 12860MB
[2024-05-29 08:02:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2400/5004]	eta 0:22:18 lr 0.000004	 wd 0.0500	time 0.4022 (0.5139)	loss 1.2199 (1.1164)	grad_norm 3.4064 (nan)	loss_scale 1024.0000 (1105.4594)	mem 12860MB
[2024-05-29 08:03:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2500/5004]	eta 0:21:26 lr 0.000004	 wd 0.0500	time 0.3715 (0.5139)	loss 0.8710 (1.1162)	grad_norm inf (nan)	loss_scale 512.0000 (1101.7929)	mem 12860MB
[2024-05-29 08:04:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2600/5004]	eta 0:20:37 lr 0.000004	 wd 0.0500	time 0.4608 (0.5148)	loss 0.6413 (1.1157)	grad_norm 3.7869 (nan)	loss_scale 512.0000 (1079.1173)	mem 12860MB
[2024-05-29 08:05:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2700/5004]	eta 0:19:46 lr 0.000004	 wd 0.0500	time 0.4343 (0.5149)	loss 1.2913 (1.1150)	grad_norm 2.9277 (nan)	loss_scale 512.0000 (1058.1207)	mem 12860MB
[2024-05-29 08:06:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2800/5004]	eta 0:18:56 lr 0.000004	 wd 0.0500	time 0.4573 (0.5155)	loss 0.8589 (1.1160)	grad_norm 4.1816 (nan)	loss_scale 512.0000 (1038.6233)	mem 12860MB
[2024-05-29 08:06:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][2900/5004]	eta 0:18:04 lr 0.000004	 wd 0.0500	time 0.4397 (0.5155)	loss 1.2407 (1.1164)	grad_norm 3.1229 (nan)	loss_scale 512.0000 (1020.4702)	mem 12860MB
[2024-05-29 08:07:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3000/5004]	eta 0:17:13 lr 0.000004	 wd 0.0500	time 0.3924 (0.5156)	loss 0.7295 (1.1157)	grad_norm 4.1584 (nan)	loss_scale 512.0000 (1003.5268)	mem 12860MB
[2024-05-29 08:08:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3100/5004]	eta 0:16:22 lr 0.000004	 wd 0.0500	time 0.4628 (0.5160)	loss 0.7765 (1.1152)	grad_norm 3.8001 (nan)	loss_scale 512.0000 (987.6762)	mem 12860MB
[2024-05-29 08:09:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3200/5004]	eta 0:15:32 lr 0.000004	 wd 0.0500	time 0.4138 (0.5171)	loss 1.3237 (1.1152)	grad_norm 3.3398 (nan)	loss_scale 512.0000 (972.8160)	mem 12860MB
[2024-05-29 08:10:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3300/5004]	eta 0:14:42 lr 0.000004	 wd 0.0500	time 0.4818 (0.5178)	loss 0.7720 (1.1150)	grad_norm 4.2627 (nan)	loss_scale 512.0000 (958.8561)	mem 12860MB
[2024-05-29 08:11:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3400/5004]	eta 0:13:50 lr 0.000004	 wd 0.0500	time 0.4273 (0.5180)	loss 1.3577 (1.1150)	grad_norm 2.7811 (nan)	loss_scale 512.0000 (945.7171)	mem 12860MB
[2024-05-29 08:12:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3500/5004]	eta 0:12:59 lr 0.000004	 wd 0.0500	time 0.4800 (0.5181)	loss 0.8625 (1.1142)	grad_norm 4.1054 (nan)	loss_scale 512.0000 (933.3288)	mem 12860MB
[2024-05-29 08:13:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3600/5004]	eta 0:12:07 lr 0.000004	 wd 0.0500	time 0.4372 (0.5185)	loss 1.0782 (1.1148)	grad_norm 2.8861 (nan)	loss_scale 512.0000 (921.6284)	mem 12860MB
[2024-05-29 08:14:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3700/5004]	eta 0:11:16 lr 0.000004	 wd 0.0500	time 0.4137 (0.5186)	loss 1.2066 (1.1158)	grad_norm 3.4033 (nan)	loss_scale 512.0000 (910.5604)	mem 12860MB
[2024-05-29 08:14:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3800/5004]	eta 0:10:24 lr 0.000004	 wd 0.0500	time 0.4865 (0.5188)	loss 0.9617 (1.1161)	grad_norm 3.1809 (nan)	loss_scale 512.0000 (900.0747)	mem 12860MB
[2024-05-29 08:15:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][3900/5004]	eta 0:09:32 lr 0.000004	 wd 0.0500	time 0.4400 (0.5187)	loss 1.2640 (1.1157)	grad_norm 13.3576 (nan)	loss_scale 512.0000 (890.1266)	mem 12860MB
[2024-05-29 08:16:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4000/5004]	eta 0:08:41 lr 0.000004	 wd 0.0500	time 0.4429 (0.5192)	loss 0.9737 (1.1149)	grad_norm 3.2257 (nan)	loss_scale 512.0000 (880.6758)	mem 12860MB
[2024-05-29 08:17:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4100/5004]	eta 0:07:49 lr 0.000004	 wd 0.0500	time 0.4709 (0.5193)	loss 1.1180 (1.1137)	grad_norm 3.1562 (nan)	loss_scale 512.0000 (871.6859)	mem 12860MB
[2024-05-29 08:18:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4200/5004]	eta 0:06:58 lr 0.000004	 wd 0.0500	time 0.4285 (0.5199)	loss 1.3018 (1.1140)	grad_norm 4.1310 (nan)	loss_scale 512.0000 (863.1240)	mem 12860MB
[2024-05-29 08:19:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4300/5004]	eta 0:06:06 lr 0.000004	 wd 0.0500	time 0.4066 (0.5200)	loss 0.8979 (1.1149)	grad_norm 4.5024 (nan)	loss_scale 512.0000 (854.9602)	mem 12860MB
[2024-05-29 08:20:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4400/5004]	eta 0:05:15 lr 0.000004	 wd 0.0500	time 0.5833 (0.5223)	loss 0.8738 (1.1138)	grad_norm 3.0383 (nan)	loss_scale 512.0000 (847.1675)	mem 12860MB
[2024-05-29 08:21:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4500/5004]	eta 0:04:24 lr 0.000004	 wd 0.0500	time 0.4640 (0.5246)	loss 0.9659 (1.1152)	grad_norm 3.4824 (nan)	loss_scale 512.0000 (839.7210)	mem 12860MB
[2024-05-29 08:22:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4600/5004]	eta 0:03:31 lr 0.000004	 wd 0.0500	time 0.4096 (0.5247)	loss 1.0417 (1.1151)	grad_norm 2.5127 (nan)	loss_scale 512.0000 (832.5981)	mem 12860MB
[2024-05-29 08:23:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4700/5004]	eta 0:02:39 lr 0.000004	 wd 0.0500	time 0.4873 (0.5251)	loss 1.3938 (1.1160)	grad_norm 2.7682 (nan)	loss_scale 512.0000 (825.7783)	mem 12860MB
[2024-05-29 08:24:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4800/5004]	eta 0:01:47 lr 0.000004	 wd 0.0500	time 0.4202 (0.5250)	loss 1.1234 (1.1157)	grad_norm 3.6675 (nan)	loss_scale 512.0000 (819.2427)	mem 12860MB
[2024-05-29 08:24:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][4900/5004]	eta 0:00:54 lr 0.000004	 wd 0.0500	time 0.4721 (0.5251)	loss 1.3019 (1.1158)	grad_norm 4.3488 (nan)	loss_scale 512.0000 (812.9737)	mem 12860MB
[2024-05-29 08:25:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [19/30][5000/5004]	eta 0:00:02 lr 0.000004	 wd 0.0500	time 0.3802 (0.5243)	loss 0.7404 (1.1150)	grad_norm 2.7229 (nan)	loss_scale 512.0000 (806.9554)	mem 12860MB
[2024-05-29 08:25:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 19 training takes 0:43:48
[2024-05-29 08:25:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.022 (8.022)	Loss 0.3821 (0.3821)	Acc@1 94.922 (94.922)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 08:26:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.164 (0.198)	Loss 0.7163 (0.6124)	Acc@1 83.203 (87.918)	Acc@5 97.656 (98.240)	Mem 12860MB
[2024-05-29 08:26:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.790 Acc@5 98.032
[2024-05-29 08:26:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 08:26:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.79%
[2024-05-29 08:26:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][0/5004]	eta 13:39:53 lr 0.000004	 wd 0.0500	time 9.8308 (9.8308)	loss 1.0848 (1.0848)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:27:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][100/5004]	eta 0:47:44 lr 0.000004	 wd 0.0500	time 0.3998 (0.5841)	loss 0.9192 (1.1055)	grad_norm 4.1757 (4.1130)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:28:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][200/5004]	eta 0:43:58 lr 0.000003	 wd 0.0500	time 0.4607 (0.5492)	loss 1.3784 (1.1044)	grad_norm 3.8568 (4.1745)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:29:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][300/5004]	eta 0:42:12 lr 0.000003	 wd 0.0500	time 0.3941 (0.5384)	loss 1.4162 (1.1014)	grad_norm 3.7004 (4.0589)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:29:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][400/5004]	eta 0:40:40 lr 0.000003	 wd 0.0500	time 0.4514 (0.5300)	loss 1.1794 (1.1083)	grad_norm 3.3380 (4.1084)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:30:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][500/5004]	eta 0:39:29 lr 0.000003	 wd 0.0500	time 0.5202 (0.5260)	loss 1.2233 (1.1064)	grad_norm 2.8209 (4.0524)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:31:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][600/5004]	eta 0:38:29 lr 0.000003	 wd 0.0500	time 0.4135 (0.5245)	loss 0.9052 (1.1089)	grad_norm 4.9425 (3.9986)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:32:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][700/5004]	eta 0:37:32 lr 0.000003	 wd 0.0500	time 0.4065 (0.5235)	loss 1.2495 (1.1105)	grad_norm 3.2713 (4.0771)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:33:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][800/5004]	eta 0:36:48 lr 0.000003	 wd 0.0500	time 0.4231 (0.5253)	loss 0.7402 (1.1101)	grad_norm 4.2738 (4.1347)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:34:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][900/5004]	eta 0:35:47 lr 0.000003	 wd 0.0500	time 0.4379 (0.5232)	loss 0.7743 (1.1077)	grad_norm 3.1602 (4.0686)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:35:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1000/5004]	eta 0:34:51 lr 0.000003	 wd 0.0500	time 0.4263 (0.5223)	loss 1.0932 (1.1087)	grad_norm 5.7025 (4.0918)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:35:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1100/5004]	eta 0:33:53 lr 0.000003	 wd 0.0500	time 0.4047 (0.5209)	loss 0.8830 (1.1078)	grad_norm 3.0263 (4.0924)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:36:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1200/5004]	eta 0:32:56 lr 0.000003	 wd 0.0500	time 0.3991 (0.5195)	loss 0.8561 (1.1057)	grad_norm 4.0864 (4.0721)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:37:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1300/5004]	eta 0:31:58 lr 0.000003	 wd 0.0500	time 0.4740 (0.5179)	loss 1.0412 (1.1065)	grad_norm 2.7369 (4.1758)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:38:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1400/5004]	eta 0:31:04 lr 0.000003	 wd 0.0500	time 0.4520 (0.5173)	loss 1.1384 (1.1070)	grad_norm 5.1472 (4.1874)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 08:39:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1500/5004]	eta 0:30:17 lr 0.000003	 wd 0.0500	time 0.4370 (0.5188)	loss 0.8942 (1.1064)	grad_norm 6.0441 (4.2025)	loss_scale 1024.0000 (514.0466)	mem 12860MB
[2024-05-29 08:40:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1600/5004]	eta 0:29:26 lr 0.000003	 wd 0.0500	time 0.5328 (0.5190)	loss 1.6908 (1.1064)	grad_norm 2.4396 (4.1740)	loss_scale 1024.0000 (545.8988)	mem 12860MB
[2024-05-29 08:41:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1700/5004]	eta 0:28:36 lr 0.000003	 wd 0.0500	time 0.4350 (0.5195)	loss 1.0654 (1.1073)	grad_norm 4.5964 (4.1467)	loss_scale 1024.0000 (574.0059)	mem 12860MB
[2024-05-29 08:42:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1800/5004]	eta 0:27:43 lr 0.000003	 wd 0.0500	time 0.4152 (0.5193)	loss 0.7896 (1.1074)	grad_norm 2.9379 (4.1518)	loss_scale 1024.0000 (598.9917)	mem 12860MB
[2024-05-29 08:42:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][1900/5004]	eta 0:26:52 lr 0.000003	 wd 0.0500	time 0.5048 (0.5196)	loss 1.1467 (1.1079)	grad_norm 3.1422 (4.1176)	loss_scale 1024.0000 (621.3488)	mem 12860MB
[2024-05-29 08:43:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2000/5004]	eta 0:26:00 lr 0.000003	 wd 0.0500	time 0.4188 (0.5193)	loss 1.4471 (1.1088)	grad_norm 3.3646 (4.1096)	loss_scale 1024.0000 (641.4713)	mem 12860MB
[2024-05-29 08:44:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2100/5004]	eta 0:25:07 lr 0.000003	 wd 0.0500	time 0.3793 (0.5193)	loss 1.1027 (1.1095)	grad_norm 3.1822 (4.1434)	loss_scale 1024.0000 (659.6782)	mem 12860MB
[2024-05-29 08:45:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2200/5004]	eta 0:24:14 lr 0.000003	 wd 0.0500	time 0.4170 (0.5187)	loss 0.8963 (1.1108)	grad_norm 3.0903 (4.1272)	loss_scale 1024.0000 (676.2308)	mem 12860MB
[2024-05-29 08:46:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2300/5004]	eta 0:23:21 lr 0.000003	 wd 0.0500	time 0.4341 (0.5182)	loss 1.4001 (1.1109)	grad_norm 2.6818 (4.1105)	loss_scale 1024.0000 (691.3446)	mem 12860MB
[2024-05-29 08:47:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2400/5004]	eta 0:22:29 lr 0.000003	 wd 0.0500	time 0.4441 (0.5181)	loss 0.9628 (1.1092)	grad_norm 3.6401 (4.1034)	loss_scale 1024.0000 (705.1995)	mem 12860MB
[2024-05-29 08:48:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2500/5004]	eta 0:21:37 lr 0.000003	 wd 0.0500	time 0.3755 (0.5181)	loss 1.3257 (1.1069)	grad_norm 3.3494 (4.1208)	loss_scale 1024.0000 (717.9464)	mem 12860MB
[2024-05-29 08:48:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2600/5004]	eta 0:20:45 lr 0.000003	 wd 0.0500	time 0.4302 (0.5179)	loss 1.1299 (1.1078)	grad_norm 3.0949 (4.0946)	loss_scale 1024.0000 (729.7132)	mem 12860MB
[2024-05-29 08:49:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2700/5004]	eta 0:19:53 lr 0.000003	 wd 0.0500	time 0.3768 (0.5181)	loss 0.8175 (1.1073)	grad_norm 3.0113 (4.0812)	loss_scale 1024.0000 (740.6087)	mem 12860MB
[2024-05-29 08:50:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2800/5004]	eta 0:19:02 lr 0.000003	 wd 0.0500	time 0.3951 (0.5182)	loss 1.4789 (1.1089)	grad_norm 3.2925 (4.0769)	loss_scale 1024.0000 (750.7262)	mem 12860MB
[2024-05-29 08:51:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][2900/5004]	eta 0:18:10 lr 0.000003	 wd 0.0500	time 0.4804 (0.5184)	loss 1.1595 (1.1103)	grad_norm 4.3372 (4.0655)	loss_scale 1024.0000 (760.1462)	mem 12860MB
[2024-05-29 08:52:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3000/5004]	eta 0:17:18 lr 0.000003	 wd 0.0500	time 0.4256 (0.5181)	loss 1.4143 (1.1109)	grad_norm 2.9493 (4.0676)	loss_scale 1024.0000 (768.9384)	mem 12860MB
[2024-05-29 08:53:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3100/5004]	eta 0:16:26 lr 0.000003	 wd 0.0500	time 0.4190 (0.5183)	loss 0.6762 (1.1116)	grad_norm 4.4430 (4.0654)	loss_scale 1024.0000 (777.1635)	mem 12860MB
[2024-05-29 08:54:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3200/5004]	eta 0:15:35 lr 0.000003	 wd 0.0500	time 0.4329 (0.5184)	loss 1.3393 (1.1115)	grad_norm 2.6599 (4.0545)	loss_scale 1024.0000 (784.8747)	mem 12860MB
[2024-05-29 08:54:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3300/5004]	eta 0:14:44 lr 0.000003	 wd 0.0500	time 0.4002 (0.5188)	loss 0.7349 (1.1113)	grad_norm 2.7015 (4.0459)	loss_scale 1024.0000 (792.1188)	mem 12860MB
[2024-05-29 08:55:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3400/5004]	eta 0:13:52 lr 0.000003	 wd 0.0500	time 0.4096 (0.5190)	loss 0.6613 (1.1116)	grad_norm 5.4884 (4.0405)	loss_scale 1024.0000 (798.9368)	mem 12860MB
[2024-05-29 08:56:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3500/5004]	eta 0:13:01 lr 0.000003	 wd 0.0500	time 0.4076 (0.5193)	loss 1.5065 (1.1112)	grad_norm 3.2626 (4.0359)	loss_scale 1024.0000 (805.3653)	mem 12860MB
[2024-05-29 08:57:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3600/5004]	eta 0:12:09 lr 0.000003	 wd 0.0500	time 0.4709 (0.5196)	loss 1.3855 (1.1107)	grad_norm 6.6693 (4.0368)	loss_scale 1024.0000 (811.4368)	mem 12860MB
[2024-05-29 08:58:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3700/5004]	eta 0:11:17 lr 0.000003	 wd 0.0500	time 0.4901 (0.5197)	loss 1.1640 (1.1116)	grad_norm 2.8126 (4.0310)	loss_scale 1024.0000 (817.1802)	mem 12860MB
[2024-05-29 08:59:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3800/5004]	eta 0:10:25 lr 0.000003	 wd 0.0500	time 0.4348 (0.5199)	loss 1.2444 (1.1109)	grad_norm 3.2526 (4.0234)	loss_scale 1024.0000 (822.6214)	mem 12860MB
[2024-05-29 09:00:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][3900/5004]	eta 0:09:33 lr 0.000003	 wd 0.0500	time 0.4689 (0.5199)	loss 1.0118 (1.1100)	grad_norm 3.1683 (4.0335)	loss_scale 1024.0000 (827.7836)	mem 12860MB
[2024-05-29 09:01:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4000/5004]	eta 0:08:42 lr 0.000003	 wd 0.0500	time 0.4711 (0.5200)	loss 1.3372 (1.1103)	grad_norm 3.5860 (4.0377)	loss_scale 1024.0000 (832.6878)	mem 12860MB
[2024-05-29 09:01:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4100/5004]	eta 0:07:50 lr 0.000003	 wd 0.0500	time 0.4684 (0.5202)	loss 0.7822 (1.1100)	grad_norm 3.8571 (4.0461)	loss_scale 1024.0000 (837.3528)	mem 12860MB
[2024-05-29 09:02:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4200/5004]	eta 0:06:58 lr 0.000003	 wd 0.0500	time 0.3914 (0.5202)	loss 1.1994 (1.1096)	grad_norm 3.0378 (4.0378)	loss_scale 1024.0000 (841.7958)	mem 12860MB
[2024-05-29 09:03:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4300/5004]	eta 0:06:06 lr 0.000003	 wd 0.0500	time 0.4391 (0.5203)	loss 1.4709 (1.1105)	grad_norm 3.1655 (4.0274)	loss_scale 1024.0000 (846.0321)	mem 12860MB
[2024-05-29 09:04:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4400/5004]	eta 0:05:14 lr 0.000003	 wd 0.0500	time 0.6185 (0.5211)	loss 0.9273 (1.1106)	grad_norm 3.2119 (4.0171)	loss_scale 1024.0000 (850.0759)	mem 12860MB
[2024-05-29 09:05:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4500/5004]	eta 0:04:23 lr 0.000003	 wd 0.0500	time 0.4888 (0.5228)	loss 1.4703 (1.1105)	grad_norm 4.1764 (4.0108)	loss_scale 1024.0000 (853.9400)	mem 12860MB
[2024-05-29 09:06:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4600/5004]	eta 0:03:31 lr 0.000003	 wd 0.0500	time 0.4713 (0.5228)	loss 0.6610 (1.1106)	grad_norm 3.3404 (4.0124)	loss_scale 1024.0000 (857.6362)	mem 12860MB
[2024-05-29 09:07:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4700/5004]	eta 0:02:39 lr 0.000003	 wd 0.0500	time 0.4663 (0.5232)	loss 1.1976 (1.1105)	grad_norm 4.5373 (3.9997)	loss_scale 1024.0000 (861.1751)	mem 12860MB
[2024-05-29 09:08:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4800/5004]	eta 0:01:46 lr 0.000003	 wd 0.0500	time 0.4755 (0.5233)	loss 1.0049 (1.1111)	grad_norm 3.0645 (3.9943)	loss_scale 1024.0000 (864.5665)	mem 12860MB
[2024-05-29 09:09:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][4900/5004]	eta 0:00:54 lr 0.000003	 wd 0.0500	time 0.3999 (0.5235)	loss 0.7784 (1.1113)	grad_norm 4.1454 (3.9948)	loss_scale 1024.0000 (867.8196)	mem 12860MB
[2024-05-29 09:09:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [20/30][5000/5004]	eta 0:00:02 lr 0.000003	 wd 0.0500	time 0.3455 (0.5226)	loss 1.3192 (1.1113)	grad_norm 2.7390 (3.9990)	loss_scale 1024.0000 (870.9426)	mem 12860MB
[2024-05-29 09:10:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 20 training takes 0:43:39
[2024-05-29 09:10:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.065 (8.065)	Loss 0.3860 (0.3860)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 09:10:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.103 (0.219)	Loss 0.7256 (0.6179)	Acc@1 82.422 (87.875)	Acc@5 97.656 (98.244)	Mem 12860MB
[2024-05-29 09:10:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.728 Acc@5 98.016
[2024-05-29 09:10:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 09:10:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.79%
[2024-05-29 09:10:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][0/5004]	eta 11:53:03 lr 0.000003	 wd 0.0500	time 8.5498 (8.5498)	loss 1.2997 (1.2997)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:11:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][100/5004]	eta 0:47:00 lr 0.000003	 wd 0.0500	time 0.3686 (0.5752)	loss 1.2761 (1.1382)	grad_norm 3.6991 (4.3294)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:12:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][200/5004]	eta 0:43:59 lr 0.000003	 wd 0.0500	time 0.3894 (0.5495)	loss 1.2270 (1.1244)	grad_norm 3.0926 (5.1018)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:13:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][300/5004]	eta 0:41:54 lr 0.000003	 wd 0.0500	time 0.4380 (0.5346)	loss 1.1528 (1.1361)	grad_norm 4.0547 (4.8256)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:14:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][400/5004]	eta 0:40:26 lr 0.000003	 wd 0.0500	time 0.4217 (0.5270)	loss 1.4438 (1.1284)	grad_norm 4.0268 (4.5451)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:15:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][500/5004]	eta 0:39:09 lr 0.000003	 wd 0.0500	time 0.4478 (0.5216)	loss 1.1658 (1.1251)	grad_norm 3.4047 (4.6227)	loss_scale 2048.0000 (1044.4391)	mem 12860MB
[2024-05-29 09:15:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][600/5004]	eta 0:38:11 lr 0.000003	 wd 0.0500	time 0.4164 (0.5202)	loss 1.3005 (1.1224)	grad_norm 3.3943 (4.5102)	loss_scale 2048.0000 (1211.4210)	mem 12860MB
[2024-05-29 09:16:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][700/5004]	eta 0:37:05 lr 0.000003	 wd 0.0500	time 0.4195 (0.5171)	loss 1.3615 (1.1196)	grad_norm 2.7720 (4.4408)	loss_scale 2048.0000 (1330.7618)	mem 12860MB
[2024-05-29 09:17:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][800/5004]	eta 0:36:09 lr 0.000003	 wd 0.0500	time 0.3681 (0.5161)	loss 0.7430 (1.1178)	grad_norm 3.1132 (4.3654)	loss_scale 2048.0000 (1420.3046)	mem 12860MB
[2024-05-29 09:18:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][900/5004]	eta 0:35:13 lr 0.000003	 wd 0.0500	time 0.3956 (0.5149)	loss 0.7937 (1.1144)	grad_norm 3.0876 (4.2849)	loss_scale 2048.0000 (1489.9711)	mem 12860MB
[2024-05-29 09:19:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1000/5004]	eta 0:34:15 lr 0.000003	 wd 0.0500	time 0.4119 (0.5135)	loss 1.1609 (1.1179)	grad_norm 3.5867 (4.2776)	loss_scale 2048.0000 (1545.7183)	mem 12860MB
[2024-05-29 09:20:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1100/5004]	eta 0:33:22 lr 0.000003	 wd 0.0500	time 0.4760 (0.5129)	loss 1.0451 (1.1194)	grad_norm 3.2025 (4.2327)	loss_scale 2048.0000 (1591.3388)	mem 12860MB
[2024-05-29 09:21:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1200/5004]	eta 0:32:34 lr 0.000003	 wd 0.0500	time 0.3870 (0.5138)	loss 1.3877 (1.1206)	grad_norm 3.2481 (4.2730)	loss_scale 2048.0000 (1629.3622)	mem 12860MB
[2024-05-29 09:21:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1300/5004]	eta 0:31:39 lr 0.000003	 wd 0.0500	time 0.4750 (0.5129)	loss 1.3173 (1.1224)	grad_norm 3.0159 (4.2372)	loss_scale 2048.0000 (1661.5404)	mem 12860MB
[2024-05-29 09:22:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1400/5004]	eta 0:30:50 lr 0.000003	 wd 0.0500	time 0.4596 (0.5135)	loss 0.8942 (1.1234)	grad_norm 3.0021 (4.2110)	loss_scale 2048.0000 (1689.1249)	mem 12860MB
[2024-05-29 09:23:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1500/5004]	eta 0:29:59 lr 0.000003	 wd 0.0500	time 0.4105 (0.5135)	loss 1.1707 (1.1230)	grad_norm 3.4022 (nan)	loss_scale 1024.0000 (1658.4570)	mem 12860MB
[2024-05-29 09:24:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1600/5004]	eta 0:29:10 lr 0.000003	 wd 0.0500	time 0.4117 (0.5144)	loss 1.0677 (1.1242)	grad_norm 3.6859 (nan)	loss_scale 1024.0000 (1618.8282)	mem 12860MB
[2024-05-29 09:25:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1700/5004]	eta 0:28:17 lr 0.000003	 wd 0.0500	time 0.3677 (0.5137)	loss 1.1235 (1.1222)	grad_norm 4.1045 (nan)	loss_scale 1024.0000 (1583.8589)	mem 12860MB
[2024-05-29 09:26:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1800/5004]	eta 0:27:26 lr 0.000003	 wd 0.0500	time 0.4230 (0.5140)	loss 1.2476 (1.1216)	grad_norm 6.3745 (nan)	loss_scale 1024.0000 (1552.7729)	mem 12860MB
[2024-05-29 09:27:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][1900/5004]	eta 0:26:34 lr 0.000003	 wd 0.0500	time 0.3874 (0.5136)	loss 1.2895 (1.1201)	grad_norm 3.2219 (nan)	loss_scale 1024.0000 (1524.9574)	mem 12860MB
[2024-05-29 09:27:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2000/5004]	eta 0:25:42 lr 0.000003	 wd 0.0500	time 0.4424 (0.5135)	loss 1.2412 (1.1219)	grad_norm 2.8885 (nan)	loss_scale 1024.0000 (1499.9220)	mem 12860MB
[2024-05-29 09:28:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2100/5004]	eta 0:24:51 lr 0.000003	 wd 0.0500	time 0.4284 (0.5137)	loss 1.4437 (1.1226)	grad_norm 7.1256 (nan)	loss_scale 1024.0000 (1477.2699)	mem 12860MB
[2024-05-29 09:29:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2200/5004]	eta 0:24:00 lr 0.000003	 wd 0.0500	time 0.4113 (0.5139)	loss 0.9267 (1.1221)	grad_norm 7.8889 (nan)	loss_scale 1024.0000 (1456.6761)	mem 12860MB
[2024-05-29 09:30:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2300/5004]	eta 0:23:13 lr 0.000003	 wd 0.0500	time 0.5220 (0.5153)	loss 0.7533 (1.1220)	grad_norm 2.4316 (nan)	loss_scale 1024.0000 (1437.8722)	mem 12860MB
[2024-05-29 09:31:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2400/5004]	eta 0:22:23 lr 0.000003	 wd 0.0500	time 0.4170 (0.5161)	loss 1.4117 (1.1236)	grad_norm 3.8792 (nan)	loss_scale 1024.0000 (1420.6347)	mem 12860MB
[2024-05-29 09:32:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2500/5004]	eta 0:21:32 lr 0.000003	 wd 0.0500	time 0.4228 (0.5163)	loss 1.0854 (1.1215)	grad_norm 2.8288 (nan)	loss_scale 1024.0000 (1404.7757)	mem 12860MB
[2024-05-29 09:33:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2600/5004]	eta 0:20:40 lr 0.000003	 wd 0.0500	time 0.3934 (0.5160)	loss 1.3231 (1.1222)	grad_norm 3.4675 (nan)	loss_scale 1024.0000 (1390.1361)	mem 12860MB
[2024-05-29 09:33:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2700/5004]	eta 0:19:48 lr 0.000003	 wd 0.0500	time 0.4014 (0.5159)	loss 1.2061 (1.1214)	grad_norm 3.4967 (nan)	loss_scale 1024.0000 (1376.5805)	mem 12860MB
[2024-05-29 09:34:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2800/5004]	eta 0:18:56 lr 0.000003	 wd 0.0500	time 0.3682 (0.5156)	loss 1.1486 (1.1220)	grad_norm 3.3912 (nan)	loss_scale 1024.0000 (1363.9929)	mem 12860MB
[2024-05-29 09:35:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][2900/5004]	eta 0:18:04 lr 0.000003	 wd 0.0500	time 0.4328 (0.5154)	loss 1.1145 (1.1214)	grad_norm 2.5960 (nan)	loss_scale 1024.0000 (1352.2730)	mem 12860MB
[2024-05-29 09:36:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3000/5004]	eta 0:17:12 lr 0.000003	 wd 0.0500	time 0.4681 (0.5152)	loss 1.4121 (1.1199)	grad_norm 3.0618 (nan)	loss_scale 1024.0000 (1341.3342)	mem 12860MB
[2024-05-29 09:37:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3100/5004]	eta 0:16:20 lr 0.000003	 wd 0.0500	time 0.3571 (0.5150)	loss 0.9692 (1.1227)	grad_norm 4.6821 (nan)	loss_scale 1024.0000 (1331.1009)	mem 12860MB
[2024-05-29 09:38:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3200/5004]	eta 0:15:28 lr 0.000003	 wd 0.0500	time 0.4269 (0.5148)	loss 1.3039 (1.1226)	grad_norm 4.1766 (nan)	loss_scale 1024.0000 (1321.5070)	mem 12860MB
[2024-05-29 09:39:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3300/5004]	eta 0:14:36 lr 0.000003	 wd 0.0500	time 0.4120 (0.5144)	loss 1.5088 (1.1235)	grad_norm 2.8509 (nan)	loss_scale 1024.0000 (1312.4944)	mem 12860MB
[2024-05-29 09:39:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3400/5004]	eta 0:13:45 lr 0.000003	 wd 0.0500	time 0.4192 (0.5149)	loss 1.1868 (1.1235)	grad_norm 7.2239 (nan)	loss_scale 1024.0000 (1304.0118)	mem 12860MB
[2024-05-29 09:40:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3500/5004]	eta 0:12:55 lr 0.000003	 wd 0.0500	time 0.5117 (0.5155)	loss 1.2268 (1.1220)	grad_norm 3.4541 (nan)	loss_scale 1024.0000 (1296.0137)	mem 12860MB
[2024-05-29 09:41:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3600/5004]	eta 0:12:05 lr 0.000003	 wd 0.0500	time 0.3976 (0.5166)	loss 1.0356 (1.1207)	grad_norm 3.0952 (nan)	loss_scale 1024.0000 (1288.4599)	mem 12860MB
[2024-05-29 09:42:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3700/5004]	eta 0:11:15 lr 0.000003	 wd 0.0500	time 0.5132 (0.5176)	loss 0.6521 (1.1207)	grad_norm 3.1966 (nan)	loss_scale 1024.0000 (1281.3142)	mem 12860MB
[2024-05-29 09:43:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3800/5004]	eta 0:10:24 lr 0.000003	 wd 0.0500	time 0.4351 (0.5183)	loss 1.3102 (1.1204)	grad_norm 3.4440 (nan)	loss_scale 1024.0000 (1274.5446)	mem 12860MB
[2024-05-29 09:44:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][3900/5004]	eta 0:09:32 lr 0.000003	 wd 0.0500	time 0.3619 (0.5183)	loss 0.8700 (1.1216)	grad_norm 3.4234 (nan)	loss_scale 1024.0000 (1268.1220)	mem 12860MB
[2024-05-29 09:45:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4000/5004]	eta 0:08:40 lr 0.000003	 wd 0.0500	time 0.4043 (0.5186)	loss 0.7380 (1.1209)	grad_norm 2.6512 (nan)	loss_scale 1024.0000 (1262.0205)	mem 12860MB
[2024-05-29 09:46:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4100/5004]	eta 0:07:49 lr 0.000002	 wd 0.0500	time 0.4025 (0.5191)	loss 1.2311 (1.1196)	grad_norm 3.8744 (nan)	loss_scale 1024.0000 (1256.2165)	mem 12860MB
[2024-05-29 09:47:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4200/5004]	eta 0:06:57 lr 0.000002	 wd 0.0500	time 0.4592 (0.5195)	loss 1.0790 (1.1195)	grad_norm 4.7260 (nan)	loss_scale 1024.0000 (1250.6889)	mem 12860MB
[2024-05-29 09:48:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4300/5004]	eta 0:06:06 lr 0.000002	 wd 0.0500	time 0.3750 (0.5203)	loss 1.0522 (1.1194)	grad_norm 2.9225 (nan)	loss_scale 1024.0000 (1245.4183)	mem 12860MB
[2024-05-29 09:48:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4400/5004]	eta 0:05:14 lr 0.000002	 wd 0.0500	time 0.4467 (0.5210)	loss 1.3945 (1.1182)	grad_norm 2.6930 (nan)	loss_scale 1024.0000 (1240.3872)	mem 12860MB
[2024-05-29 09:49:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4500/5004]	eta 0:04:23 lr 0.000002	 wd 0.0500	time 0.5576 (0.5219)	loss 1.1336 (1.1177)	grad_norm 3.1804 (nan)	loss_scale 1024.0000 (1235.5796)	mem 12860MB
[2024-05-29 09:50:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4600/5004]	eta 0:03:31 lr 0.000002	 wd 0.0500	time 0.4763 (0.5228)	loss 1.6136 (1.1182)	grad_norm 5.2536 (nan)	loss_scale 1024.0000 (1230.9811)	mem 12860MB
[2024-05-29 09:51:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4700/5004]	eta 0:02:39 lr 0.000002	 wd 0.0500	time 0.3983 (0.5231)	loss 0.8493 (1.1180)	grad_norm 2.7123 (nan)	loss_scale 1024.0000 (1226.5782)	mem 12860MB
[2024-05-29 09:52:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4800/5004]	eta 0:01:46 lr 0.000002	 wd 0.0500	time 0.4080 (0.5231)	loss 0.8672 (1.1174)	grad_norm 4.2469 (nan)	loss_scale 1024.0000 (1222.3587)	mem 12860MB
[2024-05-29 09:53:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][4900/5004]	eta 0:00:54 lr 0.000002	 wd 0.0500	time 0.4829 (0.5232)	loss 1.1139 (1.1172)	grad_norm 2.9838 (nan)	loss_scale 1024.0000 (1218.3114)	mem 12860MB
[2024-05-29 09:54:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [21/30][5000/5004]	eta 0:00:02 lr 0.000002	 wd 0.0500	time 0.4494 (0.5224)	loss 1.1975 (1.1171)	grad_norm 2.9441 (nan)	loss_scale 1024.0000 (1214.4259)	mem 12860MB
[2024-05-29 09:54:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 21 training takes 0:43:39
[2024-05-29 09:54:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.487 (7.487)	Loss 0.3875 (0.3875)	Acc@1 94.531 (94.531)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 09:54:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.096 (0.194)	Loss 0.7363 (0.6232)	Acc@1 82.812 (87.922)	Acc@5 97.656 (98.248)	Mem 12860MB
[2024-05-29 09:54:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.784 Acc@5 98.048
[2024-05-29 09:54:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 09:54:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.79%
[2024-05-29 09:55:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][0/5004]	eta 10:42:48 lr 0.000002	 wd 0.0500	time 7.7075 (7.7075)	loss 1.4832 (1.4832)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:55:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][100/5004]	eta 0:46:26 lr 0.000002	 wd 0.0500	time 0.4278 (0.5683)	loss 1.1502 (1.1278)	grad_norm 4.1307 (4.1295)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:56:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][200/5004]	eta 0:43:07 lr 0.000002	 wd 0.0500	time 0.4261 (0.5386)	loss 1.1659 (1.1265)	grad_norm 3.5228 (4.0555)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:57:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][300/5004]	eta 0:41:13 lr 0.000002	 wd 0.0500	time 0.4151 (0.5258)	loss 1.4154 (1.1298)	grad_norm 3.1416 (3.9749)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:58:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][400/5004]	eta 0:40:00 lr 0.000002	 wd 0.0500	time 0.4204 (0.5213)	loss 1.0993 (1.1266)	grad_norm 3.8803 (4.0824)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 09:59:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][500/5004]	eta 0:38:55 lr 0.000002	 wd 0.0500	time 0.3800 (0.5185)	loss 1.3376 (1.1274)	grad_norm 4.0338 (4.1843)	loss_scale 2048.0000 (1195.6886)	mem 12860MB
[2024-05-29 10:00:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][600/5004]	eta 0:38:08 lr 0.000002	 wd 0.0500	time 0.4334 (0.5197)	loss 0.8832 (1.1265)	grad_norm 4.1090 (4.1426)	loss_scale 2048.0000 (1337.5042)	mem 12860MB
[2024-05-29 10:01:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][700/5004]	eta 0:37:07 lr 0.000002	 wd 0.0500	time 0.4044 (0.5175)	loss 1.5783 (1.1233)	grad_norm 6.0591 (4.1411)	loss_scale 2048.0000 (1438.8588)	mem 12860MB
[2024-05-29 10:01:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][800/5004]	eta 0:36:10 lr 0.000002	 wd 0.0500	time 0.4999 (0.5164)	loss 1.3031 (1.1219)	grad_norm 3.8636 (4.1350)	loss_scale 2048.0000 (1514.9064)	mem 12860MB
[2024-05-29 10:02:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][900/5004]	eta 0:35:15 lr 0.000002	 wd 0.0500	time 0.4321 (0.5154)	loss 0.9129 (1.1241)	grad_norm 2.8466 (4.1008)	loss_scale 2048.0000 (1574.0733)	mem 12860MB
[2024-05-29 10:03:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1000/5004]	eta 0:34:22 lr 0.000002	 wd 0.0500	time 0.4697 (0.5151)	loss 1.3592 (1.1261)	grad_norm 6.6902 (4.1216)	loss_scale 2048.0000 (1621.4186)	mem 12860MB
[2024-05-29 10:04:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1100/5004]	eta 0:33:28 lr 0.000002	 wd 0.0500	time 0.4216 (0.5144)	loss 1.4223 (1.1249)	grad_norm 2.9904 (4.0924)	loss_scale 2048.0000 (1660.1635)	mem 12860MB
[2024-05-29 10:05:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1200/5004]	eta 0:32:33 lr 0.000002	 wd 0.0500	time 0.4186 (0.5135)	loss 1.1072 (1.1190)	grad_norm 3.6965 (4.0606)	loss_scale 2048.0000 (1692.4563)	mem 12860MB
[2024-05-29 10:06:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1300/5004]	eta 0:31:41 lr 0.000002	 wd 0.0500	time 0.4381 (0.5134)	loss 1.1476 (1.1174)	grad_norm 5.1389 (4.0504)	loss_scale 2048.0000 (1719.7848)	mem 12860MB
[2024-05-29 10:06:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1400/5004]	eta 0:30:51 lr 0.000002	 wd 0.0500	time 0.3828 (0.5138)	loss 0.9732 (1.1176)	grad_norm 2.6223 (4.0181)	loss_scale 2048.0000 (1743.2120)	mem 12860MB
[2024-05-29 10:07:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1500/5004]	eta 0:29:57 lr 0.000002	 wd 0.0500	time 0.4597 (0.5129)	loss 0.7453 (1.1192)	grad_norm 3.3127 (4.0126)	loss_scale 2048.0000 (1763.5177)	mem 12860MB
[2024-05-29 10:08:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1600/5004]	eta 0:29:03 lr 0.000002	 wd 0.0500	time 0.3893 (0.5123)	loss 1.0490 (1.1199)	grad_norm 2.6419 (3.9966)	loss_scale 2048.0000 (1781.2867)	mem 12860MB
[2024-05-29 10:09:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1700/5004]	eta 0:28:12 lr 0.000002	 wd 0.0500	time 0.4316 (0.5122)	loss 1.3784 (1.1199)	grad_norm 13.9729 (4.0776)	loss_scale 2048.0000 (1796.9665)	mem 12860MB
[2024-05-29 10:10:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1800/5004]	eta 0:27:20 lr 0.000002	 wd 0.0500	time 0.4036 (0.5119)	loss 1.3466 (1.1195)	grad_norm 2.8022 (4.0786)	loss_scale 2048.0000 (1810.9051)	mem 12860MB
[2024-05-29 10:11:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][1900/5004]	eta 0:26:28 lr 0.000002	 wd 0.0500	time 0.4374 (0.5119)	loss 1.1826 (1.1183)	grad_norm 2.8284 (4.1435)	loss_scale 2048.0000 (1823.3772)	mem 12860MB
[2024-05-29 10:12:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2000/5004]	eta 0:25:38 lr 0.000002	 wd 0.0500	time 0.4319 (0.5121)	loss 0.9057 (1.1194)	grad_norm 3.0162 (4.1092)	loss_scale 2048.0000 (1834.6027)	mem 12860MB
[2024-05-29 10:12:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2100/5004]	eta 0:24:47 lr 0.000002	 wd 0.0500	time 0.4309 (0.5123)	loss 0.6380 (1.1182)	grad_norm 4.2286 (4.0866)	loss_scale 2048.0000 (1844.7596)	mem 12860MB
[2024-05-29 10:13:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2200/5004]	eta 0:23:54 lr 0.000002	 wd 0.0500	time 0.4193 (0.5118)	loss 1.1082 (1.1181)	grad_norm 4.2858 (4.1307)	loss_scale 2048.0000 (1853.9936)	mem 12860MB
[2024-05-29 10:14:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2300/5004]	eta 0:23:04 lr 0.000002	 wd 0.0500	time 0.4117 (0.5120)	loss 1.3274 (1.1166)	grad_norm 4.1276 (4.1128)	loss_scale 2048.0000 (1862.4250)	mem 12860MB
[2024-05-29 10:15:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2400/5004]	eta 0:22:16 lr 0.000002	 wd 0.0500	time 0.4845 (0.5134)	loss 1.3849 (1.1154)	grad_norm 3.3664 (inf)	loss_scale 1024.0000 (1859.0654)	mem 12860MB
[2024-05-29 10:16:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2500/5004]	eta 0:21:25 lr 0.000002	 wd 0.0500	time 0.4206 (0.5135)	loss 1.1905 (1.1155)	grad_norm 7.0044 (inf)	loss_scale 1024.0000 (1825.6761)	mem 12860MB
[2024-05-29 10:17:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2600/5004]	eta 0:20:34 lr 0.000002	 wd 0.0500	time 0.4302 (0.5134)	loss 0.9868 (1.1157)	grad_norm 2.8649 (nan)	loss_scale 512.0000 (1775.9569)	mem 12860MB
[2024-05-29 10:18:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2700/5004]	eta 0:19:42 lr 0.000002	 wd 0.0500	time 0.3939 (0.5132)	loss 1.1516 (1.1162)	grad_norm 2.9178 (nan)	loss_scale 512.0000 (1729.1611)	mem 12860MB
[2024-05-29 10:18:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2800/5004]	eta 0:18:51 lr 0.000002	 wd 0.0500	time 0.3727 (0.5132)	loss 0.9055 (1.1160)	grad_norm 3.5184 (nan)	loss_scale 512.0000 (1685.7065)	mem 12860MB
[2024-05-29 10:19:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][2900/5004]	eta 0:17:59 lr 0.000002	 wd 0.0500	time 0.4696 (0.5130)	loss 1.3092 (1.1155)	grad_norm 4.1098 (nan)	loss_scale 512.0000 (1645.2478)	mem 12860MB
[2024-05-29 10:20:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3000/5004]	eta 0:17:08 lr 0.000002	 wd 0.0500	time 0.4092 (0.5133)	loss 0.9106 (1.1151)	grad_norm 4.4339 (nan)	loss_scale 512.0000 (1607.4855)	mem 12860MB
[2024-05-29 10:21:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3100/5004]	eta 0:16:16 lr 0.000002	 wd 0.0500	time 0.4471 (0.5130)	loss 1.4692 (1.1153)	grad_norm 2.8622 (nan)	loss_scale 512.0000 (1572.1587)	mem 12860MB
[2024-05-29 10:22:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3200/5004]	eta 0:15:25 lr 0.000002	 wd 0.0500	time 0.5097 (0.5131)	loss 1.4125 (1.1150)	grad_norm 3.0428 (nan)	loss_scale 512.0000 (1539.0391)	mem 12860MB
[2024-05-29 10:23:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3300/5004]	eta 0:14:34 lr 0.000002	 wd 0.0500	time 0.4213 (0.5130)	loss 1.2092 (1.1159)	grad_norm 9.2261 (nan)	loss_scale 512.0000 (1507.9261)	mem 12860MB
[2024-05-29 10:24:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3400/5004]	eta 0:13:42 lr 0.000002	 wd 0.0500	time 0.4052 (0.5128)	loss 0.9813 (1.1163)	grad_norm 3.0887 (nan)	loss_scale 512.0000 (1478.6428)	mem 12860MB
[2024-05-29 10:24:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3500/5004]	eta 0:12:51 lr 0.000002	 wd 0.0500	time 0.4804 (0.5127)	loss 1.5090 (1.1158)	grad_norm 2.8169 (nan)	loss_scale 512.0000 (1451.0323)	mem 12860MB
[2024-05-29 10:25:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3600/5004]	eta 0:11:59 lr 0.000002	 wd 0.0500	time 0.4427 (0.5126)	loss 0.7046 (1.1154)	grad_norm 4.1537 (nan)	loss_scale 512.0000 (1424.9553)	mem 12860MB
[2024-05-29 10:26:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3700/5004]	eta 0:11:08 lr 0.000002	 wd 0.0500	time 0.4301 (0.5127)	loss 1.3553 (1.1140)	grad_norm 2.9780 (nan)	loss_scale 512.0000 (1400.2875)	mem 12860MB
[2024-05-29 10:27:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3800/5004]	eta 0:10:17 lr 0.000002	 wd 0.0500	time 0.4473 (0.5126)	loss 1.3835 (1.1145)	grad_norm 2.8850 (nan)	loss_scale 512.0000 (1376.9177)	mem 12860MB
[2024-05-29 10:28:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][3900/5004]	eta 0:09:26 lr 0.000002	 wd 0.0500	time 0.4328 (0.5128)	loss 1.0770 (1.1139)	grad_norm 3.8616 (nan)	loss_scale 512.0000 (1354.7460)	mem 12860MB
[2024-05-29 10:29:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4000/5004]	eta 0:08:34 lr 0.000002	 wd 0.0500	time 0.4052 (0.5128)	loss 1.2952 (1.1148)	grad_norm 3.0832 (nan)	loss_scale 512.0000 (1333.6826)	mem 12860MB
[2024-05-29 10:30:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4100/5004]	eta 0:07:43 lr 0.000002	 wd 0.0500	time 0.4351 (0.5130)	loss 0.7938 (1.1158)	grad_norm 7.3650 (nan)	loss_scale 512.0000 (1313.6464)	mem 12860MB
[2024-05-29 10:30:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4200/5004]	eta 0:06:52 lr 0.000002	 wd 0.0500	time 0.3880 (0.5133)	loss 0.8680 (1.1152)	grad_norm 5.7031 (nan)	loss_scale 512.0000 (1294.5642)	mem 12860MB
[2024-05-29 10:31:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4300/5004]	eta 0:06:01 lr 0.000002	 wd 0.0500	time 0.4809 (0.5133)	loss 1.5969 (1.1149)	grad_norm 3.8533 (nan)	loss_scale 512.0000 (1276.3692)	mem 12860MB
[2024-05-29 10:32:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4400/5004]	eta 0:05:10 lr 0.000002	 wd 0.0500	time 0.4909 (0.5137)	loss 0.9484 (1.1149)	grad_norm 2.9593 (nan)	loss_scale 512.0000 (1259.0011)	mem 12860MB
[2024-05-29 10:33:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4500/5004]	eta 0:04:19 lr 0.000002	 wd 0.0500	time 0.4442 (0.5142)	loss 1.1610 (1.1147)	grad_norm 2.9654 (nan)	loss_scale 512.0000 (1242.4048)	mem 12860MB
[2024-05-29 10:34:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4600/5004]	eta 0:03:27 lr 0.000002	 wd 0.0500	time 0.3589 (0.5146)	loss 1.1135 (1.1146)	grad_norm 4.1110 (nan)	loss_scale 512.0000 (1226.5299)	mem 12860MB
[2024-05-29 10:35:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4700/5004]	eta 0:02:36 lr 0.000002	 wd 0.0500	time 0.4358 (0.5155)	loss 1.4243 (1.1151)	grad_norm 3.7790 (nan)	loss_scale 512.0000 (1211.3304)	mem 12860MB
[2024-05-29 10:36:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4800/5004]	eta 0:01:45 lr 0.000002	 wd 0.0500	time 0.4607 (0.5155)	loss 1.1552 (1.1152)	grad_norm 3.0716 (nan)	loss_scale 512.0000 (1196.7640)	mem 12860MB
[2024-05-29 10:37:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][4900/5004]	eta 0:00:53 lr 0.000002	 wd 0.0500	time 0.4632 (0.5159)	loss 1.2718 (1.1151)	grad_norm 2.8902 (nan)	loss_scale 512.0000 (1182.7921)	mem 12860MB
[2024-05-29 10:37:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [22/30][5000/5004]	eta 0:00:02 lr 0.000002	 wd 0.0500	time 0.4419 (0.5155)	loss 0.7981 (1.1145)	grad_norm 4.1521 (nan)	loss_scale 512.0000 (1169.3789)	mem 12860MB
[2024-05-29 10:38:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 22 training takes 0:43:06
[2024-05-29 10:38:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.742 (7.742)	Loss 0.3784 (0.3784)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 10:38:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.190 (0.209)	Loss 0.7148 (0.6083)	Acc@1 83.984 (87.925)	Acc@5 97.656 (98.267)	Mem 12860MB
[2024-05-29 10:38:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.788 Acc@5 98.062
[2024-05-29 10:38:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 10:38:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.79%
[2024-05-29 10:38:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][0/5004]	eta 11:22:17 lr 0.000002	 wd 0.0500	time 8.1809 (8.1809)	loss 1.1323 (1.1323)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:39:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][100/5004]	eta 0:47:58 lr 0.000002	 wd 0.0500	time 0.5075 (0.5870)	loss 0.7147 (1.0919)	grad_norm 3.8927 (3.8452)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:40:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][200/5004]	eta 0:43:45 lr 0.000002	 wd 0.0500	time 0.4360 (0.5466)	loss 1.2345 (1.0957)	grad_norm 2.7979 (4.0777)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:41:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][300/5004]	eta 0:42:20 lr 0.000002	 wd 0.0500	time 0.4006 (0.5401)	loss 1.2632 (1.1050)	grad_norm 4.0674 (3.9540)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:42:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][400/5004]	eta 0:41:01 lr 0.000002	 wd 0.0500	time 0.4338 (0.5345)	loss 1.2636 (1.1072)	grad_norm 6.4619 (3.9295)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:43:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][500/5004]	eta 0:40:02 lr 0.000002	 wd 0.0500	time 0.3920 (0.5335)	loss 0.9214 (1.1106)	grad_norm 2.7901 (3.8943)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:43:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][600/5004]	eta 0:38:45 lr 0.000002	 wd 0.0500	time 0.3908 (0.5280)	loss 0.8221 (1.1091)	grad_norm 2.6674 (3.8749)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:44:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][700/5004]	eta 0:37:41 lr 0.000002	 wd 0.0500	time 0.4478 (0.5255)	loss 1.1371 (1.1108)	grad_norm 3.7167 (3.9070)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:45:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][800/5004]	eta 0:36:55 lr 0.000002	 wd 0.0500	time 0.4634 (0.5269)	loss 0.8221 (1.1170)	grad_norm 3.8623 (3.9028)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:46:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][900/5004]	eta 0:35:58 lr 0.000002	 wd 0.0500	time 0.4389 (0.5259)	loss 0.8150 (1.1148)	grad_norm 3.1253 (3.9244)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:47:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1000/5004]	eta 0:35:00 lr 0.000002	 wd 0.0500	time 0.4964 (0.5245)	loss 1.3469 (1.1181)	grad_norm 3.7148 (3.9522)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:48:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1100/5004]	eta 0:34:00 lr 0.000002	 wd 0.0500	time 0.3608 (0.5227)	loss 0.6858 (1.1168)	grad_norm 2.5357 (3.9409)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:49:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1200/5004]	eta 0:33:13 lr 0.000002	 wd 0.0500	time 0.3805 (0.5239)	loss 1.4339 (1.1162)	grad_norm 3.5488 (3.9440)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:50:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1300/5004]	eta 0:32:17 lr 0.000002	 wd 0.0500	time 0.4357 (0.5231)	loss 1.1006 (1.1180)	grad_norm 2.9994 (3.9199)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:50:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1400/5004]	eta 0:31:24 lr 0.000002	 wd 0.0500	time 0.5128 (0.5230)	loss 0.7408 (1.1174)	grad_norm 3.1866 (3.9156)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:51:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1500/5004]	eta 0:30:30 lr 0.000002	 wd 0.0500	time 0.3901 (0.5224)	loss 1.4207 (1.1192)	grad_norm 3.1160 (3.9036)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 10:52:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1600/5004]	eta 0:29:34 lr 0.000002	 wd 0.0500	time 0.4195 (0.5213)	loss 1.0820 (1.1189)	grad_norm 4.0348 (3.8892)	loss_scale 1024.0000 (543.9800)	mem 12860MB
[2024-05-29 10:53:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1700/5004]	eta 0:28:40 lr 0.000002	 wd 0.0500	time 0.3868 (0.5206)	loss 0.8312 (1.1166)	grad_norm 3.0046 (3.8832)	loss_scale 1024.0000 (572.1999)	mem 12860MB
[2024-05-29 10:54:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1800/5004]	eta 0:27:49 lr 0.000002	 wd 0.0500	time 0.3999 (0.5209)	loss 0.8610 (1.1177)	grad_norm 3.4406 (3.9001)	loss_scale 1024.0000 (597.2860)	mem 12860MB
[2024-05-29 10:55:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][1900/5004]	eta 0:26:54 lr 0.000002	 wd 0.0500	time 0.4394 (0.5202)	loss 1.4110 (1.1197)	grad_norm 4.5810 (3.9087)	loss_scale 1024.0000 (619.7328)	mem 12860MB
[2024-05-29 10:56:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2000/5004]	eta 0:25:59 lr 0.000002	 wd 0.0500	time 0.3942 (0.5192)	loss 1.3185 (1.1189)	grad_norm 3.1406 (3.8963)	loss_scale 1024.0000 (639.9360)	mem 12860MB
[2024-05-29 10:56:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2100/5004]	eta 0:25:07 lr 0.000002	 wd 0.0500	time 0.4765 (0.5191)	loss 1.3932 (1.1197)	grad_norm 2.8945 (3.9858)	loss_scale 1024.0000 (658.2161)	mem 12860MB
[2024-05-29 10:57:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2200/5004]	eta 0:24:13 lr 0.000002	 wd 0.0500	time 0.3902 (0.5185)	loss 0.9524 (1.1203)	grad_norm 3.3522 (3.9755)	loss_scale 1024.0000 (674.8351)	mem 12860MB
[2024-05-29 10:58:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2300/5004]	eta 0:23:22 lr 0.000002	 wd 0.0500	time 0.4630 (0.5185)	loss 1.3925 (1.1208)	grad_norm 3.5648 (3.9900)	loss_scale 1024.0000 (690.0096)	mem 12860MB
[2024-05-29 10:59:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2400/5004]	eta 0:22:29 lr 0.000002	 wd 0.0500	time 0.4397 (0.5183)	loss 0.7326 (1.1197)	grad_norm 2.4142 (3.9792)	loss_scale 1024.0000 (703.9200)	mem 12860MB
[2024-05-29 11:00:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2500/5004]	eta 0:21:37 lr 0.000002	 wd 0.0500	time 0.4852 (0.5183)	loss 1.5264 (1.1202)	grad_norm 4.0440 (4.0135)	loss_scale 1024.0000 (716.7181)	mem 12860MB
[2024-05-29 11:01:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2600/5004]	eta 0:20:47 lr 0.000002	 wd 0.0500	time 0.4451 (0.5191)	loss 0.7369 (1.1196)	grad_norm 2.9932 (3.9992)	loss_scale 1024.0000 (728.5321)	mem 12860MB
[2024-05-29 11:02:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2700/5004]	eta 0:19:56 lr 0.000002	 wd 0.0500	time 0.4265 (0.5192)	loss 1.1049 (1.1202)	grad_norm 5.0995 (4.1097)	loss_scale 1024.0000 (739.4713)	mem 12860MB
[2024-05-29 11:02:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2800/5004]	eta 0:19:03 lr 0.000002	 wd 0.0500	time 0.4894 (0.5187)	loss 0.8725 (1.1218)	grad_norm 4.4769 (4.1443)	loss_scale 1024.0000 (749.6294)	mem 12860MB
[2024-05-29 11:03:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][2900/5004]	eta 0:18:10 lr 0.000002	 wd 0.0500	time 0.3643 (0.5185)	loss 1.2576 (1.1208)	grad_norm 3.4226 (4.1355)	loss_scale 1024.0000 (759.0872)	mem 12860MB
[2024-05-29 11:04:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3000/5004]	eta 0:17:20 lr 0.000002	 wd 0.0500	time 0.4378 (0.5190)	loss 1.2484 (1.1206)	grad_norm 4.2308 (4.1290)	loss_scale 1024.0000 (767.9147)	mem 12860MB
[2024-05-29 11:05:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3100/5004]	eta 0:16:30 lr 0.000002	 wd 0.0500	time 0.4901 (0.5201)	loss 1.2581 (1.1191)	grad_norm 3.2706 (4.1282)	loss_scale 1024.0000 (776.1728)	mem 12860MB
[2024-05-29 11:06:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3200/5004]	eta 0:15:38 lr 0.000002	 wd 0.0500	time 0.4878 (0.5204)	loss 1.2016 (1.1193)	grad_norm 3.0183 (4.1101)	loss_scale 1024.0000 (783.9150)	mem 12860MB
[2024-05-29 11:07:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3300/5004]	eta 0:14:48 lr 0.000002	 wd 0.0500	time 0.4921 (0.5212)	loss 1.4067 (1.1175)	grad_norm 3.4403 (4.1125)	loss_scale 1024.0000 (791.1881)	mem 12860MB
[2024-05-29 11:08:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3400/5004]	eta 0:13:57 lr 0.000002	 wd 0.0500	time 0.4731 (0.5220)	loss 1.2551 (1.1166)	grad_norm 3.8865 (4.1029)	loss_scale 1024.0000 (798.0335)	mem 12860MB
[2024-05-29 11:09:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3500/5004]	eta 0:13:06 lr 0.000002	 wd 0.0500	time 0.4298 (0.5228)	loss 1.2904 (1.1169)	grad_norm 3.2132 (4.0891)	loss_scale 1024.0000 (804.4879)	mem 12860MB
[2024-05-29 11:10:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3600/5004]	eta 0:12:14 lr 0.000002	 wd 0.0500	time 0.4315 (0.5231)	loss 0.8867 (1.1176)	grad_norm 3.1179 (4.0933)	loss_scale 1024.0000 (810.5837)	mem 12860MB
[2024-05-29 11:11:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3700/5004]	eta 0:11:26 lr 0.000002	 wd 0.0500	time 0.5854 (0.5261)	loss 1.3584 (1.1175)	grad_norm 3.2771 (4.0834)	loss_scale 1024.0000 (816.3502)	mem 12860MB
[2024-05-29 11:12:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3800/5004]	eta 0:10:34 lr 0.000002	 wd 0.0500	time 0.4711 (0.5273)	loss 1.3559 (1.1169)	grad_norm 2.6347 (4.0727)	loss_scale 1024.0000 (821.8132)	mem 12860MB
[2024-05-29 11:12:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][3900/5004]	eta 0:09:42 lr 0.000002	 wd 0.0500	time 0.4226 (0.5274)	loss 0.7294 (1.1163)	grad_norm 3.1607 (4.0735)	loss_scale 1024.0000 (826.9962)	mem 12860MB
[2024-05-29 11:13:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4000/5004]	eta 0:08:49 lr 0.000002	 wd 0.0500	time 0.4260 (0.5274)	loss 1.3466 (1.1158)	grad_norm 5.8082 (4.0692)	loss_scale 1024.0000 (831.9200)	mem 12860MB
[2024-05-29 11:14:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4100/5004]	eta 0:07:57 lr 0.000002	 wd 0.0500	time 0.4145 (0.5281)	loss 0.8052 (1.1148)	grad_norm 4.4221 (4.0646)	loss_scale 1024.0000 (836.6038)	mem 12860MB
[2024-05-29 11:15:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4200/5004]	eta 0:07:05 lr 0.000002	 wd 0.0500	time 0.3909 (0.5287)	loss 0.7941 (1.1142)	grad_norm 4.8438 (4.0533)	loss_scale 1024.0000 (841.0645)	mem 12860MB
[2024-05-29 11:16:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4300/5004]	eta 0:06:12 lr 0.000002	 wd 0.0500	time 0.3790 (0.5287)	loss 0.7786 (1.1142)	grad_norm 3.2518 (4.0695)	loss_scale 1024.0000 (845.3178)	mem 12860MB
[2024-05-29 11:17:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4400/5004]	eta 0:05:19 lr 0.000001	 wd 0.0500	time 0.4683 (0.5293)	loss 1.5136 (1.1145)	grad_norm 3.1396 (4.0594)	loss_scale 1024.0000 (849.3779)	mem 12860MB
[2024-05-29 11:18:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4500/5004]	eta 0:04:26 lr 0.000001	 wd 0.0500	time 0.4395 (0.5292)	loss 1.3601 (1.1148)	grad_norm 5.0773 (4.0657)	loss_scale 1024.0000 (853.2575)	mem 12860MB
[2024-05-29 11:19:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4600/5004]	eta 0:03:33 lr 0.000001	 wd 0.0500	time 0.3917 (0.5291)	loss 1.2539 (1.1149)	grad_norm 3.3172 (4.0626)	loss_scale 1024.0000 (856.9685)	mem 12860MB
[2024-05-29 11:20:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4700/5004]	eta 0:02:40 lr 0.000001	 wd 0.0500	time 0.4817 (0.5288)	loss 1.0015 (1.1143)	grad_norm 3.2870 (4.0705)	loss_scale 1024.0000 (860.5216)	mem 12860MB
[2024-05-29 11:21:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4800/5004]	eta 0:01:47 lr 0.000001	 wd 0.0500	time 0.3664 (0.5287)	loss 1.0763 (1.1138)	grad_norm 3.0008 (4.0660)	loss_scale 1024.0000 (863.9267)	mem 12860MB
[2024-05-29 11:21:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][4900/5004]	eta 0:00:54 lr 0.000001	 wd 0.0500	time 0.4699 (0.5286)	loss 1.4470 (1.1137)	grad_norm 8.7835 (4.0689)	loss_scale 1024.0000 (867.1928)	mem 12860MB
[2024-05-29 11:22:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [23/30][5000/5004]	eta 0:00:02 lr 0.000001	 wd 0.0500	time 0.4058 (0.5276)	loss 0.8635 (1.1131)	grad_norm 47.9101 (4.0818)	loss_scale 1024.0000 (870.3283)	mem 12860MB
[2024-05-29 11:22:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 23 training takes 0:44:04
[2024-05-29 11:22:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.643 (7.643)	Loss 0.3909 (0.3909)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 11:23:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.171 (0.197)	Loss 0.7285 (0.6291)	Acc@1 83.594 (87.914)	Acc@5 97.656 (98.256)	Mem 12860MB
[2024-05-29 11:23:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.802 Acc@5 98.040
[2024-05-29 11:23:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 11:23:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.80%
[2024-05-29 11:23:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][0/5004]	eta 13:16:03 lr 0.000001	 wd 0.0500	time 9.5450 (9.5450)	loss 0.6833 (0.6833)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 11:24:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][100/5004]	eta 0:49:25 lr 0.000001	 wd 0.0500	time 0.3816 (0.6047)	loss 1.3309 (1.1213)	grad_norm 3.8906 (4.7592)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 11:25:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][200/5004]	eta 0:44:45 lr 0.000001	 wd 0.0500	time 0.3942 (0.5590)	loss 1.1271 (1.0964)	grad_norm 3.8717 (4.9284)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 11:26:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][300/5004]	eta 0:42:33 lr 0.000001	 wd 0.0500	time 0.4242 (0.5429)	loss 0.7303 (1.1076)	grad_norm 3.6978 (4.5626)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 11:26:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][400/5004]	eta 0:40:50 lr 0.000001	 wd 0.0500	time 0.3773 (0.5323)	loss 0.7679 (1.0972)	grad_norm 3.8630 (4.4076)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 11:27:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][500/5004]	eta 0:39:45 lr 0.000001	 wd 0.0500	time 0.4603 (0.5296)	loss 0.8383 (1.0993)	grad_norm 4.2010 (4.3365)	loss_scale 2048.0000 (1032.1756)	mem 12860MB
[2024-05-29 11:28:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][600/5004]	eta 0:38:31 lr 0.000001	 wd 0.0500	time 0.4812 (0.5249)	loss 1.2809 (1.1057)	grad_norm 3.2970 (4.3864)	loss_scale 2048.0000 (1201.1980)	mem 12860MB
[2024-05-29 11:29:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][700/5004]	eta 0:37:26 lr 0.000001	 wd 0.0500	time 0.4575 (0.5219)	loss 1.4627 (1.1033)	grad_norm 4.0464 (4.3026)	loss_scale 2048.0000 (1321.9971)	mem 12860MB
[2024-05-29 11:30:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][800/5004]	eta 0:36:33 lr 0.000001	 wd 0.0500	time 0.4346 (0.5217)	loss 1.1727 (1.1055)	grad_norm 3.8114 (4.2185)	loss_scale 2048.0000 (1412.6342)	mem 12860MB
[2024-05-29 11:31:15 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][900/5004]	eta 0:35:43 lr 0.000001	 wd 0.0500	time 0.4678 (0.5222)	loss 1.1049 (1.1107)	grad_norm 25.5571 (4.3046)	loss_scale 2048.0000 (1483.1521)	mem 12860MB
[2024-05-29 11:32:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1000/5004]	eta 0:34:45 lr 0.000001	 wd 0.0500	time 0.3884 (0.5208)	loss 1.2276 (1.1123)	grad_norm 3.9493 (4.2290)	loss_scale 2048.0000 (1539.5804)	mem 12860MB
[2024-05-29 11:32:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1100/5004]	eta 0:33:49 lr 0.000001	 wd 0.0500	time 0.4006 (0.5197)	loss 1.3298 (1.1124)	grad_norm 3.9197 (4.2004)	loss_scale 2048.0000 (1585.7584)	mem 12860MB
[2024-05-29 11:33:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1200/5004]	eta 0:32:56 lr 0.000001	 wd 0.0500	time 0.4142 (0.5196)	loss 1.4119 (1.1133)	grad_norm 4.2439 (nan)	loss_scale 1024.0000 (1566.2681)	mem 12860MB
[2024-05-29 11:34:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1300/5004]	eta 0:32:04 lr 0.000001	 wd 0.0500	time 0.4730 (0.5195)	loss 1.3561 (1.1134)	grad_norm 2.7186 (nan)	loss_scale 1024.0000 (1524.5872)	mem 12860MB
[2024-05-29 11:35:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1400/5004]	eta 0:31:10 lr 0.000001	 wd 0.0500	time 0.4903 (0.5191)	loss 0.6672 (1.1131)	grad_norm 3.2843 (nan)	loss_scale 1024.0000 (1488.8565)	mem 12860MB
[2024-05-29 11:36:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1500/5004]	eta 0:30:18 lr 0.000001	 wd 0.0500	time 0.3964 (0.5189)	loss 0.6085 (1.1100)	grad_norm 3.3617 (nan)	loss_scale 1024.0000 (1457.8867)	mem 12860MB
[2024-05-29 11:37:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1600/5004]	eta 0:29:23 lr 0.000001	 wd 0.0500	time 0.4281 (0.5181)	loss 1.2249 (1.1086)	grad_norm 4.8867 (nan)	loss_scale 1024.0000 (1430.7858)	mem 12860MB
[2024-05-29 11:38:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1700/5004]	eta 0:28:29 lr 0.000001	 wd 0.0500	time 0.3996 (0.5175)	loss 0.7152 (1.1116)	grad_norm 5.8382 (nan)	loss_scale 1024.0000 (1406.8713)	mem 12860MB
[2024-05-29 11:38:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1800/5004]	eta 0:27:36 lr 0.000001	 wd 0.0500	time 0.4215 (0.5171)	loss 1.2866 (1.1094)	grad_norm 3.8794 (nan)	loss_scale 1024.0000 (1385.6124)	mem 12860MB
[2024-05-29 11:39:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][1900/5004]	eta 0:26:43 lr 0.000001	 wd 0.0500	time 0.4105 (0.5166)	loss 1.3529 (1.1098)	grad_norm 4.4272 (nan)	loss_scale 1024.0000 (1366.5902)	mem 12860MB
[2024-05-29 11:40:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2000/5004]	eta 0:25:54 lr 0.000001	 wd 0.0500	time 0.3924 (0.5174)	loss 0.9997 (1.1079)	grad_norm 4.1390 (nan)	loss_scale 1024.0000 (1349.4693)	mem 12860MB
[2024-05-29 11:41:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2100/5004]	eta 0:25:04 lr 0.000001	 wd 0.0500	time 0.4338 (0.5180)	loss 1.2428 (1.1077)	grad_norm 3.3934 (nan)	loss_scale 1024.0000 (1333.9781)	mem 12860MB
[2024-05-29 11:42:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2200/5004]	eta 0:24:11 lr 0.000001	 wd 0.0500	time 0.3638 (0.5175)	loss 1.5824 (1.1078)	grad_norm 2.8200 (nan)	loss_scale 1024.0000 (1319.8946)	mem 12860MB
[2024-05-29 11:43:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2300/5004]	eta 0:23:17 lr 0.000001	 wd 0.0500	time 0.4508 (0.5169)	loss 1.4157 (1.1063)	grad_norm 7.2400 (nan)	loss_scale 1024.0000 (1307.0352)	mem 12860MB
[2024-05-29 11:44:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2400/5004]	eta 0:22:25 lr 0.000001	 wd 0.0500	time 0.4730 (0.5167)	loss 0.6861 (1.1058)	grad_norm 3.4886 (nan)	loss_scale 1024.0000 (1295.2470)	mem 12860MB
[2024-05-29 11:44:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2500/5004]	eta 0:21:33 lr 0.000001	 wd 0.0500	time 0.4504 (0.5164)	loss 1.3658 (1.1045)	grad_norm 3.5767 (nan)	loss_scale 1024.0000 (1284.4014)	mem 12860MB
[2024-05-29 11:45:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2600/5004]	eta 0:20:41 lr 0.000001	 wd 0.0500	time 0.3848 (0.5166)	loss 0.9779 (1.1040)	grad_norm 3.6299 (nan)	loss_scale 1024.0000 (1274.3899)	mem 12860MB
[2024-05-29 11:46:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2700/5004]	eta 0:19:49 lr 0.000001	 wd 0.0500	time 0.4281 (0.5162)	loss 1.2383 (1.1030)	grad_norm 4.1546 (nan)	loss_scale 1024.0000 (1265.1196)	mem 12860MB
[2024-05-29 11:47:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2800/5004]	eta 0:18:57 lr 0.000001	 wd 0.0500	time 0.4703 (0.5162)	loss 1.0599 (1.1034)	grad_norm 4.5040 (nan)	loss_scale 1024.0000 (1256.5112)	mem 12860MB
[2024-05-29 11:48:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][2900/5004]	eta 0:18:05 lr 0.000001	 wd 0.0500	time 0.4742 (0.5159)	loss 1.4733 (1.1029)	grad_norm 4.1504 (nan)	loss_scale 1024.0000 (1248.4964)	mem 12860MB
[2024-05-29 11:49:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3000/5004]	eta 0:17:13 lr 0.000001	 wd 0.0500	time 0.4191 (0.5159)	loss 0.7582 (1.1027)	grad_norm 3.0206 (nan)	loss_scale 1024.0000 (1241.0157)	mem 12860MB
[2024-05-29 11:50:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3100/5004]	eta 0:16:22 lr 0.000001	 wd 0.0500	time 0.4895 (0.5159)	loss 1.2003 (1.1027)	grad_norm 2.9515 (nan)	loss_scale 1024.0000 (1234.0174)	mem 12860MB
[2024-05-29 11:50:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3200/5004]	eta 0:15:31 lr 0.000001	 wd 0.0500	time 0.4529 (0.5163)	loss 1.4030 (1.1031)	grad_norm 3.6886 (nan)	loss_scale 1024.0000 (1227.4564)	mem 12860MB
[2024-05-29 11:51:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3300/5004]	eta 0:14:40 lr 0.000001	 wd 0.0500	time 0.4650 (0.5165)	loss 1.0829 (1.1028)	grad_norm 4.6626 (nan)	loss_scale 1024.0000 (1221.2929)	mem 12860MB
[2024-05-29 11:52:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3400/5004]	eta 0:13:48 lr 0.000001	 wd 0.0500	time 0.3668 (0.5166)	loss 1.0716 (1.1028)	grad_norm 7.8516 (nan)	loss_scale 1024.0000 (1215.4919)	mem 12860MB
[2024-05-29 11:53:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3500/5004]	eta 0:12:56 lr 0.000001	 wd 0.0500	time 0.3866 (0.5165)	loss 1.0897 (1.1038)	grad_norm 3.8088 (nan)	loss_scale 1024.0000 (1210.0223)	mem 12860MB
[2024-05-29 11:54:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3600/5004]	eta 0:12:05 lr 0.000001	 wd 0.0500	time 0.4749 (0.5169)	loss 0.7466 (1.1056)	grad_norm 2.8741 (nan)	loss_scale 1024.0000 (1204.8564)	mem 12860MB
[2024-05-29 11:55:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3700/5004]	eta 0:11:16 lr 0.000001	 wd 0.0500	time 0.4195 (0.5191)	loss 1.2941 (1.1055)	grad_norm 4.2675 (nan)	loss_scale 1024.0000 (1199.9697)	mem 12860MB
[2024-05-29 11:56:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3800/5004]	eta 0:10:26 lr 0.000001	 wd 0.0500	time 0.4212 (0.5200)	loss 0.8355 (1.1055)	grad_norm 2.7692 (nan)	loss_scale 1024.0000 (1195.3402)	mem 12860MB
[2024-05-29 11:57:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][3900/5004]	eta 0:09:34 lr 0.000001	 wd 0.0500	time 0.4113 (0.5201)	loss 0.9243 (1.1052)	grad_norm 3.4081 (nan)	loss_scale 1024.0000 (1190.9480)	mem 12860MB
[2024-05-29 11:58:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4000/5004]	eta 0:08:42 lr 0.000001	 wd 0.0500	time 0.4398 (0.5204)	loss 1.4345 (1.1046)	grad_norm 3.9672 (nan)	loss_scale 1024.0000 (1186.7753)	mem 12860MB
[2024-05-29 11:58:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4100/5004]	eta 0:07:50 lr 0.000001	 wd 0.0500	time 0.4580 (0.5205)	loss 1.3150 (1.1044)	grad_norm 4.5755 (nan)	loss_scale 1024.0000 (1182.8061)	mem 12860MB
[2024-05-29 11:59:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4200/5004]	eta 0:06:58 lr 0.000001	 wd 0.0500	time 0.4425 (0.5209)	loss 0.6299 (1.1042)	grad_norm 7.1185 (nan)	loss_scale 1024.0000 (1179.0259)	mem 12860MB
[2024-05-29 12:00:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4300/5004]	eta 0:06:06 lr 0.000001	 wd 0.0500	time 0.4116 (0.5210)	loss 1.2429 (1.1042)	grad_norm 5.1293 (nan)	loss_scale 1024.0000 (1175.4215)	mem 12860MB
[2024-05-29 12:01:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4400/5004]	eta 0:05:15 lr 0.000001	 wd 0.0500	time 0.4153 (0.5221)	loss 1.0048 (1.1041)	grad_norm 2.8465 (nan)	loss_scale 1024.0000 (1171.9809)	mem 12860MB
[2024-05-29 12:02:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4500/5004]	eta 0:04:23 lr 0.000001	 wd 0.0500	time 0.4303 (0.5223)	loss 1.2139 (1.1046)	grad_norm 2.8721 (nan)	loss_scale 1024.0000 (1168.6932)	mem 12860MB
[2024-05-29 12:03:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4600/5004]	eta 0:03:31 lr 0.000001	 wd 0.0500	time 0.3727 (0.5226)	loss 0.6248 (1.1047)	grad_norm 6.7352 (nan)	loss_scale 1024.0000 (1165.5484)	mem 12860MB
[2024-05-29 12:04:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4700/5004]	eta 0:02:39 lr 0.000001	 wd 0.0500	time 0.5097 (0.5232)	loss 0.8745 (1.1048)	grad_norm 3.1010 (nan)	loss_scale 1024.0000 (1162.5373)	mem 12860MB
[2024-05-29 12:05:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4800/5004]	eta 0:01:46 lr 0.000001	 wd 0.0500	time 0.5866 (0.5236)	loss 1.2126 (1.1058)	grad_norm 3.8158 (nan)	loss_scale 1024.0000 (1159.6517)	mem 12860MB
[2024-05-29 12:06:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][4900/5004]	eta 0:00:54 lr 0.000001	 wd 0.0500	time 0.4808 (0.5243)	loss 1.3086 (1.1056)	grad_norm 3.2601 (nan)	loss_scale 1024.0000 (1156.8839)	mem 12860MB
[2024-05-29 12:07:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [24/30][5000/5004]	eta 0:00:02 lr 0.000001	 wd 0.0500	time 0.3453 (0.5235)	loss 0.7017 (1.1054)	grad_norm 3.8190 (nan)	loss_scale 1024.0000 (1154.2268)	mem 12860MB
[2024-05-29 12:07:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 24 training takes 0:43:45
[2024-05-29 12:07:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.030 (8.030)	Loss 0.3867 (0.3867)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 12:07:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.115 (0.202)	Loss 0.7207 (0.6204)	Acc@1 84.375 (87.937)	Acc@5 97.656 (98.252)	Mem 12860MB
[2024-05-29 12:07:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.790 Acc@5 98.042
[2024-05-29 12:07:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 12:07:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.80%
[2024-05-29 12:07:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][0/5004]	eta 11:48:25 lr 0.000001	 wd 0.0500	time 8.4943 (8.4943)	loss 1.4635 (1.4635)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:08:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][100/5004]	eta 0:47:00 lr 0.000001	 wd 0.0500	time 0.4065 (0.5751)	loss 1.7143 (1.1123)	grad_norm 3.0933 (3.7064)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:09:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][200/5004]	eta 0:43:19 lr 0.000001	 wd 0.0500	time 0.4081 (0.5411)	loss 1.3891 (1.1242)	grad_norm 2.8356 (nan)	loss_scale 1024.0000 (1166.6468)	mem 12860MB
[2024-05-29 12:10:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][300/5004]	eta 0:42:03 lr 0.000001	 wd 0.0500	time 0.4518 (0.5364)	loss 1.2858 (1.1418)	grad_norm 5.2431 (nan)	loss_scale 1024.0000 (1119.2558)	mem 12860MB
[2024-05-29 12:11:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][400/5004]	eta 0:41:48 lr 0.000001	 wd 0.0500	time 0.4350 (0.5448)	loss 1.0761 (1.1303)	grad_norm 3.1123 (nan)	loss_scale 1024.0000 (1095.5012)	mem 12860MB
[2024-05-29 12:12:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][500/5004]	eta 0:40:56 lr 0.000001	 wd 0.0500	time 0.4526 (0.5454)	loss 1.1663 (1.1338)	grad_norm 4.0915 (nan)	loss_scale 1024.0000 (1081.2295)	mem 12860MB
[2024-05-29 12:13:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][600/5004]	eta 0:39:46 lr 0.000001	 wd 0.0500	time 0.4486 (0.5419)	loss 1.2556 (1.1265)	grad_norm 3.6325 (nan)	loss_scale 1024.0000 (1071.7072)	mem 12860MB
[2024-05-29 12:14:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][700/5004]	eta 0:38:30 lr 0.000001	 wd 0.0500	time 0.4471 (0.5369)	loss 1.1543 (1.1249)	grad_norm 3.7841 (nan)	loss_scale 1024.0000 (1064.9016)	mem 12860MB
[2024-05-29 12:14:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][800/5004]	eta 0:37:23 lr 0.000001	 wd 0.0500	time 0.4012 (0.5337)	loss 1.0799 (1.1233)	grad_norm 3.5487 (nan)	loss_scale 1024.0000 (1059.7953)	mem 12860MB
[2024-05-29 12:15:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][900/5004]	eta 0:36:25 lr 0.000001	 wd 0.0500	time 0.5846 (0.5325)	loss 1.2125 (1.1237)	grad_norm 3.3387 (nan)	loss_scale 1024.0000 (1055.8224)	mem 12860MB
[2024-05-29 12:16:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1000/5004]	eta 0:35:33 lr 0.000001	 wd 0.0500	time 0.3620 (0.5329)	loss 1.3158 (1.1200)	grad_norm 5.6897 (nan)	loss_scale 1024.0000 (1052.6434)	mem 12860MB
[2024-05-29 12:17:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1100/5004]	eta 0:34:32 lr 0.000001	 wd 0.0500	time 0.4068 (0.5308)	loss 1.4097 (1.1172)	grad_norm 3.6010 (nan)	loss_scale 1024.0000 (1050.0418)	mem 12860MB
[2024-05-29 12:18:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1200/5004]	eta 0:33:35 lr 0.000001	 wd 0.0500	time 0.4261 (0.5298)	loss 1.2132 (1.1198)	grad_norm 3.4699 (nan)	loss_scale 1024.0000 (1047.8734)	mem 12860MB
[2024-05-29 12:19:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1300/5004]	eta 0:32:35 lr 0.000001	 wd 0.0500	time 0.4406 (0.5278)	loss 1.3919 (1.1190)	grad_norm 3.1519 (nan)	loss_scale 1024.0000 (1046.0384)	mem 12860MB
[2024-05-29 12:20:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1400/5004]	eta 0:31:38 lr 0.000001	 wd 0.0500	time 0.4650 (0.5267)	loss 1.0028 (1.1185)	grad_norm 4.0915 (nan)	loss_scale 1024.0000 (1044.4654)	mem 12860MB
[2024-05-29 12:20:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1500/5004]	eta 0:30:47 lr 0.000001	 wd 0.0500	time 0.4090 (0.5272)	loss 0.7209 (1.1196)	grad_norm 3.6684 (nan)	loss_scale 1024.0000 (1043.1019)	mem 12860MB
[2024-05-29 12:21:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1600/5004]	eta 0:29:53 lr 0.000001	 wd 0.0500	time 0.5456 (0.5268)	loss 1.1584 (1.1198)	grad_norm 3.4015 (nan)	loss_scale 1024.0000 (1041.9088)	mem 12860MB
[2024-05-29 12:22:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1700/5004]	eta 0:28:58 lr 0.000001	 wd 0.0500	time 0.4752 (0.5261)	loss 1.2992 (1.1202)	grad_norm 3.5408 (nan)	loss_scale 1024.0000 (1040.8560)	mem 12860MB
[2024-05-29 12:23:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1800/5004]	eta 0:28:03 lr 0.000001	 wd 0.0500	time 0.4905 (0.5255)	loss 1.1129 (1.1212)	grad_norm 2.8106 (nan)	loss_scale 1024.0000 (1039.9200)	mem 12860MB
[2024-05-29 12:24:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][1900/5004]	eta 0:27:09 lr 0.000001	 wd 0.0500	time 0.4266 (0.5249)	loss 0.6543 (1.1200)	grad_norm 3.0777 (nan)	loss_scale 1024.0000 (1039.0826)	mem 12860MB
[2024-05-29 12:25:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2000/5004]	eta 0:26:16 lr 0.000001	 wd 0.0500	time 0.4709 (0.5248)	loss 1.4457 (1.1209)	grad_norm 3.9148 (nan)	loss_scale 1024.0000 (1038.3288)	mem 12860MB
[2024-05-29 12:26:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2100/5004]	eta 0:25:20 lr 0.000001	 wd 0.0500	time 0.3878 (0.5235)	loss 1.5089 (1.1219)	grad_norm 3.0483 (nan)	loss_scale 1024.0000 (1037.6468)	mem 12860MB
[2024-05-29 12:26:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2200/5004]	eta 0:24:24 lr 0.000001	 wd 0.0500	time 0.3698 (0.5223)	loss 1.0325 (1.1223)	grad_norm 6.1044 (nan)	loss_scale 1024.0000 (1037.0268)	mem 12860MB
[2024-05-29 12:27:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2300/5004]	eta 0:23:33 lr 0.000001	 wd 0.0500	time 0.4412 (0.5228)	loss 1.1285 (1.1216)	grad_norm 3.6489 (nan)	loss_scale 1024.0000 (1036.4607)	mem 12860MB
[2024-05-29 12:28:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2400/5004]	eta 0:22:40 lr 0.000001	 wd 0.0500	time 0.3595 (0.5226)	loss 1.4158 (1.1210)	grad_norm 14.7975 (nan)	loss_scale 1024.0000 (1035.9417)	mem 12860MB
[2024-05-29 12:29:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2500/5004]	eta 0:21:47 lr 0.000001	 wd 0.0500	time 0.4063 (0.5223)	loss 0.8485 (1.1220)	grad_norm 3.3828 (nan)	loss_scale 1024.0000 (1035.4642)	mem 12860MB
[2024-05-29 12:30:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2600/5004]	eta 0:20:54 lr 0.000001	 wd 0.0500	time 0.4371 (0.5219)	loss 1.2290 (1.1211)	grad_norm 4.7018 (nan)	loss_scale 1024.0000 (1035.0235)	mem 12860MB
[2024-05-29 12:31:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2700/5004]	eta 0:20:01 lr 0.000001	 wd 0.0500	time 0.4086 (0.5214)	loss 1.3849 (1.1226)	grad_norm 2.8907 (nan)	loss_scale 1024.0000 (1034.6153)	mem 12860MB
[2024-05-29 12:32:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2800/5004]	eta 0:19:08 lr 0.000001	 wd 0.0500	time 0.4594 (0.5212)	loss 0.8050 (1.1221)	grad_norm 3.3905 (nan)	loss_scale 1024.0000 (1034.2363)	mem 12860MB
[2024-05-29 12:32:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][2900/5004]	eta 0:18:16 lr 0.000001	 wd 0.0500	time 0.4141 (0.5211)	loss 0.8871 (1.1207)	grad_norm 3.0560 (nan)	loss_scale 1024.0000 (1033.8835)	mem 12860MB
[2024-05-29 12:33:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3000/5004]	eta 0:17:24 lr 0.000001	 wd 0.0500	time 0.4288 (0.5212)	loss 1.1811 (1.1217)	grad_norm 3.6624 (nan)	loss_scale 1024.0000 (1033.5541)	mem 12860MB
[2024-05-29 12:34:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3100/5004]	eta 0:16:33 lr 0.000001	 wd 0.0500	time 0.4758 (0.5220)	loss 1.4308 (1.1221)	grad_norm 3.1160 (nan)	loss_scale 1024.0000 (1033.2460)	mem 12860MB
[2024-05-29 12:35:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3200/5004]	eta 0:15:41 lr 0.000001	 wd 0.0500	time 0.4320 (0.5217)	loss 1.1104 (1.1218)	grad_norm 3.6144 (nan)	loss_scale 1024.0000 (1032.9572)	mem 12860MB
[2024-05-29 12:36:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3300/5004]	eta 0:14:48 lr 0.000001	 wd 0.0500	time 0.4522 (0.5215)	loss 0.6773 (1.1216)	grad_norm 3.3480 (nan)	loss_scale 1024.0000 (1032.6859)	mem 12860MB
[2024-05-29 12:37:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3400/5004]	eta 0:13:56 lr 0.000001	 wd 0.0500	time 0.4531 (0.5212)	loss 1.2398 (1.1225)	grad_norm 2.9152 (nan)	loss_scale 1024.0000 (1032.4305)	mem 12860MB
[2024-05-29 12:38:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3500/5004]	eta 0:13:04 lr 0.000001	 wd 0.0500	time 0.5438 (0.5214)	loss 1.2607 (1.1220)	grad_norm 4.8201 (nan)	loss_scale 1024.0000 (1032.1897)	mem 12860MB
[2024-05-29 12:39:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3600/5004]	eta 0:12:12 lr 0.000001	 wd 0.0500	time 0.3921 (0.5215)	loss 1.3763 (1.1208)	grad_norm 3.0690 (nan)	loss_scale 1024.0000 (1031.9622)	mem 12860MB
[2024-05-29 12:39:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3700/5004]	eta 0:11:21 lr 0.000001	 wd 0.0500	time 0.4125 (0.5223)	loss 1.1528 (1.1213)	grad_norm 3.5827 (nan)	loss_scale 1024.0000 (1031.7471)	mem 12860MB
[2024-05-29 12:40:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3800/5004]	eta 0:10:30 lr 0.000001	 wd 0.0500	time 0.4511 (0.5240)	loss 1.0427 (1.1219)	grad_norm 2.9236 (nan)	loss_scale 1024.0000 (1031.5433)	mem 12860MB
[2024-05-29 12:41:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][3900/5004]	eta 0:09:39 lr 0.000001	 wd 0.0500	time 0.4744 (0.5250)	loss 1.5496 (1.1215)	grad_norm 3.6557 (nan)	loss_scale 1024.0000 (1031.3499)	mem 12860MB
[2024-05-29 12:42:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4000/5004]	eta 0:08:47 lr 0.000001	 wd 0.0500	time 0.4389 (0.5257)	loss 0.7004 (1.1219)	grad_norm 3.0373 (nan)	loss_scale 1024.0000 (1031.1662)	mem 12860MB
[2024-05-29 12:43:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4100/5004]	eta 0:07:55 lr 0.000001	 wd 0.0500	time 0.5207 (0.5258)	loss 1.5855 (1.1217)	grad_norm 3.6494 (nan)	loss_scale 1024.0000 (1030.9915)	mem 12860MB
[2024-05-29 12:44:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4200/5004]	eta 0:07:02 lr 0.000001	 wd 0.0500	time 0.3968 (0.5257)	loss 0.8764 (1.1213)	grad_norm 3.8017 (nan)	loss_scale 1024.0000 (1035.7001)	mem 12860MB
[2024-05-29 12:45:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4300/5004]	eta 0:06:10 lr 0.000001	 wd 0.0500	time 0.4851 (0.5267)	loss 0.9940 (1.1211)	grad_norm 3.5620 (nan)	loss_scale 1024.0000 (1035.4280)	mem 12860MB
[2024-05-29 12:46:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4400/5004]	eta 0:05:18 lr 0.000001	 wd 0.0500	time 0.4298 (0.5275)	loss 1.3472 (1.1215)	grad_norm 2.3020 (nan)	loss_scale 1024.0000 (1035.1684)	mem 12860MB
[2024-05-29 12:47:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4500/5004]	eta 0:04:25 lr 0.000001	 wd 0.0500	time 0.4313 (0.5274)	loss 1.1868 (1.1216)	grad_norm 3.0814 (nan)	loss_scale 1024.0000 (1034.9202)	mem 12860MB
[2024-05-29 12:48:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4600/5004]	eta 0:03:33 lr 0.000001	 wd 0.0500	time 0.4227 (0.5275)	loss 1.0422 (1.1215)	grad_norm 2.9012 (nan)	loss_scale 1024.0000 (1034.6829)	mem 12860MB
[2024-05-29 12:49:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4700/5004]	eta 0:02:40 lr 0.000001	 wd 0.0500	time 0.4210 (0.5273)	loss 1.0552 (1.1214)	grad_norm 5.6359 (nan)	loss_scale 1024.0000 (1034.4556)	mem 12860MB
[2024-05-29 12:49:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4800/5004]	eta 0:01:47 lr 0.000001	 wd 0.0500	time 0.4073 (0.5273)	loss 1.0739 (1.1207)	grad_norm 3.1148 (nan)	loss_scale 1024.0000 (1034.2379)	mem 12860MB
[2024-05-29 12:50:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][4900/5004]	eta 0:00:54 lr 0.000001	 wd 0.0500	time 0.3906 (0.5272)	loss 0.9526 (1.1209)	grad_norm 3.5037 (nan)	loss_scale 1024.0000 (1034.0290)	mem 12860MB
[2024-05-29 12:51:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [25/30][5000/5004]	eta 0:00:02 lr 0.000001	 wd 0.0500	time 0.3903 (0.5262)	loss 1.3276 (1.1211)	grad_norm 3.2510 (nan)	loss_scale 1024.0000 (1033.8284)	mem 12860MB
[2024-05-29 12:51:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 25 training takes 0:43:58
[2024-05-29 12:51:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.121 (8.121)	Loss 0.3904 (0.3904)	Acc@1 95.312 (95.312)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 12:52:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.100 (0.201)	Loss 0.7310 (0.6281)	Acc@1 83.984 (87.894)	Acc@5 97.656 (98.294)	Mem 12860MB
[2024-05-29 12:52:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.724 Acc@5 98.052
[2024-05-29 12:52:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-05-29 12:52:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.80%
[2024-05-29 12:52:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][0/5004]	eta 10:57:40 lr 0.000001	 wd 0.0500	time 7.8857 (7.8857)	loss 0.9852 (0.9852)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:53:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][100/5004]	eta 0:46:45 lr 0.000001	 wd 0.0500	time 0.3989 (0.5721)	loss 1.1117 (1.1102)	grad_norm 4.0729 (4.0823)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:54:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][200/5004]	eta 0:43:31 lr 0.000001	 wd 0.0500	time 0.4354 (0.5437)	loss 1.2470 (1.1209)	grad_norm 4.6445 (4.0377)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:54:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][300/5004]	eta 0:41:28 lr 0.000001	 wd 0.0500	time 0.4181 (0.5289)	loss 1.1091 (1.1279)	grad_norm 3.5489 (3.9598)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:55:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][400/5004]	eta 0:40:18 lr 0.000001	 wd 0.0500	time 0.4342 (0.5252)	loss 0.9554 (1.1204)	grad_norm 3.9328 (3.9517)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:56:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][500/5004]	eta 0:39:01 lr 0.000001	 wd 0.0500	time 0.4033 (0.5198)	loss 0.8359 (1.1183)	grad_norm 6.7795 (3.9390)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:57:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][600/5004]	eta 0:38:00 lr 0.000001	 wd 0.0500	time 0.4526 (0.5178)	loss 1.4158 (1.1185)	grad_norm 4.2586 (4.2972)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:58:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][700/5004]	eta 0:37:02 lr 0.000001	 wd 0.0500	time 0.4455 (0.5163)	loss 0.9035 (1.1108)	grad_norm 4.0863 (4.2479)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 12:59:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][800/5004]	eta 0:36:09 lr 0.000001	 wd 0.0500	time 0.3950 (0.5161)	loss 1.5192 (1.1097)	grad_norm 3.5794 (4.3597)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 13:00:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][900/5004]	eta 0:35:09 lr 0.000001	 wd 0.0500	time 0.4189 (0.5141)	loss 0.7136 (1.1061)	grad_norm 3.3238 (4.3306)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 13:00:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1000/5004]	eta 0:34:17 lr 0.000001	 wd 0.0500	time 0.4257 (0.5140)	loss 0.8172 (1.1055)	grad_norm 4.9384 (4.3122)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 13:01:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1100/5004]	eta 0:33:24 lr 0.000001	 wd 0.0500	time 0.3759 (0.5136)	loss 0.8045 (1.1067)	grad_norm 3.3604 (4.3065)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 13:02:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1200/5004]	eta 0:32:32 lr 0.000001	 wd 0.0500	time 0.4524 (0.5133)	loss 0.9653 (1.1079)	grad_norm 3.1668 (4.2789)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 13:03:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1300/5004]	eta 0:31:47 lr 0.000001	 wd 0.0500	time 0.4414 (0.5150)	loss 1.5477 (1.1062)	grad_norm 4.1125 (4.2744)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 13:04:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1400/5004]	eta 0:30:56 lr 0.000001	 wd 0.0500	time 0.4389 (0.5152)	loss 1.3720 (1.1052)	grad_norm 2.1582 (inf)	loss_scale 512.0000 (989.6474)	mem 12860MB
[2024-05-29 13:05:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1500/5004]	eta 0:30:04 lr 0.000001	 wd 0.0500	time 0.4545 (0.5150)	loss 0.8294 (1.1030)	grad_norm 4.4363 (inf)	loss_scale 512.0000 (957.8254)	mem 12860MB
[2024-05-29 13:06:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1600/5004]	eta 0:29:12 lr 0.000001	 wd 0.0500	time 0.4203 (0.5148)	loss 1.5900 (1.1012)	grad_norm 4.8537 (inf)	loss_scale 512.0000 (929.9788)	mem 12860MB
[2024-05-29 13:06:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1700/5004]	eta 0:28:20 lr 0.000001	 wd 0.0500	time 0.4154 (0.5147)	loss 1.1655 (1.1038)	grad_norm 3.8160 (inf)	loss_scale 512.0000 (905.4062)	mem 12860MB
[2024-05-29 13:07:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1800/5004]	eta 0:27:30 lr 0.000001	 wd 0.0500	time 0.4604 (0.5150)	loss 0.7579 (1.1009)	grad_norm 3.2597 (inf)	loss_scale 512.0000 (883.5625)	mem 12860MB
[2024-05-29 13:08:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][1900/5004]	eta 0:26:39 lr 0.000001	 wd 0.0500	time 0.4760 (0.5151)	loss 1.0218 (1.1025)	grad_norm 7.1224 (inf)	loss_scale 512.0000 (864.0168)	mem 12860MB
[2024-05-29 13:09:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2000/5004]	eta 0:25:46 lr 0.000001	 wd 0.0500	time 0.4647 (0.5147)	loss 0.7492 (1.1019)	grad_norm 4.0016 (inf)	loss_scale 512.0000 (846.4248)	mem 12860MB
[2024-05-29 13:10:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2100/5004]	eta 0:24:53 lr 0.000001	 wd 0.0500	time 0.4363 (0.5144)	loss 1.3042 (1.1046)	grad_norm 7.8676 (inf)	loss_scale 512.0000 (830.5074)	mem 12860MB
[2024-05-29 13:11:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2200/5004]	eta 0:24:01 lr 0.000001	 wd 0.0500	time 0.3797 (0.5141)	loss 1.1196 (1.1055)	grad_norm 3.3298 (inf)	loss_scale 512.0000 (816.0363)	mem 12860MB
[2024-05-29 13:12:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2300/5004]	eta 0:23:09 lr 0.000001	 wd 0.0500	time 0.3822 (0.5139)	loss 1.3599 (1.1052)	grad_norm 5.0077 (inf)	loss_scale 512.0000 (802.8231)	mem 12860MB
[2024-05-29 13:12:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2400/5004]	eta 0:22:17 lr 0.000001	 wd 0.0500	time 0.3817 (0.5137)	loss 0.7922 (1.1046)	grad_norm 3.6365 (inf)	loss_scale 512.0000 (790.7105)	mem 12860MB
[2024-05-29 13:13:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2500/5004]	eta 0:21:26 lr 0.000001	 wd 0.0500	time 0.4029 (0.5138)	loss 1.2539 (1.1067)	grad_norm 6.4069 (inf)	loss_scale 512.0000 (779.5666)	mem 12860MB
[2024-05-29 13:14:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2600/5004]	eta 0:20:35 lr 0.000001	 wd 0.0500	time 0.3967 (0.5138)	loss 0.8215 (1.1060)	grad_norm 3.0157 (inf)	loss_scale 512.0000 (769.2795)	mem 12860MB
[2024-05-29 13:15:31 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2700/5004]	eta 0:19:46 lr 0.000001	 wd 0.0500	time 0.4394 (0.5149)	loss 1.4103 (1.1053)	grad_norm 3.0099 (inf)	loss_scale 512.0000 (759.7542)	mem 12860MB
[2024-05-29 13:16:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2800/5004]	eta 0:18:54 lr 0.000001	 wd 0.0500	time 0.4969 (0.5148)	loss 1.4196 (1.1067)	grad_norm 4.6753 (inf)	loss_scale 512.0000 (750.9090)	mem 12860MB
[2024-05-29 13:17:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][2900/5004]	eta 0:18:03 lr 0.000001	 wd 0.0500	time 0.4087 (0.5149)	loss 1.2519 (1.1085)	grad_norm 3.5488 (inf)	loss_scale 512.0000 (742.6736)	mem 12860MB
[2024-05-29 13:18:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3000/5004]	eta 0:17:11 lr 0.000001	 wd 0.0500	time 0.4258 (0.5148)	loss 1.2831 (1.1075)	grad_norm 3.8600 (inf)	loss_scale 512.0000 (734.9870)	mem 12860MB
[2024-05-29 13:18:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3100/5004]	eta 0:16:21 lr 0.000001	 wd 0.0500	time 0.4503 (0.5157)	loss 1.4459 (1.1077)	grad_norm 4.7923 (inf)	loss_scale 512.0000 (727.7962)	mem 12860MB
[2024-05-29 13:19:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3200/5004]	eta 0:15:34 lr 0.000001	 wd 0.0500	time 0.4389 (0.5182)	loss 0.9215 (1.1060)	grad_norm 3.2775 (inf)	loss_scale 512.0000 (721.0547)	mem 12860MB
[2024-05-29 13:20:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3300/5004]	eta 0:14:43 lr 0.000001	 wd 0.0500	time 0.4008 (0.5186)	loss 1.1933 (1.1054)	grad_norm 2.8013 (inf)	loss_scale 512.0000 (714.7216)	mem 12860MB
[2024-05-29 13:21:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3400/5004]	eta 0:13:51 lr 0.000001	 wd 0.0500	time 0.4023 (0.5186)	loss 1.5092 (1.1054)	grad_norm 2.7526 (inf)	loss_scale 512.0000 (708.7610)	mem 12860MB
[2024-05-29 13:22:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3500/5004]	eta 0:13:00 lr 0.000001	 wd 0.0500	time 0.4363 (0.5190)	loss 0.9280 (1.1065)	grad_norm 2.7667 (inf)	loss_scale 512.0000 (703.1408)	mem 12860MB
[2024-05-29 13:23:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3600/5004]	eta 0:12:08 lr 0.000001	 wd 0.0500	time 0.3981 (0.5190)	loss 1.3249 (1.1064)	grad_norm 3.5821 (inf)	loss_scale 512.0000 (697.8328)	mem 12860MB
[2024-05-29 13:24:22 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3700/5004]	eta 0:11:17 lr 0.000001	 wd 0.0500	time 0.4291 (0.5192)	loss 1.1847 (1.1070)	grad_norm 2.9857 (inf)	loss_scale 512.0000 (692.8117)	mem 12860MB
[2024-05-29 13:25:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3800/5004]	eta 0:10:25 lr 0.000001	 wd 0.0500	time 0.4613 (0.5193)	loss 0.8660 (1.1066)	grad_norm 3.0642 (inf)	loss_scale 512.0000 (688.0547)	mem 12860MB
[2024-05-29 13:26:12 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][3900/5004]	eta 0:09:34 lr 0.000000	 wd 0.0500	time 0.3668 (0.5208)	loss 1.3220 (1.1064)	grad_norm 5.7204 (inf)	loss_scale 512.0000 (683.5417)	mem 12860MB
[2024-05-29 13:27:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4000/5004]	eta 0:08:44 lr 0.000000	 wd 0.0500	time 0.4209 (0.5226)	loss 0.5819 (1.1058)	grad_norm 3.8001 (inf)	loss_scale 512.0000 (679.2542)	mem 12860MB
[2024-05-29 13:28:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4100/5004]	eta 0:07:52 lr 0.000000	 wd 0.0500	time 0.5584 (0.5230)	loss 1.0371 (1.1055)	grad_norm 2.8448 (inf)	loss_scale 512.0000 (675.1758)	mem 12860MB
[2024-05-29 13:29:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4200/5004]	eta 0:07:01 lr 0.000000	 wd 0.0500	time 0.6393 (0.5237)	loss 1.4207 (1.1058)	grad_norm 3.4449 (inf)	loss_scale 512.0000 (671.2916)	mem 12860MB
[2024-05-29 13:29:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4300/5004]	eta 0:06:09 lr 0.000000	 wd 0.0500	time 0.3921 (0.5247)	loss 1.5316 (1.1061)	grad_norm 2.8306 (inf)	loss_scale 512.0000 (667.5880)	mem 12860MB
[2024-05-29 13:30:50 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4400/5004]	eta 0:05:16 lr 0.000000	 wd 0.0500	time 0.4154 (0.5248)	loss 1.2864 (1.1055)	grad_norm 3.6063 (inf)	loss_scale 512.0000 (664.0527)	mem 12860MB
[2024-05-29 13:31:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4500/5004]	eta 0:04:24 lr 0.000000	 wd 0.0500	time 0.4866 (0.5248)	loss 1.1383 (1.1055)	grad_norm 2.7967 (inf)	loss_scale 512.0000 (660.6745)	mem 12860MB
[2024-05-29 13:32:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4600/5004]	eta 0:03:32 lr 0.000000	 wd 0.0500	time 0.5404 (0.5254)	loss 1.5818 (1.1063)	grad_norm 4.2143 (inf)	loss_scale 512.0000 (657.4432)	mem 12860MB
[2024-05-29 13:33:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4700/5004]	eta 0:02:39 lr 0.000000	 wd 0.0500	time 0.5560 (0.5260)	loss 1.1911 (1.1060)	grad_norm 4.6534 (inf)	loss_scale 512.0000 (654.3493)	mem 12860MB
[2024-05-29 13:34:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4800/5004]	eta 0:01:47 lr 0.000000	 wd 0.0500	time 0.3853 (0.5261)	loss 1.2387 (1.1052)	grad_norm 2.6516 (inf)	loss_scale 512.0000 (651.3843)	mem 12860MB
[2024-05-29 13:35:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][4900/5004]	eta 0:00:54 lr 0.000000	 wd 0.0500	time 0.4626 (0.5264)	loss 1.2819 (1.1048)	grad_norm 2.7851 (inf)	loss_scale 512.0000 (648.5403)	mem 12860MB
[2024-05-29 13:36:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [26/30][5000/5004]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.4445 (0.5257)	loss 1.1174 (1.1052)	grad_norm 3.0002 (inf)	loss_scale 512.0000 (645.8100)	mem 12860MB
[2024-05-29 13:36:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 26 training takes 0:43:56
[2024-05-29 13:36:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.314 (7.314)	Loss 0.3801 (0.3801)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 13:36:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.126 (0.195)	Loss 0.7246 (0.6190)	Acc@1 83.594 (87.953)	Acc@5 97.656 (98.256)	Mem 12860MB
[2024-05-29 13:36:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.828 Acc@5 98.040
[2024-05-29 13:36:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 13:36:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.83%
[2024-05-29 13:37:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][0/5004]	eta 12:56:20 lr 0.000000	 wd 0.0500	time 9.3087 (9.3087)	loss 1.2692 (1.2692)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 13:37:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][100/5004]	eta 0:49:46 lr 0.000000	 wd 0.0500	time 0.4544 (0.6091)	loss 1.3634 (1.1161)	grad_norm 3.3256 (4.0274)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 13:38:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][200/5004]	eta 0:44:52 lr 0.000000	 wd 0.0500	time 0.4260 (0.5605)	loss 1.1862 (1.1058)	grad_norm 2.6514 (4.5674)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 13:39:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][300/5004]	eta 0:42:58 lr 0.000000	 wd 0.0500	time 0.5023 (0.5481)	loss 1.0512 (1.1066)	grad_norm 3.6405 (4.3093)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 13:40:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][400/5004]	eta 0:41:14 lr 0.000000	 wd 0.0500	time 0.4190 (0.5374)	loss 0.7091 (1.1154)	grad_norm 3.1583 (4.2734)	loss_scale 1024.0000 (637.1272)	mem 12860MB
[2024-05-29 13:41:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][500/5004]	eta 0:40:00 lr 0.000000	 wd 0.0500	time 0.3842 (0.5330)	loss 0.7872 (1.1191)	grad_norm 8.9786 (4.2329)	loss_scale 1024.0000 (714.3473)	mem 12860MB
[2024-05-29 13:42:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][600/5004]	eta 0:38:45 lr 0.000000	 wd 0.0500	time 0.4555 (0.5280)	loss 0.9460 (1.1148)	grad_norm 5.8854 (4.1635)	loss_scale 1024.0000 (765.8702)	mem 12860MB
[2024-05-29 13:43:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][700/5004]	eta 0:37:59 lr 0.000000	 wd 0.0500	time 0.4194 (0.5297)	loss 1.0383 (1.1095)	grad_norm 3.7539 (4.0984)	loss_scale 1024.0000 (802.6933)	mem 12860MB
[2024-05-29 13:43:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][800/5004]	eta 0:36:55 lr 0.000000	 wd 0.0500	time 0.3968 (0.5271)	loss 0.9032 (1.1116)	grad_norm 5.0129 (4.1282)	loss_scale 1024.0000 (830.3221)	mem 12860MB
[2024-05-29 13:44:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][900/5004]	eta 0:36:00 lr 0.000000	 wd 0.0500	time 0.4180 (0.5263)	loss 0.9687 (1.1100)	grad_norm 3.3892 (4.0551)	loss_scale 1024.0000 (851.8180)	mem 12860MB
[2024-05-29 13:45:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1000/5004]	eta 0:34:59 lr 0.000000	 wd 0.0500	time 0.4197 (0.5244)	loss 0.6450 (1.1087)	grad_norm 3.0834 (4.0259)	loss_scale 1024.0000 (869.0190)	mem 12860MB
[2024-05-29 13:46:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1100/5004]	eta 0:34:02 lr 0.000000	 wd 0.0500	time 0.4041 (0.5232)	loss 1.4177 (1.1125)	grad_norm 2.8948 (4.0153)	loss_scale 1024.0000 (883.0954)	mem 12860MB
[2024-05-29 13:47:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1200/5004]	eta 0:33:04 lr 0.000000	 wd 0.0500	time 0.4304 (0.5216)	loss 0.8283 (1.1088)	grad_norm 3.4575 (4.0763)	loss_scale 1024.0000 (894.8276)	mem 12860MB
[2024-05-29 13:48:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1300/5004]	eta 0:32:15 lr 0.000000	 wd 0.0500	time 0.4032 (0.5225)	loss 0.6525 (1.1088)	grad_norm 3.5214 (4.0532)	loss_scale 1024.0000 (904.7563)	mem 12860MB
[2024-05-29 13:49:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1400/5004]	eta 0:31:19 lr 0.000000	 wd 0.0500	time 0.3657 (0.5214)	loss 0.8729 (1.1089)	grad_norm 3.2974 (4.0858)	loss_scale 1024.0000 (913.2677)	mem 12860MB
[2024-05-29 13:49:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1500/5004]	eta 0:30:29 lr 0.000000	 wd 0.0500	time 0.3761 (0.5221)	loss 1.3575 (1.1093)	grad_norm 3.8573 (4.0759)	loss_scale 1024.0000 (920.6449)	mem 12860MB
[2024-05-29 13:50:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1600/5004]	eta 0:29:43 lr 0.000000	 wd 0.0500	time 0.4223 (0.5240)	loss 1.0828 (1.1067)	grad_norm 3.6043 (4.0565)	loss_scale 1024.0000 (927.1006)	mem 12860MB
[2024-05-29 13:51:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1700/5004]	eta 0:28:49 lr 0.000000	 wd 0.0500	time 0.3843 (0.5236)	loss 0.8836 (1.1072)	grad_norm 3.6594 (4.0411)	loss_scale 1024.0000 (932.7972)	mem 12860MB
[2024-05-29 13:52:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1800/5004]	eta 0:27:53 lr 0.000000	 wd 0.0500	time 0.4131 (0.5224)	loss 1.0493 (1.1057)	grad_norm 3.1783 (4.1117)	loss_scale 1024.0000 (937.8612)	mem 12860MB
[2024-05-29 13:53:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][1900/5004]	eta 0:26:58 lr 0.000000	 wd 0.0500	time 0.3670 (0.5214)	loss 1.0496 (1.1050)	grad_norm 5.4749 (4.1095)	loss_scale 1024.0000 (942.3924)	mem 12860MB
[2024-05-29 13:54:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2000/5004]	eta 0:26:06 lr 0.000000	 wd 0.0500	time 0.4061 (0.5216)	loss 1.1536 (1.1058)	grad_norm 3.1095 (4.1153)	loss_scale 1024.0000 (946.4708)	mem 12860MB
[2024-05-29 13:55:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2100/5004]	eta 0:25:12 lr 0.000000	 wd 0.0500	time 0.4921 (0.5208)	loss 0.9090 (1.1036)	grad_norm 3.2595 (4.1314)	loss_scale 1024.0000 (950.1609)	mem 12860MB
[2024-05-29 13:55:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2200/5004]	eta 0:24:20 lr 0.000000	 wd 0.0500	time 0.5569 (0.5209)	loss 1.3346 (1.1040)	grad_norm 4.2627 (4.1253)	loss_scale 1024.0000 (953.5157)	mem 12860MB
[2024-05-29 13:56:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2300/5004]	eta 0:23:27 lr 0.000000	 wd 0.0500	time 0.3569 (0.5206)	loss 1.0320 (1.1045)	grad_norm 3.9195 (4.1168)	loss_scale 1024.0000 (956.5789)	mem 12860MB
[2024-05-29 13:57:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2400/5004]	eta 0:22:33 lr 0.000000	 wd 0.0500	time 0.4066 (0.5199)	loss 0.8383 (1.1058)	grad_norm 3.0756 (4.1242)	loss_scale 1024.0000 (959.3869)	mem 12860MB
[2024-05-29 13:58:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2500/5004]	eta 0:21:41 lr 0.000000	 wd 0.0500	time 0.3954 (0.5198)	loss 0.9698 (1.1067)	grad_norm 3.4211 (inf)	loss_scale 512.0000 (955.8289)	mem 12860MB
[2024-05-29 13:59:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2600/5004]	eta 0:20:48 lr 0.000000	 wd 0.0500	time 0.4355 (0.5193)	loss 1.3429 (1.1062)	grad_norm 2.5952 (inf)	loss_scale 512.0000 (938.7651)	mem 12860MB
[2024-05-29 14:00:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2700/5004]	eta 0:19:57 lr 0.000000	 wd 0.0500	time 0.4909 (0.5198)	loss 1.1193 (1.1061)	grad_norm 2.9472 (inf)	loss_scale 512.0000 (922.9648)	mem 12860MB
[2024-05-29 14:01:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2800/5004]	eta 0:19:06 lr 0.000000	 wd 0.0500	time 0.4508 (0.5201)	loss 0.6086 (1.1063)	grad_norm 4.2236 (inf)	loss_scale 512.0000 (908.2928)	mem 12860MB
[2024-05-29 14:02:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][2900/5004]	eta 0:18:14 lr 0.000000	 wd 0.0500	time 0.4149 (0.5200)	loss 1.2255 (1.1059)	grad_norm 2.3319 (inf)	loss_scale 512.0000 (894.6322)	mem 12860MB
[2024-05-29 14:02:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3000/5004]	eta 0:17:21 lr 0.000000	 wd 0.0500	time 0.3711 (0.5195)	loss 0.9258 (1.1067)	grad_norm 3.2114 (inf)	loss_scale 512.0000 (881.8820)	mem 12860MB
[2024-05-29 14:03:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3100/5004]	eta 0:16:29 lr 0.000000	 wd 0.0500	time 0.4350 (0.5195)	loss 0.8438 (1.1068)	grad_norm 2.8483 (inf)	loss_scale 512.0000 (869.9542)	mem 12860MB
[2024-05-29 14:04:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3200/5004]	eta 0:15:36 lr 0.000000	 wd 0.0500	time 0.3689 (0.5191)	loss 1.0246 (1.1085)	grad_norm 6.0845 (inf)	loss_scale 512.0000 (858.7716)	mem 12860MB
[2024-05-29 14:05:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3300/5004]	eta 0:14:44 lr 0.000000	 wd 0.0500	time 0.4425 (0.5193)	loss 0.9554 (1.1079)	grad_norm 3.7247 (inf)	loss_scale 512.0000 (848.2666)	mem 12860MB
[2024-05-29 14:06:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3400/5004]	eta 0:13:55 lr 0.000000	 wd 0.0500	time 0.4284 (0.5208)	loss 1.0522 (1.1085)	grad_norm 3.3794 (inf)	loss_scale 512.0000 (838.3793)	mem 12860MB
[2024-05-29 14:07:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3500/5004]	eta 0:13:03 lr 0.000000	 wd 0.0500	time 0.4607 (0.5208)	loss 1.3308 (1.1082)	grad_norm 6.6415 (inf)	loss_scale 512.0000 (829.0568)	mem 12860MB
[2024-05-29 14:08:08 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3600/5004]	eta 0:12:11 lr 0.000000	 wd 0.0500	time 0.4576 (0.5207)	loss 1.1468 (1.1092)	grad_norm 2.9693 (inf)	loss_scale 512.0000 (820.2522)	mem 12860MB
[2024-05-29 14:09:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3700/5004]	eta 0:11:19 lr 0.000000	 wd 0.0500	time 0.4137 (0.5209)	loss 1.2448 (1.1096)	grad_norm 3.3787 (inf)	loss_scale 512.0000 (811.9233)	mem 12860MB
[2024-05-29 14:09:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3800/5004]	eta 0:10:27 lr 0.000000	 wd 0.0500	time 0.4560 (0.5210)	loss 1.1565 (1.1098)	grad_norm 2.7287 (inf)	loss_scale 512.0000 (804.0326)	mem 12860MB
[2024-05-29 14:10:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][3900/5004]	eta 0:09:35 lr 0.000000	 wd 0.0500	time 0.4808 (0.5212)	loss 0.7796 (1.1089)	grad_norm 3.4647 (inf)	loss_scale 512.0000 (796.5465)	mem 12860MB
[2024-05-29 14:11:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4000/5004]	eta 0:08:44 lr 0.000000	 wd 0.0500	time 0.5233 (0.5223)	loss 1.2074 (1.1087)	grad_norm 3.3304 (inf)	loss_scale 512.0000 (789.4346)	mem 12860MB
[2024-05-29 14:12:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4100/5004]	eta 0:07:55 lr 0.000000	 wd 0.0500	time 0.5159 (0.5256)	loss 1.4566 (1.1077)	grad_norm 3.0593 (inf)	loss_scale 512.0000 (782.6696)	mem 12860MB
[2024-05-29 14:13:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4200/5004]	eta 0:07:03 lr 0.000000	 wd 0.0500	time 0.4967 (0.5263)	loss 1.0845 (1.1081)	grad_norm 3.7590 (inf)	loss_scale 512.0000 (776.2266)	mem 12860MB
[2024-05-29 14:14:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4300/5004]	eta 0:06:11 lr 0.000000	 wd 0.0500	time 0.4376 (0.5272)	loss 0.8667 (1.1086)	grad_norm 2.6529 (inf)	loss_scale 512.0000 (770.0832)	mem 12860MB
[2024-05-29 14:15:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4400/5004]	eta 0:05:18 lr 0.000000	 wd 0.0500	time 0.5097 (0.5279)	loss 0.6840 (1.1091)	grad_norm 2.6568 (inf)	loss_scale 512.0000 (764.2190)	mem 12860MB
[2024-05-29 14:16:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4500/5004]	eta 0:04:26 lr 0.000000	 wd 0.0500	time 0.6302 (0.5297)	loss 1.3930 (1.1095)	grad_norm 3.8898 (inf)	loss_scale 512.0000 (758.6154)	mem 12860MB
[2024-05-29 14:17:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4600/5004]	eta 0:03:34 lr 0.000000	 wd 0.0500	time 0.3683 (0.5314)	loss 0.8786 (1.1102)	grad_norm 2.8810 (inf)	loss_scale 512.0000 (753.2554)	mem 12860MB
[2024-05-29 14:18:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4700/5004]	eta 0:02:41 lr 0.000000	 wd 0.0500	time 0.4395 (0.5313)	loss 1.3046 (1.1106)	grad_norm 3.3336 (inf)	loss_scale 512.0000 (748.1234)	mem 12860MB
[2024-05-29 14:19:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4800/5004]	eta 0:01:48 lr 0.000000	 wd 0.0500	time 0.4601 (0.5313)	loss 1.1015 (1.1119)	grad_norm 6.2992 (inf)	loss_scale 512.0000 (743.2052)	mem 12860MB
[2024-05-29 14:20:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][4900/5004]	eta 0:00:55 lr 0.000000	 wd 0.0500	time 0.4404 (0.5313)	loss 0.9473 (1.1113)	grad_norm 6.6776 (inf)	loss_scale 512.0000 (738.4877)	mem 12860MB
[2024-05-29 14:21:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [27/30][5000/5004]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.4305 (0.5306)	loss 1.0518 (1.1107)	grad_norm 4.1915 (inf)	loss_scale 512.0000 (733.9588)	mem 12860MB
[2024-05-29 14:21:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 27 training takes 0:44:20
[2024-05-29 14:21:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.765 (7.765)	Loss 0.3748 (0.3748)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 14:21:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.112 (0.211)	Loss 0.7075 (0.6096)	Acc@1 83.203 (87.956)	Acc@5 97.656 (98.267)	Mem 12860MB
[2024-05-29 14:21:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.838 Acc@5 98.040
[2024-05-29 14:21:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 14:21:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.84%
[2024-05-29 14:22:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][0/5004]	eta 13:18:45 lr 0.000000	 wd 0.0500	time 9.5774 (9.5774)	loss 1.2343 (1.2343)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:22:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][100/5004]	eta 0:47:52 lr 0.000000	 wd 0.0500	time 0.3992 (0.5857)	loss 0.7492 (1.0731)	grad_norm 3.0292 (4.4179)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:23:42 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][200/5004]	eta 0:43:56 lr 0.000000	 wd 0.0500	time 0.3822 (0.5489)	loss 1.4395 (1.0855)	grad_norm 3.1482 (4.0758)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:24:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][300/5004]	eta 0:41:48 lr 0.000000	 wd 0.0500	time 0.3544 (0.5333)	loss 0.8372 (1.0922)	grad_norm 3.1510 (4.5212)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:25:23 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][400/5004]	eta 0:40:29 lr 0.000000	 wd 0.0500	time 0.4584 (0.5276)	loss 0.9727 (1.1029)	grad_norm 3.5390 (4.4750)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:26:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][500/5004]	eta 0:39:18 lr 0.000000	 wd 0.0500	time 0.5189 (0.5237)	loss 1.3521 (1.1128)	grad_norm 4.9501 (4.3798)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:27:07 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][600/5004]	eta 0:38:28 lr 0.000000	 wd 0.0500	time 0.4621 (0.5242)	loss 0.8202 (1.1078)	grad_norm 66.2898 (4.6342)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:27:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][700/5004]	eta 0:37:35 lr 0.000000	 wd 0.0500	time 0.4789 (0.5241)	loss 0.9384 (1.1114)	grad_norm 3.1642 (4.4931)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:28:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][800/5004]	eta 0:36:42 lr 0.000000	 wd 0.0500	time 0.4106 (0.5239)	loss 1.0465 (1.1110)	grad_norm 2.5057 (4.5096)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:29:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][900/5004]	eta 0:35:44 lr 0.000000	 wd 0.0500	time 0.4173 (0.5225)	loss 1.3025 (1.1101)	grad_norm 3.3994 (4.4996)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:30:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1000/5004]	eta 0:34:53 lr 0.000000	 wd 0.0500	time 0.4401 (0.5228)	loss 1.1642 (1.1119)	grad_norm 3.6412 (4.4371)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:31:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1100/5004]	eta 0:34:01 lr 0.000000	 wd 0.0500	time 0.4315 (0.5228)	loss 1.0515 (1.1096)	grad_norm 2.9781 (4.4314)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:32:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1200/5004]	eta 0:33:13 lr 0.000000	 wd 0.0500	time 0.4721 (0.5240)	loss 1.3574 (1.1112)	grad_norm 3.4899 (4.3816)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:33:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1300/5004]	eta 0:32:22 lr 0.000000	 wd 0.0500	time 0.3709 (0.5243)	loss 1.2941 (1.1073)	grad_norm 9.7114 (4.3420)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:34:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1400/5004]	eta 0:31:26 lr 0.000000	 wd 0.0500	time 0.3900 (0.5234)	loss 0.9339 (1.1073)	grad_norm 4.0944 (4.2857)	loss_scale 512.0000 (512.0000)	mem 12860MB
[2024-05-29 14:34:56 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1500/5004]	eta 0:30:31 lr 0.000000	 wd 0.0500	time 0.4407 (0.5228)	loss 1.1464 (1.1095)	grad_norm 3.4785 (4.2199)	loss_scale 1024.0000 (523.5976)	mem 12860MB
[2024-05-29 14:35:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1600/5004]	eta 0:29:39 lr 0.000000	 wd 0.0500	time 0.3867 (0.5227)	loss 1.3647 (1.1091)	grad_norm 2.7780 (4.2027)	loss_scale 1024.0000 (554.8532)	mem 12860MB
[2024-05-29 14:36:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1700/5004]	eta 0:28:44 lr 0.000000	 wd 0.0500	time 0.3958 (0.5220)	loss 1.1626 (1.1096)	grad_norm 20.2356 (4.2386)	loss_scale 1024.0000 (582.4339)	mem 12860MB
[2024-05-29 14:37:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1800/5004]	eta 0:27:48 lr 0.000000	 wd 0.0500	time 0.4927 (0.5209)	loss 0.9875 (1.1109)	grad_norm 3.0273 (4.2534)	loss_scale 1024.0000 (606.9517)	mem 12860MB
[2024-05-29 14:38:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][1900/5004]	eta 0:26:54 lr 0.000000	 wd 0.0500	time 0.4208 (0.5203)	loss 1.2434 (1.1108)	grad_norm 2.7536 (4.2239)	loss_scale 1024.0000 (628.8901)	mem 12860MB
[2024-05-29 14:39:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2000/5004]	eta 0:26:02 lr 0.000000	 wd 0.0500	time 0.4278 (0.5202)	loss 0.8641 (1.1097)	grad_norm 2.7474 (4.2100)	loss_scale 1024.0000 (648.6357)	mem 12860MB
[2024-05-29 14:40:03 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2100/5004]	eta 0:25:08 lr 0.000000	 wd 0.0500	time 0.4127 (0.5196)	loss 1.3094 (1.1085)	grad_norm 3.5580 (4.1954)	loss_scale 1024.0000 (666.5017)	mem 12860MB
[2024-05-29 14:40:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2200/5004]	eta 0:24:15 lr 0.000000	 wd 0.0500	time 0.4279 (0.5191)	loss 0.7705 (1.1085)	grad_norm 4.1035 (4.2053)	loss_scale 1024.0000 (682.7442)	mem 12860MB
[2024-05-29 14:41:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2300/5004]	eta 0:23:22 lr 0.000000	 wd 0.0500	time 0.4235 (0.5187)	loss 0.7179 (1.1090)	grad_norm 2.7847 (4.1848)	loss_scale 1024.0000 (697.5750)	mem 12860MB
[2024-05-29 14:42:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2400/5004]	eta 0:22:33 lr 0.000000	 wd 0.0500	time 0.4048 (0.5200)	loss 0.7185 (1.1076)	grad_norm 2.4398 (4.2027)	loss_scale 1024.0000 (711.1703)	mem 12860MB
[2024-05-29 14:43:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2500/5004]	eta 0:21:43 lr 0.000000	 wd 0.0500	time 0.4735 (0.5207)	loss 1.3965 (1.1081)	grad_norm 2.8285 (4.1867)	loss_scale 1024.0000 (723.6785)	mem 12860MB
[2024-05-29 14:44:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2600/5004]	eta 0:20:51 lr 0.000000	 wd 0.0500	time 0.4470 (0.5204)	loss 1.0747 (1.1072)	grad_norm 4.8149 (4.1638)	loss_scale 1024.0000 (735.2249)	mem 12860MB
[2024-05-29 14:45:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2700/5004]	eta 0:19:58 lr 0.000000	 wd 0.0500	time 0.5997 (0.5203)	loss 1.4207 (1.1048)	grad_norm 2.9684 (4.1613)	loss_scale 1024.0000 (745.9163)	mem 12860MB
[2024-05-29 14:46:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2800/5004]	eta 0:19:07 lr 0.000000	 wd 0.0500	time 0.3827 (0.5208)	loss 0.9081 (1.1046)	grad_norm 3.3482 (4.1525)	loss_scale 1024.0000 (755.8443)	mem 12860MB
[2024-05-29 14:47:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][2900/5004]	eta 0:18:16 lr 0.000000	 wd 0.0500	time 0.3886 (0.5212)	loss 0.8258 (1.1037)	grad_norm 12.4776 (4.1808)	loss_scale 1024.0000 (765.0879)	mem 12860MB
[2024-05-29 14:47:57 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3000/5004]	eta 0:17:25 lr 0.000000	 wd 0.0500	time 0.4142 (0.5215)	loss 1.3226 (1.1052)	grad_norm 3.4224 (4.1979)	loss_scale 1024.0000 (773.7154)	mem 12860MB
[2024-05-29 14:48:49 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3100/5004]	eta 0:16:32 lr 0.000000	 wd 0.0500	time 0.4517 (0.5214)	loss 1.4025 (1.1052)	grad_norm 3.4588 (4.1849)	loss_scale 1024.0000 (781.7865)	mem 12860MB
[2024-05-29 14:49:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3200/5004]	eta 0:15:40 lr 0.000000	 wd 0.0500	time 0.4272 (0.5213)	loss 1.4090 (1.1066)	grad_norm 3.1283 (4.1860)	loss_scale 1024.0000 (789.3533)	mem 12860MB
[2024-05-29 14:50:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3300/5004]	eta 0:14:48 lr 0.000000	 wd 0.0500	time 0.4407 (0.5212)	loss 1.5128 (1.1066)	grad_norm 4.3086 (4.1769)	loss_scale 1024.0000 (796.4617)	mem 12860MB
[2024-05-29 14:51:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3400/5004]	eta 0:13:56 lr 0.000000	 wd 0.0500	time 0.4288 (0.5212)	loss 1.3700 (1.1063)	grad_norm 3.3656 (4.1670)	loss_scale 1024.0000 (803.1520)	mem 12860MB
[2024-05-29 14:52:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3500/5004]	eta 0:13:04 lr 0.000000	 wd 0.0500	time 0.4444 (0.5214)	loss 1.4146 (1.1064)	grad_norm 3.7323 (4.1712)	loss_scale 1024.0000 (809.4602)	mem 12860MB
[2024-05-29 14:53:13 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3600/5004]	eta 0:12:13 lr 0.000000	 wd 0.0500	time 0.3802 (0.5224)	loss 1.5072 (1.1071)	grad_norm 3.0591 (4.1727)	loss_scale 1024.0000 (815.4179)	mem 12860MB
[2024-05-29 14:54:05 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3700/5004]	eta 0:11:21 lr 0.000000	 wd 0.0500	time 0.4994 (0.5224)	loss 1.1681 (1.1081)	grad_norm 3.8792 (4.1822)	loss_scale 1024.0000 (821.0538)	mem 12860MB
[2024-05-29 14:54:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3800/5004]	eta 0:10:29 lr 0.000000	 wd 0.0500	time 0.4600 (0.5224)	loss 1.5193 (1.1086)	grad_norm 4.2515 (4.1803)	loss_scale 1024.0000 (826.3931)	mem 12860MB
[2024-05-29 14:55:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][3900/5004]	eta 0:09:38 lr 0.000000	 wd 0.0500	time 0.4359 (0.5238)	loss 1.3054 (1.1095)	grad_norm 2.8210 (4.1784)	loss_scale 1024.0000 (831.4586)	mem 12860MB
[2024-05-29 14:56:48 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4000/5004]	eta 0:08:45 lr 0.000000	 wd 0.0500	time 0.3898 (0.5238)	loss 1.3042 (1.1095)	grad_norm 3.0487 (4.1747)	loss_scale 1024.0000 (836.2709)	mem 12860MB
[2024-05-29 14:57:40 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4100/5004]	eta 0:07:53 lr 0.000000	 wd 0.0500	time 0.4695 (0.5237)	loss 0.8537 (1.1101)	grad_norm 2.9826 (4.1862)	loss_scale 1024.0000 (840.8486)	mem 12860MB
[2024-05-29 14:58:32 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4200/5004]	eta 0:07:01 lr 0.000000	 wd 0.0500	time 0.3973 (0.5238)	loss 1.1568 (1.1100)	grad_norm 2.9534 (4.1775)	loss_scale 1024.0000 (845.2083)	mem 12860MB
[2024-05-29 14:59:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4300/5004]	eta 0:06:08 lr 0.000000	 wd 0.0500	time 0.4317 (0.5239)	loss 1.1697 (1.1100)	grad_norm 3.5357 (4.1883)	loss_scale 1024.0000 (849.3653)	mem 12860MB
[2024-05-29 15:00:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4400/5004]	eta 0:05:16 lr 0.000000	 wd 0.0500	time 0.4686 (0.5239)	loss 1.3371 (1.1103)	grad_norm 3.1616 (4.1808)	loss_scale 1024.0000 (853.3333)	mem 12860MB
[2024-05-29 15:01:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4500/5004]	eta 0:04:24 lr 0.000000	 wd 0.0500	time 0.5508 (0.5238)	loss 0.7299 (1.1093)	grad_norm 3.0938 (4.1830)	loss_scale 1024.0000 (857.1251)	mem 12860MB
[2024-05-29 15:02:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4600/5004]	eta 0:03:31 lr 0.000000	 wd 0.0500	time 0.5259 (0.5237)	loss 0.9466 (1.1093)	grad_norm 5.3673 (4.1773)	loss_scale 1024.0000 (860.7520)	mem 12860MB
[2024-05-29 15:02:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4700/5004]	eta 0:02:39 lr 0.000000	 wd 0.0500	time 0.4335 (0.5238)	loss 1.4262 (1.1097)	grad_norm 3.1882 (4.1637)	loss_scale 1024.0000 (864.2246)	mem 12860MB
[2024-05-29 15:03:47 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4800/5004]	eta 0:01:46 lr 0.000000	 wd 0.0500	time 0.5185 (0.5240)	loss 1.1112 (1.1094)	grad_norm 2.5106 (4.1564)	loss_scale 1024.0000 (867.5526)	mem 12860MB
[2024-05-29 15:04:41 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][4900/5004]	eta 0:00:54 lr 0.000000	 wd 0.0500	time 0.4049 (0.5242)	loss 1.3428 (1.1095)	grad_norm 2.7690 (4.1546)	loss_scale 1024.0000 (870.7447)	mem 12860MB
[2024-05-29 15:05:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [28/30][5000/5004]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.4518 (0.5233)	loss 1.4372 (1.1084)	grad_norm 3.9991 (4.1448)	loss_scale 1024.0000 (873.8092)	mem 12860MB
[2024-05-29 15:05:37 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 28 training takes 0:43:44
[2024-05-29 15:05:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 8.465 (8.465)	Loss 0.3936 (0.3936)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 15:05:58 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.152 (0.215)	Loss 0.7378 (0.6322)	Acc@1 83.594 (87.960)	Acc@5 97.656 (98.294)	Mem 12860MB
[2024-05-29 15:06:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.846 Acc@5 98.054
[2024-05-29 15:06:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 15:06:16 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.85%
[2024-05-29 15:06:25 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][0/5004]	eta 13:10:53 lr 0.000000	 wd 0.0500	time 9.4831 (9.4831)	loss 1.3327 (1.3327)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 15:07:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][100/5004]	eta 0:47:33 lr 0.000000	 wd 0.0500	time 0.3814 (0.5819)	loss 1.2030 (1.1076)	grad_norm 4.2511 (4.3899)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 15:08:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][200/5004]	eta 0:43:18 lr 0.000000	 wd 0.0500	time 0.4170 (0.5409)	loss 0.8635 (1.1019)	grad_norm 2.8700 (4.4822)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 15:08:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][300/5004]	eta 0:41:16 lr 0.000000	 wd 0.0500	time 0.4145 (0.5264)	loss 1.2698 (1.1032)	grad_norm 3.4800 (4.3571)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 15:09:46 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][400/5004]	eta 0:40:15 lr 0.000000	 wd 0.0500	time 0.4551 (0.5247)	loss 1.4045 (1.1146)	grad_norm 2.8662 (4.2118)	loss_scale 1024.0000 (1024.0000)	mem 12860MB
[2024-05-29 15:10:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][500/5004]	eta 0:39:16 lr 0.000000	 wd 0.0500	time 0.3832 (0.5233)	loss 1.2498 (1.1086)	grad_norm 3.2785 (4.1313)	loss_scale 2048.0000 (1101.6687)	mem 12860MB
[2024-05-29 15:11:30 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][600/5004]	eta 0:38:20 lr 0.000000	 wd 0.0500	time 0.4905 (0.5224)	loss 1.5028 (1.1097)	grad_norm 3.3398 (4.0600)	loss_scale 2048.0000 (1259.1281)	mem 12860MB
[2024-05-29 15:12:21 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][700/5004]	eta 0:37:21 lr 0.000000	 wd 0.0500	time 0.3594 (0.5209)	loss 1.0703 (1.1095)	grad_norm 3.4270 (4.0591)	loss_scale 2048.0000 (1371.6633)	mem 12860MB
[2024-05-29 15:13:14 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][800/5004]	eta 0:36:35 lr 0.000000	 wd 0.0500	time 0.4432 (0.5222)	loss 0.8193 (1.1089)	grad_norm 4.4007 (nan)	loss_scale 1024.0000 (1361.4981)	mem 12860MB
[2024-05-29 15:14:04 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][900/5004]	eta 0:35:34 lr 0.000000	 wd 0.0500	time 0.5552 (0.5202)	loss 1.2052 (1.1054)	grad_norm 2.9964 (nan)	loss_scale 1024.0000 (1324.0400)	mem 12860MB
[2024-05-29 15:14:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1000/5004]	eta 0:34:37 lr 0.000000	 wd 0.0500	time 0.3906 (0.5189)	loss 1.1028 (1.1100)	grad_norm 3.6670 (nan)	loss_scale 1024.0000 (1294.0659)	mem 12860MB
[2024-05-29 15:15:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1100/5004]	eta 0:33:39 lr 0.000000	 wd 0.0500	time 0.4189 (0.5173)	loss 0.9683 (1.1093)	grad_norm 15.6644 (nan)	loss_scale 1024.0000 (1269.5368)	mem 12860MB
[2024-05-29 15:16:38 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1200/5004]	eta 0:32:50 lr 0.000000	 wd 0.0500	time 0.4048 (0.5179)	loss 1.1044 (1.1070)	grad_norm 4.1163 (nan)	loss_scale 1024.0000 (1249.0924)	mem 12860MB
[2024-05-29 15:17:29 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1300/5004]	eta 0:31:56 lr 0.000000	 wd 0.0500	time 0.4119 (0.5174)	loss 1.3952 (1.1027)	grad_norm 16.5737 (nan)	loss_scale 1024.0000 (1231.7909)	mem 12860MB
[2024-05-29 15:18:20 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1400/5004]	eta 0:31:03 lr 0.000000	 wd 0.0500	time 0.4353 (0.5170)	loss 1.3901 (1.1027)	grad_norm 3.1779 (nan)	loss_scale 1024.0000 (1216.9593)	mem 12860MB
[2024-05-29 15:19:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1500/5004]	eta 0:30:08 lr 0.000000	 wd 0.0500	time 0.4402 (0.5160)	loss 1.4247 (1.1036)	grad_norm 2.7079 (nan)	loss_scale 1024.0000 (1204.1039)	mem 12860MB
[2024-05-29 15:20:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1600/5004]	eta 0:29:17 lr 0.000000	 wd 0.0500	time 0.5029 (0.5165)	loss 0.8142 (1.1052)	grad_norm 3.8756 (nan)	loss_scale 1024.0000 (1192.8545)	mem 12860MB
[2024-05-29 15:20:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1700/5004]	eta 0:28:24 lr 0.000000	 wd 0.0500	time 0.4344 (0.5160)	loss 1.3333 (1.1057)	grad_norm 7.6615 (nan)	loss_scale 1024.0000 (1182.9277)	mem 12860MB
[2024-05-29 15:21:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1800/5004]	eta 0:27:31 lr 0.000000	 wd 0.0500	time 0.3808 (0.5154)	loss 0.8771 (1.1038)	grad_norm 20.2325 (nan)	loss_scale 1024.0000 (1174.1033)	mem 12860MB
[2024-05-29 15:22:35 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][1900/5004]	eta 0:26:39 lr 0.000000	 wd 0.0500	time 0.4115 (0.5154)	loss 0.7343 (1.1052)	grad_norm 3.4441 (nan)	loss_scale 1024.0000 (1166.2073)	mem 12860MB
[2024-05-29 15:23:28 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2000/5004]	eta 0:25:50 lr 0.000000	 wd 0.0500	time 0.4588 (0.5161)	loss 1.5557 (1.1046)	grad_norm 3.0217 (nan)	loss_scale 1024.0000 (1159.1004)	mem 12860MB
[2024-05-29 15:24:19 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2100/5004]	eta 0:24:58 lr 0.000000	 wd 0.0500	time 0.3643 (0.5158)	loss 0.8830 (1.1037)	grad_norm 3.2878 (nan)	loss_scale 1024.0000 (1152.6702)	mem 12860MB
[2024-05-29 15:25:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2200/5004]	eta 0:24:06 lr 0.000000	 wd 0.0500	time 0.4286 (0.5159)	loss 1.2115 (1.1023)	grad_norm 3.2513 (nan)	loss_scale 1024.0000 (1146.8242)	mem 12860MB
[2024-05-29 15:26:02 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2300/5004]	eta 0:23:14 lr 0.000000	 wd 0.0500	time 0.4107 (0.5156)	loss 0.6546 (1.1030)	grad_norm 2.8996 (nan)	loss_scale 1024.0000 (1141.4863)	mem 12860MB
[2024-05-29 15:26:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2400/5004]	eta 0:22:21 lr 0.000000	 wd 0.0500	time 0.4534 (0.5151)	loss 1.2198 (1.1029)	grad_norm 2.9450 (nan)	loss_scale 1024.0000 (1136.5931)	mem 12860MB
[2024-05-29 15:27:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2500/5004]	eta 0:21:29 lr 0.000000	 wd 0.0500	time 0.4373 (0.5149)	loss 0.9135 (1.1044)	grad_norm 2.6176 (nan)	loss_scale 1024.0000 (1132.0912)	mem 12860MB
[2024-05-29 15:28:36 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2600/5004]	eta 0:20:38 lr 0.000000	 wd 0.0500	time 0.3787 (0.5152)	loss 0.7057 (1.1053)	grad_norm 4.7892 (nan)	loss_scale 1024.0000 (1127.9354)	mem 12860MB
[2024-05-29 15:29:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2700/5004]	eta 0:19:46 lr 0.000000	 wd 0.0500	time 0.4208 (0.5150)	loss 0.8383 (1.1052)	grad_norm 2.8941 (nan)	loss_scale 1024.0000 (1124.0874)	mem 12860MB
[2024-05-29 15:30:18 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2800/5004]	eta 0:18:54 lr 0.000000	 wd 0.0500	time 0.3984 (0.5149)	loss 1.4477 (1.1037)	grad_norm 4.4297 (nan)	loss_scale 1024.0000 (1120.5141)	mem 12860MB
[2024-05-29 15:31:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][2900/5004]	eta 0:18:02 lr 0.000000	 wd 0.0500	time 0.4513 (0.5146)	loss 0.9745 (1.1028)	grad_norm 2.8851 (nan)	loss_scale 1024.0000 (1117.1872)	mem 12860MB
[2024-05-29 15:32:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3000/5004]	eta 0:17:11 lr 0.000000	 wd 0.0500	time 0.3731 (0.5145)	loss 1.5397 (1.1047)	grad_norm 3.6179 (nan)	loss_scale 1024.0000 (1114.0820)	mem 12860MB
[2024-05-29 15:32:52 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3100/5004]	eta 0:16:20 lr 0.000000	 wd 0.0500	time 0.4297 (0.5148)	loss 1.2875 (1.1042)	grad_norm 3.9155 (nan)	loss_scale 1024.0000 (1111.1770)	mem 12860MB
[2024-05-29 15:33:43 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3200/5004]	eta 0:15:28 lr 0.000000	 wd 0.0500	time 0.4988 (0.5146)	loss 1.1926 (1.1033)	grad_norm 3.6231 (nan)	loss_scale 1024.0000 (1108.4536)	mem 12860MB
[2024-05-29 15:34:34 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3300/5004]	eta 0:14:36 lr 0.000000	 wd 0.0500	time 0.4073 (0.5146)	loss 1.1627 (1.1037)	grad_norm 3.4198 (nan)	loss_scale 1024.0000 (1105.8952)	mem 12860MB
[2024-05-29 15:35:26 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3400/5004]	eta 0:13:45 lr 0.000000	 wd 0.0500	time 0.4253 (0.5147)	loss 1.1911 (1.1049)	grad_norm 3.6131 (nan)	loss_scale 1024.0000 (1103.4872)	mem 12860MB
[2024-05-29 15:36:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3500/5004]	eta 0:12:54 lr 0.000000	 wd 0.0500	time 0.4973 (0.5146)	loss 0.7304 (1.1044)	grad_norm 3.2329 (nan)	loss_scale 512.0000 (1090.9797)	mem 12860MB
[2024-05-29 15:37:09 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3600/5004]	eta 0:12:02 lr 0.000000	 wd 0.0500	time 0.4288 (0.5146)	loss 1.2139 (1.1051)	grad_norm 3.4684 (nan)	loss_scale 512.0000 (1074.9014)	mem 12860MB
[2024-05-29 15:38:01 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3700/5004]	eta 0:11:11 lr 0.000000	 wd 0.0500	time 0.4517 (0.5148)	loss 0.8288 (1.1048)	grad_norm 8.5108 (nan)	loss_scale 512.0000 (1059.6920)	mem 12860MB
[2024-05-29 15:38:53 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3800/5004]	eta 0:10:20 lr 0.000000	 wd 0.0500	time 0.3685 (0.5150)	loss 1.3557 (1.1061)	grad_norm 3.8372 (nan)	loss_scale 512.0000 (1045.2828)	mem 12860MB
[2024-05-29 15:39:45 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][3900/5004]	eta 0:09:28 lr 0.000000	 wd 0.0500	time 0.4074 (0.5151)	loss 0.6080 (1.1057)	grad_norm 2.7671 (nan)	loss_scale 512.0000 (1031.6124)	mem 12860MB
[2024-05-29 15:40:39 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4000/5004]	eta 0:08:37 lr 0.000000	 wd 0.0500	time 0.4366 (0.5157)	loss 1.1229 (1.1059)	grad_norm 3.3606 (nan)	loss_scale 512.0000 (1018.6253)	mem 12860MB
[2024-05-29 15:41:33 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4100/5004]	eta 0:07:46 lr 0.000000	 wd 0.0500	time 0.5264 (0.5162)	loss 0.9904 (1.1051)	grad_norm 3.4523 (nan)	loss_scale 512.0000 (1006.2716)	mem 12860MB
[2024-05-29 15:42:24 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4200/5004]	eta 0:06:55 lr 0.000000	 wd 0.0500	time 0.4160 (0.5163)	loss 0.9394 (1.1055)	grad_norm 2.9900 (nan)	loss_scale 512.0000 (994.5061)	mem 12860MB
[2024-05-29 15:43:17 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4300/5004]	eta 0:06:03 lr 0.000000	 wd 0.0500	time 0.4272 (0.5165)	loss 1.2993 (1.1059)	grad_norm 4.4625 (nan)	loss_scale 512.0000 (983.2876)	mem 12860MB
[2024-05-29 15:44:10 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4400/5004]	eta 0:05:12 lr 0.000000	 wd 0.0500	time 0.5133 (0.5169)	loss 0.8229 (1.1062)	grad_norm 3.3225 (nan)	loss_scale 512.0000 (972.5790)	mem 12860MB
[2024-05-29 15:45:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4500/5004]	eta 0:04:21 lr 0.000000	 wd 0.0500	time 0.4673 (0.5189)	loss 1.1607 (1.1067)	grad_norm 4.7879 (nan)	loss_scale 512.0000 (962.3461)	mem 12860MB
[2024-05-29 15:46:06 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4600/5004]	eta 0:03:29 lr 0.000000	 wd 0.0500	time 0.4362 (0.5196)	loss 0.8277 (1.1071)	grad_norm 3.7514 (nan)	loss_scale 512.0000 (952.5581)	mem 12860MB
[2024-05-29 15:47:00 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4700/5004]	eta 0:02:38 lr 0.000000	 wd 0.0500	time 0.4712 (0.5200)	loss 0.7244 (1.1074)	grad_norm 3.6526 (nan)	loss_scale 512.0000 (943.1866)	mem 12860MB
[2024-05-29 15:47:55 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4800/5004]	eta 0:01:46 lr 0.000000	 wd 0.0500	time 0.4885 (0.5206)	loss 1.1961 (1.1069)	grad_norm 2.5634 (nan)	loss_scale 512.0000 (934.2054)	mem 12860MB
[2024-05-29 15:48:54 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][4900/5004]	eta 0:00:54 lr 0.000000	 wd 0.0500	time 0.4308 (0.5219)	loss 0.9969 (1.1067)	grad_norm 3.3252 (nan)	loss_scale 512.0000 (925.5907)	mem 12860MB
[2024-05-29 15:49:44 smt_mam_v2_large_224_22kto1k_finetune] (main.py 226): INFO Train: [29/30][5000/5004]	eta 0:00:02 lr 0.000000	 wd 0.0500	time 0.3975 (0.5215)	loss 1.2334 (1.1069)	grad_norm 2.9654 (nan)	loss_scale 512.0000 (917.3205)	mem 12860MB
[2024-05-29 15:49:51 smt_mam_v2_large_224_22kto1k_finetune] (main.py 235): INFO EPOCH 29 training takes 0:43:35
[2024-05-29 15:49:59 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [0/196]	Time 7.574 (7.574)	Loss 0.3958 (0.3958)	Acc@1 95.703 (95.703)	Acc@5 99.609 (99.609)	Mem 12860MB
[2024-05-29 15:50:11 smt_mam_v2_large_224_22kto1k_finetune] (main.py 275): INFO Test: [100/196]	Time 0.156 (0.195)	Loss 0.7339 (0.6364)	Acc@1 83.594 (87.871)	Acc@5 97.656 (98.240)	Mem 12860MB
[2024-05-29 15:50:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 282): INFO  * Acc@1 86.770 Acc@5 98.036
[2024-05-29 15:50:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 166): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-05-29 15:50:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 168): INFO Max accuracy: 86.85%
[2024-05-29 15:50:27 smt_mam_v2_large_224_22kto1k_finetune] (main.py 175): INFO Training time 22:14:33
