[2024-08-05 21:47:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/config.json
[2024-08-05 21:47:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: part1
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1
PRINT_FREQ: 100
SAVE_FREQ: 30
SEED: 0
TAG: diffusion_ft_smt_l_step_cross1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-05 21:47:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_step_cross1.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_step_cross1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-05 21:47:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1
[2024-08-05 21:47:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-05 21:47:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 113): INFO number of params: 41308456
[2024-08-05 21:47:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1, ignoring auto resume
[2024-08-05 21:47:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth for fine-tuning......
[2024-08-05 21:47:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-05 21:47:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth'
[2024-08-05 21:48:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 14.565 (14.565)	Loss 0.4443 (0.4443)	Acc@1 92.383 (92.383)	Acc@5 99.023 (99.023)	Mem 2521MB
[2024-08-05 21:48:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.756 Acc@5 98.030
[2024-08-05 21:48:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 21:48:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 168): INFO Start training
[2024-08-05 21:48:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:03:03 lr 0.000000	 wd 0.0500	time 13.0228 (13.0228)	loss 1.4674 (1.4674)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 20177MB
[2024-08-05 21:49:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:29:21 lr 0.000000	 wd 0.0500	time 0.5812 (0.7334)	loss 1.2631 (1.1556)	grad_norm 2.7033 (nan)	loss_scale 16384.0000 (18168.3960)	mem 20177MB
[2024-08-05 21:50:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:25:46 lr 0.000000	 wd 0.0500	time 0.5768 (0.6719)	loss 1.0042 (1.1491)	grad_norm 2.5167 (nan)	loss_scale 8192.0000 (13938.6269)	mem 20177MB
[2024-08-05 21:51:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:23:55 lr 0.000000	 wd 0.0500	time 0.5886 (0.6517)	loss 0.8686 (1.1115)	grad_norm 2.4713 (nan)	loss_scale 8192.0000 (12029.4485)	mem 20177MB
[2024-08-05 21:52:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:22:29 lr 0.000001	 wd 0.0500	time 0.5971 (0.6420)	loss 1.0699 (1.1187)	grad_norm 1.7924 (nan)	loss_scale 8192.0000 (11072.4788)	mem 20177MB
[2024-08-05 21:53:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:21:13 lr 0.000001	 wd 0.0500	time 0.5856 (0.6360)	loss 1.1441 (1.1201)	grad_norm 8.8809 (nan)	loss_scale 8192.0000 (10497.5329)	mem 20177MB
[2024-08-05 21:54:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:20:02 lr 0.000001	 wd 0.0500	time 0.5918 (0.6320)	loss 1.2265 (1.1189)	grad_norm 2.9704 (nan)	loss_scale 4096.0000 (9459.6473)	mem 20177MB
[2024-08-05 21:55:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:18:53 lr 0.000001	 wd 0.0500	time 0.5890 (0.6292)	loss 1.2304 (1.1212)	grad_norm 3.1258 (nan)	loss_scale 4096.0000 (8694.5050)	mem 20177MB
[2024-08-05 21:56:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:17:47 lr 0.000001	 wd 0.0500	time 0.5949 (0.6272)	loss 1.0354 (1.1202)	grad_norm 1.9175 (nan)	loss_scale 4096.0000 (8120.4095)	mem 20177MB
[2024-08-05 21:57:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:16:42 lr 0.000001	 wd 0.0500	time 0.5939 (0.6256)	loss 1.2694 (1.1205)	grad_norm 3.1021 (nan)	loss_scale 4096.0000 (7673.7492)	mem 20177MB
[2024-08-05 21:58:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:15:37 lr 0.000002	 wd 0.0500	time 0.5853 (0.6243)	loss 1.3870 (1.1179)	grad_norm 2.4214 (nan)	loss_scale 4096.0000 (7316.3317)	mem 20177MB
[2024-08-05 21:59:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:14:33 lr 0.000002	 wd 0.0500	time 0.5832 (0.6232)	loss 1.2970 (1.1195)	grad_norm 1.7451 (nan)	loss_scale 4096.0000 (7023.8401)	mem 20177MB
[2024-08-05 22:00:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:13:30 lr 0.000002	 wd 0.0500	time 0.5787 (0.6223)	loss 0.9686 (1.1191)	grad_norm 2.1693 (nan)	loss_scale 4096.0000 (6780.0566)	mem 20177MB
[2024-08-05 22:01:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:12:27 lr 0.000002	 wd 0.0500	time 0.5844 (0.6216)	loss 1.1692 (1.1208)	grad_norm 2.7805 (nan)	loss_scale 4096.0000 (6573.7494)	mem 20177MB
[2024-08-05 22:03:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:11:24 lr 0.000002	 wd 0.0500	time 0.5777 (0.6208)	loss 1.4089 (1.1219)	grad_norm 1.6998 (nan)	loss_scale 4096.0000 (6396.8936)	mem 20177MB
[2024-08-05 22:04:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:10:21 lr 0.000002	 wd 0.0500	time 0.5835 (0.6203)	loss 0.9619 (1.1251)	grad_norm 1.8959 (nan)	loss_scale 4096.0000 (6243.6029)	mem 20177MB
[2024-08-05 22:05:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:09:19 lr 0.000003	 wd 0.0500	time 0.5881 (0.6198)	loss 1.1987 (1.1225)	grad_norm 1.5965 (nan)	loss_scale 4096.0000 (6109.4616)	mem 20177MB
[2024-08-05 22:06:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:08:16 lr 0.000003	 wd 0.0500	time 0.5850 (0.6194)	loss 0.9494 (1.1221)	grad_norm 2.1566 (nan)	loss_scale 4096.0000 (5991.0923)	mem 20177MB
[2024-08-05 22:07:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:07:14 lr 0.000003	 wd 0.0500	time 0.5997 (0.6191)	loss 1.0343 (1.1222)	grad_norm 3.2352 (nan)	loss_scale 4096.0000 (5885.8679)	mem 20177MB
[2024-08-05 22:08:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:06:12 lr 0.000003	 wd 0.0500	time 0.5826 (0.6188)	loss 1.4529 (1.1220)	grad_norm 1.8801 (nan)	loss_scale 4096.0000 (5791.7138)	mem 20177MB
[2024-08-05 22:09:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:05:10 lr 0.000003	 wd 0.0500	time 0.5892 (0.6185)	loss 0.7931 (1.1210)	grad_norm 2.0234 (nan)	loss_scale 4096.0000 (5706.9705)	mem 20177MB
[2024-08-05 22:10:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:04:08 lr 0.000003	 wd 0.0500	time 0.5956 (0.6182)	loss 0.9586 (1.1199)	grad_norm 2.0241 (nan)	loss_scale 4096.0000 (5630.2941)	mem 20177MB
[2024-08-05 22:11:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:03:06 lr 0.000004	 wd 0.0500	time 0.5859 (0.6179)	loss 1.4393 (1.1190)	grad_norm 2.2909 (nan)	loss_scale 4096.0000 (5560.5852)	mem 20177MB
[2024-08-05 22:12:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:02:04 lr 0.000004	 wd 0.0500	time 0.5930 (0.6177)	loss 1.3487 (1.1189)	grad_norm 2.0122 (nan)	loss_scale 4096.0000 (5496.9352)	mem 20177MB
[2024-08-05 22:13:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:02 lr 0.000004	 wd 0.0500	time 0.5833 (0.6175)	loss 1.2718 (1.1185)	grad_norm 2.2174 (nan)	loss_scale 4096.0000 (5438.5873)	mem 20177MB
[2024-08-05 22:14:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.5905 (0.6173)	loss 0.9681 (1.1171)	grad_norm 1.8705 (nan)	loss_scale 4096.0000 (5384.9052)	mem 20177MB
[2024-08-05 22:14:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 0 training takes 0:25:46
[2024-08-05 22:14:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_0.pth saving......
[2024-08-05 22:14:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_0.pth saved !!!
[2024-08-05 22:14:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.243 (11.243)	Loss 0.4910 (0.4910)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-05 22:14:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.754 Acc@5 98.020
[2024-08-05 22:14:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 22:14:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.75%
[2024-08-05 22:14:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-05 22:14:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-05 22:15:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:39:48 lr 0.000004	 wd 0.0500	time 11.0267 (11.0267)	loss 1.2844 (1.2844)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-05 22:16:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:28:34 lr 0.000004	 wd 0.0500	time 0.5826 (0.7140)	loss 1.2206 (1.1049)	grad_norm 3.6301 (3.1503)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-05 22:17:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:25:25 lr 0.000004	 wd 0.0500	time 0.5834 (0.6628)	loss 0.7653 (1.1064)	grad_norm 1.8516 (2.9163)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-05 22:18:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:23:42 lr 0.000004	 wd 0.0500	time 0.5890 (0.6459)	loss 0.8792 (1.1098)	grad_norm 1.8495 (2.7541)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-05 22:19:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:22:20 lr 0.000005	 wd 0.0500	time 0.5902 (0.6375)	loss 0.8740 (1.1243)	grad_norm 2.0485 (2.8381)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-05 22:20:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:21:06 lr 0.000005	 wd 0.0500	time 0.5803 (0.6324)	loss 1.4883 (1.1274)	grad_norm 2.6690 (2.7881)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-05 22:21:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:19:56 lr 0.000005	 wd 0.0500	time 0.5862 (0.6289)	loss 1.3784 (1.1267)	grad_norm 1.8540 (nan)	loss_scale 2048.0000 (4082.3694)	mem 20177MB
[2024-08-05 22:22:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:18:49 lr 0.000005	 wd 0.0500	time 0.5891 (0.6267)	loss 0.8235 (1.1232)	grad_norm 2.5548 (nan)	loss_scale 2048.0000 (3792.1598)	mem 20177MB
[2024-08-05 22:23:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:17:43 lr 0.000005	 wd 0.0500	time 0.5852 (0.6249)	loss 1.5027 (1.1227)	grad_norm 2.4303 (nan)	loss_scale 2048.0000 (3574.4120)	mem 20177MB
[2024-08-05 22:24:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:16:38 lr 0.000005	 wd 0.0500	time 0.5995 (0.6236)	loss 1.1231 (1.1209)	grad_norm 2.9141 (nan)	loss_scale 2048.0000 (3404.9989)	mem 20177MB
[2024-08-05 22:25:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:15:34 lr 0.000006	 wd 0.0500	time 0.5922 (0.6225)	loss 1.4059 (1.1177)	grad_norm 2.7576 (nan)	loss_scale 2048.0000 (3269.4346)	mem 20177MB
[2024-08-05 22:26:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:14:31 lr 0.000006	 wd 0.0500	time 0.6022 (0.6215)	loss 1.4646 (1.1168)	grad_norm 2.0206 (nan)	loss_scale 2048.0000 (3158.4959)	mem 20177MB
[2024-08-05 22:27:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:13:28 lr 0.000006	 wd 0.0500	time 0.5892 (0.6208)	loss 0.9047 (1.1150)	grad_norm 1.9203 (nan)	loss_scale 2048.0000 (3066.0316)	mem 20177MB
[2024-08-05 22:28:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:12:25 lr 0.000006	 wd 0.0500	time 0.5916 (0.6202)	loss 1.1891 (1.1164)	grad_norm 2.0739 (nan)	loss_scale 2048.0000 (2987.7817)	mem 20177MB
[2024-08-05 22:29:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:11:22 lr 0.000006	 wd 0.0500	time 0.5895 (0.6197)	loss 0.7781 (1.1164)	grad_norm 5.6742 (nan)	loss_scale 2048.0000 (2920.7024)	mem 20177MB
[2024-08-05 22:30:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:10:20 lr 0.000006	 wd 0.0500	time 0.5890 (0.6192)	loss 1.3834 (1.1165)	grad_norm 1.7758 (nan)	loss_scale 2048.0000 (2862.5610)	mem 20177MB
[2024-08-05 22:31:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:09:18 lr 0.000007	 wd 0.0500	time 0.5911 (0.6188)	loss 1.2902 (1.1175)	grad_norm 2.5046 (nan)	loss_scale 2048.0000 (2811.6827)	mem 20177MB
[2024-08-05 22:32:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:08:15 lr 0.000007	 wd 0.0500	time 0.5884 (0.6184)	loss 0.8085 (1.1181)	grad_norm 2.8214 (nan)	loss_scale 2048.0000 (2766.7866)	mem 20177MB
[2024-08-05 22:33:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:07:13 lr 0.000007	 wd 0.0500	time 0.5834 (0.6181)	loss 1.5816 (1.1173)	grad_norm 2.5638 (nan)	loss_scale 2048.0000 (2726.8762)	mem 20177MB
[2024-08-05 22:34:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:06:11 lr 0.000007	 wd 0.0500	time 0.5930 (0.6178)	loss 1.4310 (1.1191)	grad_norm 2.6774 (nan)	loss_scale 2048.0000 (2691.1647)	mem 20177MB
[2024-08-05 22:35:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:05:10 lr 0.000007	 wd 0.0500	time 0.5777 (0.6176)	loss 1.2218 (1.1196)	grad_norm 9.2987 (nan)	loss_scale 1024.0000 (2631.3883)	mem 20177MB
[2024-08-05 22:36:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:04:08 lr 0.000007	 wd 0.0500	time 0.6051 (0.6173)	loss 1.3432 (1.1216)	grad_norm 2.8041 (nan)	loss_scale 1024.0000 (2554.8824)	mem 20177MB
[2024-08-05 22:37:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:03:06 lr 0.000008	 wd 0.0500	time 0.5869 (0.6172)	loss 1.0806 (1.1220)	grad_norm 3.3918 (nan)	loss_scale 1024.0000 (2485.3285)	mem 20177MB
[2024-08-05 22:38:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:02:04 lr 0.000008	 wd 0.0500	time 0.5808 (0.6170)	loss 0.7666 (1.1234)	grad_norm 4.5311 (nan)	loss_scale 1024.0000 (2421.8201)	mem 20177MB
[2024-08-05 22:39:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:02 lr 0.000008	 wd 0.0500	time 0.5815 (0.6168)	loss 1.1822 (1.1234)	grad_norm 2.1399 (nan)	loss_scale 1024.0000 (2363.6018)	mem 20177MB
[2024-08-05 22:40:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.5893 (0.6166)	loss 0.8873 (1.1232)	grad_norm 1.6762 (nan)	loss_scale 1024.0000 (2310.0392)	mem 20177MB
[2024-08-05 22:40:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 1 training takes 0:25:45
[2024-08-05 22:40:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.527 (12.527)	Loss 0.4834 (0.4834)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-05 22:41:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.712 Acc@5 98.012
[2024-08-05 22:41:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-05 22:41:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.75%
[2024-08-05 22:41:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][0/2502]	eta 8:35:54 lr 0.000008	 wd 0.0500	time 12.3719 (12.3719)	loss 1.1059 (1.1059)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:42:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:29:06 lr 0.000008	 wd 0.0500	time 0.5883 (0.7272)	loss 1.3923 (1.1235)	grad_norm 2.1688 (2.8291)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:43:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:25:41 lr 0.000008	 wd 0.0500	time 0.5873 (0.6698)	loss 1.2351 (1.1144)	grad_norm 1.5849 (2.6428)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:44:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:23:52 lr 0.000008	 wd 0.0500	time 0.5639 (0.6507)	loss 1.1924 (1.1039)	grad_norm 2.1423 (2.6197)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:45:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:22:27 lr 0.000009	 wd 0.0500	time 0.5935 (0.6412)	loss 1.3080 (1.1085)	grad_norm 2.9386 (2.6837)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:46:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:21:12 lr 0.000009	 wd 0.0500	time 0.5854 (0.6354)	loss 1.2923 (1.1070)	grad_norm 2.4288 (2.7290)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:47:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:20:01 lr 0.000009	 wd 0.0500	time 0.5900 (0.6316)	loss 1.2158 (1.1061)	grad_norm 2.2441 (2.6450)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:48:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:18:53 lr 0.000009	 wd 0.0500	time 0.5769 (0.6290)	loss 1.2951 (1.1103)	grad_norm 3.6408 (2.5970)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:49:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:17:47 lr 0.000009	 wd 0.0500	time 0.5904 (0.6270)	loss 1.1752 (1.1132)	grad_norm 1.9291 (2.5790)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:50:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:16:42 lr 0.000009	 wd 0.0500	time 0.5937 (0.6255)	loss 0.8029 (1.1169)	grad_norm 2.1919 (2.5846)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:51:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:15:37 lr 0.000010	 wd 0.0500	time 0.5936 (0.6243)	loss 0.8551 (1.1147)	grad_norm 2.0432 (2.7100)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:52:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:14:33 lr 0.000010	 wd 0.0500	time 0.5931 (0.6232)	loss 1.3570 (1.1137)	grad_norm 2.3667 (2.7086)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:53:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:13:30 lr 0.000010	 wd 0.0500	time 0.5878 (0.6223)	loss 0.9722 (1.1143)	grad_norm 2.6139 (2.6768)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:54:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:12:27 lr 0.000010	 wd 0.0500	time 0.5986 (0.6216)	loss 1.3610 (1.1146)	grad_norm 2.0106 (2.6572)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:55:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:11:24 lr 0.000010	 wd 0.0500	time 0.5869 (0.6209)	loss 1.1723 (1.1161)	grad_norm 1.8607 (2.7166)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:56:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:10:21 lr 0.000010	 wd 0.0500	time 0.5918 (0.6203)	loss 1.3274 (1.1166)	grad_norm 2.3259 (2.6974)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:57:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:09:19 lr 0.000011	 wd 0.0500	time 0.5838 (0.6199)	loss 0.9148 (1.1168)	grad_norm 1.4724 (2.6919)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:58:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:08:16 lr 0.000011	 wd 0.0500	time 0.5915 (0.6195)	loss 1.1873 (1.1168)	grad_norm 1.9155 (2.6771)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 22:59:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:07:14 lr 0.000011	 wd 0.0500	time 0.5744 (0.6191)	loss 1.1166 (1.1177)	grad_norm 2.1458 (2.6672)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:00:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:06:12 lr 0.000011	 wd 0.0500	time 0.5849 (0.6187)	loss 1.2985 (1.1174)	grad_norm 2.5530 (2.6551)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:01:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:05:10 lr 0.000011	 wd 0.0500	time 0.5873 (0.6184)	loss 0.9484 (1.1174)	grad_norm 1.8738 (2.6589)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:02:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:04:08 lr 0.000011	 wd 0.0500	time 0.5819 (0.6182)	loss 0.9606 (1.1166)	grad_norm 1.9647 (2.6447)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:03:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:03:06 lr 0.000012	 wd 0.0500	time 0.5814 (0.6179)	loss 1.2170 (1.1171)	grad_norm 11.6960 (2.6612)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:04:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:02:04 lr 0.000012	 wd 0.0500	time 0.5822 (0.6177)	loss 1.0700 (1.1169)	grad_norm 1.7367 (2.6711)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:05:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:02 lr 0.000012	 wd 0.0500	time 0.5829 (0.6175)	loss 0.7729 (1.1175)	grad_norm 2.0400 (2.6731)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:06:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.5618 (0.6173)	loss 1.1671 (1.1174)	grad_norm 2.2174 (2.6651)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:06:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 2 training takes 0:25:46
[2024-08-05 23:07:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.897 (11.897)	Loss 0.4634 (0.4634)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-05 23:07:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.736 Acc@5 98.022
[2024-08-05 23:07:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-05 23:07:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.75%
[2024-08-05 23:07:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:28:22 lr 0.000012	 wd 0.0500	time 12.1914 (12.1914)	loss 0.6609 (0.6609)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:08:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:29:05 lr 0.000012	 wd 0.0500	time 0.5827 (0.7268)	loss 1.2182 (1.1536)	grad_norm 2.1143 (2.6706)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:09:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:25:40 lr 0.000012	 wd 0.0500	time 0.5901 (0.6690)	loss 1.4432 (1.1291)	grad_norm 1.7547 (2.5329)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:10:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:23:51 lr 0.000012	 wd 0.0500	time 0.5877 (0.6501)	loss 1.4034 (1.1298)	grad_norm 2.0105 (2.5695)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:11:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:22:26 lr 0.000013	 wd 0.0500	time 0.5755 (0.6407)	loss 1.3535 (1.1209)	grad_norm 1.7681 (2.5302)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:12:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:21:11 lr 0.000013	 wd 0.0500	time 0.5972 (0.6350)	loss 0.7953 (1.1211)	grad_norm 2.2176 (2.5044)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:13:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:20:00 lr 0.000013	 wd 0.0500	time 0.5933 (0.6313)	loss 1.0457 (1.1134)	grad_norm 1.8549 (2.5161)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:14:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:18:52 lr 0.000013	 wd 0.0500	time 0.5892 (0.6285)	loss 1.3105 (1.1117)	grad_norm 3.9019 (2.5825)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:15:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:17:46 lr 0.000013	 wd 0.0500	time 0.5919 (0.6266)	loss 0.9233 (1.1067)	grad_norm 2.0388 (2.5756)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:16:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:16:41 lr 0.000013	 wd 0.0500	time 0.5914 (0.6250)	loss 1.5343 (1.1094)	grad_norm 2.8359 (2.5906)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-05 23:17:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:15:36 lr 0.000014	 wd 0.0500	time 0.5907 (0.6238)	loss 1.2406 (1.1090)	grad_norm 1.9292 (2.6005)	loss_scale 2048.0000 (1083.3327)	mem 20177MB
[2024-08-05 23:18:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:14:33 lr 0.000014	 wd 0.0500	time 0.5979 (0.6228)	loss 0.7741 (1.1099)	grad_norm 2.4731 (2.6169)	loss_scale 2048.0000 (1170.9500)	mem 20177MB
[2024-08-05 23:19:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:13:29 lr 0.000014	 wd 0.0500	time 0.5824 (0.6220)	loss 1.1388 (1.1076)	grad_norm 2.0670 (2.6030)	loss_scale 2048.0000 (1243.9767)	mem 20177MB
[2024-08-05 23:20:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:12:26 lr 0.000014	 wd 0.0500	time 0.5873 (0.6212)	loss 1.2054 (1.1074)	grad_norm 1.6604 (2.5994)	loss_scale 2048.0000 (1305.7771)	mem 20177MB
[2024-08-05 23:22:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:11:23 lr 0.000014	 wd 0.0500	time 0.5812 (0.6206)	loss 1.1508 (1.1079)	grad_norm 1.4858 (2.6007)	loss_scale 2048.0000 (1358.7552)	mem 20177MB
[2024-08-05 23:23:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:10:21 lr 0.000014	 wd 0.0500	time 0.5996 (0.6201)	loss 1.3529 (1.1064)	grad_norm 2.3416 (2.5774)	loss_scale 2048.0000 (1404.6742)	mem 20177MB
[2024-08-05 23:24:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:09:18 lr 0.000015	 wd 0.0500	time 0.5906 (0.6196)	loss 0.6833 (1.1039)	grad_norm 2.1141 (2.6059)	loss_scale 2048.0000 (1444.8570)	mem 20177MB
[2024-08-05 23:25:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:08:16 lr 0.000015	 wd 0.0500	time 0.5923 (0.6192)	loss 0.9632 (1.1065)	grad_norm 2.4947 (2.6310)	loss_scale 2048.0000 (1480.3151)	mem 20177MB
[2024-08-05 23:26:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:07:14 lr 0.000015	 wd 0.0500	time 0.5950 (0.6188)	loss 1.2316 (1.1079)	grad_norm 2.4506 (2.6436)	loss_scale 2048.0000 (1511.8356)	mem 20177MB
[2024-08-05 23:27:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:06:12 lr 0.000015	 wd 0.0500	time 0.5942 (0.6185)	loss 1.3471 (1.1065)	grad_norm 1.7628 (2.6265)	loss_scale 2048.0000 (1540.0400)	mem 20177MB
[2024-08-05 23:28:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:05:10 lr 0.000015	 wd 0.0500	time 0.5860 (0.6182)	loss 1.1952 (1.1061)	grad_norm 1.7841 (2.6061)	loss_scale 2048.0000 (1565.4253)	mem 20177MB
[2024-08-05 23:29:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:04:08 lr 0.000015	 wd 0.0500	time 0.6028 (0.6180)	loss 0.7817 (1.1061)	grad_norm 1.7570 (2.5884)	loss_scale 2048.0000 (1588.3941)	mem 20177MB
[2024-08-05 23:30:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:03:06 lr 0.000016	 wd 0.0500	time 0.5890 (0.6177)	loss 0.7693 (1.1052)	grad_norm 2.9885 (2.6028)	loss_scale 2048.0000 (1609.2758)	mem 20177MB
[2024-08-05 23:31:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:02:04 lr 0.000016	 wd 0.0500	time 0.5804 (0.6175)	loss 0.8481 (1.1056)	grad_norm 2.1409 (2.5892)	loss_scale 2048.0000 (1628.3425)	mem 20177MB
[2024-08-05 23:32:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:02 lr 0.000016	 wd 0.0500	time 0.5907 (0.6173)	loss 1.2008 (1.1068)	grad_norm 2.7457 (2.5827)	loss_scale 2048.0000 (1645.8209)	mem 20177MB
[2024-08-05 23:33:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5882 (0.6171)	loss 1.1860 (1.1068)	grad_norm 2.2464 (2.5699)	loss_scale 2048.0000 (1661.9016)	mem 20177MB
[2024-08-05 23:33:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 3 training takes 0:25:46
[2024-08-05 23:33:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.240 (12.240)	Loss 0.4705 (0.4705)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-05 23:33:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.778 Acc@5 97.996
[2024-08-05 23:33:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 23:33:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-05 23:33:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-05 23:33:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-05 23:34:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:09:41 lr 0.000016	 wd 0.0500	time 10.3044 (10.3044)	loss 1.2075 (1.2075)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:35:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:28:24 lr 0.000016	 wd 0.0500	time 0.5840 (0.7098)	loss 0.8861 (1.1207)	grad_norm 4.2176 (2.6265)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:36:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:25:21 lr 0.000016	 wd 0.0500	time 0.5643 (0.6610)	loss 1.0810 (1.1096)	grad_norm 9.4334 (2.7198)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:37:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:23:40 lr 0.000016	 wd 0.0500	time 0.5922 (0.6452)	loss 0.7397 (1.1059)	grad_norm 3.4248 (2.6701)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:38:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:22:18 lr 0.000017	 wd 0.0500	time 0.5819 (0.6369)	loss 1.3266 (1.1035)	grad_norm 2.6997 (2.6172)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:39:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:21:05 lr 0.000017	 wd 0.0500	time 0.5864 (0.6321)	loss 1.1490 (1.1042)	grad_norm 2.0893 (2.5982)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:40:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:19:56 lr 0.000017	 wd 0.0500	time 0.6010 (0.6289)	loss 0.9578 (1.1052)	grad_norm 3.1574 (2.6146)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:41:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:18:48 lr 0.000017	 wd 0.0500	time 0.6031 (0.6265)	loss 0.9453 (1.1059)	grad_norm 1.9906 (2.5680)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:42:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:17:43 lr 0.000017	 wd 0.0500	time 0.5993 (0.6247)	loss 0.7216 (1.1080)	grad_norm 3.9065 (2.5753)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:43:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:16:38 lr 0.000017	 wd 0.0500	time 0.5904 (0.6233)	loss 0.7900 (1.1086)	grad_norm 1.7512 (2.5642)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:44:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:15:34 lr 0.000018	 wd 0.0500	time 0.5810 (0.6222)	loss 1.3958 (1.1096)	grad_norm 1.7761 (2.5562)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:45:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:14:30 lr 0.000018	 wd 0.0500	time 0.5634 (0.6212)	loss 1.4025 (1.1110)	grad_norm 2.0773 (2.5545)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:46:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:13:27 lr 0.000018	 wd 0.0500	time 0.5913 (0.6205)	loss 1.2335 (1.1091)	grad_norm 1.8709 (2.5517)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:47:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:12:25 lr 0.000018	 wd 0.0500	time 0.5888 (0.6199)	loss 0.9080 (1.1085)	grad_norm 5.7020 (2.5594)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:48:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:11:22 lr 0.000018	 wd 0.0500	time 0.5900 (0.6194)	loss 1.3564 (1.1087)	grad_norm 2.8184 (2.5362)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:49:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:10:20 lr 0.000018	 wd 0.0500	time 0.5851 (0.6189)	loss 1.1070 (1.1087)	grad_norm 2.3646 (2.5300)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:50:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:09:17 lr 0.000019	 wd 0.0500	time 0.5928 (0.6185)	loss 1.2656 (1.1071)	grad_norm 2.3101 (2.5368)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:51:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:08:15 lr 0.000019	 wd 0.0500	time 0.5903 (0.6182)	loss 1.1102 (1.1070)	grad_norm 2.7774 (2.5334)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:52:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:07:13 lr 0.000019	 wd 0.0500	time 0.5885 (0.6179)	loss 1.4581 (1.1083)	grad_norm 1.9121 (2.5338)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:53:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:06:11 lr 0.000019	 wd 0.0500	time 0.5790 (0.6175)	loss 1.2652 (1.1084)	grad_norm 1.9365 (2.5485)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:54:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:05:09 lr 0.000019	 wd 0.0500	time 0.5916 (0.6173)	loss 0.7775 (1.1075)	grad_norm 2.2883 (2.5436)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:55:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:04:08 lr 0.000019	 wd 0.0500	time 0.5913 (0.6171)	loss 0.8846 (1.1062)	grad_norm 1.9106 (2.5386)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:56:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:03:06 lr 0.000020	 wd 0.0500	time 0.5938 (0.6169)	loss 0.8891 (1.1064)	grad_norm 2.1650 (2.5589)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:57:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:02:04 lr 0.000020	 wd 0.0500	time 0.5855 (0.6167)	loss 0.7417 (1.1066)	grad_norm 2.3434 (2.5460)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:58:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:02 lr 0.000020	 wd 0.0500	time 0.5921 (0.6165)	loss 0.6834 (1.1058)	grad_norm 1.9466 (2.5541)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-05 23:59:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5848 (0.6163)	loss 1.0466 (1.1063)	grad_norm 2.1164 (2.5604)	loss_scale 4096.0000 (2097.1323)	mem 20177MB
[2024-08-05 23:59:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 4 training takes 0:25:44
[2024-08-05 23:59:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.212 (12.212)	Loss 0.4565 (0.4565)	Acc@1 92.383 (92.383)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 00:00:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.726 Acc@5 98.046
[2024-08-06 00:00:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 00:00:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 00:00:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:07:27 lr 0.000020	 wd 0.0500	time 11.6897 (11.6897)	loss 1.3087 (1.3087)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-06 00:01:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:28:57 lr 0.000020	 wd 0.0500	time 0.5921 (0.7234)	loss 0.9502 (1.1301)	grad_norm 2.5884 (nan)	loss_scale 2048.0000 (2473.8218)	mem 20177MB
[2024-08-06 00:02:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:25:36 lr 0.000020	 wd 0.0500	time 0.5857 (0.6675)	loss 1.1422 (1.0989)	grad_norm 3.8888 (nan)	loss_scale 2048.0000 (2261.9701)	mem 20177MB
[2024-08-06 00:03:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:23:49 lr 0.000020	 wd 0.0500	time 0.5884 (0.6490)	loss 0.7672 (1.0933)	grad_norm 4.3117 (nan)	loss_scale 2048.0000 (2190.8837)	mem 20177MB
[2024-08-06 00:04:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:22:24 lr 0.000020	 wd 0.0500	time 0.5714 (0.6396)	loss 1.3942 (1.1009)	grad_norm 2.2127 (nan)	loss_scale 2048.0000 (2155.2519)	mem 20177MB
[2024-08-06 00:05:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:21:09 lr 0.000020	 wd 0.0500	time 0.5892 (0.6341)	loss 0.9317 (1.0992)	grad_norm 2.1915 (nan)	loss_scale 2048.0000 (2133.8443)	mem 20177MB
[2024-08-06 00:06:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:19:59 lr 0.000020	 wd 0.0500	time 0.5861 (0.6305)	loss 0.8634 (1.0998)	grad_norm 3.1098 (nan)	loss_scale 2048.0000 (2119.5607)	mem 20177MB
[2024-08-06 00:07:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:18:51 lr 0.000020	 wd 0.0500	time 0.5903 (0.6279)	loss 0.8764 (1.0998)	grad_norm 2.0607 (nan)	loss_scale 2048.0000 (2109.3524)	mem 20177MB
[2024-08-06 00:08:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:17:45 lr 0.000020	 wd 0.0500	time 0.5928 (0.6259)	loss 1.4024 (1.0979)	grad_norm 1.7916 (nan)	loss_scale 2048.0000 (2101.6929)	mem 20177MB
[2024-08-06 00:09:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:16:40 lr 0.000020	 wd 0.0500	time 0.5899 (0.6244)	loss 0.9772 (1.0973)	grad_norm 2.6917 (nan)	loss_scale 2048.0000 (2095.7336)	mem 20177MB
[2024-08-06 00:10:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:15:35 lr 0.000020	 wd 0.0500	time 0.5874 (0.6232)	loss 1.0654 (1.0962)	grad_norm 2.3598 (nan)	loss_scale 2048.0000 (2090.9650)	mem 20177MB
[2024-08-06 00:11:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:14:32 lr 0.000020	 wd 0.0500	time 0.5874 (0.6222)	loss 1.1290 (1.0976)	grad_norm 5.8181 (nan)	loss_scale 2048.0000 (2087.0627)	mem 20177MB
[2024-08-06 00:12:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:13:29 lr 0.000020	 wd 0.0500	time 0.5884 (0.6214)	loss 1.3814 (1.0968)	grad_norm 3.9967 (nan)	loss_scale 2048.0000 (2083.8102)	mem 20177MB
[2024-08-06 00:13:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:12:26 lr 0.000020	 wd 0.0500	time 0.5919 (0.6207)	loss 0.7744 (1.0978)	grad_norm 2.1306 (nan)	loss_scale 2048.0000 (2081.0576)	mem 20177MB
[2024-08-06 00:14:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:11:23 lr 0.000020	 wd 0.0500	time 0.5931 (0.6201)	loss 1.2183 (1.1013)	grad_norm 2.2471 (nan)	loss_scale 2048.0000 (2078.6981)	mem 20177MB
[2024-08-06 00:15:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:10:20 lr 0.000020	 wd 0.0500	time 0.5929 (0.6196)	loss 0.7330 (1.1031)	grad_norm 2.2917 (nan)	loss_scale 2048.0000 (2076.6529)	mem 20177MB
[2024-08-06 00:16:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:09:18 lr 0.000020	 wd 0.0500	time 0.5831 (0.6192)	loss 1.2846 (1.1061)	grad_norm 2.2370 (nan)	loss_scale 2048.0000 (2074.8632)	mem 20177MB
[2024-08-06 00:17:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:08:16 lr 0.000020	 wd 0.0500	time 0.5861 (0.6188)	loss 0.7940 (1.1065)	grad_norm 2.8443 (nan)	loss_scale 2048.0000 (2073.2840)	mem 20177MB
[2024-08-06 00:18:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:07:14 lr 0.000020	 wd 0.0500	time 0.5900 (0.6184)	loss 0.7795 (1.1042)	grad_norm 2.0464 (nan)	loss_scale 2048.0000 (2071.8801)	mem 20177MB
[2024-08-06 00:19:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:06:12 lr 0.000020	 wd 0.0500	time 0.5891 (0.6181)	loss 1.0885 (1.1038)	grad_norm 1.5573 (nan)	loss_scale 2048.0000 (2070.6239)	mem 20177MB
[2024-08-06 00:20:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:05:10 lr 0.000020	 wd 0.0500	time 0.5882 (0.6179)	loss 1.4810 (1.1048)	grad_norm 2.9696 (nan)	loss_scale 2048.0000 (2069.4933)	mem 20177MB
[2024-08-06 00:21:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:04:08 lr 0.000020	 wd 0.0500	time 0.5872 (0.6176)	loss 1.4580 (1.1047)	grad_norm 2.3706 (nan)	loss_scale 2048.0000 (2068.4703)	mem 20177MB
[2024-08-06 00:22:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:03:06 lr 0.000020	 wd 0.0500	time 0.5930 (0.6174)	loss 1.2252 (1.1052)	grad_norm 2.6725 (nan)	loss_scale 2048.0000 (2067.5402)	mem 20177MB
[2024-08-06 00:23:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:02:04 lr 0.000020	 wd 0.0500	time 0.5908 (0.6172)	loss 1.1146 (1.1041)	grad_norm 2.3993 (nan)	loss_scale 2048.0000 (2066.6910)	mem 20177MB
[2024-08-06 00:24:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:02 lr 0.000020	 wd 0.0500	time 0.5885 (0.6170)	loss 0.8789 (1.1036)	grad_norm 2.0572 (nan)	loss_scale 2048.0000 (2065.9125)	mem 20177MB
[2024-08-06 00:25:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5904 (0.6168)	loss 1.3226 (1.1045)	grad_norm 2.3520 (nan)	loss_scale 2048.0000 (2065.1963)	mem 20177MB
[2024-08-06 00:25:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 5 training takes 0:25:45
[2024-08-06 00:26:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.884 (11.884)	Loss 0.4729 (0.4729)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 00:26:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.648 Acc@5 98.014
[2024-08-06 00:26:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-06 00:26:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 00:26:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:20:08 lr 0.000020	 wd 0.0500	time 11.9936 (11.9936)	loss 1.1936 (1.1936)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:27:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:29:01 lr 0.000020	 wd 0.0500	time 0.5843 (0.7248)	loss 0.9258 (1.1312)	grad_norm 2.1668 (2.9502)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:28:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:25:38 lr 0.000020	 wd 0.0500	time 0.5840 (0.6684)	loss 0.8733 (1.1139)	grad_norm 2.0201 (3.0286)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:29:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:23:50 lr 0.000020	 wd 0.0500	time 0.5885 (0.6497)	loss 0.9004 (1.1101)	grad_norm 3.0935 (2.8903)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:30:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:22:26 lr 0.000020	 wd 0.0500	time 0.6085 (0.6405)	loss 0.6531 (1.1083)	grad_norm 2.5768 (2.8391)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:31:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:21:11 lr 0.000020	 wd 0.0500	time 0.5916 (0.6349)	loss 1.2067 (1.1096)	grad_norm 2.4964 (2.7609)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:32:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:20:00 lr 0.000020	 wd 0.0500	time 0.5965 (0.6313)	loss 0.7975 (1.1018)	grad_norm 1.9633 (2.7491)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:33:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:18:52 lr 0.000020	 wd 0.0500	time 0.5864 (0.6285)	loss 1.3364 (1.1040)	grad_norm 1.9545 (2.7021)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:34:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:17:46 lr 0.000020	 wd 0.0500	time 0.5944 (0.6266)	loss 0.7286 (1.1091)	grad_norm 3.6118 (2.6995)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:35:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:16:41 lr 0.000020	 wd 0.0500	time 0.5845 (0.6250)	loss 1.5585 (1.1046)	grad_norm 1.7339 (2.6705)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:36:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:15:36 lr 0.000020	 wd 0.0500	time 0.5938 (0.6238)	loss 0.9724 (1.1045)	grad_norm 2.0955 (2.6535)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:37:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:14:33 lr 0.000020	 wd 0.0500	time 0.5890 (0.6229)	loss 0.7632 (1.1001)	grad_norm 2.2494 (2.6329)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:38:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:13:29 lr 0.000020	 wd 0.0500	time 0.5640 (0.6221)	loss 1.2321 (1.0970)	grad_norm 2.3249 (2.6183)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:39:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:12:26 lr 0.000020	 wd 0.0500	time 0.5911 (0.6213)	loss 1.0828 (1.0961)	grad_norm 2.7633 (2.6119)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:40:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:11:24 lr 0.000020	 wd 0.0500	time 0.5962 (0.6208)	loss 0.9647 (1.0954)	grad_norm 2.1486 (2.6203)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:41:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:10:21 lr 0.000020	 wd 0.0500	time 0.6040 (0.6202)	loss 0.6481 (1.0953)	grad_norm 1.7709 (2.6276)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 00:43:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:09:19 lr 0.000020	 wd 0.0500	time 0.5916 (0.6198)	loss 1.2844 (1.0934)	grad_norm 2.3996 (2.6472)	loss_scale 4096.0000 (2152.8944)	mem 20177MB
[2024-08-06 00:44:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:08:16 lr 0.000020	 wd 0.0500	time 0.5861 (0.6193)	loss 1.3115 (1.0916)	grad_norm 1.9312 (2.6361)	loss_scale 4096.0000 (2267.1276)	mem 20177MB
[2024-08-06 00:45:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:07:14 lr 0.000020	 wd 0.0500	time 0.6012 (0.6190)	loss 1.1322 (1.0918)	grad_norm 1.5221 (2.6129)	loss_scale 4096.0000 (2368.6752)	mem 20177MB
[2024-08-06 00:46:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:06:12 lr 0.000020	 wd 0.0500	time 0.5916 (0.6187)	loss 1.3375 (1.0941)	grad_norm 1.7890 (2.6390)	loss_scale 4096.0000 (2459.5392)	mem 20177MB
[2024-08-06 00:47:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:05:10 lr 0.000020	 wd 0.0500	time 0.5865 (0.6184)	loss 0.8805 (1.0947)	grad_norm 2.9198 (2.6225)	loss_scale 4096.0000 (2541.3213)	mem 20177MB
[2024-08-06 00:48:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:04:08 lr 0.000020	 wd 0.0500	time 0.5969 (0.6182)	loss 1.0510 (1.0951)	grad_norm 5.2868 (2.6152)	loss_scale 4096.0000 (2615.3184)	mem 20177MB
[2024-08-06 00:49:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:03:06 lr 0.000020	 wd 0.0500	time 0.5604 (0.6179)	loss 0.7633 (1.0946)	grad_norm 5.1194 (2.6072)	loss_scale 4096.0000 (2682.5915)	mem 20177MB
[2024-08-06 00:50:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:02:04 lr 0.000020	 wd 0.0500	time 0.5826 (0.6177)	loss 0.8900 (1.0965)	grad_norm 1.9257 (2.6107)	loss_scale 4096.0000 (2744.0174)	mem 20177MB
[2024-08-06 00:51:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:02 lr 0.000020	 wd 0.0500	time 0.5876 (0.6175)	loss 1.3522 (1.0955)	grad_norm 2.2390 (2.6287)	loss_scale 4096.0000 (2800.3265)	mem 20177MB
[2024-08-06 00:52:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5898 (0.6173)	loss 1.3415 (1.0950)	grad_norm 2.5502 (2.6189)	loss_scale 4096.0000 (2852.1327)	mem 20177MB
[2024-08-06 00:52:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 6 training takes 0:25:46
[2024-08-06 00:52:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.302 (11.302)	Loss 0.4722 (0.4722)	Acc@1 91.992 (91.992)	Acc@5 98.633 (98.633)	Mem 20177MB
[2024-08-06 00:52:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.674 Acc@5 97.994
[2024-08-06 00:52:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 00:52:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 00:53:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:33:43 lr 0.000020	 wd 0.0500	time 12.3196 (12.3196)	loss 0.8312 (0.8312)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-06 00:54:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:29:05 lr 0.000020	 wd 0.0500	time 0.5943 (0.7268)	loss 1.1571 (1.0781)	grad_norm 3.9108 (3.5882)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-06 00:55:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:25:40 lr 0.000020	 wd 0.0500	time 0.5828 (0.6692)	loss 1.0355 (1.0995)	grad_norm 1.6483 (3.3645)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-06 00:56:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:23:51 lr 0.000020	 wd 0.0500	time 0.5890 (0.6501)	loss 1.4092 (1.0984)	grad_norm 2.0305 (3.0687)	loss_scale 4096.0000 (4096.0000)	mem 20177MB
[2024-08-06 00:57:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:22:26 lr 0.000020	 wd 0.0500	time 0.5635 (0.6406)	loss 0.7800 (1.1020)	grad_norm 1.8879 (nan)	loss_scale 2048.0000 (3748.7082)	mem 20177MB
[2024-08-06 00:58:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:21:11 lr 0.000020	 wd 0.0500	time 0.5862 (0.6349)	loss 1.3284 (1.0999)	grad_norm 2.3438 (nan)	loss_scale 2048.0000 (3409.2455)	mem 20177MB
[2024-08-06 00:59:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:20:00 lr 0.000020	 wd 0.0500	time 0.5859 (0.6312)	loss 1.2090 (1.1047)	grad_norm 2.7226 (nan)	loss_scale 2048.0000 (3182.7488)	mem 20177MB
[2024-08-06 01:00:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:18:52 lr 0.000020	 wd 0.0500	time 0.5958 (0.6285)	loss 1.0109 (1.0992)	grad_norm 2.2859 (nan)	loss_scale 2048.0000 (3020.8730)	mem 20177MB
[2024-08-06 01:01:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:17:46 lr 0.000020	 wd 0.0500	time 0.5907 (0.6264)	loss 0.8758 (1.0966)	grad_norm 2.4975 (nan)	loss_scale 2048.0000 (2899.4157)	mem 20177MB
[2024-08-06 01:02:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:16:40 lr 0.000020	 wd 0.0500	time 0.5898 (0.6248)	loss 1.4512 (1.0997)	grad_norm 3.3055 (nan)	loss_scale 1024.0000 (2709.4517)	mem 20177MB
[2024-08-06 01:03:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:15:36 lr 0.000020	 wd 0.0500	time 0.5910 (0.6236)	loss 0.8064 (1.0994)	grad_norm 2.2639 (nan)	loss_scale 1024.0000 (2541.0749)	mem 20177MB
[2024-08-06 01:04:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:14:32 lr 0.000020	 wd 0.0500	time 0.5802 (0.6225)	loss 1.1503 (1.0975)	grad_norm 2.8389 (nan)	loss_scale 1024.0000 (2403.2843)	mem 20177MB
[2024-08-06 01:05:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:13:29 lr 0.000020	 wd 0.0500	time 0.5932 (0.6217)	loss 1.2095 (1.0943)	grad_norm 1.9837 (nan)	loss_scale 1024.0000 (2288.4396)	mem 20177MB
[2024-08-06 01:06:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:12:26 lr 0.000020	 wd 0.0500	time 0.5921 (0.6210)	loss 1.2472 (1.0943)	grad_norm 4.6100 (nan)	loss_scale 1024.0000 (2191.2498)	mem 20177MB
[2024-08-06 01:07:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:11:23 lr 0.000019	 wd 0.0500	time 0.5907 (0.6204)	loss 1.1565 (1.0955)	grad_norm 2.4904 (nan)	loss_scale 1024.0000 (2107.9343)	mem 20177MB
[2024-08-06 01:08:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:10:21 lr 0.000019	 wd 0.0500	time 0.5992 (0.6199)	loss 0.7579 (1.0964)	grad_norm 1.8476 (nan)	loss_scale 1024.0000 (2035.7202)	mem 20177MB
[2024-08-06 01:09:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:09:18 lr 0.000019	 wd 0.0500	time 0.5906 (0.6194)	loss 1.1481 (1.0971)	grad_norm 2.3031 (nan)	loss_scale 1024.0000 (1972.5272)	mem 20177MB
[2024-08-06 01:10:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:08:16 lr 0.000019	 wd 0.0500	time 0.5767 (0.6190)	loss 1.1900 (1.0968)	grad_norm 1.7335 (nan)	loss_scale 1024.0000 (1916.7643)	mem 20177MB
[2024-08-06 01:11:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:07:14 lr 0.000019	 wd 0.0500	time 0.5871 (0.6186)	loss 0.7098 (1.0988)	grad_norm 2.5904 (nan)	loss_scale 1024.0000 (1867.1938)	mem 20177MB
[2024-08-06 01:12:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:06:12 lr 0.000019	 wd 0.0500	time 0.5678 (0.6182)	loss 1.1132 (1.0993)	grad_norm 2.3336 (nan)	loss_scale 1024.0000 (1822.8385)	mem 20177MB
[2024-08-06 01:13:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:05:10 lr 0.000019	 wd 0.0500	time 0.5866 (0.6179)	loss 1.1770 (1.1000)	grad_norm 1.8529 (nan)	loss_scale 1024.0000 (1782.9165)	mem 20177MB
[2024-08-06 01:14:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:04:08 lr 0.000019	 wd 0.0500	time 0.5824 (0.6176)	loss 1.4426 (1.1024)	grad_norm 2.0877 (nan)	loss_scale 1024.0000 (1746.7949)	mem 20177MB
[2024-08-06 01:15:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:03:06 lr 0.000019	 wd 0.0500	time 0.5823 (0.6174)	loss 1.1921 (1.1005)	grad_norm 1.9742 (nan)	loss_scale 1024.0000 (1713.9555)	mem 20177MB
[2024-08-06 01:16:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:02:04 lr 0.000019	 wd 0.0500	time 0.5873 (0.6172)	loss 1.3687 (1.1000)	grad_norm 1.9561 (nan)	loss_scale 1024.0000 (1683.9704)	mem 20177MB
[2024-08-06 01:17:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:02 lr 0.000019	 wd 0.0500	time 0.5865 (0.6170)	loss 1.2887 (1.1000)	grad_norm 1.6687 (nan)	loss_scale 1024.0000 (1656.4831)	mem 20177MB
[2024-08-06 01:18:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5815 (0.6167)	loss 1.4175 (1.0994)	grad_norm 2.5602 (nan)	loss_scale 1024.0000 (1631.1939)	mem 20177MB
[2024-08-06 01:18:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 7 training takes 0:25:45
[2024-08-06 01:18:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.860 (11.860)	Loss 0.4446 (0.4446)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 01:19:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.634 Acc@5 97.986
[2024-08-06 01:19:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-06 01:19:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 01:19:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:26:04 lr 0.000019	 wd 0.0500	time 12.1360 (12.1360)	loss 1.1944 (1.1944)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:20:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:29:00 lr 0.000019	 wd 0.0500	time 0.5851 (0.7247)	loss 0.8476 (1.1528)	grad_norm 2.5262 (2.3937)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:21:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:25:38 lr 0.000019	 wd 0.0500	time 0.5862 (0.6685)	loss 0.7837 (1.1293)	grad_norm 2.3346 (2.3322)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:22:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:23:51 lr 0.000019	 wd 0.0500	time 0.5908 (0.6499)	loss 1.4141 (1.1147)	grad_norm 1.8278 (2.4859)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:23:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:22:26 lr 0.000019	 wd 0.0500	time 0.5854 (0.6405)	loss 0.8060 (1.1122)	grad_norm 6.4099 (2.4247)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:24:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:21:10 lr 0.000019	 wd 0.0500	time 0.5854 (0.6348)	loss 0.7606 (1.1028)	grad_norm 2.4072 (2.4272)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:25:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:20:00 lr 0.000019	 wd 0.0500	time 0.5874 (0.6310)	loss 0.7933 (1.1024)	grad_norm 2.0527 (2.4161)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:26:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:18:52 lr 0.000019	 wd 0.0500	time 0.5853 (0.6284)	loss 1.2300 (1.1026)	grad_norm 2.1532 (2.4560)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:27:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:17:46 lr 0.000019	 wd 0.0500	time 0.5811 (0.6264)	loss 1.2715 (1.1047)	grad_norm 2.5764 (2.4865)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:28:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:16:40 lr 0.000019	 wd 0.0500	time 0.5894 (0.6248)	loss 0.8737 (1.1008)	grad_norm 5.4185 (2.5257)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:29:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:15:36 lr 0.000019	 wd 0.0500	time 0.5828 (0.6235)	loss 1.2826 (1.1027)	grad_norm 2.2215 (2.5320)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:30:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:14:32 lr 0.000019	 wd 0.0500	time 0.5884 (0.6224)	loss 1.2355 (1.0999)	grad_norm 1.7863 (2.5139)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:31:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:13:29 lr 0.000019	 wd 0.0500	time 0.5850 (0.6216)	loss 1.0105 (1.1006)	grad_norm 2.8114 (2.5065)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:32:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:12:26 lr 0.000019	 wd 0.0500	time 0.5914 (0.6209)	loss 0.7839 (1.1003)	grad_norm 8.8721 (2.4975)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:33:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:11:23 lr 0.000019	 wd 0.0500	time 0.5888 (0.6203)	loss 1.2598 (1.1015)	grad_norm 2.2791 (2.4932)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:34:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:10:21 lr 0.000019	 wd 0.0500	time 0.5887 (0.6198)	loss 1.0172 (1.1024)	grad_norm 2.0487 (2.5461)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:35:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:09:18 lr 0.000019	 wd 0.0500	time 0.5859 (0.6193)	loss 1.3293 (1.1042)	grad_norm 3.1104 (2.5461)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:36:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:08:16 lr 0.000019	 wd 0.0500	time 0.5815 (0.6189)	loss 0.7305 (1.1063)	grad_norm 1.8413 (2.5729)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:37:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:07:14 lr 0.000019	 wd 0.0500	time 0.5820 (0.6186)	loss 0.7440 (1.1047)	grad_norm 2.5723 (2.5648)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:38:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:06:12 lr 0.000019	 wd 0.0500	time 0.5876 (0.6183)	loss 1.2174 (1.1063)	grad_norm 2.3253 (2.5523)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:39:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:05:10 lr 0.000019	 wd 0.0500	time 0.5866 (0.6180)	loss 0.7160 (1.1052)	grad_norm 1.8646 (2.5768)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:40:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:04:08 lr 0.000019	 wd 0.0500	time 0.5863 (0.6178)	loss 0.8687 (1.1059)	grad_norm 2.4157 (2.5973)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:41:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:03:06 lr 0.000019	 wd 0.0500	time 0.5882 (0.6175)	loss 1.1672 (1.1047)	grad_norm 1.7085 (2.5826)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:42:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:02:04 lr 0.000019	 wd 0.0500	time 0.5865 (0.6173)	loss 1.2152 (1.1032)	grad_norm 2.8306 (2.5965)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 01:43:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:02 lr 0.000019	 wd 0.0500	time 0.5896 (0.6171)	loss 1.0242 (1.1032)	grad_norm 3.0249 (2.5990)	loss_scale 2048.0000 (1060.6781)	mem 20177MB
[2024-08-06 01:44:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5946 (0.6169)	loss 0.8010 (1.1029)	grad_norm 1.9197 (2.5885)	loss_scale 2048.0000 (1100.1551)	mem 20177MB
[2024-08-06 01:44:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 8 training takes 0:25:45
[2024-08-06 01:45:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.614 (12.614)	Loss 0.4558 (0.4558)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 01:45:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.714 Acc@5 98.042
[2024-08-06 01:45:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 01:45:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 01:45:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][0/2502]	eta 8:47:04 lr 0.000019	 wd 0.0500	time 12.6397 (12.6397)	loss 1.2653 (1.2653)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:46:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:29:12 lr 0.000019	 wd 0.0500	time 0.5876 (0.7296)	loss 1.2057 (1.0872)	grad_norm 2.6203 (3.3754)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:47:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:25:43 lr 0.000019	 wd 0.0500	time 0.5912 (0.6705)	loss 0.9055 (1.1101)	grad_norm 2.2576 (2.8450)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:48:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:23:53 lr 0.000019	 wd 0.0500	time 0.5867 (0.6510)	loss 1.3689 (1.1155)	grad_norm 1.8125 (2.6856)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:49:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:22:27 lr 0.000019	 wd 0.0500	time 0.5941 (0.6413)	loss 0.8639 (1.0973)	grad_norm 1.8795 (2.7380)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:50:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:21:12 lr 0.000019	 wd 0.0500	time 0.5770 (0.6355)	loss 0.6366 (1.0927)	grad_norm 2.5652 (2.6883)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:51:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:20:01 lr 0.000019	 wd 0.0500	time 0.5794 (0.6316)	loss 0.7534 (1.0954)	grad_norm 1.8449 (2.7109)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:52:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:18:53 lr 0.000019	 wd 0.0500	time 0.5929 (0.6288)	loss 1.3660 (1.0981)	grad_norm 1.7510 (2.6914)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:53:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:17:46 lr 0.000019	 wd 0.0500	time 0.5912 (0.6267)	loss 1.3715 (1.0997)	grad_norm 1.7904 (2.6651)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:54:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:16:41 lr 0.000019	 wd 0.0500	time 0.5800 (0.6252)	loss 1.2969 (1.1002)	grad_norm 1.9857 (2.6662)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:55:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:15:37 lr 0.000019	 wd 0.0500	time 0.5856 (0.6239)	loss 1.2350 (1.1007)	grad_norm 1.7315 (2.6420)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:56:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:14:33 lr 0.000018	 wd 0.0500	time 0.5898 (0.6227)	loss 1.3299 (1.0968)	grad_norm 1.8511 (2.6241)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:57:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:13:29 lr 0.000018	 wd 0.0500	time 0.5844 (0.6219)	loss 0.9356 (1.0970)	grad_norm 2.3360 (2.6351)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:58:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:12:26 lr 0.000018	 wd 0.0500	time 0.5874 (0.6212)	loss 1.2829 (1.0981)	grad_norm 2.6068 (2.6489)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 01:59:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:11:23 lr 0.000018	 wd 0.0500	time 0.5764 (0.6206)	loss 0.8901 (1.0961)	grad_norm 2.5636 (2.6351)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:00:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:10:21 lr 0.000018	 wd 0.0500	time 0.5873 (0.6200)	loss 0.8383 (1.0988)	grad_norm 2.4485 (2.6388)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:01:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:09:18 lr 0.000018	 wd 0.0500	time 0.5991 (0.6195)	loss 0.8679 (1.0968)	grad_norm 2.4851 (2.6350)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:02:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:08:16 lr 0.000018	 wd 0.0500	time 0.5755 (0.6191)	loss 1.3449 (1.0970)	grad_norm 3.4127 (2.6272)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:04:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:07:14 lr 0.000018	 wd 0.0500	time 0.5869 (0.6187)	loss 1.2263 (1.0963)	grad_norm 2.1856 (2.6188)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:05:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:06:12 lr 0.000018	 wd 0.0500	time 0.5896 (0.6184)	loss 1.3165 (1.0953)	grad_norm 2.3539 (2.6212)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:06:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:05:10 lr 0.000018	 wd 0.0500	time 0.5896 (0.6180)	loss 1.1313 (1.0957)	grad_norm 1.6011 (2.6168)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:07:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:04:08 lr 0.000018	 wd 0.0500	time 0.5884 (0.6178)	loss 1.2895 (1.0969)	grad_norm 3.3731 (2.6212)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:08:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:03:06 lr 0.000018	 wd 0.0500	time 0.5867 (0.6175)	loss 1.1800 (1.0977)	grad_norm 3.2294 (2.6351)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:09:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:02:04 lr 0.000018	 wd 0.0500	time 0.5959 (0.6173)	loss 0.7952 (1.0978)	grad_norm 3.0427 (2.6205)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:10:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:02 lr 0.000018	 wd 0.0500	time 0.5941 (0.6171)	loss 1.0845 (1.0983)	grad_norm 2.1075 (2.6161)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:11:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.5894 (0.6169)	loss 1.3284 (1.0977)	grad_norm 2.5927 (2.6173)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:11:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 9 training takes 0:25:45
[2024-08-06 02:11:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.868 (11.868)	Loss 0.4575 (0.4575)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 20177MB
[2024-08-06 02:11:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.754 Acc@5 98.032
[2024-08-06 02:11:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 02:11:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 02:11:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][0/2502]	eta 8:52:13 lr 0.000018	 wd 0.0500	time 12.7631 (12.7631)	loss 1.3305 (1.3305)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:12:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:29:14 lr 0.000018	 wd 0.0500	time 0.5792 (0.7306)	loss 0.7984 (1.1037)	grad_norm 2.9318 (2.7724)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:13:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:25:44 lr 0.000018	 wd 0.0500	time 0.5905 (0.6711)	loss 1.3095 (1.0930)	grad_norm 2.1212 (2.5880)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:15:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:23:54 lr 0.000018	 wd 0.0500	time 0.5882 (0.6512)	loss 1.2896 (1.0928)	grad_norm 2.2418 (2.5672)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:16:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:22:28 lr 0.000018	 wd 0.0500	time 0.5870 (0.6414)	loss 0.7088 (1.1002)	grad_norm 2.6338 (2.5447)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:17:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:21:12 lr 0.000018	 wd 0.0500	time 0.5917 (0.6356)	loss 0.6975 (1.0979)	grad_norm 1.8732 (2.5135)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:18:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:20:01 lr 0.000018	 wd 0.0500	time 0.5925 (0.6317)	loss 1.1654 (1.1042)	grad_norm 2.7431 (2.5038)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 02:19:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:18:53 lr 0.000018	 wd 0.0500	time 0.5829 (0.6289)	loss 1.2332 (1.1019)	grad_norm 1.8491 (nan)	loss_scale 1024.0000 (1974.9615)	mem 20177MB
[2024-08-06 02:20:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:17:46 lr 0.000018	 wd 0.0500	time 0.5929 (0.6269)	loss 1.2046 (1.1043)	grad_norm 4.0377 (nan)	loss_scale 1024.0000 (1856.2397)	mem 20177MB
[2024-08-06 02:21:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:16:41 lr 0.000018	 wd 0.0500	time 0.5955 (0.6252)	loss 1.2464 (1.1030)	grad_norm 2.6459 (nan)	loss_scale 1024.0000 (1763.8713)	mem 20177MB
[2024-08-06 02:22:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:15:37 lr 0.000018	 wd 0.0500	time 0.5821 (0.6240)	loss 1.2482 (1.1042)	grad_norm 2.3473 (nan)	loss_scale 1024.0000 (1689.9580)	mem 20177MB
[2024-08-06 02:23:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:14:33 lr 0.000018	 wd 0.0500	time 0.5755 (0.6229)	loss 0.9542 (1.1110)	grad_norm 3.5354 (nan)	loss_scale 1024.0000 (1629.4714)	mem 20177MB
[2024-08-06 02:24:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:13:29 lr 0.000018	 wd 0.0500	time 0.5871 (0.6221)	loss 1.3773 (1.1080)	grad_norm 3.3620 (nan)	loss_scale 1024.0000 (1579.0575)	mem 20177MB
[2024-08-06 02:25:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:12:26 lr 0.000018	 wd 0.0500	time 0.5905 (0.6214)	loss 1.1401 (1.1062)	grad_norm 3.4065 (nan)	loss_scale 1024.0000 (1536.3935)	mem 20177MB
[2024-08-06 02:26:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:11:24 lr 0.000018	 wd 0.0500	time 0.5821 (0.6208)	loss 1.3819 (1.1049)	grad_norm 2.0942 (nan)	loss_scale 1024.0000 (1499.8201)	mem 20177MB
[2024-08-06 02:27:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:10:21 lr 0.000018	 wd 0.0500	time 0.5844 (0.6202)	loss 1.1031 (1.1032)	grad_norm 4.4732 (nan)	loss_scale 1024.0000 (1468.1199)	mem 20177MB
[2024-08-06 02:28:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:09:18 lr 0.000018	 wd 0.0500	time 0.5898 (0.6197)	loss 1.1006 (1.1015)	grad_norm 2.3126 (nan)	loss_scale 1024.0000 (1440.3798)	mem 20177MB
[2024-08-06 02:29:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:08:16 lr 0.000018	 wd 0.0500	time 0.5809 (0.6193)	loss 1.2113 (1.1032)	grad_norm 5.4940 (nan)	loss_scale 1024.0000 (1415.9012)	mem 20177MB
[2024-08-06 02:30:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:07:14 lr 0.000018	 wd 0.0500	time 0.5842 (0.6190)	loss 0.9102 (1.1016)	grad_norm 2.6640 (nan)	loss_scale 1024.0000 (1394.1410)	mem 20177MB
[2024-08-06 02:31:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:06:12 lr 0.000018	 wd 0.0500	time 0.5939 (0.6186)	loss 0.9996 (1.1004)	grad_norm 2.7129 (nan)	loss_scale 1024.0000 (1374.6702)	mem 20177MB
[2024-08-06 02:32:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:05:10 lr 0.000017	 wd 0.0500	time 0.5950 (0.6184)	loss 1.2223 (1.1035)	grad_norm 1.9970 (nan)	loss_scale 1024.0000 (1357.1454)	mem 20177MB
[2024-08-06 02:33:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:04:08 lr 0.000017	 wd 0.0500	time 0.5891 (0.6180)	loss 0.9880 (1.1050)	grad_norm 2.6036 (nan)	loss_scale 1024.0000 (1341.2889)	mem 20177MB
[2024-08-06 02:34:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:03:06 lr 0.000017	 wd 0.0500	time 0.5791 (0.6178)	loss 1.1882 (1.1059)	grad_norm 6.4620 (nan)	loss_scale 1024.0000 (1326.8732)	mem 20177MB
[2024-08-06 02:35:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:02:04 lr 0.000017	 wd 0.0500	time 0.5958 (0.6175)	loss 1.2310 (1.1038)	grad_norm 12.3304 (nan)	loss_scale 1024.0000 (1313.7106)	mem 20177MB
[2024-08-06 02:36:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:02 lr 0.000017	 wd 0.0500	time 0.5921 (0.6173)	loss 1.3606 (1.1027)	grad_norm 2.2263 (nan)	loss_scale 1024.0000 (1301.6443)	mem 20177MB
[2024-08-06 02:37:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.5868 (0.6171)	loss 0.7632 (1.1018)	grad_norm 1.8534 (nan)	loss_scale 1024.0000 (1290.5430)	mem 20177MB
[2024-08-06 02:37:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 10 training takes 0:25:46
[2024-08-06 02:37:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.232 (12.232)	Loss 0.4580 (0.4580)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 20177MB
[2024-08-06 02:38:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.744 Acc@5 98.012
[2024-08-06 02:38:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 02:38:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 02:38:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:03:54 lr 0.000017	 wd 0.0500	time 11.6044 (11.6044)	loss 0.9191 (0.9191)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:39:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:28:52 lr 0.000017	 wd 0.0500	time 0.5890 (0.7214)	loss 1.3005 (1.0905)	grad_norm 2.4536 (2.9233)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:40:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:25:34 lr 0.000017	 wd 0.0500	time 0.5873 (0.6665)	loss 1.1897 (1.1001)	grad_norm 4.5118 (2.9271)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:41:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:23:47 lr 0.000017	 wd 0.0500	time 0.5890 (0.6484)	loss 0.8533 (1.0941)	grad_norm 1.7755 (3.2048)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:42:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:22:23 lr 0.000017	 wd 0.0500	time 0.5830 (0.6392)	loss 1.3443 (1.0958)	grad_norm 3.0238 (2.9996)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:43:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:21:08 lr 0.000017	 wd 0.0500	time 0.5896 (0.6338)	loss 0.7408 (1.0945)	grad_norm 1.9036 (3.0281)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:44:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:19:58 lr 0.000017	 wd 0.0500	time 0.5900 (0.6302)	loss 1.4682 (1.0968)	grad_norm 3.1074 (3.0746)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:45:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:18:51 lr 0.000017	 wd 0.0500	time 0.5884 (0.6276)	loss 0.8772 (1.0941)	grad_norm 4.0829 (3.0510)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:46:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:17:45 lr 0.000017	 wd 0.0500	time 0.5912 (0.6258)	loss 1.3717 (1.0925)	grad_norm 1.6274 (3.1034)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:47:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:16:40 lr 0.000017	 wd 0.0500	time 0.5812 (0.6243)	loss 0.8233 (1.0944)	grad_norm 1.9716 (3.0162)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:48:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:15:35 lr 0.000017	 wd 0.0500	time 0.5942 (0.6231)	loss 1.1862 (1.0952)	grad_norm 2.4445 (2.9557)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:49:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:14:32 lr 0.000017	 wd 0.0500	time 0.5810 (0.6222)	loss 0.8848 (1.0935)	grad_norm 2.9203 (2.9118)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:50:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:13:29 lr 0.000017	 wd 0.0500	time 0.5864 (0.6215)	loss 0.9443 (1.0899)	grad_norm 2.4134 (2.8818)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:51:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:12:26 lr 0.000017	 wd 0.0500	time 0.5636 (0.6208)	loss 1.2009 (1.0905)	grad_norm 2.7427 (2.8535)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:52:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:11:23 lr 0.000017	 wd 0.0500	time 0.5894 (0.6202)	loss 0.7943 (1.0914)	grad_norm 1.7725 (2.9317)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:53:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:10:20 lr 0.000017	 wd 0.0500	time 0.6068 (0.6197)	loss 1.3514 (1.0913)	grad_norm 2.6955 (2.9173)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:54:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:09:18 lr 0.000017	 wd 0.0500	time 0.5883 (0.6193)	loss 1.2071 (1.0926)	grad_norm 1.8604 (2.8971)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:55:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:08:16 lr 0.000017	 wd 0.0500	time 0.5847 (0.6189)	loss 0.9644 (1.0932)	grad_norm 1.9695 (2.8728)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:56:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:07:14 lr 0.000017	 wd 0.0500	time 0.5932 (0.6186)	loss 0.8002 (1.0922)	grad_norm 2.7292 (2.8401)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:57:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:06:12 lr 0.000017	 wd 0.0500	time 0.5871 (0.6182)	loss 1.0227 (1.0907)	grad_norm 2.7637 (2.8232)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:58:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:05:10 lr 0.000017	 wd 0.0500	time 0.5833 (0.6179)	loss 1.1444 (1.0912)	grad_norm 2.4038 (2.8143)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 02:59:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:04:08 lr 0.000017	 wd 0.0500	time 0.5864 (0.6176)	loss 1.2998 (1.0907)	grad_norm 2.1334 (2.7918)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:00:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:03:06 lr 0.000017	 wd 0.0500	time 0.5868 (0.6174)	loss 0.8548 (1.0920)	grad_norm 1.9518 (2.8056)	loss_scale 2048.0000 (1048.1926)	mem 20177MB
[2024-08-06 03:01:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:02:04 lr 0.000016	 wd 0.0500	time 0.5918 (0.6171)	loss 1.1236 (1.0906)	grad_norm 15.2157 (2.8220)	loss_scale 2048.0000 (1091.6436)	mem 20177MB
[2024-08-06 03:02:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:02 lr 0.000016	 wd 0.0500	time 0.5919 (0.6170)	loss 1.2223 (1.0917)	grad_norm 1.8606 (2.8152)	loss_scale 2048.0000 (1131.4752)	mem 20177MB
[2024-08-06 03:03:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5904 (0.6168)	loss 0.7702 (1.0899)	grad_norm 1.9941 (2.8026)	loss_scale 2048.0000 (1168.1216)	mem 20177MB
[2024-08-06 03:03:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 11 training takes 0:25:45
[2024-08-06 03:04:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.148 (12.148)	Loss 0.4502 (0.4502)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 03:04:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.732 Acc@5 98.026
[2024-08-06 03:04:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 03:04:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 03:04:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:24:27 lr 0.000016	 wd 0.0500	time 12.0975 (12.0975)	loss 0.8511 (0.8511)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 03:05:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:29:10 lr 0.000016	 wd 0.0500	time 0.5809 (0.7290)	loss 1.2339 (1.0874)	grad_norm 1.6267 (2.3708)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 03:06:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:25:42 lr 0.000016	 wd 0.0500	time 0.5801 (0.6703)	loss 1.1400 (1.0861)	grad_norm 2.1932 (nan)	loss_scale 1024.0000 (1620.0597)	mem 20177MB
[2024-08-06 03:07:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:23:53 lr 0.000016	 wd 0.0500	time 0.5874 (0.6509)	loss 0.8731 (1.0944)	grad_norm 2.1882 (nan)	loss_scale 1024.0000 (1422.0332)	mem 20177MB
[2024-08-06 03:08:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:22:27 lr 0.000016	 wd 0.0500	time 0.5894 (0.6413)	loss 1.1669 (1.0895)	grad_norm 2.0336 (nan)	loss_scale 1024.0000 (1322.7731)	mem 20177MB
[2024-08-06 03:09:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:21:12 lr 0.000016	 wd 0.0500	time 0.5686 (0.6355)	loss 1.3286 (1.0881)	grad_norm 1.6849 (nan)	loss_scale 1024.0000 (1263.1377)	mem 20177MB
[2024-08-06 03:10:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:20:01 lr 0.000016	 wd 0.0500	time 0.5922 (0.6316)	loss 1.3533 (1.0858)	grad_norm 1.7374 (nan)	loss_scale 1024.0000 (1223.3478)	mem 20177MB
[2024-08-06 03:11:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:18:53 lr 0.000016	 wd 0.0500	time 0.5876 (0.6289)	loss 1.1863 (1.0836)	grad_norm 2.7758 (nan)	loss_scale 1024.0000 (1194.9101)	mem 20177MB
[2024-08-06 03:12:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:17:47 lr 0.000016	 wd 0.0500	time 0.5864 (0.6269)	loss 1.3024 (1.0849)	grad_norm 2.8340 (nan)	loss_scale 1024.0000 (1173.5730)	mem 20177MB
[2024-08-06 03:13:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:16:41 lr 0.000016	 wd 0.0500	time 0.5834 (0.6253)	loss 1.3080 (1.0823)	grad_norm 3.7220 (nan)	loss_scale 1024.0000 (1156.9723)	mem 20177MB
[2024-08-06 03:14:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:15:37 lr 0.000016	 wd 0.0500	time 0.5884 (0.6240)	loss 1.2265 (1.0861)	grad_norm 1.9093 (nan)	loss_scale 1024.0000 (1143.6883)	mem 20177MB
[2024-08-06 03:15:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:14:33 lr 0.000016	 wd 0.0500	time 0.5854 (0.6230)	loss 0.8512 (1.0882)	grad_norm 2.6107 (nan)	loss_scale 1024.0000 (1132.8174)	mem 20177MB
[2024-08-06 03:16:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:13:30 lr 0.000016	 wd 0.0500	time 0.5806 (0.6222)	loss 1.2098 (1.0872)	grad_norm 6.3381 (nan)	loss_scale 1024.0000 (1123.7569)	mem 20177MB
[2024-08-06 03:17:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:12:27 lr 0.000016	 wd 0.0500	time 0.5901 (0.6215)	loss 0.9978 (1.0886)	grad_norm 1.7641 (nan)	loss_scale 1024.0000 (1116.0892)	mem 20177MB
[2024-08-06 03:18:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:11:24 lr 0.000016	 wd 0.0500	time 0.5826 (0.6208)	loss 1.3083 (1.0894)	grad_norm 2.3988 (nan)	loss_scale 1024.0000 (1109.5161)	mem 20177MB
[2024-08-06 03:19:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:10:21 lr 0.000016	 wd 0.0500	time 0.5890 (0.6203)	loss 0.7692 (1.0862)	grad_norm 3.8115 (nan)	loss_scale 1024.0000 (1103.8188)	mem 20177MB
[2024-08-06 03:20:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:09:19 lr 0.000016	 wd 0.0500	time 0.5947 (0.6198)	loss 1.3662 (1.0837)	grad_norm 1.8843 (nan)	loss_scale 1024.0000 (1098.8332)	mem 20177MB
[2024-08-06 03:21:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:08:16 lr 0.000016	 wd 0.0500	time 0.5904 (0.6194)	loss 1.1212 (1.0862)	grad_norm 2.1225 (nan)	loss_scale 1024.0000 (1094.4339)	mem 20177MB
[2024-08-06 03:22:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:07:14 lr 0.000016	 wd 0.0500	time 0.5873 (0.6190)	loss 0.7585 (1.0849)	grad_norm 2.0428 (nan)	loss_scale 1024.0000 (1090.5230)	mem 20177MB
[2024-08-06 03:23:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:06:12 lr 0.000016	 wd 0.0500	time 0.5887 (0.6187)	loss 1.1317 (1.0855)	grad_norm 1.8082 (nan)	loss_scale 1024.0000 (1087.0237)	mem 20177MB
[2024-08-06 03:25:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:05:10 lr 0.000016	 wd 0.0500	time 0.5878 (0.6184)	loss 0.9418 (1.0865)	grad_norm 2.2276 (nan)	loss_scale 1024.0000 (1083.8741)	mem 20177MB
[2024-08-06 03:26:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:04:08 lr 0.000016	 wd 0.0500	time 0.5937 (0.6182)	loss 0.7575 (1.0857)	grad_norm 2.4436 (nan)	loss_scale 1024.0000 (1081.0243)	mem 20177MB
[2024-08-06 03:27:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:03:06 lr 0.000016	 wd 0.0500	time 0.6054 (0.6179)	loss 0.7257 (1.0871)	grad_norm 2.5714 (nan)	loss_scale 1024.0000 (1078.4334)	mem 20177MB
[2024-08-06 03:28:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:02:04 lr 0.000015	 wd 0.0500	time 0.5944 (0.6177)	loss 1.0280 (1.0872)	grad_norm 2.1655 (nan)	loss_scale 1024.0000 (1076.0678)	mem 20177MB
[2024-08-06 03:29:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:02 lr 0.000015	 wd 0.0500	time 0.5916 (0.6175)	loss 1.3305 (1.0878)	grad_norm 3.2209 (nan)	loss_scale 1024.0000 (1073.8992)	mem 20177MB
[2024-08-06 03:30:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.5884 (0.6172)	loss 0.7085 (1.0870)	grad_norm 2.2293 (nan)	loss_scale 1024.0000 (1071.9040)	mem 20177MB
[2024-08-06 03:30:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 12 training takes 0:25:46
[2024-08-06 03:30:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.013 (12.013)	Loss 0.4670 (0.4670)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 20177MB
[2024-08-06 03:30:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.754 Acc@5 98.040
[2024-08-06 03:30:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 03:30:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 03:30:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:37:08 lr 0.000015	 wd 0.0500	time 12.4016 (12.4016)	loss 1.2659 (1.2659)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:31:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:29:07 lr 0.000015	 wd 0.0500	time 0.5817 (0.7274)	loss 1.3237 (1.1468)	grad_norm 1.8855 (2.3363)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:32:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:25:41 lr 0.000015	 wd 0.0500	time 0.5830 (0.6697)	loss 1.3262 (1.1308)	grad_norm 1.7857 (2.7005)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:33:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:23:52 lr 0.000015	 wd 0.0500	time 0.5790 (0.6504)	loss 1.3299 (1.1305)	grad_norm 2.4882 (3.0191)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:35:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:22:27 lr 0.000015	 wd 0.0500	time 0.5874 (0.6408)	loss 1.3832 (1.1147)	grad_norm 2.2610 (3.0332)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:36:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:21:11 lr 0.000015	 wd 0.0500	time 0.5859 (0.6351)	loss 1.3033 (1.1079)	grad_norm 2.4304 (2.8938)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:37:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:20:00 lr 0.000015	 wd 0.0500	time 0.5892 (0.6313)	loss 1.1805 (1.1017)	grad_norm 2.0508 (2.8878)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:38:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:18:52 lr 0.000015	 wd 0.0500	time 0.5905 (0.6286)	loss 1.0260 (1.0954)	grad_norm 1.8572 (2.9048)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:39:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:17:46 lr 0.000015	 wd 0.0500	time 0.5859 (0.6266)	loss 1.3458 (1.0954)	grad_norm 2.0070 (2.8607)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:40:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:16:41 lr 0.000015	 wd 0.0500	time 0.5879 (0.6251)	loss 1.0841 (1.0949)	grad_norm 2.1808 (2.8627)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:41:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:15:37 lr 0.000015	 wd 0.0500	time 0.5943 (0.6239)	loss 1.2152 (1.0940)	grad_norm 2.3930 (2.8789)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:42:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:14:33 lr 0.000015	 wd 0.0500	time 0.5856 (0.6228)	loss 1.2555 (1.0908)	grad_norm 2.1950 (2.8981)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:43:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:13:29 lr 0.000015	 wd 0.0500	time 0.5834 (0.6220)	loss 1.1856 (1.0948)	grad_norm 2.6988 (2.8566)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:44:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:12:26 lr 0.000015	 wd 0.0500	time 0.5879 (0.6212)	loss 0.8139 (1.0945)	grad_norm 2.1447 (2.8570)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:45:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:11:23 lr 0.000015	 wd 0.0500	time 0.5904 (0.6206)	loss 1.3110 (1.0931)	grad_norm 1.6347 (3.0024)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:46:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:10:21 lr 0.000015	 wd 0.0500	time 0.5880 (0.6201)	loss 1.1091 (1.0934)	grad_norm 2.9863 (2.9602)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:47:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:09:18 lr 0.000015	 wd 0.0500	time 0.5937 (0.6197)	loss 0.9539 (1.0929)	grad_norm 3.0131 (2.9598)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 03:48:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:08:16 lr 0.000015	 wd 0.0500	time 0.5821 (0.6193)	loss 1.0608 (1.0938)	grad_norm 1.8942 (2.9284)	loss_scale 2048.0000 (1075.7719)	mem 20177MB
[2024-08-06 03:49:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:07:14 lr 0.000015	 wd 0.0500	time 0.5851 (0.6189)	loss 1.3279 (1.0941)	grad_norm 1.9752 (2.9563)	loss_scale 2048.0000 (1129.7546)	mem 20177MB
[2024-08-06 03:50:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:06:12 lr 0.000015	 wd 0.0500	time 0.5826 (0.6186)	loss 1.0645 (1.0953)	grad_norm 1.7628 (2.9261)	loss_scale 2048.0000 (1178.0579)	mem 20177MB
[2024-08-06 03:51:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:05:10 lr 0.000015	 wd 0.0500	time 0.5861 (0.6183)	loss 1.2676 (1.0950)	grad_norm 1.8931 (2.9091)	loss_scale 2048.0000 (1221.5332)	mem 20177MB
[2024-08-06 03:52:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:04:08 lr 0.000014	 wd 0.0500	time 0.5811 (0.6180)	loss 1.3612 (1.0954)	grad_norm 1.7700 (2.8795)	loss_scale 2048.0000 (1260.8701)	mem 20177MB
[2024-08-06 03:53:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:03:06 lr 0.000014	 wd 0.0500	time 0.5821 (0.6178)	loss 1.1387 (1.0939)	grad_norm 2.3724 (2.8768)	loss_scale 2048.0000 (1296.6324)	mem 20177MB
[2024-08-06 03:54:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:02:04 lr 0.000014	 wd 0.0500	time 0.5886 (0.6175)	loss 1.2946 (1.0935)	grad_norm 2.2575 (2.8588)	loss_scale 2048.0000 (1329.2864)	mem 20177MB
[2024-08-06 03:55:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:02 lr 0.000014	 wd 0.0500	time 0.5887 (0.6173)	loss 0.7995 (1.0962)	grad_norm 2.7755 (2.8344)	loss_scale 2048.0000 (1359.2203)	mem 20177MB
[2024-08-06 03:56:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.5896 (0.6171)	loss 0.7923 (1.0945)	grad_norm 2.4391 (2.8261)	loss_scale 2048.0000 (1386.7605)	mem 20177MB
[2024-08-06 03:56:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 13 training takes 0:25:46
[2024-08-06 03:56:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.454 (12.454)	Loss 0.4524 (0.4524)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 03:57:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.768 Acc@5 98.032
[2024-08-06 03:57:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 03:57:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 03:57:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][0/2502]	eta 8:03:23 lr 0.000014	 wd 0.0500	time 11.5923 (11.5923)	loss 1.2627 (1.2627)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 03:58:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:28:53 lr 0.000014	 wd 0.0500	time 0.5848 (0.7218)	loss 0.9854 (1.0968)	grad_norm 4.0164 (2.5190)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 03:59:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:25:35 lr 0.000014	 wd 0.0500	time 0.5769 (0.6670)	loss 1.3184 (1.1031)	grad_norm 1.8313 (2.4915)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:00:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:23:48 lr 0.000014	 wd 0.0500	time 0.5790 (0.6487)	loss 0.8931 (1.1049)	grad_norm 3.6803 (2.4840)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:01:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:22:24 lr 0.000014	 wd 0.0500	time 0.5854 (0.6396)	loss 1.4895 (1.1063)	grad_norm 2.6302 (2.4308)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:02:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:21:09 lr 0.000014	 wd 0.0500	time 0.5934 (0.6342)	loss 0.9006 (1.1021)	grad_norm 2.5816 (2.4134)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:03:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:19:59 lr 0.000014	 wd 0.0500	time 0.5620 (0.6306)	loss 0.9420 (1.0993)	grad_norm 2.1578 (2.4872)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:04:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:18:51 lr 0.000014	 wd 0.0500	time 0.5869 (0.6280)	loss 1.1630 (1.1008)	grad_norm 3.1088 (2.5269)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:05:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:17:45 lr 0.000014	 wd 0.0500	time 0.5913 (0.6260)	loss 0.7420 (1.0962)	grad_norm 2.0207 (2.4978)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:06:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:16:40 lr 0.000014	 wd 0.0500	time 0.5987 (0.6246)	loss 0.7607 (1.0949)	grad_norm 1.8394 (2.5414)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:07:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:15:36 lr 0.000014	 wd 0.0500	time 0.5886 (0.6234)	loss 0.9385 (1.0932)	grad_norm 2.0383 (2.5807)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:08:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:14:32 lr 0.000014	 wd 0.0500	time 0.5867 (0.6224)	loss 1.3344 (1.0941)	grad_norm 1.8754 (2.5659)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:09:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:13:29 lr 0.000014	 wd 0.0500	time 0.5903 (0.6216)	loss 1.4586 (1.0939)	grad_norm 2.1397 (2.5724)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:10:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:12:26 lr 0.000014	 wd 0.0500	time 0.5839 (0.6209)	loss 1.1295 (1.0973)	grad_norm 2.1145 (2.5706)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:11:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:11:23 lr 0.000014	 wd 0.0500	time 0.5848 (0.6203)	loss 1.2388 (1.0978)	grad_norm 2.3771 (2.5755)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:12:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:10:21 lr 0.000014	 wd 0.0500	time 0.5849 (0.6198)	loss 1.3686 (1.0963)	grad_norm 2.1704 (2.5941)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:13:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:09:18 lr 0.000014	 wd 0.0500	time 0.5865 (0.6194)	loss 0.8594 (1.0954)	grad_norm 2.0827 (2.6218)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:14:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:08:16 lr 0.000014	 wd 0.0500	time 0.6007 (0.6190)	loss 0.7926 (1.0949)	grad_norm 1.6973 (2.6417)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:15:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:07:14 lr 0.000013	 wd 0.0500	time 0.5881 (0.6187)	loss 0.9548 (1.0961)	grad_norm 2.4845 (2.6783)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:16:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:06:12 lr 0.000013	 wd 0.0500	time 0.5956 (0.6183)	loss 1.1892 (1.0968)	grad_norm 2.1377 (2.7310)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:17:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:05:10 lr 0.000013	 wd 0.0500	time 0.5819 (0.6181)	loss 1.3385 (1.0955)	grad_norm 2.2384 (2.7378)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:18:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:04:08 lr 0.000013	 wd 0.0500	time 0.5903 (0.6178)	loss 1.2125 (1.0946)	grad_norm 2.9956 (2.7293)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:19:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:03:06 lr 0.000013	 wd 0.0500	time 0.5866 (0.6176)	loss 1.1922 (1.0939)	grad_norm 3.9271 (2.7644)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:20:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:02:04 lr 0.000013	 wd 0.0500	time 0.5856 (0.6174)	loss 0.7927 (1.0934)	grad_norm 1.6861 (2.7557)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:21:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:02 lr 0.000013	 wd 0.0500	time 0.5885 (0.6172)	loss 0.7364 (1.0929)	grad_norm 3.0455 (2.7408)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:22:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.5871 (0.6170)	loss 1.2418 (1.0923)	grad_norm 2.1795 (2.7248)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:22:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 14 training takes 0:25:46
[2024-08-06 04:23:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.815 (11.815)	Loss 0.4438 (0.4438)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 20177MB
[2024-08-06 04:23:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.746 Acc@5 98.060
[2024-08-06 04:23:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 04:23:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 04:23:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:46:10 lr 0.000013	 wd 0.0500	time 11.1793 (11.1793)	loss 1.1922 (1.1922)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:24:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:28:52 lr 0.000013	 wd 0.0500	time 0.5878 (0.7212)	loss 0.7406 (1.0909)	grad_norm 3.6994 (2.9155)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:25:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:25:34 lr 0.000013	 wd 0.0500	time 0.5884 (0.6666)	loss 1.4555 (1.0808)	grad_norm 1.9999 (2.8213)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:26:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:23:48 lr 0.000013	 wd 0.0500	time 0.5932 (0.6485)	loss 0.8459 (1.0790)	grad_norm 2.0581 (2.6907)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:27:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:22:24 lr 0.000013	 wd 0.0500	time 0.5903 (0.6394)	loss 0.7292 (1.0773)	grad_norm 2.4872 (2.6504)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:28:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:21:09 lr 0.000013	 wd 0.0500	time 0.5852 (0.6340)	loss 0.6819 (1.0862)	grad_norm 2.2222 (2.7090)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:29:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:19:59 lr 0.000013	 wd 0.0500	time 0.5928 (0.6304)	loss 1.1994 (1.0915)	grad_norm 2.6447 (2.7004)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:30:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:18:51 lr 0.000013	 wd 0.0500	time 0.5862 (0.6279)	loss 1.2653 (1.0938)	grad_norm 1.7188 (nan)	loss_scale 2048.0000 (2100.5877)	mem 20177MB
[2024-08-06 04:31:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:17:45 lr 0.000013	 wd 0.0500	time 0.5848 (0.6259)	loss 1.1167 (1.0947)	grad_norm 2.1189 (nan)	loss_scale 2048.0000 (2094.0225)	mem 20177MB
[2024-08-06 04:32:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:16:40 lr 0.000013	 wd 0.0500	time 0.5858 (0.6244)	loss 1.3117 (1.0960)	grad_norm 2.0616 (nan)	loss_scale 2048.0000 (2088.9145)	mem 20177MB
[2024-08-06 04:33:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:15:36 lr 0.000013	 wd 0.0500	time 0.5925 (0.6233)	loss 0.7418 (1.0957)	grad_norm 1.9479 (nan)	loss_scale 2048.0000 (2084.8272)	mem 20177MB
[2024-08-06 04:34:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:14:32 lr 0.000013	 wd 0.0500	time 0.5940 (0.6223)	loss 0.8062 (1.0967)	grad_norm 2.0281 (nan)	loss_scale 2048.0000 (2081.4823)	mem 20177MB
[2024-08-06 04:35:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:13:29 lr 0.000013	 wd 0.0500	time 0.5895 (0.6214)	loss 0.8352 (1.0972)	grad_norm 1.9816 (nan)	loss_scale 2048.0000 (2078.6944)	mem 20177MB
[2024-08-06 04:36:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:12:26 lr 0.000013	 wd 0.0500	time 0.5749 (0.6207)	loss 0.8825 (1.0985)	grad_norm 1.8256 (nan)	loss_scale 2048.0000 (2076.3351)	mem 20177MB
[2024-08-06 04:37:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:11:23 lr 0.000012	 wd 0.0500	time 0.6057 (0.6201)	loss 0.7399 (1.0982)	grad_norm 2.3139 (nan)	loss_scale 2048.0000 (2074.3126)	mem 20177MB
[2024-08-06 04:38:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:10:20 lr 0.000012	 wd 0.0500	time 0.5799 (0.6196)	loss 1.0344 (1.0943)	grad_norm 2.2722 (nan)	loss_scale 2048.0000 (2072.5596)	mem 20177MB
[2024-08-06 04:39:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:09:18 lr 0.000012	 wd 0.0500	time 0.5853 (0.6192)	loss 0.8762 (1.0954)	grad_norm 3.4361 (nan)	loss_scale 2048.0000 (2071.0256)	mem 20177MB
[2024-08-06 04:40:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:08:16 lr 0.000012	 wd 0.0500	time 0.5894 (0.6188)	loss 0.7521 (1.0958)	grad_norm 1.9992 (nan)	loss_scale 2048.0000 (2069.6720)	mem 20177MB
[2024-08-06 04:41:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:07:14 lr 0.000012	 wd 0.0500	time 0.5906 (0.6184)	loss 1.5812 (1.0944)	grad_norm 2.1057 (nan)	loss_scale 2048.0000 (2068.4686)	mem 20177MB
[2024-08-06 04:42:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:06:12 lr 0.000012	 wd 0.0500	time 0.5991 (0.6181)	loss 0.8468 (1.0935)	grad_norm 2.1603 (nan)	loss_scale 2048.0000 (2067.3919)	mem 20177MB
[2024-08-06 04:43:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:05:10 lr 0.000012	 wd 0.0500	time 0.5860 (0.6178)	loss 0.8816 (1.0945)	grad_norm 2.2660 (nan)	loss_scale 2048.0000 (2066.4228)	mem 20177MB
[2024-08-06 04:45:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:04:08 lr 0.000012	 wd 0.0500	time 0.5881 (0.6176)	loss 0.7081 (1.0933)	grad_norm 1.9690 (nan)	loss_scale 2048.0000 (2065.5459)	mem 20177MB
[2024-08-06 04:46:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:03:06 lr 0.000012	 wd 0.0500	time 0.5860 (0.6173)	loss 0.8286 (1.0927)	grad_norm 2.0271 (nan)	loss_scale 2048.0000 (2064.7488)	mem 20177MB
[2024-08-06 04:47:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:02:04 lr 0.000012	 wd 0.0500	time 0.5862 (0.6171)	loss 0.9335 (1.0932)	grad_norm 3.0179 (nan)	loss_scale 2048.0000 (2064.0209)	mem 20177MB
[2024-08-06 04:48:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:02 lr 0.000012	 wd 0.0500	time 0.5783 (0.6170)	loss 1.2594 (1.0935)	grad_norm 2.4925 (nan)	loss_scale 2048.0000 (2063.3536)	mem 20177MB
[2024-08-06 04:49:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.5900 (0.6168)	loss 1.3803 (1.0943)	grad_norm 2.7776 (nan)	loss_scale 2048.0000 (2062.7397)	mem 20177MB
[2024-08-06 04:49:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 15 training takes 0:25:45
[2024-08-06 04:49:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.132 (12.132)	Loss 0.4900 (0.4900)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 20177MB
[2024-08-06 04:49:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.780 Acc@5 97.998
[2024-08-06 04:49:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 04:49:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 04:49:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-06 04:49:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-06 04:49:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:37:05 lr 0.000012	 wd 0.0500	time 10.9612 (10.9612)	loss 1.1739 (1.1739)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:50:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:28:31 lr 0.000012	 wd 0.0500	time 0.5893 (0.7126)	loss 1.0112 (1.1034)	grad_norm 2.3148 (2.7503)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:51:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:25:24 lr 0.000012	 wd 0.0500	time 0.5827 (0.6622)	loss 1.1509 (1.0931)	grad_norm 1.9048 (2.6195)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:52:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:23:40 lr 0.000012	 wd 0.0500	time 0.5848 (0.6453)	loss 0.9858 (1.0867)	grad_norm 2.2736 (2.6251)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:53:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:22:18 lr 0.000012	 wd 0.0500	time 0.5919 (0.6368)	loss 0.7913 (1.0765)	grad_norm 1.8909 (2.6906)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:54:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:21:05 lr 0.000012	 wd 0.0500	time 0.5861 (0.6319)	loss 1.3640 (1.0808)	grad_norm 2.2329 (2.7462)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:56:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:19:55 lr 0.000012	 wd 0.0500	time 0.5809 (0.6287)	loss 1.3416 (1.0794)	grad_norm 1.9480 (2.7596)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:57:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:18:48 lr 0.000012	 wd 0.0500	time 0.5819 (0.6264)	loss 0.9651 (1.0815)	grad_norm 2.0130 (2.7780)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:58:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:17:43 lr 0.000012	 wd 0.0500	time 0.5801 (0.6246)	loss 1.2407 (1.0829)	grad_norm 2.0996 (2.7809)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 04:59:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:16:38 lr 0.000012	 wd 0.0500	time 0.5857 (0.6233)	loss 1.0033 (1.0812)	grad_norm 2.0908 (2.7180)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:00:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:15:34 lr 0.000011	 wd 0.0500	time 0.5882 (0.6222)	loss 1.2563 (1.0825)	grad_norm 1.6276 (2.7059)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:01:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:14:31 lr 0.000011	 wd 0.0500	time 0.5948 (0.6213)	loss 0.8729 (1.0876)	grad_norm 1.7766 (2.7035)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:02:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:13:27 lr 0.000011	 wd 0.0500	time 0.5836 (0.6206)	loss 0.8600 (1.0870)	grad_norm 2.6210 (2.7236)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:03:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:12:25 lr 0.000011	 wd 0.0500	time 0.5813 (0.6200)	loss 1.1897 (1.0900)	grad_norm 1.9756 (2.7062)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:04:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:11:22 lr 0.000011	 wd 0.0500	time 0.5893 (0.6195)	loss 1.1844 (1.0907)	grad_norm 2.6949 (2.7326)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:05:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:10:20 lr 0.000011	 wd 0.0500	time 0.5907 (0.6191)	loss 1.1376 (1.0927)	grad_norm 1.8801 (2.7513)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:06:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:09:18 lr 0.000011	 wd 0.0500	time 0.5799 (0.6187)	loss 1.1954 (1.0929)	grad_norm 2.1548 (2.7261)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:07:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:08:15 lr 0.000011	 wd 0.0500	time 0.5803 (0.6184)	loss 0.9854 (1.0924)	grad_norm 2.1248 (2.8270)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 05:08:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:07:13 lr 0.000011	 wd 0.0500	time 0.5868 (0.6180)	loss 0.8504 (1.0914)	grad_norm 1.8355 (nan)	loss_scale 1024.0000 (2013.8856)	mem 20177MB
[2024-08-06 05:09:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:06:11 lr 0.000011	 wd 0.0500	time 0.5907 (0.6177)	loss 0.7964 (1.0922)	grad_norm 1.9213 (nan)	loss_scale 1024.0000 (1961.8138)	mem 20177MB
[2024-08-06 05:10:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:05:09 lr 0.000011	 wd 0.0500	time 0.5923 (0.6174)	loss 1.2502 (1.0928)	grad_norm 2.3662 (nan)	loss_scale 1024.0000 (1914.9465)	mem 20177MB
[2024-08-06 05:11:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:04:08 lr 0.000011	 wd 0.0500	time 0.5917 (0.6172)	loss 1.1974 (1.0936)	grad_norm 1.8873 (nan)	loss_scale 1024.0000 (1872.5407)	mem 20177MB
[2024-08-06 05:12:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:03:06 lr 0.000011	 wd 0.0500	time 0.5931 (0.6170)	loss 0.8633 (1.0933)	grad_norm 3.4564 (nan)	loss_scale 1024.0000 (1833.9882)	mem 20177MB
[2024-08-06 05:13:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:02:04 lr 0.000011	 wd 0.0500	time 0.5874 (0.6168)	loss 0.7511 (1.0916)	grad_norm 2.1948 (nan)	loss_scale 1024.0000 (1798.7866)	mem 20177MB
[2024-08-06 05:14:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:02 lr 0.000011	 wd 0.0500	time 0.5791 (0.6166)	loss 1.1657 (1.0912)	grad_norm 1.5586 (nan)	loss_scale 1024.0000 (1766.5173)	mem 20177MB
[2024-08-06 05:15:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.5673 (0.6164)	loss 0.7795 (1.0908)	grad_norm 2.1557 (nan)	loss_scale 1024.0000 (1736.8285)	mem 20177MB
[2024-08-06 05:15:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 16 training takes 0:25:44
[2024-08-06 05:15:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.097 (12.097)	Loss 0.4619 (0.4619)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 05:16:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.772 Acc@5 98.016
[2024-08-06 05:16:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 05:16:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.78%
[2024-08-06 05:16:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:35:06 lr 0.000011	 wd 0.0500	time 12.3527 (12.3527)	loss 1.2023 (1.2023)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:17:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:29:10 lr 0.000011	 wd 0.0500	time 0.5900 (0.7288)	loss 1.1959 (1.0691)	grad_norm 1.9571 (2.5053)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:18:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:25:42 lr 0.000011	 wd 0.0500	time 0.5853 (0.6702)	loss 1.3196 (1.0729)	grad_norm 1.9784 (2.5731)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:19:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:23:53 lr 0.000011	 wd 0.0500	time 0.5912 (0.6509)	loss 1.3681 (1.0869)	grad_norm 3.5814 (2.6100)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:20:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:22:27 lr 0.000011	 wd 0.0500	time 0.5904 (0.6413)	loss 1.4602 (1.0830)	grad_norm 2.3321 (2.5546)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:21:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:21:12 lr 0.000010	 wd 0.0500	time 0.5874 (0.6354)	loss 1.3117 (1.0896)	grad_norm 2.0811 (2.6334)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:22:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:20:01 lr 0.000010	 wd 0.0500	time 0.5893 (0.6316)	loss 0.8337 (1.0910)	grad_norm 2.1334 (2.5765)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:23:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:18:53 lr 0.000010	 wd 0.0500	time 0.5896 (0.6289)	loss 1.2840 (1.0882)	grad_norm 2.2412 (2.5304)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:24:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:17:46 lr 0.000010	 wd 0.0500	time 0.5884 (0.6267)	loss 1.1395 (1.0880)	grad_norm 2.1519 (2.5507)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:25:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:16:41 lr 0.000010	 wd 0.0500	time 0.5967 (0.6252)	loss 1.2308 (1.0899)	grad_norm 1.7044 (2.5508)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:26:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:15:37 lr 0.000010	 wd 0.0500	time 0.5854 (0.6239)	loss 0.9281 (1.0885)	grad_norm 1.8540 (2.5376)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:27:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:14:33 lr 0.000010	 wd 0.0500	time 0.5928 (0.6229)	loss 0.9192 (1.0931)	grad_norm 1.8157 (2.5479)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:28:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:13:29 lr 0.000010	 wd 0.0500	time 0.5911 (0.6220)	loss 0.7798 (1.0932)	grad_norm 3.0065 (2.5361)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:29:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:12:26 lr 0.000010	 wd 0.0500	time 0.5865 (0.6213)	loss 1.4110 (1.0919)	grad_norm 2.4211 (2.5396)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:30:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:11:24 lr 0.000010	 wd 0.0500	time 0.5862 (0.6207)	loss 1.3881 (1.0920)	grad_norm 1.8673 (2.5529)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:31:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:10:21 lr 0.000010	 wd 0.0500	time 0.5872 (0.6201)	loss 1.2505 (1.0921)	grad_norm 2.6519 (2.5851)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:32:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:09:18 lr 0.000010	 wd 0.0500	time 0.5860 (0.6197)	loss 1.2773 (1.0906)	grad_norm 1.8702 (2.5792)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:33:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:08:16 lr 0.000010	 wd 0.0500	time 0.5792 (0.6192)	loss 1.4704 (1.0924)	grad_norm 1.7547 (2.5678)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:34:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:07:14 lr 0.000010	 wd 0.0500	time 0.5921 (0.6188)	loss 0.9345 (1.0924)	grad_norm 7.8748 (2.5697)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:35:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:06:12 lr 0.000010	 wd 0.0500	time 0.5951 (0.6185)	loss 1.1592 (1.0914)	grad_norm 2.8244 (2.5561)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:36:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:05:10 lr 0.000010	 wd 0.0500	time 0.5870 (0.6182)	loss 0.9366 (1.0922)	grad_norm 3.2656 (2.5611)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:37:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:04:08 lr 0.000010	 wd 0.0500	time 0.5904 (0.6179)	loss 1.3691 (1.0923)	grad_norm 2.3129 (2.5530)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:38:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:03:06 lr 0.000010	 wd 0.0500	time 0.5874 (0.6177)	loss 1.4120 (1.0930)	grad_norm 2.3569 (2.5602)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:39:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:02:04 lr 0.000010	 wd 0.0500	time 0.5877 (0.6174)	loss 0.7933 (1.0927)	grad_norm 2.0140 (2.5882)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:40:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:02 lr 0.000010	 wd 0.0500	time 0.5824 (0.6172)	loss 0.8051 (1.0912)	grad_norm 1.7031 (2.5733)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:41:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.5867 (0.6170)	loss 1.4189 (1.0914)	grad_norm 2.6196 (2.5586)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:41:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 17 training takes 0:25:46
[2024-08-06 05:41:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.977 (11.977)	Loss 0.4492 (0.4492)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 05:42:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.824 Acc@5 98.038
[2024-08-06 05:42:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 05:42:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.82%
[2024-08-06 05:42:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-06 05:42:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-06 05:42:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:01:50 lr 0.000009	 wd 0.0500	time 11.5550 (11.5550)	loss 1.4169 (1.4169)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:43:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:28:46 lr 0.000009	 wd 0.0500	time 0.5808 (0.7186)	loss 1.4306 (1.1005)	grad_norm 2.0439 (2.9936)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:44:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:25:31 lr 0.000009	 wd 0.0500	time 0.5870 (0.6652)	loss 0.9167 (1.1041)	grad_norm 2.4588 (2.6555)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:45:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:23:45 lr 0.000009	 wd 0.0500	time 0.5990 (0.6474)	loss 1.5168 (1.1056)	grad_norm 2.1715 (2.6056)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:46:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:22:22 lr 0.000009	 wd 0.0500	time 0.5930 (0.6386)	loss 0.6964 (1.1051)	grad_norm 8.3414 (2.5782)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:47:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:21:07 lr 0.000009	 wd 0.0500	time 0.5885 (0.6333)	loss 1.3049 (1.1015)	grad_norm 1.8046 (2.5523)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:48:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:19:58 lr 0.000009	 wd 0.0500	time 0.5894 (0.6299)	loss 1.4397 (1.0959)	grad_norm 2.3668 (2.5345)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:49:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:18:50 lr 0.000009	 wd 0.0500	time 0.5941 (0.6274)	loss 1.2955 (1.0986)	grad_norm 1.8841 (2.6217)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 05:50:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:17:44 lr 0.000009	 wd 0.0500	time 0.5987 (0.6256)	loss 1.2998 (1.1023)	grad_norm 4.8010 (2.6069)	loss_scale 2048.0000 (1105.8177)	mem 20177MB
[2024-08-06 05:51:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:16:39 lr 0.000009	 wd 0.0500	time 0.5910 (0.6241)	loss 0.9069 (1.1007)	grad_norm 4.1887 (2.6117)	loss_scale 2048.0000 (1210.3885)	mem 20177MB
[2024-08-06 05:52:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:15:35 lr 0.000009	 wd 0.0500	time 0.5883 (0.6230)	loss 1.3398 (1.0989)	grad_norm 2.9692 (2.6501)	loss_scale 2048.0000 (1294.0659)	mem 20177MB
[2024-08-06 05:53:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:14:32 lr 0.000009	 wd 0.0500	time 0.5895 (0.6221)	loss 0.9494 (1.1003)	grad_norm 3.7499 (2.6744)	loss_scale 2048.0000 (1362.5431)	mem 20177MB
[2024-08-06 05:54:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:13:28 lr 0.000009	 wd 0.0500	time 0.5868 (0.6213)	loss 0.9391 (1.0951)	grad_norm 1.9203 (2.6810)	loss_scale 2048.0000 (1419.6170)	mem 20177MB
[2024-08-06 05:55:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:12:26 lr 0.000009	 wd 0.0500	time 0.5890 (0.6207)	loss 1.4373 (1.0949)	grad_norm 2.1208 (2.6904)	loss_scale 2048.0000 (1467.9170)	mem 20177MB
[2024-08-06 05:56:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:11:23 lr 0.000009	 wd 0.0500	time 0.5884 (0.6201)	loss 1.1960 (1.0956)	grad_norm 2.0738 (2.6903)	loss_scale 2048.0000 (1509.3219)	mem 20177MB
[2024-08-06 05:57:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:10:20 lr 0.000009	 wd 0.0500	time 0.6100 (0.6196)	loss 1.2819 (1.0958)	grad_norm 1.9230 (2.6726)	loss_scale 2048.0000 (1545.2099)	mem 20177MB
[2024-08-06 05:58:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:09:18 lr 0.000009	 wd 0.0500	time 0.5696 (0.6192)	loss 1.2645 (1.0973)	grad_norm 2.0248 (2.6744)	loss_scale 2048.0000 (1576.6146)	mem 20177MB
[2024-08-06 05:59:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:08:16 lr 0.000009	 wd 0.0500	time 0.5949 (0.6188)	loss 1.2296 (1.0986)	grad_norm 1.8551 (2.6748)	loss_scale 2048.0000 (1604.3269)	mem 20177MB
[2024-08-06 06:00:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:07:14 lr 0.000009	 wd 0.0500	time 0.5898 (0.6184)	loss 0.7281 (1.0989)	grad_norm 2.5740 (2.6674)	loss_scale 2048.0000 (1628.9617)	mem 20177MB
[2024-08-06 06:01:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:06:12 lr 0.000009	 wd 0.0500	time 0.5822 (0.6181)	loss 1.4264 (1.0988)	grad_norm 1.6184 (2.6819)	loss_scale 2048.0000 (1651.0047)	mem 20177MB
[2024-08-06 06:02:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:05:10 lr 0.000008	 wd 0.0500	time 0.5891 (0.6178)	loss 0.9231 (1.0980)	grad_norm 2.2377 (2.6973)	loss_scale 2048.0000 (1670.8446)	mem 20177MB
[2024-08-06 06:03:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:04:08 lr 0.000008	 wd 0.0500	time 0.5555 (0.6175)	loss 1.2363 (1.0977)	grad_norm 2.0558 (2.6984)	loss_scale 2048.0000 (1688.7958)	mem 20177MB
[2024-08-06 06:05:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:03:06 lr 0.000008	 wd 0.0500	time 0.5820 (0.6173)	loss 1.0196 (1.0979)	grad_norm 2.6340 (2.6841)	loss_scale 2048.0000 (1705.1159)	mem 20177MB
[2024-08-06 06:06:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:02:04 lr 0.000008	 wd 0.0500	time 0.5905 (0.6171)	loss 1.2957 (1.0988)	grad_norm 2.6086 (2.6742)	loss_scale 2048.0000 (1720.0174)	mem 20177MB
[2024-08-06 06:07:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:02 lr 0.000008	 wd 0.0500	time 0.5854 (0.6169)	loss 1.3444 (1.0991)	grad_norm 3.9108 (2.7030)	loss_scale 2048.0000 (1733.6776)	mem 20177MB
[2024-08-06 06:08:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.5882 (0.6167)	loss 1.0899 (1.0994)	grad_norm 1.8611 (2.7127)	loss_scale 2048.0000 (1746.2455)	mem 20177MB
[2024-08-06 06:08:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 18 training takes 0:25:45
[2024-08-06 06:08:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.938 (12.938)	Loss 0.4617 (0.4617)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 06:08:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.858 Acc@5 98.032
[2024-08-06 06:08:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 06:08:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-06 06:08:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-06 06:08:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-06 06:08:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:26:09 lr 0.000008	 wd 0.0500	time 10.6991 (10.6991)	loss 0.7958 (0.7958)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:09:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:28:27 lr 0.000008	 wd 0.0500	time 0.5752 (0.7108)	loss 1.5066 (1.1405)	grad_norm 1.7136 (2.5280)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:10:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:25:21 lr 0.000008	 wd 0.0500	time 0.5838 (0.6610)	loss 0.8419 (1.1102)	grad_norm 1.9295 (2.8335)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:11:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:23:39 lr 0.000008	 wd 0.0500	time 0.5794 (0.6447)	loss 1.3615 (1.0994)	grad_norm 3.0316 (2.8558)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:12:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:22:18 lr 0.000008	 wd 0.0500	time 0.5818 (0.6366)	loss 0.9777 (1.0910)	grad_norm 2.0275 (2.8847)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:13:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:21:04 lr 0.000008	 wd 0.0500	time 0.5786 (0.6317)	loss 1.3265 (1.0983)	grad_norm 2.4814 (2.8552)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:14:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:19:55 lr 0.000008	 wd 0.0500	time 0.5812 (0.6285)	loss 0.8421 (1.0910)	grad_norm 2.0714 (2.8337)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:16:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:18:48 lr 0.000008	 wd 0.0500	time 0.5846 (0.6262)	loss 1.0503 (1.0898)	grad_norm 2.1003 (2.7972)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 06:17:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:17:42 lr 0.000008	 wd 0.0500	time 0.5901 (0.6244)	loss 1.3262 (1.0910)	grad_norm 4.4627 (inf)	loss_scale 1024.0000 (2032.6592)	mem 20177MB
[2024-08-06 06:18:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:16:38 lr 0.000008	 wd 0.0500	time 0.5858 (0.6231)	loss 1.1333 (1.0920)	grad_norm 2.5981 (inf)	loss_scale 1024.0000 (1920.7103)	mem 20177MB
[2024-08-06 06:19:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:15:34 lr 0.000008	 wd 0.0500	time 0.5854 (0.6221)	loss 1.3142 (1.0923)	grad_norm 2.4892 (inf)	loss_scale 1024.0000 (1831.1289)	mem 20177MB
[2024-08-06 06:20:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:14:30 lr 0.000008	 wd 0.0500	time 0.5953 (0.6212)	loss 1.0686 (1.0905)	grad_norm 3.6234 (inf)	loss_scale 1024.0000 (1757.8202)	mem 20177MB
[2024-08-06 06:21:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:13:27 lr 0.000008	 wd 0.0500	time 0.6062 (0.6204)	loss 1.2827 (1.0889)	grad_norm 1.8308 (inf)	loss_scale 1024.0000 (1696.7194)	mem 20177MB
[2024-08-06 06:22:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:12:25 lr 0.000008	 wd 0.0500	time 0.5858 (0.6198)	loss 0.8751 (1.0885)	grad_norm 4.2647 (inf)	loss_scale 1024.0000 (1645.0115)	mem 20177MB
[2024-08-06 06:23:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:11:22 lr 0.000008	 wd 0.0500	time 0.5931 (0.6193)	loss 0.8429 (1.0884)	grad_norm 2.8936 (inf)	loss_scale 1024.0000 (1600.6852)	mem 20177MB
[2024-08-06 06:24:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:10:20 lr 0.000008	 wd 0.0500	time 0.5859 (0.6189)	loss 1.4144 (1.0893)	grad_norm 2.4693 (inf)	loss_scale 1024.0000 (1562.2652)	mem 20177MB
[2024-08-06 06:25:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:09:17 lr 0.000007	 wd 0.0500	time 0.5935 (0.6185)	loss 1.1485 (1.0908)	grad_norm 2.3547 (inf)	loss_scale 1024.0000 (1528.6446)	mem 20177MB
[2024-08-06 06:26:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:08:15 lr 0.000007	 wd 0.0500	time 0.5884 (0.6181)	loss 0.7417 (1.0912)	grad_norm 3.2802 (inf)	loss_scale 1024.0000 (1498.9771)	mem 20177MB
[2024-08-06 06:27:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:07:13 lr 0.000007	 wd 0.0500	time 0.5917 (0.6178)	loss 1.2010 (1.0930)	grad_norm 1.8087 (inf)	loss_scale 1024.0000 (1472.6041)	mem 20177MB
[2024-08-06 06:28:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:06:11 lr 0.000007	 wd 0.0500	time 0.5950 (0.6175)	loss 1.1024 (1.0915)	grad_norm 2.1884 (inf)	loss_scale 1024.0000 (1449.0058)	mem 20177MB
[2024-08-06 06:29:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:05:09 lr 0.000007	 wd 0.0500	time 0.5962 (0.6173)	loss 1.2219 (1.0926)	grad_norm 2.5559 (inf)	loss_scale 1024.0000 (1427.7661)	mem 20177MB
[2024-08-06 06:30:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:04:08 lr 0.000007	 wd 0.0500	time 0.5947 (0.6171)	loss 1.5530 (1.0946)	grad_norm 1.7028 (inf)	loss_scale 1024.0000 (1408.5483)	mem 20177MB
[2024-08-06 06:31:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:03:06 lr 0.000007	 wd 0.0500	time 0.5895 (0.6169)	loss 1.1979 (1.0939)	grad_norm 2.7771 (inf)	loss_scale 1024.0000 (1391.0768)	mem 20177MB
[2024-08-06 06:32:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:02:04 lr 0.000007	 wd 0.0500	time 0.5929 (0.6167)	loss 0.9345 (1.0936)	grad_norm 5.1906 (inf)	loss_scale 1024.0000 (1375.1239)	mem 20177MB
[2024-08-06 06:33:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:02 lr 0.000007	 wd 0.0500	time 0.5901 (0.6165)	loss 1.0645 (1.0929)	grad_norm 1.9935 (inf)	loss_scale 1024.0000 (1360.4998)	mem 20177MB
[2024-08-06 06:34:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.5870 (0.6163)	loss 0.7777 (1.0929)	grad_norm 2.4501 (inf)	loss_scale 1024.0000 (1347.0452)	mem 20177MB
[2024-08-06 06:34:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 19 training takes 0:25:44
[2024-08-06 06:34:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.679 (12.679)	Loss 0.4453 (0.4453)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 06:35:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.854 Acc@5 98.038
[2024-08-06 06:35:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 06:35:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-06 06:35:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:51:54 lr 0.000007	 wd 0.0500	time 11.3166 (11.3166)	loss 1.2881 (1.2881)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:36:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:28:49 lr 0.000007	 wd 0.0500	time 0.5841 (0.7200)	loss 0.8876 (1.1017)	grad_norm 2.5318 (2.3748)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:37:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:25:32 lr 0.000007	 wd 0.0500	time 0.5946 (0.6658)	loss 1.2487 (1.0905)	grad_norm 3.5865 (2.3434)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:38:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:23:46 lr 0.000007	 wd 0.0500	time 0.5606 (0.6479)	loss 0.6950 (1.0994)	grad_norm 2.4095 (2.5970)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:39:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:22:23 lr 0.000007	 wd 0.0500	time 0.5867 (0.6391)	loss 1.2174 (1.0967)	grad_norm 1.8061 (2.6168)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:40:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:21:08 lr 0.000007	 wd 0.0500	time 0.5886 (0.6338)	loss 1.2347 (1.0959)	grad_norm 1.7419 (2.5772)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:41:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:19:58 lr 0.000007	 wd 0.0500	time 0.5901 (0.6303)	loss 1.2984 (1.0998)	grad_norm 2.5536 (2.5685)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:42:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:18:51 lr 0.000007	 wd 0.0500	time 0.5848 (0.6277)	loss 1.2286 (1.0982)	grad_norm 2.0185 (2.5619)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:43:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:17:45 lr 0.000007	 wd 0.0500	time 0.5836 (0.6258)	loss 1.1390 (1.1000)	grad_norm 2.1038 (2.5600)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:44:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:16:40 lr 0.000007	 wd 0.0500	time 0.5964 (0.6244)	loss 1.3072 (1.0985)	grad_norm 1.8206 (2.5606)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:45:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:15:36 lr 0.000007	 wd 0.0500	time 0.5984 (0.6233)	loss 1.1879 (1.1000)	grad_norm 2.7532 (2.5361)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:46:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:14:32 lr 0.000007	 wd 0.0500	time 0.5914 (0.6223)	loss 0.8681 (1.0980)	grad_norm 2.0543 (2.5251)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:47:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:13:29 lr 0.000006	 wd 0.0500	time 0.5850 (0.6216)	loss 1.1730 (1.0951)	grad_norm 1.7579 (2.5088)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:48:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:12:26 lr 0.000006	 wd 0.0500	time 0.5925 (0.6209)	loss 1.3322 (1.0950)	grad_norm 1.8562 (2.5027)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:49:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:11:23 lr 0.000006	 wd 0.0500	time 0.5913 (0.6203)	loss 1.0321 (1.0971)	grad_norm 2.6598 (2.5318)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:50:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:10:21 lr 0.000006	 wd 0.0500	time 0.5829 (0.6198)	loss 1.3208 (1.0986)	grad_norm 1.9903 (2.5467)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:51:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:09:18 lr 0.000006	 wd 0.0500	time 0.5864 (0.6193)	loss 1.1529 (1.1003)	grad_norm 2.5685 (2.5346)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:52:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:08:16 lr 0.000006	 wd 0.0500	time 0.5971 (0.6189)	loss 1.4591 (1.1010)	grad_norm 2.0616 (2.5428)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:53:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:07:14 lr 0.000006	 wd 0.0500	time 0.5949 (0.6186)	loss 0.7699 (1.0990)	grad_norm 2.2107 (2.5315)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 06:54:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:06:12 lr 0.000006	 wd 0.0500	time 0.5850 (0.6182)	loss 0.7528 (1.0995)	grad_norm 1.9069 (inf)	loss_scale 512.0000 (1015.3814)	mem 20177MB
[2024-08-06 06:55:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:05:10 lr 0.000006	 wd 0.0500	time 0.5911 (0.6180)	loss 1.0474 (1.0990)	grad_norm 2.1813 (inf)	loss_scale 512.0000 (990.2249)	mem 20177MB
[2024-08-06 06:56:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:04:08 lr 0.000006	 wd 0.0500	time 0.5911 (0.6178)	loss 1.2462 (1.0994)	grad_norm 1.7595 (inf)	loss_scale 512.0000 (967.4631)	mem 20177MB
[2024-08-06 06:57:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:03:06 lr 0.000006	 wd 0.0500	time 0.5782 (0.6175)	loss 1.2926 (1.1009)	grad_norm 2.3800 (inf)	loss_scale 512.0000 (946.7697)	mem 20177MB
[2024-08-06 06:58:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:02:04 lr 0.000006	 wd 0.0500	time 0.5898 (0.6173)	loss 0.7883 (1.1023)	grad_norm 1.7551 (inf)	loss_scale 512.0000 (927.8748)	mem 20177MB
[2024-08-06 06:59:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:02 lr 0.000006	 wd 0.0500	time 0.5897 (0.6171)	loss 0.6965 (1.1018)	grad_norm 4.5343 (inf)	loss_scale 512.0000 (910.5539)	mem 20177MB
[2024-08-06 07:00:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.5844 (0.6169)	loss 1.0770 (1.1037)	grad_norm 2.3229 (inf)	loss_scale 512.0000 (894.6182)	mem 20177MB
[2024-08-06 07:00:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 20 training takes 0:25:46
[2024-08-06 07:00:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.670 (12.670)	Loss 0.4600 (0.4600)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 20177MB
[2024-08-06 07:01:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.818 Acc@5 98.030
[2024-08-06 07:01:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 07:01:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-06 07:01:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:31:18 lr 0.000006	 wd 0.0500	time 12.2616 (12.2616)	loss 0.8703 (0.8703)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:02:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:29:04 lr 0.000006	 wd 0.0500	time 0.5836 (0.7262)	loss 1.2830 (1.1052)	grad_norm 2.4483 (2.2810)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:03:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:25:38 lr 0.000006	 wd 0.0500	time 0.5913 (0.6685)	loss 1.2494 (1.1080)	grad_norm 2.9370 (2.3915)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:04:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:23:50 lr 0.000006	 wd 0.0500	time 0.5919 (0.6498)	loss 1.0571 (1.1036)	grad_norm 2.9993 (2.4428)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:05:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:22:26 lr 0.000006	 wd 0.0500	time 0.5822 (0.6404)	loss 1.3963 (1.0944)	grad_norm 3.9524 (2.4697)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:06:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:21:10 lr 0.000006	 wd 0.0500	time 0.5854 (0.6347)	loss 0.7712 (1.0902)	grad_norm 6.9778 (2.5295)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:07:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:19:59 lr 0.000006	 wd 0.0500	time 0.5787 (0.6309)	loss 1.2542 (1.0862)	grad_norm 2.8218 (2.5082)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:08:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:18:52 lr 0.000006	 wd 0.0500	time 0.5909 (0.6282)	loss 1.2040 (1.0884)	grad_norm 2.2037 (2.5158)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:09:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:17:45 lr 0.000006	 wd 0.0500	time 0.5845 (0.6263)	loss 1.2392 (1.0874)	grad_norm 2.2012 (2.5137)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:10:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:16:40 lr 0.000005	 wd 0.0500	time 0.5874 (0.6248)	loss 1.2953 (1.0910)	grad_norm 22.5579 (2.6229)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:11:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:15:36 lr 0.000005	 wd 0.0500	time 0.5953 (0.6235)	loss 1.3012 (1.0943)	grad_norm 1.6897 (2.5865)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:12:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:14:32 lr 0.000005	 wd 0.0500	time 0.6303 (0.6225)	loss 0.8009 (1.0980)	grad_norm 2.0854 (2.5915)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:13:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:13:29 lr 0.000005	 wd 0.0500	time 0.5891 (0.6216)	loss 1.2334 (1.0946)	grad_norm 2.4765 (2.5753)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:14:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:12:26 lr 0.000005	 wd 0.0500	time 0.5937 (0.6209)	loss 1.2346 (1.0946)	grad_norm 1.8681 (2.5637)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:15:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:11:23 lr 0.000005	 wd 0.0500	time 0.5853 (0.6203)	loss 0.7704 (1.0926)	grad_norm 2.1947 (2.5546)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:16:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:10:20 lr 0.000005	 wd 0.0500	time 0.5845 (0.6197)	loss 0.7705 (1.0940)	grad_norm 1.8183 (2.5842)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:17:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:09:18 lr 0.000005	 wd 0.0500	time 0.5920 (0.6193)	loss 1.0839 (1.0907)	grad_norm 2.7781 (2.5718)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:18:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:08:16 lr 0.000005	 wd 0.0500	time 0.5860 (0.6189)	loss 1.3109 (1.0907)	grad_norm 1.7291 (2.5660)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:19:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:07:14 lr 0.000005	 wd 0.0500	time 0.5777 (0.6185)	loss 0.7959 (1.0910)	grad_norm 2.4687 (2.5652)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:20:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:06:12 lr 0.000005	 wd 0.0500	time 0.5858 (0.6182)	loss 0.6967 (1.0925)	grad_norm 2.0795 (2.5900)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:21:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:05:10 lr 0.000005	 wd 0.0500	time 0.5886 (0.6179)	loss 1.0774 (1.0920)	grad_norm 4.3464 (2.6019)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:23:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:04:08 lr 0.000005	 wd 0.0500	time 0.5849 (0.6177)	loss 1.1785 (1.0916)	grad_norm 2.6141 (2.6000)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:24:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:03:06 lr 0.000005	 wd 0.0500	time 0.5881 (0.6174)	loss 1.3459 (1.0899)	grad_norm 2.7591 (2.5880)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:25:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:02:04 lr 0.000005	 wd 0.0500	time 0.5878 (0.6173)	loss 1.3432 (1.0920)	grad_norm 2.8032 (2.5968)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:26:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:02 lr 0.000005	 wd 0.0500	time 0.5920 (0.6171)	loss 1.0487 (1.0929)	grad_norm 2.6531 (2.5975)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:27:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.5631 (0.6168)	loss 1.3745 (1.0924)	grad_norm 2.9380 (2.6121)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:27:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 21 training takes 0:25:47
[2024-08-06 07:27:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.096 (12.096)	Loss 0.4722 (0.4722)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 07:27:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.850 Acc@5 98.032
[2024-08-06 07:27:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 07:27:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-06 07:27:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:11:50 lr 0.000005	 wd 0.0500	time 11.7949 (11.7949)	loss 0.9897 (0.9897)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:28:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:28:51 lr 0.000005	 wd 0.0500	time 0.5881 (0.7207)	loss 1.2637 (1.1036)	grad_norm 2.1374 (2.6863)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:29:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:25:33 lr 0.000005	 wd 0.0500	time 0.5908 (0.6662)	loss 0.6471 (1.1142)	grad_norm 3.2154 (2.7510)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:31:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:23:47 lr 0.000005	 wd 0.0500	time 0.6018 (0.6484)	loss 1.4694 (1.1159)	grad_norm 2.1563 (2.6663)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:32:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:22:24 lr 0.000005	 wd 0.0500	time 0.5870 (0.6396)	loss 0.7435 (1.1162)	grad_norm 2.1474 (2.6238)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:33:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:21:09 lr 0.000005	 wd 0.0500	time 0.5770 (0.6342)	loss 1.0405 (1.1123)	grad_norm 3.8324 (2.8128)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:34:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:19:59 lr 0.000005	 wd 0.0500	time 0.5908 (0.6305)	loss 0.8224 (1.1116)	grad_norm 2.1045 (2.7486)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:35:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:18:51 lr 0.000005	 wd 0.0500	time 0.5882 (0.6279)	loss 1.3616 (1.1057)	grad_norm 2.0296 (2.7727)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:36:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:17:45 lr 0.000004	 wd 0.0500	time 0.5820 (0.6260)	loss 0.9913 (1.1055)	grad_norm 1.6647 (2.7168)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 07:37:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:16:40 lr 0.000004	 wd 0.0500	time 0.5879 (0.6245)	loss 1.0906 (1.1057)	grad_norm 1.9006 (2.7570)	loss_scale 1024.0000 (532.4573)	mem 20177MB
[2024-08-06 07:38:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:15:36 lr 0.000004	 wd 0.0500	time 0.5822 (0.6233)	loss 1.3144 (1.1062)	grad_norm 1.9364 (2.7547)	loss_scale 1024.0000 (581.5624)	mem 20177MB
[2024-08-06 07:39:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:14:32 lr 0.000004	 wd 0.0500	time 0.5812 (0.6222)	loss 0.7602 (1.1022)	grad_norm 2.0004 (2.7529)	loss_scale 1024.0000 (621.7475)	mem 20177MB
[2024-08-06 07:40:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:13:29 lr 0.000004	 wd 0.0500	time 0.5837 (0.6214)	loss 1.2409 (1.1028)	grad_norm 2.0077 (2.7233)	loss_scale 1024.0000 (655.2406)	mem 20177MB
[2024-08-06 07:41:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:12:26 lr 0.000004	 wd 0.0500	time 0.5983 (0.6208)	loss 0.7739 (1.0970)	grad_norm 1.9646 (2.6921)	loss_scale 1024.0000 (683.5849)	mem 20177MB
[2024-08-06 07:42:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:11:23 lr 0.000004	 wd 0.0500	time 0.5779 (0.6202)	loss 0.9815 (1.0923)	grad_norm 2.3827 (2.6914)	loss_scale 1024.0000 (707.8829)	mem 20177MB
[2024-08-06 07:43:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:10:20 lr 0.000004	 wd 0.0500	time 0.5929 (0.6197)	loss 1.0854 (1.0920)	grad_norm 2.0027 (2.6811)	loss_scale 1024.0000 (728.9434)	mem 20177MB
[2024-08-06 07:44:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:09:18 lr 0.000004	 wd 0.0500	time 0.5902 (0.6193)	loss 1.3101 (1.0915)	grad_norm 2.2234 (2.6826)	loss_scale 1024.0000 (747.3729)	mem 20177MB
[2024-08-06 07:45:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:08:16 lr 0.000004	 wd 0.0500	time 0.5879 (0.6189)	loss 0.8946 (1.0896)	grad_norm 3.6013 (2.6625)	loss_scale 1024.0000 (763.6355)	mem 20177MB
[2024-08-06 07:46:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:07:14 lr 0.000004	 wd 0.0500	time 0.5887 (0.6186)	loss 1.1379 (1.0883)	grad_norm 2.2554 (2.6624)	loss_scale 1024.0000 (778.0922)	mem 20177MB
[2024-08-06 07:47:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:06:12 lr 0.000004	 wd 0.0500	time 0.5923 (0.6183)	loss 0.9175 (1.0892)	grad_norm 2.0589 (2.7200)	loss_scale 1024.0000 (791.0279)	mem 20177MB
[2024-08-06 07:48:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:05:10 lr 0.000004	 wd 0.0500	time 0.5840 (0.6180)	loss 0.8759 (1.0905)	grad_norm 1.8494 (2.7059)	loss_scale 1024.0000 (802.6707)	mem 20177MB
[2024-08-06 07:49:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:04:08 lr 0.000004	 wd 0.0500	time 0.6029 (0.6178)	loss 1.2144 (1.0916)	grad_norm 1.8850 (2.6922)	loss_scale 1024.0000 (813.2051)	mem 20177MB
[2024-08-06 07:50:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:03:06 lr 0.000004	 wd 0.0500	time 0.5868 (0.6175)	loss 1.0522 (1.0914)	grad_norm 5.5195 (2.7184)	loss_scale 1024.0000 (822.7824)	mem 20177MB
[2024-08-06 07:51:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:02:04 lr 0.000004	 wd 0.0500	time 0.5866 (0.6173)	loss 1.1510 (1.0919)	grad_norm 2.0497 (2.7024)	loss_scale 1024.0000 (831.5272)	mem 20177MB
[2024-08-06 07:52:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:02 lr 0.000004	 wd 0.0500	time 0.5894 (0.6171)	loss 1.3768 (1.0935)	grad_norm 1.7643 (2.7108)	loss_scale 1024.0000 (839.5435)	mem 20177MB
[2024-08-06 07:53:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.5951 (0.6169)	loss 0.8544 (1.0937)	grad_norm 4.5108 (2.7022)	loss_scale 1024.0000 (846.9188)	mem 20177MB
[2024-08-06 07:53:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 22 training takes 0:25:48
[2024-08-06 07:53:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.698 (12.698)	Loss 0.4531 (0.4531)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 07:54:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.846 Acc@5 98.044
[2024-08-06 07:54:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 07:54:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-06 07:54:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:24:58 lr 0.000004	 wd 0.0500	time 10.6707 (10.6707)	loss 0.7586 (0.7586)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 07:55:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:28:48 lr 0.000004	 wd 0.0500	time 0.5777 (0.7196)	loss 1.2831 (1.0793)	grad_norm 2.5422 (2.1794)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 07:56:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:25:32 lr 0.000004	 wd 0.0500	time 0.5924 (0.6656)	loss 0.8854 (1.0882)	grad_norm 1.9241 (2.4124)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 07:57:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:23:46 lr 0.000004	 wd 0.0500	time 0.5845 (0.6480)	loss 1.2848 (1.0781)	grad_norm 2.0535 (2.5823)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 07:58:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:22:23 lr 0.000004	 wd 0.0500	time 0.5858 (0.6391)	loss 0.8618 (1.0855)	grad_norm 2.0203 (2.6146)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 07:59:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:21:08 lr 0.000004	 wd 0.0500	time 0.5906 (0.6338)	loss 0.8656 (1.0860)	grad_norm 2.1451 (2.5994)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:00:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:19:58 lr 0.000004	 wd 0.0500	time 0.5867 (0.6301)	loss 1.2840 (1.0854)	grad_norm 2.3843 (2.5834)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:01:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:18:50 lr 0.000004	 wd 0.0500	time 0.5853 (0.6276)	loss 1.1985 (1.0880)	grad_norm 2.9255 (2.5711)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:02:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:17:44 lr 0.000003	 wd 0.0500	time 0.5838 (0.6257)	loss 1.3079 (1.0906)	grad_norm 11.3025 (2.6345)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:03:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:16:40 lr 0.000003	 wd 0.0500	time 0.5853 (0.6243)	loss 1.0939 (1.0915)	grad_norm 1.6453 (2.6703)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:04:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:15:35 lr 0.000003	 wd 0.0500	time 0.5851 (0.6232)	loss 1.1262 (1.0877)	grad_norm 1.4680 (2.6876)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:05:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:14:32 lr 0.000003	 wd 0.0500	time 0.5894 (0.6222)	loss 0.9701 (1.0898)	grad_norm 1.9165 (2.6672)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:06:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:13:29 lr 0.000003	 wd 0.0500	time 0.5936 (0.6214)	loss 1.2821 (1.0923)	grad_norm 2.0425 (2.7169)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:07:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:12:26 lr 0.000003	 wd 0.0500	time 0.5857 (0.6207)	loss 1.1760 (1.0934)	grad_norm 2.5422 (2.7733)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:08:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:11:23 lr 0.000003	 wd 0.0500	time 0.5850 (0.6201)	loss 1.3142 (1.0933)	grad_norm 1.9507 (2.7421)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:09:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:10:20 lr 0.000003	 wd 0.0500	time 0.5915 (0.6197)	loss 1.2966 (1.0917)	grad_norm 2.8927 (2.7258)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:10:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:09:18 lr 0.000003	 wd 0.0500	time 0.5941 (0.6192)	loss 1.1273 (1.0944)	grad_norm 2.4558 (2.6982)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:11:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:08:16 lr 0.000003	 wd 0.0500	time 0.5648 (0.6188)	loss 0.6881 (1.0919)	grad_norm 1.8403 (2.6991)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:12:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:07:14 lr 0.000003	 wd 0.0500	time 0.5906 (0.6185)	loss 1.3270 (1.0914)	grad_norm 3.0072 (2.6952)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:13:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:06:12 lr 0.000003	 wd 0.0500	time 0.5844 (0.6182)	loss 1.2942 (1.0908)	grad_norm 2.1100 (2.7046)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:14:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:05:10 lr 0.000003	 wd 0.0500	time 0.5902 (0.6179)	loss 0.7948 (1.0899)	grad_norm 2.3649 (2.7016)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:15:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:04:08 lr 0.000003	 wd 0.0500	time 0.5767 (0.6177)	loss 1.2208 (1.0908)	grad_norm 2.5895 (2.6853)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:16:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:03:06 lr 0.000003	 wd 0.0500	time 0.5857 (0.6174)	loss 1.2616 (1.0897)	grad_norm 3.1771 (2.6740)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:17:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:02:04 lr 0.000003	 wd 0.0500	time 0.5865 (0.6172)	loss 1.3930 (1.0893)	grad_norm 2.6180 (2.6708)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:18:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:02 lr 0.000003	 wd 0.0500	time 0.5897 (0.6170)	loss 0.9293 (1.0891)	grad_norm 1.9940 (2.6620)	loss_scale 2048.0000 (1040.2066)	mem 20177MB
[2024-08-06 08:19:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.5938 (0.6168)	loss 0.8217 (1.0880)	grad_norm 2.7706 (2.6930)	loss_scale 2048.0000 (1080.5022)	mem 20177MB
[2024-08-06 08:20:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 23 training takes 0:25:49
[2024-08-06 08:20:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.831 (11.831)	Loss 0.4707 (0.4707)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 08:20:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.834 Acc@5 98.038
[2024-08-06 08:20:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 08:20:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-06 08:20:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:32:07 lr 0.000003	 wd 0.0500	time 12.2812 (12.2812)	loss 1.2009 (1.2009)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:21:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:29:02 lr 0.000003	 wd 0.0500	time 0.5845 (0.7254)	loss 0.9096 (1.1031)	grad_norm 2.6657 (2.4013)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:22:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:25:39 lr 0.000003	 wd 0.0500	time 0.5851 (0.6686)	loss 1.1291 (1.0939)	grad_norm 2.2206 (2.5295)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:23:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:23:50 lr 0.000003	 wd 0.0500	time 0.5816 (0.6497)	loss 1.2937 (1.1051)	grad_norm 2.0486 (2.5100)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:24:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:22:26 lr 0.000003	 wd 0.0500	time 0.5781 (0.6404)	loss 0.6503 (1.1026)	grad_norm 3.7609 (2.5451)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:25:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:21:10 lr 0.000003	 wd 0.0500	time 0.5793 (0.6347)	loss 1.2354 (1.1033)	grad_norm 3.9354 (2.5164)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:26:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:20:00 lr 0.000003	 wd 0.0500	time 0.5871 (0.6310)	loss 0.8178 (1.0997)	grad_norm 2.3801 (2.5183)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:28:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:18:52 lr 0.000003	 wd 0.0500	time 0.5917 (0.6284)	loss 1.2527 (1.0983)	grad_norm 1.9011 (2.5657)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 08:29:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:17:46 lr 0.000003	 wd 0.0500	time 0.5854 (0.6264)	loss 1.2160 (1.0991)	grad_norm 1.8298 (inf)	loss_scale 1024.0000 (1953.3983)	mem 20177MB
[2024-08-06 08:30:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:16:41 lr 0.000003	 wd 0.0500	time 0.5874 (0.6249)	loss 0.8840 (1.1037)	grad_norm 2.0992 (inf)	loss_scale 1024.0000 (1850.2464)	mem 20177MB
[2024-08-06 08:31:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:15:36 lr 0.000003	 wd 0.0500	time 0.5856 (0.6237)	loss 0.8276 (1.1012)	grad_norm 1.9940 (inf)	loss_scale 1024.0000 (1767.7043)	mem 20177MB
[2024-08-06 08:32:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:14:32 lr 0.000003	 wd 0.0500	time 0.5845 (0.6226)	loss 0.8329 (1.0996)	grad_norm 1.9016 (inf)	loss_scale 1024.0000 (1700.1562)	mem 20177MB
[2024-08-06 08:33:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:13:29 lr 0.000002	 wd 0.0500	time 0.5802 (0.6218)	loss 0.9257 (1.0973)	grad_norm 2.3122 (inf)	loss_scale 1024.0000 (1643.8568)	mem 20177MB
[2024-08-06 08:34:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:12:26 lr 0.000002	 wd 0.0500	time 0.5608 (0.6211)	loss 0.8295 (1.0951)	grad_norm 1.8265 (inf)	loss_scale 1024.0000 (1596.2121)	mem 20177MB
[2024-08-06 08:35:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:11:23 lr 0.000002	 wd 0.0500	time 0.6039 (0.6205)	loss 1.4056 (1.0963)	grad_norm 2.2683 (inf)	loss_scale 1024.0000 (1555.3690)	mem 20177MB
[2024-08-06 08:36:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:10:21 lr 0.000002	 wd 0.0500	time 0.5822 (0.6200)	loss 1.1658 (1.0929)	grad_norm 1.9610 (inf)	loss_scale 1024.0000 (1519.9680)	mem 20177MB
[2024-08-06 08:37:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:09:18 lr 0.000002	 wd 0.0500	time 0.5727 (0.6196)	loss 1.2389 (1.0918)	grad_norm 2.1073 (inf)	loss_scale 1024.0000 (1488.9894)	mem 20177MB
[2024-08-06 08:38:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:08:16 lr 0.000002	 wd 0.0500	time 0.5958 (0.6192)	loss 1.5284 (1.0931)	grad_norm 2.3053 (inf)	loss_scale 1024.0000 (1461.6531)	mem 20177MB
[2024-08-06 08:39:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:07:14 lr 0.000002	 wd 0.0500	time 0.5985 (0.6188)	loss 1.1212 (1.0935)	grad_norm 2.1785 (inf)	loss_scale 1024.0000 (1437.3526)	mem 20177MB
[2024-08-06 08:40:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:06:12 lr 0.000002	 wd 0.0500	time 0.5915 (0.6185)	loss 0.8025 (1.0941)	grad_norm 2.5798 (inf)	loss_scale 1024.0000 (1415.6086)	mem 20177MB
[2024-08-06 08:41:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:05:10 lr 0.000002	 wd 0.0500	time 0.5819 (0.6182)	loss 1.2077 (1.0938)	grad_norm 2.2837 (inf)	loss_scale 1024.0000 (1396.0380)	mem 20177MB
[2024-08-06 08:42:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:04:08 lr 0.000002	 wd 0.0500	time 0.5912 (0.6180)	loss 0.9648 (1.0935)	grad_norm 2.0758 (inf)	loss_scale 1024.0000 (1378.3303)	mem 20177MB
[2024-08-06 08:43:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:03:06 lr 0.000002	 wd 0.0500	time 0.5851 (0.6177)	loss 0.8868 (1.0952)	grad_norm 2.4300 (inf)	loss_scale 1024.0000 (1362.2317)	mem 20177MB
[2024-08-06 08:44:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:02:04 lr 0.000002	 wd 0.0500	time 0.5842 (0.6175)	loss 1.2545 (1.0962)	grad_norm 2.7191 (inf)	loss_scale 1024.0000 (1347.5324)	mem 20177MB
[2024-08-06 08:45:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:02 lr 0.000002	 wd 0.0500	time 0.5887 (0.6173)	loss 1.3340 (1.0940)	grad_norm 2.1853 (inf)	loss_scale 1024.0000 (1334.0575)	mem 20177MB
[2024-08-06 08:46:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.5924 (0.6171)	loss 0.7457 (1.0943)	grad_norm 2.4990 (inf)	loss_scale 1024.0000 (1321.6601)	mem 20177MB
[2024-08-06 08:46:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 24 training takes 0:25:49
[2024-08-06 08:46:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.987 (11.987)	Loss 0.4546 (0.4546)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 20177MB
[2024-08-06 08:47:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.884 Acc@5 98.064
[2024-08-06 08:47:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 08:47:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.88%
[2024-08-06 08:47:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-06 08:47:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-06 08:47:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:05:07 lr 0.000002	 wd 0.0500	time 11.6337 (11.6337)	loss 1.2732 (1.2732)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:48:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:28:49 lr 0.000002	 wd 0.0500	time 0.5764 (0.7198)	loss 1.1487 (1.1617)	grad_norm 2.9237 (2.8452)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:49:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:25:33 lr 0.000002	 wd 0.0500	time 0.5618 (0.6662)	loss 1.3962 (1.1294)	grad_norm 1.8310 (3.1839)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:50:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:23:47 lr 0.000002	 wd 0.0500	time 0.5628 (0.6482)	loss 1.1458 (1.1135)	grad_norm 2.0057 (2.9653)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:51:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:22:23 lr 0.000002	 wd 0.0500	time 0.5933 (0.6392)	loss 1.1244 (1.1098)	grad_norm 2.2521 (2.8593)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:52:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:21:09 lr 0.000002	 wd 0.0500	time 0.5766 (0.6339)	loss 1.2547 (1.1072)	grad_norm 2.5547 (2.7614)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:53:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:19:58 lr 0.000002	 wd 0.0500	time 0.5924 (0.6303)	loss 0.8691 (1.0983)	grad_norm 1.9790 (2.6784)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:54:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:18:51 lr 0.000002	 wd 0.0500	time 0.5907 (0.6278)	loss 0.7414 (1.1001)	grad_norm 4.2321 (2.6535)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:55:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:17:45 lr 0.000002	 wd 0.0500	time 0.5870 (0.6259)	loss 1.3133 (1.0982)	grad_norm 5.0132 (2.6544)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:56:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:16:40 lr 0.000002	 wd 0.0500	time 0.5847 (0.6244)	loss 1.2146 (1.0983)	grad_norm 2.8042 (2.6422)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:57:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:15:36 lr 0.000002	 wd 0.0500	time 0.5806 (0.6232)	loss 1.5249 (1.0988)	grad_norm 2.7332 (2.6272)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:58:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:14:32 lr 0.000002	 wd 0.0500	time 0.5878 (0.6223)	loss 1.3893 (1.1007)	grad_norm 4.6096 (2.6503)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 08:59:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:13:29 lr 0.000002	 wd 0.0500	time 0.5884 (0.6215)	loss 0.9366 (1.0974)	grad_norm 1.7853 (2.6292)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:00:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:12:26 lr 0.000002	 wd 0.0500	time 0.5872 (0.6208)	loss 0.8914 (1.0970)	grad_norm 2.1479 (2.6211)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:01:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:11:23 lr 0.000002	 wd 0.0500	time 0.5868 (0.6202)	loss 0.9801 (1.0968)	grad_norm 1.8881 (2.6023)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:02:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:10:20 lr 0.000002	 wd 0.0500	time 0.5891 (0.6197)	loss 0.7727 (1.0963)	grad_norm 2.1803 (2.5749)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:03:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:09:18 lr 0.000002	 wd 0.0500	time 0.5874 (0.6193)	loss 0.8061 (1.0954)	grad_norm 2.6086 (2.5738)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:04:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:08:16 lr 0.000002	 wd 0.0500	time 0.5836 (0.6189)	loss 0.7135 (1.0931)	grad_norm 1.8272 (2.5769)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:05:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:07:14 lr 0.000002	 wd 0.0500	time 0.5892 (0.6185)	loss 0.7798 (1.0947)	grad_norm 2.5757 (2.5689)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:06:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:06:12 lr 0.000002	 wd 0.0500	time 0.5857 (0.6182)	loss 1.4082 (1.0965)	grad_norm 2.0787 (2.5738)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:07:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:05:10 lr 0.000002	 wd 0.0500	time 0.5892 (0.6180)	loss 0.8793 (1.0966)	grad_norm 1.8632 (2.5745)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:08:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:04:08 lr 0.000002	 wd 0.0500	time 0.5730 (0.6177)	loss 1.2224 (1.0975)	grad_norm 3.8109 (2.5891)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:09:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:03:06 lr 0.000001	 wd 0.0500	time 0.5852 (0.6174)	loss 1.0337 (1.0978)	grad_norm 4.9596 (2.5864)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:10:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:02:04 lr 0.000001	 wd 0.0500	time 0.5881 (0.6172)	loss 1.3018 (1.0976)	grad_norm 1.5340 (2.5931)	loss_scale 2048.0000 (1057.8218)	mem 20177MB
[2024-08-06 09:11:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:01:02 lr 0.000001	 wd 0.0500	time 0.5924 (0.6170)	loss 1.4363 (1.0972)	grad_norm 2.0391 (2.5920)	loss_scale 2048.0000 (1099.0621)	mem 20177MB
[2024-08-06 09:12:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5885 (0.6168)	loss 0.9802 (1.0959)	grad_norm 2.8807 (2.5910)	loss_scale 2048.0000 (1137.0044)	mem 20177MB
[2024-08-06 09:12:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 25 training takes 0:25:49
[2024-08-06 09:13:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.455 (11.455)	Loss 0.4756 (0.4756)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 09:13:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.904 Acc@5 98.048
[2024-08-06 09:13:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 09:13:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-06 09:13:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-06 09:13:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-06 09:13:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:30:27 lr 0.000001	 wd 0.0500	time 10.8026 (10.8026)	loss 1.0436 (1.0436)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:14:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:28:30 lr 0.000001	 wd 0.0500	time 0.5825 (0.7119)	loss 1.2663 (1.1055)	grad_norm 2.5402 (2.6626)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:15:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:25:23 lr 0.000001	 wd 0.0500	time 0.5860 (0.6619)	loss 0.9866 (1.1006)	grad_norm 2.1253 (2.8248)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:16:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:23:41 lr 0.000001	 wd 0.0500	time 0.5877 (0.6453)	loss 0.8704 (1.0884)	grad_norm 2.0398 (2.7739)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:17:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:22:19 lr 0.000001	 wd 0.0500	time 0.5890 (0.6372)	loss 1.3965 (1.0884)	grad_norm 2.1078 (2.7836)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:18:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:21:05 lr 0.000001	 wd 0.0500	time 0.5846 (0.6322)	loss 1.2947 (1.0969)	grad_norm 2.0728 (2.7478)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:19:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:19:56 lr 0.000001	 wd 0.0500	time 0.5918 (0.6289)	loss 0.7546 (1.0993)	grad_norm 1.6899 (2.6866)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:20:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:18:49 lr 0.000001	 wd 0.0500	time 0.5760 (0.6266)	loss 1.1750 (1.0953)	grad_norm 1.7238 (2.6373)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:21:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:17:43 lr 0.000001	 wd 0.0500	time 0.5896 (0.6249)	loss 1.2470 (1.0997)	grad_norm 1.7167 (2.6144)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:23:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:16:38 lr 0.000001	 wd 0.0500	time 0.5867 (0.6235)	loss 1.1537 (1.1000)	grad_norm 1.6878 (2.5946)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:24:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:15:34 lr 0.000001	 wd 0.0500	time 0.5809 (0.6224)	loss 0.9624 (1.0991)	grad_norm 2.9971 (2.6594)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:25:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:14:31 lr 0.000001	 wd 0.0500	time 0.5892 (0.6216)	loss 0.8058 (1.0978)	grad_norm 1.7618 (2.6225)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:26:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:13:28 lr 0.000001	 wd 0.0500	time 0.5913 (0.6209)	loss 0.9007 (1.0983)	grad_norm 2.7439 (2.6178)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:27:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:12:25 lr 0.000001	 wd 0.0500	time 0.5918 (0.6203)	loss 1.1924 (1.1002)	grad_norm 2.3481 (2.5988)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:28:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:11:23 lr 0.000001	 wd 0.0500	time 0.5884 (0.6198)	loss 1.3797 (1.1021)	grad_norm 3.5095 (2.5813)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:29:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:10:20 lr 0.000001	 wd 0.0500	time 0.5882 (0.6194)	loss 1.2910 (1.1038)	grad_norm 1.7691 (2.5601)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:30:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:09:18 lr 0.000001	 wd 0.0500	time 0.5634 (0.6190)	loss 1.0992 (1.1025)	grad_norm 2.0925 (2.5577)	loss_scale 2048.0000 (2048.0000)	mem 20177MB
[2024-08-06 09:31:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:08:16 lr 0.000001	 wd 0.0500	time 0.5812 (0.6186)	loss 1.0788 (1.0993)	grad_norm 1.9840 (nan)	loss_scale 1024.0000 (1996.2281)	mem 20177MB
[2024-08-06 09:32:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:07:14 lr 0.000001	 wd 0.0500	time 0.5883 (0.6183)	loss 0.7325 (1.0984)	grad_norm 1.8775 (nan)	loss_scale 1024.0000 (1942.2454)	mem 20177MB
[2024-08-06 09:33:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:06:12 lr 0.000001	 wd 0.0500	time 0.5793 (0.6180)	loss 1.2902 (1.1000)	grad_norm 1.8631 (nan)	loss_scale 1024.0000 (1893.9421)	mem 20177MB
[2024-08-06 09:34:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:05:10 lr 0.000001	 wd 0.0500	time 0.5911 (0.6177)	loss 1.1126 (1.0987)	grad_norm 19.7036 (nan)	loss_scale 1024.0000 (1850.4668)	mem 20177MB
[2024-08-06 09:35:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:04:08 lr 0.000001	 wd 0.0500	time 0.5867 (0.6175)	loss 1.2858 (1.0982)	grad_norm 3.9471 (nan)	loss_scale 1024.0000 (1811.1299)	mem 20177MB
[2024-08-06 09:36:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:03:06 lr 0.000001	 wd 0.0500	time 0.5651 (0.6173)	loss 1.2209 (1.0965)	grad_norm 1.8479 (nan)	loss_scale 1024.0000 (1775.3676)	mem 20177MB
[2024-08-06 09:37:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:02:04 lr 0.000001	 wd 0.0500	time 0.5866 (0.6171)	loss 1.3296 (1.0968)	grad_norm 1.6258 (nan)	loss_scale 1024.0000 (1742.7136)	mem 20177MB
[2024-08-06 09:38:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:01:02 lr 0.000001	 wd 0.0500	time 0.5933 (0.6169)	loss 0.7166 (1.0974)	grad_norm 9.2467 (nan)	loss_scale 1024.0000 (1712.7797)	mem 20177MB
[2024-08-06 09:39:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5903 (0.6167)	loss 0.7818 (1.0973)	grad_norm 2.8028 (nan)	loss_scale 1024.0000 (1685.2395)	mem 20177MB
[2024-08-06 09:39:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 26 training takes 0:25:49
[2024-08-06 09:39:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 12.244 (12.244)	Loss 0.4583 (0.4583)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 09:40:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.910 Acc@5 98.042
[2024-08-06 09:40:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 09:40:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-06 09:40:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-06 09:40:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-06 09:40:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:43:59 lr 0.000001	 wd 0.0500	time 11.1267 (11.1267)	loss 1.3072 (1.3072)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:41:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:28:35 lr 0.000001	 wd 0.0500	time 0.5826 (0.7141)	loss 1.1840 (1.1063)	grad_norm 2.4274 (2.1790)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:42:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:25:26 lr 0.000001	 wd 0.0500	time 0.5885 (0.6631)	loss 1.1692 (1.1114)	grad_norm 2.6988 (2.3421)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:43:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:23:42 lr 0.000001	 wd 0.0500	time 0.5905 (0.6461)	loss 1.2284 (1.1077)	grad_norm 1.9408 (2.5567)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:44:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:22:20 lr 0.000001	 wd 0.0500	time 0.5863 (0.6376)	loss 0.9142 (1.1067)	grad_norm 2.7781 (2.5719)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:45:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:21:06 lr 0.000001	 wd 0.0500	time 0.5928 (0.6325)	loss 0.7201 (1.1075)	grad_norm 2.3317 (2.5600)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:46:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:19:56 lr 0.000001	 wd 0.0500	time 0.5890 (0.6292)	loss 1.3307 (1.1070)	grad_norm 2.1670 (2.5238)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:47:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:18:49 lr 0.000001	 wd 0.0500	time 0.5910 (0.6269)	loss 0.7491 (1.1072)	grad_norm 2.3182 (2.5180)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:48:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:17:43 lr 0.000001	 wd 0.0500	time 0.5791 (0.6251)	loss 1.0685 (1.1071)	grad_norm 3.8414 (2.5086)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:49:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:16:39 lr 0.000001	 wd 0.0500	time 0.5913 (0.6236)	loss 1.2633 (1.1055)	grad_norm 1.5991 (2.5100)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:50:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:15:34 lr 0.000001	 wd 0.0500	time 0.5930 (0.6225)	loss 0.8867 (1.1014)	grad_norm 2.0916 (2.5428)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:51:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:14:31 lr 0.000001	 wd 0.0500	time 0.5894 (0.6216)	loss 1.1171 (1.1028)	grad_norm 2.0327 (2.5753)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:52:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:13:28 lr 0.000001	 wd 0.0500	time 0.5861 (0.6209)	loss 0.8068 (1.1025)	grad_norm 2.5775 (2.6728)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:53:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:12:25 lr 0.000001	 wd 0.0500	time 0.5962 (0.6202)	loss 0.7596 (1.0983)	grad_norm 3.7428 (2.6505)	loss_scale 1024.0000 (1024.0000)	mem 20177MB
[2024-08-06 09:54:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:11:22 lr 0.000001	 wd 0.0500	time 0.5855 (0.6197)	loss 0.8509 (1.0996)	grad_norm 1.9352 (inf)	loss_scale 512.0000 (1015.2291)	mem 20177MB
[2024-08-06 09:55:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:10:20 lr 0.000001	 wd 0.0500	time 0.5961 (0.6192)	loss 1.3925 (1.1010)	grad_norm 1.9113 (inf)	loss_scale 512.0000 (981.7029)	mem 20177MB
[2024-08-06 09:56:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:09:18 lr 0.000001	 wd 0.0500	time 0.5896 (0.6188)	loss 1.4627 (1.1023)	grad_norm 2.0274 (inf)	loss_scale 512.0000 (952.3648)	mem 20177MB
[2024-08-06 09:57:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:08:15 lr 0.000001	 wd 0.0500	time 0.5889 (0.6184)	loss 1.2870 (1.1032)	grad_norm 2.7498 (inf)	loss_scale 512.0000 (926.4762)	mem 20177MB
[2024-08-06 09:58:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:07:13 lr 0.000001	 wd 0.0500	time 0.5810 (0.6180)	loss 1.2625 (1.1025)	grad_norm 2.2100 (inf)	loss_scale 512.0000 (903.4625)	mem 20177MB
[2024-08-06 09:59:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:06:11 lr 0.000001	 wd 0.0500	time 0.5809 (0.6177)	loss 1.2888 (1.1031)	grad_norm 3.0277 (inf)	loss_scale 512.0000 (882.8701)	mem 20177MB
[2024-08-06 10:00:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:05:09 lr 0.000001	 wd 0.0500	time 0.5920 (0.6175)	loss 1.3058 (1.1035)	grad_norm 1.8332 (inf)	loss_scale 512.0000 (864.3358)	mem 20177MB
[2024-08-06 10:01:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:04:08 lr 0.000001	 wd 0.0500	time 0.6052 (0.6172)	loss 1.4425 (1.1031)	grad_norm 2.5628 (inf)	loss_scale 512.0000 (847.5659)	mem 20177MB
[2024-08-06 10:02:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:03:06 lr 0.000001	 wd 0.0500	time 0.5864 (0.6170)	loss 1.1575 (1.1038)	grad_norm 2.9733 (inf)	loss_scale 512.0000 (832.3199)	mem 20177MB
[2024-08-06 10:03:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:02:04 lr 0.000001	 wd 0.0500	time 0.5834 (0.6168)	loss 1.2561 (1.1038)	grad_norm 1.6686 (inf)	loss_scale 512.0000 (818.3990)	mem 20177MB
[2024-08-06 10:04:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:01:02 lr 0.000001	 wd 0.0500	time 0.5891 (0.6166)	loss 1.2963 (1.1051)	grad_norm 3.2194 (inf)	loss_scale 512.0000 (805.6377)	mem 20177MB
[2024-08-06 10:05:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5908 (0.6164)	loss 1.1031 (1.1042)	grad_norm 2.3658 (inf)	loss_scale 512.0000 (793.8968)	mem 20177MB
[2024-08-06 10:05:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 27 training takes 0:25:47
[2024-08-06 10:06:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.899 (11.899)	Loss 0.4478 (0.4478)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20177MB
[2024-08-06 10:06:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.886 Acc@5 98.036
[2024-08-06 10:06:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 10:06:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-06 10:06:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][0/2502]	eta 8:35:56 lr 0.000001	 wd 0.0500	time 12.3728 (12.3728)	loss 0.7386 (0.7386)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:07:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:29:19 lr 0.000000	 wd 0.0500	time 0.5798 (0.7326)	loss 0.7154 (1.1254)	grad_norm 2.1618 (2.9751)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:08:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:25:47 lr 0.000000	 wd 0.0500	time 0.5907 (0.6722)	loss 1.3376 (1.1218)	grad_norm 2.2465 (2.6395)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:09:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:23:56 lr 0.000000	 wd 0.0500	time 0.5848 (0.6525)	loss 1.3662 (1.1131)	grad_norm 2.3694 (2.5339)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:10:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:22:30 lr 0.000000	 wd 0.0500	time 0.5934 (0.6424)	loss 0.7105 (1.1046)	grad_norm 1.8491 (2.4679)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:11:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:21:14 lr 0.000000	 wd 0.0500	time 0.5838 (0.6364)	loss 0.7900 (1.1093)	grad_norm 1.7071 (2.6013)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:12:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:20:03 lr 0.000000	 wd 0.0500	time 0.6029 (0.6325)	loss 0.8536 (1.1022)	grad_norm 2.4276 (2.6507)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:13:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:18:54 lr 0.000000	 wd 0.0500	time 0.5625 (0.6297)	loss 1.1873 (1.0991)	grad_norm 2.0843 (2.6611)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:14:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:17:48 lr 0.000000	 wd 0.0500	time 0.5816 (0.6276)	loss 0.7891 (1.1004)	grad_norm 2.5875 (2.6968)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:15:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:16:42 lr 0.000000	 wd 0.0500	time 0.5868 (0.6259)	loss 0.7193 (1.0971)	grad_norm 2.4224 (2.7443)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:16:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:15:38 lr 0.000000	 wd 0.0500	time 0.5917 (0.6247)	loss 0.9821 (1.0982)	grad_norm 1.8881 (2.7367)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:17:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:14:34 lr 0.000000	 wd 0.0500	time 0.5851 (0.6236)	loss 1.1216 (1.0993)	grad_norm 1.7500 (2.7121)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:19:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:13:30 lr 0.000000	 wd 0.0500	time 0.5841 (0.6227)	loss 1.3152 (1.0999)	grad_norm 2.3405 (2.6879)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:20:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:12:27 lr 0.000000	 wd 0.0500	time 0.5845 (0.6219)	loss 1.3112 (1.1017)	grad_norm 1.6800 (2.6637)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:21:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:11:24 lr 0.000000	 wd 0.0500	time 0.5891 (0.6213)	loss 0.8559 (1.1004)	grad_norm 1.4580 (2.6369)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:22:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:10:21 lr 0.000000	 wd 0.0500	time 0.5968 (0.6207)	loss 1.1725 (1.1002)	grad_norm 1.7169 (2.6152)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:23:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:09:19 lr 0.000000	 wd 0.0500	time 0.5886 (0.6202)	loss 1.3646 (1.1025)	grad_norm 1.8956 (2.6049)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:24:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:08:16 lr 0.000000	 wd 0.0500	time 0.5882 (0.6197)	loss 1.3135 (1.1023)	grad_norm 4.0305 (2.6126)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:25:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:07:14 lr 0.000000	 wd 0.0500	time 0.5837 (0.6193)	loss 0.6891 (1.1019)	grad_norm 1.9929 (2.6113)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:26:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:06:12 lr 0.000000	 wd 0.0500	time 0.5946 (0.6189)	loss 0.9582 (1.0994)	grad_norm 2.4423 (2.6131)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:27:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:05:10 lr 0.000000	 wd 0.0500	time 0.5923 (0.6186)	loss 1.1028 (1.0994)	grad_norm 2.0506 (2.6273)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:28:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:04:08 lr 0.000000	 wd 0.0500	time 0.5900 (0.6183)	loss 0.9892 (1.1000)	grad_norm 3.0691 (2.6296)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:29:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:03:06 lr 0.000000	 wd 0.0500	time 0.5933 (0.6180)	loss 1.2548 (1.0994)	grad_norm 3.2528 (2.6736)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:30:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:02:04 lr 0.000000	 wd 0.0500	time 0.5917 (0.6178)	loss 1.2374 (1.0992)	grad_norm 1.7468 (2.6608)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:31:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:01:02 lr 0.000000	 wd 0.0500	time 0.5892 (0.6175)	loss 1.4601 (1.0983)	grad_norm 1.8182 (2.6550)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:32:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.5909 (0.6174)	loss 1.0150 (1.0975)	grad_norm 1.6562 (2.6464)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:32:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 28 training takes 0:25:49
[2024-08-06 10:32:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.943 (11.943)	Loss 0.4551 (0.4551)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 10:33:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.904 Acc@5 98.058
[2024-08-06 10:33:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 10:33:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-06 10:33:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:16:50 lr 0.000000	 wd 0.0500	time 11.9146 (11.9146)	loss 1.2670 (1.2670)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:34:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:28:55 lr 0.000000	 wd 0.0500	time 0.5821 (0.7226)	loss 1.3074 (1.1137)	grad_norm 2.0654 (2.3595)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:35:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:25:35 lr 0.000000	 wd 0.0500	time 0.5805 (0.6672)	loss 0.7795 (1.1145)	grad_norm 1.7889 (2.3683)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:36:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:23:48 lr 0.000000	 wd 0.0500	time 0.5778 (0.6487)	loss 0.6685 (1.1029)	grad_norm 15.4305 (2.4275)	loss_scale 512.0000 (512.0000)	mem 20177MB
[2024-08-06 10:37:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:22:24 lr 0.000000	 wd 0.0500	time 0.5626 (0.6395)	loss 1.0045 (1.0989)	grad_norm 2.1565 (2.5533)	loss_scale 1024.0000 (547.7506)	mem 20177MB
[2024-08-06 10:38:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:21:09 lr 0.000000	 wd 0.0500	time 0.5919 (0.6340)	loss 1.1363 (1.0943)	grad_norm 1.9840 (2.5083)	loss_scale 1024.0000 (642.8104)	mem 20177MB
[2024-08-06 10:39:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:19:58 lr 0.000000	 wd 0.0500	time 0.5877 (0.6304)	loss 0.6770 (1.0965)	grad_norm 2.0202 (2.5083)	loss_scale 1024.0000 (706.2363)	mem 20177MB
[2024-08-06 10:40:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:18:51 lr 0.000000	 wd 0.0500	time 0.5864 (0.6278)	loss 1.1624 (1.0962)	grad_norm 3.0609 (2.4851)	loss_scale 1024.0000 (751.5663)	mem 20177MB
[2024-08-06 10:41:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:17:45 lr 0.000000	 wd 0.0500	time 0.5883 (0.6259)	loss 0.8102 (1.0975)	grad_norm 2.0288 (2.4718)	loss_scale 1024.0000 (785.5780)	mem 20177MB
[2024-08-06 10:42:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:16:40 lr 0.000000	 wd 0.0500	time 0.5929 (0.6244)	loss 1.3810 (1.0982)	grad_norm 34.1528 (2.5678)	loss_scale 1024.0000 (812.0400)	mem 20177MB
[2024-08-06 10:43:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:15:36 lr 0.000000	 wd 0.0500	time 0.5912 (0.6232)	loss 1.2274 (1.0988)	grad_norm 4.0731 (2.5576)	loss_scale 1024.0000 (833.2148)	mem 20177MB
[2024-08-06 10:44:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:14:32 lr 0.000000	 wd 0.0500	time 0.5699 (0.6222)	loss 1.2279 (1.0966)	grad_norm 1.7523 (2.5586)	loss_scale 1024.0000 (850.5431)	mem 20177MB
[2024-08-06 10:45:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:13:29 lr 0.000000	 wd 0.0500	time 0.5924 (0.6214)	loss 0.9415 (1.0956)	grad_norm 1.8029 (2.5449)	loss_scale 1024.0000 (864.9858)	mem 20177MB
[2024-08-06 10:46:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:12:26 lr 0.000000	 wd 0.0500	time 0.5941 (0.6207)	loss 1.0835 (1.0992)	grad_norm 3.7176 (2.5671)	loss_scale 1024.0000 (877.2083)	mem 20177MB
[2024-08-06 10:47:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:11:23 lr 0.000000	 wd 0.0500	time 0.5905 (0.6201)	loss 1.0565 (1.0944)	grad_norm 2.3196 (2.5988)	loss_scale 1024.0000 (887.6859)	mem 20177MB
[2024-08-06 10:48:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:10:20 lr 0.000000	 wd 0.0500	time 0.5869 (0.6197)	loss 0.9647 (1.0935)	grad_norm 1.8114 (2.6375)	loss_scale 1024.0000 (896.7675)	mem 20177MB
[2024-08-06 10:49:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:09:18 lr 0.000000	 wd 0.0500	time 0.5820 (0.6192)	loss 0.7567 (1.0962)	grad_norm 1.9137 (2.6288)	loss_scale 1024.0000 (904.7146)	mem 20177MB
[2024-08-06 10:50:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:08:16 lr 0.000000	 wd 0.0500	time 0.5923 (0.6188)	loss 1.1526 (1.0983)	grad_norm 2.3263 (2.6088)	loss_scale 1024.0000 (911.7272)	mem 20177MB
[2024-08-06 10:51:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:07:14 lr 0.000000	 wd 0.0500	time 0.5891 (0.6184)	loss 1.2326 (1.0975)	grad_norm 2.6393 (2.6114)	loss_scale 1024.0000 (917.9611)	mem 20177MB
[2024-08-06 10:52:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:06:12 lr 0.000000	 wd 0.0500	time 0.5901 (0.6181)	loss 0.8500 (1.0976)	grad_norm 1.8391 (2.6093)	loss_scale 1024.0000 (923.5392)	mem 20177MB
[2024-08-06 10:53:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:05:10 lr 0.000000	 wd 0.0500	time 0.5801 (0.6178)	loss 0.8593 (1.0995)	grad_norm 1.9334 (2.6241)	loss_scale 1024.0000 (928.5597)	mem 20177MB
[2024-08-06 10:54:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:04:08 lr 0.000000	 wd 0.0500	time 0.5847 (0.6175)	loss 0.8022 (1.1007)	grad_norm 2.1154 (2.6381)	loss_scale 1024.0000 (933.1023)	mem 20177MB
[2024-08-06 10:55:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:03:06 lr 0.000000	 wd 0.0500	time 0.5902 (0.6173)	loss 0.8243 (1.1013)	grad_norm 2.2550 (2.7108)	loss_scale 1024.0000 (937.2322)	mem 20177MB
[2024-08-06 10:56:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:02:04 lr 0.000000	 wd 0.0500	time 0.5885 (0.6171)	loss 0.6989 (1.0984)	grad_norm 1.9695 (2.7294)	loss_scale 1024.0000 (941.0030)	mem 20177MB
[2024-08-06 10:57:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:01:02 lr 0.000000	 wd 0.0500	time 0.5889 (0.6169)	loss 1.1664 (1.0986)	grad_norm 3.9280 (2.7424)	loss_scale 1024.0000 (944.4598)	mem 20177MB
[2024-08-06 10:58:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.5896 (0.6167)	loss 1.1855 (1.0981)	grad_norm 2.1494 (2.7271)	loss_scale 1024.0000 (947.6401)	mem 20177MB
[2024-08-06 10:58:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 249): INFO EPOCH 29 training takes 0:25:48
[2024-08-06 10:58:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_29.pth saving......
[2024-08-06 10:58:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_29.pth saved !!!
[2024-08-06 10:59:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 289): INFO Test: [0/98]	Time 11.213 (11.213)	Loss 0.4456 (0.4456)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 20177MB
[2024-08-06 10:59:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 296): INFO  * Acc@1 86.902 Acc@5 98.070
[2024-08-06 10:59:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 10:59:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-06 10:59:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1] (main.py 189): INFO Training time 13:10:55
