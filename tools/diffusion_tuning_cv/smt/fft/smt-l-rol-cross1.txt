[2024-07-30 09:29:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/config.json
[2024-07-30 09:29:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_sequence_cross1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-30 09:29:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_sequence_cross1.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_sequence_cross1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-30 09:29:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft
[2024-07-30 09:29:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-30 09:29:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 113): INFO number of params: 80620264
[2024-07-30 09:29:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1, ignoring auto resume
[2024-07-30 09:29:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-30 09:29:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-30 09:29:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth'
[2024-07-30 09:29:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 13.177 (13.177)	Loss 0.4443 (0.4443)	Acc@1 92.383 (92.383)	Acc@5 99.023 (99.023)	Mem 2730MB
[2024-07-30 09:29:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.756 Acc@5 98.030
[2024-07-30 09:29:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 09:29:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 168): INFO Start training
[2024-07-30 09:30:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:14:26 lr 0.000000	 wd 0.0500	time 13.2961 (13.2961)	loss 1.4674 (1.4674)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 23350MB
[2024-07-30 09:31:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:39:37 lr 0.000000	 wd 0.0500	time 0.8216 (0.9900)	loss 1.2633 (1.1556)	grad_norm 2.7769 (nan)	loss_scale 16384.0000 (17519.5248)	mem 23350MB
[2024-07-30 09:32:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:35:33 lr 0.000000	 wd 0.0500	time 0.8200 (0.9268)	loss 1.0040 (1.1491)	grad_norm 2.6283 (nan)	loss_scale 8192.0000 (14020.1393)	mem 23350MB
[2024-07-30 09:34:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:33:15 lr 0.000000	 wd 0.0500	time 0.8226 (0.9061)	loss 0.8688 (1.1115)	grad_norm 3.7615 (nan)	loss_scale 8192.0000 (12083.8804)	mem 23350MB
[2024-07-30 09:35:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:31:23 lr 0.000001	 wd 0.0500	time 0.8783 (0.8959)	loss 1.0699 (1.1187)	grad_norm 2.4891 (nan)	loss_scale 4096.0000 (10847.7606)	mem 23350MB
[2024-07-30 09:37:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:29:41 lr 0.000001	 wd 0.0500	time 0.7619 (0.8900)	loss 1.1435 (1.1201)	grad_norm 1.8981 (nan)	loss_scale 4096.0000 (9500.1038)	mem 23350MB
[2024-07-30 09:38:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:28:05 lr 0.000001	 wd 0.0500	time 0.8309 (0.8862)	loss 1.2285 (1.1189)	grad_norm 2.4434 (nan)	loss_scale 4096.0000 (8600.9185)	mem 23350MB
[2024-07-30 09:40:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:26:31 lr 0.000001	 wd 0.0500	time 0.8208 (0.8832)	loss 1.2303 (1.1212)	grad_norm 2.6792 (nan)	loss_scale 4096.0000 (7958.2767)	mem 23350MB
[2024-07-30 09:41:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:24:59 lr 0.000001	 wd 0.0500	time 0.8071 (0.8812)	loss 1.0363 (1.1202)	grad_norm 2.2616 (nan)	loss_scale 4096.0000 (7476.0949)	mem 23350MB
[2024-07-30 09:43:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:23:28 lr 0.000001	 wd 0.0500	time 0.8193 (0.8794)	loss 1.2684 (1.1206)	grad_norm 2.8248 (nan)	loss_scale 4096.0000 (7100.9456)	mem 23350MB
[2024-07-30 09:44:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:21:58 lr 0.000002	 wd 0.0500	time 0.8565 (0.8779)	loss 1.3879 (1.1180)	grad_norm 3.3673 (nan)	loss_scale 4096.0000 (6800.7512)	mem 23350MB
[2024-07-30 09:45:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:20:29 lr 0.000002	 wd 0.0500	time 0.8074 (0.8770)	loss 1.2970 (1.1195)	grad_norm 2.1441 (nan)	loss_scale 4096.0000 (6555.0881)	mem 23350MB
[2024-07-30 09:47:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:19:00 lr 0.000002	 wd 0.0500	time 0.8069 (0.8761)	loss 0.9696 (1.1191)	grad_norm 2.7308 (nan)	loss_scale 4096.0000 (6350.3347)	mem 23350MB
[2024-07-30 09:48:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:17:32 lr 0.000002	 wd 0.0500	time 0.8137 (0.8754)	loss 1.1689 (1.1208)	grad_norm 2.2217 (nan)	loss_scale 4096.0000 (6177.0576)	mem 23350MB
[2024-07-30 09:50:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:16:03 lr 0.000002	 wd 0.0500	time 0.8184 (0.8745)	loss 1.4094 (1.1219)	grad_norm 2.5076 (nan)	loss_scale 4096.0000 (6028.5168)	mem 23350MB
[2024-07-30 09:51:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:14:35 lr 0.000002	 wd 0.0500	time 0.7642 (0.8739)	loss 0.9609 (1.1252)	grad_norm 2.1752 (nan)	loss_scale 4096.0000 (5899.7682)	mem 23350MB
[2024-07-30 09:53:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:13:07 lr 0.000003	 wd 0.0500	time 0.8005 (0.8734)	loss 1.1980 (1.1226)	grad_norm 2.0561 (nan)	loss_scale 4096.0000 (5787.1031)	mem 23350MB
[2024-07-30 09:54:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:11:39 lr 0.000003	 wd 0.0500	time 0.8253 (0.8728)	loss 0.9456 (1.1222)	grad_norm 3.6084 (nan)	loss_scale 2048.0000 (5596.1811)	mem 23350MB
[2024-07-30 09:55:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:10:12 lr 0.000003	 wd 0.0500	time 0.8035 (0.8723)	loss 1.0358 (1.1223)	grad_norm 2.6090 (nan)	loss_scale 2048.0000 (5399.1694)	mem 23350MB
[2024-07-30 09:57:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:08:44 lr 0.000003	 wd 0.0500	time 0.8057 (0.8719)	loss 1.4530 (1.1221)	grad_norm 2.1277 (nan)	loss_scale 2048.0000 (5222.8848)	mem 23350MB
[2024-07-30 09:58:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:07:17 lr 0.000003	 wd 0.0500	time 0.8357 (0.8715)	loss 0.7931 (1.1211)	grad_norm 2.0814 (nan)	loss_scale 2048.0000 (5064.2199)	mem 23350MB
[2024-07-30 10:00:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:05:50 lr 0.000003	 wd 0.0500	time 0.8234 (0.8711)	loss 0.9591 (1.1200)	grad_norm 2.2997 (nan)	loss_scale 2048.0000 (4920.6587)	mem 23350MB
[2024-07-30 10:01:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:04:23 lr 0.000004	 wd 0.0500	time 0.8094 (0.8709)	loss 1.4424 (1.1191)	grad_norm 2.6409 (nan)	loss_scale 2048.0000 (4790.1427)	mem 23350MB
[2024-07-30 10:03:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:02:55 lr 0.000004	 wd 0.0500	time 0.8191 (0.8705)	loss 1.3476 (1.1191)	grad_norm 2.4554 (nan)	loss_scale 2048.0000 (4670.9709)	mem 23350MB
[2024-07-30 10:04:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:28 lr 0.000004	 wd 0.0500	time 0.8009 (0.8702)	loss 1.2717 (1.1187)	grad_norm 2.1852 (nan)	loss_scale 2048.0000 (4561.7259)	mem 23350MB
[2024-07-30 10:06:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.8109 (0.8701)	loss 0.9685 (1.1172)	grad_norm 2.3601 (nan)	loss_scale 2048.0000 (4461.2171)	mem 23350MB
[2024-07-30 10:06:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 0 training takes 0:36:19
[2024-07-30 10:06:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_0.pth saving......
[2024-07-30 10:06:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_0.pth saved !!!
[2024-07-30 10:06:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.203 (11.203)	Loss 0.4915 (0.4915)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 10:06:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.726 Acc@5 98.016
[2024-07-30 10:06:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 10:06:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.73%
[2024-07-30 10:06:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 10:06:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 10:06:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:55:10 lr 0.000004	 wd 0.0500	time 11.3950 (11.3950)	loss 1.2824 (1.2824)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:08:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:38:44 lr 0.000004	 wd 0.0500	time 0.8224 (0.9676)	loss 1.2219 (1.1053)	grad_norm 2.3909 (2.7855)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:09:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:35:07 lr 0.000004	 wd 0.0500	time 0.8152 (0.9155)	loss 0.7635 (1.1069)	grad_norm 2.6406 (2.9464)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:11:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:32:58 lr 0.000004	 wd 0.0500	time 0.8054 (0.8986)	loss 0.8736 (1.1100)	grad_norm 3.4822 (2.8602)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:12:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:31:10 lr 0.000005	 wd 0.0500	time 0.8186 (0.8899)	loss 0.8744 (1.1245)	grad_norm 1.9879 (2.7912)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:14:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:29:32 lr 0.000005	 wd 0.0500	time 0.8225 (0.8855)	loss 1.4888 (1.1275)	grad_norm 2.4617 (2.7768)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:15:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:27:58 lr 0.000005	 wd 0.0500	time 0.8176 (0.8824)	loss 1.3780 (1.1268)	grad_norm 5.4053 (2.7970)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:16:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:26:25 lr 0.000005	 wd 0.0500	time 0.8236 (0.8797)	loss 0.8216 (1.1234)	grad_norm 3.4781 (2.7967)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:18:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:24:54 lr 0.000005	 wd 0.0500	time 0.7635 (0.8780)	loss 1.5029 (1.1229)	grad_norm 3.0867 (2.8794)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:19:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:23:24 lr 0.000005	 wd 0.0500	time 0.8010 (0.8766)	loss 1.1242 (1.1211)	grad_norm 2.4284 (2.9150)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:21:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:21:55 lr 0.000006	 wd 0.0500	time 0.8349 (0.8756)	loss 1.4076 (1.1179)	grad_norm 2.4900 (2.9541)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:22:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:20:26 lr 0.000006	 wd 0.0500	time 0.8160 (0.8745)	loss 1.4623 (1.1171)	grad_norm 2.0518 (2.9527)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:24:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:18:57 lr 0.000006	 wd 0.0500	time 0.8744 (0.8737)	loss 0.9045 (1.1154)	grad_norm 2.1319 (2.9467)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:25:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:17:29 lr 0.000006	 wd 0.0500	time 0.8151 (0.8732)	loss 1.1913 (1.1167)	grad_norm 2.3519 (2.9493)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:27:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:16:01 lr 0.000006	 wd 0.0500	time 0.8050 (0.8725)	loss 0.7784 (1.1168)	grad_norm 2.6266 (2.9506)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:28:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:14:34 lr 0.000006	 wd 0.0500	time 0.8482 (0.8723)	loss 1.3839 (1.1168)	grad_norm 2.3338 (2.9333)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:29:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:13:06 lr 0.000007	 wd 0.0500	time 0.8233 (0.8719)	loss 1.2932 (1.1179)	grad_norm 2.6232 (2.9325)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:31:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:11:38 lr 0.000007	 wd 0.0500	time 0.8380 (0.8715)	loss 0.8087 (1.1186)	grad_norm 2.4851 (2.9375)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:32:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:10:11 lr 0.000007	 wd 0.0500	time 0.8107 (0.8711)	loss 1.5863 (1.1178)	grad_norm 2.3257 (2.9163)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:34:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:08:44 lr 0.000007	 wd 0.0500	time 0.8027 (0.8706)	loss 1.4342 (1.1195)	grad_norm 2.3355 (2.9263)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:35:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:07:16 lr 0.000007	 wd 0.0500	time 0.8812 (0.8704)	loss 1.2203 (1.1200)	grad_norm 2.5792 (2.9363)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:37:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:05:49 lr 0.000007	 wd 0.0500	time 0.8052 (0.8701)	loss 1.3443 (1.1220)	grad_norm 2.4712 (2.9179)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:38:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:04:22 lr 0.000008	 wd 0.0500	time 0.8032 (0.8699)	loss 1.0813 (1.1225)	grad_norm 2.2873 (2.9345)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:40:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:02:55 lr 0.000008	 wd 0.0500	time 0.8013 (0.8696)	loss 0.7605 (1.1239)	grad_norm 2.8395 (2.9268)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:41:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:28 lr 0.000008	 wd 0.0500	time 0.8148 (0.8694)	loss 1.1800 (1.1238)	grad_norm 1.8796 (2.9274)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:42:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.8151 (0.8691)	loss 0.8917 (1.1237)	grad_norm 2.2671 (2.9216)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:42:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 1 training takes 0:36:16
[2024-07-30 10:43:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.324 (12.324)	Loss 0.4807 (0.4807)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 10:43:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.698 Acc@5 98.026
[2024-07-30 10:43:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 10:43:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.73%
[2024-07-30 10:43:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][0/2502]	eta 8:22:59 lr 0.000008	 wd 0.0500	time 12.0623 (12.0623)	loss 1.1072 (1.1072)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:45:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:39:06 lr 0.000008	 wd 0.0500	time 0.8658 (0.9770)	loss 1.4004 (1.1235)	grad_norm 2.3210 (3.9227)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:46:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:35:21 lr 0.000008	 wd 0.0500	time 0.8085 (0.9214)	loss 1.2324 (1.1144)	grad_norm 7.9385 (3.3246)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:48:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:33:07 lr 0.000008	 wd 0.0500	time 0.8231 (0.9025)	loss 1.1907 (1.1039)	grad_norm 2.3663 (3.5713)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:49:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:31:17 lr 0.000009	 wd 0.0500	time 0.8151 (0.8931)	loss 1.3074 (1.1088)	grad_norm 2.5727 (3.3760)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:50:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:29:35 lr 0.000009	 wd 0.0500	time 0.8025 (0.8871)	loss 1.2948 (1.1073)	grad_norm 6.4591 (3.2982)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:52:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:27:59 lr 0.000009	 wd 0.0500	time 0.8277 (0.8832)	loss 1.2131 (1.1063)	grad_norm 2.3440 (3.2663)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 10:53:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:26:26 lr 0.000009	 wd 0.0500	time 0.8150 (0.8806)	loss 1.2997 (1.1105)	grad_norm 2.3223 (3.2088)	loss_scale 4096.0000 (2281.7233)	mem 23350MB
[2024-07-30 10:55:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:24:56 lr 0.000009	 wd 0.0500	time 0.9042 (0.8790)	loss 1.1777 (1.1134)	grad_norm 2.2336 (3.1424)	loss_scale 4096.0000 (2508.2247)	mem 23350MB
[2024-07-30 10:56:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:23:25 lr 0.000009	 wd 0.0500	time 0.8206 (0.8775)	loss 0.8059 (1.1171)	grad_norm 2.1279 (3.0873)	loss_scale 4096.0000 (2684.4484)	mem 23350MB
[2024-07-30 10:58:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:21:56 lr 0.000010	 wd 0.0500	time 0.8236 (0.8762)	loss 0.8529 (1.1150)	grad_norm 2.4734 (3.0436)	loss_scale 4096.0000 (2825.4625)	mem 23350MB
[2024-07-30 10:59:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:20:27 lr 0.000010	 wd 0.0500	time 0.8032 (0.8752)	loss 1.3573 (1.1140)	grad_norm 2.5476 (3.0233)	loss_scale 4096.0000 (2940.8610)	mem 23350MB
[2024-07-30 11:01:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:18:58 lr 0.000010	 wd 0.0500	time 0.8216 (0.8744)	loss 0.9751 (1.1146)	grad_norm 2.4243 (2.9928)	loss_scale 4096.0000 (3037.0425)	mem 23350MB
[2024-07-30 11:02:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:17:30 lr 0.000010	 wd 0.0500	time 0.8069 (0.8737)	loss 1.3630 (1.1150)	grad_norm 1.9626 (2.9668)	loss_scale 4096.0000 (3118.4381)	mem 23350MB
[2024-07-30 11:03:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:16:02 lr 0.000010	 wd 0.0500	time 0.7851 (0.8732)	loss 1.1765 (1.1165)	grad_norm 2.2569 (2.9532)	loss_scale 4096.0000 (3188.2141)	mem 23350MB
[2024-07-30 11:05:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:14:34 lr 0.000010	 wd 0.0500	time 0.8020 (0.8728)	loss 1.3197 (1.1170)	grad_norm 2.9224 (2.9391)	loss_scale 4096.0000 (3248.6929)	mem 23350MB
[2024-07-30 11:06:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:13:06 lr 0.000011	 wd 0.0500	time 0.8657 (0.8723)	loss 0.9161 (1.1172)	grad_norm 2.2186 (2.9466)	loss_scale 4096.0000 (3301.6165)	mem 23350MB
[2024-07-30 11:08:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:11:39 lr 0.000011	 wd 0.0500	time 0.8247 (0.8718)	loss 1.1808 (1.1173)	grad_norm 2.3329 (2.9427)	loss_scale 4096.0000 (3348.3175)	mem 23350MB
[2024-07-30 11:09:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:10:11 lr 0.000011	 wd 0.0500	time 0.8261 (0.8714)	loss 1.1181 (1.1182)	grad_norm 3.5310 (2.9480)	loss_scale 4096.0000 (3389.8323)	mem 23350MB
[2024-07-30 11:11:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:08:44 lr 0.000011	 wd 0.0500	time 0.8499 (0.8711)	loss 1.2990 (1.1178)	grad_norm 3.4301 (2.9388)	loss_scale 4096.0000 (3426.9795)	mem 23350MB
[2024-07-30 11:12:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:07:17 lr 0.000011	 wd 0.0500	time 0.8179 (0.8708)	loss 0.9494 (1.1180)	grad_norm 2.0206 (2.9276)	loss_scale 4096.0000 (3460.4138)	mem 23350MB
[2024-07-30 11:14:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:05:49 lr 0.000011	 wd 0.0500	time 0.8145 (0.8706)	loss 0.9656 (1.1171)	grad_norm 3.2448 (2.9369)	loss_scale 4096.0000 (3490.6654)	mem 23350MB
[2024-07-30 11:15:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:04:22 lr 0.000012	 wd 0.0500	time 0.8328 (0.8703)	loss 1.2231 (1.1176)	grad_norm 2.4437 (nan)	loss_scale 2048.0000 (3508.8632)	mem 23350MB
[2024-07-30 11:16:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:02:55 lr 0.000012	 wd 0.0500	time 0.8807 (0.8701)	loss 1.0699 (1.1175)	grad_norm 6.2557 (nan)	loss_scale 2048.0000 (3445.3751)	mem 23350MB
[2024-07-30 11:18:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:28 lr 0.000012	 wd 0.0500	time 0.8151 (0.8699)	loss 0.7736 (1.1181)	grad_norm 2.3075 (nan)	loss_scale 2048.0000 (3387.1753)	mem 23350MB
[2024-07-30 11:19:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.8262 (0.8696)	loss 1.1691 (1.1181)	grad_norm 2.4530 (nan)	loss_scale 2048.0000 (3333.6297)	mem 23350MB
[2024-07-30 11:19:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 2 training takes 0:36:18
[2024-07-30 11:20:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.423 (12.423)	Loss 0.4590 (0.4590)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 11:20:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.708 Acc@5 98.022
[2024-07-30 11:20:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 11:20:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.73%
[2024-07-30 11:20:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:33:55 lr 0.000012	 wd 0.0500	time 12.3243 (12.3243)	loss 0.6613 (0.6613)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 11:22:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:39:08 lr 0.000012	 wd 0.0500	time 0.8076 (0.9777)	loss 1.2245 (1.1552)	grad_norm 2.5170 (2.9275)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 11:23:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:35:23 lr 0.000012	 wd 0.0500	time 0.8697 (0.9224)	loss 1.4357 (1.1301)	grad_norm 2.1796 (2.8744)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 11:24:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:33:08 lr 0.000012	 wd 0.0500	time 0.8383 (0.9031)	loss 1.4023 (1.1308)	grad_norm 2.4057 (nan)	loss_scale 1024.0000 (1789.4485)	mem 23350MB
[2024-07-30 11:26:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:31:19 lr 0.000013	 wd 0.0500	time 0.8159 (0.8941)	loss 1.3526 (1.1219)	grad_norm 2.5493 (nan)	loss_scale 1024.0000 (1598.5636)	mem 23350MB
[2024-07-30 11:27:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:29:38 lr 0.000013	 wd 0.0500	time 0.8166 (0.8881)	loss 0.7857 (1.1219)	grad_norm 2.4072 (nan)	loss_scale 1024.0000 (1483.8802)	mem 23350MB
[2024-07-30 11:29:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:28:01 lr 0.000013	 wd 0.0500	time 0.8093 (0.8842)	loss 1.0437 (1.1141)	grad_norm 2.8155 (nan)	loss_scale 1024.0000 (1407.3611)	mem 23350MB
[2024-07-30 11:30:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:26:28 lr 0.000013	 wd 0.0500	time 0.8167 (0.8818)	loss 1.3126 (1.1125)	grad_norm 3.3270 (nan)	loss_scale 1024.0000 (1352.6733)	mem 23350MB
[2024-07-30 11:32:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:24:57 lr 0.000013	 wd 0.0500	time 0.8160 (0.8797)	loss 0.9142 (1.1074)	grad_norm 2.3074 (nan)	loss_scale 1024.0000 (1311.6404)	mem 23350MB
[2024-07-30 11:33:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:23:26 lr 0.000013	 wd 0.0500	time 0.8192 (0.8782)	loss 1.5399 (1.1101)	grad_norm 2.4781 (nan)	loss_scale 1024.0000 (1279.7159)	mem 23350MB
[2024-07-30 11:35:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:21:57 lr 0.000014	 wd 0.0500	time 0.8072 (0.8769)	loss 1.2446 (1.1097)	grad_norm 2.0369 (nan)	loss_scale 1024.0000 (1254.1698)	mem 23350MB
[2024-07-30 11:36:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:20:27 lr 0.000014	 wd 0.0500	time 0.8056 (0.8757)	loss 0.7718 (1.1107)	grad_norm 9.9010 (nan)	loss_scale 1024.0000 (1233.2643)	mem 23350MB
[2024-07-30 11:37:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:18:59 lr 0.000014	 wd 0.0500	time 0.8153 (0.8750)	loss 1.1339 (1.1084)	grad_norm 3.3581 (nan)	loss_scale 1024.0000 (1215.8401)	mem 23350MB
[2024-07-30 11:39:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:17:30 lr 0.000014	 wd 0.0500	time 0.8017 (0.8743)	loss 1.2114 (1.1082)	grad_norm 2.1777 (nan)	loss_scale 1024.0000 (1201.0945)	mem 23350MB
[2024-07-30 11:40:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:16:02 lr 0.000014	 wd 0.0500	time 0.8225 (0.8736)	loss 1.1556 (1.1088)	grad_norm 2.1139 (nan)	loss_scale 1024.0000 (1188.4540)	mem 23350MB
[2024-07-30 11:42:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:14:34 lr 0.000014	 wd 0.0500	time 0.8162 (0.8730)	loss 1.3493 (1.1073)	grad_norm 2.1612 (nan)	loss_scale 1024.0000 (1177.4977)	mem 23350MB
[2024-07-30 11:43:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:13:06 lr 0.000015	 wd 0.0500	time 0.8141 (0.8724)	loss 0.6874 (1.1047)	grad_norm 3.2635 (nan)	loss_scale 1024.0000 (1167.9101)	mem 23350MB
[2024-07-30 11:45:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:11:39 lr 0.000015	 wd 0.0500	time 0.7813 (0.8719)	loss 0.9574 (1.1074)	grad_norm 2.4885 (nan)	loss_scale 1024.0000 (1159.4497)	mem 23350MB
[2024-07-30 11:46:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:10:11 lr 0.000015	 wd 0.0500	time 0.8266 (0.8715)	loss 1.2266 (1.1087)	grad_norm 5.8054 (nan)	loss_scale 1024.0000 (1151.9289)	mem 23350MB
[2024-07-30 11:47:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:08:44 lr 0.000015	 wd 0.0500	time 0.8081 (0.8711)	loss 1.3600 (1.1075)	grad_norm 2.1564 (nan)	loss_scale 1024.0000 (1145.1994)	mem 23350MB
[2024-07-30 11:49:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:07:17 lr 0.000015	 wd 0.0500	time 0.8278 (0.8707)	loss 1.1980 (1.1071)	grad_norm 3.0432 (nan)	loss_scale 1024.0000 (1139.1424)	mem 23350MB
[2024-07-30 11:50:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:05:49 lr 0.000015	 wd 0.0500	time 0.8029 (0.8704)	loss 0.7858 (1.1072)	grad_norm 2.1632 (nan)	loss_scale 1024.0000 (1133.6621)	mem 23350MB
[2024-07-30 11:52:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:04:22 lr 0.000016	 wd 0.0500	time 0.8168 (0.8703)	loss 0.7764 (1.1063)	grad_norm 2.5871 (nan)	loss_scale 1024.0000 (1128.6797)	mem 23350MB
[2024-07-30 11:53:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:02:55 lr 0.000016	 wd 0.0500	time 0.8047 (0.8700)	loss 0.8516 (1.1067)	grad_norm 3.8714 (nan)	loss_scale 1024.0000 (1124.1304)	mem 23350MB
[2024-07-30 11:55:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:28 lr 0.000016	 wd 0.0500	time 0.8191 (0.8697)	loss 1.2006 (1.1079)	grad_norm 2.4009 (nan)	loss_scale 1024.0000 (1119.9600)	mem 23350MB
[2024-07-30 11:56:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8203 (0.8694)	loss 1.1963 (1.1080)	grad_norm 3.7101 (nan)	loss_scale 1024.0000 (1116.1232)	mem 23350MB
[2024-07-30 11:56:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 3 training takes 0:36:17
[2024-07-30 11:56:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.982 (11.982)	Loss 0.4648 (0.4648)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 11:57:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.740 Acc@5 97.964
[2024-07-30 11:57:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 11:57:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.74%
[2024-07-30 11:57:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 11:57:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 11:57:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:38:36 lr 0.000016	 wd 0.0500	time 10.9978 (10.9978)	loss 1.2107 (1.2107)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 11:58:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:38:36 lr 0.000016	 wd 0.0500	time 0.8149 (0.9645)	loss 0.8800 (1.1212)	grad_norm 2.5771 (3.8721)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:00:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:35:04 lr 0.000016	 wd 0.0500	time 0.8171 (0.9140)	loss 1.0847 (1.1104)	grad_norm 1.9909 (3.2681)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:01:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:32:56 lr 0.000016	 wd 0.0500	time 0.8279 (0.8977)	loss 0.7422 (1.1066)	grad_norm 17.6864 (3.3200)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:03:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:31:08 lr 0.000017	 wd 0.0500	time 0.8210 (0.8890)	loss 1.3315 (1.1042)	grad_norm 2.3218 (3.1913)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:04:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:29:31 lr 0.000017	 wd 0.0500	time 0.8067 (0.8847)	loss 1.1378 (1.1048)	grad_norm 2.7787 (3.1054)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:06:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:27:57 lr 0.000017	 wd 0.0500	time 0.8070 (0.8820)	loss 0.9625 (1.1061)	grad_norm 2.7079 (3.1103)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:07:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:26:24 lr 0.000017	 wd 0.0500	time 0.8230 (0.8794)	loss 0.9586 (1.1068)	grad_norm 9.0314 (3.0516)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:08:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:24:53 lr 0.000017	 wd 0.0500	time 0.7525 (0.8773)	loss 0.7207 (1.1089)	grad_norm 2.9959 (3.0159)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:10:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:23:23 lr 0.000017	 wd 0.0500	time 0.8165 (0.8758)	loss 0.7901 (1.1093)	grad_norm 2.4129 (2.9894)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:11:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:21:53 lr 0.000018	 wd 0.0500	time 0.7631 (0.8747)	loss 1.3846 (1.1104)	grad_norm 2.0234 (3.0133)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:13:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:20:25 lr 0.000018	 wd 0.0500	time 0.8090 (0.8739)	loss 1.3952 (1.1119)	grad_norm 2.2019 (2.9943)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:14:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:18:56 lr 0.000018	 wd 0.0500	time 0.7684 (0.8732)	loss 1.2373 (1.1100)	grad_norm 2.8065 (2.9973)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:16:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:17:28 lr 0.000018	 wd 0.0500	time 0.8137 (0.8726)	loss 0.8994 (1.1093)	grad_norm 2.1149 (3.0058)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:17:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:16:00 lr 0.000018	 wd 0.0500	time 0.8146 (0.8720)	loss 1.3609 (1.1096)	grad_norm 3.1295 (2.9843)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:19:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:14:33 lr 0.000018	 wd 0.0500	time 0.8159 (0.8714)	loss 1.1107 (1.1096)	grad_norm 2.6162 (2.9712)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:20:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:13:05 lr 0.000019	 wd 0.0500	time 0.8677 (0.8712)	loss 1.2664 (1.1081)	grad_norm 2.0995 (2.9816)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:21:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:11:38 lr 0.000019	 wd 0.0500	time 0.8172 (0.8707)	loss 1.1063 (1.1080)	grad_norm 2.2212 (2.9673)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 12:23:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:10:10 lr 0.000019	 wd 0.0500	time 0.8272 (0.8704)	loss 1.4560 (1.1094)	grad_norm 2.6030 (2.9653)	loss_scale 2048.0000 (1068.3487)	mem 23350MB
[2024-07-30 12:24:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:08:43 lr 0.000019	 wd 0.0500	time 0.8130 (0.8703)	loss 1.2786 (1.1095)	grad_norm 2.0603 (2.9971)	loss_scale 2048.0000 (1119.8822)	mem 23350MB
[2024-07-30 12:26:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:07:16 lr 0.000019	 wd 0.0500	time 0.8001 (0.8700)	loss 0.7776 (1.1086)	grad_norm 2.2434 (2.9932)	loss_scale 2048.0000 (1166.2649)	mem 23350MB
[2024-07-30 12:27:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:05:49 lr 0.000019	 wd 0.0500	time 0.8210 (0.8698)	loss 0.8853 (1.1073)	grad_norm 2.5426 (3.0005)	loss_scale 2048.0000 (1208.2323)	mem 23350MB
[2024-07-30 12:29:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:04:22 lr 0.000020	 wd 0.0500	time 0.8210 (0.8696)	loss 0.8812 (1.1075)	grad_norm 2.9466 (2.9896)	loss_scale 2048.0000 (1246.3862)	mem 23350MB
[2024-07-30 12:30:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:02:55 lr 0.000020	 wd 0.0500	time 0.7629 (0.8694)	loss 0.7482 (1.1077)	grad_norm 2.5493 (2.9798)	loss_scale 2048.0000 (1281.2238)	mem 23350MB
[2024-07-30 12:32:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:28 lr 0.000020	 wd 0.0500	time 0.8187 (0.8693)	loss 0.6796 (1.1070)	grad_norm 2.7811 (2.9865)	loss_scale 2048.0000 (1313.1595)	mem 23350MB
[2024-07-30 12:33:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8182 (0.8692)	loss 1.0545 (1.1074)	grad_norm 2.5053 (3.0053)	loss_scale 2048.0000 (1342.5414)	mem 23350MB
[2024-07-30 12:33:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 4 training takes 0:36:17
[2024-07-30 12:33:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.171 (12.171)	Loss 0.4541 (0.4541)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 12:34:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.746 Acc@5 97.960
[2024-07-30 12:34:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 12:34:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 12:34:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 12:34:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 12:34:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:46:49 lr 0.000020	 wd 0.0500	time 11.1949 (11.1949)	loss 1.3049 (1.3049)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:35:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:38:36 lr 0.000020	 wd 0.0500	time 0.8217 (0.9642)	loss 0.9459 (1.1309)	grad_norm 2.9236 (2.7640)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:37:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:35:05 lr 0.000020	 wd 0.0500	time 0.8355 (0.9145)	loss 1.1537 (1.0999)	grad_norm 2.8110 (2.7724)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:38:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:32:57 lr 0.000020	 wd 0.0500	time 0.8170 (0.8983)	loss 0.7721 (1.0942)	grad_norm 2.4078 (2.7372)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:40:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:31:10 lr 0.000020	 wd 0.0500	time 0.8168 (0.8897)	loss 1.4037 (1.1024)	grad_norm 3.6763 (2.8143)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:41:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:29:31 lr 0.000020	 wd 0.0500	time 0.8173 (0.8847)	loss 0.9203 (1.1001)	grad_norm 3.0406 (2.9296)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:42:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:27:56 lr 0.000020	 wd 0.0500	time 0.8032 (0.8812)	loss 0.8622 (1.1007)	grad_norm 2.1358 (2.9621)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:44:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:26:23 lr 0.000020	 wd 0.0500	time 0.8150 (0.8790)	loss 0.8711 (1.1005)	grad_norm 2.2041 (3.0324)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:45:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:24:52 lr 0.000020	 wd 0.0500	time 0.8208 (0.8772)	loss 1.4072 (1.0985)	grad_norm 3.5899 (3.0455)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:47:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:23:23 lr 0.000020	 wd 0.0500	time 0.8168 (0.8760)	loss 0.9869 (1.0979)	grad_norm 4.0410 (3.0604)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:48:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:21:54 lr 0.000020	 wd 0.0500	time 0.8233 (0.8753)	loss 1.0779 (1.0970)	grad_norm 2.7918 (3.0354)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:50:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:20:26 lr 0.000020	 wd 0.0500	time 0.8167 (0.8745)	loss 1.1453 (1.0985)	grad_norm 2.4509 (3.0503)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:51:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:18:57 lr 0.000020	 wd 0.0500	time 0.8198 (0.8735)	loss 1.3743 (1.0976)	grad_norm 2.3333 (3.0197)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:53:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:17:29 lr 0.000020	 wd 0.0500	time 0.8291 (0.8728)	loss 0.7750 (1.0987)	grad_norm 2.2803 (3.0080)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:54:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:16:01 lr 0.000020	 wd 0.0500	time 0.7606 (0.8722)	loss 1.2153 (1.1022)	grad_norm 2.3224 (3.0406)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:55:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:14:33 lr 0.000020	 wd 0.0500	time 0.8198 (0.8717)	loss 0.7370 (1.1040)	grad_norm 2.5997 (3.0210)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:57:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:13:05 lr 0.000020	 wd 0.0500	time 0.8173 (0.8712)	loss 1.2893 (1.1070)	grad_norm 4.4550 (3.0123)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 12:58:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:11:38 lr 0.000020	 wd 0.0500	time 0.8203 (0.8707)	loss 0.7794 (1.1075)	grad_norm 2.1342 (3.0655)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:00:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:10:11 lr 0.000020	 wd 0.0500	time 0.8052 (0.8704)	loss 0.7799 (1.1051)	grad_norm 2.2343 (3.0646)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:01:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:08:43 lr 0.000020	 wd 0.0500	time 0.8251 (0.8702)	loss 1.0845 (1.1047)	grad_norm 1.9397 (3.0508)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:03:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:07:16 lr 0.000020	 wd 0.0500	time 0.8500 (0.8701)	loss 1.4806 (1.1057)	grad_norm 2.6025 (3.0621)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:04:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:05:49 lr 0.000020	 wd 0.0500	time 0.8860 (0.8697)	loss 1.4566 (1.1057)	grad_norm 2.4337 (3.1125)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:06:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:04:22 lr 0.000020	 wd 0.0500	time 0.8087 (0.8695)	loss 1.2311 (1.1062)	grad_norm 2.7445 (3.1139)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:07:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:02:55 lr 0.000020	 wd 0.0500	time 0.8141 (0.8693)	loss 1.1188 (1.1051)	grad_norm 2.4502 (3.1064)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:08:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:28 lr 0.000020	 wd 0.0500	time 0.8115 (0.8692)	loss 0.8624 (1.1046)	grad_norm 1.9720 (3.0962)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:10:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8021 (0.8691)	loss 1.3183 (1.1056)	grad_norm 8.8821 (3.0888)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:10:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 5 training takes 0:36:16
[2024-07-30 13:10:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.439 (11.439)	Loss 0.4751 (0.4751)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 13:10:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.672 Acc@5 97.984
[2024-07-30 13:10:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 13:10:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 13:11:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:12:14 lr 0.000020	 wd 0.0500	time 11.8043 (11.8043)	loss 1.1791 (1.1791)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:12:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:38:57 lr 0.000020	 wd 0.0500	time 0.8191 (0.9730)	loss 0.9382 (1.1311)	grad_norm 2.2483 (2.6310)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:14:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:35:13 lr 0.000020	 wd 0.0500	time 0.8162 (0.9180)	loss 0.8777 (1.1136)	grad_norm 2.5789 (2.9187)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:15:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:33:03 lr 0.000020	 wd 0.0500	time 0.8089 (0.9010)	loss 0.8880 (1.1093)	grad_norm 2.3875 (2.8621)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:16:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:31:17 lr 0.000020	 wd 0.0500	time 0.8261 (0.8933)	loss 0.6569 (1.1080)	grad_norm 2.4557 (2.8119)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:18:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:29:37 lr 0.000020	 wd 0.0500	time 0.8017 (0.8878)	loss 1.2139 (1.1095)	grad_norm 2.3226 (2.8004)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:19:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:28:00 lr 0.000020	 wd 0.0500	time 0.8278 (0.8838)	loss 0.8096 (1.1019)	grad_norm 2.9252 (2.8338)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:21:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:26:28 lr 0.000020	 wd 0.0500	time 0.7727 (0.8813)	loss 1.3316 (1.1041)	grad_norm 2.2491 (2.9463)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 13:22:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:24:56 lr 0.000020	 wd 0.0500	time 0.7986 (0.8794)	loss 0.7260 (1.1091)	grad_norm 2.4113 (2.9900)	loss_scale 4096.0000 (2257.6579)	mem 23350MB
[2024-07-30 13:24:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:23:26 lr 0.000020	 wd 0.0500	time 0.8228 (0.8778)	loss 1.5503 (1.1047)	grad_norm 3.4549 (3.0548)	loss_scale 4096.0000 (2461.6915)	mem 23350MB
[2024-07-30 13:25:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:21:56 lr 0.000020	 wd 0.0500	time 0.8165 (0.8766)	loss 0.9769 (1.1047)	grad_norm 2.8924 (inf)	loss_scale 2048.0000 (2461.2827)	mem 23350MB
[2024-07-30 13:27:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:20:27 lr 0.000020	 wd 0.0500	time 0.8090 (0.8754)	loss 0.7575 (1.1005)	grad_norm 2.6628 (inf)	loss_scale 2048.0000 (2423.7457)	mem 23350MB
[2024-07-30 13:28:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:18:58 lr 0.000020	 wd 0.0500	time 0.8686 (0.8747)	loss 1.2304 (1.0975)	grad_norm 2.1864 (inf)	loss_scale 2048.0000 (2392.4596)	mem 23350MB
[2024-07-30 13:29:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:17:30 lr 0.000020	 wd 0.0500	time 0.8268 (0.8742)	loss 1.0656 (1.0966)	grad_norm 1.9514 (inf)	loss_scale 2048.0000 (2365.9831)	mem 23350MB
[2024-07-30 13:31:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:16:02 lr 0.000020	 wd 0.0500	time 0.8107 (0.8738)	loss 0.9589 (1.0959)	grad_norm 3.0849 (inf)	loss_scale 2048.0000 (2343.2862)	mem 23350MB
[2024-07-30 13:32:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:14:34 lr 0.000020	 wd 0.0500	time 0.8209 (0.8732)	loss 0.6492 (1.0958)	grad_norm 2.0152 (inf)	loss_scale 2048.0000 (2323.6136)	mem 23350MB
[2024-07-30 13:34:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:13:07 lr 0.000020	 wd 0.0500	time 0.8014 (0.8726)	loss 1.2790 (1.0939)	grad_norm 1.8904 (inf)	loss_scale 2048.0000 (2306.3985)	mem 23350MB
[2024-07-30 13:35:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:11:39 lr 0.000020	 wd 0.0500	time 0.8014 (0.8722)	loss 1.3114 (1.0921)	grad_norm 1.9063 (inf)	loss_scale 2048.0000 (2291.2075)	mem 23350MB
[2024-07-30 13:37:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:10:11 lr 0.000020	 wd 0.0500	time 0.8163 (0.8717)	loss 1.1216 (1.0922)	grad_norm 1.9542 (inf)	loss_scale 2048.0000 (2277.7035)	mem 23350MB
[2024-07-30 13:38:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:08:44 lr 0.000020	 wd 0.0500	time 0.8236 (0.8713)	loss 1.3501 (1.0945)	grad_norm 2.6731 (inf)	loss_scale 2048.0000 (2265.6202)	mem 23350MB
[2024-07-30 13:39:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:07:17 lr 0.000020	 wd 0.0500	time 0.8093 (0.8710)	loss 0.8803 (1.0952)	grad_norm 1.9706 (inf)	loss_scale 2048.0000 (2254.7446)	mem 23350MB
[2024-07-30 13:41:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:05:50 lr 0.000020	 wd 0.0500	time 0.8110 (0.8707)	loss 1.0528 (1.0956)	grad_norm 2.8699 (inf)	loss_scale 2048.0000 (2244.9043)	mem 23350MB
[2024-07-30 13:42:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:04:22 lr 0.000020	 wd 0.0500	time 0.7631 (0.8704)	loss 0.7595 (1.0951)	grad_norm 2.3111 (nan)	loss_scale 1024.0000 (2214.5570)	mem 23350MB
[2024-07-30 13:44:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:02:55 lr 0.000020	 wd 0.0500	time 0.8327 (0.8704)	loss 0.9027 (1.0970)	grad_norm 12.5888 (nan)	loss_scale 1024.0000 (2162.8162)	mem 23350MB
[2024-07-30 13:45:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:28 lr 0.000020	 wd 0.0500	time 0.8254 (0.8702)	loss 1.3689 (1.0961)	grad_norm 2.5522 (nan)	loss_scale 1024.0000 (2115.3853)	mem 23350MB
[2024-07-30 13:47:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.7667 (0.8699)	loss 1.3581 (1.0955)	grad_norm 3.0331 (nan)	loss_scale 1024.0000 (2071.7473)	mem 23350MB
[2024-07-30 13:47:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 6 training takes 0:36:18
[2024-07-30 13:47:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.082 (12.082)	Loss 0.4741 (0.4741)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-07-30 13:47:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.620 Acc@5 97.954
[2024-07-30 13:47:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-07-30 13:47:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 13:48:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:16:52 lr 0.000020	 wd 0.0500	time 11.9153 (11.9153)	loss 0.8257 (0.8257)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:49:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:39:04 lr 0.000020	 wd 0.0500	time 0.8209 (0.9759)	loss 1.1518 (1.0772)	grad_norm 1.9026 (3.0647)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:50:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:35:18 lr 0.000020	 wd 0.0500	time 0.8154 (0.9203)	loss 1.0330 (1.0983)	grad_norm 3.2275 (3.1163)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:52:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:33:05 lr 0.000020	 wd 0.0500	time 0.8126 (0.9016)	loss 1.4091 (1.0975)	grad_norm 2.3322 (2.9827)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:53:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:31:16 lr 0.000020	 wd 0.0500	time 0.8253 (0.8926)	loss 0.7750 (1.1009)	grad_norm 3.7144 (2.8911)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:55:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:29:36 lr 0.000020	 wd 0.0500	time 0.8018 (0.8872)	loss 1.3172 (1.0991)	grad_norm 1.9167 (2.9151)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:56:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:28:00 lr 0.000020	 wd 0.0500	time 0.8199 (0.8836)	loss 1.2161 (1.1041)	grad_norm 2.7201 (2.8632)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:58:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:26:28 lr 0.000020	 wd 0.0500	time 0.8061 (0.8816)	loss 1.0090 (1.0988)	grad_norm 2.6615 (2.8871)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 13:59:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:24:57 lr 0.000020	 wd 0.0500	time 0.8037 (0.8799)	loss 0.8718 (1.0963)	grad_norm 7.4113 (2.8675)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:00:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:23:26 lr 0.000020	 wd 0.0500	time 0.8224 (0.8783)	loss 1.4532 (1.0996)	grad_norm 2.0136 (2.8494)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:02:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:21:56 lr 0.000020	 wd 0.0500	time 0.8120 (0.8767)	loss 0.8010 (1.0995)	grad_norm 2.2103 (2.8411)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:03:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:20:27 lr 0.000020	 wd 0.0500	time 0.7607 (0.8755)	loss 1.1686 (1.0977)	grad_norm 3.3992 (2.8424)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:05:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:18:59 lr 0.000020	 wd 0.0500	time 0.8701 (0.8748)	loss 1.2119 (1.0946)	grad_norm 2.8149 (2.9193)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:06:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:17:30 lr 0.000020	 wd 0.0500	time 0.8195 (0.8738)	loss 1.2455 (1.0947)	grad_norm 2.5037 (2.9324)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:08:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:16:02 lr 0.000019	 wd 0.0500	time 0.8134 (0.8732)	loss 1.1606 (1.0959)	grad_norm 2.3112 (2.9179)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:09:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:14:34 lr 0.000019	 wd 0.0500	time 0.8046 (0.8726)	loss 0.7506 (1.0968)	grad_norm 2.4094 (2.9119)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:11:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:13:06 lr 0.000019	 wd 0.0500	time 0.8573 (0.8722)	loss 1.1684 (1.0975)	grad_norm 3.1376 (2.9105)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:12:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:11:39 lr 0.000019	 wd 0.0500	time 0.7602 (0.8720)	loss 1.1874 (1.0973)	grad_norm 3.9399 (2.9481)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:13:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:10:11 lr 0.000019	 wd 0.0500	time 0.8038 (0.8716)	loss 0.7025 (1.0993)	grad_norm 2.3691 (2.9393)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:15:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:08:44 lr 0.000019	 wd 0.0500	time 0.8075 (0.8713)	loss 1.1126 (1.0999)	grad_norm 2.2251 (2.9325)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:16:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:07:17 lr 0.000019	 wd 0.0500	time 0.8149 (0.8709)	loss 1.1657 (1.1005)	grad_norm 2.2713 (2.9390)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:18:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:05:50 lr 0.000019	 wd 0.0500	time 0.8080 (0.8707)	loss 1.4506 (1.1030)	grad_norm 2.5508 (2.9293)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:19:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:04:22 lr 0.000019	 wd 0.0500	time 0.8029 (0.8704)	loss 1.1887 (1.1012)	grad_norm 2.4969 (2.9638)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:21:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:02:55 lr 0.000019	 wd 0.0500	time 0.8768 (0.8702)	loss 1.3689 (1.1006)	grad_norm 20.1144 (2.9849)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:22:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:28 lr 0.000019	 wd 0.0500	time 0.8025 (0.8700)	loss 1.2847 (1.1006)	grad_norm 2.1009 (3.0044)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:24:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8118 (0.8697)	loss 1.4080 (1.1000)	grad_norm 2.1893 (2.9999)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:24:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 7 training takes 0:36:18
[2024-07-30 14:24:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.967 (11.967)	Loss 0.4368 (0.4368)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 14:24:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.654 Acc@5 97.988
[2024-07-30 14:24:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 14:24:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 14:24:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:13:12 lr 0.000019	 wd 0.0500	time 11.8277 (11.8277)	loss 1.1920 (1.1920)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:26:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:39:10 lr 0.000019	 wd 0.0500	time 0.8134 (0.9787)	loss 0.8461 (1.1518)	grad_norm 2.4068 (2.7855)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:27:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:35:23 lr 0.000019	 wd 0.0500	time 0.8049 (0.9223)	loss 0.7804 (1.1289)	grad_norm 2.5154 (2.8689)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:29:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:33:09 lr 0.000019	 wd 0.0500	time 0.8598 (0.9037)	loss 1.4099 (1.1151)	grad_norm 3.1333 (3.0193)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:30:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:31:18 lr 0.000019	 wd 0.0500	time 0.8893 (0.8936)	loss 0.8099 (1.1123)	grad_norm 2.2116 (3.0014)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:32:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:29:37 lr 0.000019	 wd 0.0500	time 0.8191 (0.8878)	loss 0.7601 (1.1028)	grad_norm 2.8053 (3.0919)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:33:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:28:01 lr 0.000019	 wd 0.0500	time 0.8229 (0.8841)	loss 0.7877 (1.1025)	grad_norm 3.2115 (3.1750)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:34:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:26:28 lr 0.000019	 wd 0.0500	time 0.8147 (0.8814)	loss 1.2262 (1.1028)	grad_norm 5.6977 (3.1435)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:36:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:24:56 lr 0.000019	 wd 0.0500	time 0.8139 (0.8791)	loss 1.2780 (1.1048)	grad_norm 2.1144 (3.0806)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:37:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:23:26 lr 0.000019	 wd 0.0500	time 0.8158 (0.8778)	loss 0.8780 (1.1007)	grad_norm 4.8474 (3.0583)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:39:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:21:56 lr 0.000019	 wd 0.0500	time 0.8724 (0.8767)	loss 1.2818 (1.1027)	grad_norm 2.1438 (3.0241)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:40:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:20:27 lr 0.000019	 wd 0.0500	time 0.8151 (0.8759)	loss 1.2377 (1.1000)	grad_norm 2.0561 (3.0110)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 14:42:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:18:59 lr 0.000019	 wd 0.0500	time 0.8147 (0.8749)	loss 1.0086 (1.1005)	grad_norm 2.7269 (2.9811)	loss_scale 2048.0000 (1066.6311)	mem 23350MB
[2024-07-30 14:43:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:17:30 lr 0.000019	 wd 0.0500	time 0.7620 (0.8743)	loss 0.7848 (1.1003)	grad_norm 2.3487 (2.9620)	loss_scale 2048.0000 (1142.0630)	mem 23350MB
[2024-07-30 14:45:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:16:02 lr 0.000019	 wd 0.0500	time 0.8084 (0.8737)	loss 1.2541 (1.1015)	grad_norm 2.3725 (3.1038)	loss_scale 2048.0000 (1206.7266)	mem 23350MB
[2024-07-30 14:46:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:14:34 lr 0.000019	 wd 0.0500	time 0.8098 (0.8730)	loss 1.0072 (1.1025)	grad_norm 2.2395 (3.1060)	loss_scale 2048.0000 (1262.7742)	mem 23350MB
[2024-07-30 14:47:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:13:07 lr 0.000019	 wd 0.0500	time 0.8108 (0.8725)	loss 1.3069 (1.1043)	grad_norm 2.2866 (3.0839)	loss_scale 2048.0000 (1311.8201)	mem 23350MB
[2024-07-30 14:49:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:11:39 lr 0.000019	 wd 0.0500	time 0.8199 (0.8720)	loss 0.7253 (1.1065)	grad_norm 2.9430 (inf)	loss_scale 1024.0000 (1323.7954)	mem 23350MB
[2024-07-30 14:50:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:10:11 lr 0.000019	 wd 0.0500	time 0.8215 (0.8714)	loss 0.7411 (1.1048)	grad_norm 10.4759 (inf)	loss_scale 1024.0000 (1307.1494)	mem 23350MB
[2024-07-30 14:52:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:08:44 lr 0.000019	 wd 0.0500	time 0.8669 (0.8714)	loss 1.2176 (1.1064)	grad_norm 2.3075 (inf)	loss_scale 1024.0000 (1292.2546)	mem 23350MB
[2024-07-30 14:53:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:07:17 lr 0.000019	 wd 0.0500	time 0.8906 (0.8710)	loss 0.7146 (1.1053)	grad_norm 3.3430 (inf)	loss_scale 1024.0000 (1278.8486)	mem 23350MB
[2024-07-30 14:55:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:05:50 lr 0.000019	 wd 0.0500	time 0.8479 (0.8710)	loss 0.8741 (1.1060)	grad_norm 2.7864 (inf)	loss_scale 1024.0000 (1266.7187)	mem 23350MB
[2024-07-30 14:56:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:04:22 lr 0.000019	 wd 0.0500	time 0.8161 (0.8707)	loss 1.1681 (1.1049)	grad_norm 3.0150 (inf)	loss_scale 1024.0000 (1255.6910)	mem 23350MB
[2024-07-30 14:58:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:02:55 lr 0.000019	 wd 0.0500	time 0.8032 (0.8705)	loss 1.2146 (1.1034)	grad_norm 2.7956 (inf)	loss_scale 1024.0000 (1245.6219)	mem 23350MB
[2024-07-30 14:59:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:28 lr 0.000019	 wd 0.0500	time 0.8074 (0.8704)	loss 1.0394 (1.1034)	grad_norm 3.3331 (inf)	loss_scale 1024.0000 (1236.3915)	mem 23350MB
[2024-07-30 15:00:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8129 (0.8701)	loss 0.7935 (1.1032)	grad_norm 2.4888 (inf)	loss_scale 1024.0000 (1227.8992)	mem 23350MB
[2024-07-30 15:00:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 8 training takes 0:36:19
[2024-07-30 15:01:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.882 (11.882)	Loss 0.4448 (0.4448)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 15:01:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.704 Acc@5 98.044
[2024-07-30 15:01:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 15:01:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 15:01:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][0/2502]	eta 8:15:16 lr 0.000019	 wd 0.0500	time 11.8770 (11.8770)	loss 1.2706 (1.2706)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:03:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:38:55 lr 0.000019	 wd 0.0500	time 0.8085 (0.9724)	loss 1.2325 (1.0863)	grad_norm 2.4598 (2.8518)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:04:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:35:15 lr 0.000019	 wd 0.0500	time 0.7754 (0.9192)	loss 0.8978 (1.1092)	grad_norm 2.3431 (3.1726)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:06:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:33:05 lr 0.000019	 wd 0.0500	time 0.8651 (0.9018)	loss 1.3666 (1.1153)	grad_norm 2.9492 (3.4370)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:07:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:31:16 lr 0.000019	 wd 0.0500	time 0.8245 (0.8927)	loss 0.8956 (1.0971)	grad_norm 2.5525 (3.4320)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:08:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:29:38 lr 0.000019	 wd 0.0500	time 0.8697 (0.8883)	loss 0.6382 (1.0926)	grad_norm 6.5424 (3.3140)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:10:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:28:02 lr 0.000019	 wd 0.0500	time 0.8088 (0.8843)	loss 0.7476 (1.0955)	grad_norm 4.6475 (3.2505)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:11:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:26:28 lr 0.000019	 wd 0.0500	time 0.8143 (0.8817)	loss 1.3849 (1.0984)	grad_norm 1.7849 (3.1717)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:13:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:24:56 lr 0.000019	 wd 0.0500	time 0.8224 (0.8795)	loss 1.3676 (1.0998)	grad_norm 2.0338 (3.2040)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:14:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:23:26 lr 0.000019	 wd 0.0500	time 0.8242 (0.8777)	loss 1.2939 (1.1002)	grad_norm 4.1449 (3.2336)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:16:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:21:56 lr 0.000019	 wd 0.0500	time 0.8221 (0.8764)	loss 1.2357 (1.1007)	grad_norm 2.1798 (3.2170)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:17:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:20:26 lr 0.000018	 wd 0.0500	time 0.8153 (0.8751)	loss 1.3375 (1.0968)	grad_norm 2.3930 (3.1725)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:19:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:18:58 lr 0.000018	 wd 0.0500	time 0.8057 (0.8741)	loss 0.9239 (1.0971)	grad_norm 2.6818 (3.1490)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:20:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:17:29 lr 0.000018	 wd 0.0500	time 0.8197 (0.8733)	loss 1.2942 (1.0980)	grad_norm 3.3904 (3.1386)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:21:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:16:01 lr 0.000018	 wd 0.0500	time 0.8165 (0.8729)	loss 0.8910 (1.0960)	grad_norm 3.5632 (3.1388)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:23:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:14:34 lr 0.000018	 wd 0.0500	time 0.7995 (0.8727)	loss 0.8391 (1.0986)	grad_norm 3.6357 (3.1117)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:24:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:13:06 lr 0.000018	 wd 0.0500	time 0.8117 (0.8722)	loss 0.8597 (1.0965)	grad_norm 2.5382 (3.0917)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:26:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:11:39 lr 0.000018	 wd 0.0500	time 0.8211 (0.8718)	loss 1.3413 (1.0967)	grad_norm 3.7872 (3.0830)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:27:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:10:11 lr 0.000018	 wd 0.0500	time 0.8083 (0.8714)	loss 1.2179 (1.0960)	grad_norm 2.3958 (3.0880)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:29:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:08:44 lr 0.000018	 wd 0.0500	time 0.8329 (0.8710)	loss 1.3134 (1.0949)	grad_norm 3.9986 (3.0941)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:30:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:07:17 lr 0.000018	 wd 0.0500	time 0.8208 (0.8707)	loss 1.0997 (1.0953)	grad_norm 2.6275 (3.0820)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:31:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:05:49 lr 0.000018	 wd 0.0500	time 0.8154 (0.8704)	loss 1.2936 (1.0964)	grad_norm 2.6664 (3.0785)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:33:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:04:22 lr 0.000018	 wd 0.0500	time 0.8731 (0.8702)	loss 1.1750 (1.0973)	grad_norm 2.8988 (3.0877)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:34:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:02:55 lr 0.000018	 wd 0.0500	time 0.8153 (0.8699)	loss 0.7814 (1.0974)	grad_norm 2.6314 (3.0826)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:36:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:28 lr 0.000018	 wd 0.0500	time 0.8206 (0.8697)	loss 1.0945 (1.0979)	grad_norm 4.9776 (3.0758)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:37:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.8147 (0.8695)	loss 1.3178 (1.0974)	grad_norm 2.8855 (3.0874)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:37:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 9 training takes 0:36:17
[2024-07-30 15:38:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.042 (12.042)	Loss 0.4573 (0.4573)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 15:38:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.740 Acc@5 98.018
[2024-07-30 15:38:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 15:38:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 15:38:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][0/2502]	eta 8:41:11 lr 0.000018	 wd 0.0500	time 12.4987 (12.4987)	loss 1.3239 (1.3239)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:40:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:39:13 lr 0.000018	 wd 0.0500	time 0.7976 (0.9796)	loss 0.8018 (1.1026)	grad_norm 4.6542 (2.9520)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:41:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:35:23 lr 0.000018	 wd 0.0500	time 0.8134 (0.9224)	loss 1.3102 (1.0916)	grad_norm 2.0853 (2.8546)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:42:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:33:10 lr 0.000018	 wd 0.0500	time 0.8216 (0.9040)	loss 1.2764 (1.0914)	grad_norm 10.2258 (2.9589)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:44:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:31:19 lr 0.000018	 wd 0.0500	time 0.8068 (0.8943)	loss 0.6992 (1.0987)	grad_norm 2.6733 (2.8985)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:45:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:29:38 lr 0.000018	 wd 0.0500	time 0.8559 (0.8885)	loss 0.6866 (1.0964)	grad_norm 2.2704 (3.2563)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:47:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:28:02 lr 0.000018	 wd 0.0500	time 0.8226 (0.8845)	loss 1.1638 (1.1028)	grad_norm 4.8265 (3.1705)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 15:48:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:26:28 lr 0.000018	 wd 0.0500	time 0.8096 (0.8817)	loss 1.2386 (1.1008)	grad_norm 2.0492 (3.1295)	loss_scale 2048.0000 (1105.8031)	mem 23350MB
[2024-07-30 15:50:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:24:57 lr 0.000018	 wd 0.0500	time 0.8097 (0.8796)	loss 1.1884 (1.1033)	grad_norm 2.2209 (3.1359)	loss_scale 2048.0000 (1223.4307)	mem 23350MB
[2024-07-30 15:51:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:23:27 lr 0.000018	 wd 0.0500	time 0.8271 (0.8789)	loss 1.2447 (1.1019)	grad_norm 2.1782 (3.1471)	loss_scale 2048.0000 (1314.9478)	mem 23350MB
[2024-07-30 15:53:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:21:57 lr 0.000018	 wd 0.0500	time 0.7634 (0.8775)	loss 1.2388 (1.1032)	grad_norm 2.2730 (3.0940)	loss_scale 2048.0000 (1388.1798)	mem 23350MB
[2024-07-30 15:54:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:20:28 lr 0.000018	 wd 0.0500	time 0.8625 (0.8765)	loss 0.9464 (1.1099)	grad_norm 2.8884 (3.0920)	loss_scale 2048.0000 (1448.1090)	mem 23350MB
[2024-07-30 15:55:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:18:59 lr 0.000018	 wd 0.0500	time 0.8764 (0.8754)	loss 1.3901 (1.1070)	grad_norm 2.3446 (3.0965)	loss_scale 2048.0000 (1498.0583)	mem 23350MB
[2024-07-30 15:57:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:17:31 lr 0.000018	 wd 0.0500	time 0.8039 (0.8745)	loss 1.1383 (1.1053)	grad_norm 2.5000 (3.0781)	loss_scale 2048.0000 (1540.3290)	mem 23350MB
[2024-07-30 15:58:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:16:02 lr 0.000018	 wd 0.0500	time 0.8030 (0.8738)	loss 1.3605 (1.1039)	grad_norm 2.6790 (3.0711)	loss_scale 2048.0000 (1576.5653)	mem 23350MB
[2024-07-30 16:00:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:14:34 lr 0.000018	 wd 0.0500	time 0.8219 (0.8732)	loss 1.0832 (1.1022)	grad_norm 3.0718 (3.0724)	loss_scale 2048.0000 (1607.9734)	mem 23350MB
[2024-07-30 16:01:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:13:07 lr 0.000018	 wd 0.0500	time 0.8249 (0.8728)	loss 1.0982 (1.1004)	grad_norm 2.8901 (3.0579)	loss_scale 2048.0000 (1635.4578)	mem 23350MB
[2024-07-30 16:03:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:11:39 lr 0.000018	 wd 0.0500	time 0.8272 (0.8724)	loss 1.2042 (1.1021)	grad_norm 2.9259 (3.0605)	loss_scale 2048.0000 (1659.7108)	mem 23350MB
[2024-07-30 16:04:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:10:12 lr 0.000018	 wd 0.0500	time 0.8194 (0.8722)	loss 0.9068 (1.1005)	grad_norm 2.4138 (3.0522)	loss_scale 2048.0000 (1681.2704)	mem 23350MB
[2024-07-30 16:05:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:08:44 lr 0.000018	 wd 0.0500	time 0.8464 (0.8721)	loss 1.0038 (1.0993)	grad_norm 2.2487 (3.0354)	loss_scale 2048.0000 (1700.5618)	mem 23350MB
[2024-07-30 16:07:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:07:17 lr 0.000017	 wd 0.0500	time 0.8232 (0.8716)	loss 1.2254 (1.1024)	grad_norm 2.3289 (nan)	loss_scale 1024.0000 (1701.5492)	mem 23350MB
[2024-07-30 16:08:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:05:50 lr 0.000017	 wd 0.0500	time 0.8176 (0.8713)	loss 0.9807 (1.1040)	grad_norm 3.3400 (nan)	loss_scale 1024.0000 (1669.3003)	mem 23350MB
[2024-07-30 16:10:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:04:23 lr 0.000017	 wd 0.0500	time 0.8228 (0.8710)	loss 1.2181 (1.1049)	grad_norm 2.3606 (nan)	loss_scale 1024.0000 (1639.9818)	mem 23350MB
[2024-07-30 16:11:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:02:55 lr 0.000017	 wd 0.0500	time 0.8142 (0.8707)	loss 1.2187 (1.1028)	grad_norm 2.4478 (nan)	loss_scale 1024.0000 (1613.2116)	mem 23350MB
[2024-07-30 16:13:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:28 lr 0.000017	 wd 0.0500	time 0.8207 (0.8705)	loss 1.3703 (1.1016)	grad_norm 2.3546 (nan)	loss_scale 1024.0000 (1588.6714)	mem 23350MB
[2024-07-30 16:14:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.8110 (0.8702)	loss 0.7543 (1.1008)	grad_norm 2.5295 (nan)	loss_scale 1024.0000 (1566.0936)	mem 23350MB
[2024-07-30 16:14:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 10 training takes 0:36:19
[2024-07-30 16:14:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.988 (11.988)	Loss 0.4626 (0.4626)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 16:15:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.704 Acc@5 98.008
[2024-07-30 16:15:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 16:15:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 16:15:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:27:03 lr 0.000017	 wd 0.0500	time 12.1595 (12.1595)	loss 0.9097 (0.9097)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:16:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:39:06 lr 0.000017	 wd 0.0500	time 0.8099 (0.9770)	loss 1.3024 (1.0875)	grad_norm 2.1799 (3.1633)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:18:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:35:24 lr 0.000017	 wd 0.0500	time 0.8258 (0.9228)	loss 1.1930 (1.0971)	grad_norm 2.6937 (3.0523)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:19:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:33:12 lr 0.000017	 wd 0.0500	time 0.8347 (0.9050)	loss 0.8412 (1.0909)	grad_norm 2.5135 (2.9958)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:21:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:31:20 lr 0.000017	 wd 0.0500	time 0.8058 (0.8948)	loss 1.3290 (1.0929)	grad_norm 2.2108 (3.1207)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:22:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:29:39 lr 0.000017	 wd 0.0500	time 0.8200 (0.8891)	loss 0.7337 (1.0915)	grad_norm 2.6194 (3.1238)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:24:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:28:03 lr 0.000017	 wd 0.0500	time 0.8118 (0.8851)	loss 1.4516 (1.0938)	grad_norm 2.7493 (3.0536)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:25:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:26:30 lr 0.000017	 wd 0.0500	time 0.8295 (0.8824)	loss 0.8705 (1.0912)	grad_norm 2.8263 (3.1231)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:26:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:24:58 lr 0.000017	 wd 0.0500	time 0.8191 (0.8804)	loss 1.3772 (1.0898)	grad_norm 2.0754 (3.0524)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:28:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:23:27 lr 0.000017	 wd 0.0500	time 0.8100 (0.8788)	loss 0.8228 (1.0918)	grad_norm 3.0785 (3.0094)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:29:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:21:58 lr 0.000017	 wd 0.0500	time 0.8086 (0.8775)	loss 1.1845 (1.0928)	grad_norm 4.4575 (3.0721)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:31:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:20:28 lr 0.000017	 wd 0.0500	time 0.8215 (0.8764)	loss 0.8904 (1.0912)	grad_norm 2.5676 (3.0519)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:32:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:19:00 lr 0.000017	 wd 0.0500	time 0.8609 (0.8757)	loss 0.9357 (1.0875)	grad_norm 2.5114 (3.0336)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:34:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:17:31 lr 0.000017	 wd 0.0500	time 0.8036 (0.8750)	loss 1.1952 (1.0881)	grad_norm 2.9667 (3.0015)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:35:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:16:03 lr 0.000017	 wd 0.0500	time 0.8204 (0.8745)	loss 0.7846 (1.0892)	grad_norm 2.1265 (2.9713)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:37:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:14:35 lr 0.000017	 wd 0.0500	time 0.8258 (0.8740)	loss 1.3443 (1.0891)	grad_norm 3.2919 (3.0348)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:38:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:13:07 lr 0.000017	 wd 0.0500	time 0.8117 (0.8735)	loss 1.2092 (1.0905)	grad_norm 1.8661 (3.0289)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:39:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:11:40 lr 0.000017	 wd 0.0500	time 0.8226 (0.8729)	loss 0.9733 (1.0912)	grad_norm 2.2435 (3.0637)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:41:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:10:12 lr 0.000017	 wd 0.0500	time 0.8206 (0.8724)	loss 0.7901 (1.0902)	grad_norm 2.2682 (3.0866)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:42:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:08:44 lr 0.000017	 wd 0.0500	time 0.8273 (0.8720)	loss 1.0103 (1.0887)	grad_norm 2.5677 (3.0758)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:44:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:07:17 lr 0.000017	 wd 0.0500	time 0.8155 (0.8716)	loss 1.1429 (1.0892)	grad_norm 3.1359 (3.0617)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:45:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:05:50 lr 0.000017	 wd 0.0500	time 0.8014 (0.8712)	loss 1.2859 (1.0888)	grad_norm 1.8979 (3.0440)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:47:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:04:23 lr 0.000017	 wd 0.0500	time 0.8129 (0.8711)	loss 0.8503 (1.0902)	grad_norm 2.6796 (3.0412)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:48:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:02:55 lr 0.000016	 wd 0.0500	time 0.8181 (0.8709)	loss 1.1231 (1.0887)	grad_norm 2.8437 (3.0256)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:50:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:28 lr 0.000016	 wd 0.0500	time 0.8343 (0.8707)	loss 1.2204 (1.0899)	grad_norm 3.2661 (3.0136)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:51:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8213 (0.8705)	loss 0.7651 (1.0881)	grad_norm 2.0806 (3.0226)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:51:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 11 training takes 0:36:20
[2024-07-30 16:51:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.788 (12.788)	Loss 0.4578 (0.4578)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 16:52:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.654 Acc@5 98.002
[2024-07-30 16:52:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 16:52:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.75%
[2024-07-30 16:52:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:38:48 lr 0.000016	 wd 0.0500	time 12.4415 (12.4415)	loss 0.8441 (0.8441)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:53:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:39:06 lr 0.000016	 wd 0.0500	time 0.8576 (0.9767)	loss 1.2249 (1.0828)	grad_norm 2.0626 (3.1641)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:55:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:35:18 lr 0.000016	 wd 0.0500	time 0.8221 (0.9202)	loss 1.1300 (1.0826)	grad_norm 3.6121 (2.9357)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:56:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:33:05 lr 0.000016	 wd 0.0500	time 0.8575 (0.9019)	loss 0.8591 (1.0910)	grad_norm 3.6725 (2.8220)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:58:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:31:16 lr 0.000016	 wd 0.0500	time 0.8248 (0.8928)	loss 1.1519 (1.0862)	grad_norm 2.6357 (2.8428)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 16:59:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:29:36 lr 0.000016	 wd 0.0500	time 0.8098 (0.8872)	loss 1.3260 (1.0846)	grad_norm 2.4243 (2.9330)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 17:00:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:28:01 lr 0.000016	 wd 0.0500	time 0.8147 (0.8842)	loss 1.3529 (1.0821)	grad_norm 2.3356 (2.9036)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 17:02:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:26:28 lr 0.000016	 wd 0.0500	time 0.8231 (0.8815)	loss 1.1822 (1.0802)	grad_norm 30.7148 (3.0126)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 17:03:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:24:56 lr 0.000016	 wd 0.0500	time 0.8415 (0.8793)	loss 1.2870 (1.0815)	grad_norm 2.3839 (3.2068)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 17:05:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:23:26 lr 0.000016	 wd 0.0500	time 0.8252 (0.8778)	loss 1.3074 (1.0791)	grad_norm 2.1826 (3.2436)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 17:06:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:21:56 lr 0.000016	 wd 0.0500	time 0.8013 (0.8766)	loss 1.2246 (1.0828)	grad_norm 1.9663 (3.2053)	loss_scale 2048.0000 (1060.8272)	mem 23350MB
[2024-07-30 17:08:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:20:27 lr 0.000016	 wd 0.0500	time 0.8084 (0.8756)	loss 0.8536 (1.0850)	grad_norm 2.4772 (3.1859)	loss_scale 2048.0000 (1150.4886)	mem 23350MB
[2024-07-30 17:09:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:18:58 lr 0.000016	 wd 0.0500	time 0.8244 (0.8745)	loss 1.2252 (1.0842)	grad_norm 2.4808 (3.1687)	loss_scale 2048.0000 (1225.2190)	mem 23350MB
[2024-07-30 17:11:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:17:30 lr 0.000016	 wd 0.0500	time 0.8109 (0.8738)	loss 0.9841 (1.0857)	grad_norm 1.8602 (3.1673)	loss_scale 2048.0000 (1288.4612)	mem 23350MB
[2024-07-30 17:12:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:16:02 lr 0.000016	 wd 0.0500	time 0.8205 (0.8732)	loss 1.3041 (1.0866)	grad_norm 2.2751 (3.1983)	loss_scale 2048.0000 (1342.6752)	mem 23350MB
[2024-07-30 17:13:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:14:34 lr 0.000016	 wd 0.0500	time 0.8140 (0.8726)	loss 0.7711 (1.0835)	grad_norm 2.4257 (3.2245)	loss_scale 2048.0000 (1389.6656)	mem 23350MB
[2024-07-30 17:15:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:13:06 lr 0.000016	 wd 0.0500	time 0.8161 (0.8722)	loss 1.3545 (1.0811)	grad_norm 2.2100 (3.2214)	loss_scale 2048.0000 (1430.7858)	mem 23350MB
[2024-07-30 17:16:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:11:39 lr 0.000016	 wd 0.0500	time 0.8190 (0.8718)	loss 1.1434 (1.0836)	grad_norm 2.9246 (3.2163)	loss_scale 2048.0000 (1467.0711)	mem 23350MB
[2024-07-30 17:18:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:10:11 lr 0.000016	 wd 0.0500	time 0.8183 (0.8715)	loss 0.7535 (1.0824)	grad_norm 3.6698 (3.2266)	loss_scale 2048.0000 (1499.3270)	mem 23350MB
[2024-07-30 17:19:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:08:44 lr 0.000016	 wd 0.0500	time 0.8154 (0.8712)	loss 1.1394 (1.0829)	grad_norm 2.0702 (3.2267)	loss_scale 2048.0000 (1528.1894)	mem 23350MB
[2024-07-30 17:21:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:07:17 lr 0.000016	 wd 0.0500	time 0.8557 (0.8710)	loss 0.9420 (1.0839)	grad_norm 4.4082 (3.2509)	loss_scale 2048.0000 (1554.1669)	mem 23350MB
[2024-07-30 17:22:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:05:50 lr 0.000016	 wd 0.0500	time 0.8191 (0.8707)	loss 0.7418 (1.0829)	grad_norm 2.1723 (3.2372)	loss_scale 2048.0000 (1577.6716)	mem 23350MB
[2024-07-30 17:24:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:04:22 lr 0.000016	 wd 0.0500	time 0.8241 (0.8705)	loss 0.7253 (1.0844)	grad_norm 3.8163 (3.2281)	loss_scale 2048.0000 (1599.0404)	mem 23350MB
[2024-07-30 17:25:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:02:55 lr 0.000015	 wd 0.0500	time 0.8297 (0.8702)	loss 1.0287 (1.0847)	grad_norm 3.9913 (3.2112)	loss_scale 2048.0000 (1618.5519)	mem 23350MB
[2024-07-30 17:26:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:28 lr 0.000015	 wd 0.0500	time 0.8116 (0.8700)	loss 1.3294 (1.0853)	grad_norm 2.3847 (3.1927)	loss_scale 2048.0000 (1636.4382)	mem 23350MB
[2024-07-30 17:28:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.8865 (0.8698)	loss 0.7066 (1.0845)	grad_norm 5.4119 (3.2114)	loss_scale 2048.0000 (1652.8940)	mem 23350MB
[2024-07-30 17:28:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 12 training takes 0:36:18
[2024-07-30 17:28:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.052 (12.052)	Loss 0.4685 (0.4685)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 17:28:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.764 Acc@5 97.982
[2024-07-30 17:28:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 17:28:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.76%
[2024-07-30 17:28:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 17:29:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 17:29:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:33:56 lr 0.000015	 wd 0.0500	time 10.8858 (10.8858)	loss 1.2454 (1.2454)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 17:30:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:38:45 lr 0.000015	 wd 0.0500	time 0.8114 (0.9680)	loss 1.3285 (1.1432)	grad_norm 1.7925 (2.7110)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 17:32:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:35:10 lr 0.000015	 wd 0.0500	time 0.8154 (0.9168)	loss 1.3270 (1.1273)	grad_norm inf (inf)	loss_scale 1024.0000 (2037.8109)	mem 23350MB
[2024-07-30 17:33:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:33:01 lr 0.000015	 wd 0.0500	time 0.8156 (0.8997)	loss 1.3473 (1.1266)	grad_norm 3.6592 (inf)	loss_scale 1024.0000 (1700.9967)	mem 23350MB
[2024-07-30 17:34:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:31:12 lr 0.000015	 wd 0.0500	time 0.8219 (0.8910)	loss 1.3703 (1.1109)	grad_norm 3.8943 (inf)	loss_scale 1024.0000 (1532.1696)	mem 23350MB
[2024-07-30 17:36:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:29:34 lr 0.000015	 wd 0.0500	time 0.8823 (0.8862)	loss 1.2989 (1.1043)	grad_norm 3.7121 (inf)	loss_scale 1024.0000 (1430.7385)	mem 23350MB
[2024-07-30 17:37:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:27:58 lr 0.000015	 wd 0.0500	time 0.8055 (0.8827)	loss 1.1777 (1.0983)	grad_norm 2.1667 (inf)	loss_scale 1024.0000 (1363.0616)	mem 23350MB
[2024-07-30 17:39:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:26:26 lr 0.000015	 wd 0.0500	time 0.8089 (0.8806)	loss 1.0283 (1.0922)	grad_norm 3.3218 (inf)	loss_scale 1024.0000 (1314.6933)	mem 23350MB
[2024-07-30 17:40:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:24:55 lr 0.000015	 wd 0.0500	time 0.8146 (0.8787)	loss 1.3447 (1.0920)	grad_norm 2.2330 (inf)	loss_scale 1024.0000 (1278.4020)	mem 23350MB
[2024-07-30 17:42:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:23:25 lr 0.000015	 wd 0.0500	time 0.8139 (0.8775)	loss 1.0833 (1.0916)	grad_norm 2.7826 (inf)	loss_scale 1024.0000 (1250.1665)	mem 23350MB
[2024-07-30 17:43:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:21:56 lr 0.000015	 wd 0.0500	time 0.8234 (0.8766)	loss 1.2096 (1.0906)	grad_norm 3.1496 (inf)	loss_scale 1024.0000 (1227.5724)	mem 23350MB
[2024-07-30 17:45:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:20:27 lr 0.000015	 wd 0.0500	time 0.8207 (0.8755)	loss 1.2465 (1.0873)	grad_norm 3.5077 (inf)	loss_scale 1024.0000 (1209.0827)	mem 23350MB
[2024-07-30 17:46:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:18:59 lr 0.000015	 wd 0.0500	time 0.8228 (0.8749)	loss 1.1851 (1.0915)	grad_norm 3.0490 (inf)	loss_scale 1024.0000 (1193.6719)	mem 23350MB
[2024-07-30 17:47:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:17:30 lr 0.000015	 wd 0.0500	time 0.8242 (0.8741)	loss 0.8125 (1.0911)	grad_norm 2.4252 (inf)	loss_scale 1024.0000 (1180.6303)	mem 23350MB
[2024-07-30 17:49:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:16:02 lr 0.000015	 wd 0.0500	time 0.8067 (0.8733)	loss 1.3226 (1.0897)	grad_norm 3.9032 (inf)	loss_scale 1024.0000 (1169.4504)	mem 23350MB
[2024-07-30 17:50:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:14:34 lr 0.000015	 wd 0.0500	time 0.8336 (0.8729)	loss 1.1215 (1.0899)	grad_norm 3.0502 (inf)	loss_scale 1024.0000 (1159.7602)	mem 23350MB
[2024-07-30 17:52:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:13:07 lr 0.000015	 wd 0.0500	time 0.8186 (0.8726)	loss 0.9449 (1.0894)	grad_norm 2.0918 (inf)	loss_scale 1024.0000 (1151.2804)	mem 23350MB
[2024-07-30 17:53:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:11:39 lr 0.000015	 wd 0.0500	time 0.7750 (0.8722)	loss 1.0360 (1.0903)	grad_norm 2.4334 (inf)	loss_scale 1024.0000 (1143.7978)	mem 23350MB
[2024-07-30 17:55:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:10:11 lr 0.000015	 wd 0.0500	time 0.8554 (0.8717)	loss 1.3390 (1.0906)	grad_norm 2.2879 (inf)	loss_scale 1024.0000 (1137.1460)	mem 23350MB
[2024-07-30 17:56:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:08:44 lr 0.000015	 wd 0.0500	time 0.8115 (0.8716)	loss 1.0433 (1.0919)	grad_norm 2.2036 (inf)	loss_scale 1024.0000 (1131.1941)	mem 23350MB
[2024-07-30 17:58:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:07:17 lr 0.000015	 wd 0.0500	time 0.8158 (0.8714)	loss 1.2611 (1.0916)	grad_norm 2.3545 (inf)	loss_scale 1024.0000 (1125.8371)	mem 23350MB
[2024-07-30 17:59:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:05:50 lr 0.000014	 wd 0.0500	time 0.8137 (0.8711)	loss 1.3556 (1.0920)	grad_norm 2.6597 (inf)	loss_scale 1024.0000 (1120.9900)	mem 23350MB
[2024-07-30 18:00:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:04:22 lr 0.000014	 wd 0.0500	time 0.8090 (0.8707)	loss 1.1545 (1.0906)	grad_norm 2.5618 (inf)	loss_scale 1024.0000 (1116.5834)	mem 23350MB
[2024-07-30 18:02:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:02:55 lr 0.000014	 wd 0.0500	time 0.8199 (0.8704)	loss 1.2744 (1.0903)	grad_norm 2.2564 (nan)	loss_scale 512.0000 (1101.8792)	mem 23350MB
[2024-07-30 18:03:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:28 lr 0.000014	 wd 0.0500	time 0.8170 (0.8702)	loss 0.7843 (1.0930)	grad_norm 2.5938 (nan)	loss_scale 512.0000 (1077.3111)	mem 23350MB
[2024-07-30 18:05:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.7660 (0.8699)	loss 0.7863 (1.0913)	grad_norm 2.1837 (nan)	loss_scale 512.0000 (1054.7077)	mem 23350MB
[2024-07-30 18:05:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 13 training takes 0:36:18
[2024-07-30 18:05:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.536 (12.536)	Loss 0.4441 (0.4441)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 18:05:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.684 Acc@5 98.028
[2024-07-30 18:05:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 18:05:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.76%
[2024-07-30 18:06:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][0/2502]	eta 8:09:49 lr 0.000014	 wd 0.0500	time 11.7464 (11.7464)	loss 1.2458 (1.2458)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:07:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:38:51 lr 0.000014	 wd 0.0500	time 0.8234 (0.9707)	loss 0.9870 (1.0914)	grad_norm 2.6533 (4.1651)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:08:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:35:13 lr 0.000014	 wd 0.0500	time 0.8095 (0.9181)	loss 1.3002 (1.0978)	grad_norm 2.3747 (3.3511)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:10:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:33:03 lr 0.000014	 wd 0.0500	time 0.8217 (0.9010)	loss 0.8878 (1.0999)	grad_norm 4.4813 (3.1335)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:11:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:31:17 lr 0.000014	 wd 0.0500	time 0.8233 (0.8930)	loss 1.4928 (1.1012)	grad_norm 2.0642 (3.0021)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:13:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:29:38 lr 0.000014	 wd 0.0500	time 0.8122 (0.8881)	loss 0.8896 (1.0973)	grad_norm 2.8796 (2.9473)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:14:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:28:02 lr 0.000014	 wd 0.0500	time 0.8054 (0.8844)	loss 0.9346 (1.0946)	grad_norm 3.4360 (3.0210)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:16:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:26:28 lr 0.000014	 wd 0.0500	time 0.8244 (0.8815)	loss 1.1626 (1.0964)	grad_norm 2.5208 (3.0055)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:17:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:24:57 lr 0.000014	 wd 0.0500	time 0.8094 (0.8796)	loss 0.7446 (1.0920)	grad_norm 2.2452 (3.0033)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:19:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:23:27 lr 0.000014	 wd 0.0500	time 0.7746 (0.8784)	loss 0.7526 (1.0907)	grad_norm 3.0030 (2.9867)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:20:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:21:57 lr 0.000014	 wd 0.0500	time 0.8163 (0.8771)	loss 0.9423 (1.0890)	grad_norm 2.2481 (2.9693)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:21:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:20:28 lr 0.000014	 wd 0.0500	time 0.8208 (0.8760)	loss 1.3276 (1.0899)	grad_norm 2.4390 (2.9825)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:23:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:18:59 lr 0.000014	 wd 0.0500	time 0.8149 (0.8752)	loss 1.4542 (1.0899)	grad_norm 2.5674 (2.9696)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:24:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:17:31 lr 0.000014	 wd 0.0500	time 0.8141 (0.8744)	loss 1.1311 (1.0934)	grad_norm 2.2711 (2.9407)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:26:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:16:02 lr 0.000014	 wd 0.0500	time 0.8173 (0.8738)	loss 1.2466 (1.0940)	grad_norm 5.5580 (2.9583)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:27:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:14:35 lr 0.000014	 wd 0.0500	time 0.8220 (0.8733)	loss 1.3861 (1.0925)	grad_norm 2.4933 (2.9706)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:29:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:13:07 lr 0.000014	 wd 0.0500	time 0.8164 (0.8729)	loss 0.8576 (1.0916)	grad_norm 2.1735 (3.0087)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:30:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:11:39 lr 0.000014	 wd 0.0500	time 0.8182 (0.8723)	loss 0.7947 (1.0911)	grad_norm 2.9467 (3.0094)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:32:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:10:12 lr 0.000013	 wd 0.0500	time 0.8329 (0.8719)	loss 0.9909 (1.0922)	grad_norm 4.5376 (3.0050)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:33:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:08:44 lr 0.000013	 wd 0.0500	time 0.8835 (0.8717)	loss 1.2042 (1.0929)	grad_norm 2.4938 (3.0079)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:34:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:07:17 lr 0.000013	 wd 0.0500	time 0.8248 (0.8713)	loss 1.3395 (1.0915)	grad_norm 2.0865 (3.0084)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:36:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:05:50 lr 0.000013	 wd 0.0500	time 0.8148 (0.8711)	loss 1.1909 (1.0906)	grad_norm 2.0347 (3.0200)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:37:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:04:22 lr 0.000013	 wd 0.0500	time 0.8284 (0.8707)	loss 1.1841 (1.0899)	grad_norm 3.5215 (3.0053)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:39:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:02:55 lr 0.000013	 wd 0.0500	time 0.8208 (0.8704)	loss 0.8001 (1.0894)	grad_norm 2.5598 (2.9946)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:40:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:28 lr 0.000013	 wd 0.0500	time 0.8061 (0.8704)	loss 0.7272 (1.0888)	grad_norm 2.1450 (2.9859)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:42:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.7995 (0.8701)	loss 1.2375 (1.0882)	grad_norm 2.4822 (3.0071)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:42:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 14 training takes 0:36:19
[2024-07-30 18:42:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.166 (12.166)	Loss 0.4397 (0.4397)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-07-30 18:42:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.732 Acc@5 98.004
[2024-07-30 18:42:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 18:42:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.76%
[2024-07-30 18:42:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][0/2502]	eta 8:27:03 lr 0.000013	 wd 0.0500	time 12.1598 (12.1598)	loss 1.1893 (1.1893)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:44:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:39:00 lr 0.000013	 wd 0.0500	time 0.7582 (0.9743)	loss 0.7330 (1.0854)	grad_norm 4.2073 (3.2122)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:45:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:35:17 lr 0.000013	 wd 0.0500	time 0.8353 (0.9198)	loss 1.4441 (1.0751)	grad_norm 2.8333 (3.0756)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:47:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:33:04 lr 0.000013	 wd 0.0500	time 0.8152 (0.9010)	loss 0.8381 (1.0735)	grad_norm 2.6230 (2.9777)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:48:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:31:15 lr 0.000013	 wd 0.0500	time 0.8077 (0.8921)	loss 0.7117 (1.0722)	grad_norm 2.5030 (2.9913)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:50:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:29:36 lr 0.000013	 wd 0.0500	time 0.8043 (0.8871)	loss 0.6824 (1.0812)	grad_norm 2.9871 (3.0119)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:51:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:27:59 lr 0.000013	 wd 0.0500	time 0.8137 (0.8833)	loss 1.2001 (1.0862)	grad_norm 2.8822 (2.9861)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:53:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:26:27 lr 0.000013	 wd 0.0500	time 0.8354 (0.8810)	loss 1.2575 (1.0885)	grad_norm 2.3594 (3.0347)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:54:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:24:57 lr 0.000013	 wd 0.0500	time 0.8713 (0.8797)	loss 1.1207 (1.0894)	grad_norm 2.3175 (3.0113)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:55:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:23:26 lr 0.000013	 wd 0.0500	time 0.8023 (0.8780)	loss 1.2957 (1.0908)	grad_norm 2.4127 (3.0053)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:57:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:21:56 lr 0.000013	 wd 0.0500	time 0.8134 (0.8766)	loss 0.7422 (1.0906)	grad_norm 2.1609 (2.9957)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 18:58:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:20:27 lr 0.000013	 wd 0.0500	time 0.8155 (0.8756)	loss 0.8138 (1.0916)	grad_norm 2.2537 (2.9953)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 19:00:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:18:58 lr 0.000013	 wd 0.0500	time 0.8683 (0.8747)	loss 0.8408 (1.0921)	grad_norm 13.1000 (3.0170)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 19:01:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:17:30 lr 0.000013	 wd 0.0500	time 0.8238 (0.8739)	loss 0.8700 (1.0934)	grad_norm 2.4448 (2.9991)	loss_scale 1024.0000 (532.4643)	mem 23350MB
[2024-07-30 19:03:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:16:02 lr 0.000012	 wd 0.0500	time 0.8145 (0.8733)	loss 0.7395 (1.0932)	grad_norm 2.5915 (2.9850)	loss_scale 1024.0000 (567.5489)	mem 23350MB
[2024-07-30 19:04:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:14:34 lr 0.000012	 wd 0.0500	time 0.8326 (0.8728)	loss 1.0171 (1.0893)	grad_norm 3.1618 (3.0533)	loss_scale 1024.0000 (597.9587)	mem 23350MB
[2024-07-30 19:06:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:13:06 lr 0.000012	 wd 0.0500	time 0.7714 (0.8722)	loss 0.8794 (1.0904)	grad_norm 2.8888 (3.0522)	loss_scale 1024.0000 (624.5696)	mem 23350MB
[2024-07-30 19:07:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:11:39 lr 0.000012	 wd 0.0500	time 0.8174 (0.8717)	loss 0.7566 (1.0908)	grad_norm 2.8368 (3.0461)	loss_scale 1024.0000 (648.0517)	mem 23350MB
[2024-07-30 19:08:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:10:11 lr 0.000012	 wd 0.0500	time 0.8397 (0.8716)	loss 1.5752 (1.0894)	grad_norm 2.2694 (3.0276)	loss_scale 1024.0000 (668.9262)	mem 23350MB
[2024-07-30 19:10:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:08:44 lr 0.000012	 wd 0.0500	time 0.8118 (0.8712)	loss 0.8482 (1.0886)	grad_norm 2.7235 (3.0164)	loss_scale 1024.0000 (687.6044)	mem 23350MB
[2024-07-30 19:11:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:07:17 lr 0.000012	 wd 0.0500	time 0.8478 (0.8708)	loss 0.8596 (1.0897)	grad_norm 2.7354 (3.0071)	loss_scale 1024.0000 (704.4158)	mem 23350MB
[2024-07-30 19:13:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:05:49 lr 0.000012	 wd 0.0500	time 0.8768 (0.8705)	loss 0.7041 (1.0884)	grad_norm 4.4458 (3.0068)	loss_scale 1024.0000 (719.6268)	mem 23350MB
[2024-07-30 19:14:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:04:22 lr 0.000012	 wd 0.0500	time 0.8235 (0.8704)	loss 0.8383 (1.0878)	grad_norm 2.6738 (3.0130)	loss_scale 1024.0000 (733.4557)	mem 23350MB
[2024-07-30 19:16:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:02:55 lr 0.000012	 wd 0.0500	time 0.8090 (0.8702)	loss 0.9141 (1.0883)	grad_norm 3.1404 (3.0125)	loss_scale 1024.0000 (746.0826)	mem 23350MB
[2024-07-30 19:17:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:28 lr 0.000012	 wd 0.0500	time 0.8242 (0.8700)	loss 1.2700 (1.0886)	grad_norm 1.9407 (3.0472)	loss_scale 1024.0000 (757.6576)	mem 23350MB
[2024-07-30 19:19:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.8072 (0.8697)	loss 1.3795 (1.0895)	grad_norm 2.2856 (3.0631)	loss_scale 1024.0000 (768.3071)	mem 23350MB
[2024-07-30 19:19:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 15 training takes 0:36:18
[2024-07-30 19:19:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_15.pth saving......
[2024-07-30 19:19:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_15.pth saved !!!
[2024-07-30 19:19:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.335 (11.335)	Loss 0.4834 (0.4834)	Acc@1 93.359 (93.359)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-07-30 19:19:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.752 Acc@5 97.974
[2024-07-30 19:19:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 19:19:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.76%
[2024-07-30 19:19:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:35:49 lr 0.000012	 wd 0.0500	time 12.3699 (12.3699)	loss 1.1672 (1.1672)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:21:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:39:13 lr 0.000012	 wd 0.0500	time 0.8188 (0.9799)	loss 1.0122 (1.0975)	grad_norm 2.1033 (2.7666)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:22:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:35:27 lr 0.000012	 wd 0.0500	time 0.8188 (0.9243)	loss 1.1511 (1.0872)	grad_norm 4.8849 (3.0553)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:24:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:33:12 lr 0.000012	 wd 0.0500	time 0.7997 (0.9047)	loss 0.9804 (1.0803)	grad_norm 2.5586 (2.9682)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:25:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:31:21 lr 0.000012	 wd 0.0500	time 0.8086 (0.8952)	loss 0.7854 (1.0704)	grad_norm 2.4923 (2.8992)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:27:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:29:39 lr 0.000012	 wd 0.0500	time 0.8249 (0.8891)	loss 1.3440 (1.0745)	grad_norm 2.4818 (2.8578)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:28:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:28:02 lr 0.000012	 wd 0.0500	time 0.8134 (0.8845)	loss 1.3518 (1.0731)	grad_norm 2.1726 (2.9629)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:29:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:26:29 lr 0.000012	 wd 0.0500	time 0.8243 (0.8820)	loss 0.9599 (1.0751)	grad_norm 2.5462 (2.9362)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:31:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:24:57 lr 0.000012	 wd 0.0500	time 0.8561 (0.8798)	loss 1.2448 (1.0765)	grad_norm 3.6862 (2.9127)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:32:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:23:26 lr 0.000012	 wd 0.0500	time 0.8182 (0.8780)	loss 0.9989 (1.0749)	grad_norm 2.2647 (2.8997)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:34:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:21:56 lr 0.000011	 wd 0.0500	time 0.7650 (0.8767)	loss 1.2624 (1.0763)	grad_norm 1.8401 (2.9166)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:35:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:20:27 lr 0.000011	 wd 0.0500	time 0.8168 (0.8757)	loss 0.8707 (1.0814)	grad_norm 2.2279 (2.8931)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:37:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:18:59 lr 0.000011	 wd 0.0500	time 0.8050 (0.8750)	loss 0.8543 (1.0808)	grad_norm 2.6684 (2.9069)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:38:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:17:30 lr 0.000011	 wd 0.0500	time 0.8159 (0.8742)	loss 1.1814 (1.0839)	grad_norm 3.3058 (2.9323)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:40:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:16:02 lr 0.000011	 wd 0.0500	time 0.8363 (0.8735)	loss 1.1750 (1.0846)	grad_norm 2.3746 (2.9171)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:41:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:14:34 lr 0.000011	 wd 0.0500	time 0.8192 (0.8731)	loss 1.1503 (1.0865)	grad_norm 2.8615 (2.9165)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:42:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:13:06 lr 0.000011	 wd 0.0500	time 0.8283 (0.8725)	loss 1.1944 (1.0867)	grad_norm 2.6359 (2.9240)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:44:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:11:39 lr 0.000011	 wd 0.0500	time 0.8426 (0.8720)	loss 0.9806 (1.0863)	grad_norm 2.3577 (2.9276)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:45:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:10:11 lr 0.000011	 wd 0.0500	time 0.8262 (0.8717)	loss 0.8489 (1.0853)	grad_norm 2.1332 (2.9219)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:47:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:08:44 lr 0.000011	 wd 0.0500	time 0.8085 (0.8714)	loss 0.7628 (1.0861)	grad_norm 3.2986 (2.9267)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:48:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:07:17 lr 0.000011	 wd 0.0500	time 0.8164 (0.8711)	loss 1.2387 (1.0867)	grad_norm 2.0917 (2.9322)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:50:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:05:50 lr 0.000011	 wd 0.0500	time 0.8219 (0.8708)	loss 1.1916 (1.0875)	grad_norm 2.6190 (2.9344)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:51:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:04:22 lr 0.000011	 wd 0.0500	time 0.8130 (0.8707)	loss 0.8705 (1.0873)	grad_norm 2.3991 (2.9272)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:52:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:02:55 lr 0.000011	 wd 0.0500	time 0.8231 (0.8704)	loss 0.7562 (1.0855)	grad_norm 5.6815 (2.9346)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:54:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:28 lr 0.000011	 wd 0.0500	time 0.8095 (0.8702)	loss 1.1389 (1.0850)	grad_norm 1.9240 (2.9390)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:55:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.8027 (0.8699)	loss 0.7685 (1.0847)	grad_norm 3.4267 (2.9399)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:55:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 16 training takes 0:36:18
[2024-07-30 19:56:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.539 (12.539)	Loss 0.4619 (0.4619)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 19:56:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.756 Acc@5 97.978
[2024-07-30 19:56:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 19:56:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.76%
[2024-07-30 19:56:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:00:56 lr 0.000011	 wd 0.0500	time 11.5336 (11.5336)	loss 1.1799 (1.1799)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:58:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:39:00 lr 0.000011	 wd 0.0500	time 0.8262 (0.9745)	loss 1.1927 (1.0625)	grad_norm 2.2220 (2.5809)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 19:59:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:35:16 lr 0.000011	 wd 0.0500	time 0.8206 (0.9196)	loss 1.3120 (1.0667)	grad_norm 2.9491 (2.7271)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 20:00:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:33:04 lr 0.000011	 wd 0.0500	time 0.8143 (0.9013)	loss 1.3508 (1.0805)	grad_norm 3.7601 (2.6787)	loss_scale 2048.0000 (1214.5116)	mem 23350MB
[2024-07-30 20:02:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:31:14 lr 0.000011	 wd 0.0500	time 0.8099 (0.8916)	loss 1.4541 (1.0763)	grad_norm 2.7416 (2.7869)	loss_scale 2048.0000 (1422.3641)	mem 23350MB
[2024-07-30 20:03:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:29:34 lr 0.000010	 wd 0.0500	time 0.8239 (0.8863)	loss 1.3081 (1.0829)	grad_norm 2.6117 (2.8482)	loss_scale 2048.0000 (1547.2415)	mem 23350MB
[2024-07-30 20:05:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:28:01 lr 0.000010	 wd 0.0500	time 0.8203 (0.8840)	loss 0.8280 (1.0843)	grad_norm 2.3940 (2.7860)	loss_scale 2048.0000 (1630.5624)	mem 23350MB
[2024-07-30 20:06:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:26:27 lr 0.000010	 wd 0.0500	time 0.8068 (0.8811)	loss 1.2778 (1.0816)	grad_norm 2.1171 (2.7615)	loss_scale 2048.0000 (1690.1113)	mem 23350MB
[2024-07-30 20:08:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:24:56 lr 0.000010	 wd 0.0500	time 0.8443 (0.8790)	loss 1.1306 (1.0816)	grad_norm 2.8093 (2.7678)	loss_scale 2048.0000 (1734.7915)	mem 23350MB
[2024-07-30 20:09:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:23:26 lr 0.000010	 wd 0.0500	time 0.8142 (0.8778)	loss 1.2337 (1.0836)	grad_norm 2.2175 (2.7783)	loss_scale 2048.0000 (1769.5538)	mem 23350MB
[2024-07-30 20:11:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:21:56 lr 0.000010	 wd 0.0500	time 0.8817 (0.8767)	loss 0.9284 (1.0823)	grad_norm 2.5687 (2.7697)	loss_scale 2048.0000 (1797.3706)	mem 23350MB
[2024-07-30 20:12:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:20:27 lr 0.000010	 wd 0.0500	time 0.8246 (0.8756)	loss 0.9259 (1.0866)	grad_norm 2.6701 (2.7613)	loss_scale 2048.0000 (1820.1344)	mem 23350MB
[2024-07-30 20:13:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:18:58 lr 0.000010	 wd 0.0500	time 0.7658 (0.8748)	loss 0.7855 (1.0868)	grad_norm 2.6697 (2.7655)	loss_scale 2048.0000 (1839.1074)	mem 23350MB
[2024-07-30 20:15:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:17:30 lr 0.000010	 wd 0.0500	time 0.8171 (0.8741)	loss 1.3995 (1.0856)	grad_norm 2.1993 (2.7790)	loss_scale 2048.0000 (1855.1637)	mem 23350MB
[2024-07-30 20:16:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:16:02 lr 0.000010	 wd 0.0500	time 0.8350 (0.8736)	loss 1.3751 (1.0856)	grad_norm 2.2093 (2.7665)	loss_scale 2048.0000 (1868.9279)	mem 23350MB
[2024-07-30 20:18:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:14:34 lr 0.000010	 wd 0.0500	time 0.8062 (0.8733)	loss 1.2514 (1.0856)	grad_norm 3.2455 (2.8054)	loss_scale 2048.0000 (1880.8581)	mem 23350MB
[2024-07-30 20:19:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:13:07 lr 0.000010	 wd 0.0500	time 0.8191 (0.8730)	loss 1.2750 (1.0841)	grad_norm 2.2374 (2.8161)	loss_scale 2048.0000 (1891.2979)	mem 23350MB
[2024-07-30 20:21:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:11:39 lr 0.000010	 wd 0.0500	time 0.8397 (0.8727)	loss 1.4495 (1.0859)	grad_norm 2.6110 (2.8193)	loss_scale 2048.0000 (1900.5103)	mem 23350MB
[2024-07-30 20:22:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:10:12 lr 0.000010	 wd 0.0500	time 0.8197 (0.8723)	loss 0.9143 (1.0859)	grad_norm 8.6040 (2.8335)	loss_scale 2048.0000 (1908.6996)	mem 23350MB
[2024-07-30 20:24:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:08:44 lr 0.000010	 wd 0.0500	time 0.8366 (0.8720)	loss 1.1511 (1.0849)	grad_norm 2.8409 (2.8407)	loss_scale 2048.0000 (1916.0274)	mem 23350MB
[2024-07-30 20:25:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:07:17 lr 0.000010	 wd 0.0500	time 0.8210 (0.8716)	loss 0.9185 (1.0859)	grad_norm 4.0959 (2.8315)	loss_scale 2048.0000 (1922.6227)	mem 23350MB
[2024-07-30 20:26:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:05:50 lr 0.000010	 wd 0.0500	time 0.8209 (0.8714)	loss 1.3727 (1.0859)	grad_norm 2.7355 (2.8342)	loss_scale 2048.0000 (1928.5902)	mem 23350MB
[2024-07-30 20:28:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:04:23 lr 0.000010	 wd 0.0500	time 0.8270 (0.8712)	loss 1.3996 (1.0865)	grad_norm 2.3773 (2.8510)	loss_scale 2048.0000 (1934.0154)	mem 23350MB
[2024-07-30 20:29:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:02:55 lr 0.000010	 wd 0.0500	time 0.8154 (0.8709)	loss 0.7953 (1.0861)	grad_norm 2.2944 (2.8546)	loss_scale 2048.0000 (1938.9691)	mem 23350MB
[2024-07-30 20:31:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:28 lr 0.000010	 wd 0.0500	time 0.8095 (0.8707)	loss 0.7981 (1.0847)	grad_norm 2.2940 (2.8458)	loss_scale 2048.0000 (1943.5102)	mem 23350MB
[2024-07-30 20:32:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.7683 (0.8706)	loss 1.4269 (1.0849)	grad_norm 2.3653 (2.8586)	loss_scale 2048.0000 (1947.6881)	mem 23350MB
[2024-07-30 20:32:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 17 training takes 0:36:20
[2024-07-30 20:33:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.538 (11.538)	Loss 0.4395 (0.4395)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 20:33:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.768 Acc@5 97.998
[2024-07-30 20:33:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 20:33:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.77%
[2024-07-30 20:33:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 20:33:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 20:33:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:51:35 lr 0.000009	 wd 0.0500	time 11.3092 (11.3092)	loss 1.3987 (1.3987)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:35:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:38:52 lr 0.000009	 wd 0.0500	time 0.8046 (0.9712)	loss 1.4297 (1.0942)	grad_norm 1.9449 (2.7521)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:36:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:35:13 lr 0.000009	 wd 0.0500	time 0.8139 (0.9182)	loss 0.9142 (1.0969)	grad_norm 2.7478 (2.8213)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:37:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:33:04 lr 0.000009	 wd 0.0500	time 0.8259 (0.9012)	loss 1.5030 (1.0987)	grad_norm 2.8798 (2.8257)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:39:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:31:14 lr 0.000009	 wd 0.0500	time 0.8752 (0.8920)	loss 0.7102 (1.0978)	grad_norm 2.9925 (2.8538)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:40:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:29:35 lr 0.000009	 wd 0.0500	time 0.8220 (0.8868)	loss 1.3086 (1.0941)	grad_norm 2.4750 (2.8596)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:42:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:27:59 lr 0.000009	 wd 0.0500	time 0.8191 (0.8831)	loss 1.4443 (1.0888)	grad_norm 2.8753 (2.8432)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:43:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:26:26 lr 0.000009	 wd 0.0500	time 0.8169 (0.8805)	loss 1.3529 (1.0918)	grad_norm 4.5653 (2.8777)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:45:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:24:56 lr 0.000009	 wd 0.0500	time 0.8145 (0.8790)	loss 1.2997 (1.0954)	grad_norm 2.6836 (2.9829)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:46:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:23:26 lr 0.000009	 wd 0.0500	time 0.8914 (0.8778)	loss 0.9091 (1.0937)	grad_norm 1.9457 (2.9883)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:48:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:21:56 lr 0.000009	 wd 0.0500	time 0.8233 (0.8768)	loss 1.3388 (1.0919)	grad_norm 2.8237 (2.9480)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:49:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:20:27 lr 0.000009	 wd 0.0500	time 0.8072 (0.8758)	loss 0.9284 (1.0931)	grad_norm 3.6729 (2.9698)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:50:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:18:59 lr 0.000009	 wd 0.0500	time 0.8239 (0.8750)	loss 0.9383 (1.0879)	grad_norm 2.4257 (2.9653)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 20:52:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:17:30 lr 0.000009	 wd 0.0500	time 0.8076 (0.8741)	loss 1.4224 (1.0877)	grad_norm 2.8594 (nan)	loss_scale 1024.0000 (2008.6457)	mem 23350MB
[2024-07-30 20:53:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:16:02 lr 0.000009	 wd 0.0500	time 0.8095 (0.8734)	loss 1.1879 (1.0884)	grad_norm 2.4701 (nan)	loss_scale 1024.0000 (1938.3640)	mem 23350MB
[2024-07-30 20:55:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:14:34 lr 0.000009	 wd 0.0500	time 0.8191 (0.8729)	loss 1.2757 (1.0885)	grad_norm 1.9102 (nan)	loss_scale 1024.0000 (1877.4470)	mem 23350MB
[2024-07-30 20:56:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:13:06 lr 0.000009	 wd 0.0500	time 0.8213 (0.8723)	loss 1.2413 (1.0900)	grad_norm 2.7160 (nan)	loss_scale 1024.0000 (1824.1399)	mem 23350MB
[2024-07-30 20:58:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:11:39 lr 0.000009	 wd 0.0500	time 0.8097 (0.8720)	loss 1.2216 (1.0914)	grad_norm 2.4197 (nan)	loss_scale 1024.0000 (1777.1005)	mem 23350MB
[2024-07-30 20:59:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:10:11 lr 0.000009	 wd 0.0500	time 0.8096 (0.8717)	loss 0.7253 (1.0916)	grad_norm 2.2914 (nan)	loss_scale 1024.0000 (1735.2848)	mem 23350MB
[2024-07-30 21:01:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:08:44 lr 0.000009	 wd 0.0500	time 0.8244 (0.8715)	loss 1.4458 (1.0916)	grad_norm 1.8665 (nan)	loss_scale 1024.0000 (1697.8685)	mem 23350MB
[2024-07-30 21:02:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:07:17 lr 0.000008	 wd 0.0500	time 0.8246 (0.8712)	loss 0.9168 (1.0907)	grad_norm 2.4396 (nan)	loss_scale 1024.0000 (1664.1919)	mem 23350MB
[2024-07-30 21:03:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:05:50 lr 0.000008	 wd 0.0500	time 0.8292 (0.8709)	loss 1.2398 (1.0904)	grad_norm 3.5638 (nan)	loss_scale 1024.0000 (1633.7211)	mem 23350MB
[2024-07-30 21:05:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:04:22 lr 0.000008	 wd 0.0500	time 0.8226 (0.8706)	loss 1.0160 (1.0906)	grad_norm 3.7895 (nan)	loss_scale 1024.0000 (1606.0191)	mem 23350MB
[2024-07-30 21:06:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:02:55 lr 0.000008	 wd 0.0500	time 0.8007 (0.8704)	loss 1.2900 (1.0915)	grad_norm 2.1523 (nan)	loss_scale 1024.0000 (1580.7249)	mem 23350MB
[2024-07-30 21:08:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:28 lr 0.000008	 wd 0.0500	time 0.8260 (0.8703)	loss 1.3269 (1.0917)	grad_norm 2.2867 (nan)	loss_scale 1024.0000 (1557.5377)	mem 23350MB
[2024-07-30 21:09:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.7628 (0.8700)	loss 1.0761 (1.0919)	grad_norm 2.1475 (nan)	loss_scale 1024.0000 (1536.2047)	mem 23350MB
[2024-07-30 21:09:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 18 training takes 0:36:19
[2024-07-30 21:09:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.747 (12.747)	Loss 0.4553 (0.4553)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 21:10:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.794 Acc@5 97.992
[2024-07-30 21:10:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 21:10:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.79%
[2024-07-30 21:10:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 21:10:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 21:10:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:35:17 lr 0.000008	 wd 0.0500	time 10.9183 (10.9183)	loss 0.7718 (0.7718)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:11:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:38:40 lr 0.000008	 wd 0.0500	time 0.8221 (0.9660)	loss 1.4896 (1.1303)	grad_norm 2.1938 (2.8413)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:13:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:35:11 lr 0.000008	 wd 0.0500	time 0.8223 (0.9173)	loss 0.8287 (1.1015)	grad_norm 2.5195 (2.8171)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:14:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:33:04 lr 0.000008	 wd 0.0500	time 0.8217 (0.9013)	loss 1.3591 (1.0911)	grad_norm 2.5185 (2.9982)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:16:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:31:15 lr 0.000008	 wd 0.0500	time 0.8036 (0.8923)	loss 0.9820 (1.0824)	grad_norm 3.2651 (2.9426)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:17:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:29:34 lr 0.000008	 wd 0.0500	time 0.8197 (0.8863)	loss 1.3134 (1.0897)	grad_norm 2.2904 (2.8785)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:19:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:27:58 lr 0.000008	 wd 0.0500	time 0.8134 (0.8826)	loss 0.8439 (1.0826)	grad_norm 2.4683 (2.8419)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:20:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:26:25 lr 0.000008	 wd 0.0500	time 0.8223 (0.8797)	loss 1.0443 (1.0814)	grad_norm 2.6771 (2.8979)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:22:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:24:54 lr 0.000008	 wd 0.0500	time 0.8219 (0.8780)	loss 1.3217 (1.0826)	grad_norm 3.3887 (2.9852)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:23:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:23:24 lr 0.000008	 wd 0.0500	time 0.8154 (0.8766)	loss 1.1318 (1.0838)	grad_norm 2.4189 (2.9532)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:24:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:21:54 lr 0.000008	 wd 0.0500	time 0.7562 (0.8755)	loss 1.2993 (1.0841)	grad_norm 2.4101 (2.9893)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:26:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:20:26 lr 0.000008	 wd 0.0500	time 0.7562 (0.8747)	loss 1.0655 (1.0823)	grad_norm 2.5951 (2.9636)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:27:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:18:57 lr 0.000008	 wd 0.0500	time 0.8078 (0.8739)	loss 1.2808 (1.0805)	grad_norm 2.1074 (2.9334)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:29:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:17:29 lr 0.000008	 wd 0.0500	time 0.8147 (0.8733)	loss 0.8670 (1.0801)	grad_norm 3.6079 (2.9243)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:30:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:16:01 lr 0.000008	 wd 0.0500	time 0.7644 (0.8727)	loss 0.8435 (1.0800)	grad_norm 2.4914 (2.9261)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:32:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:14:33 lr 0.000008	 wd 0.0500	time 0.8614 (0.8719)	loss 1.3924 (1.0809)	grad_norm 2.6074 (2.9452)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:33:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:13:06 lr 0.000007	 wd 0.0500	time 0.8509 (0.8715)	loss 1.1517 (1.0824)	grad_norm 2.3000 (2.9436)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:35:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:11:38 lr 0.000007	 wd 0.0500	time 0.8155 (0.8711)	loss 0.7299 (1.0828)	grad_norm 2.1580 (2.9195)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:36:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:10:11 lr 0.000007	 wd 0.0500	time 0.8208 (0.8706)	loss 1.2032 (1.0846)	grad_norm 2.4480 (2.9429)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:37:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:08:43 lr 0.000007	 wd 0.0500	time 0.8181 (0.8703)	loss 1.0836 (1.0832)	grad_norm 2.5083 (2.9563)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:39:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:07:16 lr 0.000007	 wd 0.0500	time 0.8095 (0.8700)	loss 1.1948 (1.0841)	grad_norm 2.4742 (2.9549)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:40:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:05:49 lr 0.000007	 wd 0.0500	time 0.8087 (0.8699)	loss 1.5505 (1.0862)	grad_norm 2.1898 (2.9652)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:42:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:04:22 lr 0.000007	 wd 0.0500	time 0.8021 (0.8697)	loss 1.1920 (1.0855)	grad_norm 2.9444 (2.9639)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:43:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:02:55 lr 0.000007	 wd 0.0500	time 0.8591 (0.8695)	loss 0.9274 (1.0851)	grad_norm 3.9240 (2.9605)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:45:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:28 lr 0.000007	 wd 0.0500	time 0.7639 (0.8694)	loss 1.0392 (1.0844)	grad_norm 3.1289 (2.9471)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:46:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.8025 (0.8692)	loss 0.7609 (1.0844)	grad_norm 2.8932 (2.9318)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:46:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 19 training takes 0:36:17
[2024-07-30 21:46:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.305 (12.305)	Loss 0.4404 (0.4404)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 21:47:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.770 Acc@5 98.036
[2024-07-30 21:47:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 21:47:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.79%
[2024-07-30 21:47:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:55:59 lr 0.000007	 wd 0.0500	time 11.4145 (11.4145)	loss 1.2811 (1.2811)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:48:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:38:53 lr 0.000007	 wd 0.0500	time 0.8236 (0.9715)	loss 0.8812 (1.0925)	grad_norm 2.9643 (2.9510)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:50:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:35:14 lr 0.000007	 wd 0.0500	time 0.8070 (0.9186)	loss 1.2389 (1.0808)	grad_norm 2.7121 (2.9511)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-30 21:51:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:33:03 lr 0.000007	 wd 0.0500	time 0.8136 (0.9008)	loss 0.6945 (1.0904)	grad_norm 2.3158 (2.8175)	loss_scale 2048.0000 (1207.7076)	mem 23350MB
[2024-07-30 21:53:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:31:15 lr 0.000007	 wd 0.0500	time 0.8164 (0.8923)	loss 1.2004 (1.0879)	grad_norm 2.0873 (2.8968)	loss_scale 2048.0000 (1417.2569)	mem 23350MB
[2024-07-30 21:54:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:29:35 lr 0.000007	 wd 0.0500	time 0.8208 (0.8868)	loss 1.2306 (1.0871)	grad_norm 2.8222 (2.8455)	loss_scale 2048.0000 (1543.1537)	mem 23350MB
[2024-07-30 21:56:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:27:59 lr 0.000007	 wd 0.0500	time 0.8139 (0.8833)	loss 1.3143 (1.0909)	grad_norm 2.6156 (2.8446)	loss_scale 2048.0000 (1627.1547)	mem 23350MB
[2024-07-30 21:57:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:26:27 lr 0.000007	 wd 0.0500	time 0.8025 (0.8811)	loss 1.2090 (1.0892)	grad_norm 2.3607 (2.9371)	loss_scale 2048.0000 (1687.1897)	mem 23350MB
[2024-07-30 21:58:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:24:56 lr 0.000007	 wd 0.0500	time 0.8186 (0.8794)	loss 1.1276 (1.0909)	grad_norm 2.2806 (3.0066)	loss_scale 2048.0000 (1732.2347)	mem 23350MB
[2024-07-30 22:00:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:23:26 lr 0.000007	 wd 0.0500	time 0.8106 (0.8780)	loss 1.2851 (1.0893)	grad_norm 2.6673 (2.9854)	loss_scale 2048.0000 (1767.2808)	mem 23350MB
[2024-07-30 22:01:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:21:56 lr 0.000007	 wd 0.0500	time 0.8061 (0.8768)	loss 1.1756 (1.0907)	grad_norm 5.2570 (2.9844)	loss_scale 2048.0000 (1795.3247)	mem 23350MB
[2024-07-30 22:03:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:20:27 lr 0.000007	 wd 0.0500	time 0.8154 (0.8758)	loss 0.8594 (1.0887)	grad_norm 2.3463 (2.9651)	loss_scale 2048.0000 (1818.2743)	mem 23350MB
[2024-07-30 22:04:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:18:59 lr 0.000006	 wd 0.0500	time 0.8447 (0.8749)	loss 1.1622 (1.0858)	grad_norm 4.0833 (2.9752)	loss_scale 2048.0000 (1837.4022)	mem 23350MB
[2024-07-30 22:06:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:17:30 lr 0.000006	 wd 0.0500	time 0.8150 (0.8742)	loss 1.3243 (1.0858)	grad_norm 2.4681 (3.0437)	loss_scale 2048.0000 (1853.5895)	mem 23350MB
[2024-07-30 22:07:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:16:02 lr 0.000006	 wd 0.0500	time 0.8216 (0.8735)	loss 1.0061 (1.0879)	grad_norm 3.3460 (3.0279)	loss_scale 2048.0000 (1867.4661)	mem 23350MB
[2024-07-30 22:08:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:14:34 lr 0.000006	 wd 0.0500	time 0.8226 (0.8730)	loss 1.3196 (1.0893)	grad_norm 4.5726 (3.0128)	loss_scale 2048.0000 (1879.4937)	mem 23350MB
[2024-07-30 22:10:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:13:06 lr 0.000006	 wd 0.0500	time 0.7829 (0.8725)	loss 1.1483 (1.0911)	grad_norm 2.5184 (2.9961)	loss_scale 2048.0000 (1890.0187)	mem 23350MB
[2024-07-30 22:11:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:11:39 lr 0.000006	 wd 0.0500	time 0.8461 (0.8721)	loss 1.4527 (1.0917)	grad_norm 2.2936 (2.9836)	loss_scale 2048.0000 (1899.3063)	mem 23350MB
[2024-07-30 22:13:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:10:11 lr 0.000006	 wd 0.0500	time 0.8161 (0.8718)	loss 0.7647 (1.0898)	grad_norm 2.3214 (2.9666)	loss_scale 2048.0000 (1907.5625)	mem 23350MB
[2024-07-30 22:14:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:08:44 lr 0.000006	 wd 0.0500	time 0.8220 (0.8714)	loss 0.7480 (1.0902)	grad_norm 2.3455 (2.9529)	loss_scale 2048.0000 (1914.9500)	mem 23350MB
[2024-07-30 22:16:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:07:17 lr 0.000006	 wd 0.0500	time 0.8186 (0.8710)	loss 1.0310 (1.0897)	grad_norm 2.3866 (2.9483)	loss_scale 2048.0000 (1921.5992)	mem 23350MB
[2024-07-30 22:17:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:05:50 lr 0.000006	 wd 0.0500	time 0.8121 (0.8707)	loss 1.2324 (1.0901)	grad_norm 2.1062 (2.9557)	loss_scale 2048.0000 (1927.6154)	mem 23350MB
[2024-07-30 22:19:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:04:22 lr 0.000006	 wd 0.0500	time 0.8107 (0.8706)	loss 1.2819 (1.0917)	grad_norm 2.1409 (2.9501)	loss_scale 2048.0000 (1933.0850)	mem 23350MB
[2024-07-30 22:20:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:02:55 lr 0.000006	 wd 0.0500	time 0.8034 (0.8703)	loss 0.7883 (1.0931)	grad_norm 2.3224 (2.9386)	loss_scale 2048.0000 (1938.0791)	mem 23350MB
[2024-07-30 22:21:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:28 lr 0.000006	 wd 0.0500	time 0.8072 (0.8700)	loss 0.6873 (1.0927)	grad_norm 3.1276 (2.9231)	loss_scale 2048.0000 (1942.6572)	mem 23350MB
[2024-07-30 22:23:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.8051 (0.8698)	loss 1.0727 (1.0945)	grad_norm 2.6480 (2.9156)	loss_scale 2048.0000 (1946.8693)	mem 23350MB
[2024-07-30 22:23:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 20 training takes 0:36:19
[2024-07-30 22:23:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.110 (12.110)	Loss 0.4524 (0.4524)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-07-30 22:24:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.744 Acc@5 98.034
[2024-07-30 22:24:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 22:24:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.79%
[2024-07-30 22:24:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:40:54 lr 0.000006	 wd 0.0500	time 12.4917 (12.4917)	loss 0.8601 (0.8601)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:25:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:39:14 lr 0.000006	 wd 0.0500	time 0.8153 (0.9804)	loss 1.2673 (1.0958)	grad_norm 2.3483 (3.7365)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:27:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:35:28 lr 0.000006	 wd 0.0500	time 0.8177 (0.9247)	loss 1.2396 (1.0985)	grad_norm 2.8143 (3.5017)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:28:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:33:12 lr 0.000006	 wd 0.0500	time 0.8267 (0.9049)	loss 1.0592 (1.0934)	grad_norm 5.6849 (3.2533)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:30:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:31:21 lr 0.000006	 wd 0.0500	time 0.8436 (0.8950)	loss 1.3867 (1.0845)	grad_norm 3.3923 (3.1782)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:31:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:29:39 lr 0.000006	 wd 0.0500	time 0.8328 (0.8890)	loss 0.7600 (1.0804)	grad_norm 2.8445 (3.1001)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:32:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:28:03 lr 0.000006	 wd 0.0500	time 0.8650 (0.8850)	loss 1.2328 (1.0764)	grad_norm 3.3202 (3.0462)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:34:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:26:29 lr 0.000006	 wd 0.0500	time 0.8206 (0.8820)	loss 1.1868 (1.0783)	grad_norm 2.4113 (3.0398)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-30 22:35:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:24:57 lr 0.000006	 wd 0.0500	time 0.8079 (0.8796)	loss 1.2311 (1.0774)	grad_norm inf (inf)	loss_scale 1024.0000 (2045.4432)	mem 23350MB
[2024-07-30 22:37:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:23:26 lr 0.000005	 wd 0.0500	time 0.8133 (0.8778)	loss 1.2828 (1.0811)	grad_norm 2.6787 (inf)	loss_scale 1024.0000 (1932.0755)	mem 23350MB
[2024-07-30 22:38:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:21:56 lr 0.000005	 wd 0.0500	time 0.8048 (0.8765)	loss 1.2960 (1.0844)	grad_norm 2.1067 (inf)	loss_scale 1024.0000 (1841.3586)	mem 23350MB
[2024-07-30 22:40:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:20:27 lr 0.000005	 wd 0.0500	time 0.8755 (0.8756)	loss 0.7951 (1.0880)	grad_norm 2.6313 (nan)	loss_scale 512.0000 (1730.8483)	mem 23350MB
[2024-07-30 22:41:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:18:59 lr 0.000005	 wd 0.0500	time 0.8605 (0.8750)	loss 1.2333 (1.0846)	grad_norm 2.5365 (nan)	loss_scale 512.0000 (1629.3622)	mem 23350MB
[2024-07-30 22:42:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:17:30 lr 0.000005	 wd 0.0500	time 0.8033 (0.8742)	loss 1.2219 (1.0847)	grad_norm 1.8550 (nan)	loss_scale 512.0000 (1543.4773)	mem 23350MB
[2024-07-30 22:44:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:16:02 lr 0.000005	 wd 0.0500	time 0.8273 (0.8736)	loss 0.7661 (1.0827)	grad_norm 2.9235 (nan)	loss_scale 512.0000 (1469.8530)	mem 23350MB
[2024-07-30 22:45:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:14:34 lr 0.000005	 wd 0.0500	time 0.8237 (0.8730)	loss 0.7605 (1.0841)	grad_norm 2.0071 (nan)	loss_scale 512.0000 (1406.0386)	mem 23350MB
[2024-07-30 22:47:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:13:07 lr 0.000005	 wd 0.0500	time 0.8062 (0.8726)	loss 1.0841 (1.0809)	grad_norm 6.8841 (nan)	loss_scale 512.0000 (1350.1961)	mem 23350MB
[2024-07-30 22:48:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:11:39 lr 0.000005	 wd 0.0500	time 0.8247 (0.8721)	loss 1.2934 (1.0809)	grad_norm 2.1735 (nan)	loss_scale 512.0000 (1300.9195)	mem 23350MB
[2024-07-30 22:50:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:10:11 lr 0.000005	 wd 0.0500	time 0.8035 (0.8716)	loss 0.7772 (1.0811)	grad_norm 2.9205 (nan)	loss_scale 512.0000 (1257.1149)	mem 23350MB
[2024-07-30 22:51:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:08:44 lr 0.000005	 wd 0.0500	time 0.8824 (0.8713)	loss 0.6976 (1.0824)	grad_norm 7.2250 (nan)	loss_scale 512.0000 (1217.9190)	mem 23350MB
[2024-07-30 22:53:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:07:17 lr 0.000005	 wd 0.0500	time 0.8257 (0.8710)	loss 1.0604 (1.0820)	grad_norm 2.7757 (nan)	loss_scale 512.0000 (1182.6407)	mem 23350MB
[2024-07-30 22:54:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:05:50 lr 0.000005	 wd 0.0500	time 0.8289 (0.8708)	loss 1.1768 (1.0816)	grad_norm 2.7235 (nan)	loss_scale 512.0000 (1150.7206)	mem 23350MB
[2024-07-30 22:55:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:04:22 lr 0.000005	 wd 0.0500	time 0.8023 (0.8706)	loss 1.3292 (1.0799)	grad_norm 2.5220 (nan)	loss_scale 512.0000 (1121.7010)	mem 23350MB
[2024-07-30 22:57:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:02:55 lr 0.000005	 wd 0.0500	time 0.8172 (0.8702)	loss 1.3413 (1.0820)	grad_norm 2.8246 (nan)	loss_scale 512.0000 (1095.2038)	mem 23350MB
[2024-07-30 22:58:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:28 lr 0.000005	 wd 0.0500	time 0.8700 (0.8700)	loss 1.0497 (1.0829)	grad_norm 2.2998 (nan)	loss_scale 512.0000 (1070.9138)	mem 23350MB
[2024-07-30 23:00:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.8055 (0.8698)	loss 1.3618 (1.0824)	grad_norm 2.2784 (nan)	loss_scale 512.0000 (1048.5662)	mem 23350MB
[2024-07-30 23:00:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 21 training takes 0:36:20
[2024-07-30 23:00:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.148 (12.148)	Loss 0.4600 (0.4600)	Acc@1 92.969 (92.969)	Acc@5 99.219 (99.219)	Mem 23350MB
[2024-07-30 23:00:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.798 Acc@5 98.036
[2024-07-30 23:00:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 23:00:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.80%
[2024-07-30 23:00:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-30 23:00:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-30 23:01:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:35:01 lr 0.000005	 wd 0.0500	time 10.9118 (10.9118)	loss 0.9721 (0.9721)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:02:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:38:36 lr 0.000005	 wd 0.0500	time 0.8122 (0.9643)	loss 1.2549 (1.0922)	grad_norm 2.4300 (3.0291)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:04:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:35:07 lr 0.000005	 wd 0.0500	time 0.8215 (0.9154)	loss 0.6431 (1.1031)	grad_norm 2.5103 (2.9973)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:05:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:32:59 lr 0.000005	 wd 0.0500	time 0.8225 (0.8988)	loss 1.4496 (1.1046)	grad_norm 2.9700 (2.9997)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:06:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:31:12 lr 0.000005	 wd 0.0500	time 0.8069 (0.8910)	loss 0.7351 (1.1051)	grad_norm 2.3200 (3.0435)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:08:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:29:34 lr 0.000005	 wd 0.0500	time 0.8227 (0.8863)	loss 1.0379 (1.1013)	grad_norm 3.0049 (3.0553)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:09:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:28:00 lr 0.000005	 wd 0.0500	time 0.8628 (0.8833)	loss 0.8114 (1.1005)	grad_norm 1.8816 (3.0526)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:11:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:26:27 lr 0.000005	 wd 0.0500	time 0.8152 (0.8809)	loss 1.3506 (1.0945)	grad_norm 2.3897 (2.9805)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:12:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:24:55 lr 0.000004	 wd 0.0500	time 0.8150 (0.8787)	loss 0.9736 (1.0944)	grad_norm 2.0425 (2.9770)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:14:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:23:25 lr 0.000004	 wd 0.0500	time 0.8147 (0.8773)	loss 1.0930 (1.0947)	grad_norm 2.1831 (3.0157)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:15:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:21:55 lr 0.000004	 wd 0.0500	time 0.8187 (0.8760)	loss 1.2810 (1.0951)	grad_norm 1.9179 (3.0262)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:17:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:20:26 lr 0.000004	 wd 0.0500	time 0.8144 (0.8750)	loss 0.7618 (1.0912)	grad_norm 2.0623 (3.0881)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:18:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:18:58 lr 0.000004	 wd 0.0500	time 0.8314 (0.8742)	loss 1.2507 (1.0919)	grad_norm 2.3046 (3.0610)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:19:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:17:29 lr 0.000004	 wd 0.0500	time 0.8601 (0.8734)	loss 0.7450 (1.0862)	grad_norm 2.9584 (3.0209)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:21:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:16:01 lr 0.000004	 wd 0.0500	time 0.8166 (0.8727)	loss 0.9776 (1.0816)	grad_norm 2.4257 (3.0152)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:22:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:14:34 lr 0.000004	 wd 0.0500	time 0.8210 (0.8724)	loss 1.0687 (1.0813)	grad_norm 2.9357 (2.9880)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:24:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:13:06 lr 0.000004	 wd 0.0500	time 0.8616 (0.8719)	loss 1.2801 (1.0808)	grad_norm 2.0155 (2.9747)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:25:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:11:38 lr 0.000004	 wd 0.0500	time 0.8055 (0.8715)	loss 0.8849 (1.0789)	grad_norm 2.5120 (2.9698)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:27:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:10:11 lr 0.000004	 wd 0.0500	time 0.8056 (0.8712)	loss 1.1167 (1.0776)	grad_norm 2.3974 (2.9583)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:28:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:08:44 lr 0.000004	 wd 0.0500	time 0.8345 (0.8708)	loss 0.9017 (1.0784)	grad_norm 2.2115 (2.9477)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:30:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:07:16 lr 0.000004	 wd 0.0500	time 0.8217 (0.8705)	loss 0.8669 (1.0797)	grad_norm 2.5907 (2.9473)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:31:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:05:49 lr 0.000004	 wd 0.0500	time 0.8212 (0.8703)	loss 1.1899 (1.0807)	grad_norm 2.5723 (2.9335)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:32:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:04:22 lr 0.000004	 wd 0.0500	time 0.8204 (0.8700)	loss 1.0430 (1.0805)	grad_norm 2.2834 (2.9292)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:34:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:02:55 lr 0.000004	 wd 0.0500	time 0.8208 (0.8699)	loss 1.1415 (1.0811)	grad_norm 2.8221 (2.9520)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:35:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:28 lr 0.000004	 wd 0.0500	time 0.8873 (0.8698)	loss 1.3578 (1.0827)	grad_norm 2.2868 (2.9437)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:37:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.8366 (0.8696)	loss 0.8482 (1.0829)	grad_norm 3.3676 (2.9344)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:37:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 22 training takes 0:36:20
[2024-07-30 23:37:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.334 (12.334)	Loss 0.4473 (0.4473)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-30 23:37:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.758 Acc@5 98.042
[2024-07-30 23:37:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-30 23:37:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.80%
[2024-07-30 23:38:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:23:36 lr 0.000004	 wd 0.0500	time 12.0769 (12.0769)	loss 0.7605 (0.7605)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-07-30 23:39:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:39:01 lr 0.000004	 wd 0.0500	time 0.8059 (0.9746)	loss 1.2503 (1.0678)	grad_norm 4.4577 (2.7833)	loss_scale 1024.0000 (927.6832)	mem 23350MB
[2024-07-30 23:41:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:35:16 lr 0.000004	 wd 0.0500	time 0.8100 (0.9194)	loss 0.8765 (1.0767)	grad_norm 4.6445 (2.8823)	loss_scale 1024.0000 (975.6020)	mem 23350MB
[2024-07-30 23:42:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:33:04 lr 0.000004	 wd 0.0500	time 0.8034 (0.9013)	loss 1.2785 (1.0672)	grad_norm 2.2931 (2.8925)	loss_scale 1024.0000 (991.6811)	mem 23350MB
[2024-07-30 23:43:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:31:15 lr 0.000004	 wd 0.0500	time 0.8228 (0.8923)	loss 0.8427 (1.0741)	grad_norm 2.6706 (2.9004)	loss_scale 1024.0000 (999.7406)	mem 23350MB
[2024-07-30 23:45:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:29:35 lr 0.000004	 wd 0.0500	time 0.8078 (0.8870)	loss 0.8465 (1.0745)	grad_norm 3.0094 (2.9128)	loss_scale 1024.0000 (1004.5828)	mem 23350MB
[2024-07-30 23:46:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:27:59 lr 0.000004	 wd 0.0500	time 0.8083 (0.8833)	loss 1.2660 (1.0740)	grad_norm 2.7138 (3.0048)	loss_scale 1024.0000 (1007.8136)	mem 23350MB
[2024-07-30 23:48:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:26:26 lr 0.000004	 wd 0.0500	time 0.8208 (0.8806)	loss 1.1915 (1.0766)	grad_norm 12.6706 (3.0305)	loss_scale 1024.0000 (1010.1227)	mem 23350MB
[2024-07-30 23:49:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:24:55 lr 0.000003	 wd 0.0500	time 0.8211 (0.8789)	loss 1.2866 (1.0794)	grad_norm 3.3307 (2.9844)	loss_scale 1024.0000 (1011.8552)	mem 23350MB
[2024-07-30 23:51:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:23:25 lr 0.000003	 wd 0.0500	time 0.8043 (0.8776)	loss 1.0697 (1.0804)	grad_norm 1.9623 (2.9658)	loss_scale 1024.0000 (1013.2031)	mem 23350MB
[2024-07-30 23:52:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:21:56 lr 0.000003	 wd 0.0500	time 0.8005 (0.8763)	loss 1.1156 (1.0766)	grad_norm 1.8204 (2.9790)	loss_scale 1024.0000 (1014.2817)	mem 23350MB
[2024-07-30 23:53:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:20:26 lr 0.000003	 wd 0.0500	time 0.8785 (0.8752)	loss 0.9630 (1.0787)	grad_norm 2.3293 (2.9847)	loss_scale 1024.0000 (1015.1644)	mem 23350MB
[2024-07-30 23:55:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:18:58 lr 0.000003	 wd 0.0500	time 0.8059 (0.8743)	loss 1.2724 (1.0812)	grad_norm 4.7981 (2.9666)	loss_scale 1024.0000 (1015.9001)	mem 23350MB
[2024-07-30 23:56:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:17:30 lr 0.000003	 wd 0.0500	time 0.7628 (0.8737)	loss 1.1579 (1.0824)	grad_norm 2.9540 (2.9513)	loss_scale 1024.0000 (1016.5227)	mem 23350MB
[2024-07-30 23:58:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:16:02 lr 0.000003	 wd 0.0500	time 0.8059 (0.8730)	loss 1.2974 (1.0823)	grad_norm 2.3625 (2.9953)	loss_scale 1024.0000 (1017.0564)	mem 23350MB
[2024-07-30 23:59:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:14:34 lr 0.000003	 wd 0.0500	time 0.8226 (0.8725)	loss 1.2762 (1.0808)	grad_norm 2.5023 (3.0823)	loss_scale 1024.0000 (1017.5190)	mem 23350MB
[2024-07-31 00:01:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:13:06 lr 0.000003	 wd 0.0500	time 0.8235 (0.8721)	loss 1.0991 (1.0834)	grad_norm 2.5972 (3.0467)	loss_scale 1024.0000 (1017.9238)	mem 23350MB
[2024-07-31 00:02:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:11:39 lr 0.000003	 wd 0.0500	time 0.8236 (0.8717)	loss 0.6907 (1.0810)	grad_norm 2.2298 (3.0261)	loss_scale 1024.0000 (1018.2810)	mem 23350MB
[2024-07-31 00:04:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:10:11 lr 0.000003	 wd 0.0500	time 0.7994 (0.8713)	loss 1.3050 (1.0805)	grad_norm 11.5039 (3.0194)	loss_scale 1024.0000 (1018.5986)	mem 23350MB
[2024-07-31 00:05:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:08:44 lr 0.000003	 wd 0.0500	time 0.8167 (0.8710)	loss 1.2933 (1.0799)	grad_norm 2.6111 (3.0295)	loss_scale 1024.0000 (1018.8827)	mem 23350MB
[2024-07-31 00:06:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:07:17 lr 0.000003	 wd 0.0500	time 0.7979 (0.8707)	loss 0.7913 (1.0789)	grad_norm 4.4718 (3.0251)	loss_scale 1024.0000 (1019.1384)	mem 23350MB
[2024-07-31 00:08:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:05:49 lr 0.000003	 wd 0.0500	time 0.8327 (0.8705)	loss 1.2162 (1.0798)	grad_norm 2.6592 (3.0146)	loss_scale 1024.0000 (1019.3698)	mem 23350MB
[2024-07-31 00:09:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:04:22 lr 0.000003	 wd 0.0500	time 0.8141 (0.8702)	loss 1.2506 (1.0787)	grad_norm 2.3717 (3.0018)	loss_scale 1024.0000 (1019.5802)	mem 23350MB
[2024-07-31 00:11:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:02:55 lr 0.000003	 wd 0.0500	time 0.8138 (0.8700)	loss 1.3896 (1.0782)	grad_norm 1.9519 (2.9985)	loss_scale 1024.0000 (1019.7723)	mem 23350MB
[2024-07-31 00:12:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:28 lr 0.000003	 wd 0.0500	time 0.7998 (0.8698)	loss 0.9071 (1.0780)	grad_norm 2.5208 (2.9928)	loss_scale 1024.0000 (1019.9484)	mem 23350MB
[2024-07-31 00:14:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.8109 (0.8695)	loss 0.8197 (1.0769)	grad_norm 2.6434 (2.9958)	loss_scale 1024.0000 (1020.1104)	mem 23350MB
[2024-07-31 00:14:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 23 training takes 0:36:20
[2024-07-31 00:14:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.251 (12.251)	Loss 0.4653 (0.4653)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-31 00:14:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.830 Acc@5 98.002
[2024-07-31 00:14:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-31 00:14:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.83%
[2024-07-31 00:14:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-31 00:14:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-31 00:15:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:56:18 lr 0.000003	 wd 0.0500	time 11.4225 (11.4225)	loss 1.2023 (1.2023)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:16:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:38:51 lr 0.000003	 wd 0.0500	time 0.8244 (0.9707)	loss 0.8969 (1.0912)	grad_norm 2.3551 (3.2223)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:18:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:35:16 lr 0.000003	 wd 0.0500	time 0.8227 (0.9196)	loss 1.1168 (1.0822)	grad_norm 2.1420 (2.9825)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:19:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:33:08 lr 0.000003	 wd 0.0500	time 0.8163 (0.9029)	loss 1.2783 (1.0934)	grad_norm 2.4657 (2.9775)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:20:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:31:18 lr 0.000003	 wd 0.0500	time 0.8305 (0.8935)	loss 0.6555 (1.0912)	grad_norm 2.5048 (3.1226)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:22:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:29:37 lr 0.000003	 wd 0.0500	time 0.8480 (0.8879)	loss 1.2190 (1.0920)	grad_norm 2.2697 (3.0369)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:23:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:28:01 lr 0.000003	 wd 0.0500	time 0.8218 (0.8840)	loss 0.7960 (1.0884)	grad_norm 2.5060 (2.9942)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:25:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:26:28 lr 0.000003	 wd 0.0500	time 0.8195 (0.8815)	loss 1.2384 (1.0869)	grad_norm 2.2001 (2.9527)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:26:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:24:56 lr 0.000003	 wd 0.0500	time 0.8041 (0.8795)	loss 1.2118 (1.0877)	grad_norm 2.2203 (2.9007)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:28:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:23:26 lr 0.000003	 wd 0.0500	time 0.8102 (0.8780)	loss 0.8697 (1.0922)	grad_norm 2.5552 (2.8999)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:29:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:21:56 lr 0.000003	 wd 0.0500	time 0.8075 (0.8768)	loss 0.8056 (1.0895)	grad_norm 2.2145 (2.9582)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:31:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:20:28 lr 0.000003	 wd 0.0500	time 0.8155 (0.8761)	loss 0.8198 (1.0880)	grad_norm 2.6716 (2.9618)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:32:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:18:59 lr 0.000002	 wd 0.0500	time 0.8209 (0.8752)	loss 0.9148 (1.0856)	grad_norm 2.7417 (2.9916)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:33:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:17:31 lr 0.000002	 wd 0.0500	time 0.8000 (0.8746)	loss 0.8190 (1.0835)	grad_norm 2.4809 (2.9779)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:35:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:16:02 lr 0.000002	 wd 0.0500	time 0.8165 (0.8738)	loss 1.4003 (1.0847)	grad_norm 2.6666 (2.9673)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:36:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:14:35 lr 0.000002	 wd 0.0500	time 0.8162 (0.8735)	loss 1.1852 (1.0813)	grad_norm 2.6400 (2.9609)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 00:38:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:13:07 lr 0.000002	 wd 0.0500	time 0.8211 (0.8729)	loss 1.2258 (1.0802)	grad_norm 2.7588 (2.9486)	loss_scale 2048.0000 (1077.7264)	mem 23350MB
[2024-07-31 00:39:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:11:39 lr 0.000002	 wd 0.0500	time 0.8152 (0.8724)	loss 1.5204 (1.0815)	grad_norm 18.7107 (2.9749)	loss_scale 2048.0000 (1134.7678)	mem 23350MB
[2024-07-31 00:41:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:10:12 lr 0.000002	 wd 0.0500	time 0.8270 (0.8719)	loss 1.1260 (1.0819)	grad_norm 2.5467 (2.9728)	loss_scale 2048.0000 (1185.4747)	mem 23350MB
[2024-07-31 00:42:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:08:44 lr 0.000002	 wd 0.0500	time 0.8247 (0.8714)	loss 0.7938 (1.0825)	grad_norm 2.3786 (2.9516)	loss_scale 2048.0000 (1230.8469)	mem 23350MB
[2024-07-31 00:43:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:07:17 lr 0.000002	 wd 0.0500	time 0.7646 (0.8712)	loss 1.2042 (1.0822)	grad_norm 2.6113 (2.9518)	loss_scale 2048.0000 (1271.6842)	mem 23350MB
[2024-07-31 00:45:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:05:50 lr 0.000002	 wd 0.0500	time 0.8172 (0.8709)	loss 0.9407 (1.0819)	grad_norm 2.1196 (2.9456)	loss_scale 2048.0000 (1308.6340)	mem 23350MB
[2024-07-31 00:46:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:04:22 lr 0.000002	 wd 0.0500	time 0.8211 (0.8708)	loss 0.8669 (1.0836)	grad_norm 2.6307 (2.9508)	loss_scale 2048.0000 (1342.2263)	mem 23350MB
[2024-07-31 00:48:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:02:55 lr 0.000002	 wd 0.0500	time 0.8184 (0.8706)	loss 1.2419 (1.0846)	grad_norm 2.8051 (2.9586)	loss_scale 2048.0000 (1372.8987)	mem 23350MB
[2024-07-31 00:49:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:28 lr 0.000002	 wd 0.0500	time 0.8247 (0.8706)	loss 1.3197 (1.0824)	grad_norm 2.2063 (2.9529)	loss_scale 2048.0000 (1401.0162)	mem 23350MB
[2024-07-31 00:51:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.8205 (0.8702)	loss 0.7317 (1.0827)	grad_norm 2.5513 (2.9494)	loss_scale 2048.0000 (1426.8852)	mem 23350MB
[2024-07-31 00:51:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 24 training takes 0:36:25
[2024-07-31 00:51:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.016 (12.016)	Loss 0.4504 (0.4504)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-31 00:51:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.834 Acc@5 98.046
[2024-07-31 00:51:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-31 00:51:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.83%
[2024-07-31 00:51:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-31 00:52:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-31 00:52:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:57:49 lr 0.000002	 wd 0.0500	time 11.4586 (11.4586)	loss 1.2685 (1.2685)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 00:53:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:38:52 lr 0.000002	 wd 0.0500	time 0.8128 (0.9712)	loss 1.1384 (1.1484)	grad_norm 3.2929 (2.9984)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 00:55:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:35:12 lr 0.000002	 wd 0.0500	time 0.8254 (0.9178)	loss 1.3939 (1.1169)	grad_norm 2.0852 (3.6456)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 00:56:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:33:02 lr 0.000002	 wd 0.0500	time 0.8088 (0.9005)	loss 1.1321 (1.1011)	grad_norm 4.4653 (3.3500)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 00:57:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:31:14 lr 0.000002	 wd 0.0500	time 0.8658 (0.8920)	loss 1.1025 (1.0971)	grad_norm 2.7346 (3.3013)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 00:59:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:29:35 lr 0.000002	 wd 0.0500	time 0.8171 (0.8867)	loss 1.2460 (1.0946)	grad_norm 1.9502 (3.2050)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 01:00:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:28:00 lr 0.000002	 wd 0.0500	time 0.8252 (0.8834)	loss 0.8543 (1.0856)	grad_norm 2.2568 (3.1291)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 01:02:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:26:27 lr 0.000002	 wd 0.0500	time 0.8182 (0.8810)	loss 0.7350 (1.0877)	grad_norm 2.2348 (3.0545)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 01:03:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:24:55 lr 0.000002	 wd 0.0500	time 0.8171 (0.8787)	loss 1.3077 (1.0858)	grad_norm 2.6383 (3.0130)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 01:05:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:23:25 lr 0.000002	 wd 0.0500	time 0.8125 (0.8773)	loss 1.2000 (1.0858)	grad_norm 2.2776 (2.9942)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 01:06:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:21:55 lr 0.000002	 wd 0.0500	time 0.8540 (0.8760)	loss 1.5115 (1.0862)	grad_norm 3.4898 (2.9926)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 01:08:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:20:26 lr 0.000002	 wd 0.0500	time 0.8275 (0.8751)	loss 1.3643 (1.0880)	grad_norm 2.4792 (3.0571)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 01:09:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:18:57 lr 0.000002	 wd 0.0500	time 0.8159 (0.8740)	loss 0.9078 (1.0847)	grad_norm 2.2769 (nan)	loss_scale 1024.0000 (2019.0108)	mem 23350MB
[2024-07-31 01:10:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:17:29 lr 0.000002	 wd 0.0500	time 0.7657 (0.8732)	loss 0.8598 (1.0842)	grad_norm 8.8866 (nan)	loss_scale 1024.0000 (1942.5304)	mem 23350MB
[2024-07-31 01:12:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:16:01 lr 0.000002	 wd 0.0500	time 0.8574 (0.8729)	loss 0.9777 (1.0841)	grad_norm 2.6555 (nan)	loss_scale 1024.0000 (1876.9679)	mem 23350MB
[2024-07-31 01:13:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:14:34 lr 0.000002	 wd 0.0500	time 0.8137 (0.8724)	loss 0.7582 (1.0836)	grad_norm 2.3376 (nan)	loss_scale 1024.0000 (1820.1412)	mem 23350MB
[2024-07-31 01:15:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:13:06 lr 0.000002	 wd 0.0500	time 0.8533 (0.8721)	loss 0.7934 (1.0828)	grad_norm 16.6283 (nan)	loss_scale 1024.0000 (1770.4135)	mem 23350MB
[2024-07-31 01:16:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:11:38 lr 0.000002	 wd 0.0500	time 0.8104 (0.8716)	loss 0.7094 (1.0804)	grad_norm 1.9989 (nan)	loss_scale 1024.0000 (1726.5326)	mem 23350MB
[2024-07-31 01:18:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:10:11 lr 0.000002	 wd 0.0500	time 0.8203 (0.8713)	loss 0.7659 (1.0820)	grad_norm 6.1205 (nan)	loss_scale 1024.0000 (1687.5247)	mem 23350MB
[2024-07-31 01:19:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:08:44 lr 0.000002	 wd 0.0500	time 0.7700 (0.8709)	loss 1.4041 (1.0838)	grad_norm 2.2106 (nan)	loss_scale 1024.0000 (1652.6207)	mem 23350MB
[2024-07-31 01:21:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:07:17 lr 0.000002	 wd 0.0500	time 0.8191 (0.8707)	loss 0.8720 (1.0840)	grad_norm 2.2775 (nan)	loss_scale 1024.0000 (1621.2054)	mem 23350MB
[2024-07-31 01:22:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:05:49 lr 0.000002	 wd 0.0500	time 0.8039 (0.8704)	loss 1.2076 (1.0849)	grad_norm 2.3534 (nan)	loss_scale 1024.0000 (1592.7806)	mem 23350MB
[2024-07-31 01:23:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:04:22 lr 0.000001	 wd 0.0500	time 0.8236 (0.8702)	loss 1.0305 (1.0852)	grad_norm 2.2128 (nan)	loss_scale 1024.0000 (1566.9387)	mem 23350MB
[2024-07-31 01:25:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:02:55 lr 0.000001	 wd 0.0500	time 0.8008 (0.8700)	loss 1.2867 (1.0850)	grad_norm 2.0125 (nan)	loss_scale 1024.0000 (1543.3429)	mem 23350MB
[2024-07-31 01:26:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:01:28 lr 0.000001	 wd 0.0500	time 0.8094 (0.8698)	loss 1.4199 (1.0845)	grad_norm 2.8533 (nan)	loss_scale 1024.0000 (1521.7126)	mem 23350MB
[2024-07-31 01:28:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.8139 (0.8697)	loss 0.9548 (1.0832)	grad_norm 3.3320 (nan)	loss_scale 1024.0000 (1501.8121)	mem 23350MB
[2024-07-31 01:28:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 25 training takes 0:36:21
[2024-07-31 01:28:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.593 (11.593)	Loss 0.4707 (0.4707)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-31 01:29:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.772 Acc@5 98.014
[2024-07-31 01:29:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-31 01:29:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.83%
[2024-07-31 01:29:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:12:37 lr 0.000001	 wd 0.0500	time 11.8134 (11.8134)	loss 1.0408 (1.0408)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:30:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:39:08 lr 0.000001	 wd 0.0500	time 0.7990 (0.9776)	loss 1.2584 (1.0938)	grad_norm 2.0614 (2.7194)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:32:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:35:19 lr 0.000001	 wd 0.0500	time 0.8251 (0.9208)	loss 0.9819 (1.0886)	grad_norm 2.9232 (2.7348)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:33:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:33:06 lr 0.000001	 wd 0.0500	time 0.8181 (0.9021)	loss 0.8553 (1.0761)	grad_norm 2.3362 (2.9371)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:34:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:31:17 lr 0.000001	 wd 0.0500	time 0.8040 (0.8931)	loss 1.3934 (1.0760)	grad_norm 2.3690 (2.8770)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:36:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:29:36 lr 0.000001	 wd 0.0500	time 0.8132 (0.8874)	loss 1.2715 (1.0846)	grad_norm 2.6410 (2.9122)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:37:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:28:00 lr 0.000001	 wd 0.0500	time 0.8117 (0.8836)	loss 0.7381 (1.0867)	grad_norm 2.0831 (2.9812)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:39:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:26:28 lr 0.000001	 wd 0.0500	time 0.8082 (0.8813)	loss 1.1309 (1.0827)	grad_norm 2.2048 (2.9594)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:40:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:24:57 lr 0.000001	 wd 0.0500	time 0.8091 (0.8799)	loss 1.2535 (1.0869)	grad_norm 3.3199 (2.9718)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:42:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:23:27 lr 0.000001	 wd 0.0500	time 0.8236 (0.8783)	loss 1.1481 (1.0871)	grad_norm 2.7855 (2.9724)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:43:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:21:57 lr 0.000001	 wd 0.0500	time 0.8160 (0.8773)	loss 0.9499 (1.0864)	grad_norm 3.2449 (2.9989)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:45:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:20:28 lr 0.000001	 wd 0.0500	time 0.8031 (0.8760)	loss 0.7928 (1.0853)	grad_norm 2.3343 (2.9711)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:46:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:18:59 lr 0.000001	 wd 0.0500	time 0.8248 (0.8750)	loss 0.8904 (1.0858)	grad_norm 2.1307 (3.0335)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:47:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:17:30 lr 0.000001	 wd 0.0500	time 0.8567 (0.8741)	loss 1.1711 (1.0878)	grad_norm 2.5485 (3.0476)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:49:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:16:02 lr 0.000001	 wd 0.0500	time 0.8191 (0.8734)	loss 1.3608 (1.0896)	grad_norm 2.7041 (3.0409)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:50:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:14:34 lr 0.000001	 wd 0.0500	time 0.8021 (0.8727)	loss 1.2751 (1.0914)	grad_norm 1.9753 (3.0285)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:52:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:13:06 lr 0.000001	 wd 0.0500	time 0.8124 (0.8722)	loss 1.0843 (1.0901)	grad_norm 3.7487 (3.0146)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:53:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:11:39 lr 0.000001	 wd 0.0500	time 0.8270 (0.8718)	loss 1.0620 (1.0868)	grad_norm 2.2979 (3.1233)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:55:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:10:11 lr 0.000001	 wd 0.0500	time 0.8115 (0.8714)	loss 0.7143 (1.0858)	grad_norm 2.7580 (3.1002)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:56:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:08:44 lr 0.000001	 wd 0.0500	time 0.7624 (0.8712)	loss 1.2780 (1.0873)	grad_norm 2.1165 (3.0772)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:58:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:07:17 lr 0.000001	 wd 0.0500	time 0.8142 (0.8710)	loss 1.0839 (1.0860)	grad_norm 2.4940 (3.0502)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 01:59:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:05:50 lr 0.000001	 wd 0.0500	time 0.8310 (0.8707)	loss 1.2784 (1.0855)	grad_norm 2.5591 (3.0914)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 02:00:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:04:22 lr 0.000001	 wd 0.0500	time 0.8333 (0.8704)	loss 1.2127 (1.0838)	grad_norm 2.3478 (3.0722)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 02:02:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:02:55 lr 0.000001	 wd 0.0500	time 0.8205 (0.8702)	loss 1.3215 (1.0842)	grad_norm 2.6565 (3.0550)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 02:03:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:01:28 lr 0.000001	 wd 0.0500	time 0.8185 (0.8700)	loss 0.7003 (1.0847)	grad_norm 2.3290 (3.0395)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 02:05:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.7629 (0.8697)	loss 0.7764 (1.0848)	grad_norm 5.4346 (3.0404)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 02:05:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 26 training takes 0:36:22
[2024-07-31 02:05:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.155 (11.155)	Loss 0.4526 (0.4526)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-31 02:06:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.840 Acc@5 98.056
[2024-07-31 02:06:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-31 02:06:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-07-31 02:06:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-31 02:06:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-31 02:06:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:20:02 lr 0.000001	 wd 0.0500	time 10.5526 (10.5526)	loss 1.2873 (1.2873)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 02:07:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:38:23 lr 0.000001	 wd 0.0500	time 0.8091 (0.9591)	loss 1.1727 (1.0944)	grad_norm 2.3617 (2.9283)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 02:09:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:35:04 lr 0.000001	 wd 0.0500	time 0.8399 (0.9142)	loss 1.1628 (1.0985)	grad_norm 2.3274 (3.0569)	loss_scale 2048.0000 (1217.5920)	mem 23350MB
[2024-07-31 02:10:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:33:00 lr 0.000001	 wd 0.0500	time 0.8211 (0.8993)	loss 1.2164 (1.0951)	grad_norm 2.7333 (2.9248)	loss_scale 2048.0000 (1493.4751)	mem 23350MB
[2024-07-31 02:12:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:31:12 lr 0.000001	 wd 0.0500	time 0.9081 (0.8908)	loss 0.8981 (1.0940)	grad_norm 3.0755 (3.0178)	loss_scale 2048.0000 (1631.7606)	mem 23350MB
[2024-07-31 02:13:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:29:33 lr 0.000001	 wd 0.0500	time 0.8269 (0.8857)	loss 0.7027 (1.0949)	grad_norm 3.2491 (3.0003)	loss_scale 2048.0000 (1714.8423)	mem 23350MB
[2024-07-31 02:14:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:27:57 lr 0.000001	 wd 0.0500	time 0.8286 (0.8818)	loss 1.3300 (1.0944)	grad_norm 2.5424 (2.9380)	loss_scale 2048.0000 (1770.2762)	mem 23350MB
[2024-07-31 02:16:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:26:24 lr 0.000001	 wd 0.0500	time 0.8192 (0.8795)	loss 0.7193 (1.0945)	grad_norm 2.9596 (2.8871)	loss_scale 2048.0000 (1809.8944)	mem 23350MB
[2024-07-31 02:17:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:24:54 lr 0.000001	 wd 0.0500	time 0.8750 (0.8781)	loss 1.0501 (1.0946)	grad_norm 2.8780 (2.9524)	loss_scale 2048.0000 (1839.6205)	mem 23350MB
[2024-07-31 02:19:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:23:24 lr 0.000001	 wd 0.0500	time 0.8164 (0.8769)	loss 1.2518 (1.0930)	grad_norm 1.9622 (2.9246)	loss_scale 2048.0000 (1862.7481)	mem 23350MB
[2024-07-31 02:20:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:21:55 lr 0.000001	 wd 0.0500	time 0.8099 (0.8758)	loss 0.8730 (1.0887)	grad_norm 2.6687 (2.9057)	loss_scale 2048.0000 (1881.2547)	mem 23350MB
[2024-07-31 02:22:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:20:26 lr 0.000001	 wd 0.0500	time 0.8245 (0.8749)	loss 1.0996 (1.0902)	grad_norm 2.2386 (2.9046)	loss_scale 2048.0000 (1896.3996)	mem 23350MB
[2024-07-31 02:23:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:18:58 lr 0.000001	 wd 0.0500	time 0.8194 (0.8742)	loss 0.7971 (1.0900)	grad_norm 2.3032 (2.9075)	loss_scale 2048.0000 (1909.0225)	mem 23350MB
[2024-07-31 02:24:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:17:29 lr 0.000001	 wd 0.0500	time 0.8116 (0.8735)	loss 0.7464 (1.0857)	grad_norm 2.3607 (2.8817)	loss_scale 2048.0000 (1919.7048)	mem 23350MB
[2024-07-31 02:26:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:16:01 lr 0.000001	 wd 0.0500	time 0.8028 (0.8727)	loss 0.8388 (1.0871)	grad_norm 2.0213 (2.9031)	loss_scale 2048.0000 (1928.8622)	mem 23350MB
[2024-07-31 02:27:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:14:34 lr 0.000001	 wd 0.0500	time 0.7953 (0.8723)	loss 1.3684 (1.0885)	grad_norm 2.2478 (2.9037)	loss_scale 2048.0000 (1936.7995)	mem 23350MB
[2024-07-31 02:29:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:13:06 lr 0.000001	 wd 0.0500	time 0.8343 (0.8718)	loss 1.4590 (1.0897)	grad_norm 2.5181 (2.8945)	loss_scale 2048.0000 (1943.7452)	mem 23350MB
[2024-07-31 02:30:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:11:38 lr 0.000001	 wd 0.0500	time 0.8322 (0.8714)	loss 1.2709 (1.0906)	grad_norm 2.4229 (2.8937)	loss_scale 2048.0000 (1949.8742)	mem 23350MB
[2024-07-31 02:32:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:10:11 lr 0.000001	 wd 0.0500	time 0.7634 (0.8709)	loss 1.2441 (1.0899)	grad_norm 2.8355 (2.8800)	loss_scale 2048.0000 (1955.3226)	mem 23350MB
[2024-07-31 02:33:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:08:44 lr 0.000001	 wd 0.0500	time 0.8088 (0.8707)	loss 1.2700 (1.0904)	grad_norm 3.3531 (2.8718)	loss_scale 2048.0000 (1960.1978)	mem 23350MB
[2024-07-31 02:35:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:07:16 lr 0.000001	 wd 0.0500	time 0.8240 (0.8705)	loss 1.2875 (1.0909)	grad_norm 2.2717 (2.8755)	loss_scale 2048.0000 (1964.5857)	mem 23350MB
[2024-07-31 02:36:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:05:49 lr 0.000001	 wd 0.0500	time 0.8223 (0.8703)	loss 1.4099 (1.0904)	grad_norm 2.9733 (2.8782)	loss_scale 2048.0000 (1968.5559)	mem 23350MB
[2024-07-31 02:37:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:04:22 lr 0.000001	 wd 0.0500	time 0.8083 (0.8701)	loss 1.1423 (1.0911)	grad_norm 2.5504 (2.8683)	loss_scale 2048.0000 (1972.1654)	mem 23350MB
[2024-07-31 02:39:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:02:55 lr 0.000001	 wd 0.0500	time 0.8030 (0.8699)	loss 1.2530 (1.0911)	grad_norm 2.7329 (2.8670)	loss_scale 2048.0000 (1975.4611)	mem 23350MB
[2024-07-31 02:40:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:01:28 lr 0.000001	 wd 0.0500	time 0.8192 (0.8698)	loss 1.2781 (1.0924)	grad_norm 3.5108 (2.8638)	loss_scale 2048.0000 (1978.4823)	mem 23350MB
[2024-07-31 02:42:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.8348 (0.8695)	loss 1.0883 (1.0914)	grad_norm 4.5131 (2.8569)	loss_scale 2048.0000 (1981.2619)	mem 23350MB
[2024-07-31 02:42:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 27 training takes 0:36:22
[2024-07-31 02:42:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.766 (11.766)	Loss 0.4424 (0.4424)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-31 02:43:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.840 Acc@5 98.066
[2024-07-31 02:43:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-31 02:43:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-07-31 02:43:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saving......
[2024-07-31 02:43:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_best.pth saved !!!
[2024-07-31 02:43:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:36:29 lr 0.000001	 wd 0.0500	time 10.9472 (10.9472)	loss 0.7330 (0.7330)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 02:44:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:38:38 lr 0.000000	 wd 0.0500	time 0.8374 (0.9651)	loss 0.6931 (1.1125)	grad_norm 2.7196 (3.0133)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 02:46:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:35:06 lr 0.000000	 wd 0.0500	time 0.8162 (0.9149)	loss 1.3217 (1.1083)	grad_norm 2.7538 (2.8884)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 02:47:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:32:57 lr 0.000000	 wd 0.0500	time 0.8213 (0.8981)	loss 1.3595 (1.1003)	grad_norm 3.5154 (2.8104)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 02:49:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:31:09 lr 0.000000	 wd 0.0500	time 0.8704 (0.8896)	loss 0.7082 (1.0919)	grad_norm 2.0502 (2.7626)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 02:50:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:29:32 lr 0.000000	 wd 0.0500	time 0.8163 (0.8853)	loss 0.7742 (1.0966)	grad_norm 2.2338 (2.7427)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-07-31 02:51:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:27:58 lr 0.000000	 wd 0.0500	time 0.8033 (0.8824)	loss 0.8435 (1.0893)	grad_norm 12.5475 (nan)	loss_scale 1024.0000 (2007.1082)	mem 23350MB
[2024-07-31 02:53:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:26:25 lr 0.000000	 wd 0.0500	time 0.8231 (0.8800)	loss 1.1930 (1.0862)	grad_norm 2.0707 (nan)	loss_scale 1024.0000 (1866.8645)	mem 23350MB
[2024-07-31 02:54:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:24:55 lr 0.000000	 wd 0.0500	time 0.8177 (0.8784)	loss 0.7636 (1.0877)	grad_norm 6.8987 (nan)	loss_scale 1024.0000 (1761.6380)	mem 23350MB
[2024-07-31 02:56:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:23:25 lr 0.000000	 wd 0.0500	time 0.8159 (0.8772)	loss 0.6894 (1.0843)	grad_norm 2.3970 (nan)	loss_scale 1024.0000 (1679.7691)	mem 23350MB
[2024-07-31 02:57:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:21:56 lr 0.000000	 wd 0.0500	time 0.8224 (0.8764)	loss 0.9684 (1.0852)	grad_norm 2.1412 (nan)	loss_scale 1024.0000 (1614.2577)	mem 23350MB
[2024-07-31 02:59:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:20:26 lr 0.000000	 wd 0.0500	time 0.8153 (0.8750)	loss 1.1137 (1.0863)	grad_norm 2.3276 (nan)	loss_scale 1024.0000 (1560.6467)	mem 23350MB
[2024-07-31 03:00:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:18:58 lr 0.000000	 wd 0.0500	time 0.7608 (0.8742)	loss 1.3113 (1.0869)	grad_norm 2.5856 (nan)	loss_scale 1024.0000 (1515.9634)	mem 23350MB
[2024-07-31 03:02:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:17:29 lr 0.000000	 wd 0.0500	time 0.8110 (0.8734)	loss 1.2806 (1.0887)	grad_norm 2.2750 (nan)	loss_scale 1024.0000 (1478.1491)	mem 23350MB
[2024-07-31 03:03:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:16:01 lr 0.000000	 wd 0.0500	time 0.8193 (0.8728)	loss 0.8410 (1.0874)	grad_norm 2.2132 (nan)	loss_scale 1024.0000 (1445.7330)	mem 23350MB
[2024-07-31 03:04:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:14:33 lr 0.000000	 wd 0.0500	time 0.8231 (0.8723)	loss 1.1730 (1.0873)	grad_norm 2.7790 (nan)	loss_scale 1024.0000 (1417.6362)	mem 23350MB
[2024-07-31 03:06:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:13:06 lr 0.000000	 wd 0.0500	time 0.8231 (0.8718)	loss 1.3512 (1.0896)	grad_norm 3.3149 (nan)	loss_scale 1024.0000 (1393.0493)	mem 23350MB
[2024-07-31 03:07:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:11:38 lr 0.000000	 wd 0.0500	time 0.8094 (0.8715)	loss 1.3084 (1.0895)	grad_norm 2.8284 (nan)	loss_scale 1024.0000 (1371.3533)	mem 23350MB
[2024-07-31 03:09:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:10:11 lr 0.000000	 wd 0.0500	time 0.8774 (0.8712)	loss 0.6782 (1.0890)	grad_norm 2.3105 (nan)	loss_scale 1024.0000 (1352.0666)	mem 23350MB
[2024-07-31 03:10:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:08:44 lr 0.000000	 wd 0.0500	time 0.8174 (0.8709)	loss 0.9300 (1.0865)	grad_norm 2.4758 (nan)	loss_scale 1024.0000 (1334.8090)	mem 23350MB
[2024-07-31 03:12:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:07:17 lr 0.000000	 wd 0.0500	time 0.8239 (0.8706)	loss 1.0850 (1.0866)	grad_norm 7.3973 (nan)	loss_scale 1024.0000 (1319.2764)	mem 23350MB
[2024-07-31 03:13:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:05:49 lr 0.000000	 wd 0.0500	time 0.8174 (0.8702)	loss 0.9822 (1.0872)	grad_norm 4.0940 (nan)	loss_scale 1024.0000 (1305.2223)	mem 23350MB
[2024-07-31 03:15:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:04:22 lr 0.000000	 wd 0.0500	time 0.8222 (0.8700)	loss 1.2446 (1.0866)	grad_norm 2.5335 (nan)	loss_scale 1024.0000 (1292.4453)	mem 23350MB
[2024-07-31 03:16:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:02:55 lr 0.000000	 wd 0.0500	time 0.8092 (0.8697)	loss 1.2230 (1.0864)	grad_norm 2.4044 (nan)	loss_scale 1024.0000 (1280.7788)	mem 23350MB
[2024-07-31 03:17:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:01:28 lr 0.000000	 wd 0.0500	time 0.8202 (0.8695)	loss 1.4433 (1.0854)	grad_norm 2.0026 (nan)	loss_scale 1024.0000 (1270.0841)	mem 23350MB
[2024-07-31 03:19:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.8020 (0.8693)	loss 1.0203 (1.0845)	grad_norm 2.1491 (nan)	loss_scale 1024.0000 (1260.2447)	mem 23350MB
[2024-07-31 03:19:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 28 training takes 0:36:20
[2024-07-31 03:19:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.622 (11.622)	Loss 0.4490 (0.4490)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-31 03:20:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.832 Acc@5 98.046
[2024-07-31 03:20:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-31 03:20:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-07-31 03:20:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:47:02 lr 0.000000	 wd 0.0500	time 12.6389 (12.6389)	loss 1.2490 (1.2490)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:21:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:39:22 lr 0.000000	 wd 0.0500	time 0.8094 (0.9835)	loss 1.3015 (1.1011)	grad_norm 2.2858 (3.2453)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:23:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:35:30 lr 0.000000	 wd 0.0500	time 0.7773 (0.9255)	loss 0.7611 (1.1018)	grad_norm 2.2513 (3.0337)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:24:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:33:10 lr 0.000000	 wd 0.0500	time 0.8039 (0.9042)	loss 0.6590 (1.0902)	grad_norm 2.3743 (2.9197)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:26:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:31:20 lr 0.000000	 wd 0.0500	time 0.8119 (0.8949)	loss 1.0016 (1.0860)	grad_norm 2.8998 (2.8797)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:27:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:29:40 lr 0.000000	 wd 0.0500	time 0.8743 (0.8891)	loss 1.1219 (1.0815)	grad_norm 2.6868 (2.9222)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:28:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:28:03 lr 0.000000	 wd 0.0500	time 0.7644 (0.8853)	loss 0.6800 (1.0840)	grad_norm 2.4175 (2.9523)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:30:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:26:29 lr 0.000000	 wd 0.0500	time 0.8173 (0.8823)	loss 1.1776 (1.0836)	grad_norm 3.2182 (2.8997)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:31:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:24:57 lr 0.000000	 wd 0.0500	time 0.8153 (0.8801)	loss 0.8071 (1.0848)	grad_norm 2.2003 (2.8801)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:33:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:23:27 lr 0.000000	 wd 0.0500	time 0.8179 (0.8784)	loss 1.3626 (1.0853)	grad_norm 2.8249 (2.8890)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:34:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:21:57 lr 0.000000	 wd 0.0500	time 0.8284 (0.8774)	loss 1.2192 (1.0858)	grad_norm 2.3199 (2.9388)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:36:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:20:28 lr 0.000000	 wd 0.0500	time 0.8094 (0.8763)	loss 1.2161 (1.0836)	grad_norm 2.5581 (2.9075)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:37:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:18:59 lr 0.000000	 wd 0.0500	time 0.8170 (0.8754)	loss 0.9428 (1.0825)	grad_norm 2.4304 (2.8728)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:39:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:17:31 lr 0.000000	 wd 0.0500	time 0.8137 (0.8747)	loss 1.0529 (1.0861)	grad_norm 4.4541 (2.8598)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:40:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:16:03 lr 0.000000	 wd 0.0500	time 0.8233 (0.8740)	loss 1.0497 (1.0813)	grad_norm 3.2886 (2.8968)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:41:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:14:35 lr 0.000000	 wd 0.0500	time 0.8231 (0.8735)	loss 0.9665 (1.0803)	grad_norm 2.3706 (2.9057)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:43:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:13:07 lr 0.000000	 wd 0.0500	time 0.8675 (0.8731)	loss 0.7526 (1.0831)	grad_norm 2.3370 (2.9027)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:44:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:11:39 lr 0.000000	 wd 0.0500	time 0.8132 (0.8726)	loss 1.1405 (1.0852)	grad_norm 2.2926 (2.8863)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:46:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:10:12 lr 0.000000	 wd 0.0500	time 0.8179 (0.8723)	loss 1.2186 (1.0843)	grad_norm 2.3235 (2.8933)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:47:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:08:44 lr 0.000000	 wd 0.0500	time 0.8259 (0.8719)	loss 0.8339 (1.0844)	grad_norm 2.2889 (2.9028)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:49:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:07:17 lr 0.000000	 wd 0.0500	time 0.8206 (0.8716)	loss 0.8603 (1.0863)	grad_norm 1.9473 (2.9140)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-07-31 03:50:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:05:50 lr 0.000000	 wd 0.0500	time 0.8229 (0.8715)	loss 0.7813 (1.0875)	grad_norm 2.5566 (2.9085)	loss_scale 2048.0000 (1036.6721)	mem 23350MB
[2024-07-31 03:52:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:04:23 lr 0.000000	 wd 0.0500	time 0.8032 (0.8712)	loss 0.8247 (1.0881)	grad_norm 2.0354 (2.9036)	loss_scale 2048.0000 (1082.6206)	mem 23350MB
[2024-07-31 03:53:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:02:55 lr 0.000000	 wd 0.0500	time 0.8038 (0.8709)	loss 0.6869 (1.0853)	grad_norm 2.9942 (2.8994)	loss_scale 2048.0000 (1124.5754)	mem 23350MB
[2024-07-31 03:54:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:01:28 lr 0.000000	 wd 0.0500	time 0.8238 (0.8707)	loss 1.1763 (1.0855)	grad_norm 3.1174 (2.8970)	loss_scale 2048.0000 (1163.0354)	mem 23350MB
[2024-07-31 03:56:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.8004 (0.8704)	loss 1.1753 (1.0850)	grad_norm 2.0515 (2.8907)	loss_scale 2048.0000 (1198.4198)	mem 23350MB
[2024-07-31 03:56:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 249): INFO EPOCH 29 training takes 0:36:23
[2024-07-31 03:56:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_29.pth saving......
[2024-07-31 03:56:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft/diffusion_ft_smt_l_sequence_cross1/ckpt_epoch_29.pth saved !!!
[2024-07-31 03:56:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 289): INFO Test: [0/98]	Time 10.859 (10.859)	Loss 0.4399 (0.4399)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-07-31 03:57:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 296): INFO  * Acc@1 86.830 Acc@5 98.070
[2024-07-31 03:57:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-07-31 03:57:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-07-31 03:57:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer1-full-ft] (main.py 189): INFO Training time 18:27:18
