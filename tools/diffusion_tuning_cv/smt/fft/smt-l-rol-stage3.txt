[2024-08-04 10:09:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/config.json
[2024-08-04 10:09:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_sequence_stage3
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-04 10:09:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_sequence_stage3.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_sequence_stage3", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-04 10:09:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft
[2024-08-04 10:09:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-04 10:09:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 113): INFO number of params: 80620264
[2024-08-04 10:09:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3, ignoring auto resume
[2024-08-04 10:09:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth for fine-tuning......
[2024-08-04 10:09:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-04 10:09:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth'
[2024-08-04 10:09:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 13.525 (13.525)	Loss 0.4712 (0.4712)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 2730MB
[2024-08-04 10:10:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.856 Acc@5 98.094
[2024-08-04 10:10:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-04 10:10:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 168): INFO Start training
[2024-08-04 10:10:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:21:34 lr 0.000000	 wd 0.0500	time 13.4672 (13.4672)	loss 1.4352 (1.4352)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 23350MB
[2024-08-04 10:11:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:41:07 lr 0.000000	 wd 0.0500	time 0.8566 (1.0275)	loss 1.2420 (1.1516)	grad_norm 2.1200 (nan)	loss_scale 8192.0000 (14761.8218)	mem 23350MB
[2024-08-04 10:13:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:37:01 lr 0.000000	 wd 0.0500	time 0.8467 (0.9651)	loss 0.9882 (1.1458)	grad_norm 2.0552 (nan)	loss_scale 8192.0000 (11493.2537)	mem 23350MB
[2024-08-04 10:14:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:34:39 lr 0.000000	 wd 0.0500	time 0.8607 (0.9443)	loss 0.8636 (1.1082)	grad_norm 2.6457 (nan)	loss_scale 4096.0000 (10342.0598)	mem 23350MB
[2024-08-04 10:16:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:32:44 lr 0.000001	 wd 0.0500	time 0.8511 (0.9346)	loss 1.0448 (1.1155)	grad_norm 5.5794 (nan)	loss_scale 4096.0000 (8784.4389)	mem 23350MB
[2024-08-04 10:17:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:30:58 lr 0.000001	 wd 0.0500	time 0.8637 (0.9282)	loss 1.1344 (1.1173)	grad_norm 1.9200 (nan)	loss_scale 4096.0000 (7848.6228)	mem 23350MB
[2024-08-04 10:19:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:29:17 lr 0.000001	 wd 0.0500	time 0.8653 (0.9241)	loss 1.2432 (1.1159)	grad_norm 3.0544 (nan)	loss_scale 4096.0000 (7224.2263)	mem 23350MB
[2024-08-04 10:20:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:27:39 lr 0.000001	 wd 0.0500	time 0.8594 (0.9210)	loss 1.2280 (1.1183)	grad_norm 2.6277 (nan)	loss_scale 4096.0000 (6777.9743)	mem 23350MB
[2024-08-04 10:22:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:26:03 lr 0.000001	 wd 0.0500	time 0.8541 (0.9187)	loss 1.0462 (1.1172)	grad_norm 2.6894 (nan)	loss_scale 4096.0000 (6443.1461)	mem 23350MB
[2024-08-04 10:23:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:24:29 lr 0.000001	 wd 0.0500	time 0.8016 (0.9171)	loss 1.2624 (1.1176)	grad_norm 3.4951 (nan)	loss_scale 4096.0000 (6182.6415)	mem 23350MB
[2024-08-04 10:25:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:22:55 lr 0.000002	 wd 0.0500	time 0.8611 (0.9158)	loss 1.4066 (1.1149)	grad_norm 2.7678 (nan)	loss_scale 4096.0000 (5974.1858)	mem 23350MB
[2024-08-04 10:26:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:21:22 lr 0.000002	 wd 0.0500	time 0.8487 (0.9148)	loss 1.3033 (1.1164)	grad_norm 2.0476 (nan)	loss_scale 4096.0000 (5803.5967)	mem 23350MB
[2024-08-04 10:28:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:19:49 lr 0.000002	 wd 0.0500	time 0.8465 (0.9138)	loss 0.9758 (1.1159)	grad_norm 2.3299 (nan)	loss_scale 4096.0000 (5661.4155)	mem 23350MB
[2024-08-04 10:29:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:18:17 lr 0.000002	 wd 0.0500	time 0.8631 (0.9130)	loss 1.1512 (1.1177)	grad_norm 2.6275 (nan)	loss_scale 4096.0000 (5541.0915)	mem 23350MB
[2024-08-04 10:31:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:16:45 lr 0.000002	 wd 0.0500	time 0.8591 (0.9124)	loss 1.4251 (1.1188)	grad_norm 2.9269 (nan)	loss_scale 4096.0000 (5437.9443)	mem 23350MB
[2024-08-04 10:32:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:15:13 lr 0.000002	 wd 0.0500	time 0.8575 (0.9118)	loss 0.9492 (1.1218)	grad_norm 2.3549 (nan)	loss_scale 4096.0000 (5348.5410)	mem 23350MB
[2024-08-04 10:34:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:13:42 lr 0.000003	 wd 0.0500	time 0.8500 (0.9113)	loss 1.1778 (1.1191)	grad_norm 2.0471 (nan)	loss_scale 2048.0000 (5185.8788)	mem 23350MB
[2024-08-04 10:35:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:12:10 lr 0.000003	 wd 0.0500	time 0.8566 (0.9108)	loss 0.9199 (1.1185)	grad_norm 4.4860 (nan)	loss_scale 2048.0000 (5001.4062)	mem 23350MB
[2024-08-04 10:37:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:10:39 lr 0.000003	 wd 0.0500	time 0.8456 (0.9104)	loss 1.0302 (1.1186)	grad_norm 2.2459 (nan)	loss_scale 2048.0000 (4837.4192)	mem 23350MB
[2024-08-04 10:38:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:09:07 lr 0.000003	 wd 0.0500	time 0.8468 (0.9100)	loss 1.4626 (1.1184)	grad_norm 2.7525 (nan)	loss_scale 2048.0000 (4690.6849)	mem 23350MB
[2024-08-04 10:40:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:07:36 lr 0.000003	 wd 0.0500	time 0.8639 (0.9098)	loss 0.7849 (1.1175)	grad_norm 2.1357 (nan)	loss_scale 2048.0000 (4558.6167)	mem 23350MB
[2024-08-04 10:41:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:06:05 lr 0.000003	 wd 0.0500	time 0.9202 (0.9095)	loss 0.9492 (1.1164)	grad_norm 2.4955 (nan)	loss_scale 2048.0000 (4439.1204)	mem 23350MB
[2024-08-04 10:43:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:04:34 lr 0.000004	 wd 0.0500	time 0.8669 (0.9093)	loss 1.4382 (1.1156)	grad_norm 3.8220 (nan)	loss_scale 2048.0000 (4330.4825)	mem 23350MB
[2024-08-04 10:44:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:03:03 lr 0.000004	 wd 0.0500	time 0.8597 (0.9091)	loss 1.3549 (1.1154)	grad_norm 2.8669 (nan)	loss_scale 2048.0000 (4231.2873)	mem 23350MB
[2024-08-04 10:46:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:32 lr 0.000004	 wd 0.0500	time 0.8564 (0.9088)	loss 1.2589 (1.1150)	grad_norm 2.3619 (nan)	loss_scale 2048.0000 (4140.3549)	mem 23350MB
[2024-08-04 10:47:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.8606 (0.9085)	loss 0.9630 (1.1135)	grad_norm 2.7945 (nan)	loss_scale 2048.0000 (4056.6941)	mem 23350MB
[2024-08-04 10:48:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 0 training takes 0:37:55
[2024-08-04 10:48:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_0.pth saving......
[2024-08-04 10:48:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_0.pth saved !!!
[2024-08-04 10:48:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 10.771 (10.771)	Loss 0.4922 (0.4922)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-04 10:48:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.842 Acc@5 98.068
[2024-08-04 10:48:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 10:48:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 10:48:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-04 10:48:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-04 10:48:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][0/2502]	eta 8:07:27 lr 0.000004	 wd 0.0500	time 11.6896 (11.6896)	loss 1.2862 (1.2862)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 10:50:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:40:23 lr 0.000004	 wd 0.0500	time 0.8640 (1.0088)	loss 1.2374 (1.1023)	grad_norm 2.0312 (2.7056)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 10:51:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:36:40 lr 0.000004	 wd 0.0500	time 0.8475 (0.9561)	loss 0.7489 (1.1036)	grad_norm 2.9538 (2.6758)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 10:53:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:34:26 lr 0.000004	 wd 0.0500	time 0.8569 (0.9384)	loss 0.8629 (1.1059)	grad_norm 2.0150 (2.8300)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 10:54:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:32:35 lr 0.000005	 wd 0.0500	time 0.8577 (0.9301)	loss 0.8670 (1.1202)	grad_norm 2.0568 (2.8369)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 10:56:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:30:51 lr 0.000005	 wd 0.0500	time 0.8602 (0.9248)	loss 1.4941 (1.1232)	grad_norm 2.5357 (nan)	loss_scale 1024.0000 (2003.0339)	mem 23350MB
[2024-08-04 10:57:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:29:12 lr 0.000005	 wd 0.0500	time 0.8592 (0.9215)	loss 1.3651 (1.1224)	grad_norm 3.5526 (nan)	loss_scale 1024.0000 (1840.1331)	mem 23350MB
[2024-08-04 10:59:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:27:35 lr 0.000005	 wd 0.0500	time 0.8405 (0.9189)	loss 0.8107 (1.1188)	grad_norm 3.0772 (nan)	loss_scale 1024.0000 (1723.7090)	mem 23350MB
[2024-08-04 11:00:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:26:00 lr 0.000005	 wd 0.0500	time 0.8585 (0.9171)	loss 1.4997 (1.1185)	grad_norm 2.5418 (nan)	loss_scale 1024.0000 (1636.3546)	mem 23350MB
[2024-08-04 11:02:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:24:26 lr 0.000005	 wd 0.0500	time 0.8394 (0.9156)	loss 1.1169 (1.1167)	grad_norm 3.2181 (nan)	loss_scale 1024.0000 (1568.3907)	mem 23350MB
[2024-08-04 11:03:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:22:53 lr 0.000006	 wd 0.0500	time 0.8544 (0.9142)	loss 1.4167 (1.1135)	grad_norm 3.1019 (nan)	loss_scale 1024.0000 (1514.0060)	mem 23350MB
[2024-08-04 11:05:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:21:20 lr 0.000006	 wd 0.0500	time 0.8010 (0.9132)	loss 1.4652 (1.1125)	grad_norm 2.6545 (nan)	loss_scale 1024.0000 (1469.5005)	mem 23350MB
[2024-08-04 11:06:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:19:47 lr 0.000006	 wd 0.0500	time 0.8590 (0.9122)	loss 0.8983 (1.1106)	grad_norm 2.1095 (nan)	loss_scale 1024.0000 (1432.4063)	mem 23350MB
[2024-08-04 11:08:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:18:15 lr 0.000006	 wd 0.0500	time 0.8581 (0.9115)	loss 1.1752 (1.1120)	grad_norm 5.1171 (nan)	loss_scale 1024.0000 (1401.0146)	mem 23350MB
[2024-08-04 11:09:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:16:44 lr 0.000006	 wd 0.0500	time 0.8581 (0.9111)	loss 0.7740 (1.1120)	grad_norm 2.3133 (nan)	loss_scale 1024.0000 (1374.1042)	mem 23350MB
[2024-08-04 11:11:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:15:12 lr 0.000006	 wd 0.0500	time 0.8592 (0.9107)	loss 1.4060 (1.1121)	grad_norm 2.3421 (nan)	loss_scale 1024.0000 (1350.7795)	mem 23350MB
[2024-08-04 11:12:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:13:41 lr 0.000007	 wd 0.0500	time 0.8598 (0.9102)	loss 1.3092 (1.1132)	grad_norm 2.4809 (nan)	loss_scale 1024.0000 (1330.3685)	mem 23350MB
[2024-08-04 11:14:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:12:09 lr 0.000007	 wd 0.0500	time 0.8454 (0.9097)	loss 0.7989 (1.1138)	grad_norm 2.5333 (nan)	loss_scale 1024.0000 (1312.3574)	mem 23350MB
[2024-08-04 11:15:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:10:38 lr 0.000007	 wd 0.0500	time 0.8613 (0.9093)	loss 1.5752 (1.1130)	grad_norm 2.3528 (nan)	loss_scale 1024.0000 (1296.3465)	mem 23350MB
[2024-08-04 11:17:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:09:07 lr 0.000007	 wd 0.0500	time 0.8517 (0.9090)	loss 1.4398 (1.1147)	grad_norm 1.9267 (nan)	loss_scale 1024.0000 (1282.0200)	mem 23350MB
[2024-08-04 11:18:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:07:36 lr 0.000007	 wd 0.0500	time 0.8459 (0.9088)	loss 1.2295 (1.1152)	grad_norm 2.8316 (nan)	loss_scale 1024.0000 (1269.1254)	mem 23350MB
[2024-08-04 11:20:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:06:05 lr 0.000007	 wd 0.0500	time 0.8901 (0.9085)	loss 1.3460 (1.1171)	grad_norm 4.9756 (nan)	loss_scale 1024.0000 (1257.4584)	mem 23350MB
[2024-08-04 11:21:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:04:34 lr 0.000008	 wd 0.0500	time 0.8513 (0.9083)	loss 1.0797 (1.1175)	grad_norm 3.4641 (nan)	loss_scale 1024.0000 (1246.8514)	mem 23350MB
[2024-08-04 11:23:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:03:03 lr 0.000008	 wd 0.0500	time 0.8504 (0.9080)	loss 0.7526 (1.1189)	grad_norm 2.1288 (nan)	loss_scale 1024.0000 (1237.1664)	mem 23350MB
[2024-08-04 11:24:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.9026 (0.9079)	loss 1.1801 (1.1188)	grad_norm 2.0639 (nan)	loss_scale 1024.0000 (1228.2882)	mem 23350MB
[2024-08-04 11:26:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.8541 (0.9076)	loss 0.8924 (1.1186)	grad_norm 2.3847 (nan)	loss_scale 1024.0000 (1220.1200)	mem 23350MB
[2024-08-04 11:26:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 1 training takes 0:37:53
[2024-08-04 11:26:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.424 (12.424)	Loss 0.4822 (0.4822)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-04 11:27:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.802 Acc@5 98.086
[2024-08-04 11:27:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 11:27:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 11:27:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][0/2502]	eta 9:02:04 lr 0.000008	 wd 0.0500	time 12.9992 (12.9992)	loss 1.1055 (1.1055)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:28:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:40:59 lr 0.000008	 wd 0.0500	time 0.9259 (1.0238)	loss 1.3917 (1.1180)	grad_norm 1.9269 (2.9771)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:30:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:36:57 lr 0.000008	 wd 0.0500	time 0.8470 (0.9633)	loss 1.2321 (1.1079)	grad_norm 2.3390 (2.8982)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:31:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:34:37 lr 0.000008	 wd 0.0500	time 0.8599 (0.9435)	loss 1.2013 (1.0968)	grad_norm 2.6655 (2.9192)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:33:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:32:42 lr 0.000009	 wd 0.0500	time 0.8597 (0.9335)	loss 1.3055 (1.1018)	grad_norm 2.3554 (3.0043)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:34:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:30:57 lr 0.000009	 wd 0.0500	time 0.8587 (0.9277)	loss 1.2728 (1.1001)	grad_norm 3.1033 (3.2630)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:36:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:29:15 lr 0.000009	 wd 0.0500	time 0.7855 (0.9232)	loss 1.2142 (1.0989)	grad_norm 2.7464 (3.7008)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:37:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:27:38 lr 0.000009	 wd 0.0500	time 0.7898 (0.9205)	loss 1.2850 (1.1032)	grad_norm 2.7581 (3.5536)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:39:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:26:03 lr 0.000009	 wd 0.0500	time 0.8536 (0.9187)	loss 1.1758 (1.1062)	grad_norm 2.5807 (3.5497)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:40:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:24:28 lr 0.000009	 wd 0.0500	time 0.8508 (0.9169)	loss 0.8152 (1.1100)	grad_norm 3.8140 (3.4883)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:42:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:22:55 lr 0.000010	 wd 0.0500	time 0.8637 (0.9155)	loss 0.8408 (1.1079)	grad_norm 2.7746 (3.4870)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:43:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:21:21 lr 0.000010	 wd 0.0500	time 0.7816 (0.9142)	loss 1.3536 (1.1069)	grad_norm 2.6869 (3.5290)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:45:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:19:48 lr 0.000010	 wd 0.0500	time 0.8555 (0.9131)	loss 0.9495 (1.1073)	grad_norm 2.8373 (3.4619)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:46:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:18:16 lr 0.000010	 wd 0.0500	time 0.8565 (0.9123)	loss 1.3684 (1.1077)	grad_norm 2.7117 (3.4275)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:48:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:16:44 lr 0.000010	 wd 0.0500	time 0.8575 (0.9117)	loss 1.1638 (1.1090)	grad_norm 1.8554 (3.3807)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:49:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:15:12 lr 0.000010	 wd 0.0500	time 0.8583 (0.9111)	loss 1.3053 (1.1095)	grad_norm 3.2319 (3.3573)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:51:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:13:41 lr 0.000011	 wd 0.0500	time 0.8446 (0.9104)	loss 0.9056 (1.1097)	grad_norm 2.1636 (3.3944)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:52:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:12:09 lr 0.000011	 wd 0.0500	time 0.8580 (0.9100)	loss 1.1639 (1.1097)	grad_norm 2.7368 (3.3757)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:54:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:10:38 lr 0.000011	 wd 0.0500	time 0.8937 (0.9097)	loss 1.1151 (1.1105)	grad_norm 2.5634 (3.3727)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:55:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:09:07 lr 0.000011	 wd 0.0500	time 0.8513 (0.9093)	loss 1.2794 (1.1101)	grad_norm 2.2764 (3.3426)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 11:57:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:07:36 lr 0.000011	 wd 0.0500	time 0.8510 (0.9090)	loss 0.9471 (1.1104)	grad_norm 2.0956 (3.3245)	loss_scale 2048.0000 (1036.2819)	mem 23350MB
[2024-08-04 11:58:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:06:05 lr 0.000011	 wd 0.0500	time 0.8446 (0.9087)	loss 0.9620 (1.1095)	grad_norm 3.3702 (3.3017)	loss_scale 2048.0000 (1084.4360)	mem 23350MB
[2024-08-04 12:00:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:04:34 lr 0.000012	 wd 0.0500	time 0.8631 (0.9084)	loss 1.2274 (1.1100)	grad_norm 2.2264 (3.2922)	loss_scale 2048.0000 (1128.2144)	mem 23350MB
[2024-08-04 12:01:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:03:03 lr 0.000012	 wd 0.0500	time 0.8595 (0.9082)	loss 1.0560 (1.1098)	grad_norm 2.3144 (3.2865)	loss_scale 2048.0000 (1168.1877)	mem 23350MB
[2024-08-04 12:03:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:32 lr 0.000012	 wd 0.0500	time 0.8500 (0.9080)	loss 0.7627 (1.1103)	grad_norm 3.2687 (3.2861)	loss_scale 2048.0000 (1204.8313)	mem 23350MB
[2024-08-04 12:04:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.8463 (0.9077)	loss 1.1504 (1.1103)	grad_norm 2.8005 (3.2667)	loss_scale 2048.0000 (1238.5446)	mem 23350MB
[2024-08-04 12:04:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 2 training takes 0:37:53
[2024-08-04 12:05:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.859 (11.859)	Loss 0.4634 (0.4634)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-04 12:05:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.764 Acc@5 98.034
[2024-08-04 12:05:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 12:05:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 12:05:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:48:03 lr 0.000012	 wd 0.0500	time 12.6632 (12.6632)	loss 0.6597 (0.6597)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:07:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:40:44 lr 0.000012	 wd 0.0500	time 0.8874 (1.0177)	loss 1.2187 (1.1467)	grad_norm 2.5739 (3.0897)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:08:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:36:56 lr 0.000012	 wd 0.0500	time 0.8639 (0.9629)	loss 1.4165 (1.1210)	grad_norm 2.1892 (3.0634)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:10:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:34:36 lr 0.000012	 wd 0.0500	time 0.8578 (0.9428)	loss 1.3812 (1.1219)	grad_norm 2.2755 (3.1892)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:11:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:32:41 lr 0.000013	 wd 0.0500	time 0.8483 (0.9330)	loss 1.3447 (1.1127)	grad_norm 3.3615 (3.2180)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:13:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:30:56 lr 0.000013	 wd 0.0500	time 0.8666 (0.9273)	loss 0.7788 (1.1128)	grad_norm 3.7735 (3.2534)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:14:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:29:15 lr 0.000013	 wd 0.0500	time 0.7842 (0.9231)	loss 1.0239 (1.1050)	grad_norm 3.0436 (3.2992)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:16:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:27:37 lr 0.000013	 wd 0.0500	time 0.8564 (0.9200)	loss 1.3132 (1.1033)	grad_norm 2.6388 (3.2044)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:17:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:26:02 lr 0.000013	 wd 0.0500	time 0.8440 (0.9178)	loss 0.9400 (1.0983)	grad_norm 2.6486 (3.2480)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:19:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:24:27 lr 0.000013	 wd 0.0500	time 0.8593 (0.9162)	loss 1.5193 (1.1008)	grad_norm 3.2617 (3.2140)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:20:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:22:54 lr 0.000014	 wd 0.0500	time 0.8558 (0.9150)	loss 1.2299 (1.1004)	grad_norm 1.9209 (3.2058)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:22:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:21:21 lr 0.000014	 wd 0.0500	time 0.9506 (0.9141)	loss 0.7619 (1.1015)	grad_norm 2.0414 (3.1811)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:23:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:19:49 lr 0.000014	 wd 0.0500	time 0.8560 (0.9133)	loss 1.0991 (1.0993)	grad_norm 2.6695 (3.1366)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:25:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:18:16 lr 0.000014	 wd 0.0500	time 0.8807 (0.9124)	loss 1.1959 (1.0989)	grad_norm 1.9888 (3.1523)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:26:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:16:44 lr 0.000014	 wd 0.0500	time 0.8537 (0.9116)	loss 1.1717 (1.0997)	grad_norm 1.8889 (3.1247)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:28:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:15:12 lr 0.000014	 wd 0.0500	time 0.8564 (0.9109)	loss 1.3299 (1.0982)	grad_norm 2.4733 (3.1121)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:29:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:13:41 lr 0.000015	 wd 0.0500	time 0.8560 (0.9104)	loss 0.6923 (1.0957)	grad_norm 2.7885 (3.1384)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:31:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:12:09 lr 0.000015	 wd 0.0500	time 0.8514 (0.9100)	loss 0.9551 (1.0983)	grad_norm 2.0793 (3.1451)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:32:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:10:38 lr 0.000015	 wd 0.0500	time 0.8708 (0.9097)	loss 1.2063 (1.0997)	grad_norm 2.3269 (3.1369)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:34:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:09:07 lr 0.000015	 wd 0.0500	time 0.8017 (0.9093)	loss 1.3147 (1.0985)	grad_norm 2.1541 (3.1242)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:35:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:07:36 lr 0.000015	 wd 0.0500	time 0.8155 (0.9090)	loss 1.1916 (1.0982)	grad_norm 2.2207 (3.1070)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:37:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:06:05 lr 0.000015	 wd 0.0500	time 0.8569 (0.9087)	loss 0.7820 (1.0983)	grad_norm 2.2950 (3.1251)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:38:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:04:34 lr 0.000016	 wd 0.0500	time 0.8591 (0.9085)	loss 0.7705 (1.0974)	grad_norm 2.3798 (3.1349)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:40:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:03:03 lr 0.000016	 wd 0.0500	time 0.8402 (0.9083)	loss 0.8545 (1.0977)	grad_norm 2.4372 (3.1219)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:41:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:32 lr 0.000016	 wd 0.0500	time 0.8648 (0.9080)	loss 1.2040 (1.0989)	grad_norm 2.8501 (3.1299)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:43:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8537 (0.9077)	loss 1.1599 (1.0989)	grad_norm 2.7265 (3.1222)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:43:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 3 training takes 0:37:53
[2024-08-04 12:43:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.287 (11.287)	Loss 0.4736 (0.4736)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-04 12:43:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.786 Acc@5 98.052
[2024-08-04 12:43:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 12:43:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 12:44:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][0/2502]	eta 9:24:34 lr 0.000016	 wd 0.0500	time 13.5391 (13.5391)	loss 1.1977 (1.1977)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:45:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:41:08 lr 0.000016	 wd 0.0500	time 0.8640 (1.0276)	loss 0.8795 (1.1121)	grad_norm 6.1010 (3.0987)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:47:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:37:02 lr 0.000016	 wd 0.0500	time 0.8537 (0.9653)	loss 1.0720 (1.1007)	grad_norm 2.4251 (3.1624)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:48:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:34:39 lr 0.000016	 wd 0.0500	time 0.8044 (0.9446)	loss 0.7393 (1.0970)	grad_norm 3.1495 (3.0644)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:50:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:32:42 lr 0.000017	 wd 0.0500	time 0.8554 (0.9337)	loss 1.2938 (1.0945)	grad_norm 2.6653 (2.9857)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:51:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:30:56 lr 0.000017	 wd 0.0500	time 0.8863 (0.9274)	loss 1.1569 (1.0951)	grad_norm 3.1219 (3.0240)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:53:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:29:16 lr 0.000017	 wd 0.0500	time 0.9098 (0.9237)	loss 0.9596 (1.0962)	grad_norm 3.6234 (3.1472)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:54:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:27:39 lr 0.000017	 wd 0.0500	time 0.8572 (0.9207)	loss 0.9480 (1.0965)	grad_norm 2.9861 (3.1396)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:56:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:26:03 lr 0.000017	 wd 0.0500	time 0.8542 (0.9184)	loss 0.7184 (1.0983)	grad_norm 3.8972 (3.0901)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:57:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:24:28 lr 0.000017	 wd 0.0500	time 0.8618 (0.9169)	loss 0.7884 (1.0989)	grad_norm 4.7359 (3.1613)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 12:59:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:22:54 lr 0.000018	 wd 0.0500	time 0.8594 (0.9154)	loss 1.3812 (1.0996)	grad_norm 2.3567 (3.1460)	loss_scale 4096.0000 (2105.2867)	mem 23350MB
[2024-08-04 13:00:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:21:21 lr 0.000018	 wd 0.0500	time 0.8533 (0.9142)	loss 1.3801 (1.1008)	grad_norm 2.0347 (3.1416)	loss_scale 4096.0000 (2286.0963)	mem 23350MB
[2024-08-04 13:02:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:19:48 lr 0.000018	 wd 0.0500	time 0.8589 (0.9131)	loss 1.2188 (1.0989)	grad_norm 2.3963 (inf)	loss_scale 2048.0000 (2348.1232)	mem 23350MB
[2024-08-04 13:03:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:18:16 lr 0.000018	 wd 0.0500	time 0.8574 (0.9123)	loss 0.9035 (1.0984)	grad_norm 3.7975 (inf)	loss_scale 2048.0000 (2325.0546)	mem 23350MB
[2024-08-04 13:05:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:16:44 lr 0.000018	 wd 0.0500	time 0.8562 (0.9117)	loss 1.3674 (1.0984)	grad_norm 2.6076 (inf)	loss_scale 2048.0000 (2305.2791)	mem 23350MB
[2024-08-04 13:06:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:15:12 lr 0.000018	 wd 0.0500	time 0.8566 (0.9111)	loss 1.1225 (1.0984)	grad_norm 2.2217 (inf)	loss_scale 2048.0000 (2288.1386)	mem 23350MB
[2024-08-04 13:08:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:13:41 lr 0.000019	 wd 0.0500	time 0.8597 (0.9105)	loss 1.2487 (1.0968)	grad_norm 2.0018 (inf)	loss_scale 2048.0000 (2273.1393)	mem 23350MB
[2024-08-04 13:09:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:12:09 lr 0.000019	 wd 0.0500	time 0.8579 (0.9101)	loss 1.0873 (1.0968)	grad_norm 2.1316 (inf)	loss_scale 2048.0000 (2259.9036)	mem 23350MB
[2024-08-04 13:11:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:10:38 lr 0.000019	 wd 0.0500	time 0.8480 (0.9097)	loss 1.4384 (1.0981)	grad_norm 2.1561 (inf)	loss_scale 2048.0000 (2248.1377)	mem 23350MB
[2024-08-04 13:12:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:09:07 lr 0.000019	 wd 0.0500	time 0.9045 (0.9093)	loss 1.2761 (1.0981)	grad_norm 4.1345 (inf)	loss_scale 2048.0000 (2237.6097)	mem 23350MB
[2024-08-04 13:14:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:07:36 lr 0.000019	 wd 0.0500	time 0.8569 (0.9090)	loss 0.7612 (1.0972)	grad_norm 2.1358 (inf)	loss_scale 2048.0000 (2228.1339)	mem 23350MB
[2024-08-04 13:15:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:06:05 lr 0.000019	 wd 0.0500	time 0.8543 (0.9087)	loss 0.8878 (1.0958)	grad_norm 2.1549 (inf)	loss_scale 2048.0000 (2219.5602)	mem 23350MB
[2024-08-04 13:17:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:04:34 lr 0.000020	 wd 0.0500	time 0.8594 (0.9084)	loss 0.8642 (1.0960)	grad_norm 5.3974 (inf)	loss_scale 2048.0000 (2211.7656)	mem 23350MB
[2024-08-04 13:18:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:03:03 lr 0.000020	 wd 0.0500	time 0.8867 (0.9082)	loss 0.7580 (1.0961)	grad_norm 6.5673 (inf)	loss_scale 2048.0000 (2204.6484)	mem 23350MB
[2024-08-04 13:20:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8569 (0.9080)	loss 0.6778 (1.0954)	grad_norm 2.1251 (inf)	loss_scale 2048.0000 (2198.1241)	mem 23350MB
[2024-08-04 13:21:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8542 (0.9078)	loss 1.0388 (1.0959)	grad_norm 2.5769 (inf)	loss_scale 2048.0000 (2192.1216)	mem 23350MB
[2024-08-04 13:21:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 4 training takes 0:37:53
[2024-08-04 13:22:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.330 (12.330)	Loss 0.4587 (0.4587)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-04 13:22:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.698 Acc@5 98.028
[2024-08-04 13:22:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-04 13:22:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 13:22:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][0/2502]	eta 9:06:15 lr 0.000020	 wd 0.0500	time 13.0997 (13.0997)	loss 1.2838 (1.2838)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:24:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:40:56 lr 0.000020	 wd 0.0500	time 0.8454 (1.0226)	loss 0.9459 (1.1218)	grad_norm 2.5081 (3.1763)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:25:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:36:59 lr 0.000020	 wd 0.0500	time 0.8788 (0.9642)	loss 1.1056 (1.0891)	grad_norm 2.7943 (3.2909)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:27:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:34:37 lr 0.000020	 wd 0.0500	time 0.8659 (0.9436)	loss 0.7682 (1.0827)	grad_norm 2.5897 (3.1710)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:28:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:32:42 lr 0.000020	 wd 0.0500	time 0.8449 (0.9335)	loss 1.3895 (1.0905)	grad_norm 2.6139 (3.1148)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:30:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:30:57 lr 0.000020	 wd 0.0500	time 0.8575 (0.9276)	loss 0.9273 (1.0892)	grad_norm 4.5263 (3.1720)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:31:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:29:15 lr 0.000020	 wd 0.0500	time 0.8629 (0.9230)	loss 0.8424 (1.0893)	grad_norm 2.6585 (3.1486)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:33:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:27:38 lr 0.000020	 wd 0.0500	time 0.8562 (0.9201)	loss 0.8694 (1.0892)	grad_norm 2.1541 (3.1287)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:34:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:26:02 lr 0.000020	 wd 0.0500	time 0.8575 (0.9180)	loss 1.4199 (1.0871)	grad_norm 3.1756 (3.0679)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:36:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:24:28 lr 0.000020	 wd 0.0500	time 0.8483 (0.9167)	loss 0.9603 (1.0863)	grad_norm 2.2376 (3.0614)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:37:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:22:54 lr 0.000020	 wd 0.0500	time 0.8536 (0.9152)	loss 1.0686 (1.0852)	grad_norm 2.7185 (3.0402)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:39:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:21:21 lr 0.000020	 wd 0.0500	time 0.8555 (0.9141)	loss 1.1203 (1.0865)	grad_norm 6.8545 (3.0928)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:40:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:19:48 lr 0.000020	 wd 0.0500	time 0.8879 (0.9130)	loss 1.3805 (1.0857)	grad_norm 2.9429 (3.1280)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:42:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:18:16 lr 0.000020	 wd 0.0500	time 0.8557 (0.9124)	loss 0.7565 (1.0869)	grad_norm 2.6673 (3.1448)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:43:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:16:44 lr 0.000020	 wd 0.0500	time 0.8597 (0.9117)	loss 1.2122 (1.0904)	grad_norm 6.2614 (3.1448)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:45:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:15:12 lr 0.000020	 wd 0.0500	time 0.8617 (0.9111)	loss 0.7025 (1.0921)	grad_norm 3.0667 (3.1611)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:46:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:13:41 lr 0.000020	 wd 0.0500	time 0.8451 (0.9105)	loss 1.2709 (1.0954)	grad_norm 2.2212 (3.1239)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:48:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:12:09 lr 0.000020	 wd 0.0500	time 0.8577 (0.9100)	loss 0.7496 (1.0958)	grad_norm 2.9695 (3.1889)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:49:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:10:38 lr 0.000020	 wd 0.0500	time 0.8659 (0.9097)	loss 0.7978 (1.0934)	grad_norm 4.1860 (3.1742)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:51:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:09:07 lr 0.000020	 wd 0.0500	time 0.8466 (0.9094)	loss 1.0818 (1.0930)	grad_norm 2.4982 (3.1550)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:52:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:07:36 lr 0.000020	 wd 0.0500	time 0.9204 (0.9092)	loss 1.4939 (1.0939)	grad_norm 2.8775 (3.1992)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:54:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:06:05 lr 0.000020	 wd 0.0500	time 0.8663 (0.9089)	loss 1.4528 (1.0939)	grad_norm 2.5077 (3.1912)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:55:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:04:34 lr 0.000020	 wd 0.0500	time 0.8453 (0.9087)	loss 1.2099 (1.0943)	grad_norm 6.3590 (3.1773)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:57:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:03:03 lr 0.000020	 wd 0.0500	time 0.8619 (0.9085)	loss 1.1006 (1.0932)	grad_norm 2.7068 (3.1653)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 13:58:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8490 (0.9083)	loss 0.8643 (1.0928)	grad_norm 2.2106 (nan)	loss_scale 1024.0000 (2022.4107)	mem 23350MB
[2024-08-04 14:00:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8395 (0.9080)	loss 1.3106 (1.0937)	grad_norm 2.5272 (nan)	loss_scale 1024.0000 (1982.4902)	mem 23350MB
[2024-08-04 14:00:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 5 training takes 0:37:54
[2024-08-04 14:00:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.418 (12.418)	Loss 0.4736 (0.4736)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-04 14:00:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.688 Acc@5 98.052
[2024-08-04 14:00:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-04 14:00:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 14:01:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:40:16 lr 0.000020	 wd 0.0500	time 12.4767 (12.4767)	loss 1.1733 (1.1733)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:02:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:40:45 lr 0.000020	 wd 0.0500	time 0.8556 (1.0179)	loss 0.9195 (1.1195)	grad_norm 2.6845 (3.1079)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:04:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:36:52 lr 0.000020	 wd 0.0500	time 0.8858 (0.9609)	loss 0.8683 (1.1016)	grad_norm 3.8610 (3.1098)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:05:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:34:37 lr 0.000020	 wd 0.0500	time 0.8328 (0.9436)	loss 0.8690 (1.0980)	grad_norm 2.9760 (3.0296)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:07:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:32:42 lr 0.000020	 wd 0.0500	time 0.8598 (0.9336)	loss 0.6332 (1.0970)	grad_norm 3.9516 (3.3808)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:08:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:30:56 lr 0.000020	 wd 0.0500	time 0.8602 (0.9276)	loss 1.1878 (1.0972)	grad_norm 2.0789 (3.3538)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:10:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:29:16 lr 0.000020	 wd 0.0500	time 0.8404 (0.9235)	loss 0.8067 (1.0894)	grad_norm 2.0770 (3.3254)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:11:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:27:38 lr 0.000020	 wd 0.0500	time 0.8464 (0.9205)	loss 1.3122 (1.0919)	grad_norm 2.5535 (3.2926)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:13:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:26:02 lr 0.000020	 wd 0.0500	time 0.8510 (0.9182)	loss 0.7212 (1.0967)	grad_norm 2.2664 (3.2667)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:14:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:24:28 lr 0.000020	 wd 0.0500	time 0.8760 (0.9166)	loss 1.5348 (1.0921)	grad_norm 2.2613 (3.2863)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:16:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:22:54 lr 0.000020	 wd 0.0500	time 0.8582 (0.9153)	loss 0.9595 (1.0921)	grad_norm 3.1843 (3.3091)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:17:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:21:21 lr 0.000020	 wd 0.0500	time 0.8569 (0.9142)	loss 0.7447 (1.0878)	grad_norm 2.3591 (3.2851)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:19:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:19:48 lr 0.000020	 wd 0.0500	time 0.8458 (0.9132)	loss 1.2329 (1.0847)	grad_norm 2.3663 (3.2923)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:20:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:18:16 lr 0.000020	 wd 0.0500	time 0.9014 (0.9126)	loss 1.0526 (1.0839)	grad_norm 2.3483 (3.2703)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:22:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:16:45 lr 0.000020	 wd 0.0500	time 0.8503 (0.9120)	loss 0.9326 (1.0833)	grad_norm 2.5155 (3.2986)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:23:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:15:13 lr 0.000020	 wd 0.0500	time 0.8423 (0.9114)	loss 0.6547 (1.0832)	grad_norm 7.1567 (3.2936)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:25:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:13:41 lr 0.000020	 wd 0.0500	time 0.8242 (0.9108)	loss 1.2764 (1.0812)	grad_norm 2.4976 (3.2556)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:26:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:12:10 lr 0.000020	 wd 0.0500	time 0.8802 (0.9104)	loss 1.2952 (1.0796)	grad_norm 2.0319 (3.2437)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:28:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:10:38 lr 0.000020	 wd 0.0500	time 0.8585 (0.9101)	loss 1.1243 (1.0797)	grad_norm 2.0542 (3.2399)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:29:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:09:07 lr 0.000020	 wd 0.0500	time 0.8564 (0.9096)	loss 1.3368 (1.0820)	grad_norm 3.0045 (3.2416)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:31:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:07:36 lr 0.000020	 wd 0.0500	time 0.8569 (0.9093)	loss 0.8797 (1.0826)	grad_norm 2.9040 (3.2569)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:32:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:06:05 lr 0.000020	 wd 0.0500	time 0.8333 (0.9090)	loss 1.0605 (1.0830)	grad_norm 2.4995 (3.2282)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:34:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:04:34 lr 0.000020	 wd 0.0500	time 0.8431 (0.9087)	loss 0.7591 (1.0825)	grad_norm 2.7289 (3.2121)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:35:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:03:03 lr 0.000020	 wd 0.0500	time 0.8428 (0.9086)	loss 0.9085 (1.0845)	grad_norm 5.0427 (3.2419)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:37:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8563 (0.9084)	loss 1.3384 (1.0836)	grad_norm 2.3685 (3.2346)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:38:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8894 (0.9080)	loss 1.3535 (1.0831)	grad_norm 2.5459 (3.2879)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:38:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 6 training takes 0:37:54
[2024-08-04 14:39:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.657 (11.657)	Loss 0.4695 (0.4695)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 23350MB
[2024-08-04 14:39:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.760 Acc@5 98.024
[2024-08-04 14:39:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 14:39:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 14:39:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:56:19 lr 0.000020	 wd 0.0500	time 12.8614 (12.8614)	loss 0.8163 (0.8163)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:41:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:40:59 lr 0.000020	 wd 0.0500	time 0.8548 (1.0239)	loss 1.1606 (1.0654)	grad_norm 2.1995 (3.1649)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:42:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:36:57 lr 0.000020	 wd 0.0500	time 0.8441 (0.9635)	loss 1.0215 (1.0854)	grad_norm 2.6185 (3.1079)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:44:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:34:37 lr 0.000020	 wd 0.0500	time 0.8608 (0.9433)	loss 1.4020 (1.0851)	grad_norm 2.5529 (3.1645)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:45:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:32:41 lr 0.000020	 wd 0.0500	time 0.8586 (0.9331)	loss 0.7681 (1.0888)	grad_norm 4.4603 (3.1465)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:47:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:30:56 lr 0.000020	 wd 0.0500	time 0.8903 (0.9274)	loss 1.3101 (1.0870)	grad_norm 2.1662 (3.1572)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:48:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:29:16 lr 0.000020	 wd 0.0500	time 0.8523 (0.9235)	loss 1.1935 (1.0919)	grad_norm 5.1843 (3.1706)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:50:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:27:39 lr 0.000020	 wd 0.0500	time 0.8568 (0.9211)	loss 1.0011 (1.0864)	grad_norm 2.3258 (3.1577)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:51:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:26:03 lr 0.000020	 wd 0.0500	time 0.8576 (0.9188)	loss 0.8682 (1.0839)	grad_norm 2.7927 (3.1677)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:53:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:24:29 lr 0.000020	 wd 0.0500	time 0.8612 (0.9171)	loss 1.4380 (1.0873)	grad_norm 1.9490 (3.1547)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:54:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:22:55 lr 0.000020	 wd 0.0500	time 0.8628 (0.9159)	loss 0.7862 (1.0873)	grad_norm 3.0866 (3.1528)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:56:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:21:22 lr 0.000020	 wd 0.0500	time 0.7870 (0.9148)	loss 1.1760 (1.0857)	grad_norm 3.4990 (3.1844)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:57:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:19:49 lr 0.000020	 wd 0.0500	time 0.8112 (0.9138)	loss 1.2141 (1.0825)	grad_norm 2.3003 (3.1971)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 14:59:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:18:17 lr 0.000020	 wd 0.0500	time 0.8604 (0.9129)	loss 1.2395 (1.0826)	grad_norm 3.7461 (3.1866)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-04 15:00:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:16:45 lr 0.000019	 wd 0.0500	time 0.8665 (0.9121)	loss 1.1338 (1.0837)	grad_norm 3.0504 (3.1971)	loss_scale 2048.0000 (1070.7780)	mem 23350MB
[2024-08-04 15:02:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:15:13 lr 0.000019	 wd 0.0500	time 0.8620 (0.9115)	loss 0.7607 (1.0847)	grad_norm 2.5195 (3.2006)	loss_scale 2048.0000 (1135.8827)	mem 23350MB
[2024-08-04 15:03:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:13:41 lr 0.000019	 wd 0.0500	time 0.8935 (0.9110)	loss 1.1384 (1.0854)	grad_norm 3.5162 (3.1824)	loss_scale 2048.0000 (1192.8545)	mem 23350MB
[2024-08-04 15:05:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:12:10 lr 0.000019	 wd 0.0500	time 0.7878 (0.9107)	loss 1.1758 (1.0852)	grad_norm 2.0311 (3.1571)	loss_scale 2048.0000 (1243.1276)	mem 23350MB
[2024-08-04 15:06:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:10:39 lr 0.000019	 wd 0.0500	time 0.8422 (0.9103)	loss 0.7024 (1.0873)	grad_norm 2.6075 (3.1523)	loss_scale 2048.0000 (1287.8179)	mem 23350MB
[2024-08-04 15:08:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:09:07 lr 0.000019	 wd 0.0500	time 0.8630 (0.9098)	loss 1.1147 (1.0879)	grad_norm 3.3719 (3.1426)	loss_scale 2048.0000 (1327.8064)	mem 23350MB
[2024-08-04 15:09:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:07:36 lr 0.000019	 wd 0.0500	time 0.8423 (0.9095)	loss 1.1726 (1.0885)	grad_norm 2.6182 (3.2057)	loss_scale 2048.0000 (1363.7981)	mem 23350MB
[2024-08-04 15:11:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:06:05 lr 0.000019	 wd 0.0500	time 0.8431 (0.9092)	loss 1.4380 (1.0909)	grad_norm 2.5125 (3.2058)	loss_scale 2048.0000 (1396.3636)	mem 23350MB
[2024-08-04 15:12:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:04:34 lr 0.000019	 wd 0.0500	time 0.8442 (0.9089)	loss 1.1903 (1.0890)	grad_norm 3.6473 (3.1967)	loss_scale 2048.0000 (1425.9700)	mem 23350MB
[2024-08-04 15:14:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:03:03 lr 0.000019	 wd 0.0500	time 0.8587 (0.9087)	loss 1.3370 (1.0884)	grad_norm 2.5140 (3.1859)	loss_scale 2048.0000 (1453.0030)	mem 23350MB
[2024-08-04 15:15:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:32 lr 0.000019	 wd 0.0500	time 0.8477 (0.9084)	loss 1.2899 (1.0885)	grad_norm 2.3119 (3.1720)	loss_scale 2048.0000 (1477.7843)	mem 23350MB
[2024-08-04 15:17:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8000 (0.9082)	loss 1.4153 (1.0879)	grad_norm 2.0397 (3.1618)	loss_scale 2048.0000 (1500.5838)	mem 23350MB
[2024-08-04 15:17:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 7 training takes 0:37:54
[2024-08-04 15:17:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.830 (12.830)	Loss 0.4482 (0.4482)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-04 15:17:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.672 Acc@5 98.022
[2024-08-04 15:17:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-04 15:17:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 15:18:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][0/2502]	eta 9:17:50 lr 0.000019	 wd 0.0500	time 13.3777 (13.3777)	loss 1.1834 (1.1834)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:19:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:41:10 lr 0.000019	 wd 0.0500	time 0.8575 (1.0287)	loss 0.8429 (1.1385)	grad_norm 2.7132 (3.0011)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:21:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:37:04 lr 0.000019	 wd 0.0500	time 0.8491 (0.9663)	loss 0.7616 (1.1165)	grad_norm 3.6180 (2.9885)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:22:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:34:41 lr 0.000019	 wd 0.0500	time 0.8638 (0.9455)	loss 1.3905 (1.1023)	grad_norm 2.3730 (3.0978)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:24:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:32:45 lr 0.000019	 wd 0.0500	time 0.8426 (0.9350)	loss 0.8183 (1.0997)	grad_norm 4.1798 (3.1288)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:25:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:30:59 lr 0.000019	 wd 0.0500	time 0.8608 (0.9286)	loss 0.7255 (1.0901)	grad_norm 2.7664 (3.1311)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:27:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:29:18 lr 0.000019	 wd 0.0500	time 0.8758 (0.9245)	loss 0.7748 (1.0904)	grad_norm 2.9694 (3.2255)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:28:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:27:40 lr 0.000019	 wd 0.0500	time 0.8626 (0.9212)	loss 1.2171 (1.0903)	grad_norm 2.8078 (3.4089)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:30:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:26:04 lr 0.000019	 wd 0.0500	time 0.9034 (0.9192)	loss 1.2632 (1.0923)	grad_norm 3.3799 (3.3793)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:31:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:24:29 lr 0.000019	 wd 0.0500	time 0.8455 (0.9174)	loss 0.8750 (1.0886)	grad_norm 7.9895 (3.4267)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:33:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:22:56 lr 0.000019	 wd 0.0500	time 0.8573 (0.9161)	loss 1.2865 (1.0906)	grad_norm 2.3352 (3.3987)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:34:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:21:22 lr 0.000019	 wd 0.0500	time 0.8564 (0.9151)	loss 1.2314 (1.0879)	grad_norm 2.1711 (3.3800)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:36:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:19:50 lr 0.000019	 wd 0.0500	time 0.8490 (0.9141)	loss 1.0211 (1.0887)	grad_norm 2.7890 (3.4006)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:37:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:18:17 lr 0.000019	 wd 0.0500	time 0.8611 (0.9135)	loss 0.7850 (1.0885)	grad_norm 17.9871 (3.3882)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:39:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:16:45 lr 0.000019	 wd 0.0500	time 0.8468 (0.9127)	loss 1.2562 (1.0898)	grad_norm 7.2710 (3.3496)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:40:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:15:13 lr 0.000019	 wd 0.0500	time 0.8473 (0.9121)	loss 1.0095 (1.0907)	grad_norm 3.7039 (3.3404)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:42:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:13:42 lr 0.000019	 wd 0.0500	time 0.8564 (0.9116)	loss 1.2810 (1.0925)	grad_norm 4.2861 (3.3357)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:43:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:12:10 lr 0.000019	 wd 0.0500	time 0.8616 (0.9111)	loss 0.7274 (1.0946)	grad_norm 2.5932 (3.3082)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:45:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:10:39 lr 0.000019	 wd 0.0500	time 0.8436 (0.9107)	loss 0.7256 (1.0929)	grad_norm 3.1951 (3.2827)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:46:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:09:07 lr 0.000019	 wd 0.0500	time 0.8597 (0.9102)	loss 1.2066 (1.0945)	grad_norm 4.6278 (3.2722)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:48:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:07:36 lr 0.000019	 wd 0.0500	time 0.8657 (0.9100)	loss 0.7257 (1.0934)	grad_norm 4.6407 (3.2616)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:49:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:06:05 lr 0.000019	 wd 0.0500	time 0.8455 (0.9097)	loss 0.8653 (1.0941)	grad_norm 3.1330 (3.2378)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:51:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:04:34 lr 0.000019	 wd 0.0500	time 0.8616 (0.9095)	loss 1.1444 (1.0929)	grad_norm 3.2309 (3.2668)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:52:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:03:03 lr 0.000019	 wd 0.0500	time 0.8493 (0.9092)	loss 1.2222 (1.0914)	grad_norm 3.5028 (3.3227)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:54:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:32 lr 0.000019	 wd 0.0500	time 0.8467 (0.9089)	loss 1.0148 (1.0915)	grad_norm 4.6490 (3.3202)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:55:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8578 (0.9086)	loss 0.7582 (1.0912)	grad_norm 2.4167 (3.3058)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:55:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 8 training takes 0:37:55
[2024-08-04 15:56:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.276 (12.276)	Loss 0.4531 (0.4531)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-04 15:56:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.776 Acc@5 98.078
[2024-08-04 15:56:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 15:56:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.84%
[2024-08-04 15:56:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][0/2502]	eta 8:54:46 lr 0.000019	 wd 0.0500	time 12.8242 (12.8242)	loss 1.2658 (1.2658)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:58:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:40:58 lr 0.000019	 wd 0.0500	time 0.8539 (1.0235)	loss 1.1827 (1.0735)	grad_norm 3.4096 (3.2965)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 15:59:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:36:59 lr 0.000019	 wd 0.0500	time 0.8584 (0.9643)	loss 0.8772 (1.0963)	grad_norm 2.3747 (3.1353)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-04 16:43:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/config.json
[2024-08-04 16:43:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_sequence_stage3
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-04 16:43:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_sequence_stage3.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_sequence_stage3", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-04 16:43:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft
[2024-08-04 16:43:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-04 16:43:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 113): INFO number of params: 80620264
[2024-08-04 16:43:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 148): INFO auto resuming from pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth
[2024-08-04 16:43:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 19): INFO ==============> Resuming form pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth....................
[2024-08-04 16:43:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 26): INFO <All keys matched successfully>
[2024-08-04 16:43:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 36): INFO => loaded successfully 'pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth' (epoch 0)
[2024-08-04 16:43:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 13.704 (13.704)	Loss 0.4802 (0.4802)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 3364MB
[2024-08-04 16:43:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.866 Acc@5 98.066
[2024-08-04 16:43:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 155): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-04 16:43:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 168): INFO Start training
[2024-08-04 16:44:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][0/2502]	eta 9:39:58 lr 0.000004	 wd 0.0500	time 13.9083 (13.9083)	loss 1.1842 (1.1842)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:45:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:41:11 lr 0.000004	 wd 0.0500	time 0.8476 (1.0288)	loss 1.1446 (1.1620)	grad_norm 2.4825 (3.0652)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:47:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:36:58 lr 0.000004	 wd 0.0500	time 0.8377 (0.9637)	loss 0.8941 (1.1481)	grad_norm 2.0009 (2.9136)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:48:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:34:34 lr 0.000004	 wd 0.0500	time 0.8494 (0.9423)	loss 0.8538 (1.1077)	grad_norm 2.9072 (2.9340)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:50:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:32:38 lr 0.000005	 wd 0.0500	time 0.8540 (0.9317)	loss 0.6750 (1.1151)	grad_norm 2.7617 (2.9355)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:51:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:30:52 lr 0.000005	 wd 0.0500	time 0.8520 (0.9254)	loss 1.2806 (1.1158)	grad_norm 2.3716 (3.0074)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:53:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:29:12 lr 0.000005	 wd 0.0500	time 0.8370 (0.9212)	loss 1.4608 (1.1155)	grad_norm 2.6988 (3.1473)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:54:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:27:34 lr 0.000005	 wd 0.0500	time 0.7850 (0.9179)	loss 1.3560 (1.1195)	grad_norm 2.8763 (3.1985)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:56:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:25:58 lr 0.000005	 wd 0.0500	time 0.8977 (0.9157)	loss 1.0676 (1.1190)	grad_norm 3.7970 (3.2265)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:57:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:24:24 lr 0.000005	 wd 0.0500	time 0.8393 (0.9142)	loss 1.3687 (1.1193)	grad_norm 2.9790 (3.2045)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 16:59:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:22:51 lr 0.000006	 wd 0.0500	time 0.8418 (0.9129)	loss 1.4812 (1.1177)	grad_norm 2.3620 (3.1727)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:00:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:21:18 lr 0.000006	 wd 0.0500	time 0.8536 (0.9117)	loss 1.4204 (1.1207)	grad_norm 5.2680 (3.1534)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:02:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:19:45 lr 0.000006	 wd 0.0500	time 0.8596 (0.9105)	loss 0.8800 (1.1195)	grad_norm 2.3906 (3.1346)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:03:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:18:13 lr 0.000006	 wd 0.0500	time 0.8347 (0.9097)	loss 1.2369 (1.1207)	grad_norm 3.4472 (3.1531)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:05:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:16:41 lr 0.000006	 wd 0.0500	time 0.8503 (0.9089)	loss 1.2585 (1.1226)	grad_norm 2.1062 (3.1543)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:06:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:15:10 lr 0.000006	 wd 0.0500	time 0.8473 (0.9084)	loss 0.9069 (1.1248)	grad_norm 2.4266 (3.1653)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:08:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:13:38 lr 0.000007	 wd 0.0500	time 0.8317 (0.9078)	loss 0.9775 (1.1223)	grad_norm 2.0503 (3.1372)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:09:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:12:07 lr 0.000007	 wd 0.0500	time 0.7841 (0.9074)	loss 0.8600 (1.1216)	grad_norm 26.0713 (3.1697)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:11:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:10:36 lr 0.000007	 wd 0.0500	time 0.8372 (0.9071)	loss 1.2610 (1.1215)	grad_norm 2.2899 (3.1688)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:12:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:09:05 lr 0.000007	 wd 0.0500	time 0.8490 (0.9065)	loss 1.4644 (1.1215)	grad_norm 2.0543 (3.1544)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:14:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:07:34 lr 0.000007	 wd 0.0500	time 0.8988 (0.9062)	loss 0.8312 (1.1201)	grad_norm 2.1137 (3.1595)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:15:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:06:04 lr 0.000007	 wd 0.0500	time 0.8860 (0.9059)	loss 0.8618 (1.1190)	grad_norm 2.3460 (3.1389)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:17:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:04:33 lr 0.000008	 wd 0.0500	time 0.8536 (0.9056)	loss 1.3154 (1.1192)	grad_norm 1.8574 (3.1393)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:18:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.8429 (0.9053)	loss 1.4092 (1.1204)	grad_norm 2.5262 (3.1353)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:20:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.8511 (0.9050)	loss 1.2587 (1.1200)	grad_norm 2.4734 (3.1280)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:21:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.8358 (0.9047)	loss 0.8855 (1.1188)	grad_norm 2.6257 (3.1068)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:21:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 1 training takes 0:37:45
[2024-08-04 17:21:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.828 (11.828)	Loss 0.4866 (0.4866)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-04 17:22:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.898 Acc@5 98.084
[2024-08-04 17:22:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-04 17:22:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-04 17:22:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-04 17:22:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-04 17:22:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:28:48 lr 0.000008	 wd 0.0500	time 10.7630 (10.7630)	loss 1.2770 (1.2770)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:23:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:39:55 lr 0.000008	 wd 0.0500	time 0.8450 (0.9973)	loss 1.2918 (1.0931)	grad_norm 2.7280 (3.3990)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:25:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:36:23 lr 0.000008	 wd 0.0500	time 0.8593 (0.9486)	loss 0.9301 (1.0995)	grad_norm 2.8516 (3.8635)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:26:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:34:14 lr 0.000008	 wd 0.0500	time 0.8553 (0.9331)	loss 0.7635 (1.1038)	grad_norm 2.0053 (3.5449)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:28:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:32:23 lr 0.000009	 wd 0.0500	time 0.8291 (0.9245)	loss 0.9960 (1.1219)	grad_norm 2.3247 (3.3965)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:29:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:30:41 lr 0.000009	 wd 0.0500	time 0.8542 (0.9196)	loss 1.2755 (1.1229)	grad_norm 3.3735 (3.4355)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 17:31:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:29:03 lr 0.000009	 wd 0.0500	time 0.8417 (0.9165)	loss 1.3646 (1.1225)	grad_norm 2.9099 (3.3327)	loss_scale 4096.0000 (2286.5358)	mem 23977MB
[2024-08-04 17:32:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:27:26 lr 0.000009	 wd 0.0500	time 0.8503 (0.9138)	loss 0.7913 (1.1194)	grad_norm 4.0262 (3.2682)	loss_scale 4096.0000 (2544.6619)	mem 23977MB
[2024-08-04 17:34:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:25:52 lr 0.000009	 wd 0.0500	time 0.8401 (0.9122)	loss 1.2807 (1.1170)	grad_norm 3.0728 (3.2595)	loss_scale 4096.0000 (2738.3371)	mem 23977MB
[2024-08-04 17:35:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:24:18 lr 0.000009	 wd 0.0500	time 0.8448 (0.9106)	loss 1.1023 (1.1190)	grad_norm 3.4648 (nan)	loss_scale 2048.0000 (2666.2642)	mem 23977MB
[2024-08-04 17:37:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:22:46 lr 0.000010	 wd 0.0500	time 0.8518 (0.9095)	loss 1.4152 (1.1172)	grad_norm 2.3626 (nan)	loss_scale 2048.0000 (2604.4995)	mem 23977MB
[2024-08-04 17:38:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:21:14 lr 0.000010	 wd 0.0500	time 0.8388 (0.9087)	loss 1.3341 (1.1145)	grad_norm 2.2060 (nan)	loss_scale 2048.0000 (2553.9546)	mem 23977MB
[2024-08-04 17:40:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:19:42 lr 0.000010	 wd 0.0500	time 0.8533 (0.9081)	loss 0.9872 (1.1123)	grad_norm 2.3753 (nan)	loss_scale 2048.0000 (2511.8268)	mem 23977MB
[2024-08-04 17:41:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:18:10 lr 0.000010	 wd 0.0500	time 0.8474 (0.9075)	loss 1.4244 (1.1134)	grad_norm 4.3544 (nan)	loss_scale 2048.0000 (2476.1752)	mem 23977MB
[2024-08-04 17:43:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:16:39 lr 0.000010	 wd 0.0500	time 0.8360 (0.9070)	loss 0.9869 (1.1140)	grad_norm 5.2501 (nan)	loss_scale 2048.0000 (2445.6131)	mem 23977MB
[2024-08-04 17:44:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:15:08 lr 0.000010	 wd 0.0500	time 0.8538 (0.9064)	loss 1.4193 (1.1143)	grad_norm 5.1704 (nan)	loss_scale 1024.0000 (2394.5636)	mem 23977MB
[2024-08-04 17:46:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:13:37 lr 0.000011	 wd 0.0500	time 0.8578 (0.9062)	loss 1.3679 (1.1158)	grad_norm 3.0458 (nan)	loss_scale 1024.0000 (2308.9569)	mem 23977MB
[2024-08-04 17:47:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:12:06 lr 0.000011	 wd 0.0500	time 0.8363 (0.9058)	loss 0.8887 (1.1169)	grad_norm 2.7955 (nan)	loss_scale 1024.0000 (2233.4156)	mem 23977MB
[2024-08-04 17:49:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:10:35 lr 0.000011	 wd 0.0500	time 0.8330 (0.9054)	loss 1.2254 (1.1160)	grad_norm 3.0655 (nan)	loss_scale 1024.0000 (2166.2632)	mem 23977MB
[2024-08-04 17:50:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:09:04 lr 0.000011	 wd 0.0500	time 0.8355 (0.9051)	loss 1.3883 (1.1178)	grad_norm 2.3530 (nan)	loss_scale 1024.0000 (2106.1757)	mem 23977MB
[2024-08-04 17:52:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:07:34 lr 0.000011	 wd 0.0500	time 0.8452 (0.9047)	loss 1.3172 (1.1186)	grad_norm 6.1160 (nan)	loss_scale 1024.0000 (2052.0940)	mem 23977MB
[2024-08-04 17:53:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:06:03 lr 0.000011	 wd 0.0500	time 0.8825 (0.9045)	loss 1.3379 (1.1210)	grad_norm 2.4568 (nan)	loss_scale 1024.0000 (2003.1604)	mem 23977MB
[2024-08-04 17:55:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:04:33 lr 0.000012	 wd 0.0500	time 0.8440 (0.9044)	loss 1.3475 (1.1209)	grad_norm 3.0448 (nan)	loss_scale 1024.0000 (1958.6733)	mem 23977MB
[2024-08-04 17:56:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:03:02 lr 0.000012	 wd 0.0500	time 0.8485 (0.9041)	loss 0.8400 (1.1230)	grad_norm 2.3647 (nan)	loss_scale 1024.0000 (1918.0530)	mem 23977MB
[2024-08-04 17:58:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:32 lr 0.000012	 wd 0.0500	time 0.8498 (0.9039)	loss 1.2469 (1.1232)	grad_norm 2.1955 (nan)	loss_scale 1024.0000 (1880.8163)	mem 23977MB
[2024-08-04 17:59:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.8802 (0.9037)	loss 1.0395 (1.1232)	grad_norm 1.9272 (nan)	loss_scale 1024.0000 (1846.5574)	mem 23977MB
[2024-08-04 17:59:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 2 training takes 0:37:43
[2024-08-04 18:00:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.970 (11.970)	Loss 0.4761 (0.4761)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 23977MB
[2024-08-04 18:00:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.842 Acc@5 98.088
[2024-08-04 18:00:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 18:00:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-04 18:00:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:43:36 lr 0.000012	 wd 0.0500	time 11.1177 (11.1177)	loss 1.1422 (1.1422)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:02:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:40:15 lr 0.000012	 wd 0.0500	time 0.8321 (1.0057)	loss 1.4154 (1.1405)	grad_norm 2.1551 (3.6161)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:03:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:36:30 lr 0.000012	 wd 0.0500	time 0.8520 (0.9518)	loss 1.4136 (1.1277)	grad_norm 2.1112 (3.1694)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:05:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:34:19 lr 0.000012	 wd 0.0500	time 0.8428 (0.9351)	loss 1.3106 (1.1135)	grad_norm 2.0826 (3.2011)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:06:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:32:26 lr 0.000013	 wd 0.0500	time 0.8554 (0.9259)	loss 1.5843 (1.1169)	grad_norm 2.1429 (3.0855)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:08:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:30:42 lr 0.000013	 wd 0.0500	time 0.8333 (0.9204)	loss 1.2769 (1.1142)	grad_norm 2.6835 (3.1378)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:09:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:29:04 lr 0.000013	 wd 0.0500	time 1.0375 (0.9173)	loss 1.0895 (1.1171)	grad_norm 4.1115 (3.1563)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:11:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:27:28 lr 0.000013	 wd 0.0500	time 0.8433 (0.9146)	loss 1.4869 (1.1203)	grad_norm 3.2280 (3.1822)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:12:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:25:53 lr 0.000013	 wd 0.0500	time 0.8894 (0.9128)	loss 1.3286 (1.1230)	grad_norm 2.6252 (3.2217)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:14:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:24:19 lr 0.000013	 wd 0.0500	time 0.8458 (0.9113)	loss 0.9894 (1.1264)	grad_norm 2.6392 (3.1902)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:15:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:22:46 lr 0.000014	 wd 0.0500	time 0.8460 (0.9101)	loss 0.8472 (1.1244)	grad_norm 2.2828 (3.1765)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:17:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:21:14 lr 0.000014	 wd 0.0500	time 0.8538 (0.9092)	loss 1.3742 (1.1229)	grad_norm 3.7439 (3.1655)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:18:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:19:42 lr 0.000014	 wd 0.0500	time 0.8521 (0.9083)	loss 0.7952 (1.1230)	grad_norm 2.4014 (3.1274)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:20:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:18:10 lr 0.000014	 wd 0.0500	time 0.8570 (0.9076)	loss 1.2906 (1.1234)	grad_norm 1.9290 (3.1489)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:21:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:16:39 lr 0.000014	 wd 0.0500	time 0.8651 (0.9072)	loss 1.2120 (1.1253)	grad_norm 4.5777 (3.1298)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:23:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:15:08 lr 0.000014	 wd 0.0500	time 0.8301 (0.9066)	loss 1.2106 (1.1254)	grad_norm 2.4016 (3.1073)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:24:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:13:37 lr 0.000015	 wd 0.0500	time 0.8526 (0.9062)	loss 0.8213 (1.1246)	grad_norm 1.9922 (3.0970)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:26:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:12:06 lr 0.000015	 wd 0.0500	time 0.8315 (0.9057)	loss 1.2325 (1.1251)	grad_norm 2.3122 (3.0755)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:27:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:10:35 lr 0.000015	 wd 0.0500	time 0.8522 (0.9052)	loss 1.2383 (1.1261)	grad_norm 4.1574 (3.0662)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:29:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:09:04 lr 0.000015	 wd 0.0500	time 0.8663 (0.9048)	loss 1.4472 (1.1249)	grad_norm 2.6232 (3.0971)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:30:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:07:33 lr 0.000015	 wd 0.0500	time 0.8512 (0.9044)	loss 1.0761 (1.1251)	grad_norm 1.8872 (3.0726)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:32:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:06:03 lr 0.000015	 wd 0.0500	time 0.8450 (0.9041)	loss 1.0161 (1.1239)	grad_norm 2.7816 (3.0783)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:33:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:04:32 lr 0.000016	 wd 0.0500	time 0.8324 (0.9039)	loss 1.1490 (1.1242)	grad_norm 2.6638 (3.0775)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:35:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:03:02 lr 0.000016	 wd 0.0500	time 0.8368 (0.9037)	loss 1.2266 (1.1241)	grad_norm 2.4866 (3.0798)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:36:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:32 lr 0.000016	 wd 0.0500	time 0.8464 (0.9035)	loss 0.6301 (1.1245)	grad_norm 2.4987 (3.0786)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:38:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8469 (0.9032)	loss 1.2132 (1.1249)	grad_norm 10.5245 (3.0653)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:38:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 3 training takes 0:37:42
[2024-08-04 18:38:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.350 (12.350)	Loss 0.4524 (0.4524)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-04 18:38:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.848 Acc@5 98.064
[2024-08-04 18:38:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 18:38:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-04 18:39:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][0/2502]	eta 8:19:57 lr 0.000016	 wd 0.0500	time 11.9894 (11.9894)	loss 0.8235 (0.8235)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:40:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:40:36 lr 0.000016	 wd 0.0500	time 0.8457 (1.0145)	loss 1.1505 (1.1531)	grad_norm 2.6831 (3.2532)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:42:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:36:44 lr 0.000016	 wd 0.0500	time 0.8521 (0.9576)	loss 1.3432 (1.1348)	grad_norm 2.5840 (3.1304)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:43:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:34:25 lr 0.000016	 wd 0.0500	time 0.8383 (0.9379)	loss 1.2773 (1.1296)	grad_norm 2.4186 (3.0941)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:45:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:32:31 lr 0.000017	 wd 0.0500	time 0.8468 (0.9286)	loss 1.3514 (1.1228)	grad_norm 2.7429 (3.0001)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 18:46:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:30:47 lr 0.000017	 wd 0.0500	time 0.8489 (0.9227)	loss 0.8474 (1.1218)	grad_norm 24.6462 (3.2992)	loss_scale 2048.0000 (1105.7565)	mem 23977MB
[2024-08-04 18:48:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:29:06 lr 0.000017	 wd 0.0500	time 0.8452 (0.9183)	loss 1.0718 (1.1171)	grad_norm 2.9233 (3.3507)	loss_scale 2048.0000 (1262.5358)	mem 23977MB
[2024-08-04 18:49:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:27:29 lr 0.000017	 wd 0.0500	time 0.8469 (0.9155)	loss 1.4617 (1.1137)	grad_norm 2.3428 (3.3163)	loss_scale 2048.0000 (1374.5849)	mem 23977MB
[2024-08-04 18:51:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:25:55 lr 0.000017	 wd 0.0500	time 0.8423 (0.9137)	loss 0.8073 (1.1080)	grad_norm 2.5827 (nan)	loss_scale 1024.0000 (1384.5094)	mem 23977MB
[2024-08-04 18:52:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:24:21 lr 0.000017	 wd 0.0500	time 0.9105 (0.9121)	loss 1.4050 (1.1097)	grad_norm 3.3873 (nan)	loss_scale 1024.0000 (1344.4972)	mem 23977MB
[2024-08-04 18:54:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:22:48 lr 0.000018	 wd 0.0500	time 0.8565 (0.9111)	loss 1.3007 (1.1095)	grad_norm 2.4250 (nan)	loss_scale 1024.0000 (1312.4795)	mem 23977MB
[2024-08-04 18:55:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:21:15 lr 0.000018	 wd 0.0500	time 0.8619 (0.9100)	loss 0.8434 (1.1102)	grad_norm 3.3790 (nan)	loss_scale 1024.0000 (1286.2779)	mem 23977MB
[2024-08-04 18:57:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:19:43 lr 0.000018	 wd 0.0500	time 0.8406 (0.9091)	loss 1.1770 (1.1095)	grad_norm 2.3451 (nan)	loss_scale 1024.0000 (1264.4396)	mem 23977MB
[2024-08-04 18:58:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:18:11 lr 0.000018	 wd 0.0500	time 0.8881 (0.9081)	loss 1.2905 (1.1077)	grad_norm 2.2440 (nan)	loss_scale 1024.0000 (1245.9585)	mem 23977MB
[2024-08-04 19:00:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:16:40 lr 0.000018	 wd 0.0500	time 0.8469 (0.9076)	loss 1.1406 (1.1075)	grad_norm 1.7610 (nan)	loss_scale 1024.0000 (1230.1156)	mem 23977MB
[2024-08-04 19:01:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:15:08 lr 0.000018	 wd 0.0500	time 0.8527 (0.9070)	loss 1.2023 (1.1058)	grad_norm 2.2905 (nan)	loss_scale 1024.0000 (1216.3837)	mem 23977MB
[2024-08-04 19:03:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:13:37 lr 0.000019	 wd 0.0500	time 0.8543 (0.9065)	loss 0.8521 (1.1049)	grad_norm 2.5959 (nan)	loss_scale 1024.0000 (1204.3673)	mem 23977MB
[2024-08-04 19:04:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:12:06 lr 0.000019	 wd 0.0500	time 0.8448 (0.9061)	loss 1.0698 (1.1064)	grad_norm 2.0047 (nan)	loss_scale 1024.0000 (1193.7637)	mem 23977MB
[2024-08-04 19:05:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:10:35 lr 0.000019	 wd 0.0500	time 0.8566 (0.9057)	loss 1.1536 (1.1087)	grad_norm 3.8777 (nan)	loss_scale 1024.0000 (1184.3376)	mem 23977MB
[2024-08-04 19:07:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:09:04 lr 0.000019	 wd 0.0500	time 0.8494 (0.9052)	loss 1.2255 (1.1078)	grad_norm 1.9904 (nan)	loss_scale 1024.0000 (1175.9032)	mem 23977MB
[2024-08-04 19:08:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:07:34 lr 0.000019	 wd 0.0500	time 0.8498 (0.9049)	loss 1.0293 (1.1075)	grad_norm 2.5041 (nan)	loss_scale 1024.0000 (1168.3118)	mem 23977MB
[2024-08-04 19:10:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:06:03 lr 0.000019	 wd 0.0500	time 0.8397 (0.9046)	loss 0.8127 (1.1088)	grad_norm 2.7536 (nan)	loss_scale 1024.0000 (1161.4431)	mem 23977MB
[2024-08-04 19:11:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:04:33 lr 0.000020	 wd 0.0500	time 0.8398 (0.9044)	loss 0.7270 (1.1078)	grad_norm 2.2861 (nan)	loss_scale 1024.0000 (1155.1985)	mem 23977MB
[2024-08-04 19:13:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:03:02 lr 0.000020	 wd 0.0500	time 0.8520 (0.9041)	loss 0.7744 (1.1081)	grad_norm 2.4741 (nan)	loss_scale 1024.0000 (1149.4967)	mem 23977MB
[2024-08-04 19:14:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8532 (0.9039)	loss 1.2600 (1.1102)	grad_norm 4.2908 (nan)	loss_scale 1024.0000 (1144.2699)	mem 23977MB
[2024-08-04 19:16:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8563 (0.9038)	loss 1.1503 (1.1102)	grad_norm 2.9427 (nan)	loss_scale 1024.0000 (1139.4610)	mem 23977MB
[2024-08-04 19:16:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 4 training takes 0:37:43
[2024-08-04 19:16:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.613 (12.613)	Loss 0.4570 (0.4570)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-04 19:17:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.746 Acc@5 98.082
[2024-08-04 19:17:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-04 19:17:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-04 19:17:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:11:18 lr 0.000020	 wd 0.0500	time 11.7818 (11.7818)	loss 1.2952 (1.2952)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:18:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:40:19 lr 0.000020	 wd 0.0500	time 0.8361 (1.0071)	loss 0.8110 (1.1389)	grad_norm 4.1134 (2.9087)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:20:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:36:33 lr 0.000020	 wd 0.0500	time 0.8573 (0.9528)	loss 0.8914 (1.1170)	grad_norm 2.2902 (2.9615)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:21:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:34:18 lr 0.000020	 wd 0.0500	time 0.8589 (0.9348)	loss 0.7924 (1.1170)	grad_norm 3.6518 (3.0936)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:23:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:32:27 lr 0.000020	 wd 0.0500	time 0.8482 (0.9266)	loss 1.2391 (1.1120)	grad_norm 2.0691 (2.9722)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:24:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:30:44 lr 0.000020	 wd 0.0500	time 0.8523 (0.9215)	loss 1.1947 (1.1122)	grad_norm 2.7151 (3.0185)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:26:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:29:05 lr 0.000020	 wd 0.0500	time 0.8532 (0.9179)	loss 1.1457 (1.1168)	grad_norm 2.1746 (3.0394)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:27:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:27:29 lr 0.000020	 wd 0.0500	time 0.7887 (0.9154)	loss 0.6579 (1.1168)	grad_norm 2.6636 (3.0306)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:29:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:25:54 lr 0.000020	 wd 0.0500	time 0.8441 (0.9136)	loss 0.8735 (1.1203)	grad_norm 2.3580 (3.0183)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:30:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:24:20 lr 0.000020	 wd 0.0500	time 0.8390 (0.9118)	loss 0.8282 (1.1207)	grad_norm 2.4261 (3.0071)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:32:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:22:47 lr 0.000020	 wd 0.0500	time 0.8544 (0.9106)	loss 1.4101 (1.1203)	grad_norm 1.9771 (2.9723)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:33:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:21:15 lr 0.000020	 wd 0.0500	time 0.9006 (0.9094)	loss 1.3060 (1.1203)	grad_norm 2.2462 (2.9510)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:35:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:19:43 lr 0.000020	 wd 0.0500	time 0.8471 (0.9088)	loss 1.2493 (1.1183)	grad_norm 2.1175 (2.9301)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:36:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:18:11 lr 0.000020	 wd 0.0500	time 0.8445 (0.9080)	loss 1.0462 (1.1193)	grad_norm 4.5927 (2.9112)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:38:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:16:40 lr 0.000020	 wd 0.0500	time 0.8048 (0.9078)	loss 1.4391 (1.1192)	grad_norm 2.9926 (2.8787)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:39:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:15:09 lr 0.000020	 wd 0.0500	time 0.8507 (0.9072)	loss 1.1505 (1.1193)	grad_norm 2.2987 (2.8875)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:41:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:13:37 lr 0.000020	 wd 0.0500	time 0.8524 (0.9067)	loss 1.3901 (1.1190)	grad_norm 2.3893 (2.8904)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:42:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:12:06 lr 0.000020	 wd 0.0500	time 0.8578 (0.9063)	loss 1.0458 (1.1197)	grad_norm 2.4005 (2.8944)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:44:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:10:35 lr 0.000020	 wd 0.0500	time 0.8879 (0.9058)	loss 1.3265 (1.1202)	grad_norm 2.0777 (2.8820)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:45:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:09:05 lr 0.000020	 wd 0.0500	time 0.8501 (0.9054)	loss 1.2525 (1.1202)	grad_norm 2.4539 (2.9260)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:47:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:07:34 lr 0.000020	 wd 0.0500	time 0.8473 (0.9049)	loss 1.0273 (1.1196)	grad_norm 2.1808 (2.9081)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:48:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:06:03 lr 0.000020	 wd 0.0500	time 0.8468 (0.9047)	loss 0.9263 (1.1186)	grad_norm 2.1979 (2.8978)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:50:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:04:33 lr 0.000020	 wd 0.0500	time 0.8474 (0.9045)	loss 0.7993 (1.1190)	grad_norm 2.7642 (2.8926)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-04 19:51:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:03:02 lr 0.000020	 wd 0.0500	time 0.8356 (0.9042)	loss 0.7635 (1.1185)	grad_norm 2.5962 (2.8844)	loss_scale 2048.0000 (1050.7014)	mem 23977MB
[2024-08-04 19:53:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8589 (0.9041)	loss 0.9025 (1.1177)	grad_norm 3.7328 (2.8815)	loss_scale 2048.0000 (1092.2382)	mem 23977MB
[2024-08-04 19:54:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8586 (0.9039)	loss 1.3711 (1.1185)	grad_norm 2.2707 (2.9145)	loss_scale 2048.0000 (1130.4534)	mem 23977MB
[2024-08-04 19:54:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 5 training takes 0:37:44
[2024-08-04 19:55:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.987 (11.987)	Loss 0.4517 (0.4517)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-04 19:55:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.738 Acc@5 98.054
[2024-08-04 19:55:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-04 19:55:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-04 19:55:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:49:03 lr 0.000020	 wd 0.0500	time 12.6871 (12.6871)	loss 1.3986 (1.3986)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 19:57:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:40:37 lr 0.000020	 wd 0.0500	time 0.8367 (1.0148)	loss 0.9832 (1.1388)	grad_norm 4.5812 (3.5139)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 19:58:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:36:42 lr 0.000020	 wd 0.0500	time 0.9058 (0.9566)	loss 1.1310 (1.1100)	grad_norm 2.7445 (3.1581)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:00:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:34:23 lr 0.000020	 wd 0.0500	time 0.8461 (0.9372)	loss 0.7558 (1.1020)	grad_norm 2.8245 (3.4308)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:01:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:32:31 lr 0.000020	 wd 0.0500	time 0.8399 (0.9282)	loss 1.1960 (1.1128)	grad_norm 2.7547 (3.3319)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:03:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:30:46 lr 0.000020	 wd 0.0500	time 0.8196 (0.9223)	loss 0.9321 (1.1109)	grad_norm 2.8795 (3.2198)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:04:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:29:06 lr 0.000020	 wd 0.0500	time 0.8832 (0.9185)	loss 0.9118 (1.1123)	grad_norm 3.2759 (3.1428)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:06:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:27:30 lr 0.000020	 wd 0.0500	time 0.8282 (0.9159)	loss 1.0065 (1.1130)	grad_norm 2.1690 (3.1387)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:07:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:25:55 lr 0.000020	 wd 0.0500	time 0.8466 (0.9140)	loss 1.3855 (1.1114)	grad_norm 2.4888 (3.1702)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:09:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:24:21 lr 0.000020	 wd 0.0500	time 0.8447 (0.9124)	loss 0.9980 (1.1105)	grad_norm 2.2128 (3.1566)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:10:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:22:48 lr 0.000020	 wd 0.0500	time 0.8414 (0.9109)	loss 1.0986 (1.1098)	grad_norm 2.4661 (3.1087)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:12:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:21:15 lr 0.000020	 wd 0.0500	time 0.8472 (0.9096)	loss 1.1259 (1.1105)	grad_norm 3.0130 (3.1141)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:13:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:19:43 lr 0.000020	 wd 0.0500	time 0.8578 (0.9087)	loss 1.2268 (1.1096)	grad_norm 2.2273 (3.0928)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:15:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:18:11 lr 0.000020	 wd 0.0500	time 0.8319 (0.9079)	loss 0.8304 (1.1101)	grad_norm 53.7823 (3.1691)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:16:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:16:39 lr 0.000020	 wd 0.0500	time 0.8495 (0.9072)	loss 1.3855 (1.1138)	grad_norm 4.5497 (3.1411)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:18:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:15:08 lr 0.000020	 wd 0.0500	time 0.8372 (0.9065)	loss 0.7783 (1.1164)	grad_norm 2.8821 (3.2593)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:19:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:13:37 lr 0.000020	 wd 0.0500	time 0.8755 (0.9060)	loss 1.2897 (1.1197)	grad_norm 2.1903 (3.2395)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:21:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:12:06 lr 0.000020	 wd 0.0500	time 0.8479 (0.9056)	loss 0.7362 (1.1185)	grad_norm 42.9901 (3.2851)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:22:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:10:35 lr 0.000020	 wd 0.0500	time 0.8538 (0.9054)	loss 0.7770 (1.1165)	grad_norm 2.4925 (3.2511)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:24:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:09:04 lr 0.000020	 wd 0.0500	time 0.8490 (0.9053)	loss 1.1057 (1.1158)	grad_norm 1.9428 (3.2509)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:25:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:07:34 lr 0.000020	 wd 0.0500	time 0.8426 (0.9048)	loss 1.4294 (1.1176)	grad_norm 3.1399 (3.2339)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:27:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:06:03 lr 0.000020	 wd 0.0500	time 0.8213 (0.9044)	loss 1.4382 (1.1171)	grad_norm 5.3568 (3.2316)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:28:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:04:33 lr 0.000020	 wd 0.0500	time 0.8466 (0.9041)	loss 1.1485 (1.1183)	grad_norm 2.6216 (3.2223)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:30:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:03:02 lr 0.000020	 wd 0.0500	time 0.7915 (0.9038)	loss 1.1381 (1.1169)	grad_norm 2.2658 (3.2225)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:31:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8531 (0.9035)	loss 0.9286 (1.1164)	grad_norm 2.4574 (3.2119)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:33:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8478 (0.9033)	loss 1.3888 (1.1176)	grad_norm 1.9988 (3.1917)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:33:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 6 training takes 0:37:42
[2024-08-04 20:33:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.413 (11.413)	Loss 0.4612 (0.4612)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-04 20:33:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.904 Acc@5 98.068
[2024-08-04 20:33:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-04 20:33:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-04 20:33:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth saving......
[2024-08-04 20:33:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-04 20:33:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:22:21 lr 0.000020	 wd 0.0500	time 10.6079 (10.6079)	loss 1.1310 (1.1310)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:35:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:39:43 lr 0.000020	 wd 0.0500	time 0.8356 (0.9921)	loss 0.8452 (1.1409)	grad_norm 2.0578 (3.2542)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:36:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:36:23 lr 0.000020	 wd 0.0500	time 0.8498 (0.9484)	loss 0.7724 (1.1173)	grad_norm 3.1480 (3.3663)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-04 20:38:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:34:11 lr 0.000020	 wd 0.0500	time 0.8290 (0.9315)	loss 0.8518 (1.1120)	grad_norm 2.3834 (3.4627)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 08:35:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/config.json
[2024-08-05 08:35:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_sequence_stage3
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-05 08:35:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_sequence_stage3.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_sequence_stage3", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-05 08:35:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft
[2024-08-05 08:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-05 08:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 113): INFO number of params: 80620264
[2024-08-05 08:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 148): INFO auto resuming from pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth
[2024-08-05 08:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 19): INFO ==============> Resuming form pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth....................
[2024-08-05 08:35:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 26): INFO <All keys matched successfully>
[2024-08-05 08:35:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 36): INFO => loaded successfully 'pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_best.pth' (epoch 6)
[2024-08-05 08:35:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 13.519 (13.519)	Loss 0.4822 (0.4822)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 3364MB
[2024-08-05 08:36:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.878 Acc@5 98.054
[2024-08-05 08:36:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 155): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-05 08:36:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 168): INFO Start training
[2024-08-05 08:36:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 9:09:17 lr 0.000020	 wd 0.0500	time 13.1726 (13.1726)	loss 1.4771 (1.4771)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 08:38:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:40:56 lr 0.000020	 wd 0.0500	time 0.8237 (1.0227)	loss 1.1990 (1.1600)	grad_norm 2.1441 (2.7873)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 08:39:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:36:51 lr 0.000020	 wd 0.0500	time 0.8449 (0.9606)	loss 0.9021 (1.1406)	grad_norm 2.7616 (inf)	loss_scale 1024.0000 (2007.2438)	mem 23977MB
[2024-08-05 08:41:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:34:28 lr 0.000020	 wd 0.0500	time 0.8103 (0.9392)	loss 0.7436 (1.0964)	grad_norm 3.4025 (inf)	loss_scale 1024.0000 (1680.5847)	mem 23977MB
[2024-08-05 08:42:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:32:32 lr 0.000020	 wd 0.0500	time 0.8404 (0.9291)	loss 0.8423 (1.1042)	grad_norm 2.2041 (inf)	loss_scale 1024.0000 (1516.8479)	mem 23977MB
[2024-08-05 08:44:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:30:46 lr 0.000020	 wd 0.0500	time 0.8305 (0.9223)	loss 1.1650 (1.1064)	grad_norm 1.9162 (inf)	loss_scale 1024.0000 (1418.4750)	mem 23977MB
[2024-08-05 08:45:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:29:06 lr 0.000020	 wd 0.0500	time 0.8535 (0.9181)	loss 1.2025 (1.1048)	grad_norm 2.5882 (inf)	loss_scale 1024.0000 (1352.8386)	mem 23977MB
[2024-08-05 08:47:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:27:29 lr 0.000020	 wd 0.0500	time 0.8943 (0.9155)	loss 1.2369 (1.1082)	grad_norm 3.0399 (inf)	loss_scale 1024.0000 (1305.9287)	mem 23977MB
[2024-08-05 08:48:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:25:54 lr 0.000020	 wd 0.0500	time 0.8007 (0.9135)	loss 1.0991 (1.1072)	grad_norm 5.4169 (inf)	loss_scale 1024.0000 (1270.7316)	mem 23977MB
[2024-08-05 08:50:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:24:21 lr 0.000020	 wd 0.0500	time 0.8407 (0.9122)	loss 1.2500 (1.1075)	grad_norm 2.1743 (inf)	loss_scale 1024.0000 (1243.3474)	mem 23977MB
[2024-08-05 08:51:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:22:48 lr 0.000020	 wd 0.0500	time 0.8394 (0.9110)	loss 1.3712 (1.1046)	grad_norm 2.3534 (inf)	loss_scale 1024.0000 (1221.4346)	mem 23977MB
[2024-08-05 08:53:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:21:15 lr 0.000020	 wd 0.0500	time 0.8363 (0.9098)	loss 1.1269 (1.1064)	grad_norm 2.5571 (inf)	loss_scale 1024.0000 (1203.5023)	mem 23977MB
[2024-08-05 08:54:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:19:43 lr 0.000020	 wd 0.0500	time 0.8411 (0.9089)	loss 0.8730 (1.1053)	grad_norm 2.2457 (inf)	loss_scale 1024.0000 (1188.5562)	mem 23977MB
[2024-08-05 08:56:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:18:11 lr 0.000020	 wd 0.0500	time 0.9038 (0.9080)	loss 1.1155 (1.1052)	grad_norm 2.2386 (inf)	loss_scale 1024.0000 (1175.9078)	mem 23977MB
[2024-08-05 08:57:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:16:39 lr 0.000019	 wd 0.0500	time 0.8344 (0.9072)	loss 1.4322 (1.1081)	grad_norm 2.3930 (inf)	loss_scale 1024.0000 (1165.0650)	mem 23977MB
[2024-08-05 08:59:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:15:08 lr 0.000019	 wd 0.0500	time 0.8618 (0.9066)	loss 1.0459 (1.1111)	grad_norm 3.1348 (inf)	loss_scale 1024.0000 (1155.6669)	mem 23977MB
[2024-08-05 09:00:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:13:37 lr 0.000019	 wd 0.0500	time 0.8378 (0.9060)	loss 1.0297 (1.1094)	grad_norm 2.1649 (inf)	loss_scale 1024.0000 (1147.4428)	mem 23977MB
[2024-08-05 09:01:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:12:06 lr 0.000019	 wd 0.0500	time 0.8509 (0.9054)	loss 0.9180 (1.1083)	grad_norm 3.0165 (inf)	loss_scale 1024.0000 (1140.1858)	mem 23977MB
[2024-08-05 09:03:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:10:35 lr 0.000019	 wd 0.0500	time 0.8435 (0.9050)	loss 1.0993 (1.1085)	grad_norm 2.7600 (inf)	loss_scale 1024.0000 (1133.7346)	mem 23977MB
[2024-08-05 09:04:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:09:04 lr 0.000019	 wd 0.0500	time 0.8522 (0.9045)	loss 1.3138 (1.1089)	grad_norm 2.4629 (inf)	loss_scale 1024.0000 (1127.9621)	mem 23977MB
[2024-08-05 09:06:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:07:33 lr 0.000019	 wd 0.0500	time 0.8396 (0.9043)	loss 0.7996 (1.1076)	grad_norm 3.3628 (inf)	loss_scale 1024.0000 (1122.7666)	mem 23977MB
[2024-08-05 09:07:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:06:03 lr 0.000019	 wd 0.0500	time 0.8342 (0.9039)	loss 0.9927 (1.1067)	grad_norm 6.1693 (inf)	loss_scale 1024.0000 (1118.0657)	mem 23977MB
[2024-08-05 09:09:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:04:32 lr 0.000019	 wd 0.0500	time 0.8215 (0.9037)	loss 1.1967 (1.1065)	grad_norm 2.4103 (inf)	loss_scale 1024.0000 (1113.7919)	mem 23977MB
[2024-08-05 09:10:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:03:02 lr 0.000019	 wd 0.0500	time 0.9124 (0.9035)	loss 1.4949 (1.1073)	grad_norm 2.5613 (inf)	loss_scale 1024.0000 (1109.8896)	mem 23977MB
[2024-08-05 09:12:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:32 lr 0.000019	 wd 0.0500	time 0.8890 (0.9032)	loss 1.3131 (1.1076)	grad_norm 4.4763 (inf)	loss_scale 1024.0000 (1106.3124)	mem 23977MB
[2024-08-05 09:13:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8360 (0.9028)	loss 0.9230 (1.1061)	grad_norm 2.1147 (inf)	loss_scale 1024.0000 (1103.0212)	mem 23977MB
[2024-08-05 09:14:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 7 training takes 0:37:41
[2024-08-05 09:14:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.238 (11.238)	Loss 0.4565 (0.4565)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 09:14:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.848 Acc@5 98.056
[2024-08-05 09:14:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 09:14:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 09:14:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:25:13 lr 0.000019	 wd 0.0500	time 12.1157 (12.1157)	loss 1.3381 (1.3381)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:16:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:40:28 lr 0.000019	 wd 0.0500	time 0.8578 (1.0110)	loss 1.1778 (1.0872)	grad_norm 3.3366 (3.1511)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:17:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:36:38 lr 0.000019	 wd 0.0500	time 0.8560 (0.9549)	loss 0.7950 (1.0997)	grad_norm 2.4688 (3.1794)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:19:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:34:21 lr 0.000019	 wd 0.0500	time 0.8350 (0.9361)	loss 0.7936 (1.1026)	grad_norm 3.4061 (3.1255)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:20:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:32:28 lr 0.000019	 wd 0.0500	time 0.8510 (0.9269)	loss 1.1335 (1.1187)	grad_norm 2.5015 (3.0827)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:22:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:30:44 lr 0.000019	 wd 0.0500	time 0.8304 (0.9211)	loss 1.3399 (1.1207)	grad_norm 2.6936 (3.0278)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:23:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:29:04 lr 0.000019	 wd 0.0500	time 0.7895 (0.9173)	loss 1.2229 (1.1192)	grad_norm 2.3017 (3.0277)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:25:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:27:27 lr 0.000019	 wd 0.0500	time 0.7921 (0.9142)	loss 0.9017 (1.1163)	grad_norm 3.7905 (3.1957)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:26:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:25:51 lr 0.000019	 wd 0.0500	time 0.8526 (0.9118)	loss 1.4469 (1.1154)	grad_norm 2.3581 (3.1968)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:28:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:24:17 lr 0.000019	 wd 0.0500	time 0.8483 (0.9099)	loss 1.2498 (1.1156)	grad_norm 3.1088 (3.1543)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:29:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:22:44 lr 0.000019	 wd 0.0500	time 0.8475 (0.9086)	loss 1.4355 (1.1143)	grad_norm 3.3390 (3.1523)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:31:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:21:12 lr 0.000019	 wd 0.0500	time 0.8449 (0.9074)	loss 1.1553 (1.1117)	grad_norm 2.1033 (3.1431)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:32:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:19:40 lr 0.000019	 wd 0.0500	time 0.8431 (0.9066)	loss 0.8779 (1.1098)	grad_norm 1.8675 (3.1472)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:34:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:18:08 lr 0.000019	 wd 0.0500	time 0.8896 (0.9059)	loss 1.1048 (1.1102)	grad_norm 3.0568 (3.1530)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:35:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:16:37 lr 0.000019	 wd 0.0500	time 0.8577 (0.9051)	loss 0.8440 (1.1106)	grad_norm 2.4596 (3.1393)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:37:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:15:06 lr 0.000019	 wd 0.0500	time 0.8564 (0.9045)	loss 1.2832 (1.1101)	grad_norm 2.0519 (3.1033)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:38:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:13:35 lr 0.000019	 wd 0.0500	time 0.8438 (0.9040)	loss 1.3939 (1.1112)	grad_norm 2.7121 (3.0881)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 09:40:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:12:04 lr 0.000019	 wd 0.0500	time 0.8414 (0.9035)	loss 0.9315 (1.1125)	grad_norm 2.2865 (3.0811)	loss_scale 2048.0000 (1030.0200)	mem 23977MB
[2024-08-05 09:41:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:10:34 lr 0.000019	 wd 0.0500	time 0.8681 (0.9033)	loss 1.2680 (1.1112)	grad_norm 2.7361 (3.0594)	loss_scale 2048.0000 (1086.5430)	mem 23977MB
[2024-08-05 09:43:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:09:03 lr 0.000019	 wd 0.0500	time 0.8502 (0.9029)	loss 1.2239 (1.1123)	grad_norm 2.0745 (3.0394)	loss_scale 2048.0000 (1137.1194)	mem 23977MB
[2024-08-05 09:44:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:07:33 lr 0.000019	 wd 0.0500	time 0.8889 (0.9026)	loss 1.3746 (1.1134)	grad_norm 2.6245 (3.0414)	loss_scale 2048.0000 (1182.6407)	mem 23977MB
[2024-08-05 09:46:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:06:02 lr 0.000019	 wd 0.0500	time 0.8420 (0.9023)	loss 1.3530 (1.1158)	grad_norm 2.8696 (3.0566)	loss_scale 2048.0000 (1223.8287)	mem 23977MB
[2024-08-05 09:47:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:04:32 lr 0.000019	 wd 0.0500	time 0.8398 (0.9021)	loss 1.1959 (1.1161)	grad_norm 3.9731 (3.0399)	loss_scale 2048.0000 (1261.2740)	mem 23977MB
[2024-08-05 09:49:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:03:02 lr 0.000019	 wd 0.0500	time 0.8446 (0.9020)	loss 0.7276 (1.1171)	grad_norm 3.1972 (3.0412)	loss_scale 2048.0000 (1295.4646)	mem 23977MB
[2024-08-05 09:50:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:31 lr 0.000019	 wd 0.0500	time 0.7943 (0.9017)	loss 1.1717 (1.1173)	grad_norm 2.1122 (3.0438)	loss_scale 2048.0000 (1326.8072)	mem 23977MB
[2024-08-05 09:52:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8513 (0.9014)	loss 0.8694 (1.1177)	grad_norm 2.1526 (3.0362)	loss_scale 2048.0000 (1355.6433)	mem 23977MB
[2024-08-05 09:52:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 8 training takes 0:37:37
[2024-08-05 09:52:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 13.518 (13.518)	Loss 0.4624 (0.4624)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 09:52:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.806 Acc@5 98.054
[2024-08-05 09:52:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 09:52:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 09:52:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][0/2502]	eta 8:49:15 lr 0.000019	 wd 0.0500	time 12.6921 (12.6921)	loss 1.1233 (1.1233)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 09:54:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:40:46 lr 0.000019	 wd 0.0500	time 0.8516 (1.0184)	loss 1.2868 (1.1119)	grad_norm 2.1149 (2.6066)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 09:55:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:36:42 lr 0.000019	 wd 0.0500	time 0.8480 (0.9566)	loss 1.3458 (1.1102)	grad_norm 1.7509 (2.8564)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 09:57:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:34:23 lr 0.000019	 wd 0.0500	time 0.8493 (0.9369)	loss 1.2075 (1.0978)	grad_norm 2.4223 (3.0921)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 09:58:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:32:28 lr 0.000019	 wd 0.0500	time 0.8472 (0.9271)	loss 1.2899 (1.1003)	grad_norm 2.4128 (3.0797)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 10:00:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:30:43 lr 0.000019	 wd 0.0500	time 0.8449 (0.9210)	loss 1.1917 (1.0980)	grad_norm 2.2646 (3.0676)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 10:01:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:29:05 lr 0.000019	 wd 0.0500	time 0.8255 (0.9177)	loss 1.1697 (1.0980)	grad_norm 2.8075 (3.1093)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 10:03:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:27:28 lr 0.000019	 wd 0.0500	time 0.7884 (0.9146)	loss 1.3194 (1.1017)	grad_norm 2.4474 (3.0457)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 10:04:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:25:53 lr 0.000019	 wd 0.0500	time 0.8709 (0.9126)	loss 1.2409 (1.1066)	grad_norm 2.4374 (3.0376)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 10:06:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:24:19 lr 0.000019	 wd 0.0500	time 0.8765 (0.9108)	loss 0.7612 (1.1105)	grad_norm 3.5300 (nan)	loss_scale 1024.0000 (1950.2597)	mem 23977MB
[2024-08-05 10:07:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:22:45 lr 0.000019	 wd 0.0500	time 0.8528 (0.9093)	loss 0.7948 (1.1086)	grad_norm 2.8867 (nan)	loss_scale 1024.0000 (1857.7263)	mem 23977MB
[2024-08-05 10:09:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:21:13 lr 0.000018	 wd 0.0500	time 0.8920 (0.9080)	loss 1.3374 (1.1073)	grad_norm 2.9239 (nan)	loss_scale 1024.0000 (1782.0018)	mem 23977MB
[2024-08-05 10:10:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:19:40 lr 0.000018	 wd 0.0500	time 0.8514 (0.9071)	loss 0.8005 (1.1071)	grad_norm 11.4682 (nan)	loss_scale 1024.0000 (1718.8876)	mem 23977MB
[2024-08-05 10:12:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:18:09 lr 0.000018	 wd 0.0500	time 0.8478 (0.9063)	loss 1.2937 (1.1079)	grad_norm 2.1990 (nan)	loss_scale 1024.0000 (1665.4758)	mem 23977MB
[2024-08-05 10:13:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:16:38 lr 0.000018	 wd 0.0500	time 0.7868 (0.9057)	loss 1.1792 (1.1103)	grad_norm 2.2295 (nan)	loss_scale 1024.0000 (1619.6888)	mem 23977MB
[2024-08-05 10:15:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:15:07 lr 0.000018	 wd 0.0500	time 0.8878 (0.9052)	loss 1.2488 (1.1118)	grad_norm 5.5949 (nan)	loss_scale 1024.0000 (1580.0027)	mem 23977MB
[2024-08-05 10:16:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:13:36 lr 0.000018	 wd 0.0500	time 0.8480 (0.9047)	loss 0.8840 (1.1117)	grad_norm 2.1273 (nan)	loss_scale 1024.0000 (1545.2742)	mem 23977MB
[2024-08-05 10:18:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:12:05 lr 0.000018	 wd 0.0500	time 0.8425 (0.9044)	loss 1.5451 (1.1115)	grad_norm 7.4764 (nan)	loss_scale 1024.0000 (1514.6290)	mem 23977MB
[2024-08-05 10:19:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:10:34 lr 0.000018	 wd 0.0500	time 0.9183 (0.9041)	loss 1.1200 (1.1125)	grad_norm 2.5065 (nan)	loss_scale 1024.0000 (1487.3870)	mem 23977MB
[2024-08-05 10:21:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:09:04 lr 0.000018	 wd 0.0500	time 0.8431 (0.9037)	loss 1.4837 (1.1120)	grad_norm 2.8806 (nan)	loss_scale 1024.0000 (1463.0110)	mem 23977MB
[2024-08-05 10:22:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:07:33 lr 0.000018	 wd 0.0500	time 0.8530 (0.9034)	loss 1.0187 (1.1127)	grad_norm 1.8824 (nan)	loss_scale 1024.0000 (1441.0715)	mem 23977MB
[2024-08-05 10:24:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:06:03 lr 0.000018	 wd 0.0500	time 0.8442 (0.9031)	loss 0.9271 (1.1111)	grad_norm 2.3851 (nan)	loss_scale 1024.0000 (1421.2204)	mem 23977MB
[2024-08-05 10:25:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:04:32 lr 0.000018	 wd 0.0500	time 0.8498 (0.9029)	loss 1.2751 (1.1116)	grad_norm 2.0693 (nan)	loss_scale 1024.0000 (1403.1731)	mem 23977MB
[2024-08-05 10:27:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:03:02 lr 0.000018	 wd 0.0500	time 0.8497 (0.9026)	loss 1.0550 (1.1112)	grad_norm 2.3282 (nan)	loss_scale 1024.0000 (1386.6945)	mem 23977MB
[2024-08-05 10:28:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:32 lr 0.000018	 wd 0.0500	time 0.8512 (0.9023)	loss 0.7560 (1.1124)	grad_norm 2.5479 (nan)	loss_scale 1024.0000 (1371.5885)	mem 23977MB
[2024-08-05 10:30:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.8429 (0.9020)	loss 1.0580 (1.1128)	grad_norm 3.1293 (nan)	loss_scale 1024.0000 (1357.6905)	mem 23977MB
[2024-08-05 10:30:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 9 training takes 0:37:39
[2024-08-05 10:30:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.585 (12.585)	Loss 0.4407 (0.4407)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 10:30:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.878 Acc@5 98.052
[2024-08-05 10:30:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-05 10:30:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 10:31:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][0/2502]	eta 8:50:35 lr 0.000018	 wd 0.0500	time 12.7241 (12.7241)	loss 0.8551 (0.8551)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:32:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:40:39 lr 0.000018	 wd 0.0500	time 0.8485 (1.0157)	loss 1.2255 (1.1632)	grad_norm 2.4421 (2.8302)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:34:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:36:40 lr 0.000018	 wd 0.0500	time 0.8461 (0.9561)	loss 1.4064 (1.1312)	grad_norm 2.4401 (2.8216)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:34:21 lr 0.000018	 wd 0.0500	time 0.8440 (0.9361)	loss 1.4414 (1.1270)	grad_norm 2.1994 (3.0718)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:37:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:32:27 lr 0.000018	 wd 0.0500	time 0.8953 (0.9264)	loss 1.3270 (1.1191)	grad_norm 2.3022 (2.9939)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:38:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:30:43 lr 0.000018	 wd 0.0500	time 0.8464 (0.9206)	loss 0.7446 (1.1172)	grad_norm 2.2037 (3.0158)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:40:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:29:03 lr 0.000018	 wd 0.0500	time 0.8346 (0.9165)	loss 1.1946 (1.1089)	grad_norm 2.7037 (2.9628)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:41:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:27:26 lr 0.000018	 wd 0.0500	time 0.8033 (0.9135)	loss 1.2092 (1.1032)	grad_norm 2.7240 (2.9286)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:43:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:25:51 lr 0.000018	 wd 0.0500	time 0.9135 (0.9115)	loss 0.7411 (1.0976)	grad_norm 2.2420 (2.9398)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:44:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:24:17 lr 0.000018	 wd 0.0500	time 0.8492 (0.9098)	loss 1.2500 (1.1010)	grad_norm 2.6442 (2.9238)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:46:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:22:45 lr 0.000018	 wd 0.0500	time 0.8418 (0.9090)	loss 1.1607 (1.1006)	grad_norm 1.9509 (2.9240)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:47:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:21:12 lr 0.000018	 wd 0.0500	time 0.8636 (0.9077)	loss 0.7468 (1.1007)	grad_norm 2.6732 (3.0494)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:49:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:19:40 lr 0.000018	 wd 0.0500	time 0.8766 (0.9067)	loss 1.1893 (1.0992)	grad_norm 2.9087 (3.0132)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:50:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:18:08 lr 0.000018	 wd 0.0500	time 0.8187 (0.9057)	loss 1.3717 (1.0996)	grad_norm 1.9017 (3.0103)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:52:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:16:37 lr 0.000018	 wd 0.0500	time 0.8435 (0.9052)	loss 1.1475 (1.0994)	grad_norm 1.8980 (3.0421)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:53:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:15:06 lr 0.000018	 wd 0.0500	time 0.8433 (0.9045)	loss 1.1704 (1.0969)	grad_norm 2.1146 (3.0491)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:55:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:13:35 lr 0.000018	 wd 0.0500	time 0.8099 (0.9040)	loss 0.6968 (1.0951)	grad_norm 3.2180 (3.1197)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:56:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:12:04 lr 0.000018	 wd 0.0500	time 0.8546 (0.9037)	loss 0.9354 (1.0979)	grad_norm 2.9768 (3.1274)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:58:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:10:34 lr 0.000018	 wd 0.0500	time 0.8526 (0.9034)	loss 0.9851 (1.0997)	grad_norm 2.0740 (3.1542)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 10:59:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:09:03 lr 0.000018	 wd 0.0500	time 0.8473 (0.9031)	loss 1.1706 (1.0990)	grad_norm 1.9496 (3.1491)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:01:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:07:33 lr 0.000017	 wd 0.0500	time 0.8446 (0.9028)	loss 1.1791 (1.0996)	grad_norm 2.2893 (3.1422)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:02:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:06:02 lr 0.000017	 wd 0.0500	time 0.8528 (0.9027)	loss 0.9098 (1.1004)	grad_norm 3.6100 (3.1557)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:04:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:04:32 lr 0.000017	 wd 0.0500	time 0.8585 (0.9024)	loss 0.7184 (1.1001)	grad_norm 2.3117 (3.1712)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:05:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:03:02 lr 0.000017	 wd 0.0500	time 0.7972 (0.9021)	loss 0.8056 (1.0998)	grad_norm 2.4824 (3.1530)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:07:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:31 lr 0.000017	 wd 0.0500	time 0.8517 (0.9019)	loss 1.4177 (1.1015)	grad_norm 2.6797 (3.1332)	loss_scale 2048.0000 (1061.5310)	mem 23977MB
[2024-08-05 11:08:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.8479 (0.9016)	loss 1.1600 (1.1012)	grad_norm 2.0376 (3.1195)	loss_scale 2048.0000 (1100.9740)	mem 23977MB
[2024-08-05 11:08:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 10 training takes 0:37:38
[2024-08-05 11:08:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.331 (12.331)	Loss 0.4658 (0.4658)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-05 11:09:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.804 Acc@5 98.052
[2024-08-05 11:09:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 11:09:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 11:09:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:59:03 lr 0.000017	 wd 0.0500	time 12.9270 (12.9270)	loss 1.1593 (1.1593)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:10:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:40:34 lr 0.000017	 wd 0.0500	time 0.8358 (1.0136)	loss 0.8545 (1.1154)	grad_norm 2.5754 (2.8915)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:12:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:36:38 lr 0.000017	 wd 0.0500	time 0.9015 (0.9552)	loss 0.8333 (1.1039)	grad_norm 2.5934 (3.0668)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:13:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:34:20 lr 0.000017	 wd 0.0500	time 0.8473 (0.9357)	loss 0.8342 (1.1061)	grad_norm 3.9842 (2.9244)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:15:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:32:30 lr 0.000017	 wd 0.0500	time 0.8041 (0.9277)	loss 1.3166 (1.1010)	grad_norm 2.3951 (3.0823)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:16:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:30:44 lr 0.000017	 wd 0.0500	time 0.8490 (0.9215)	loss 1.1116 (1.0997)	grad_norm 2.2557 (3.0797)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:18:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:29:04 lr 0.000017	 wd 0.0500	time 0.9029 (0.9173)	loss 1.0546 (1.1044)	grad_norm 2.5252 (3.0362)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:19:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:27:28 lr 0.000017	 wd 0.0500	time 0.7907 (0.9146)	loss 0.7792 (1.1060)	grad_norm 2.9117 (3.0247)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:21:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:25:52 lr 0.000017	 wd 0.0500	time 0.7929 (0.9124)	loss 0.9660 (1.1098)	grad_norm 2.5529 (3.0700)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:22:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:24:19 lr 0.000017	 wd 0.0500	time 0.8456 (0.9109)	loss 0.9318 (1.1117)	grad_norm 3.5254 (3.1289)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:24:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:22:46 lr 0.000017	 wd 0.0500	time 0.8412 (0.9097)	loss 1.3254 (1.1119)	grad_norm 2.0441 (3.0766)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:25:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:21:13 lr 0.000017	 wd 0.0500	time 0.8540 (0.9085)	loss 1.1220 (1.1106)	grad_norm 2.4172 (3.0389)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:27:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:19:41 lr 0.000017	 wd 0.0500	time 0.8409 (0.9077)	loss 1.2458 (1.1085)	grad_norm 3.3788 (3.0708)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:28:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:18:09 lr 0.000017	 wd 0.0500	time 0.8452 (0.9067)	loss 1.0535 (1.1080)	grad_norm 2.3499 (3.1252)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:30:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:16:38 lr 0.000017	 wd 0.0500	time 0.8408 (0.9061)	loss 1.3958 (1.1092)	grad_norm 4.6380 (3.1060)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:31:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:15:07 lr 0.000017	 wd 0.0500	time 0.8457 (0.9055)	loss 1.2229 (1.1087)	grad_norm 2.4576 (3.0779)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:33:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:13:36 lr 0.000017	 wd 0.0500	time 0.8817 (0.9049)	loss 1.1823 (1.1070)	grad_norm 2.0452 (3.0949)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:34:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:12:05 lr 0.000017	 wd 0.0500	time 0.8436 (0.9043)	loss 0.8955 (1.1067)	grad_norm 2.7796 (3.0841)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:36:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:10:34 lr 0.000017	 wd 0.0500	time 0.8514 (0.9038)	loss 1.4403 (1.1083)	grad_norm 2.3704 (3.0689)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:37:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:09:03 lr 0.000017	 wd 0.0500	time 0.8464 (0.9036)	loss 1.2368 (1.1083)	grad_norm 2.0631 (3.0626)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:39:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:07:33 lr 0.000017	 wd 0.0500	time 0.8581 (0.9033)	loss 0.7563 (1.1063)	grad_norm 2.1398 (3.0392)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 11:40:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:06:02 lr 0.000017	 wd 0.0500	time 0.8471 (0.9030)	loss 1.0687 (1.1048)	grad_norm 2.1990 (inf)	loss_scale 1024.0000 (2021.6811)	mem 23977MB
[2024-08-05 11:42:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:04:32 lr 0.000017	 wd 0.0500	time 0.8411 (0.9028)	loss 0.7648 (1.1052)	grad_norm 2.3654 (inf)	loss_scale 1024.0000 (1976.3526)	mem 23977MB
[2024-08-05 11:43:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:03:02 lr 0.000016	 wd 0.0500	time 0.8501 (0.9025)	loss 0.6693 (1.1048)	grad_norm 2.3810 (inf)	loss_scale 1024.0000 (1934.9639)	mem 23977MB
[2024-08-05 11:45:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:32 lr 0.000016	 wd 0.0500	time 0.8576 (0.9024)	loss 0.7255 (1.1039)	grad_norm 4.7859 (inf)	loss_scale 1024.0000 (1897.0229)	mem 23977MB
[2024-08-05 11:46:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8372 (0.9021)	loss 1.3216 (1.1042)	grad_norm 6.9636 (inf)	loss_scale 1024.0000 (1862.1160)	mem 23977MB
[2024-08-05 11:46:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 11 training takes 0:37:39
[2024-08-05 11:47:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.343 (12.343)	Loss 0.4434 (0.4434)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-05 11:47:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.766 Acc@5 98.034
[2024-08-05 11:47:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 11:47:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 11:47:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:42:45 lr 0.000016	 wd 0.0500	time 12.5363 (12.5363)	loss 1.4107 (1.4107)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:49:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:40:41 lr 0.000016	 wd 0.0500	time 0.9172 (1.0165)	loss 1.0426 (1.1154)	grad_norm 2.5192 (2.8658)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:50:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:36:40 lr 0.000016	 wd 0.0500	time 0.8509 (0.9560)	loss 1.0570 (1.0898)	grad_norm 2.2983 (2.9375)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:52:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:34:19 lr 0.000016	 wd 0.0500	time 0.7964 (0.9353)	loss 0.7251 (1.0869)	grad_norm 4.6665 (2.9998)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:53:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:32:25 lr 0.000016	 wd 0.0500	time 0.8377 (0.9255)	loss 1.1977 (1.0976)	grad_norm 12.8132 (3.1711)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:55:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:30:41 lr 0.000016	 wd 0.0500	time 0.8375 (0.9198)	loss 0.8419 (1.0968)	grad_norm 2.9021 (3.2521)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:56:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:29:02 lr 0.000016	 wd 0.0500	time 0.8474 (0.9161)	loss 0.7297 (1.0971)	grad_norm 3.2173 (3.2483)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:58:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:27:25 lr 0.000016	 wd 0.0500	time 0.8484 (0.9133)	loss 0.8018 (1.0979)	grad_norm 2.0578 (3.2710)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 11:59:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:25:51 lr 0.000016	 wd 0.0500	time 0.8505 (0.9116)	loss 1.3371 (1.0965)	grad_norm 3.8240 (3.3142)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:01:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:24:17 lr 0.000016	 wd 0.0500	time 0.8316 (0.9101)	loss 0.9927 (1.0956)	grad_norm 4.8377 (3.2598)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:02:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:22:44 lr 0.000016	 wd 0.0500	time 0.8633 (0.9088)	loss 1.2779 (1.0949)	grad_norm 2.8164 (3.2302)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:04:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:21:12 lr 0.000016	 wd 0.0500	time 0.8711 (0.9078)	loss 0.9823 (1.0981)	grad_norm 6.1027 (3.2250)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:05:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:19:40 lr 0.000016	 wd 0.0500	time 0.8457 (0.9070)	loss 1.2722 (1.0954)	grad_norm 3.4307 (3.2117)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:07:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:18:09 lr 0.000016	 wd 0.0500	time 0.8412 (0.9063)	loss 0.8224 (1.0965)	grad_norm 9.0581 (3.2032)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:08:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:16:38 lr 0.000016	 wd 0.0500	time 0.8047 (0.9058)	loss 1.3571 (1.1006)	grad_norm 4.0094 (3.1801)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:10:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:15:06 lr 0.000016	 wd 0.0500	time 0.8449 (0.9052)	loss 0.7050 (1.1021)	grad_norm 11.0117 (3.2731)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:11:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:13:36 lr 0.000016	 wd 0.0500	time 0.8554 (0.9047)	loss 1.3224 (1.1044)	grad_norm 2.2227 (3.2574)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:13:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:12:05 lr 0.000016	 wd 0.0500	time 0.8505 (0.9043)	loss 0.7508 (1.1045)	grad_norm 2.4715 (3.2269)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:14:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:10:34 lr 0.000016	 wd 0.0500	time 0.8500 (0.9041)	loss 0.7715 (1.1026)	grad_norm 2.7267 (3.2057)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:16:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:09:04 lr 0.000016	 wd 0.0500	time 0.8520 (0.9038)	loss 1.1238 (1.1018)	grad_norm 2.6893 (3.1824)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:17:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:07:33 lr 0.000016	 wd 0.0500	time 0.8557 (0.9033)	loss 1.3444 (1.1042)	grad_norm 2.4501 (3.1662)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:19:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:06:02 lr 0.000016	 wd 0.0500	time 0.8466 (0.9030)	loss 1.5412 (1.1042)	grad_norm 2.1523 (3.1443)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:20:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:04:32 lr 0.000016	 wd 0.0500	time 0.8566 (0.9026)	loss 1.1642 (1.1054)	grad_norm 2.6493 (3.1343)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:22:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:03:02 lr 0.000015	 wd 0.0500	time 0.8959 (0.9024)	loss 1.0428 (1.1040)	grad_norm 2.7264 (3.1231)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:23:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:32 lr 0.000015	 wd 0.0500	time 0.8541 (0.9023)	loss 0.8619 (1.1031)	grad_norm 2.7776 (3.1554)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 12:25:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.8467 (0.9020)	loss 1.3203 (1.1043)	grad_norm 4.3597 (nan)	loss_scale 512.0000 (1013.3547)	mem 23977MB
[2024-08-05 12:25:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 12 training takes 0:37:39
[2024-08-05 12:25:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.779 (11.779)	Loss 0.4468 (0.4468)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 12:25:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.876 Acc@5 98.026
[2024-08-05 12:25:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-05 12:25:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 12:25:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:47:24 lr 0.000015	 wd 0.0500	time 12.6475 (12.6475)	loss 0.9176 (0.9176)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:27:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:40:29 lr 0.000015	 wd 0.0500	time 0.7927 (1.0113)	loss 0.8836 (1.1322)	grad_norm 2.4594 (3.4530)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:28:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:36:41 lr 0.000015	 wd 0.0500	time 0.8441 (0.9561)	loss 0.8151 (1.1139)	grad_norm 10.1508 (3.8366)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:30:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:34:22 lr 0.000015	 wd 0.0500	time 0.8347 (0.9367)	loss 0.7482 (1.1043)	grad_norm 2.4958 (4.1412)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:31:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:32:27 lr 0.000015	 wd 0.0500	time 0.8391 (0.9264)	loss 0.8174 (1.1095)	grad_norm 2.2533 (3.8394)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:33:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:30:42 lr 0.000015	 wd 0.0500	time 0.8516 (0.9203)	loss 1.1667 (1.1072)	grad_norm 3.3295 (3.6463)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:34:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:29:03 lr 0.000015	 wd 0.0500	time 0.8363 (0.9166)	loss 0.7110 (1.0989)	grad_norm 2.2132 (3.4920)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:36:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:27:27 lr 0.000015	 wd 0.0500	time 0.8443 (0.9140)	loss 1.4497 (1.0970)	grad_norm 2.8221 (3.3736)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:37:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:25:51 lr 0.000015	 wd 0.0500	time 0.8493 (0.9115)	loss 0.7070 (1.1000)	grad_norm 3.5332 (3.3401)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:39:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:24:17 lr 0.000015	 wd 0.0500	time 0.8551 (0.9099)	loss 1.4175 (1.0952)	grad_norm 1.9530 (3.3204)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:40:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:22:45 lr 0.000015	 wd 0.0500	time 0.8488 (0.9088)	loss 1.0234 (1.0951)	grad_norm 2.8650 (3.3012)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:42:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:21:13 lr 0.000015	 wd 0.0500	time 0.8538 (0.9080)	loss 0.8807 (1.0926)	grad_norm 2.6194 (3.3007)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:43:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:19:41 lr 0.000015	 wd 0.0500	time 0.8440 (0.9074)	loss 1.4178 (1.0900)	grad_norm 2.2559 (3.2988)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:45:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:18:09 lr 0.000015	 wd 0.0500	time 0.8552 (0.9065)	loss 1.0910 (1.0899)	grad_norm 2.4703 (3.3977)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:46:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:16:38 lr 0.000015	 wd 0.0500	time 0.8461 (0.9057)	loss 0.8603 (1.0898)	grad_norm 4.0011 (3.3662)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:48:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:15:06 lr 0.000015	 wd 0.0500	time 0.8436 (0.9051)	loss 0.8473 (1.0912)	grad_norm 3.9265 (3.3367)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:49:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:13:35 lr 0.000015	 wd 0.0500	time 0.8434 (0.9046)	loss 1.3495 (1.0891)	grad_norm 3.1454 (3.3642)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:51:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:12:05 lr 0.000015	 wd 0.0500	time 0.8420 (0.9042)	loss 1.4308 (1.0887)	grad_norm 6.7201 (3.3591)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:52:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:10:34 lr 0.000015	 wd 0.0500	time 0.8238 (0.9038)	loss 1.2314 (1.0891)	grad_norm 2.4651 (3.3719)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:54:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:09:03 lr 0.000015	 wd 0.0500	time 0.8450 (0.9034)	loss 1.1854 (1.0907)	grad_norm 2.7672 (3.3391)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:55:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:07:33 lr 0.000015	 wd 0.0500	time 0.8520 (0.9031)	loss 0.8976 (1.0917)	grad_norm 2.1892 (3.3351)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:57:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:06:02 lr 0.000014	 wd 0.0500	time 0.8473 (0.9029)	loss 1.0341 (1.0928)	grad_norm 4.6462 (3.3431)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 12:58:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:04:32 lr 0.000014	 wd 0.0500	time 0.8439 (0.9027)	loss 0.7923 (1.0926)	grad_norm 2.6705 (3.3421)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:00:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:03:02 lr 0.000014	 wd 0.0500	time 0.8500 (0.9025)	loss 0.9857 (1.0948)	grad_norm 2.9623 (3.3457)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:01:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:32 lr 0.000014	 wd 0.0500	time 0.8484 (0.9023)	loss 1.3287 (1.0939)	grad_norm 2.6289 (3.3482)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:03:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.8480 (0.9019)	loss 1.2064 (1.0928)	grad_norm 2.6599 (3.4198)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:03:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 13 training takes 0:37:39
[2024-08-05 13:03:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.008 (12.008)	Loss 0.4658 (0.4658)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-05 13:03:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.782 Acc@5 98.056
[2024-08-05 13:03:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 13:03:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 13:04:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][0/2502]	eta 8:27:24 lr 0.000014	 wd 0.0500	time 12.1681 (12.1681)	loss 0.8145 (0.8145)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:05:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:40:21 lr 0.000014	 wd 0.0500	time 0.8425 (1.0080)	loss 1.0650 (1.0744)	grad_norm 2.2458 (4.1611)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:07:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:36:34 lr 0.000014	 wd 0.0500	time 0.8505 (0.9532)	loss 0.9853 (1.1005)	grad_norm 2.9592 (3.8394)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:08:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:34:17 lr 0.000014	 wd 0.0500	time 0.8481 (0.9345)	loss 1.4873 (1.0992)	grad_norm 2.8346 (3.7283)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:10:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:32:25 lr 0.000014	 wd 0.0500	time 0.8471 (0.9254)	loss 0.8847 (1.1007)	grad_norm 2.6772 (3.8245)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:11:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:30:43 lr 0.000014	 wd 0.0500	time 0.9470 (0.9206)	loss 1.3711 (1.0984)	grad_norm 2.0018 (3.6272)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:13:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:29:04 lr 0.000014	 wd 0.0500	time 0.8569 (0.9172)	loss 1.3931 (1.1046)	grad_norm 2.7576 (3.6716)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:14:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:27:28 lr 0.000014	 wd 0.0500	time 0.8565 (0.9147)	loss 0.9130 (1.1011)	grad_norm 2.2051 (3.6439)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:16:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:25:52 lr 0.000014	 wd 0.0500	time 0.8568 (0.9122)	loss 0.8482 (1.0978)	grad_norm 3.2031 (3.7499)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:17:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:24:18 lr 0.000014	 wd 0.0500	time 0.8546 (0.9106)	loss 1.4211 (1.0986)	grad_norm 2.2510 (3.6236)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:18:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:22:45 lr 0.000014	 wd 0.0500	time 0.8481 (0.9093)	loss 0.7477 (1.0996)	grad_norm 3.9205 (3.5801)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:20:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:21:13 lr 0.000014	 wd 0.0500	time 0.8474 (0.9082)	loss 1.2793 (1.0982)	grad_norm 2.7962 (3.5197)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:21:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:19:41 lr 0.000014	 wd 0.0500	time 0.8622 (0.9075)	loss 1.1779 (1.0949)	grad_norm 3.3722 (3.5032)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:23:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:18:09 lr 0.000014	 wd 0.0500	time 0.8454 (0.9067)	loss 1.2154 (1.0941)	grad_norm 2.7357 (3.4887)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:24:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:16:38 lr 0.000014	 wd 0.0500	time 0.8530 (0.9061)	loss 1.1068 (1.0939)	grad_norm 4.9002 (3.4603)	loss_scale 512.0000 (512.0000)	mem 23977MB
[2024-08-05 13:26:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:15:07 lr 0.000014	 wd 0.0500	time 0.8481 (0.9057)	loss 0.9882 (1.0946)	grad_norm 2.0962 (3.4180)	loss_scale 1024.0000 (531.1019)	mem 23977MB
[2024-08-05 13:27:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:13:36 lr 0.000014	 wd 0.0500	time 0.8283 (0.9052)	loss 0.9980 (1.0957)	grad_norm 3.0414 (3.4052)	loss_scale 1024.0000 (561.8888)	mem 23977MB
[2024-08-05 13:29:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:12:05 lr 0.000014	 wd 0.0500	time 0.8654 (0.9048)	loss 1.3310 (1.0960)	grad_norm 2.4221 (3.3728)	loss_scale 1024.0000 (589.0558)	mem 23977MB
[2024-08-05 13:30:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:10:34 lr 0.000013	 wd 0.0500	time 0.8478 (0.9044)	loss 0.6628 (1.0976)	grad_norm 11.3377 (3.4247)	loss_scale 1024.0000 (613.2060)	mem 23977MB
[2024-08-05 13:32:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:09:04 lr 0.000013	 wd 0.0500	time 0.8400 (0.9039)	loss 1.1031 (1.0974)	grad_norm 2.3089 (3.4144)	loss_scale 1024.0000 (634.8154)	mem 23977MB
[2024-08-05 13:33:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:07:33 lr 0.000013	 wd 0.0500	time 0.8501 (0.9036)	loss 1.0321 (1.0982)	grad_norm 2.4151 (3.4413)	loss_scale 1024.0000 (654.2649)	mem 23977MB
[2024-08-05 13:35:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:06:03 lr 0.000013	 wd 0.0500	time 0.8479 (0.9033)	loss 1.3385 (1.1000)	grad_norm 3.6349 (3.4245)	loss_scale 1024.0000 (671.8629)	mem 23977MB
[2024-08-05 13:36:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:04:32 lr 0.000013	 wd 0.0500	time 0.8469 (0.9031)	loss 1.1700 (1.0982)	grad_norm 4.0073 (3.4009)	loss_scale 1024.0000 (687.8619)	mem 23977MB
[2024-08-05 13:38:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:03:02 lr 0.000013	 wd 0.0500	time 0.8525 (0.9028)	loss 1.1314 (1.0970)	grad_norm 2.1958 (3.4069)	loss_scale 1024.0000 (702.4702)	mem 23977MB
[2024-08-05 13:39:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:32 lr 0.000013	 wd 0.0500	time 0.8757 (0.9026)	loss 1.1822 (1.0975)	grad_norm 2.2368 (3.4004)	loss_scale 1024.0000 (715.8617)	mem 23977MB
[2024-08-05 13:41:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.8369 (0.9023)	loss 1.1953 (1.0969)	grad_norm 2.6797 (3.3863)	loss_scale 1024.0000 (728.1823)	mem 23977MB
[2024-08-05 13:41:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 14 training takes 0:37:40
[2024-08-05 13:41:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.372 (12.372)	Loss 0.4485 (0.4485)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-05 13:42:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.732 Acc@5 98.050
[2024-08-05 13:42:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-05 13:42:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 13:42:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][0/2502]	eta 8:04:52 lr 0.000013	 wd 0.0500	time 11.6279 (11.6279)	loss 1.0919 (1.0919)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:43:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:40:15 lr 0.000013	 wd 0.0500	time 0.8409 (1.0056)	loss 0.8683 (1.1344)	grad_norm 2.7424 (2.8767)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:45:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:36:30 lr 0.000013	 wd 0.0500	time 0.8697 (0.9514)	loss 1.0023 (1.1091)	grad_norm 3.4326 (2.8420)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:46:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:34:13 lr 0.000013	 wd 0.0500	time 0.8488 (0.9328)	loss 1.3553 (1.1058)	grad_norm 2.2970 (2.8473)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:48:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:32:21 lr 0.000013	 wd 0.0500	time 0.8397 (0.9236)	loss 0.7104 (1.1033)	grad_norm 2.4000 (2.8215)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:49:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:30:37 lr 0.000013	 wd 0.0500	time 0.8518 (0.9180)	loss 0.7895 (1.0972)	grad_norm 2.9178 (2.8601)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:51:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:28:59 lr 0.000013	 wd 0.0500	time 0.8417 (0.9147)	loss 0.8004 (1.0955)	grad_norm 1.8459 (2.8918)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:52:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:27:23 lr 0.000013	 wd 0.0500	time 0.8518 (0.9121)	loss 1.0811 (1.0946)	grad_norm 3.1606 (2.9039)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:54:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:25:49 lr 0.000013	 wd 0.0500	time 0.7895 (0.9104)	loss 1.2727 (1.0973)	grad_norm 2.2179 (2.9169)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:55:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:24:17 lr 0.000013	 wd 0.0500	time 0.8966 (0.9097)	loss 0.7634 (1.0934)	grad_norm 3.1247 (3.0944)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:57:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:22:44 lr 0.000013	 wd 0.0500	time 0.8506 (0.9086)	loss 1.2415 (1.0966)	grad_norm 2.6591 (3.0982)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 13:58:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:21:12 lr 0.000013	 wd 0.0500	time 0.8531 (0.9077)	loss 1.2945 (1.0942)	grad_norm 2.0387 (3.0761)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:00:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:19:40 lr 0.000013	 wd 0.0500	time 0.8610 (0.9067)	loss 1.0073 (1.0952)	grad_norm 4.6301 (3.0935)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:01:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:18:08 lr 0.000013	 wd 0.0500	time 0.8549 (0.9059)	loss 0.8880 (1.0950)	grad_norm 3.5683 (3.0842)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:03:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:16:37 lr 0.000012	 wd 0.0500	time 0.8452 (0.9053)	loss 1.1325 (1.0959)	grad_norm 2.3286 (3.0725)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:04:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:15:06 lr 0.000012	 wd 0.0500	time 0.8517 (0.9048)	loss 1.0833 (1.0957)	grad_norm 2.2144 (3.0758)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:06:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:13:35 lr 0.000012	 wd 0.0500	time 0.8408 (0.9043)	loss 1.3009 (1.0976)	grad_norm 4.0003 (3.0689)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:07:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:12:04 lr 0.000012	 wd 0.0500	time 0.8572 (0.9040)	loss 0.7428 (1.0992)	grad_norm 2.5904 (3.0616)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:09:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:10:34 lr 0.000012	 wd 0.0500	time 0.8537 (0.9036)	loss 0.7910 (1.0976)	grad_norm 3.9063 (3.0650)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:10:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:09:03 lr 0.000012	 wd 0.0500	time 0.8464 (0.9035)	loss 1.3230 (1.0991)	grad_norm 2.4116 (3.0751)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:12:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:07:33 lr 0.000012	 wd 0.0500	time 0.8453 (0.9031)	loss 0.7710 (1.0986)	grad_norm 2.4421 (3.0952)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:13:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:06:02 lr 0.000012	 wd 0.0500	time 0.8895 (0.9028)	loss 0.9089 (1.0982)	grad_norm 3.9112 (3.1199)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:15:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:04:32 lr 0.000012	 wd 0.0500	time 0.8507 (0.9027)	loss 1.1940 (1.0975)	grad_norm 2.8148 (3.1426)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:16:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:03:02 lr 0.000012	 wd 0.0500	time 0.8448 (0.9024)	loss 1.1523 (1.0968)	grad_norm 2.9901 (3.1463)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:18:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:32 lr 0.000012	 wd 0.0500	time 0.8336 (0.9022)	loss 1.0045 (1.0963)	grad_norm 2.7283 (3.1558)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:19:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.8226 (0.9020)	loss 0.8554 (1.0967)	grad_norm 3.1866 (3.1623)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:19:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 15 training takes 0:37:39
[2024-08-05 14:19:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_15.pth saving......
[2024-08-05 14:19:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft/diffusion_ft_smt_l_sequence_stage3/ckpt_epoch_15.pth saved !!!
[2024-08-05 14:19:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.562 (11.562)	Loss 0.4385 (0.4385)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-05 14:20:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.838 Acc@5 98.050
[2024-08-05 14:20:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 14:20:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 14:20:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:12:12 lr 0.000012	 wd 0.0500	time 11.8037 (11.8037)	loss 1.1094 (1.1094)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:21:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:40:18 lr 0.000012	 wd 0.0500	time 0.8553 (1.0071)	loss 1.4419 (1.0886)	grad_norm 2.1899 (3.2932)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:23:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:36:32 lr 0.000012	 wd 0.0500	time 0.8422 (0.9524)	loss 0.9591 (1.1068)	grad_norm 4.2285 (3.1894)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:24:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:34:19 lr 0.000012	 wd 0.0500	time 0.8410 (0.9354)	loss 1.2932 (1.1152)	grad_norm 6.2316 (3.1245)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:26:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:32:25 lr 0.000012	 wd 0.0500	time 0.9025 (0.9254)	loss 1.0590 (1.1002)	grad_norm 2.6441 (3.0887)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 14:27:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:30:41 lr 0.000012	 wd 0.0500	time 0.8448 (0.9200)	loss 0.6729 (1.0949)	grad_norm 2.6849 (3.0989)	loss_scale 2048.0000 (1146.6347)	mem 23977MB
[2024-08-05 14:29:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:29:02 lr 0.000012	 wd 0.0500	time 0.8451 (0.9160)	loss 0.7492 (1.0954)	grad_norm 2.6815 (3.0885)	loss_scale 2048.0000 (1296.6123)	mem 23977MB
[2024-08-05 14:30:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:27:26 lr 0.000012	 wd 0.0500	time 0.8539 (0.9135)	loss 1.2911 (1.0980)	grad_norm 2.2194 (3.0711)	loss_scale 2048.0000 (1403.8003)	mem 23977MB
[2024-08-05 14:32:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:25:51 lr 0.000012	 wd 0.0500	time 0.8546 (0.9114)	loss 1.4340 (1.1001)	grad_norm 2.3728 (3.0525)	loss_scale 2048.0000 (1484.2247)	mem 23977MB
[2024-08-05 14:33:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:24:17 lr 0.000012	 wd 0.0500	time 0.8337 (0.9098)	loss 1.2624 (1.0995)	grad_norm 2.5574 (3.0372)	loss_scale 2048.0000 (1546.7969)	mem 23977MB
[2024-08-05 14:35:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:22:44 lr 0.000011	 wd 0.0500	time 0.8474 (0.9087)	loss 0.9343 (1.1002)	grad_norm 3.0826 (3.0336)	loss_scale 2048.0000 (1596.8671)	mem 23977MB
[2024-08-05 14:36:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:21:12 lr 0.000011	 wd 0.0500	time 0.8361 (0.9077)	loss 1.2739 (1.0953)	grad_norm 2.5202 (3.0109)	loss_scale 2048.0000 (1637.8420)	mem 23977MB
[2024-08-05 14:38:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:19:40 lr 0.000011	 wd 0.0500	time 0.8545 (0.9067)	loss 0.8275 (1.0944)	grad_norm 2.3930 (3.0103)	loss_scale 2048.0000 (1671.9933)	mem 23977MB
[2024-08-05 14:39:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:18:09 lr 0.000011	 wd 0.0500	time 0.8516 (0.9063)	loss 1.3411 (1.0960)	grad_norm 2.0933 (3.0275)	loss_scale 2048.0000 (1700.8947)	mem 23977MB
[2024-08-05 14:41:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:16:38 lr 0.000011	 wd 0.0500	time 0.8573 (0.9057)	loss 1.0359 (1.0942)	grad_norm 2.4931 (3.0619)	loss_scale 2048.0000 (1725.6702)	mem 23977MB
[2024-08-05 14:42:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:15:06 lr 0.000011	 wd 0.0500	time 0.8508 (0.9052)	loss 0.7443 (1.0964)	grad_norm 4.5362 (inf)	loss_scale 1024.0000 (1728.0426)	mem 23977MB
[2024-08-05 14:44:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:13:35 lr 0.000011	 wd 0.0500	time 0.8450 (0.9046)	loss 0.7342 (1.0946)	grad_norm 2.3018 (inf)	loss_scale 1024.0000 (1684.0675)	mem 23977MB
[2024-08-05 14:45:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:12:05 lr 0.000011	 wd 0.0500	time 0.8419 (0.9043)	loss 1.3407 (1.0950)	grad_norm 5.9660 (inf)	loss_scale 1024.0000 (1645.2628)	mem 23977MB
[2024-08-05 14:47:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:10:34 lr 0.000011	 wd 0.0500	time 0.8915 (0.9042)	loss 1.4214 (1.0947)	grad_norm 2.2834 (inf)	loss_scale 1024.0000 (1610.7674)	mem 23977MB
[2024-08-05 14:48:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:09:04 lr 0.000011	 wd 0.0500	time 0.8840 (0.9039)	loss 1.3286 (1.0935)	grad_norm 2.2340 (inf)	loss_scale 1024.0000 (1579.9011)	mem 23977MB
[2024-08-05 14:50:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:07:33 lr 0.000011	 wd 0.0500	time 0.7975 (0.9037)	loss 1.2073 (1.0930)	grad_norm 2.5519 (inf)	loss_scale 1024.0000 (1552.1199)	mem 23977MB
[2024-08-05 14:51:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:06:03 lr 0.000011	 wd 0.0500	time 0.8549 (0.9034)	loss 1.2869 (1.0939)	grad_norm 2.8070 (inf)	loss_scale 1024.0000 (1526.9833)	mem 23977MB
[2024-08-05 14:53:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:04:32 lr 0.000011	 wd 0.0500	time 0.8519 (0.9031)	loss 1.1877 (1.0951)	grad_norm 3.9040 (inf)	loss_scale 1024.0000 (1504.1308)	mem 23977MB
[2024-08-05 14:54:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:03:02 lr 0.000011	 wd 0.0500	time 0.8494 (0.9030)	loss 0.7037 (1.0955)	grad_norm 4.1118 (inf)	loss_scale 1024.0000 (1483.2647)	mem 23977MB
[2024-08-05 14:56:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:32 lr 0.000011	 wd 0.0500	time 0.8551 (0.9027)	loss 1.0775 (1.0958)	grad_norm 2.7308 (inf)	loss_scale 1024.0000 (1464.1366)	mem 23977MB
[2024-08-05 14:57:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.8376 (0.9024)	loss 1.1743 (1.0945)	grad_norm 2.5266 (inf)	loss_scale 1024.0000 (1446.5382)	mem 23977MB
[2024-08-05 14:57:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 16 training takes 0:37:40
[2024-08-05 14:58:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.323 (12.323)	Loss 0.4458 (0.4458)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 14:58:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.802 Acc@5 98.056
[2024-08-05 14:58:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 14:58:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 14:58:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:43:41 lr 0.000011	 wd 0.0500	time 12.5587 (12.5587)	loss 1.2442 (1.2442)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:00:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:40:35 lr 0.000011	 wd 0.0500	time 0.8821 (1.0139)	loss 0.8693 (1.0972)	grad_norm 2.9911 (4.5501)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:01:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:36:41 lr 0.000011	 wd 0.0500	time 0.8391 (0.9565)	loss 1.3954 (1.0934)	grad_norm 2.2470 (3.8006)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:03:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:34:25 lr 0.000011	 wd 0.0500	time 0.8387 (0.9378)	loss 1.1690 (1.0953)	grad_norm 3.1204 (3.5987)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:04:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:32:31 lr 0.000011	 wd 0.0500	time 0.8541 (0.9283)	loss 0.8419 (1.0980)	grad_norm 2.8063 (3.3559)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:06:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:30:46 lr 0.000010	 wd 0.0500	time 0.8517 (0.9222)	loss 0.6410 (1.0913)	grad_norm 2.0762 (3.2775)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:07:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:29:06 lr 0.000010	 wd 0.0500	time 0.8530 (0.9181)	loss 1.1460 (1.0963)	grad_norm 2.5047 (3.2382)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:09:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:27:30 lr 0.000010	 wd 0.0500	time 0.8599 (0.9159)	loss 1.2060 (1.0943)	grad_norm 2.2078 (3.1741)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:10:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:25:54 lr 0.000010	 wd 0.0500	time 0.8463 (0.9135)	loss 1.1545 (1.0977)	grad_norm 2.5686 (3.3553)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:12:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:24:20 lr 0.000010	 wd 0.0500	time 0.8533 (0.9117)	loss 1.3604 (1.0961)	grad_norm 2.8566 (3.3368)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:13:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:22:47 lr 0.000010	 wd 0.0500	time 0.8508 (0.9104)	loss 1.2076 (1.0972)	grad_norm 2.2861 (3.2846)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:15:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:21:14 lr 0.000010	 wd 0.0500	time 0.7884 (0.9090)	loss 0.9398 (1.1032)	grad_norm 2.5706 (3.2386)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:16:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:19:42 lr 0.000010	 wd 0.0500	time 0.8517 (0.9082)	loss 1.2109 (1.1017)	grad_norm 2.3322 (3.2278)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:18:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:18:10 lr 0.000010	 wd 0.0500	time 0.8522 (0.9074)	loss 1.2550 (1.0989)	grad_norm 2.7793 (3.2063)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:19:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:16:39 lr 0.000010	 wd 0.0500	time 0.8951 (0.9068)	loss 1.3512 (1.0969)	grad_norm 2.6013 (3.1795)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:21:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:15:08 lr 0.000010	 wd 0.0500	time 0.8397 (0.9063)	loss 1.1459 (1.0956)	grad_norm 3.7706 (3.1614)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:22:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:13:37 lr 0.000010	 wd 0.0500	time 0.8449 (0.9059)	loss 1.1111 (1.0937)	grad_norm 2.6929 (3.2301)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:24:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:12:06 lr 0.000010	 wd 0.0500	time 0.8405 (0.9057)	loss 1.3562 (1.0959)	grad_norm 2.9570 (3.2253)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:25:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:10:35 lr 0.000010	 wd 0.0500	time 0.8457 (0.9052)	loss 0.9955 (1.0960)	grad_norm 3.0311 (3.2600)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:27:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:09:04 lr 0.000010	 wd 0.0500	time 0.8470 (0.9049)	loss 0.9343 (1.0943)	grad_norm 3.5069 (3.2522)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:28:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:07:34 lr 0.000010	 wd 0.0500	time 0.8546 (0.9045)	loss 1.1847 (1.0970)	grad_norm 2.7061 (3.2577)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:30:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:06:03 lr 0.000010	 wd 0.0500	time 0.8739 (0.9043)	loss 1.1184 (1.0990)	grad_norm 2.6524 (3.2588)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:31:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:04:32 lr 0.000010	 wd 0.0500	time 0.8269 (0.9039)	loss 1.2513 (1.0999)	grad_norm 3.1294 (3.2390)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:33:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:03:02 lr 0.000010	 wd 0.0500	time 0.8359 (0.9036)	loss 1.1150 (1.0979)	grad_norm 2.6949 (3.2430)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:34:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:32 lr 0.000010	 wd 0.0500	time 0.8493 (0.9033)	loss 1.1995 (1.0970)	grad_norm 3.9431 (3.2586)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:36:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.8499 (0.9030)	loss 0.9596 (1.0968)	grad_norm 3.1416 (3.2468)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:36:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 17 training takes 0:37:41
[2024-08-05 15:36:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.768 (12.768)	Loss 0.4463 (0.4463)	Acc@1 92.188 (92.188)	Acc@5 98.828 (98.828)	Mem 23977MB
[2024-08-05 15:36:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.836 Acc@5 98.050
[2024-08-05 15:36:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 15:36:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 15:36:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:31:08 lr 0.000009	 wd 0.0500	time 12.2574 (12.2574)	loss 0.8709 (0.8709)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:38:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:40:32 lr 0.000009	 wd 0.0500	time 0.9181 (1.0128)	loss 1.4577 (1.0981)	grad_norm 2.9177 (3.2096)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:39:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:36:38 lr 0.000009	 wd 0.0500	time 0.8596 (0.9549)	loss 1.2133 (1.1018)	grad_norm 2.6484 (3.2120)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:41:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:34:19 lr 0.000009	 wd 0.0500	time 0.9091 (0.9355)	loss 0.8686 (1.0906)	grad_norm 2.4169 (3.1362)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:42:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:32:26 lr 0.000009	 wd 0.0500	time 0.8547 (0.9259)	loss 1.2358 (1.0927)	grad_norm 2.4813 (3.1111)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 15:44:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:30:42 lr 0.000009	 wd 0.0500	time 0.8572 (0.9202)	loss 0.6636 (1.0927)	grad_norm 1.9591 (3.0653)	loss_scale 2048.0000 (1089.4052)	mem 23977MB
[2024-08-05 15:45:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:29:02 lr 0.000009	 wd 0.0500	time 0.8415 (0.9161)	loss 1.3392 (1.0922)	grad_norm 3.8617 (3.0646)	loss_scale 2048.0000 (1248.9052)	mem 23977MB
[2024-08-05 15:47:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:27:25 lr 0.000009	 wd 0.0500	time 0.8281 (0.9133)	loss 0.8589 (1.0869)	grad_norm 2.8627 (3.2066)	loss_scale 2048.0000 (1362.8987)	mem 23977MB
[2024-08-05 15:48:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:25:50 lr 0.000009	 wd 0.0500	time 0.8452 (0.9112)	loss 1.1329 (1.0846)	grad_norm 2.1628 (3.2289)	loss_scale 2048.0000 (1448.4295)	mem 23977MB
[2024-08-05 15:50:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:24:17 lr 0.000009	 wd 0.0500	time 0.8519 (0.9096)	loss 0.8697 (1.0864)	grad_norm 2.1719 (3.1755)	loss_scale 2048.0000 (1514.9745)	mem 23977MB
[2024-08-05 15:51:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:22:44 lr 0.000009	 wd 0.0500	time 0.9180 (0.9087)	loss 1.1665 (1.0874)	grad_norm 3.1498 (3.2378)	loss_scale 2048.0000 (1568.2238)	mem 23977MB
[2024-08-05 15:53:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:21:12 lr 0.000009	 wd 0.0500	time 0.8968 (0.9079)	loss 0.8136 (1.0873)	grad_norm 2.9977 (3.2055)	loss_scale 2048.0000 (1611.8002)	mem 23977MB
[2024-08-05 15:54:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:19:41 lr 0.000009	 wd 0.0500	time 0.8510 (0.9071)	loss 1.0583 (1.0841)	grad_norm 3.4702 (3.1895)	loss_scale 2048.0000 (1648.1199)	mem 23977MB
[2024-08-05 15:56:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:18:09 lr 0.000009	 wd 0.0500	time 0.8732 (0.9064)	loss 1.3527 (1.0849)	grad_norm 2.4612 (3.1713)	loss_scale 2048.0000 (1678.8563)	mem 23977MB
[2024-08-05 15:57:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:16:38 lr 0.000009	 wd 0.0500	time 0.8454 (0.9057)	loss 0.8571 (1.0841)	grad_norm 2.4193 (3.1735)	loss_scale 2048.0000 (1705.2049)	mem 23977MB
[2024-08-05 15:59:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:15:07 lr 0.000009	 wd 0.0500	time 0.8701 (0.9052)	loss 1.3414 (1.0847)	grad_norm 3.2250 (3.2180)	loss_scale 2048.0000 (1728.0426)	mem 23977MB
[2024-08-05 16:00:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:13:36 lr 0.000009	 wd 0.0500	time 0.8448 (0.9048)	loss 1.2815 (1.0856)	grad_norm 2.0602 (3.1814)	loss_scale 2048.0000 (1748.0275)	mem 23977MB
[2024-08-05 16:02:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:12:05 lr 0.000009	 wd 0.0500	time 0.8412 (0.9044)	loss 1.0426 (1.0863)	grad_norm 2.0900 (3.1488)	loss_scale 2048.0000 (1765.6626)	mem 23977MB
[2024-08-05 16:03:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:10:34 lr 0.000009	 wd 0.0500	time 0.8590 (0.9040)	loss 0.6712 (1.0855)	grad_norm 2.4097 (3.1716)	loss_scale 2048.0000 (1781.3393)	mem 23977MB
[2024-08-05 16:05:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:09:04 lr 0.000009	 wd 0.0500	time 0.8503 (0.9038)	loss 1.0144 (1.0840)	grad_norm 2.1100 (3.1493)	loss_scale 2048.0000 (1795.3666)	mem 23977MB
[2024-08-05 16:06:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:07:33 lr 0.000008	 wd 0.0500	time 0.8396 (0.9036)	loss 1.0548 (1.0848)	grad_norm 2.0279 (3.1497)	loss_scale 2048.0000 (1807.9920)	mem 23977MB
[2024-08-05 16:08:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:06:03 lr 0.000008	 wd 0.0500	time 0.8499 (0.9033)	loss 1.2826 (1.0843)	grad_norm 2.0772 (nan)	loss_scale 1024.0000 (1817.4660)	mem 23977MB
[2024-08-05 16:09:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:04:32 lr 0.000008	 wd 0.0500	time 0.8429 (0.9030)	loss 0.8319 (1.0857)	grad_norm 3.1555 (nan)	loss_scale 1024.0000 (1781.4157)	mem 23977MB
[2024-08-05 16:11:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.8522 (0.9027)	loss 1.1168 (1.0842)	grad_norm 2.8678 (nan)	loss_scale 1024.0000 (1748.4989)	mem 23977MB
[2024-08-05 16:12:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.8495 (0.9025)	loss 1.2939 (1.0852)	grad_norm 2.5903 (nan)	loss_scale 1024.0000 (1718.3240)	mem 23977MB
[2024-08-05 16:14:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.9047 (0.9022)	loss 0.7812 (1.0841)	grad_norm 2.0957 (nan)	loss_scale 1024.0000 (1690.5622)	mem 23977MB
[2024-08-05 16:14:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 18 training takes 0:37:39
[2024-08-05 16:14:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.704 (12.704)	Loss 0.4446 (0.4446)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 16:15:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.828 Acc@5 98.042
[2024-08-05 16:15:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 16:15:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 16:15:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:51:21 lr 0.000008	 wd 0.0500	time 12.7423 (12.7423)	loss 0.8586 (0.8586)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:16:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:40:39 lr 0.000008	 wd 0.0500	time 0.8094 (1.0157)	loss 1.3618 (1.0880)	grad_norm 2.7310 (2.8464)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:18:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:36:39 lr 0.000008	 wd 0.0500	time 0.8452 (0.9553)	loss 1.1305 (1.0781)	grad_norm 3.2035 (2.9982)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:19:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:34:20 lr 0.000008	 wd 0.0500	time 0.8448 (0.9360)	loss 0.7627 (1.0887)	grad_norm 2.5299 (2.9603)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:21:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:32:27 lr 0.000008	 wd 0.0500	time 0.8479 (0.9266)	loss 1.1924 (1.0794)	grad_norm 3.0263 (2.9584)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:22:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:30:43 lr 0.000008	 wd 0.0500	time 0.8482 (0.9210)	loss 1.2367 (1.0778)	grad_norm 2.1058 (2.8886)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:24:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:29:03 lr 0.000008	 wd 0.0500	time 0.8456 (0.9168)	loss 1.3577 (1.0750)	grad_norm 2.8748 (2.9086)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:25:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:27:26 lr 0.000008	 wd 0.0500	time 0.8408 (0.9137)	loss 1.3029 (1.0761)	grad_norm 2.5991 (2.9613)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:27:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:25:51 lr 0.000008	 wd 0.0500	time 0.8571 (0.9116)	loss 1.3137 (1.0755)	grad_norm 3.5799 (2.9852)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:28:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:24:17 lr 0.000008	 wd 0.0500	time 0.8442 (0.9098)	loss 1.2811 (1.0735)	grad_norm 2.2122 (3.0961)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:30:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:22:44 lr 0.000008	 wd 0.0500	time 0.8513 (0.9087)	loss 1.3735 (1.0768)	grad_norm 1.8833 (3.1239)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:31:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:21:12 lr 0.000008	 wd 0.0500	time 0.9024 (0.9079)	loss 0.8167 (1.0782)	grad_norm 2.6346 (3.0915)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:33:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:19:41 lr 0.000008	 wd 0.0500	time 0.8646 (0.9072)	loss 1.2388 (1.0797)	grad_norm 2.4106 (3.1066)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:34:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:18:09 lr 0.000008	 wd 0.0500	time 0.8832 (0.9065)	loss 1.0020 (1.0821)	grad_norm 1.8314 (3.1087)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:36:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:16:38 lr 0.000008	 wd 0.0500	time 0.8398 (0.9061)	loss 1.1971 (1.0823)	grad_norm 3.6027 (3.1210)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:37:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:15:07 lr 0.000008	 wd 0.0500	time 0.8415 (0.9056)	loss 0.7562 (1.0789)	grad_norm 3.1816 (3.1412)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:39:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:13:36 lr 0.000007	 wd 0.0500	time 0.8421 (0.9050)	loss 1.3309 (1.0776)	grad_norm 2.0263 (3.1884)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:40:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:12:05 lr 0.000007	 wd 0.0500	time 0.8354 (0.9046)	loss 1.1891 (1.0802)	grad_norm 3.1098 (3.1694)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:42:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:10:34 lr 0.000007	 wd 0.0500	time 0.8511 (0.9042)	loss 0.8619 (1.0796)	grad_norm 2.7723 (3.1917)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:43:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:09:04 lr 0.000007	 wd 0.0500	time 0.8340 (0.9038)	loss 1.1967 (1.0797)	grad_norm 2.0964 (3.1968)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:45:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:07:33 lr 0.000007	 wd 0.0500	time 0.8426 (0.9034)	loss 1.0307 (1.0798)	grad_norm 3.6310 (3.1772)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:46:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:06:03 lr 0.000007	 wd 0.0500	time 0.8456 (0.9031)	loss 0.7999 (1.0782)	grad_norm 4.3562 (3.1585)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:48:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:04:32 lr 0.000007	 wd 0.0500	time 0.8581 (0.9028)	loss 0.7939 (1.0792)	grad_norm 3.2347 (3.1635)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:49:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:03:02 lr 0.000007	 wd 0.0500	time 0.9081 (0.9025)	loss 1.2887 (1.0799)	grad_norm 2.2274 (3.1679)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:51:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:32 lr 0.000007	 wd 0.0500	time 0.8448 (0.9025)	loss 1.2398 (1.0800)	grad_norm 2.2255 (3.1729)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:52:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.8422 (0.9022)	loss 0.7909 (1.0797)	grad_norm 2.3728 (3.1776)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:52:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 19 training takes 0:37:39
[2024-08-05 16:52:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.153 (12.153)	Loss 0.4536 (0.4536)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 16:53:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.792 Acc@5 98.038
[2024-08-05 16:53:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 16:53:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 16:53:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][0/2502]	eta 8:56:33 lr 0.000007	 wd 0.0500	time 12.8671 (12.8671)	loss 1.3495 (1.3495)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:54:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:40:41 lr 0.000007	 wd 0.0500	time 0.8154 (1.0166)	loss 1.3484 (1.1498)	grad_norm 4.0524 (3.1164)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:56:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:36:46 lr 0.000007	 wd 0.0500	time 0.8389 (0.9585)	loss 1.4231 (1.1248)	grad_norm 2.3762 (2.9709)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:57:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:34:24 lr 0.000007	 wd 0.0500	time 0.8482 (0.9376)	loss 1.3098 (1.1274)	grad_norm 2.4160 (2.9255)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 16:59:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:32:29 lr 0.000007	 wd 0.0500	time 0.8538 (0.9277)	loss 1.2253 (1.1090)	grad_norm 2.2931 (2.9566)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 17:00:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:30:43 lr 0.000007	 wd 0.0500	time 0.8431 (0.9210)	loss 1.2260 (1.1054)	grad_norm 2.8485 (2.9643)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 17:02:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:29:04 lr 0.000007	 wd 0.0500	time 0.8480 (0.9172)	loss 1.1559 (1.0995)	grad_norm 1.9973 (2.9345)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 17:03:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:27:27 lr 0.000007	 wd 0.0500	time 0.8530 (0.9145)	loss 1.0634 (1.0917)	grad_norm 2.3863 (2.9685)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 17:05:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:25:53 lr 0.000007	 wd 0.0500	time 0.7843 (0.9127)	loss 1.2261 (1.0906)	grad_norm 2.1593 (2.9540)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 17:06:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:24:19 lr 0.000007	 wd 0.0500	time 0.8302 (0.9110)	loss 1.0056 (1.0907)	grad_norm 2.8000 (2.9470)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 17:08:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:22:46 lr 0.000007	 wd 0.0500	time 0.8482 (0.9100)	loss 1.2399 (1.0888)	grad_norm 18.4079 (2.9508)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 17:09:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:21:14 lr 0.000007	 wd 0.0500	time 0.8442 (0.9088)	loss 1.2044 (1.0845)	grad_norm 2.4706 (2.9591)	loss_scale 2048.0000 (1031.4405)	mem 23977MB
[2024-08-05 17:11:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:19:41 lr 0.000006	 wd 0.0500	time 0.8492 (0.9078)	loss 0.9778 (1.0884)	grad_norm 3.0115 (2.9896)	loss_scale 2048.0000 (1116.0833)	mem 23977MB
[2024-08-05 17:12:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:18:10 lr 0.000006	 wd 0.0500	time 0.8509 (0.9070)	loss 0.7770 (1.0878)	grad_norm 2.5122 (3.0084)	loss_scale 2048.0000 (1187.7141)	mem 23977MB
[2024-08-05 17:14:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:16:38 lr 0.000006	 wd 0.0500	time 0.8480 (0.9064)	loss 1.3164 (1.0871)	grad_norm 2.2957 (3.0730)	loss_scale 2048.0000 (1249.1192)	mem 23977MB
[2024-08-05 17:15:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:15:07 lr 0.000006	 wd 0.0500	time 0.7891 (0.9058)	loss 1.0844 (1.0877)	grad_norm 3.1985 (3.0806)	loss_scale 2048.0000 (1302.3424)	mem 23977MB
[2024-08-05 17:17:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:13:36 lr 0.000006	 wd 0.0500	time 0.8277 (0.9053)	loss 0.9704 (1.0866)	grad_norm 2.0595 (3.0805)	loss_scale 2048.0000 (1348.9169)	mem 23977MB
[2024-08-05 17:18:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:12:05 lr 0.000006	 wd 0.0500	time 0.8575 (0.9049)	loss 1.0735 (1.0878)	grad_norm 2.7970 (3.0903)	loss_scale 2048.0000 (1390.0153)	mem 23977MB
[2024-08-05 17:20:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:10:35 lr 0.000006	 wd 0.0500	time 0.8440 (0.9046)	loss 1.2974 (1.0877)	grad_norm 2.4242 (3.0864)	loss_scale 2048.0000 (1426.5497)	mem 23977MB
[2024-08-05 17:21:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:09:04 lr 0.000006	 wd 0.0500	time 0.8592 (0.9041)	loss 1.0142 (1.0887)	grad_norm 2.6568 (3.0646)	loss_scale 2048.0000 (1459.2404)	mem 23977MB
[2024-08-05 17:23:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:07:33 lr 0.000006	 wd 0.0500	time 0.8446 (0.9036)	loss 1.2690 (1.0883)	grad_norm 5.7251 (3.0911)	loss_scale 2048.0000 (1488.6637)	mem 23977MB
[2024-08-05 17:24:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:06:03 lr 0.000006	 wd 0.0500	time 0.9066 (0.9033)	loss 1.2136 (1.0885)	grad_norm 1.9713 (3.0785)	loss_scale 2048.0000 (1515.2861)	mem 23977MB
[2024-08-05 17:26:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:04:32 lr 0.000006	 wd 0.0500	time 0.8487 (0.9029)	loss 0.9872 (1.0864)	grad_norm 2.1080 (3.1319)	loss_scale 2048.0000 (1539.4893)	mem 23977MB
[2024-08-05 17:27:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:03:02 lr 0.000006	 wd 0.0500	time 0.8570 (0.9027)	loss 1.2818 (1.0860)	grad_norm 2.5143 (3.1222)	loss_scale 2048.0000 (1561.5889)	mem 23977MB
[2024-08-05 17:29:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:32 lr 0.000006	 wd 0.0500	time 0.8407 (0.9024)	loss 0.8814 (1.0888)	grad_norm 2.8866 (3.1191)	loss_scale 2048.0000 (1581.8476)	mem 23977MB
[2024-08-05 17:30:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.8266 (0.9021)	loss 0.8100 (1.0875)	grad_norm 2.6024 (3.1296)	loss_scale 2048.0000 (1600.4862)	mem 23977MB
[2024-08-05 17:30:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 20 training takes 0:37:39
[2024-08-05 17:31:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.041 (12.041)	Loss 0.4353 (0.4353)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 17:31:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.852 Acc@5 97.994
[2024-08-05 17:31:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-05 17:31:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 17:31:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][0/2502]	eta 9:18:55 lr 0.000006	 wd 0.0500	time 13.4036 (13.4036)	loss 1.3406 (1.3406)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 17:33:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:40:51 lr 0.000006	 wd 0.0500	time 0.8043 (1.0207)	loss 1.0996 (1.0942)	grad_norm 3.3841 (3.3001)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 17:34:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:36:54 lr 0.000006	 wd 0.0500	time 0.8684 (0.9619)	loss 1.2654 (1.1108)	grad_norm 2.7425 (3.1383)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 17:36:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:34:31 lr 0.000006	 wd 0.0500	time 0.8415 (0.9406)	loss 0.8711 (1.1096)	grad_norm 3.1833 (3.1042)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 17:37:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:32:34 lr 0.000006	 wd 0.0500	time 0.8473 (0.9297)	loss 1.3778 (1.1082)	grad_norm 5.0287 (3.0315)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 17:39:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:30:48 lr 0.000006	 wd 0.0500	time 0.8090 (0.9232)	loss 0.6085 (1.0998)	grad_norm 5.0389 (3.0646)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 17:40:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:29:07 lr 0.000006	 wd 0.0500	time 0.8260 (0.9188)	loss 0.9317 (1.0962)	grad_norm 4.7276 (nan)	loss_scale 1024.0000 (1983.2546)	mem 23977MB
[2024-08-05 17:42:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:27:30 lr 0.000006	 wd 0.0500	time 0.7866 (0.9159)	loss 1.1556 (1.0946)	grad_norm 5.2815 (nan)	loss_scale 1024.0000 (1846.4137)	mem 23977MB
[2024-08-05 17:43:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:25:55 lr 0.000006	 wd 0.0500	time 0.8517 (0.9136)	loss 0.7868 (1.0907)	grad_norm 6.0220 (nan)	loss_scale 1024.0000 (1743.7403)	mem 23977MB
[2024-08-05 17:45:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:24:21 lr 0.000005	 wd 0.0500	time 0.9049 (0.9122)	loss 0.7055 (1.0902)	grad_norm 2.3205 (nan)	loss_scale 1024.0000 (1663.8579)	mem 23977MB
[2024-08-05 17:46:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:22:48 lr 0.000005	 wd 0.0500	time 0.8505 (0.9109)	loss 0.9164 (1.0875)	grad_norm 2.1762 (nan)	loss_scale 1024.0000 (1599.9361)	mem 23977MB
[2024-08-05 17:48:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:21:15 lr 0.000005	 wd 0.0500	time 0.8195 (0.9097)	loss 1.4301 (1.0878)	grad_norm 2.5580 (nan)	loss_scale 1024.0000 (1547.6258)	mem 23977MB
[2024-08-05 17:49:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:19:43 lr 0.000005	 wd 0.0500	time 0.8375 (0.9087)	loss 1.3235 (1.0872)	grad_norm 3.4723 (nan)	loss_scale 1024.0000 (1504.0266)	mem 23977MB
[2024-08-05 17:51:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:18:11 lr 0.000005	 wd 0.0500	time 0.8454 (0.9083)	loss 1.1830 (1.0902)	grad_norm 4.4308 (nan)	loss_scale 1024.0000 (1467.1299)	mem 23977MB
[2024-08-05 17:52:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:16:39 lr 0.000005	 wd 0.0500	time 0.8486 (0.9074)	loss 1.1375 (1.0904)	grad_norm 2.9224 (nan)	loss_scale 1024.0000 (1435.5004)	mem 23977MB
[2024-08-05 17:54:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:15:08 lr 0.000005	 wd 0.0500	time 0.8479 (0.9067)	loss 1.2464 (1.0887)	grad_norm 2.5435 (nan)	loss_scale 1024.0000 (1408.0853)	mem 23977MB
[2024-08-05 17:55:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:13:37 lr 0.000005	 wd 0.0500	time 0.8537 (0.9062)	loss 0.7150 (1.0879)	grad_norm 2.5282 (nan)	loss_scale 1024.0000 (1384.0949)	mem 23977MB
[2024-08-05 17:57:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:12:06 lr 0.000005	 wd 0.0500	time 0.8475 (0.9059)	loss 0.7470 (1.0870)	grad_norm 2.9547 (nan)	loss_scale 1024.0000 (1362.9253)	mem 23977MB
[2024-08-05 17:58:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:10:35 lr 0.000005	 wd 0.0500	time 0.8301 (0.9054)	loss 0.8651 (1.0871)	grad_norm 2.5987 (nan)	loss_scale 1024.0000 (1344.1066)	mem 23977MB
[2024-08-05 18:00:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:09:04 lr 0.000005	 wd 0.0500	time 0.8443 (0.9051)	loss 1.1877 (1.0877)	grad_norm 3.6560 (nan)	loss_scale 1024.0000 (1327.2678)	mem 23977MB
[2024-08-05 18:01:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:07:34 lr 0.000005	 wd 0.0500	time 0.8907 (0.9048)	loss 1.1600 (1.0861)	grad_norm 2.5154 (nan)	loss_scale 1024.0000 (1312.1119)	mem 23977MB
[2024-08-05 18:03:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:06:03 lr 0.000005	 wd 0.0500	time 0.8457 (0.9045)	loss 1.1852 (1.0851)	grad_norm 2.3696 (nan)	loss_scale 1024.0000 (1298.3989)	mem 23977MB
[2024-08-05 18:04:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:04:33 lr 0.000005	 wd 0.0500	time 0.8460 (0.9044)	loss 1.2877 (1.0844)	grad_norm 2.4840 (nan)	loss_scale 1024.0000 (1285.9318)	mem 23977MB
[2024-08-05 18:06:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:03:02 lr 0.000005	 wd 0.0500	time 0.8745 (0.9042)	loss 0.7848 (1.0840)	grad_norm 2.1532 (nan)	loss_scale 1024.0000 (1274.5485)	mem 23977MB
[2024-08-05 18:07:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:32 lr 0.000005	 wd 0.0500	time 0.8378 (0.9039)	loss 0.6927 (1.0832)	grad_norm 2.5373 (nan)	loss_scale 1024.0000 (1264.1133)	mem 23977MB
[2024-08-05 18:09:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.8402 (0.9036)	loss 1.4299 (1.0823)	grad_norm 2.0542 (nan)	loss_scale 1024.0000 (1254.5126)	mem 23977MB
[2024-08-05 18:09:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 21 training takes 0:37:43
[2024-08-05 18:09:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.565 (12.565)	Loss 0.4243 (0.4243)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 18:09:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.812 Acc@5 98.064
[2024-08-05 18:09:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 18:09:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 18:09:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:29:58 lr 0.000005	 wd 0.0500	time 12.2297 (12.2297)	loss 1.3844 (1.3844)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:11:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:40:24 lr 0.000005	 wd 0.0500	time 0.8388 (1.0095)	loss 0.8083 (1.0876)	grad_norm 2.7717 (2.8401)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:12:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:36:34 lr 0.000005	 wd 0.0500	time 0.8365 (0.9535)	loss 1.2165 (1.0757)	grad_norm 2.6366 (2.9739)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:14:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:34:18 lr 0.000005	 wd 0.0500	time 0.8121 (0.9348)	loss 0.9964 (1.0742)	grad_norm 2.2120 (3.0401)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:15:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:32:26 lr 0.000005	 wd 0.0500	time 0.8516 (0.9258)	loss 0.8397 (1.0719)	grad_norm 7.7516 (3.0254)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:17:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:30:43 lr 0.000005	 wd 0.0500	time 0.8379 (0.9209)	loss 0.7159 (1.0799)	grad_norm 2.6733 (2.9608)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:18:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:29:06 lr 0.000005	 wd 0.0500	time 0.8336 (0.9181)	loss 1.3717 (1.0845)	grad_norm 3.7762 (2.9257)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:20:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:27:29 lr 0.000005	 wd 0.0500	time 0.8401 (0.9153)	loss 1.2764 (1.0867)	grad_norm 2.1911 (3.0779)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:21:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:25:54 lr 0.000004	 wd 0.0500	time 0.8333 (0.9133)	loss 1.1887 (1.0878)	grad_norm 2.9601 (3.0360)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:23:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:24:20 lr 0.000004	 wd 0.0500	time 0.8353 (0.9114)	loss 1.2920 (1.0870)	grad_norm 2.5828 (3.0384)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:24:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:22:46 lr 0.000004	 wd 0.0500	time 0.8536 (0.9099)	loss 0.7344 (1.0864)	grad_norm 2.3784 (3.0544)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:26:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:21:14 lr 0.000004	 wd 0.0500	time 0.8519 (0.9091)	loss 0.8316 (1.0891)	grad_norm 2.2643 (3.0531)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:27:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:19:42 lr 0.000004	 wd 0.0500	time 0.8381 (0.9081)	loss 0.7887 (1.0884)	grad_norm 2.5255 (3.0558)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:29:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:18:10 lr 0.000004	 wd 0.0500	time 0.7928 (0.9074)	loss 0.8492 (1.0878)	grad_norm 2.0025 (3.0539)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:30:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:16:39 lr 0.000004	 wd 0.0500	time 0.8491 (0.9067)	loss 0.7612 (1.0887)	grad_norm 2.3525 (3.1169)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:32:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:15:08 lr 0.000004	 wd 0.0500	time 0.8478 (0.9063)	loss 0.8183 (1.0866)	grad_norm 3.0837 (3.1009)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:33:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:13:37 lr 0.000004	 wd 0.0500	time 0.8778 (0.9059)	loss 0.8600 (1.0873)	grad_norm 2.6371 (3.0889)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:35:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:12:06 lr 0.000004	 wd 0.0500	time 0.8516 (0.9054)	loss 0.6910 (1.0868)	grad_norm 16.3395 (3.1161)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:36:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:10:35 lr 0.000004	 wd 0.0500	time 0.8594 (0.9050)	loss 1.4237 (1.0845)	grad_norm 2.7069 (3.1221)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:38:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:09:04 lr 0.000004	 wd 0.0500	time 0.8540 (0.9047)	loss 0.8411 (1.0840)	grad_norm 2.0714 (3.1087)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:39:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:07:33 lr 0.000004	 wd 0.0500	time 0.8567 (0.9042)	loss 0.7461 (1.0848)	grad_norm 2.0368 (3.1043)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 18:41:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:06:03 lr 0.000004	 wd 0.0500	time 0.8590 (0.9038)	loss 0.7818 (1.0839)	grad_norm 2.6257 (3.0889)	loss_scale 2048.0000 (1043.4955)	mem 23977MB
[2024-08-05 18:42:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:04:32 lr 0.000004	 wd 0.0500	time 0.8781 (0.9035)	loss 0.8427 (1.0833)	grad_norm 2.5445 (3.1070)	loss_scale 2048.0000 (1089.1340)	mem 23977MB
[2024-08-05 18:44:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:03:02 lr 0.000004	 wd 0.0500	time 0.9008 (0.9033)	loss 0.8798 (1.0826)	grad_norm 2.4856 (3.1465)	loss_scale 2048.0000 (1130.8057)	mem 23977MB
[2024-08-05 18:45:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:32 lr 0.000004	 wd 0.0500	time 0.8796 (0.9032)	loss 1.2846 (1.0835)	grad_norm 2.1447 (3.1466)	loss_scale 2048.0000 (1169.0062)	mem 23977MB
[2024-08-05 18:47:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.8528 (0.9030)	loss 1.2882 (1.0838)	grad_norm 2.8872 (3.1502)	loss_scale 2048.0000 (1204.1519)	mem 23977MB
[2024-08-05 18:47:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 22 training takes 0:37:41
[2024-08-05 18:47:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 13.538 (13.538)	Loss 0.4590 (0.4590)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 18:48:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.896 Acc@5 98.044
[2024-08-05 18:48:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-05 18:48:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 18:48:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:38:41 lr 0.000004	 wd 0.0500	time 12.4386 (12.4386)	loss 1.2359 (1.2359)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 18:49:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:40:37 lr 0.000004	 wd 0.0500	time 0.8392 (1.0148)	loss 0.7798 (1.0857)	grad_norm 2.4983 (4.0067)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 18:51:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:36:44 lr 0.000004	 wd 0.0500	time 0.8656 (0.9576)	loss 1.2399 (1.0823)	grad_norm 2.1597 (3.3645)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 18:52:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:34:23 lr 0.000004	 wd 0.0500	time 0.8486 (0.9370)	loss 0.9868 (1.0751)	grad_norm 3.2993 (3.2078)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 18:54:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:32:28 lr 0.000004	 wd 0.0500	time 0.8345 (0.9270)	loss 0.6766 (1.0657)	grad_norm 2.2151 (3.1380)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 18:55:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:30:43 lr 0.000004	 wd 0.0500	time 0.8659 (0.9209)	loss 1.2501 (1.0687)	grad_norm 3.7095 (3.0973)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 18:57:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:29:04 lr 0.000004	 wd 0.0500	time 0.8461 (0.9171)	loss 1.1823 (1.0681)	grad_norm 2.4141 (3.0818)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 18:58:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:27:28 lr 0.000004	 wd 0.0500	time 0.8625 (0.9147)	loss 0.9674 (1.0692)	grad_norm 2.1658 (3.0844)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:00:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:25:53 lr 0.000003	 wd 0.0500	time 0.8431 (0.9126)	loss 1.2868 (1.0702)	grad_norm 2.0755 (3.1680)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:01:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:24:19 lr 0.000003	 wd 0.0500	time 0.9439 (0.9110)	loss 1.2652 (1.0699)	grad_norm 3.3535 (3.1224)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:03:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:22:46 lr 0.000003	 wd 0.0500	time 0.7889 (0.9100)	loss 1.1855 (1.0704)	grad_norm 3.9767 (3.1990)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:04:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:21:13 lr 0.000003	 wd 0.0500	time 0.8467 (0.9087)	loss 0.8233 (1.0744)	grad_norm 1.9810 (3.2394)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:06:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:19:41 lr 0.000003	 wd 0.0500	time 0.8366 (0.9075)	loss 0.7185 (1.0743)	grad_norm 2.5724 (3.2732)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:07:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:18:09 lr 0.000003	 wd 0.0500	time 0.8436 (0.9067)	loss 1.3527 (1.0773)	grad_norm 2.5281 (3.2675)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:09:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:16:38 lr 0.000003	 wd 0.0500	time 0.8523 (0.9060)	loss 1.3317 (1.0785)	grad_norm 8.7971 (3.2377)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:10:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:15:07 lr 0.000003	 wd 0.0500	time 0.8535 (0.9056)	loss 1.1637 (1.0813)	grad_norm 2.2118 (3.2239)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:12:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:13:36 lr 0.000003	 wd 0.0500	time 0.8465 (0.9051)	loss 1.1752 (1.0825)	grad_norm 2.6645 (3.2522)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:13:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:12:05 lr 0.000003	 wd 0.0500	time 0.8297 (0.9046)	loss 0.9148 (1.0817)	grad_norm 2.6572 (3.2482)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:15:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:10:34 lr 0.000003	 wd 0.0500	time 0.8102 (0.9041)	loss 0.8712 (1.0804)	grad_norm 2.2886 (3.2361)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:16:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:09:04 lr 0.000003	 wd 0.0500	time 0.8523 (0.9040)	loss 0.7067 (1.0813)	grad_norm 2.2757 (3.2161)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:18:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:07:33 lr 0.000003	 wd 0.0500	time 0.8160 (0.9037)	loss 1.3560 (1.0826)	grad_norm 10.7325 (3.2239)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:19:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:06:03 lr 0.000003	 wd 0.0500	time 0.9006 (0.9035)	loss 1.2944 (1.0836)	grad_norm 2.3847 (3.2165)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:21:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:04:32 lr 0.000003	 wd 0.0500	time 0.8559 (0.9032)	loss 1.0812 (1.0834)	grad_norm 2.4799 (3.1952)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:22:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:03:02 lr 0.000003	 wd 0.0500	time 0.8094 (0.9029)	loss 0.8338 (1.0817)	grad_norm 12.9460 (3.1912)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:24:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:32 lr 0.000003	 wd 0.0500	time 0.7970 (0.9026)	loss 1.2719 (1.0813)	grad_norm 2.2555 (3.1856)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:25:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.8320 (0.9024)	loss 0.6719 (1.0808)	grad_norm 2.9716 (3.2094)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:25:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 23 training takes 0:37:40
[2024-08-05 19:25:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.212 (12.212)	Loss 0.4431 (0.4431)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 19:26:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.848 Acc@5 98.026
[2024-08-05 19:26:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-05 19:26:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 19:26:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:58:03 lr 0.000003	 wd 0.0500	time 12.9030 (12.9030)	loss 1.1826 (1.1826)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:27:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:40:45 lr 0.000003	 wd 0.0500	time 0.8486 (1.0182)	loss 1.2199 (1.0542)	grad_norm 16.5804 (3.2942)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:29:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:36:46 lr 0.000003	 wd 0.0500	time 0.8516 (0.9585)	loss 1.2603 (1.0479)	grad_norm 5.0439 (3.1889)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:30:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:34:26 lr 0.000003	 wd 0.0500	time 0.8468 (0.9385)	loss 1.2225 (1.0621)	grad_norm 2.6236 (3.0532)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:32:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:32:31 lr 0.000003	 wd 0.0500	time 0.8434 (0.9285)	loss 1.2700 (1.0643)	grad_norm 2.8688 (2.9751)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:33:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:30:47 lr 0.000003	 wd 0.0500	time 0.8408 (0.9227)	loss 1.3633 (1.0730)	grad_norm 5.8826 (3.0156)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:35:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:29:06 lr 0.000003	 wd 0.0500	time 0.8437 (0.9185)	loss 0.7033 (1.0747)	grad_norm 3.3601 (3.0209)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:36:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:27:29 lr 0.000003	 wd 0.0500	time 0.7888 (0.9156)	loss 1.3604 (1.0728)	grad_norm 2.5668 (3.0171)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:38:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:25:54 lr 0.000003	 wd 0.0500	time 0.8458 (0.9131)	loss 1.0520 (1.0731)	grad_norm 6.1586 (3.0605)	loss_scale 2048.0000 (2048.0000)	mem 23977MB
[2024-08-05 19:39:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:24:19 lr 0.000003	 wd 0.0500	time 0.8508 (0.9113)	loss 1.3041 (1.0769)	grad_norm 2.0340 (nan)	loss_scale 1024.0000 (1945.7137)	mem 23977MB
[2024-08-05 19:41:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:22:46 lr 0.000003	 wd 0.0500	time 0.8485 (0.9099)	loss 0.9721 (1.0768)	grad_norm 2.1635 (nan)	loss_scale 1024.0000 (1853.6344)	mem 23977MB
[2024-08-05 19:42:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:21:14 lr 0.000003	 wd 0.0500	time 0.8414 (0.9088)	loss 1.0185 (1.0808)	grad_norm 2.4119 (nan)	loss_scale 1024.0000 (1778.2816)	mem 23977MB
[2024-08-05 19:44:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:19:41 lr 0.000002	 wd 0.0500	time 0.8451 (0.9078)	loss 0.7589 (1.0795)	grad_norm 4.3321 (nan)	loss_scale 1024.0000 (1715.4771)	mem 23977MB
[2024-08-05 19:45:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:18:10 lr 0.000002	 wd 0.0500	time 0.8421 (0.9071)	loss 1.2180 (1.0772)	grad_norm 3.2060 (nan)	loss_scale 1024.0000 (1662.3274)	mem 23977MB
[2024-08-05 19:47:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:16:38 lr 0.000002	 wd 0.0500	time 0.8385 (0.9064)	loss 1.2092 (1.0781)	grad_norm 2.3499 (nan)	loss_scale 1024.0000 (1616.7652)	mem 23977MB
[2024-08-05 19:48:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:15:07 lr 0.000002	 wd 0.0500	time 0.8497 (0.9057)	loss 1.1569 (1.0780)	grad_norm 2.5639 (nan)	loss_scale 1024.0000 (1577.2738)	mem 23977MB
[2024-08-05 19:50:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:13:36 lr 0.000002	 wd 0.0500	time 0.8462 (0.9052)	loss 1.2831 (1.0780)	grad_norm 4.7856 (nan)	loss_scale 1024.0000 (1542.7158)	mem 23977MB
[2024-08-05 19:51:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:12:05 lr 0.000002	 wd 0.0500	time 0.8893 (0.9046)	loss 1.3095 (1.0796)	grad_norm 2.7645 (nan)	loss_scale 1024.0000 (1512.2210)	mem 23977MB
[2024-08-05 19:53:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:10:34 lr 0.000002	 wd 0.0500	time 0.8812 (0.9044)	loss 0.7863 (1.0787)	grad_norm 2.9917 (nan)	loss_scale 1024.0000 (1485.1127)	mem 23977MB
[2024-08-05 19:54:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:09:04 lr 0.000002	 wd 0.0500	time 0.8444 (0.9040)	loss 1.2267 (1.0780)	grad_norm 4.5595 (nan)	loss_scale 1024.0000 (1460.8564)	mem 23977MB
[2024-08-05 19:56:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:07:33 lr 0.000002	 wd 0.0500	time 0.8364 (0.9037)	loss 0.9763 (1.0792)	grad_norm 2.8189 (nan)	loss_scale 1024.0000 (1439.0245)	mem 23977MB
[2024-08-05 19:57:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:06:03 lr 0.000002	 wd 0.0500	time 0.8362 (0.9034)	loss 1.3546 (1.0793)	grad_norm 3.9985 (nan)	loss_scale 1024.0000 (1419.2708)	mem 23977MB
[2024-08-05 19:59:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:04:32 lr 0.000002	 wd 0.0500	time 0.8832 (0.9032)	loss 1.4026 (1.0794)	grad_norm 2.9005 (nan)	loss_scale 1024.0000 (1401.3121)	mem 23977MB
[2024-08-05 20:00:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:03:02 lr 0.000002	 wd 0.0500	time 0.8988 (0.9030)	loss 0.8537 (1.0796)	grad_norm 2.2678 (nan)	loss_scale 1024.0000 (1384.9144)	mem 23977MB
[2024-08-05 20:02:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:32 lr 0.000002	 wd 0.0500	time 0.8791 (0.9029)	loss 0.9328 (1.0780)	grad_norm 2.8474 (nan)	loss_scale 1024.0000 (1369.8825)	mem 23977MB
[2024-08-05 20:03:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.8448 (0.9026)	loss 1.3785 (1.0779)	grad_norm 10.3449 (nan)	loss_scale 1024.0000 (1356.0528)	mem 23977MB
[2024-08-05 20:03:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 249): INFO EPOCH 24 training takes 0:37:40
[2024-08-05 20:04:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.987 (11.987)	Loss 0.4399 (0.4399)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23977MB
[2024-08-05 20:04:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 296): INFO  * Acc@1 86.870 Acc@5 98.036
[2024-08-05 20:04:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-05 20:04:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 182): INFO Max accuracy: 86.90%
[2024-08-05 20:04:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][0/2502]	eta 9:02:02 lr 0.000002	 wd 0.0500	time 12.9986 (12.9986)	loss 1.2922 (1.2922)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:06:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:40:42 lr 0.000002	 wd 0.0500	time 0.8432 (1.0169)	loss 1.2594 (1.1014)	grad_norm 2.0298 (2.8937)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:07:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:36:42 lr 0.000002	 wd 0.0500	time 0.8474 (0.9568)	loss 0.8147 (1.0944)	grad_norm 2.7554 (2.9235)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:09:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:34:23 lr 0.000002	 wd 0.0500	time 0.8438 (0.9370)	loss 1.2643 (1.0898)	grad_norm 2.3782 (2.8502)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:10:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:32:28 lr 0.000002	 wd 0.0500	time 0.8626 (0.9270)	loss 0.6613 (1.0908)	grad_norm 2.9745 (2.8794)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:12:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:30:44 lr 0.000002	 wd 0.0500	time 0.8455 (0.9212)	loss 1.3088 (1.0884)	grad_norm 3.1931 (2.9432)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:13:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:29:05 lr 0.000002	 wd 0.0500	time 0.7972 (0.9176)	loss 1.2554 (1.0837)	grad_norm 3.7753 (2.9834)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:15:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:27:28 lr 0.000002	 wd 0.0500	time 0.8435 (0.9147)	loss 1.0876 (1.0894)	grad_norm 6.1925 (3.1112)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:16:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:25:53 lr 0.000002	 wd 0.0500	time 0.8476 (0.9128)	loss 1.2896 (1.0939)	grad_norm 2.2733 (3.1247)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:18:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:24:19 lr 0.000002	 wd 0.0500	time 0.8156 (0.9111)	loss 0.8851 (1.0933)	grad_norm 2.2057 (3.1045)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:19:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:22:46 lr 0.000002	 wd 0.0500	time 0.8561 (0.9101)	loss 1.5110 (1.0916)	grad_norm 2.6969 (3.1053)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:21:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:21:14 lr 0.000002	 wd 0.0500	time 0.9106 (0.9089)	loss 1.0276 (1.0931)	grad_norm 2.6452 (3.1218)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:22:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:19:42 lr 0.000002	 wd 0.0500	time 0.8421 (0.9081)	loss 0.9791 (1.0879)	grad_norm 1.9880 (3.0976)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:24:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:18:10 lr 0.000002	 wd 0.0500	time 0.7928 (0.9073)	loss 1.3287 (1.0881)	grad_norm 2.2140 (3.0890)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
[2024-08-05 20:25:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence3-full-ft] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:16:39 lr 0.000002	 wd 0.0500	time 0.8454 (0.9067)	loss 1.0992 (1.0890)	grad_norm 2.1018 (3.0651)	loss_scale 1024.0000 (1024.0000)	mem 23977MB
