[2024-07-29 19:32:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/config.json
[2024-07-29 19:32:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_part0
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_sequence_cross0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-29 19:32:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_sequence_cross0.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_sequence_cross0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-29 19:32:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0
[2024-07-29 19:32:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-29 19:32:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 113): INFO number of params: 43621480
[2024-07-29 19:32:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0, ignoring auto resume
[2024-07-29 19:32:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth for fine-tuning......
[2024-07-29 19:32:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-29 19:32:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-29 19:32:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth'
[2024-07-29 19:33:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 13.392 (13.392)	Loss 0.4011 (0.4011)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 2536MB
[2024-07-29 19:33:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 84.496 Acc@5 97.134
[2024-07-29 19:33:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-29 19:33:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 168): INFO Start training
[2024-07-29 19:33:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:53:11 lr 0.000000	 wd 0.0500	time 14.2252 (14.2252)	loss 1.6913 (1.6913)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 20188MB
[2024-07-29 19:34:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:30:34 lr 0.000000	 wd 0.0500	time 0.5856 (0.7638)	loss 1.5037 (1.3248)	grad_norm 2.9720 (inf)	loss_scale 16384.0000 (20115.0099)	mem 20188MB
[2024-07-29 19:35:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:26:41 lr 0.000000	 wd 0.0500	time 0.6267 (0.6955)	loss 1.1544 (1.3198)	grad_norm 2.0951 (nan)	loss_scale 8192.0000 (14183.1642)	mem 20188MB
[2024-07-29 19:36:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:24:40 lr 0.000000	 wd 0.0500	time 0.5797 (0.6721)	loss 0.9201 (1.2742)	grad_norm 2.7710 (nan)	loss_scale 4096.0000 (11512.3455)	mem 20188MB
[2024-07-29 19:37:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:23:08 lr 0.000001	 wd 0.0500	time 0.6019 (0.6605)	loss 1.1726 (1.2823)	grad_norm 3.5128 (nan)	loss_scale 4096.0000 (9662.8828)	mem 20188MB
[2024-07-29 19:38:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:21:48 lr 0.000001	 wd 0.0500	time 0.5927 (0.6534)	loss 1.2869 (1.2830)	grad_norm 8.3482 (nan)	loss_scale 4096.0000 (8551.7285)	mem 20188MB
[2024-07-29 19:39:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:20:34 lr 0.000001	 wd 0.0500	time 0.5891 (0.6488)	loss 1.4352 (1.2788)	grad_norm 2.9295 (nan)	loss_scale 4096.0000 (7810.3428)	mem 20188MB
[2024-07-29 19:41:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:19:23 lr 0.000001	 wd 0.0500	time 0.5946 (0.6454)	loss 1.4087 (1.2799)	grad_norm 5.9913 (nan)	loss_scale 2048.0000 (7140.2454)	mem 20188MB
[2024-07-29 19:42:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:18:14 lr 0.000001	 wd 0.0500	time 0.5878 (0.6430)	loss 1.1672 (1.2755)	grad_norm 2.9811 (nan)	loss_scale 2048.0000 (6504.5094)	mem 20188MB
[2024-07-29 19:43:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:17:06 lr 0.000001	 wd 0.0500	time 0.5905 (0.6410)	loss 1.4150 (1.2733)	grad_norm 2.2852 (nan)	loss_scale 2048.0000 (6009.8912)	mem 20188MB
[2024-07-29 19:44:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:16:00 lr 0.000002	 wd 0.0500	time 0.5855 (0.6395)	loss 1.5378 (1.2685)	grad_norm 3.4963 (nan)	loss_scale 2048.0000 (5614.0979)	mem 20188MB
[2024-07-29 19:45:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:14:54 lr 0.000002	 wd 0.0500	time 0.5841 (0.6381)	loss 1.4245 (1.2679)	grad_norm 2.6180 (nan)	loss_scale 2048.0000 (5290.2016)	mem 20188MB
[2024-07-29 19:46:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:13:49 lr 0.000002	 wd 0.0500	time 0.5963 (0.6370)	loss 1.0349 (1.2649)	grad_norm 4.0896 (nan)	loss_scale 2048.0000 (5020.2431)	mem 20188MB
[2024-07-29 19:47:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:12:44 lr 0.000002	 wd 0.0500	time 0.5883 (0.6361)	loss 1.2482 (1.2638)	grad_norm 6.1150 (nan)	loss_scale 2048.0000 (4791.7848)	mem 20188MB
[2024-07-29 19:48:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:11:40 lr 0.000002	 wd 0.0500	time 0.5893 (0.6353)	loss 1.5372 (1.2628)	grad_norm 2.4769 (nan)	loss_scale 2048.0000 (4595.9400)	mem 20188MB
[2024-07-29 19:49:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:10:35 lr 0.000002	 wd 0.0500	time 0.5911 (0.6347)	loss 1.1191 (1.2636)	grad_norm 2.2796 (nan)	loss_scale 2048.0000 (4426.1905)	mem 20188MB
[2024-07-29 19:50:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:09:31 lr 0.000003	 wd 0.0500	time 0.6202 (0.6341)	loss 1.2482 (1.2588)	grad_norm 2.2462 (nan)	loss_scale 2048.0000 (4277.6465)	mem 20188MB
[2024-07-29 19:51:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:08:28 lr 0.000003	 wd 0.0500	time 0.5995 (0.6336)	loss 1.0390 (1.2561)	grad_norm 1.9294 (nan)	loss_scale 2048.0000 (4146.5679)	mem 20188MB
[2024-07-29 19:52:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:07:24 lr 0.000003	 wd 0.0500	time 0.6004 (0.6330)	loss 1.1245 (1.2538)	grad_norm 1.7070 (nan)	loss_scale 1024.0000 (4001.6169)	mem 20188MB
[2024-07-29 19:53:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:06:20 lr 0.000003	 wd 0.0500	time 0.6085 (0.6326)	loss 1.5728 (1.2513)	grad_norm 2.0887 (nan)	loss_scale 1024.0000 (3844.9826)	mem 20188MB
[2024-07-29 19:54:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:05:17 lr 0.000003	 wd 0.0500	time 0.5969 (0.6322)	loss 0.8595 (1.2484)	grad_norm 1.8404 (nan)	loss_scale 1024.0000 (3704.0040)	mem 20188MB
[2024-07-29 19:55:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:04:14 lr 0.000003	 wd 0.0500	time 0.5780 (0.6319)	loss 1.0661 (1.2455)	grad_norm 1.9626 (nan)	loss_scale 1024.0000 (3576.4455)	mem 20188MB
[2024-07-29 19:56:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:03:10 lr 0.000004	 wd 0.0500	time 0.5788 (0.6315)	loss 1.5761 (1.2429)	grad_norm 2.2548 (nan)	loss_scale 1024.0000 (3460.4780)	mem 20188MB
[2024-07-29 19:57:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:02:07 lr 0.000004	 wd 0.0500	time 0.6042 (0.6312)	loss 1.4611 (1.2415)	grad_norm 3.7284 (nan)	loss_scale 1024.0000 (3354.5902)	mem 20188MB
[2024-07-29 19:58:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:04 lr 0.000004	 wd 0.0500	time 0.5905 (0.6310)	loss 1.3942 (1.2397)	grad_norm 1.7349 (nan)	loss_scale 1024.0000 (3257.5227)	mem 20188MB
[2024-07-29 19:59:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.5898 (0.6308)	loss 1.0369 (1.2366)	grad_norm 2.2705 (nan)	loss_scale 512.0000 (3161.6665)	mem 20188MB
[2024-07-29 19:59:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 0 training takes 0:26:20
[2024-07-29 19:59:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_0.pth saving......
[2024-07-29 19:59:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_0.pth saved !!!
[2024-07-29 19:59:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 9.724 (9.724)	Loss 0.5449 (0.5449)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 20188MB
[2024-07-29 20:00:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 85.242 Acc@5 97.384
[2024-07-29 20:00:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-29 20:00:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 85.24%
[2024-07-29 20:00:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 20:00:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:00:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:08:06 lr 0.000004	 wd 0.0500	time 10.2664 (10.2664)	loss 1.3708 (1.3708)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:01:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:28:45 lr 0.000004	 wd 0.0500	time 0.5966 (0.7184)	loss 1.2705 (1.1871)	grad_norm 1.6920 (2.5122)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:02:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:25:45 lr 0.000004	 wd 0.0500	time 0.5770 (0.6712)	loss 0.8215 (1.1877)	grad_norm 2.0711 (2.7801)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:03:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:24:03 lr 0.000004	 wd 0.0500	time 0.5988 (0.6557)	loss 0.9598 (1.1930)	grad_norm 2.2719 (2.7477)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:04:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:22:41 lr 0.000005	 wd 0.0500	time 0.5787 (0.6478)	loss 0.9657 (1.2088)	grad_norm 2.3246 (2.7493)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:05:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:21:27 lr 0.000005	 wd 0.0500	time 0.5877 (0.6432)	loss 1.5689 (1.2115)	grad_norm 7.2517 (2.7069)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:06:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:20:17 lr 0.000005	 wd 0.0500	time 0.5915 (0.6402)	loss 1.4951 (1.2106)	grad_norm 2.3387 (2.6399)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:07:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:19:09 lr 0.000005	 wd 0.0500	time 0.5950 (0.6379)	loss 0.8874 (1.2064)	grad_norm 3.1204 (2.6273)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:08:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:18:02 lr 0.000005	 wd 0.0500	time 0.5927 (0.6362)	loss 1.6262 (1.2058)	grad_norm 1.9901 (2.6173)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:09:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:16:57 lr 0.000005	 wd 0.0500	time 0.5935 (0.6352)	loss 1.1995 (1.2036)	grad_norm 2.2237 (2.5813)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:10:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:15:52 lr 0.000006	 wd 0.0500	time 0.5972 (0.6342)	loss 1.4785 (1.2000)	grad_norm 2.0928 (2.5563)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:11:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:14:48 lr 0.000006	 wd 0.0500	time 0.6020 (0.6335)	loss 1.5524 (1.1982)	grad_norm 1.8179 (2.5373)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:13:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:13:43 lr 0.000006	 wd 0.0500	time 0.6001 (0.6328)	loss 0.9807 (1.1961)	grad_norm 1.8975 (2.5268)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:14:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:12:39 lr 0.000006	 wd 0.0500	time 0.5922 (0.6321)	loss 1.2477 (1.1970)	grad_norm 3.4117 (2.5386)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:15:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:11:36 lr 0.000006	 wd 0.0500	time 0.5967 (0.6316)	loss 0.8292 (1.1966)	grad_norm 2.3355 (2.5459)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:16:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:10:32 lr 0.000006	 wd 0.0500	time 0.5985 (0.6311)	loss 1.5158 (1.1963)	grad_norm 1.8561 (2.5284)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:17:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:09:28 lr 0.000007	 wd 0.0500	time 0.5999 (0.6306)	loss 1.3861 (1.1968)	grad_norm 1.8458 (2.5393)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:18:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:08:25 lr 0.000007	 wd 0.0500	time 0.5957 (0.6303)	loss 0.8538 (1.1972)	grad_norm 2.9748 (2.5358)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:19:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:07:22 lr 0.000007	 wd 0.0500	time 0.5913 (0.6300)	loss 1.6648 (1.1962)	grad_norm 2.4416 (2.5289)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:20:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:06:19 lr 0.000007	 wd 0.0500	time 0.5892 (0.6297)	loss 1.5041 (1.1976)	grad_norm 4.4694 (2.5522)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:21:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:05:16 lr 0.000007	 wd 0.0500	time 0.6111 (0.6295)	loss 1.3139 (1.1979)	grad_norm 2.1454 (2.5511)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:22:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:04:12 lr 0.000007	 wd 0.0500	time 0.6001 (0.6293)	loss 1.4427 (1.1999)	grad_norm 3.0441 (2.5445)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:23:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:03:09 lr 0.000008	 wd 0.0500	time 0.5954 (0.6291)	loss 1.1511 (1.2001)	grad_norm 1.7909 (2.5982)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:24:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:02:07 lr 0.000008	 wd 0.0500	time 0.5791 (0.6290)	loss 0.8463 (1.2014)	grad_norm 2.3024 (2.6306)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:25:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:04 lr 0.000008	 wd 0.0500	time 0.5785 (0.6289)	loss 1.2552 (1.2010)	grad_norm 2.1694 (2.6149)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:26:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.6213 (0.6287)	loss 0.9429 (1.2007)	grad_norm 2.6033 (2.6032)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:26:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 1 training takes 0:26:15
[2024-07-29 20:26:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.526 (10.526)	Loss 0.5303 (0.5303)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-29 20:27:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 85.662 Acc@5 97.610
[2024-07-29 20:27:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-29 20:27:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 85.66%
[2024-07-29 20:27:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 20:27:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:27:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][0/2502]	eta 6:51:46 lr 0.000008	 wd 0.0500	time 9.8749 (9.8749)	loss 1.1797 (1.1797)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:28:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:28:54 lr 0.000008	 wd 0.0500	time 0.6042 (0.7220)	loss 1.5325 (1.1961)	grad_norm 2.5567 (2.6947)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:29:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:25:49 lr 0.000008	 wd 0.0500	time 0.6023 (0.6731)	loss 1.3073 (1.1871)	grad_norm 1.7057 (2.6998)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:30:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:24:06 lr 0.000008	 wd 0.0500	time 0.5928 (0.6570)	loss 1.2798 (1.1752)	grad_norm 2.3376 (2.7690)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:31:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:22:43 lr 0.000009	 wd 0.0500	time 0.5974 (0.6487)	loss 1.3990 (1.1787)	grad_norm 2.5763 (2.6421)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:32:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:21:29 lr 0.000009	 wd 0.0500	time 0.5888 (0.6441)	loss 1.3773 (1.1765)	grad_norm 1.9620 (2.6276)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:33:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:20:18 lr 0.000009	 wd 0.0500	time 0.5988 (0.6409)	loss 1.2767 (1.1758)	grad_norm 2.0976 (2.7040)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:34:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:19:10 lr 0.000009	 wd 0.0500	time 0.5821 (0.6385)	loss 1.3581 (1.1799)	grad_norm 2.9853 (2.6780)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:35:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:18:03 lr 0.000009	 wd 0.0500	time 0.5955 (0.6368)	loss 1.2518 (1.1827)	grad_norm 2.1596 (2.6341)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:36:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:16:58 lr 0.000009	 wd 0.0500	time 0.5798 (0.6356)	loss 0.8261 (1.1863)	grad_norm 3.3990 (2.6069)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:37:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:15:53 lr 0.000010	 wd 0.0500	time 0.5911 (0.6346)	loss 0.8954 (1.1837)	grad_norm 2.4560 (2.5940)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:38:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:14:48 lr 0.000010	 wd 0.0500	time 0.5939 (0.6337)	loss 1.4310 (1.1825)	grad_norm 2.0882 (2.5993)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:39:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:13:44 lr 0.000010	 wd 0.0500	time 0.6012 (0.6331)	loss 1.0303 (1.1830)	grad_norm 4.0588 (2.5799)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:40:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:12:40 lr 0.000010	 wd 0.0500	time 0.5892 (0.6324)	loss 1.4460 (1.1834)	grad_norm 2.2522 (2.6312)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:41:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:11:36 lr 0.000010	 wd 0.0500	time 0.6103 (0.6319)	loss 1.2457 (1.1846)	grad_norm 2.2739 (2.6143)	loss_scale 512.0000 (512.0000)	mem 20188MB
[2024-07-29 20:42:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:10:32 lr 0.000010	 wd 0.0500	time 0.5785 (0.6314)	loss 1.4358 (1.1852)	grad_norm 1.8983 (2.6066)	loss_scale 1024.0000 (524.2798)	mem 20188MB
[2024-07-29 20:44:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:09:29 lr 0.000011	 wd 0.0500	time 0.5939 (0.6310)	loss 1.0014 (1.1853)	grad_norm 1.7016 (2.6026)	loss_scale 1024.0000 (555.4928)	mem 20188MB
[2024-07-29 20:45:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:08:25 lr 0.000011	 wd 0.0500	time 0.5917 (0.6308)	loss 1.2521 (1.1852)	grad_norm 1.9863 (2.5913)	loss_scale 1024.0000 (583.0359)	mem 20188MB
[2024-07-29 20:46:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:07:22 lr 0.000011	 wd 0.0500	time 0.5942 (0.6304)	loss 1.1866 (1.1861)	grad_norm 3.4791 (2.6171)	loss_scale 1024.0000 (607.5203)	mem 20188MB
[2024-07-29 20:47:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:06:19 lr 0.000011	 wd 0.0500	time 0.5989 (0.6301)	loss 1.3726 (1.1857)	grad_norm 2.6961 (2.6144)	loss_scale 1024.0000 (629.4287)	mem 20188MB
[2024-07-29 20:48:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:05:16 lr 0.000011	 wd 0.0500	time 0.6120 (0.6299)	loss 1.0059 (1.1858)	grad_norm 1.5670 (2.6072)	loss_scale 1024.0000 (649.1474)	mem 20188MB
[2024-07-29 20:49:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:04:13 lr 0.000011	 wd 0.0500	time 0.5954 (0.6296)	loss 1.0163 (1.1849)	grad_norm 2.6701 (2.6007)	loss_scale 1024.0000 (666.9891)	mem 20188MB
[2024-07-29 20:50:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:03:10 lr 0.000012	 wd 0.0500	time 0.5936 (0.6294)	loss 1.3036 (1.1853)	grad_norm 2.7357 (2.6270)	loss_scale 1024.0000 (683.2095)	mem 20188MB
[2024-07-29 20:51:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:02:07 lr 0.000012	 wd 0.0500	time 0.5993 (0.6292)	loss 1.1412 (1.1849)	grad_norm 1.6052 (2.6138)	loss_scale 1024.0000 (698.0200)	mem 20188MB
[2024-07-29 20:52:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:04 lr 0.000012	 wd 0.0500	time 0.5828 (0.6291)	loss 0.8086 (1.1854)	grad_norm 1.8061 (2.6164)	loss_scale 1024.0000 (711.5968)	mem 20188MB
[2024-07-29 20:53:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.5921 (0.6289)	loss 1.2278 (1.1851)	grad_norm 1.8555 (2.6100)	loss_scale 1024.0000 (724.0880)	mem 20188MB
[2024-07-29 20:53:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 2 training takes 0:26:15
[2024-07-29 20:53:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.689 (10.689)	Loss 0.5103 (0.5103)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-29 20:53:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 85.848 Acc@5 97.722
[2024-07-29 20:53:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-29 20:53:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 85.85%
[2024-07-29 20:53:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 20:53:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 20:54:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][0/2502]	eta 6:38:34 lr 0.000012	 wd 0.0500	time 9.5580 (9.5580)	loss 0.6864 (0.6864)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 20:55:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:28:31 lr 0.000012	 wd 0.0500	time 0.5929 (0.7126)	loss 1.2871 (1.2187)	grad_norm 2.0543 (2.6484)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 20:56:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:25:36 lr 0.000012	 wd 0.0500	time 0.6071 (0.6677)	loss 1.5118 (1.1939)	grad_norm 4.3660 (2.6662)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 20:57:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:23:58 lr 0.000012	 wd 0.0500	time 0.5956 (0.6534)	loss 1.4765 (1.1933)	grad_norm 1.7498 (2.6652)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 20:58:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:22:37 lr 0.000013	 wd 0.0500	time 0.5889 (0.6460)	loss 1.4393 (1.1842)	grad_norm 2.7840 (2.6276)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 20:59:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:21:24 lr 0.000013	 wd 0.0500	time 0.5969 (0.6418)	loss 0.8290 (1.1840)	grad_norm 2.1220 (2.5900)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:00:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:20:15 lr 0.000013	 wd 0.0500	time 0.5985 (0.6390)	loss 1.0826 (1.1764)	grad_norm 2.1136 (2.5744)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:01:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:19:08 lr 0.000013	 wd 0.0500	time 0.5837 (0.6371)	loss 1.4327 (1.1743)	grad_norm 3.3491 (2.5628)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:02:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:18:01 lr 0.000013	 wd 0.0500	time 0.5964 (0.6356)	loss 0.9806 (1.1691)	grad_norm 5.3373 (2.5469)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:03:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:16:56 lr 0.000013	 wd 0.0500	time 0.5975 (0.6345)	loss 1.5904 (1.1719)	grad_norm 2.7661 (2.5285)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:04:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:15:51 lr 0.000014	 wd 0.0500	time 0.5958 (0.6335)	loss 1.2886 (1.1717)	grad_norm 1.7081 (2.5641)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:05:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:14:47 lr 0.000014	 wd 0.0500	time 0.6048 (0.6328)	loss 0.8266 (1.1726)	grad_norm 2.4695 (2.6017)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:06:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:13:43 lr 0.000014	 wd 0.0500	time 0.5906 (0.6322)	loss 1.2058 (1.1706)	grad_norm 3.9306 (2.6038)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:07:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:12:39 lr 0.000014	 wd 0.0500	time 0.5961 (0.6316)	loss 1.2822 (1.1706)	grad_norm 1.3207 (2.6441)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:08:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:11:35 lr 0.000014	 wd 0.0500	time 0.5927 (0.6312)	loss 1.2180 (1.1712)	grad_norm 1.4936 (2.6058)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:09:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:10:32 lr 0.000014	 wd 0.0500	time 0.5966 (0.6308)	loss 1.4421 (1.1695)	grad_norm 1.8829 (2.5862)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:10:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:09:28 lr 0.000015	 wd 0.0500	time 0.6004 (0.6305)	loss 0.7327 (1.1670)	grad_norm 3.2462 (2.5851)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:11:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:08:25 lr 0.000015	 wd 0.0500	time 0.5964 (0.6302)	loss 1.0145 (1.1696)	grad_norm 1.9263 (2.5889)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:12:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:07:22 lr 0.000015	 wd 0.0500	time 0.5882 (0.6300)	loss 1.2786 (1.1709)	grad_norm 1.8077 (2.6122)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:13:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:06:19 lr 0.000015	 wd 0.0500	time 0.5976 (0.6298)	loss 1.3834 (1.1693)	grad_norm 2.5834 (2.6124)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:14:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:05:16 lr 0.000015	 wd 0.0500	time 0.5955 (0.6296)	loss 1.2566 (1.1687)	grad_norm 1.7393 (2.5994)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:16:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:04:13 lr 0.000015	 wd 0.0500	time 0.6244 (0.6294)	loss 0.8225 (1.1687)	grad_norm 1.6777 (2.5833)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:17:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:03:10 lr 0.000016	 wd 0.0500	time 0.5782 (0.6292)	loss 0.8049 (1.1678)	grad_norm 2.8250 (2.5735)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:18:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:02:07 lr 0.000016	 wd 0.0500	time 0.5975 (0.6290)	loss 0.9263 (1.1682)	grad_norm 2.4423 (2.5745)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:19:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:04 lr 0.000016	 wd 0.0500	time 0.6038 (0.6288)	loss 1.3125 (1.1695)	grad_norm 2.4697 (2.5656)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:20:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5991 (0.6287)	loss 1.2490 (1.1695)	grad_norm 2.1214 (2.5628)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:20:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 3 training takes 0:26:15
[2024-07-29 21:20:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.518 (10.518)	Loss 0.5146 (0.5146)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-29 21:20:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 85.950 Acc@5 97.790
[2024-07-29 21:20:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-29 21:20:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 85.95%
[2024-07-29 21:20:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 21:20:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 21:20:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][0/2502]	eta 6:54:45 lr 0.000016	 wd 0.0500	time 9.9461 (9.9461)	loss 1.2623 (1.2623)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:21:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:28:36 lr 0.000016	 wd 0.0500	time 0.5957 (0.7144)	loss 0.9531 (1.1810)	grad_norm 2.7599 (2.5291)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:23:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:25:41 lr 0.000016	 wd 0.0500	time 0.5888 (0.6697)	loss 1.1105 (1.1714)	grad_norm 1.8235 (2.5744)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:24:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:24:01 lr 0.000016	 wd 0.0500	time 0.5877 (0.6548)	loss 0.7686 (1.1672)	grad_norm 2.7274 (2.5649)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:25:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:22:40 lr 0.000017	 wd 0.0500	time 0.5946 (0.6473)	loss 1.3783 (1.1648)	grad_norm 1.7818 (2.5441)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 21:26:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:21:27 lr 0.000017	 wd 0.0500	time 0.5976 (0.6430)	loss 1.2155 (1.1647)	grad_norm 2.0256 (2.5008)	loss_scale 2048.0000 (1105.7565)	mem 20188MB
[2024-07-29 21:27:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:20:17 lr 0.000017	 wd 0.0500	time 0.5881 (0.6400)	loss 1.0089 (1.1655)	grad_norm 2.5394 (2.4948)	loss_scale 2048.0000 (1262.5358)	mem 20188MB
[2024-07-29 21:28:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:19:09 lr 0.000017	 wd 0.0500	time 0.5940 (0.6379)	loss 1.0215 (1.1666)	grad_norm 4.3494 (2.5497)	loss_scale 2048.0000 (1374.5849)	mem 20188MB
[2024-07-29 21:29:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:18:03 lr 0.000017	 wd 0.0500	time 0.5774 (0.6363)	loss 0.7651 (1.1684)	grad_norm 1.8166 (2.6444)	loss_scale 2048.0000 (1458.6567)	mem 20188MB
[2024-07-29 21:30:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:16:57 lr 0.000017	 wd 0.0500	time 0.5974 (0.6352)	loss 0.8660 (1.1692)	grad_norm 2.1225 (2.6272)	loss_scale 2048.0000 (1524.0666)	mem 20188MB
[2024-07-29 21:31:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:15:52 lr 0.000018	 wd 0.0500	time 0.5998 (0.6342)	loss 1.4581 (1.1701)	grad_norm 2.1118 (2.6813)	loss_scale 2048.0000 (1576.4076)	mem 20188MB
[2024-07-29 21:32:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:14:48 lr 0.000018	 wd 0.0500	time 0.5828 (0.6335)	loss 1.4633 (1.1714)	grad_norm 1.7667 (2.6637)	loss_scale 2048.0000 (1619.2407)	mem 20188MB
[2024-07-29 21:33:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:13:43 lr 0.000018	 wd 0.0500	time 0.5899 (0.6327)	loss 1.2751 (1.1695)	grad_norm 1.8291 (2.6552)	loss_scale 2048.0000 (1654.9409)	mem 20188MB
[2024-07-29 21:34:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:12:39 lr 0.000018	 wd 0.0500	time 0.5953 (0.6321)	loss 0.9496 (1.1687)	grad_norm 2.0194 (2.6418)	loss_scale 2048.0000 (1685.1530)	mem 20188MB
[2024-07-29 21:35:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:11:36 lr 0.000018	 wd 0.0500	time 0.5943 (0.6316)	loss 1.4010 (1.1689)	grad_norm 2.0105 (2.6323)	loss_scale 2048.0000 (1711.0521)	mem 20188MB
[2024-07-29 21:36:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:10:32 lr 0.000018	 wd 0.0500	time 0.5981 (0.6312)	loss 1.1912 (1.1690)	grad_norm 2.3207 (2.6418)	loss_scale 2048.0000 (1733.5003)	mem 20188MB
[2024-07-29 21:37:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:09:29 lr 0.000019	 wd 0.0500	time 0.6001 (0.6309)	loss 1.3477 (1.1672)	grad_norm 1.9299 (2.6370)	loss_scale 2048.0000 (1753.1443)	mem 20188MB
[2024-07-29 21:38:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:08:25 lr 0.000019	 wd 0.0500	time 0.5979 (0.6307)	loss 1.1741 (1.1671)	grad_norm 1.8790 (2.6377)	loss_scale 2048.0000 (1770.4785)	mem 20188MB
[2024-07-29 21:39:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:07:22 lr 0.000019	 wd 0.0500	time 0.6023 (0.6304)	loss 1.5079 (1.1687)	grad_norm 2.1846 (2.6250)	loss_scale 2048.0000 (1785.8878)	mem 20188MB
[2024-07-29 21:40:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:06:19 lr 0.000019	 wd 0.0500	time 0.5978 (0.6301)	loss 1.3418 (1.1689)	grad_norm 2.1629 (2.6313)	loss_scale 2048.0000 (1799.6760)	mem 20188MB
[2024-07-29 21:41:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:05:16 lr 0.000019	 wd 0.0500	time 0.5949 (0.6299)	loss 0.8058 (1.1679)	grad_norm 3.6282 (2.6407)	loss_scale 2048.0000 (1812.0860)	mem 20188MB
[2024-07-29 21:42:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:04:13 lr 0.000019	 wd 0.0500	time 0.5917 (0.6296)	loss 0.9194 (1.1666)	grad_norm 2.1833 (2.6789)	loss_scale 2048.0000 (1823.3146)	mem 20188MB
[2024-07-29 21:43:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:03:10 lr 0.000020	 wd 0.0500	time 0.5907 (0.6295)	loss 0.9267 (1.1666)	grad_norm 2.0308 (2.6752)	loss_scale 2048.0000 (1833.5229)	mem 20188MB
[2024-07-29 21:44:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:02:07 lr 0.000020	 wd 0.0500	time 0.5993 (0.6293)	loss 0.8125 (1.1668)	grad_norm 3.6766 (2.6885)	loss_scale 2048.0000 (1842.8440)	mem 20188MB
[2024-07-29 21:45:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:04 lr 0.000020	 wd 0.0500	time 0.5973 (0.6292)	loss 0.7644 (1.1660)	grad_norm 2.0877 (2.6745)	loss_scale 2048.0000 (1851.3886)	mem 20188MB
[2024-07-29 21:47:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.6144 (0.6290)	loss 1.0907 (1.1663)	grad_norm 3.1412 (2.7162)	loss_scale 2048.0000 (1859.2499)	mem 20188MB
[2024-07-29 21:47:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 4 training takes 0:26:16
[2024-07-29 21:47:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.544 (10.544)	Loss 0.5005 (0.5005)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-29 21:47:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.082 Acc@5 97.838
[2024-07-29 21:47:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-29 21:47:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-29 21:47:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 21:47:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 21:47:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][0/2502]	eta 6:53:42 lr 0.000020	 wd 0.0500	time 9.9212 (9.9212)	loss 1.3828 (1.3828)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:48:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:28:49 lr 0.000020	 wd 0.0500	time 0.5870 (0.7200)	loss 0.9831 (1.1917)	grad_norm 2.1049 (2.2151)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:49:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:25:48 lr 0.000020	 wd 0.0500	time 0.5900 (0.6725)	loss 1.2150 (1.1576)	grad_norm 2.1592 (2.3722)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:50:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:24:05 lr 0.000020	 wd 0.0500	time 0.5956 (0.6566)	loss 0.8294 (1.1516)	grad_norm 2.2158 (2.4470)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:51:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:22:43 lr 0.000020	 wd 0.0500	time 0.5920 (0.6488)	loss 1.4798 (1.1593)	grad_norm 1.9684 (2.5125)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:52:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:21:30 lr 0.000020	 wd 0.0500	time 0.5935 (0.6445)	loss 1.0073 (1.1570)	grad_norm 2.6126 (2.5322)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:54:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:20:19 lr 0.000020	 wd 0.0500	time 0.5939 (0.6412)	loss 0.9272 (1.1579)	grad_norm 1.9605 (2.4825)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:55:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:19:11 lr 0.000020	 wd 0.0500	time 0.5981 (0.6389)	loss 0.8945 (1.1580)	grad_norm 1.7996 (2.5084)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 21:56:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:18:04 lr 0.000020	 wd 0.0500	time 0.5961 (0.6370)	loss 1.4587 (1.1557)	grad_norm 2.2093 (nan)	loss_scale 1024.0000 (1994.3071)	mem 20188MB
[2024-07-29 21:57:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:16:58 lr 0.000020	 wd 0.0500	time 0.5947 (0.6357)	loss 1.0368 (1.1549)	grad_norm 1.8682 (nan)	loss_scale 1024.0000 (1886.6149)	mem 20188MB
[2024-07-29 21:58:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:15:53 lr 0.000020	 wd 0.0500	time 0.5969 (0.6346)	loss 1.1317 (1.1538)	grad_norm 2.0321 (nan)	loss_scale 1024.0000 (1800.4396)	mem 20188MB
[2024-07-29 21:59:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:14:48 lr 0.000020	 wd 0.0500	time 0.6188 (0.6338)	loss 1.1776 (1.1555)	grad_norm 2.4613 (nan)	loss_scale 1024.0000 (1729.9183)	mem 20188MB
[2024-07-29 22:00:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:13:44 lr 0.000020	 wd 0.0500	time 0.5944 (0.6331)	loss 1.4586 (1.1545)	grad_norm 2.6062 (nan)	loss_scale 1024.0000 (1671.1407)	mem 20188MB
[2024-07-29 22:01:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:12:40 lr 0.000020	 wd 0.0500	time 0.6029 (0.6326)	loss 0.8264 (1.1555)	grad_norm 2.4213 (nan)	loss_scale 1024.0000 (1621.3989)	mem 20188MB
[2024-07-29 22:02:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:11:36 lr 0.000020	 wd 0.0500	time 0.6115 (0.6321)	loss 1.2747 (1.1590)	grad_norm 2.1374 (nan)	loss_scale 1024.0000 (1578.7580)	mem 20188MB
[2024-07-29 22:03:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:10:32 lr 0.000020	 wd 0.0500	time 0.5925 (0.6316)	loss 0.7759 (1.1607)	grad_norm 2.8430 (nan)	loss_scale 1024.0000 (1541.7988)	mem 20188MB
[2024-07-29 22:04:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:09:29 lr 0.000020	 wd 0.0500	time 0.6209 (0.6312)	loss 1.3448 (1.1636)	grad_norm 1.7817 (nan)	loss_scale 1024.0000 (1509.4566)	mem 20188MB
[2024-07-29 22:05:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:08:25 lr 0.000020	 wd 0.0500	time 0.5914 (0.6309)	loss 0.8391 (1.1639)	grad_norm 1.7734 (nan)	loss_scale 1024.0000 (1480.9171)	mem 20188MB
[2024-07-29 22:06:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:07:22 lr 0.000020	 wd 0.0500	time 0.6181 (0.6306)	loss 0.8311 (1.1614)	grad_norm 2.5016 (nan)	loss_scale 1024.0000 (1455.5469)	mem 20188MB
[2024-07-29 22:07:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:06:19 lr 0.000020	 wd 0.0500	time 0.6000 (0.6303)	loss 1.1429 (1.1608)	grad_norm 2.0377 (nan)	loss_scale 1024.0000 (1432.8459)	mem 20188MB
[2024-07-29 22:08:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:05:16 lr 0.000020	 wd 0.0500	time 0.6000 (0.6301)	loss 1.5449 (1.1618)	grad_norm 2.4129 (nan)	loss_scale 1024.0000 (1412.4138)	mem 20188MB
[2024-07-29 22:09:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:04:13 lr 0.000020	 wd 0.0500	time 0.6280 (0.6299)	loss 1.5350 (1.1617)	grad_norm 1.7410 (nan)	loss_scale 1024.0000 (1393.9267)	mem 20188MB
[2024-07-29 22:10:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:03:10 lr 0.000020	 wd 0.0500	time 0.5977 (0.6297)	loss 1.2732 (1.1621)	grad_norm 1.5714 (nan)	loss_scale 1024.0000 (1377.1195)	mem 20188MB
[2024-07-29 22:11:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:02:07 lr 0.000020	 wd 0.0500	time 0.6005 (0.6295)	loss 1.1477 (1.1608)	grad_norm 2.1480 (nan)	loss_scale 1024.0000 (1361.7731)	mem 20188MB
[2024-07-29 22:12:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:04 lr 0.000020	 wd 0.0500	time 0.5950 (0.6293)	loss 0.9147 (1.1602)	grad_norm 2.8553 (nan)	loss_scale 1024.0000 (1347.7051)	mem 20188MB
[2024-07-29 22:13:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5936 (0.6291)	loss 1.3776 (1.1611)	grad_norm 2.0078 (nan)	loss_scale 1024.0000 (1334.7621)	mem 20188MB
[2024-07-29 22:13:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 5 training takes 0:26:16
[2024-07-29 22:14:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.712 (10.712)	Loss 0.5029 (0.5029)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-29 22:14:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.204 Acc@5 97.868
[2024-07-29 22:14:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-29 22:14:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.20%
[2024-07-29 22:14:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 22:14:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 22:14:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][0/2502]	eta 6:59:30 lr 0.000020	 wd 0.0500	time 10.0603 (10.0603)	loss 1.2196 (1.2196)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:15:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:28:44 lr 0.000020	 wd 0.0500	time 0.6070 (0.7179)	loss 0.9686 (1.1868)	grad_norm 1.5481 (2.3648)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:16:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:25:45 lr 0.000020	 wd 0.0500	time 0.6280 (0.6714)	loss 0.8968 (1.1687)	grad_norm 2.4235 (2.6710)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:17:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:24:04 lr 0.000020	 wd 0.0500	time 0.6157 (0.6558)	loss 0.9633 (1.1648)	grad_norm 2.1433 (2.6147)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:18:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:22:42 lr 0.000020	 wd 0.0500	time 0.5950 (0.6483)	loss 0.7031 (1.1631)	grad_norm 2.4712 (2.7144)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:19:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:21:27 lr 0.000020	 wd 0.0500	time 0.5984 (0.6433)	loss 1.2753 (1.1643)	grad_norm 1.6341 (3.0479)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:20:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:20:17 lr 0.000020	 wd 0.0500	time 0.6061 (0.6403)	loss 0.8836 (1.1563)	grad_norm 2.2229 (3.1594)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:21:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:19:09 lr 0.000020	 wd 0.0500	time 0.5861 (0.6381)	loss 1.3841 (1.1588)	grad_norm 1.5645 (3.0255)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:22:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:18:03 lr 0.000020	 wd 0.0500	time 0.5917 (0.6366)	loss 0.7703 (1.1638)	grad_norm 5.4673 (2.9664)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:23:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:16:57 lr 0.000020	 wd 0.0500	time 0.5990 (0.6354)	loss 1.6258 (1.1596)	grad_norm 1.9358 (2.9896)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:25:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:15:52 lr 0.000020	 wd 0.0500	time 0.5896 (0.6344)	loss 1.0145 (1.1595)	grad_norm 2.2202 (3.0024)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:26:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:14:48 lr 0.000020	 wd 0.0500	time 0.5964 (0.6336)	loss 0.7814 (1.1548)	grad_norm 1.6882 (2.9864)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:27:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:13:44 lr 0.000020	 wd 0.0500	time 0.5769 (0.6330)	loss 1.2790 (1.1515)	grad_norm 1.8272 (2.9995)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:28:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:12:40 lr 0.000020	 wd 0.0500	time 0.5984 (0.6324)	loss 1.1251 (1.1505)	grad_norm 1.9457 (3.0082)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:29:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:11:36 lr 0.000020	 wd 0.0500	time 0.5900 (0.6320)	loss 1.0108 (1.1498)	grad_norm 2.6505 (2.9800)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:30:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:10:32 lr 0.000020	 wd 0.0500	time 0.6060 (0.6315)	loss 0.6885 (1.1497)	grad_norm 2.1859 (2.9432)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:31:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:09:29 lr 0.000020	 wd 0.0500	time 0.6228 (0.6312)	loss 1.3508 (1.1477)	grad_norm 3.5114 (2.9537)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:32:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:08:25 lr 0.000020	 wd 0.0500	time 0.6060 (0.6309)	loss 1.3617 (1.1458)	grad_norm 2.0953 (2.9382)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:33:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:07:22 lr 0.000020	 wd 0.0500	time 0.5911 (0.6305)	loss 1.2016 (1.1459)	grad_norm 1.5466 (2.9284)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:34:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:06:19 lr 0.000020	 wd 0.0500	time 0.5965 (0.6303)	loss 1.4046 (1.1480)	grad_norm 2.2996 (2.9685)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:35:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:05:16 lr 0.000020	 wd 0.0500	time 0.6280 (0.6300)	loss 0.9084 (1.1486)	grad_norm 2.0065 (3.0460)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:36:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:04:13 lr 0.000020	 wd 0.0500	time 0.5869 (0.6298)	loss 1.0935 (1.1489)	grad_norm 2.0845 (3.0333)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:37:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:03:10 lr 0.000020	 wd 0.0500	time 0.6101 (0.6296)	loss 0.8061 (1.1484)	grad_norm 2.2696 (3.0206)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 22:38:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:02:07 lr 0.000020	 wd 0.0500	time 0.5783 (0.6293)	loss 0.9413 (1.1504)	grad_norm 1.9378 (3.0091)	loss_scale 2048.0000 (1043.5811)	mem 20188MB
[2024-07-29 22:39:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:04 lr 0.000020	 wd 0.0500	time 0.5847 (0.6291)	loss 1.4048 (1.1493)	grad_norm 2.3722 (2.9975)	loss_scale 2048.0000 (1085.4144)	mem 20188MB
[2024-07-29 22:40:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5980 (0.6290)	loss 1.4141 (1.1487)	grad_norm 1.8219 (2.9736)	loss_scale 2048.0000 (1123.9024)	mem 20188MB
[2024-07-29 22:40:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 6 training takes 0:26:15
[2024-07-29 22:40:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 11.165 (11.165)	Loss 0.5044 (0.5044)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 20188MB
[2024-07-29 22:41:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.218 Acc@5 97.904
[2024-07-29 22:41:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-29 22:41:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.22%
[2024-07-29 22:41:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 22:41:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 22:41:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:03:07 lr 0.000020	 wd 0.0500	time 10.1470 (10.1470)	loss 0.9305 (0.9305)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 22:42:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:28:44 lr 0.000020	 wd 0.0500	time 0.5896 (0.7178)	loss 1.2134 (1.1287)	grad_norm 1.7832 (3.1464)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 22:43:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:25:44 lr 0.000020	 wd 0.0500	time 0.5934 (0.6711)	loss 1.0982 (1.1487)	grad_norm 2.1192 (2.9955)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 22:44:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:24:03 lr 0.000020	 wd 0.0500	time 0.5772 (0.6557)	loss 1.4762 (1.1484)	grad_norm 2.4540 (nan)	loss_scale 1024.0000 (1762.2326)	mem 20188MB
[2024-07-29 22:45:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:22:42 lr 0.000020	 wd 0.0500	time 0.5985 (0.6483)	loss 0.8386 (1.1534)	grad_norm 2.1350 (nan)	loss_scale 1024.0000 (1578.1347)	mem 20188MB
[2024-07-29 22:46:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:21:28 lr 0.000020	 wd 0.0500	time 0.5905 (0.6438)	loss 1.3856 (1.1518)	grad_norm 1.5922 (nan)	loss_scale 1024.0000 (1467.5289)	mem 20188MB
[2024-07-29 22:47:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:20:18 lr 0.000020	 wd 0.0500	time 0.5967 (0.6407)	loss 1.2621 (1.1567)	grad_norm 2.6604 (nan)	loss_scale 1024.0000 (1393.7304)	mem 20188MB
[2024-07-29 22:48:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:19:10 lr 0.000020	 wd 0.0500	time 0.5989 (0.6385)	loss 1.0646 (1.1511)	grad_norm 2.0397 (nan)	loss_scale 1024.0000 (1340.9872)	mem 20188MB
[2024-07-29 22:49:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:18:04 lr 0.000020	 wd 0.0500	time 0.6117 (0.6369)	loss 0.9072 (1.1486)	grad_norm 1.7954 (nan)	loss_scale 1024.0000 (1301.4132)	mem 20188MB
[2024-07-29 22:50:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:16:58 lr 0.000020	 wd 0.0500	time 0.5922 (0.6358)	loss 1.4905 (1.1518)	grad_norm 1.6002 (nan)	loss_scale 1024.0000 (1270.6238)	mem 20188MB
[2024-07-29 22:51:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:15:53 lr 0.000020	 wd 0.0500	time 0.6114 (0.6347)	loss 0.8548 (1.1517)	grad_norm 2.3995 (nan)	loss_scale 1024.0000 (1245.9860)	mem 20188MB
[2024-07-29 22:52:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:14:48 lr 0.000020	 wd 0.0500	time 0.6002 (0.6338)	loss 1.1812 (1.1496)	grad_norm 2.6091 (nan)	loss_scale 1024.0000 (1225.8238)	mem 20188MB
[2024-07-29 22:53:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:13:44 lr 0.000020	 wd 0.0500	time 0.5962 (0.6331)	loss 1.2915 (1.1462)	grad_norm 7.5157 (nan)	loss_scale 1024.0000 (1209.0192)	mem 20188MB
[2024-07-29 22:54:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:12:40 lr 0.000020	 wd 0.0500	time 0.5893 (0.6326)	loss 1.2870 (1.1460)	grad_norm 2.2802 (nan)	loss_scale 1024.0000 (1194.7978)	mem 20188MB
[2024-07-29 22:55:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:11:36 lr 0.000019	 wd 0.0500	time 0.5955 (0.6321)	loss 1.2119 (1.1472)	grad_norm 2.2457 (nan)	loss_scale 1024.0000 (1182.6067)	mem 20188MB
[2024-07-29 22:57:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:10:32 lr 0.000019	 wd 0.0500	time 0.5933 (0.6317)	loss 0.8252 (1.1481)	grad_norm 2.0824 (nan)	loss_scale 1024.0000 (1172.0400)	mem 20188MB
[2024-07-29 22:58:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:09:29 lr 0.000019	 wd 0.0500	time 0.5789 (0.6313)	loss 1.2115 (1.1487)	grad_norm 2.0165 (nan)	loss_scale 1024.0000 (1162.7933)	mem 20188MB
[2024-07-29 22:59:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:08:26 lr 0.000019	 wd 0.0500	time 0.5973 (0.6311)	loss 1.2356 (1.1484)	grad_norm 1.6726 (nan)	loss_scale 1024.0000 (1154.6337)	mem 20188MB
[2024-07-29 23:00:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:07:22 lr 0.000019	 wd 0.0500	time 0.6090 (0.6308)	loss 0.7488 (1.1504)	grad_norm 1.8404 (nan)	loss_scale 1024.0000 (1147.3803)	mem 20188MB
[2024-07-29 23:01:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:06:19 lr 0.000019	 wd 0.0500	time 0.5991 (0.6305)	loss 1.1684 (1.1509)	grad_norm 2.3097 (nan)	loss_scale 1024.0000 (1140.8901)	mem 20188MB
[2024-07-29 23:02:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:05:16 lr 0.000019	 wd 0.0500	time 0.6007 (0.6303)	loss 1.2546 (1.1514)	grad_norm 2.0874 (nan)	loss_scale 1024.0000 (1135.0485)	mem 20188MB
[2024-07-29 23:03:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:04:13 lr 0.000019	 wd 0.0500	time 0.5953 (0.6301)	loss 1.5157 (1.1539)	grad_norm 2.3103 (nan)	loss_scale 1024.0000 (1129.7630)	mem 20188MB
[2024-07-29 23:04:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:03:10 lr 0.000019	 wd 0.0500	time 0.5995 (0.6299)	loss 1.2411 (1.1519)	grad_norm 2.9769 (nan)	loss_scale 1024.0000 (1124.9577)	mem 20188MB
[2024-07-29 23:05:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:02:07 lr 0.000019	 wd 0.0500	time 0.5913 (0.6297)	loss 1.4461 (1.1514)	grad_norm 1.8934 (nan)	loss_scale 1024.0000 (1120.5702)	mem 20188MB
[2024-07-29 23:06:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:04 lr 0.000019	 wd 0.0500	time 0.5967 (0.6296)	loss 1.3222 (1.1513)	grad_norm 1.5479 (nan)	loss_scale 1024.0000 (1116.5481)	mem 20188MB
[2024-07-29 23:07:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5944 (0.6295)	loss 1.4836 (1.1506)	grad_norm 1.8207 (nan)	loss_scale 1024.0000 (1112.8477)	mem 20188MB
[2024-07-29 23:07:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 7 training takes 0:26:17
[2024-07-29 23:07:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.088 (10.088)	Loss 0.4697 (0.4697)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-29 23:08:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.250 Acc@5 97.866
[2024-07-29 23:08:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-07-29 23:08:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.25%
[2024-07-29 23:08:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 23:08:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 23:08:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][0/2502]	eta 6:48:57 lr 0.000019	 wd 0.0500	time 9.8073 (9.8073)	loss 1.2500 (1.2500)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:09:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:28:38 lr 0.000019	 wd 0.0500	time 0.5913 (0.7156)	loss 0.8939 (1.2060)	grad_norm 2.1671 (2.6066)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:10:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:25:42 lr 0.000019	 wd 0.0500	time 0.5882 (0.6700)	loss 0.8327 (1.1797)	grad_norm 3.2879 (2.9166)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:11:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:24:02 lr 0.000019	 wd 0.0500	time 0.5911 (0.6552)	loss 1.4519 (1.1645)	grad_norm 1.9946 (2.8257)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:12:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:22:41 lr 0.000019	 wd 0.0500	time 0.5828 (0.6475)	loss 0.8691 (1.1620)	grad_norm 2.1239 (2.7172)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:13:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:21:27 lr 0.000019	 wd 0.0500	time 0.5895 (0.6430)	loss 0.8295 (1.1529)	grad_norm 2.1266 (2.6799)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:14:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:20:17 lr 0.000019	 wd 0.0500	time 0.5890 (0.6402)	loss 0.8287 (1.1526)	grad_norm 2.2553 (2.6414)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:15:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:19:09 lr 0.000019	 wd 0.0500	time 0.5897 (0.6382)	loss 1.2763 (1.1525)	grad_norm 3.0808 (2.6375)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:16:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:18:03 lr 0.000019	 wd 0.0500	time 0.5896 (0.6365)	loss 1.2993 (1.1546)	grad_norm 1.8822 (2.6024)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:17:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:16:57 lr 0.000019	 wd 0.0500	time 0.5954 (0.6352)	loss 0.9192 (1.1507)	grad_norm 3.1679 (2.5955)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:18:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:15:52 lr 0.000019	 wd 0.0500	time 0.5898 (0.6342)	loss 1.3193 (1.1525)	grad_norm 1.8457 (2.5787)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:19:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:14:47 lr 0.000019	 wd 0.0500	time 0.5944 (0.6334)	loss 1.2831 (1.1496)	grad_norm 1.6187 (2.5516)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:20:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:13:43 lr 0.000019	 wd 0.0500	time 0.5934 (0.6327)	loss 1.0363 (1.1503)	grad_norm 1.9250 (2.5407)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:21:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:12:39 lr 0.000019	 wd 0.0500	time 0.6032 (0.6322)	loss 0.8088 (1.1500)	grad_norm 2.0644 (2.5132)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:22:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:11:36 lr 0.000019	 wd 0.0500	time 0.6034 (0.6318)	loss 1.2814 (1.1511)	grad_norm 1.6338 (2.5111)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:23:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:10:32 lr 0.000019	 wd 0.0500	time 0.6173 (0.6314)	loss 1.0575 (1.1518)	grad_norm 1.8678 (2.5334)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:24:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:09:29 lr 0.000019	 wd 0.0500	time 0.5966 (0.6310)	loss 1.3793 (1.1536)	grad_norm 2.2850 (2.5264)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:25:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:08:25 lr 0.000019	 wd 0.0500	time 0.5928 (0.6307)	loss 0.7718 (1.1559)	grad_norm 2.2810 (2.5401)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-29 23:27:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:07:22 lr 0.000019	 wd 0.0500	time 0.5889 (0.6304)	loss 0.7813 (1.1542)	grad_norm 21.3000 (2.5946)	loss_scale 2048.0000 (1072.8973)	mem 20188MB
[2024-07-29 23:28:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:06:19 lr 0.000019	 wd 0.0500	time 0.5987 (0.6301)	loss 1.2588 (1.1558)	grad_norm 1.8021 (2.5885)	loss_scale 2048.0000 (1124.1915)	mem 20188MB
[2024-07-29 23:29:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:05:16 lr 0.000019	 wd 0.0500	time 0.5882 (0.6299)	loss 0.7421 (1.1546)	grad_norm 2.3973 (2.5725)	loss_scale 2048.0000 (1170.3588)	mem 20188MB
[2024-07-29 23:30:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:04:13 lr 0.000019	 wd 0.0500	time 0.6146 (0.6297)	loss 0.8874 (1.1553)	grad_norm 2.4710 (2.5536)	loss_scale 2048.0000 (1212.1314)	mem 20188MB
[2024-07-29 23:31:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:03:10 lr 0.000019	 wd 0.0500	time 0.5956 (0.6295)	loss 1.2023 (1.1540)	grad_norm 1.7842 (2.5504)	loss_scale 2048.0000 (1250.1081)	mem 20188MB
[2024-07-29 23:32:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:02:07 lr 0.000019	 wd 0.0500	time 0.6269 (0.6293)	loss 1.2456 (1.1524)	grad_norm 2.0380 (2.5394)	loss_scale 2048.0000 (1284.7840)	mem 20188MB
[2024-07-29 23:33:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:04 lr 0.000019	 wd 0.0500	time 0.6253 (0.6291)	loss 1.0722 (1.1523)	grad_norm 2.1535 (2.5355)	loss_scale 2048.0000 (1316.5714)	mem 20188MB
[2024-07-29 23:34:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5872 (0.6290)	loss 0.8711 (1.1519)	grad_norm 2.9540 (2.5244)	loss_scale 2048.0000 (1345.8169)	mem 20188MB
[2024-07-29 23:34:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 8 training takes 0:26:16
[2024-07-29 23:34:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 11.042 (11.042)	Loss 0.4751 (0.4751)	Acc@1 92.383 (92.383)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-29 23:34:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.396 Acc@5 97.948
[2024-07-29 23:34:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-07-29 23:34:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.40%
[2024-07-29 23:34:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-29 23:34:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-29 23:35:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:01:09 lr 0.000019	 wd 0.0500	time 10.0996 (10.0996)	loss 1.3252 (1.3252)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 23:36:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:28:41 lr 0.000019	 wd 0.0500	time 0.5964 (0.7165)	loss 1.2434 (1.1367)	grad_norm 4.7648 (2.4916)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 23:37:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:25:44 lr 0.000019	 wd 0.0500	time 0.5875 (0.6709)	loss 0.9697 (1.1597)	grad_norm 1.9409 (2.5148)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-29 23:38:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:24:03 lr 0.000019	 wd 0.0500	time 0.5933 (0.6555)	loss 1.4243 (1.1644)	grad_norm 1.7425 (inf)	loss_scale 1024.0000 (1782.6445)	mem 20188MB
[2024-07-29 23:39:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:22:41 lr 0.000019	 wd 0.0500	time 0.5953 (0.6478)	loss 0.9134 (1.1450)	grad_norm 3.2205 (inf)	loss_scale 1024.0000 (1593.4564)	mem 20188MB
[2024-07-29 23:40:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:21:27 lr 0.000019	 wd 0.0500	time 0.5885 (0.6432)	loss 0.6786 (1.1404)	grad_norm 2.3362 (inf)	loss_scale 1024.0000 (1479.7924)	mem 20188MB
[2024-07-29 23:41:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:20:17 lr 0.000019	 wd 0.0500	time 0.5955 (0.6402)	loss 0.7791 (1.1431)	grad_norm 1.9283 (inf)	loss_scale 1024.0000 (1403.9534)	mem 20188MB
[2024-07-29 23:42:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:19:09 lr 0.000019	 wd 0.0500	time 0.6059 (0.6381)	loss 1.4247 (1.1457)	grad_norm 1.6301 (inf)	loss_scale 1024.0000 (1349.7518)	mem 20188MB
[2024-07-29 23:43:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:18:03 lr 0.000019	 wd 0.0500	time 0.5965 (0.6365)	loss 1.4117 (1.1474)	grad_norm 1.7930 (inf)	loss_scale 1024.0000 (1309.0836)	mem 20188MB
[2024-07-29 23:44:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:16:57 lr 0.000019	 wd 0.0500	time 0.5920 (0.6353)	loss 1.3463 (1.1476)	grad_norm 1.8727 (inf)	loss_scale 1024.0000 (1277.4428)	mem 20188MB
[2024-07-29 23:45:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:15:52 lr 0.000019	 wd 0.0500	time 0.6197 (0.6343)	loss 1.2677 (1.1480)	grad_norm 1.3610 (inf)	loss_scale 1024.0000 (1252.1239)	mem 20188MB
[2024-07-29 23:46:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:14:48 lr 0.000018	 wd 0.0500	time 0.5964 (0.6335)	loss 1.3935 (1.1441)	grad_norm 1.8331 (inf)	loss_scale 1024.0000 (1231.4042)	mem 20188MB
[2024-07-29 23:47:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:13:44 lr 0.000018	 wd 0.0500	time 0.5844 (0.6329)	loss 0.9727 (1.1443)	grad_norm 1.9089 (inf)	loss_scale 1024.0000 (1214.1349)	mem 20188MB
[2024-07-29 23:48:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:12:39 lr 0.000018	 wd 0.0500	time 0.5905 (0.6322)	loss 1.3281 (1.1453)	grad_norm 1.8826 (inf)	loss_scale 1024.0000 (1199.5204)	mem 20188MB
[2024-07-29 23:49:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:11:36 lr 0.000018	 wd 0.0500	time 0.5959 (0.6318)	loss 0.9211 (1.1433)	grad_norm 1.6242 (inf)	loss_scale 1024.0000 (1186.9921)	mem 20188MB
[2024-07-29 23:50:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:10:32 lr 0.000018	 wd 0.0500	time 0.5942 (0.6313)	loss 0.8836 (1.1458)	grad_norm 3.0675 (inf)	loss_scale 1024.0000 (1176.1332)	mem 20188MB
[2024-07-29 23:51:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:09:29 lr 0.000018	 wd 0.0500	time 0.6006 (0.6309)	loss 0.8954 (1.1439)	grad_norm 1.6007 (inf)	loss_scale 1024.0000 (1166.6309)	mem 20188MB
[2024-07-29 23:52:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:08:25 lr 0.000018	 wd 0.0500	time 0.5955 (0.6307)	loss 1.3902 (1.1440)	grad_norm 2.1750 (inf)	loss_scale 1024.0000 (1158.2457)	mem 20188MB
[2024-07-29 23:53:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:07:22 lr 0.000018	 wd 0.0500	time 0.5914 (0.6304)	loss 1.2728 (1.1433)	grad_norm 1.9846 (inf)	loss_scale 1024.0000 (1150.7918)	mem 20188MB
[2024-07-29 23:54:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:06:19 lr 0.000018	 wd 0.0500	time 0.6004 (0.6301)	loss 1.3815 (1.1421)	grad_norm 2.0469 (inf)	loss_scale 1024.0000 (1144.1220)	mem 20188MB
[2024-07-29 23:55:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:05:16 lr 0.000018	 wd 0.0500	time 0.5772 (0.6300)	loss 1.1798 (1.1425)	grad_norm 1.5796 (inf)	loss_scale 1024.0000 (1138.1189)	mem 20188MB
[2024-07-29 23:56:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:04:13 lr 0.000018	 wd 0.0500	time 0.6117 (0.6297)	loss 1.3647 (1.1437)	grad_norm 2.3066 (inf)	loss_scale 1024.0000 (1132.6873)	mem 20188MB
[2024-07-29 23:57:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:03:10 lr 0.000018	 wd 0.0500	time 0.6089 (0.6296)	loss 1.2211 (1.1446)	grad_norm 1.7411 (inf)	loss_scale 1024.0000 (1127.7492)	mem 20188MB
[2024-07-29 23:59:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:02:07 lr 0.000018	 wd 0.0500	time 0.6260 (0.6294)	loss 0.8248 (1.1446)	grad_norm 4.3546 (inf)	loss_scale 1024.0000 (1123.2403)	mem 20188MB
[2024-07-30 00:00:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:04 lr 0.000018	 wd 0.0500	time 0.5976 (0.6292)	loss 1.1187 (1.1450)	grad_norm 3.0246 (inf)	loss_scale 1024.0000 (1119.1070)	mem 20188MB
[2024-07-30 00:01:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.5932 (0.6291)	loss 1.3630 (1.1444)	grad_norm 2.0857 (inf)	loss_scale 1024.0000 (1115.3043)	mem 20188MB
[2024-07-30 00:01:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 9 training takes 0:26:16
[2024-07-30 00:01:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.267 (10.267)	Loss 0.4844 (0.4844)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 20188MB
[2024-07-30 00:01:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.470 Acc@5 97.920
[2024-07-30 00:01:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.5%
[2024-07-30 00:01:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.47%
[2024-07-30 00:01:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 00:01:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 00:01:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][0/2502]	eta 6:46:01 lr 0.000018	 wd 0.0500	time 9.7367 (9.7367)	loss 1.3944 (1.3944)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:02:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:28:39 lr 0.000018	 wd 0.0500	time 0.6085 (0.7158)	loss 0.8232 (1.1504)	grad_norm 22.6595 (3.5519)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:03:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:25:43 lr 0.000018	 wd 0.0500	time 0.5945 (0.6705)	loss 1.3502 (1.1395)	grad_norm 2.2082 (2.9616)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:05:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:24:02 lr 0.000018	 wd 0.0500	time 0.5947 (0.6553)	loss 1.3414 (1.1397)	grad_norm 2.3958 (2.7664)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:06:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:22:41 lr 0.000018	 wd 0.0500	time 0.5985 (0.6479)	loss 0.7618 (1.1467)	grad_norm 2.1841 (2.9759)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:07:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:21:28 lr 0.000018	 wd 0.0500	time 0.5942 (0.6435)	loss 0.7259 (1.1442)	grad_norm 5.0033 (3.1227)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:08:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:20:18 lr 0.000018	 wd 0.0500	time 0.5792 (0.6405)	loss 1.2109 (1.1505)	grad_norm 2.1712 (3.0102)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:09:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:19:10 lr 0.000018	 wd 0.0500	time 0.5959 (0.6384)	loss 1.2644 (1.1478)	grad_norm 1.3843 (2.8848)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:10:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:18:03 lr 0.000018	 wd 0.0500	time 0.5978 (0.6368)	loss 1.2574 (1.1504)	grad_norm 1.9782 (2.8616)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:11:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:16:58 lr 0.000018	 wd 0.0500	time 0.5945 (0.6355)	loss 1.2895 (1.1488)	grad_norm 1.6346 (2.8215)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:12:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:15:52 lr 0.000018	 wd 0.0500	time 0.5927 (0.6345)	loss 1.2942 (1.1497)	grad_norm 2.7561 (2.7939)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:13:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:14:48 lr 0.000018	 wd 0.0500	time 0.6117 (0.6336)	loss 1.0090 (1.1567)	grad_norm 2.2536 (2.7662)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:14:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:13:44 lr 0.000018	 wd 0.0500	time 0.6035 (0.6329)	loss 1.4306 (1.1536)	grad_norm 1.9485 (2.7320)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:15:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:12:39 lr 0.000018	 wd 0.0500	time 0.6017 (0.6323)	loss 1.1898 (1.1518)	grad_norm 4.0327 (2.7011)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:16:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:11:36 lr 0.000018	 wd 0.0500	time 0.5952 (0.6317)	loss 1.4208 (1.1504)	grad_norm 2.2811 (2.6729)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:17:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:10:32 lr 0.000018	 wd 0.0500	time 0.5896 (0.6313)	loss 1.1656 (1.1486)	grad_norm 1.8734 (2.6540)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:18:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:09:29 lr 0.000018	 wd 0.0500	time 0.5963 (0.6309)	loss 1.1410 (1.1468)	grad_norm 2.2326 (2.6607)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:19:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:08:25 lr 0.000018	 wd 0.0500	time 0.5994 (0.6306)	loss 1.2526 (1.1486)	grad_norm 2.1064 (2.6469)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 00:20:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:07:22 lr 0.000018	 wd 0.0500	time 0.5928 (0.6303)	loss 0.9430 (1.1470)	grad_norm 2.3939 (2.6640)	loss_scale 2048.0000 (1069.4858)	mem 20188MB
[2024-07-30 00:21:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:06:19 lr 0.000018	 wd 0.0500	time 0.5909 (0.6301)	loss 1.0458 (1.1457)	grad_norm 17.0976 (2.6807)	loss_scale 2048.0000 (1120.9595)	mem 20188MB
[2024-07-30 00:22:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:05:16 lr 0.000017	 wd 0.0500	time 0.5819 (0.6299)	loss 1.2826 (1.1487)	grad_norm 2.1709 (2.6850)	loss_scale 2048.0000 (1167.2884)	mem 20188MB
[2024-07-30 00:23:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:04:13 lr 0.000017	 wd 0.0500	time 0.6004 (0.6297)	loss 1.0246 (1.1502)	grad_norm 3.1318 (2.6781)	loss_scale 2048.0000 (1209.2070)	mem 20188MB
[2024-07-30 00:24:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:03:10 lr 0.000017	 wd 0.0500	time 0.5852 (0.6296)	loss 1.2479 (1.1512)	grad_norm 1.8192 (2.6830)	loss_scale 2048.0000 (1247.3167)	mem 20188MB
[2024-07-30 00:25:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:02:07 lr 0.000017	 wd 0.0500	time 0.5987 (0.6295)	loss 1.2665 (1.1491)	grad_norm 2.4691 (2.6691)	loss_scale 2048.0000 (1282.1139)	mem 20188MB
[2024-07-30 00:26:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:04 lr 0.000017	 wd 0.0500	time 0.6007 (0.6293)	loss 1.4290 (1.1479)	grad_norm 2.1029 (2.6630)	loss_scale 2048.0000 (1314.0125)	mem 20188MB
[2024-07-30 00:27:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.5904 (0.6292)	loss 0.8287 (1.1470)	grad_norm 1.6049 (2.6814)	loss_scale 2048.0000 (1343.3603)	mem 20188MB
[2024-07-30 00:27:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 10 training takes 0:26:16
[2024-07-30 00:28:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.942 (10.942)	Loss 0.4795 (0.4795)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-30 00:28:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.426 Acc@5 97.950
[2024-07-30 00:28:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-07-30 00:28:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.47%
[2024-07-30 00:28:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:32:24 lr 0.000017	 wd 0.0500	time 10.8489 (10.8489)	loss 0.9457 (0.9457)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:29:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:29:16 lr 0.000017	 wd 0.0500	time 0.5982 (0.7311)	loss 1.3384 (1.1340)	grad_norm 1.9729 (2.4255)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:30:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:26:00 lr 0.000017	 wd 0.0500	time 0.5910 (0.6777)	loss 1.2438 (1.1440)	grad_norm 2.2438 (2.5347)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:31:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:24:13 lr 0.000017	 wd 0.0500	time 0.5992 (0.6602)	loss 0.8916 (1.1370)	grad_norm 1.9167 (2.5506)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:32:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:22:49 lr 0.000017	 wd 0.0500	time 0.5955 (0.6513)	loss 1.4042 (1.1393)	grad_norm 1.7667 (2.4938)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:33:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:21:33 lr 0.000017	 wd 0.0500	time 0.5944 (0.6460)	loss 0.7593 (1.1380)	grad_norm 5.1898 (2.5022)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:34:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:20:22 lr 0.000017	 wd 0.0500	time 0.5921 (0.6426)	loss 1.5196 (1.1400)	grad_norm 2.1326 (2.6012)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:35:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:19:13 lr 0.000017	 wd 0.0500	time 0.6041 (0.6402)	loss 0.9173 (1.1372)	grad_norm 2.7398 (2.5908)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:37:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:18:06 lr 0.000017	 wd 0.0500	time 0.5952 (0.6383)	loss 1.4218 (1.1357)	grad_norm 1.8034 (2.5818)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:38:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:17:00 lr 0.000017	 wd 0.0500	time 0.5963 (0.6368)	loss 0.8630 (1.1379)	grad_norm 2.3470 (2.6148)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:39:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:15:54 lr 0.000017	 wd 0.0500	time 0.5772 (0.6357)	loss 1.2306 (1.1388)	grad_norm 2.2763 (2.5993)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:40:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:14:49 lr 0.000017	 wd 0.0500	time 0.5869 (0.6347)	loss 0.9111 (1.1371)	grad_norm 2.2298 (2.5849)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:41:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:13:45 lr 0.000017	 wd 0.0500	time 0.5902 (0.6339)	loss 0.9836 (1.1334)	grad_norm 2.6491 (2.5737)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:42:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:12:41 lr 0.000017	 wd 0.0500	time 0.6045 (0.6332)	loss 1.2574 (1.1339)	grad_norm 3.9630 (2.5716)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:43:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:11:37 lr 0.000017	 wd 0.0500	time 0.6135 (0.6326)	loss 0.8359 (1.1348)	grad_norm 4.2814 (2.6028)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:44:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:10:33 lr 0.000017	 wd 0.0500	time 0.6216 (0.6321)	loss 1.3604 (1.1345)	grad_norm 2.2306 (2.6015)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:45:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:09:29 lr 0.000017	 wd 0.0500	time 0.5976 (0.6317)	loss 1.2631 (1.1358)	grad_norm 1.7863 (2.6268)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:46:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:08:26 lr 0.000017	 wd 0.0500	time 0.5891 (0.6313)	loss 1.0090 (1.1365)	grad_norm 2.4600 (2.6780)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:47:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:07:22 lr 0.000017	 wd 0.0500	time 0.6003 (0.6310)	loss 0.8575 (1.1354)	grad_norm 2.5835 (2.6802)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:48:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:06:19 lr 0.000017	 wd 0.0500	time 0.5980 (0.6306)	loss 1.0434 (1.1338)	grad_norm 3.1988 (2.6783)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:49:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:05:16 lr 0.000017	 wd 0.0500	time 0.6005 (0.6304)	loss 1.1823 (1.1343)	grad_norm 1.5969 (2.6745)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:50:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:04:13 lr 0.000017	 wd 0.0500	time 0.6156 (0.6302)	loss 1.3631 (1.1339)	grad_norm 2.1251 (2.6746)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:51:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:03:10 lr 0.000017	 wd 0.0500	time 0.5988 (0.6299)	loss 0.8759 (1.1352)	grad_norm 2.6120 (2.6719)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:52:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:02:07 lr 0.000016	 wd 0.0500	time 0.5964 (0.6297)	loss 1.1651 (1.1337)	grad_norm 2.1124 (2.6735)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:53:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:04 lr 0.000016	 wd 0.0500	time 0.5973 (0.6296)	loss 1.2569 (1.1349)	grad_norm 2.0459 (2.6828)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:54:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5966 (0.6294)	loss 0.8138 (1.1330)	grad_norm 2.0548 (2.7268)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:54:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 11 training takes 0:26:17
[2024-07-30 00:54:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.909 (10.909)	Loss 0.4763 (0.4763)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-30 00:55:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.448 Acc@5 97.980
[2024-07-30 00:55:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-07-30 00:55:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.47%
[2024-07-30 00:55:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:10:28 lr 0.000016	 wd 0.0500	time 10.3232 (10.3232)	loss 0.9034 (0.9034)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:56:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:29:00 lr 0.000016	 wd 0.0500	time 0.5933 (0.7245)	loss 1.2728 (1.1279)	grad_norm 1.7967 (2.2079)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:57:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:25:52 lr 0.000016	 wd 0.0500	time 0.5791 (0.6745)	loss 1.1732 (1.1269)	grad_norm 4.8925 (2.3772)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:58:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:24:08 lr 0.000016	 wd 0.0500	time 0.5893 (0.6579)	loss 0.9098 (1.1369)	grad_norm 2.2981 (2.4629)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 00:59:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:22:45 lr 0.000016	 wd 0.0500	time 0.5936 (0.6497)	loss 1.1968 (1.1321)	grad_norm 3.5949 (2.7285)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 01:00:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:21:30 lr 0.000016	 wd 0.0500	time 0.5956 (0.6448)	loss 1.3841 (1.1305)	grad_norm 1.6098 (2.6972)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 01:01:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:20:19 lr 0.000016	 wd 0.0500	time 0.5951 (0.6414)	loss 1.3722 (1.1279)	grad_norm 2.8549 (2.6624)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 01:02:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:19:11 lr 0.000016	 wd 0.0500	time 0.5943 (0.6391)	loss 1.2447 (1.1257)	grad_norm 2.5030 (2.6299)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 01:03:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:18:05 lr 0.000016	 wd 0.0500	time 0.6019 (0.6376)	loss 1.3485 (1.1270)	grad_norm 1.6448 (2.6107)	loss_scale 4096.0000 (2262.7715)	mem 20188MB
[2024-07-30 01:04:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:16:59 lr 0.000016	 wd 0.0500	time 0.5965 (0.6363)	loss 1.3586 (1.1243)	grad_norm 1.9338 (2.6057)	loss_scale 4096.0000 (2466.2375)	mem 20188MB
[2024-07-30 01:05:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:15:54 lr 0.000016	 wd 0.0500	time 0.6174 (0.6352)	loss 1.2673 (1.1282)	grad_norm 1.7389 (2.5843)	loss_scale 4096.0000 (2629.0509)	mem 20188MB
[2024-07-30 01:06:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:14:49 lr 0.000016	 wd 0.0500	time 0.6022 (0.6344)	loss 0.8995 (1.1302)	grad_norm 6.4407 (2.5661)	loss_scale 4096.0000 (2762.2888)	mem 20188MB
[2024-07-30 01:08:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:13:45 lr 0.000016	 wd 0.0500	time 0.6076 (0.6338)	loss 1.2686 (1.1291)	grad_norm 2.0096 (2.5504)	loss_scale 4096.0000 (2873.3389)	mem 20188MB
[2024-07-30 01:09:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:12:40 lr 0.000016	 wd 0.0500	time 0.5857 (0.6331)	loss 1.0334 (1.1305)	grad_norm 2.1697 (2.5704)	loss_scale 4096.0000 (2967.3174)	mem 20188MB
[2024-07-30 01:10:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:11:37 lr 0.000016	 wd 0.0500	time 0.5897 (0.6325)	loss 1.3928 (1.1312)	grad_norm 2.8132 (2.5819)	loss_scale 4096.0000 (3047.8801)	mem 20188MB
[2024-07-30 01:11:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:10:33 lr 0.000016	 wd 0.0500	time 0.6242 (0.6321)	loss 0.8138 (1.1280)	grad_norm 2.9153 (2.6046)	loss_scale 4096.0000 (3117.7082)	mem 20188MB
[2024-07-30 01:12:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:09:29 lr 0.000016	 wd 0.0500	time 0.5927 (0.6316)	loss 1.3990 (1.1255)	grad_norm 1.5911 (2.6047)	loss_scale 4096.0000 (3178.8132)	mem 20188MB
[2024-07-30 01:13:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:08:26 lr 0.000016	 wd 0.0500	time 0.5925 (0.6312)	loss 1.1608 (1.1279)	grad_norm 5.5112 (2.6007)	loss_scale 4096.0000 (3232.7337)	mem 20188MB
[2024-07-30 01:14:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:07:22 lr 0.000016	 wd 0.0500	time 0.5885 (0.6309)	loss 0.7980 (1.1267)	grad_norm 2.5682 (2.6410)	loss_scale 4096.0000 (3280.6663)	mem 20188MB
[2024-07-30 01:15:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:06:19 lr 0.000016	 wd 0.0500	time 0.6197 (0.6306)	loss 1.1850 (1.1270)	grad_norm 1.7517 (2.6296)	loss_scale 4096.0000 (3323.5560)	mem 20188MB
[2024-07-30 01:16:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:05:16 lr 0.000016	 wd 0.0500	time 0.6117 (0.6304)	loss 0.9910 (1.1281)	grad_norm 2.2666 (2.6307)	loss_scale 4096.0000 (3362.1589)	mem 20188MB
[2024-07-30 01:17:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:04:13 lr 0.000016	 wd 0.0500	time 0.5960 (0.6302)	loss 0.7998 (1.1272)	grad_norm 2.1761 (2.6368)	loss_scale 4096.0000 (3397.0871)	mem 20188MB
[2024-07-30 01:18:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:03:10 lr 0.000016	 wd 0.0500	time 0.5987 (0.6300)	loss 0.7578 (1.1286)	grad_norm 2.2919 (2.6339)	loss_scale 4096.0000 (3428.8414)	mem 20188MB
[2024-07-30 01:19:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:02:07 lr 0.000015	 wd 0.0500	time 0.5783 (0.6299)	loss 1.0469 (1.1289)	grad_norm 2.4009 (2.6237)	loss_scale 4096.0000 (3457.8357)	mem 20188MB
[2024-07-30 01:20:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:04 lr 0.000015	 wd 0.0500	time 0.5851 (0.6296)	loss 1.3604 (1.1294)	grad_norm 2.0130 (2.6132)	loss_scale 4096.0000 (3484.4148)	mem 20188MB
[2024-07-30 01:21:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.6135 (0.6294)	loss 0.7504 (1.1287)	grad_norm 3.5031 (2.6145)	loss_scale 4096.0000 (3508.8685)	mem 20188MB
[2024-07-30 01:21:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 12 training takes 0:26:17
[2024-07-30 01:21:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.595 (10.595)	Loss 0.4849 (0.4849)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 20188MB
[2024-07-30 01:22:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.562 Acc@5 97.970
[2024-07-30 01:22:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-07-30 01:22:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.56%
[2024-07-30 01:22:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 01:22:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 01:22:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][0/2502]	eta 6:48:58 lr 0.000015	 wd 0.0500	time 9.8075 (9.8075)	loss 1.2952 (1.2952)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 01:23:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:28:38 lr 0.000015	 wd 0.0500	time 0.5924 (0.7156)	loss 1.3666 (1.1870)	grad_norm 1.8153 (2.6782)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 01:24:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:25:43 lr 0.000015	 wd 0.0500	time 0.5958 (0.6705)	loss 1.3854 (1.1717)	grad_norm 2.0173 (2.5541)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 01:25:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:24:02 lr 0.000015	 wd 0.0500	time 0.6166 (0.6552)	loss 1.3909 (1.1707)	grad_norm 2.1733 (2.4504)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 01:26:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:22:41 lr 0.000015	 wd 0.0500	time 0.6062 (0.6479)	loss 1.4177 (1.1547)	grad_norm 2.4084 (2.4301)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 01:27:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:21:27 lr 0.000015	 wd 0.0500	time 0.5978 (0.6432)	loss 1.3404 (1.1479)	grad_norm 4.2101 (2.4483)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 01:28:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:20:17 lr 0.000015	 wd 0.0500	time 0.6016 (0.6403)	loss 1.2239 (1.1417)	grad_norm 2.1496 (nan)	loss_scale 2048.0000 (3993.7704)	mem 20188MB
[2024-07-30 01:29:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:19:10 lr 0.000015	 wd 0.0500	time 0.5989 (0.6383)	loss 1.0700 (1.1353)	grad_norm 3.3172 (nan)	loss_scale 2048.0000 (3716.1997)	mem 20188MB
[2024-07-30 01:30:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:18:03 lr 0.000015	 wd 0.0500	time 0.5948 (0.6366)	loss 1.4019 (1.1353)	grad_norm 2.0088 (nan)	loss_scale 2048.0000 (3507.9351)	mem 20188MB
[2024-07-30 01:31:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:16:58 lr 0.000015	 wd 0.0500	time 0.5794 (0.6355)	loss 1.1070 (1.1349)	grad_norm 2.2223 (nan)	loss_scale 2048.0000 (3345.9001)	mem 20188MB
[2024-07-30 01:32:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:15:53 lr 0.000015	 wd 0.0500	time 0.6172 (0.6345)	loss 1.2462 (1.1342)	grad_norm 2.2939 (nan)	loss_scale 2048.0000 (3216.2398)	mem 20188MB
[2024-07-30 01:33:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:14:48 lr 0.000015	 wd 0.0500	time 0.6045 (0.6337)	loss 1.3012 (1.1309)	grad_norm 2.0278 (nan)	loss_scale 2048.0000 (3110.1326)	mem 20188MB
[2024-07-30 01:34:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:13:44 lr 0.000015	 wd 0.0500	time 0.5981 (0.6331)	loss 1.2421 (1.1349)	grad_norm 3.7887 (nan)	loss_scale 2048.0000 (3021.6953)	mem 20188MB
[2024-07-30 01:35:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:12:40 lr 0.000015	 wd 0.0500	time 0.6134 (0.6325)	loss 0.8504 (1.1347)	grad_norm 2.6191 (nan)	loss_scale 2048.0000 (2946.8532)	mem 20188MB
[2024-07-30 01:36:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:11:36 lr 0.000015	 wd 0.0500	time 0.5852 (0.6320)	loss 1.3601 (1.1334)	grad_norm 2.0853 (nan)	loss_scale 2048.0000 (2882.6952)	mem 20188MB
[2024-07-30 01:37:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:10:32 lr 0.000015	 wd 0.0500	time 0.5994 (0.6315)	loss 1.1486 (1.1336)	grad_norm 3.0499 (nan)	loss_scale 2048.0000 (2827.0859)	mem 20188MB
[2024-07-30 01:39:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:09:29 lr 0.000015	 wd 0.0500	time 0.5861 (0.6311)	loss 0.9798 (1.1332)	grad_norm 3.8586 (nan)	loss_scale 2048.0000 (2778.4235)	mem 20188MB
[2024-07-30 01:40:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:08:25 lr 0.000015	 wd 0.0500	time 0.5950 (0.6308)	loss 1.0675 (1.1340)	grad_norm 2.0463 (nan)	loss_scale 2048.0000 (2735.4827)	mem 20188MB
[2024-07-30 01:41:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:07:22 lr 0.000015	 wd 0.0500	time 0.5864 (0.6305)	loss 1.3603 (1.1341)	grad_norm 2.0376 (nan)	loss_scale 2048.0000 (2697.3104)	mem 20188MB
[2024-07-30 01:42:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:06:19 lr 0.000015	 wd 0.0500	time 0.5903 (0.6303)	loss 1.0866 (1.1353)	grad_norm 2.5152 (nan)	loss_scale 2048.0000 (2663.1541)	mem 20188MB
[2024-07-30 01:43:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:05:16 lr 0.000015	 wd 0.0500	time 0.5948 (0.6301)	loss 1.3239 (1.1350)	grad_norm 2.0040 (nan)	loss_scale 2048.0000 (2632.4118)	mem 20188MB
[2024-07-30 01:44:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:04:13 lr 0.000014	 wd 0.0500	time 0.5894 (0.6299)	loss 1.4039 (1.1355)	grad_norm 1.7637 (nan)	loss_scale 2048.0000 (2604.5959)	mem 20188MB
[2024-07-30 01:45:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:03:10 lr 0.000014	 wd 0.0500	time 0.5935 (0.6297)	loss 1.1656 (1.1340)	grad_norm 2.0944 (nan)	loss_scale 2048.0000 (2579.3076)	mem 20188MB
[2024-07-30 01:46:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:02:07 lr 0.000014	 wd 0.0500	time 0.5916 (0.6295)	loss 1.3128 (1.1336)	grad_norm 1.8232 (nan)	loss_scale 2048.0000 (2556.2173)	mem 20188MB
[2024-07-30 01:47:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:04 lr 0.000014	 wd 0.0500	time 0.5977 (0.6293)	loss 0.8490 (1.1364)	grad_norm 1.7590 (nan)	loss_scale 2048.0000 (2535.0504)	mem 20188MB
[2024-07-30 01:48:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.5952 (0.6292)	loss 0.8119 (1.1347)	grad_norm 4.2070 (nan)	loss_scale 2048.0000 (2515.5762)	mem 20188MB
[2024-07-30 01:48:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 13 training takes 0:26:16
[2024-07-30 01:48:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 11.084 (11.084)	Loss 0.4678 (0.4678)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 20188MB
[2024-07-30 01:48:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.540 Acc@5 98.002
[2024-07-30 01:48:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.5%
[2024-07-30 01:48:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.56%
[2024-07-30 01:49:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:18:14 lr 0.000014	 wd 0.0500	time 10.5094 (10.5094)	loss 1.2950 (1.2950)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 01:50:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:29:10 lr 0.000014	 wd 0.0500	time 0.5958 (0.7287)	loss 1.0340 (1.1359)	grad_norm 2.0178 (2.9397)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 01:51:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:25:58 lr 0.000014	 wd 0.0500	time 0.5938 (0.6769)	loss 1.3558 (1.1420)	grad_norm 1.8692 (2.8836)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 01:52:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:24:11 lr 0.000014	 wd 0.0500	time 0.5909 (0.6591)	loss 0.9411 (1.1441)	grad_norm 2.0530 (nan)	loss_scale 1024.0000 (1769.0365)	mem 20188MB
[2024-07-30 01:53:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:22:47 lr 0.000014	 wd 0.0500	time 0.6008 (0.6505)	loss 1.5420 (1.1458)	grad_norm 2.6672 (nan)	loss_scale 1024.0000 (1583.2419)	mem 20188MB
[2024-07-30 01:54:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:21:32 lr 0.000014	 wd 0.0500	time 0.6162 (0.6454)	loss 0.9431 (1.1420)	grad_norm 2.5261 (nan)	loss_scale 1024.0000 (1471.6168)	mem 20188MB
[2024-07-30 01:55:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:20:21 lr 0.000014	 wd 0.0500	time 0.6000 (0.6422)	loss 0.9616 (1.1393)	grad_norm 2.2740 (nan)	loss_scale 1024.0000 (1397.1381)	mem 20188MB
[2024-07-30 01:56:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:19:12 lr 0.000014	 wd 0.0500	time 0.5843 (0.6397)	loss 1.2026 (1.1410)	grad_norm 2.5776 (nan)	loss_scale 1024.0000 (1343.9087)	mem 20188MB
[2024-07-30 01:57:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:18:05 lr 0.000014	 wd 0.0500	time 0.5951 (0.6380)	loss 0.7710 (1.1360)	grad_norm 2.5816 (nan)	loss_scale 1024.0000 (1303.9700)	mem 20188MB
[2024-07-30 01:58:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:16:59 lr 0.000014	 wd 0.0500	time 0.5911 (0.6366)	loss 0.7907 (1.1346)	grad_norm 2.1950 (nan)	loss_scale 1024.0000 (1272.8968)	mem 20188MB
[2024-07-30 01:59:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:15:54 lr 0.000014	 wd 0.0500	time 0.5943 (0.6355)	loss 0.9743 (1.1327)	grad_norm 2.9627 (nan)	loss_scale 1024.0000 (1248.0320)	mem 20188MB
[2024-07-30 02:00:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:14:49 lr 0.000014	 wd 0.0500	time 0.5977 (0.6346)	loss 1.3683 (1.1335)	grad_norm 2.3516 (nan)	loss_scale 1024.0000 (1227.6839)	mem 20188MB
[2024-07-30 02:01:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:13:45 lr 0.000014	 wd 0.0500	time 0.6025 (0.6338)	loss 1.4937 (1.1334)	grad_norm 2.2477 (nan)	loss_scale 1024.0000 (1210.7244)	mem 20188MB
[2024-07-30 02:02:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:12:41 lr 0.000014	 wd 0.0500	time 0.5898 (0.6331)	loss 1.1664 (1.1366)	grad_norm 2.1590 (nan)	loss_scale 1024.0000 (1196.3720)	mem 20188MB
[2024-07-30 02:03:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:11:37 lr 0.000014	 wd 0.0500	time 0.6114 (0.6326)	loss 1.2702 (1.1369)	grad_norm 2.1804 (nan)	loss_scale 1024.0000 (1184.0685)	mem 20188MB
[2024-07-30 02:04:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:10:33 lr 0.000014	 wd 0.0500	time 0.5987 (0.6321)	loss 1.4319 (1.1356)	grad_norm 2.4154 (nan)	loss_scale 1024.0000 (1173.4044)	mem 20188MB
[2024-07-30 02:05:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:09:29 lr 0.000014	 wd 0.0500	time 0.5956 (0.6316)	loss 0.8917 (1.1347)	grad_norm 4.8188 (nan)	loss_scale 1024.0000 (1164.0725)	mem 20188MB
[2024-07-30 02:06:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:08:26 lr 0.000014	 wd 0.0500	time 0.5964 (0.6314)	loss 0.8466 (1.1340)	grad_norm 2.2726 (nan)	loss_scale 1024.0000 (1155.8377)	mem 20188MB
[2024-07-30 02:07:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:07:22 lr 0.000013	 wd 0.0500	time 0.5968 (0.6310)	loss 0.9977 (1.1351)	grad_norm 9.4929 (nan)	loss_scale 1024.0000 (1148.5175)	mem 20188MB
[2024-07-30 02:08:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:06:19 lr 0.000013	 wd 0.0500	time 0.5776 (0.6307)	loss 1.2183 (1.1358)	grad_norm 2.3797 (nan)	loss_scale 1024.0000 (1141.9674)	mem 20188MB
[2024-07-30 02:09:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:05:16 lr 0.000013	 wd 0.0500	time 0.5848 (0.6304)	loss 1.3691 (1.1344)	grad_norm 2.4822 (nan)	loss_scale 1024.0000 (1136.0720)	mem 20188MB
[2024-07-30 02:11:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:04:13 lr 0.000013	 wd 0.0500	time 0.5922 (0.6302)	loss 1.2495 (1.1336)	grad_norm 1.8342 (nan)	loss_scale 1024.0000 (1130.7377)	mem 20188MB
[2024-07-30 02:12:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:03:10 lr 0.000013	 wd 0.0500	time 0.6013 (0.6301)	loss 1.2176 (1.1328)	grad_norm 3.3058 (nan)	loss_scale 1024.0000 (1125.8882)	mem 20188MB
[2024-07-30 02:13:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:02:07 lr 0.000013	 wd 0.0500	time 0.5906 (0.6298)	loss 0.8123 (1.1322)	grad_norm 3.3185 (nan)	loss_scale 1024.0000 (1121.4602)	mem 20188MB
[2024-07-30 02:14:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:04 lr 0.000013	 wd 0.0500	time 0.5989 (0.6296)	loss 0.7881 (1.1317)	grad_norm 1.9200 (nan)	loss_scale 1024.0000 (1117.4011)	mem 20188MB
[2024-07-30 02:15:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.6187 (0.6295)	loss 1.2687 (1.1310)	grad_norm 1.9914 (nan)	loss_scale 1024.0000 (1113.6665)	mem 20188MB
[2024-07-30 02:15:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 14 training takes 0:26:17
[2024-07-30 02:15:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 11.134 (11.134)	Loss 0.4585 (0.4585)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 20188MB
[2024-07-30 02:15:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.610 Acc@5 97.984
[2024-07-30 02:15:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-07-30 02:15:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.61%
[2024-07-30 02:15:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 02:15:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 02:15:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:03:11 lr 0.000013	 wd 0.0500	time 10.1483 (10.1483)	loss 1.2478 (1.2478)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:17:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:28:46 lr 0.000013	 wd 0.0500	time 0.5917 (0.7188)	loss 0.7797 (1.1280)	grad_norm 2.7827 (3.1449)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:18:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:25:45 lr 0.000013	 wd 0.0500	time 0.5980 (0.6715)	loss 1.4955 (1.1172)	grad_norm 1.7579 (2.7538)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:19:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:24:04 lr 0.000013	 wd 0.0500	time 0.6182 (0.6562)	loss 0.8688 (1.1156)	grad_norm 1.4580 (2.6041)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:20:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:22:42 lr 0.000013	 wd 0.0500	time 0.5921 (0.6481)	loss 0.7407 (1.1137)	grad_norm 2.0540 (2.6600)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:21:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:21:28 lr 0.000013	 wd 0.0500	time 0.6001 (0.6434)	loss 0.7112 (1.1232)	grad_norm 1.8100 (2.7783)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:22:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:20:18 lr 0.000013	 wd 0.0500	time 0.5916 (0.6405)	loss 1.2351 (1.1282)	grad_norm 3.6548 (2.7433)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:23:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:19:10 lr 0.000013	 wd 0.0500	time 0.6027 (0.6384)	loss 1.3096 (1.1303)	grad_norm 4.4152 (2.7377)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:24:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:18:04 lr 0.000013	 wd 0.0500	time 0.5901 (0.6369)	loss 1.1482 (1.1312)	grad_norm 1.9040 (2.7207)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:25:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:16:58 lr 0.000013	 wd 0.0500	time 0.5970 (0.6357)	loss 1.3362 (1.1325)	grad_norm 2.3624 (2.7006)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:26:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:15:53 lr 0.000013	 wd 0.0500	time 0.5864 (0.6346)	loss 0.7708 (1.1324)	grad_norm 5.0972 (2.6853)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:27:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:14:48 lr 0.000013	 wd 0.0500	time 0.5967 (0.6337)	loss 0.8467 (1.1332)	grad_norm 1.8415 (2.6633)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:28:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:13:44 lr 0.000013	 wd 0.0500	time 0.5904 (0.6330)	loss 0.8696 (1.1337)	grad_norm 2.0064 (2.6605)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:29:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:12:40 lr 0.000013	 wd 0.0500	time 0.6182 (0.6324)	loss 0.9321 (1.1349)	grad_norm 2.4859 (2.7466)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:30:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:11:36 lr 0.000012	 wd 0.0500	time 0.5988 (0.6319)	loss 0.7921 (1.1347)	grad_norm 1.7612 (2.7861)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:31:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:10:32 lr 0.000012	 wd 0.0500	time 0.5914 (0.6317)	loss 1.0648 (1.1307)	grad_norm 2.2010 (2.7569)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:32:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:09:29 lr 0.000012	 wd 0.0500	time 0.5987 (0.6314)	loss 0.9101 (1.1319)	grad_norm 3.7432 (2.7573)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:33:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:08:26 lr 0.000012	 wd 0.0500	time 0.6011 (0.6310)	loss 0.7811 (1.1322)	grad_norm 2.0854 (2.7470)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:34:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:07:22 lr 0.000012	 wd 0.0500	time 0.5994 (0.6307)	loss 1.6199 (1.1308)	grad_norm 3.6804 (2.7305)	loss_scale 2048.0000 (1071.7601)	mem 20188MB
[2024-07-30 02:35:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:06:19 lr 0.000012	 wd 0.0500	time 0.5992 (0.6304)	loss 0.8862 (1.1299)	grad_norm 2.5920 (2.7067)	loss_scale 2048.0000 (1123.1142)	mem 20188MB
[2024-07-30 02:36:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:05:16 lr 0.000012	 wd 0.0500	time 0.5969 (0.6302)	loss 0.9208 (1.1311)	grad_norm 1.8387 (2.7060)	loss_scale 2048.0000 (1169.3353)	mem 20188MB
[2024-07-30 02:37:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:04:13 lr 0.000012	 wd 0.0500	time 0.5931 (0.6299)	loss 0.7240 (1.1298)	grad_norm 1.7752 (2.7036)	loss_scale 2048.0000 (1211.1566)	mem 20188MB
[2024-07-30 02:38:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:03:10 lr 0.000012	 wd 0.0500	time 0.6056 (0.6297)	loss 0.8721 (1.1292)	grad_norm 1.8958 (2.6877)	loss_scale 2048.0000 (1249.1776)	mem 20188MB
[2024-07-30 02:39:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:02:07 lr 0.000012	 wd 0.0500	time 0.5959 (0.6295)	loss 0.9682 (1.1298)	grad_norm 2.3491 (nan)	loss_scale 1024.0000 (1268.7631)	mem 20188MB
[2024-07-30 02:40:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:04 lr 0.000012	 wd 0.0500	time 0.6017 (0.6293)	loss 1.3053 (1.1300)	grad_norm 1.6108 (nan)	loss_scale 1024.0000 (1258.5689)	mem 20188MB
[2024-07-30 02:42:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.5935 (0.6292)	loss 1.4345 (1.1309)	grad_norm 2.7611 (nan)	loss_scale 1024.0000 (1249.1899)	mem 20188MB
[2024-07-30 02:42:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 15 training takes 0:26:16
[2024-07-30 02:42:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_15.pth saving......
[2024-07-30 02:42:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_15.pth saved !!!
[2024-07-30 02:42:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 9.928 (9.928)	Loss 0.5005 (0.5005)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 20188MB
[2024-07-30 02:42:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.548 Acc@5 97.974
[2024-07-30 02:42:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.5%
[2024-07-30 02:42:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.61%
[2024-07-30 02:42:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:23:38 lr 0.000012	 wd 0.0500	time 10.6389 (10.6389)	loss 1.2019 (1.2019)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:43:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:29:06 lr 0.000012	 wd 0.0500	time 0.5946 (0.7273)	loss 1.0735 (1.1390)	grad_norm 1.8418 (2.9647)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:44:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:25:56 lr 0.000012	 wd 0.0500	time 0.6177 (0.6761)	loss 1.1871 (1.1282)	grad_norm 1.7492 (2.8069)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:45:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:24:12 lr 0.000012	 wd 0.0500	time 0.5941 (0.6597)	loss 1.0265 (1.1224)	grad_norm 2.3511 (2.9413)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:46:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:22:47 lr 0.000012	 wd 0.0500	time 0.5967 (0.6508)	loss 0.8459 (1.1119)	grad_norm 1.8770 (2.7959)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:48:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:21:32 lr 0.000012	 wd 0.0500	time 0.5939 (0.6456)	loss 1.4000 (1.1160)	grad_norm 2.4325 (2.8382)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:49:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:20:21 lr 0.000012	 wd 0.0500	time 0.5897 (0.6422)	loss 1.3728 (1.1148)	grad_norm 1.6890 (2.7686)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:50:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:19:13 lr 0.000012	 wd 0.0500	time 0.6158 (0.6398)	loss 0.9774 (1.1169)	grad_norm 2.2472 (2.7495)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:51:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:18:06 lr 0.000012	 wd 0.0500	time 0.6226 (0.6381)	loss 1.2823 (1.1185)	grad_norm 6.7782 (3.0486)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:52:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:17:00 lr 0.000012	 wd 0.0500	time 0.5888 (0.6367)	loss 1.0346 (1.1166)	grad_norm 3.0854 (2.9800)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:53:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:15:54 lr 0.000011	 wd 0.0500	time 0.5961 (0.6355)	loss 1.2968 (1.1178)	grad_norm 1.4322 (2.8982)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:54:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:14:49 lr 0.000011	 wd 0.0500	time 0.5962 (0.6345)	loss 0.9031 (1.1230)	grad_norm 1.6192 (2.9016)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:55:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:13:45 lr 0.000011	 wd 0.0500	time 0.5827 (0.6337)	loss 0.8901 (1.1225)	grad_norm 3.5927 (2.8762)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:56:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:12:40 lr 0.000011	 wd 0.0500	time 0.5956 (0.6331)	loss 1.2359 (1.1256)	grad_norm 1.9965 (2.8362)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:57:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:11:37 lr 0.000011	 wd 0.0500	time 0.5788 (0.6326)	loss 1.2071 (1.1262)	grad_norm 2.1751 (2.8088)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:58:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:10:33 lr 0.000011	 wd 0.0500	time 0.5869 (0.6322)	loss 1.1633 (1.1282)	grad_norm 1.7620 (2.8192)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 02:59:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:09:29 lr 0.000011	 wd 0.0500	time 0.5819 (0.6317)	loss 1.2182 (1.1285)	grad_norm 2.6859 (2.8387)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:00:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:08:26 lr 0.000011	 wd 0.0500	time 0.5951 (0.6313)	loss 1.0304 (1.1279)	grad_norm 2.1240 (2.8291)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:01:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:07:22 lr 0.000011	 wd 0.0500	time 0.6253 (0.6309)	loss 0.8903 (1.1269)	grad_norm 1.8479 (2.8046)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:02:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:06:19 lr 0.000011	 wd 0.0500	time 0.5867 (0.6306)	loss 0.8437 (1.1275)	grad_norm 1.8024 (2.7805)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:03:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:05:16 lr 0.000011	 wd 0.0500	time 0.5961 (0.6304)	loss 1.2880 (1.1281)	grad_norm 3.3788 (2.7813)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:04:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:04:13 lr 0.000011	 wd 0.0500	time 0.5910 (0.6301)	loss 1.2211 (1.1287)	grad_norm 2.4420 (2.7575)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:05:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:03:10 lr 0.000011	 wd 0.0500	time 0.6011 (0.6299)	loss 0.8949 (1.1285)	grad_norm 2.3177 (2.7721)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:06:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:02:07 lr 0.000011	 wd 0.0500	time 0.5987 (0.6297)	loss 0.8030 (1.1267)	grad_norm 3.5539 (2.7731)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:07:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:04 lr 0.000011	 wd 0.0500	time 0.6104 (0.6296)	loss 1.1931 (1.1261)	grad_norm 1.7040 (2.8078)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:08:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.6003 (0.6294)	loss 0.7961 (1.1256)	grad_norm 4.8981 (2.8016)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:08:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 16 training takes 0:26:17
[2024-07-30 03:09:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.286 (10.286)	Loss 0.4697 (0.4697)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 20188MB
[2024-07-30 03:09:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.624 Acc@5 97.974
[2024-07-30 03:09:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-07-30 03:09:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.62%
[2024-07-30 03:09:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 03:09:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 03:09:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][0/2502]	eta 6:59:40 lr 0.000011	 wd 0.0500	time 10.0643 (10.0643)	loss 1.2122 (1.2122)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:10:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:28:52 lr 0.000011	 wd 0.0500	time 0.6027 (0.7214)	loss 1.2402 (1.1017)	grad_norm 1.7638 (2.6954)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:11:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:25:48 lr 0.000011	 wd 0.0500	time 0.5936 (0.6727)	loss 1.3544 (1.1072)	grad_norm 2.0705 (2.4347)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:12:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:24:06 lr 0.000011	 wd 0.0500	time 0.5948 (0.6568)	loss 1.4028 (1.1211)	grad_norm 3.3648 (2.4766)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:13:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:22:43 lr 0.000011	 wd 0.0500	time 0.6017 (0.6487)	loss 1.4986 (1.1169)	grad_norm 2.0572 (2.4018)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:14:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:21:29 lr 0.000010	 wd 0.0500	time 0.5981 (0.6443)	loss 1.3278 (1.1234)	grad_norm 3.4630 (2.4642)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:15:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:20:19 lr 0.000010	 wd 0.0500	time 0.6006 (0.6410)	loss 0.8690 (1.1247)	grad_norm 1.6672 (2.4086)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:16:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:19:10 lr 0.000010	 wd 0.0500	time 0.5899 (0.6387)	loss 1.3189 (1.1220)	grad_norm 3.4480 (2.4019)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:17:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:18:04 lr 0.000010	 wd 0.0500	time 0.6133 (0.6370)	loss 1.1731 (1.1218)	grad_norm 2.5615 (2.4518)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:18:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:16:58 lr 0.000010	 wd 0.0500	time 0.6296 (0.6359)	loss 1.2764 (1.1238)	grad_norm 2.1287 (2.6411)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:20:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:15:53 lr 0.000010	 wd 0.0500	time 0.5936 (0.6350)	loss 0.9629 (1.1223)	grad_norm 1.8939 (2.6256)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:21:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:14:49 lr 0.000010	 wd 0.0500	time 0.6032 (0.6342)	loss 0.9540 (1.1270)	grad_norm 1.8732 (2.5918)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:22:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:13:44 lr 0.000010	 wd 0.0500	time 0.5970 (0.6335)	loss 0.8255 (1.1270)	grad_norm 2.1076 (2.5731)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:23:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:12:40 lr 0.000010	 wd 0.0500	time 0.6090 (0.6329)	loss 1.4499 (1.1258)	grad_norm 1.9793 (2.5722)	loss_scale 2048.0000 (1053.9093)	mem 20188MB
[2024-07-30 03:24:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:11:36 lr 0.000010	 wd 0.0500	time 0.5902 (0.6324)	loss 1.4010 (1.1259)	grad_norm 1.6568 (2.5650)	loss_scale 2048.0000 (1124.8651)	mem 20188MB
[2024-07-30 03:25:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:10:33 lr 0.000010	 wd 0.0500	time 0.5954 (0.6319)	loss 1.2925 (1.1259)	grad_norm 2.8061 (2.5674)	loss_scale 2048.0000 (1186.3664)	mem 20188MB
[2024-07-30 03:26:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:09:29 lr 0.000010	 wd 0.0500	time 0.6267 (0.6315)	loss 1.3143 (1.1243)	grad_norm 1.9363 (2.5563)	loss_scale 2048.0000 (1240.1849)	mem 20188MB
[2024-07-30 03:27:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:08:26 lr 0.000010	 wd 0.0500	time 0.6002 (0.6312)	loss 1.5189 (1.1262)	grad_norm 1.7363 (2.5532)	loss_scale 2048.0000 (1287.6755)	mem 20188MB
[2024-07-30 03:28:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:07:22 lr 0.000010	 wd 0.0500	time 0.5953 (0.6309)	loss 0.9717 (1.1262)	grad_norm 1.9197 (2.5548)	loss_scale 2048.0000 (1329.8923)	mem 20188MB
[2024-07-30 03:29:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:06:19 lr 0.000010	 wd 0.0500	time 0.5983 (0.6306)	loss 1.1970 (1.1252)	grad_norm 2.3367 (2.5440)	loss_scale 2048.0000 (1367.6675)	mem 20188MB
[2024-07-30 03:30:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:05:16 lr 0.000010	 wd 0.0500	time 0.5920 (0.6304)	loss 0.9583 (1.1260)	grad_norm 2.6018 (2.5574)	loss_scale 2048.0000 (1401.6672)	mem 20188MB
[2024-07-30 03:31:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:04:13 lr 0.000010	 wd 0.0500	time 0.5970 (0.6301)	loss 1.4100 (1.1260)	grad_norm 2.3945 (2.5597)	loss_scale 2048.0000 (1432.4303)	mem 20188MB
[2024-07-30 03:32:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:03:10 lr 0.000010	 wd 0.0500	time 0.6258 (0.6299)	loss 1.4486 (1.1266)	grad_norm 1.6054 (2.5465)	loss_scale 2048.0000 (1460.3980)	mem 20188MB
[2024-07-30 03:33:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:02:07 lr 0.000010	 wd 0.0500	time 0.5919 (0.6297)	loss 0.8130 (1.1262)	grad_norm 2.5602 (2.5353)	loss_scale 2048.0000 (1485.9348)	mem 20188MB
[2024-07-30 03:34:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:04 lr 0.000010	 wd 0.0500	time 0.5921 (0.6295)	loss 0.8554 (1.1248)	grad_norm 1.9508 (nan)	loss_scale 1024.0000 (1479.4902)	mem 20188MB
[2024-07-30 03:35:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.5926 (0.6293)	loss 1.4643 (1.1248)	grad_norm 2.1358 (nan)	loss_scale 1024.0000 (1461.2779)	mem 20188MB
[2024-07-30 03:35:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 17 training takes 0:26:17
[2024-07-30 03:35:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.897 (10.897)	Loss 0.4609 (0.4609)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 20188MB
[2024-07-30 03:36:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.662 Acc@5 98.002
[2024-07-30 03:36:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 03:36:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.66%
[2024-07-30 03:36:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 03:36:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 03:36:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:04:23 lr 0.000009	 wd 0.0500	time 10.1773 (10.1773)	loss 1.4636 (1.4636)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:37:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:28:43 lr 0.000009	 wd 0.0500	time 0.5899 (0.7174)	loss 1.4690 (1.1342)	grad_norm 1.6831 (2.7013)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:38:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:25:44 lr 0.000009	 wd 0.0500	time 0.5948 (0.6708)	loss 0.9631 (1.1378)	grad_norm 3.1450 (2.5483)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:39:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:24:03 lr 0.000009	 wd 0.0500	time 0.5996 (0.6555)	loss 1.5457 (1.1393)	grad_norm 1.9529 (2.5993)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:40:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:22:41 lr 0.000009	 wd 0.0500	time 0.6117 (0.6478)	loss 0.7235 (1.1386)	grad_norm 18.6674 (2.5990)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:41:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:21:27 lr 0.000009	 wd 0.0500	time 0.5977 (0.6433)	loss 1.3392 (1.1350)	grad_norm 1.6353 (2.5551)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:42:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:20:17 lr 0.000009	 wd 0.0500	time 0.5968 (0.6404)	loss 1.4895 (1.1290)	grad_norm 2.2107 (2.5196)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:43:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:19:10 lr 0.000009	 wd 0.0500	time 0.5903 (0.6384)	loss 1.3574 (1.1316)	grad_norm 2.0171 (2.5072)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:44:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:18:04 lr 0.000009	 wd 0.0500	time 0.6076 (0.6370)	loss 1.3320 (1.1354)	grad_norm 2.0564 (2.6085)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:45:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:16:58 lr 0.000009	 wd 0.0500	time 0.6222 (0.6357)	loss 0.9295 (1.1335)	grad_norm 2.2176 (2.5816)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:46:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:15:53 lr 0.000009	 wd 0.0500	time 0.5971 (0.6348)	loss 1.3660 (1.1315)	grad_norm 2.1291 (2.6934)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:47:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:14:48 lr 0.000009	 wd 0.0500	time 0.5981 (0.6339)	loss 0.9967 (1.1328)	grad_norm 2.2750 (2.8834)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:48:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:13:44 lr 0.000009	 wd 0.0500	time 0.5933 (0.6332)	loss 0.9824 (1.1276)	grad_norm 1.9014 (2.8391)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:49:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:12:40 lr 0.000009	 wd 0.0500	time 0.5991 (0.6326)	loss 1.4721 (1.1274)	grad_norm 1.9446 (2.8590)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:51:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:11:36 lr 0.000009	 wd 0.0500	time 0.5905 (0.6320)	loss 1.2251 (1.1281)	grad_norm 3.8776 (2.8133)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:52:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:10:32 lr 0.000009	 wd 0.0500	time 0.6057 (0.6315)	loss 1.3290 (1.1283)	grad_norm 1.6906 (2.7804)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:53:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:09:29 lr 0.000009	 wd 0.0500	time 0.5982 (0.6311)	loss 1.3077 (1.1298)	grad_norm 3.7427 (2.7503)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:54:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:08:25 lr 0.000009	 wd 0.0500	time 0.5954 (0.6308)	loss 1.2677 (1.1310)	grad_norm 2.6314 (2.7475)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:55:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:07:22 lr 0.000009	 wd 0.0500	time 0.5909 (0.6305)	loss 0.7484 (1.1313)	grad_norm 3.6466 (2.7220)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:56:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:06:19 lr 0.000009	 wd 0.0500	time 0.5881 (0.6302)	loss 1.4710 (1.1313)	grad_norm 1.6129 (2.7163)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:57:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:05:16 lr 0.000008	 wd 0.0500	time 0.6051 (0.6300)	loss 0.9547 (1.1306)	grad_norm 2.5979 (2.7034)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:58:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:04:13 lr 0.000008	 wd 0.0500	time 0.5970 (0.6298)	loss 1.2586 (1.1303)	grad_norm 2.7050 (2.7008)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 03:59:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:03:10 lr 0.000008	 wd 0.0500	time 0.6253 (0.6297)	loss 1.0374 (1.1304)	grad_norm 2.0355 (2.7169)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:00:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:02:07 lr 0.000008	 wd 0.0500	time 0.6082 (0.6295)	loss 1.3180 (1.1313)	grad_norm 1.6529 (2.7107)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:01:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:04 lr 0.000008	 wd 0.0500	time 0.5918 (0.6294)	loss 1.3931 (1.1316)	grad_norm 1.9237 (2.6942)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:02:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.6242 (0.6292)	loss 1.1257 (1.1318)	grad_norm 1.7204 (2.6894)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:02:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 18 training takes 0:26:16
[2024-07-30 04:02:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.442 (10.442)	Loss 0.4741 (0.4741)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 20188MB
[2024-07-30 04:03:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.654 Acc@5 97.974
[2024-07-30 04:03:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 04:03:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.66%
[2024-07-30 04:03:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:14:51 lr 0.000008	 wd 0.0500	time 10.4282 (10.4282)	loss 0.8290 (0.8290)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:04:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:29:11 lr 0.000008	 wd 0.0500	time 0.5941 (0.7293)	loss 1.5430 (1.1706)	grad_norm 1.6091 (2.5518)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:05:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:25:57 lr 0.000008	 wd 0.0500	time 0.5952 (0.6765)	loss 0.8628 (1.1397)	grad_norm 1.9785 (2.4821)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:06:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:24:11 lr 0.000008	 wd 0.0500	time 0.5959 (0.6593)	loss 1.3780 (1.1291)	grad_norm 1.8731 (2.4444)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:07:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:22:47 lr 0.000008	 wd 0.0500	time 0.5907 (0.6507)	loss 0.9899 (1.1206)	grad_norm 2.1658 (2.5233)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:08:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:21:32 lr 0.000008	 wd 0.0500	time 0.5897 (0.6456)	loss 1.3660 (1.1285)	grad_norm 2.2042 (2.5007)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:09:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:20:21 lr 0.000008	 wd 0.0500	time 0.5978 (0.6422)	loss 0.8708 (1.1210)	grad_norm 3.7043 (2.5586)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:10:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:19:13 lr 0.000008	 wd 0.0500	time 0.5985 (0.6399)	loss 1.0776 (1.1200)	grad_norm 1.7531 (2.6011)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:11:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:18:06 lr 0.000008	 wd 0.0500	time 0.5982 (0.6381)	loss 1.3531 (1.1214)	grad_norm 2.5453 (2.5784)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:12:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:16:59 lr 0.000008	 wd 0.0500	time 0.5826 (0.6366)	loss 1.1479 (1.1225)	grad_norm 1.9874 (2.5662)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:13:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:15:54 lr 0.000008	 wd 0.0500	time 0.5941 (0.6355)	loss 1.3444 (1.1230)	grad_norm 2.5774 (2.5915)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:14:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:14:49 lr 0.000008	 wd 0.0500	time 0.6171 (0.6345)	loss 1.0931 (1.1212)	grad_norm 1.8008 (2.5769)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:15:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:13:45 lr 0.000008	 wd 0.0500	time 0.5941 (0.6338)	loss 1.3205 (1.1195)	grad_norm 2.0294 (2.5607)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:16:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:12:41 lr 0.000008	 wd 0.0500	time 0.5912 (0.6332)	loss 0.8980 (1.1191)	grad_norm 2.5195 (2.5569)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:17:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:11:37 lr 0.000008	 wd 0.0500	time 0.6004 (0.6325)	loss 0.8839 (1.1191)	grad_norm 2.8195 (2.5814)	loss_scale 2048.0000 (1078.0871)	mem 20188MB
[2024-07-30 04:18:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:10:33 lr 0.000008	 wd 0.0500	time 0.5934 (0.6321)	loss 1.4580 (1.1200)	grad_norm 3.6217 (2.5810)	loss_scale 2048.0000 (1142.7049)	mem 20188MB
[2024-07-30 04:19:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:09:29 lr 0.000007	 wd 0.0500	time 0.6058 (0.6318)	loss 1.1860 (1.1216)	grad_norm 2.5153 (2.5951)	loss_scale 2048.0000 (1199.2505)	mem 20188MB
[2024-07-30 04:20:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:08:26 lr 0.000007	 wd 0.0500	time 0.5944 (0.6314)	loss 0.7783 (1.1220)	grad_norm 1.6967 (inf)	loss_scale 1024.0000 (1216.6396)	mem 20188MB
[2024-07-30 04:22:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:07:22 lr 0.000007	 wd 0.0500	time 0.5987 (0.6310)	loss 1.2403 (1.1239)	grad_norm 1.8094 (inf)	loss_scale 1024.0000 (1205.9434)	mem 20188MB
[2024-07-30 04:23:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:06:19 lr 0.000007	 wd 0.0500	time 0.6023 (0.6308)	loss 1.1281 (1.1224)	grad_norm 1.9043 (inf)	loss_scale 1024.0000 (1196.3724)	mem 20188MB
[2024-07-30 04:24:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:05:16 lr 0.000007	 wd 0.0500	time 0.5984 (0.6305)	loss 1.2739 (1.1235)	grad_norm 2.2752 (inf)	loss_scale 1024.0000 (1187.7581)	mem 20188MB
[2024-07-30 04:25:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:04:13 lr 0.000007	 wd 0.0500	time 0.5872 (0.6302)	loss 1.5926 (1.1255)	grad_norm 1.9272 (inf)	loss_scale 1024.0000 (1179.9638)	mem 20188MB
[2024-07-30 04:26:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:03:10 lr 0.000007	 wd 0.0500	time 0.6139 (0.6300)	loss 1.2204 (1.1247)	grad_norm 2.7073 (inf)	loss_scale 1024.0000 (1172.8778)	mem 20188MB
[2024-07-30 04:27:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:02:07 lr 0.000007	 wd 0.0500	time 0.5938 (0.6299)	loss 0.9557 (1.1243)	grad_norm 7.0383 (inf)	loss_scale 1024.0000 (1166.4076)	mem 20188MB
[2024-07-30 04:28:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:04 lr 0.000007	 wd 0.0500	time 0.6004 (0.6298)	loss 1.0832 (1.1236)	grad_norm 4.0668 (inf)	loss_scale 1024.0000 (1160.4765)	mem 20188MB
[2024-07-30 04:29:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.5983 (0.6296)	loss 0.8028 (1.1236)	grad_norm 1.7045 (inf)	loss_scale 1024.0000 (1155.0196)	mem 20188MB
[2024-07-30 04:29:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 19 training takes 0:26:17
[2024-07-30 04:29:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 9.194 (9.194)	Loss 0.4575 (0.4575)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 20188MB
[2024-07-30 04:29:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.704 Acc@5 98.022
[2024-07-30 04:29:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 04:29:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.70%
[2024-07-30 04:29:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 04:29:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 04:30:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][0/2502]	eta 6:52:31 lr 0.000007	 wd 0.0500	time 9.8925 (9.8925)	loss 1.3187 (1.3187)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:31:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:28:43 lr 0.000007	 wd 0.0500	time 0.6167 (0.7173)	loss 0.9323 (1.1305)	grad_norm 2.3241 (2.4758)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:32:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:25:43 lr 0.000007	 wd 0.0500	time 0.5939 (0.6706)	loss 1.2849 (1.1193)	grad_norm 2.5687 (2.9552)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:33:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:24:02 lr 0.000007	 wd 0.0500	time 0.5941 (0.6553)	loss 0.7119 (1.1292)	grad_norm 2.0848 (2.6967)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:34:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:22:40 lr 0.000007	 wd 0.0500	time 0.5949 (0.6474)	loss 1.2613 (1.1263)	grad_norm 1.8448 (2.6315)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:35:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:21:28 lr 0.000007	 wd 0.0500	time 0.5881 (0.6434)	loss 1.2500 (1.1256)	grad_norm 2.1596 (2.8013)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:36:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:20:17 lr 0.000007	 wd 0.0500	time 0.5942 (0.6404)	loss 1.3278 (1.1296)	grad_norm 2.0224 (2.7327)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:37:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:19:10 lr 0.000007	 wd 0.0500	time 0.5932 (0.6383)	loss 1.2632 (1.1278)	grad_norm 1.9287 (2.6810)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:38:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:18:03 lr 0.000007	 wd 0.0500	time 0.5996 (0.6367)	loss 1.1742 (1.1297)	grad_norm 2.1517 (2.6786)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:39:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:16:57 lr 0.000007	 wd 0.0500	time 0.5833 (0.6355)	loss 1.3294 (1.1282)	grad_norm 2.4239 (2.6538)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:40:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:15:52 lr 0.000007	 wd 0.0500	time 0.5978 (0.6344)	loss 1.2190 (1.1297)	grad_norm 2.0048 (2.6660)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:41:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:14:48 lr 0.000007	 wd 0.0500	time 0.5970 (0.6336)	loss 0.9084 (1.1277)	grad_norm 2.1071 (2.6240)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:42:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:13:44 lr 0.000006	 wd 0.0500	time 0.5959 (0.6329)	loss 1.2062 (1.1248)	grad_norm 2.5593 (2.6121)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:43:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:12:40 lr 0.000006	 wd 0.0500	time 0.6189 (0.6323)	loss 1.3478 (1.1246)	grad_norm 1.7995 (2.6385)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:44:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:11:36 lr 0.000006	 wd 0.0500	time 0.5962 (0.6319)	loss 1.0476 (1.1267)	grad_norm 2.6786 (2.6172)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:45:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:10:32 lr 0.000006	 wd 0.0500	time 0.5932 (0.6314)	loss 1.3539 (1.1282)	grad_norm 2.1243 (2.6249)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:46:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:09:29 lr 0.000006	 wd 0.0500	time 0.5946 (0.6311)	loss 1.1859 (1.1299)	grad_norm 4.5579 (2.6208)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:47:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:08:25 lr 0.000006	 wd 0.0500	time 0.5917 (0.6308)	loss 1.5037 (1.1305)	grad_norm 2.9609 (2.6265)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:48:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:07:22 lr 0.000006	 wd 0.0500	time 0.5930 (0.6305)	loss 0.7834 (1.1284)	grad_norm 3.1095 (2.6238)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:49:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:06:19 lr 0.000006	 wd 0.0500	time 0.5961 (0.6302)	loss 0.7758 (1.1289)	grad_norm 2.5909 (2.6212)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:50:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:05:16 lr 0.000006	 wd 0.0500	time 0.5890 (0.6300)	loss 1.0661 (1.1284)	grad_norm 1.9365 (2.6329)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:51:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:04:13 lr 0.000006	 wd 0.0500	time 0.5911 (0.6298)	loss 1.2879 (1.1287)	grad_norm 1.5780 (2.6410)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:53:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:03:10 lr 0.000006	 wd 0.0500	time 0.5894 (0.6296)	loss 1.3138 (1.1301)	grad_norm 2.7297 (2.6467)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:54:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:02:07 lr 0.000006	 wd 0.0500	time 0.5966 (0.6294)	loss 0.8080 (1.1315)	grad_norm 1.9731 (2.6318)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:55:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:04 lr 0.000006	 wd 0.0500	time 0.5958 (0.6292)	loss 0.7048 (1.1310)	grad_norm 2.0000 (2.6291)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:56:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.5959 (0.6291)	loss 1.1113 (1.1329)	grad_norm 2.2597 (2.6396)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:56:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 20 training takes 0:26:16
[2024-07-30 04:56:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.309 (10.309)	Loss 0.4668 (0.4668)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20188MB
[2024-07-30 04:56:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.686 Acc@5 98.018
[2024-07-30 04:56:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 04:56:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.70%
[2024-07-30 04:56:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:19:24 lr 0.000006	 wd 0.0500	time 10.5372 (10.5372)	loss 0.8851 (0.8851)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:57:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:29:07 lr 0.000006	 wd 0.0500	time 0.5900 (0.7277)	loss 1.3153 (1.1338)	grad_norm 2.2106 (2.4914)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 04:58:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:25:55 lr 0.000006	 wd 0.0500	time 0.5973 (0.6757)	loss 1.2750 (1.1367)	grad_norm 1.9577 (2.3908)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 05:00:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:24:10 lr 0.000006	 wd 0.0500	time 0.5999 (0.6587)	loss 1.0848 (1.1325)	grad_norm 3.6191 (2.4785)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 05:01:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:22:46 lr 0.000006	 wd 0.0500	time 0.5974 (0.6502)	loss 1.4375 (1.1235)	grad_norm 2.2783 (2.4466)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 05:02:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:21:31 lr 0.000006	 wd 0.0500	time 0.5876 (0.6452)	loss 0.8105 (1.1190)	grad_norm 2.1158 (2.4382)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 05:03:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:20:21 lr 0.000006	 wd 0.0500	time 0.5983 (0.6420)	loss 1.2865 (1.1148)	grad_norm 2.2165 (2.4877)	loss_scale 1024.0000 (1024.0000)	mem 20188MB
[2024-07-30 05:04:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:19:12 lr 0.000006	 wd 0.0500	time 0.5970 (0.6398)	loss 1.2367 (1.1169)	grad_norm 1.9868 (2.5401)	loss_scale 2048.0000 (1108.7247)	mem 20188MB
[2024-07-30 05:05:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:18:06 lr 0.000006	 wd 0.0500	time 0.6168 (0.6381)	loss 1.2575 (1.1158)	grad_norm 2.6102 (2.5454)	loss_scale 2048.0000 (1225.9875)	mem 20188MB
[2024-07-30 05:06:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:16:59 lr 0.000005	 wd 0.0500	time 0.5903 (0.6366)	loss 1.3225 (1.1196)	grad_norm 2.1273 (2.5497)	loss_scale 2048.0000 (1317.2209)	mem 20188MB
[2024-07-30 05:07:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:15:54 lr 0.000005	 wd 0.0500	time 0.5973 (0.6355)	loss 1.3417 (1.1229)	grad_norm 1.5283 (2.5399)	loss_scale 2048.0000 (1390.2258)	mem 20188MB
[2024-07-30 05:08:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:14:49 lr 0.000005	 wd 0.0500	time 0.6040 (0.6345)	loss 0.8228 (1.1265)	grad_norm 1.8803 (2.5310)	loss_scale 2048.0000 (1449.9691)	mem 20188MB
[2024-07-30 05:09:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:13:45 lr 0.000005	 wd 0.0500	time 0.5922 (0.6338)	loss 1.2532 (1.1230)	grad_norm 2.3125 (2.5622)	loss_scale 2048.0000 (1499.7635)	mem 20188MB
[2024-07-30 05:10:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:12:40 lr 0.000005	 wd 0.0500	time 0.5968 (0.6330)	loss 1.2792 (1.1230)	grad_norm 1.5798 (2.5585)	loss_scale 2048.0000 (1541.9032)	mem 20188MB
[2024-07-30 05:11:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:11:37 lr 0.000005	 wd 0.0500	time 0.5968 (0.6325)	loss 0.7933 (1.1209)	grad_norm 2.8538 (2.5967)	loss_scale 2048.0000 (1578.0271)	mem 20188MB
[2024-07-30 05:12:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:10:33 lr 0.000005	 wd 0.0500	time 0.5935 (0.6320)	loss 0.7865 (1.1223)	grad_norm 2.1905 (2.5936)	loss_scale 2048.0000 (1609.3378)	mem 20188MB
[2024-07-30 05:13:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:09:29 lr 0.000005	 wd 0.0500	time 0.6084 (0.6317)	loss 1.1052 (1.1190)	grad_norm 2.3302 (2.5972)	loss_scale 2048.0000 (1636.7370)	mem 20188MB
[2024-07-30 05:14:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:08:26 lr 0.000005	 wd 0.0500	time 0.5980 (0.6314)	loss 1.3366 (1.1189)	grad_norm 1.9592 (2.5796)	loss_scale 2048.0000 (1660.9148)	mem 20188MB
[2024-07-30 05:15:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:07:23 lr 0.000005	 wd 0.0500	time 0.5957 (0.6311)	loss 0.8199 (1.1192)	grad_norm 2.7591 (2.5638)	loss_scale 2048.0000 (1682.4076)	mem 20188MB
[2024-07-30 05:16:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:06:19 lr 0.000005	 wd 0.0500	time 0.5850 (0.6307)	loss 0.7121 (1.1207)	grad_norm 2.7259 (2.5517)	loss_scale 2048.0000 (1701.6391)	mem 20188MB
[2024-07-30 05:17:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:05:16 lr 0.000005	 wd 0.0500	time 0.5936 (0.6305)	loss 1.1132 (1.1202)	grad_norm 20.9825 (2.5657)	loss_scale 2048.0000 (1718.9485)	mem 20188MB
[2024-07-30 05:18:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:04:13 lr 0.000005	 wd 0.0500	time 0.5880 (0.6302)	loss 1.2175 (1.1197)	grad_norm 1.9759 (2.5582)	loss_scale 2048.0000 (1734.6102)	mem 20188MB
[2024-07-30 05:19:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:03:10 lr 0.000005	 wd 0.0500	time 0.5861 (0.6299)	loss 1.3720 (1.1179)	grad_norm 2.3788 (2.5935)	loss_scale 2048.0000 (1748.8487)	mem 20188MB
[2024-07-30 05:20:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:02:07 lr 0.000005	 wd 0.0500	time 0.5913 (0.6297)	loss 1.3836 (1.1201)	grad_norm 2.3183 (2.5874)	loss_scale 2048.0000 (1761.8496)	mem 20188MB
[2024-07-30 05:21:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:04 lr 0.000005	 wd 0.0500	time 0.5899 (0.6296)	loss 1.0745 (1.1209)	grad_norm 1.7247 (2.5801)	loss_scale 2048.0000 (1773.7676)	mem 20188MB
[2024-07-30 05:22:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.5884 (0.6293)	loss 1.4141 (1.1205)	grad_norm 2.1444 (2.5780)	loss_scale 2048.0000 (1784.7325)	mem 20188MB
[2024-07-30 05:22:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 21 training takes 0:26:17
[2024-07-30 05:23:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.625 (10.625)	Loss 0.4810 (0.4810)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20188MB
[2024-07-30 05:23:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.668 Acc@5 98.006
[2024-07-30 05:23:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 05:23:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.70%
[2024-07-30 05:23:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:29:28 lr 0.000005	 wd 0.0500	time 10.7788 (10.7788)	loss 1.0271 (1.0271)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:24:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:29:05 lr 0.000005	 wd 0.0500	time 0.5881 (0.7265)	loss 1.2844 (1.1316)	grad_norm 5.2814 (2.4066)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:25:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:25:55 lr 0.000005	 wd 0.0500	time 0.5970 (0.6758)	loss 0.6609 (1.1420)	grad_norm 3.3356 (2.3930)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:26:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:24:10 lr 0.000005	 wd 0.0500	time 0.5865 (0.6587)	loss 1.4922 (1.1435)	grad_norm 2.2396 (2.5589)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:27:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:22:46 lr 0.000005	 wd 0.0500	time 0.6269 (0.6502)	loss 0.7754 (1.1435)	grad_norm 1.6525 (2.5702)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:28:54 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:21:31 lr 0.000005	 wd 0.0500	time 0.5886 (0.6453)	loss 1.0708 (1.1397)	grad_norm 2.1969 (2.5670)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:29:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:20:20 lr 0.000005	 wd 0.0500	time 0.6004 (0.6419)	loss 0.8397 (1.1390)	grad_norm 1.6478 (2.5740)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:30:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:19:12 lr 0.000005	 wd 0.0500	time 0.5948 (0.6396)	loss 1.3785 (1.1329)	grad_norm 1.8796 (2.5839)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:32:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:18:05 lr 0.000004	 wd 0.0500	time 0.5892 (0.6378)	loss 1.0303 (1.1330)	grad_norm 1.8368 (2.5566)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:33:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:16:59 lr 0.000004	 wd 0.0500	time 0.5910 (0.6364)	loss 1.0999 (1.1330)	grad_norm 2.1045 (2.6171)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:34:07 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:15:54 lr 0.000004	 wd 0.0500	time 0.5943 (0.6354)	loss 1.3442 (1.1333)	grad_norm 1.6224 (2.5943)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:35:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:14:49 lr 0.000004	 wd 0.0500	time 0.5942 (0.6346)	loss 0.7783 (1.1293)	grad_norm 1.7950 (2.5834)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:36:12 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:13:45 lr 0.000004	 wd 0.0500	time 0.5932 (0.6338)	loss 1.2573 (1.1300)	grad_norm 2.1562 (2.5901)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:37:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:12:41 lr 0.000004	 wd 0.0500	time 0.6181 (0.6331)	loss 0.8176 (1.1240)	grad_norm 1.7120 (2.5819)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:38:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:11:37 lr 0.000004	 wd 0.0500	time 0.5881 (0.6325)	loss 0.9908 (1.1193)	grad_norm 4.4234 (2.6010)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:39:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:10:33 lr 0.000004	 wd 0.0500	time 0.5921 (0.6321)	loss 1.1062 (1.1189)	grad_norm 2.1114 (2.5828)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:40:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:09:29 lr 0.000004	 wd 0.0500	time 0.5921 (0.6316)	loss 1.3402 (1.1185)	grad_norm 1.9427 (2.5739)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:41:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:08:26 lr 0.000004	 wd 0.0500	time 0.5868 (0.6312)	loss 0.9164 (1.1165)	grad_norm 1.9422 (2.5661)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:42:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:07:22 lr 0.000004	 wd 0.0500	time 0.5908 (0.6309)	loss 1.1845 (1.1152)	grad_norm 1.8164 (2.5662)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:43:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:06:19 lr 0.000004	 wd 0.0500	time 0.5867 (0.6306)	loss 0.9456 (1.1162)	grad_norm 1.7120 (2.5648)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:44:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:05:16 lr 0.000004	 wd 0.0500	time 0.6057 (0.6304)	loss 0.9091 (1.1173)	grad_norm 1.5512 (2.5557)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:45:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:04:13 lr 0.000004	 wd 0.0500	time 0.5931 (0.6301)	loss 1.2344 (1.1184)	grad_norm 2.3798 (2.6002)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 05:46:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:03:10 lr 0.000004	 wd 0.0500	time 0.6046 (0.6299)	loss 1.0766 (1.1182)	grad_norm 1.9582 (2.6369)	loss_scale 4096.0000 (2103.8292)	mem 20188MB
[2024-07-30 05:47:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:02:07 lr 0.000004	 wd 0.0500	time 0.5990 (0.6297)	loss 1.1888 (1.1186)	grad_norm 2.3280 (2.6595)	loss_scale 4096.0000 (2190.4076)	mem 20188MB
[2024-07-30 05:48:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:04 lr 0.000004	 wd 0.0500	time 0.5952 (0.6296)	loss 1.4000 (1.1202)	grad_norm 1.7271 (2.6457)	loss_scale 4096.0000 (2269.7743)	mem 20188MB
[2024-07-30 05:49:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.5786 (0.6295)	loss 0.8835 (1.1205)	grad_norm 1.8042 (2.6364)	loss_scale 4096.0000 (2342.7941)	mem 20188MB
[2024-07-30 05:49:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 22 training takes 0:26:17
[2024-07-30 05:49:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.212 (10.212)	Loss 0.4670 (0.4670)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 20188MB
[2024-07-30 05:50:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.732 Acc@5 98.032
[2024-07-30 05:50:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 05:50:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.73%
[2024-07-30 05:50:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 05:50:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 05:50:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][0/2502]	eta 6:42:10 lr 0.000004	 wd 0.0500	time 9.6443 (9.6443)	loss 0.7641 (0.7641)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:51:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:28:35 lr 0.000004	 wd 0.0500	time 0.6198 (0.7143)	loss 1.3146 (1.1068)	grad_norm 2.3508 (2.5964)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:52:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:25:42 lr 0.000004	 wd 0.0500	time 0.6098 (0.6699)	loss 0.8887 (1.1157)	grad_norm 4.1902 (2.6626)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:53:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:24:02 lr 0.000004	 wd 0.0500	time 0.5774 (0.6552)	loss 1.3117 (1.1052)	grad_norm 1.8838 (2.6387)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:54:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:22:41 lr 0.000004	 wd 0.0500	time 0.5941 (0.6476)	loss 0.8945 (1.1123)	grad_norm 1.9032 (2.5599)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:55:44 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:21:27 lr 0.000004	 wd 0.0500	time 0.5939 (0.6431)	loss 0.8893 (1.1127)	grad_norm 3.1301 (2.5689)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:56:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:20:17 lr 0.000004	 wd 0.0500	time 0.5995 (0.6402)	loss 1.3171 (1.1120)	grad_norm 6.5909 (2.5953)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:57:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:19:09 lr 0.000004	 wd 0.0500	time 0.5796 (0.6380)	loss 1.2233 (1.1142)	grad_norm 2.7086 (2.5374)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:58:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:18:03 lr 0.000003	 wd 0.0500	time 0.5987 (0.6366)	loss 1.3304 (1.1167)	grad_norm 4.1692 (2.5787)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 05:59:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:16:57 lr 0.000003	 wd 0.0500	time 0.6105 (0.6354)	loss 1.1078 (1.1175)	grad_norm 1.3780 (2.6006)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:00:57 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:15:52 lr 0.000003	 wd 0.0500	time 0.5982 (0.6344)	loss 1.1517 (1.1137)	grad_norm 2.0992 (2.6100)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:02:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:14:48 lr 0.000003	 wd 0.0500	time 0.6237 (0.6336)	loss 0.9859 (1.1158)	grad_norm 2.8283 (2.5992)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:03:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:13:44 lr 0.000003	 wd 0.0500	time 0.6331 (0.6331)	loss 1.3118 (1.1182)	grad_norm 2.4439 (2.6534)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:04:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:12:40 lr 0.000003	 wd 0.0500	time 0.5852 (0.6324)	loss 1.2182 (1.1192)	grad_norm 2.2788 (2.6201)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:05:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:11:36 lr 0.000003	 wd 0.0500	time 0.5873 (0.6320)	loss 1.3471 (1.1192)	grad_norm 2.3427 (2.6165)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:06:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:10:32 lr 0.000003	 wd 0.0500	time 0.5957 (0.6316)	loss 1.3310 (1.1177)	grad_norm 3.1762 (2.6031)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:07:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:09:29 lr 0.000003	 wd 0.0500	time 0.5959 (0.6312)	loss 1.1585 (1.1204)	grad_norm 4.9318 (2.5868)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:08:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:08:26 lr 0.000003	 wd 0.0500	time 0.5990 (0.6309)	loss 0.7147 (1.1178)	grad_norm 2.2119 (2.5747)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:09:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:07:22 lr 0.000003	 wd 0.0500	time 0.5920 (0.6307)	loss 1.3492 (1.1173)	grad_norm 2.0023 (2.5622)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:10:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:06:19 lr 0.000003	 wd 0.0500	time 0.5909 (0.6304)	loss 1.3098 (1.1167)	grad_norm 2.4292 (2.5714)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:11:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:05:16 lr 0.000003	 wd 0.0500	time 0.6015 (0.6302)	loss 0.8155 (1.1158)	grad_norm 2.4799 (2.5791)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:12:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:04:13 lr 0.000003	 wd 0.0500	time 0.6161 (0.6299)	loss 1.2435 (1.1166)	grad_norm 2.4595 (2.5741)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:13:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:03:10 lr 0.000003	 wd 0.0500	time 0.6082 (0.6298)	loss 1.2935 (1.1155)	grad_norm 1.9806 (2.5602)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:14:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:02:07 lr 0.000003	 wd 0.0500	time 0.6160 (0.6296)	loss 1.4153 (1.1151)	grad_norm 1.7020 (2.5555)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:15:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:04 lr 0.000003	 wd 0.0500	time 0.6005 (0.6294)	loss 0.9732 (1.1149)	grad_norm 2.4748 (2.5563)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:16:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.5776 (0.6293)	loss 0.8393 (1.1137)	grad_norm 2.7268 (2.5540)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:16:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 23 training takes 0:26:18
[2024-07-30 06:16:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.429 (10.429)	Loss 0.4812 (0.4812)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20188MB
[2024-07-30 06:17:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.700 Acc@5 98.018
[2024-07-30 06:17:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 06:17:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.73%
[2024-07-30 06:17:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:31:42 lr 0.000003	 wd 0.0500	time 10.8323 (10.8323)	loss 1.2355 (1.2355)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:18:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:29:02 lr 0.000003	 wd 0.0500	time 0.5995 (0.7252)	loss 0.9243 (1.1285)	grad_norm 5.6956 (2.4808)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:19:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:25:53 lr 0.000003	 wd 0.0500	time 0.6227 (0.6750)	loss 1.1408 (1.1199)	grad_norm 3.1675 (2.5359)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:20:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:24:09 lr 0.000003	 wd 0.0500	time 0.5879 (0.6582)	loss 1.3269 (1.1305)	grad_norm 2.6809 (2.6040)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:21:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:22:46 lr 0.000003	 wd 0.0500	time 0.5899 (0.6500)	loss 0.6668 (1.1281)	grad_norm 1.9214 (2.5797)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:22:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:21:31 lr 0.000003	 wd 0.0500	time 0.6137 (0.6449)	loss 1.2703 (1.1288)	grad_norm 1.8078 (2.5699)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:23:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:20:20 lr 0.000003	 wd 0.0500	time 0.6089 (0.6418)	loss 0.8368 (1.1246)	grad_norm 3.1823 (2.6466)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:24:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:19:12 lr 0.000003	 wd 0.0500	time 0.5956 (0.6394)	loss 1.2808 (1.1231)	grad_norm 1.9365 (2.6318)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:25:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:18:05 lr 0.000003	 wd 0.0500	time 0.6013 (0.6378)	loss 1.2343 (1.1241)	grad_norm 1.8624 (2.5871)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:26:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:16:59 lr 0.000003	 wd 0.0500	time 0.5973 (0.6365)	loss 0.9125 (1.1288)	grad_norm 1.7522 (2.5558)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:27:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:15:54 lr 0.000003	 wd 0.0500	time 0.5947 (0.6354)	loss 0.8503 (1.1263)	grad_norm 1.8918 (2.6696)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:28:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:14:49 lr 0.000003	 wd 0.0500	time 0.5966 (0.6344)	loss 0.8506 (1.1247)	grad_norm 2.3678 (2.6619)	loss_scale 4096.0000 (4096.0000)	mem 20188MB
[2024-07-30 06:29:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:13:45 lr 0.000002	 wd 0.0500	time 0.5985 (0.6337)	loss 0.9399 (1.1222)	grad_norm 2.2256 (2.6309)	loss_scale 8192.0000 (4314.2714)	mem 20188MB
[2024-07-30 06:30:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:12:40 lr 0.000002	 wd 0.0500	time 0.5783 (0.6331)	loss 0.8487 (1.1201)	grad_norm 2.4277 (2.6240)	loss_scale 8192.0000 (4612.3290)	mem 20188MB
[2024-07-30 06:32:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:11:36 lr 0.000002	 wd 0.0500	time 0.5949 (0.6325)	loss 1.4292 (1.1214)	grad_norm 2.2214 (2.6152)	loss_scale 8192.0000 (4867.8373)	mem 20188MB
[2024-07-30 06:33:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:10:33 lr 0.000002	 wd 0.0500	time 0.5981 (0.6320)	loss 1.1954 (1.1180)	grad_norm 2.1623 (nan)	loss_scale 4096.0000 (4827.3311)	mem 20188MB
[2024-07-30 06:34:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:09:29 lr 0.000002	 wd 0.0500	time 0.6014 (0.6315)	loss 1.2612 (1.1168)	grad_norm 2.4158 (nan)	loss_scale 4096.0000 (4781.6515)	mem 20188MB
[2024-07-30 06:35:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:08:26 lr 0.000002	 wd 0.0500	time 0.5956 (0.6311)	loss 1.5456 (1.1182)	grad_norm 2.3658 (nan)	loss_scale 4096.0000 (4741.3427)	mem 20188MB
[2024-07-30 06:36:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:07:22 lr 0.000002	 wd 0.0500	time 0.6126 (0.6309)	loss 1.1515 (1.1186)	grad_norm 4.7364 (nan)	loss_scale 4096.0000 (4705.5103)	mem 20188MB
[2024-07-30 06:37:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:06:19 lr 0.000002	 wd 0.0500	time 0.6138 (0.6306)	loss 0.8227 (1.1192)	grad_norm 2.6142 (nan)	loss_scale 4096.0000 (4673.4477)	mem 20188MB
[2024-07-30 06:38:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:05:16 lr 0.000002	 wd 0.0500	time 0.5842 (0.6304)	loss 1.2317 (1.1188)	grad_norm 1.9670 (nan)	loss_scale 4096.0000 (4644.5897)	mem 20188MB
[2024-07-30 06:39:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:04:13 lr 0.000002	 wd 0.0500	time 0.5964 (0.6302)	loss 0.9875 (1.1184)	grad_norm 2.0134 (nan)	loss_scale 2048.0000 (4606.7815)	mem 20188MB
[2024-07-30 06:40:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:03:10 lr 0.000002	 wd 0.0500	time 0.5962 (0.6300)	loss 0.9073 (1.1200)	grad_norm 2.2626 (nan)	loss_scale 2048.0000 (4490.5261)	mem 20188MB
[2024-07-30 06:41:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:02:07 lr 0.000002	 wd 0.0500	time 0.5913 (0.6298)	loss 1.2831 (1.1210)	grad_norm 2.8599 (nan)	loss_scale 2048.0000 (4384.3755)	mem 20188MB
[2024-07-30 06:42:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:04 lr 0.000002	 wd 0.0500	time 0.5942 (0.6296)	loss 1.3820 (1.1187)	grad_norm 2.4924 (nan)	loss_scale 2048.0000 (4287.0671)	mem 20188MB
[2024-07-30 06:43:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.6251 (0.6295)	loss 0.7622 (1.1189)	grad_norm 2.0639 (nan)	loss_scale 2048.0000 (4197.5402)	mem 20188MB
[2024-07-30 06:43:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 24 training takes 0:26:20
[2024-07-30 06:43:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 9.233 (9.233)	Loss 0.4670 (0.4670)	Acc@1 92.188 (92.188)	Acc@5 99.023 (99.023)	Mem 20188MB
[2024-07-30 06:44:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.736 Acc@5 98.028
[2024-07-30 06:44:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 06:44:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.74%
[2024-07-30 06:44:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saving......
[2024-07-30 06:44:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_best.pth saved !!!
[2024-07-30 06:44:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:07:30 lr 0.000002	 wd 0.0500	time 10.2520 (10.2520)	loss 1.2953 (1.2953)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:45:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:28:46 lr 0.000002	 wd 0.0500	time 0.5952 (0.7187)	loss 1.1785 (1.1831)	grad_norm 2.4201 (2.3844)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:46:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:25:46 lr 0.000002	 wd 0.0500	time 0.6004 (0.6720)	loss 1.4181 (1.1511)	grad_norm 1.5589 (3.1625)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:47:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:24:04 lr 0.000002	 wd 0.0500	time 0.5987 (0.6561)	loss 1.1639 (1.1346)	grad_norm 1.6136 (2.8582)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:48:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:22:42 lr 0.000002	 wd 0.0500	time 0.6149 (0.6481)	loss 1.1552 (1.1316)	grad_norm 2.0148 (2.9350)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:49:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:21:29 lr 0.000002	 wd 0.0500	time 0.5967 (0.6439)	loss 1.2679 (1.1292)	grad_norm 1.8639 (2.8065)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:50:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:20:18 lr 0.000002	 wd 0.0500	time 0.5898 (0.6406)	loss 0.8930 (1.1202)	grad_norm 2.0644 (2.7782)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:51:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:19:10 lr 0.000002	 wd 0.0500	time 0.5911 (0.6384)	loss 0.7565 (1.1218)	grad_norm 1.9423 (2.7098)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:52:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:18:03 lr 0.000002	 wd 0.0500	time 0.5969 (0.6367)	loss 1.3564 (1.1199)	grad_norm 2.8989 (2.6431)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:53:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:16:57 lr 0.000002	 wd 0.0500	time 0.5923 (0.6354)	loss 1.2450 (1.1200)	grad_norm 2.2926 (2.6887)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:54:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:15:52 lr 0.000002	 wd 0.0500	time 0.5938 (0.6344)	loss 1.5588 (1.1207)	grad_norm 2.0917 (2.6847)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:55:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:14:48 lr 0.000002	 wd 0.0500	time 0.5971 (0.6337)	loss 1.4215 (1.1227)	grad_norm 3.0422 (2.6450)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:56:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:13:44 lr 0.000002	 wd 0.0500	time 0.5947 (0.6330)	loss 0.9801 (1.1195)	grad_norm 2.4177 (2.6458)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:57:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:12:40 lr 0.000002	 wd 0.0500	time 0.5987 (0.6324)	loss 0.8974 (1.1189)	grad_norm 2.6558 (2.6378)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:58:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:11:36 lr 0.000002	 wd 0.0500	time 0.6055 (0.6319)	loss 0.9852 (1.1187)	grad_norm 2.7517 (2.6218)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 06:59:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:10:32 lr 0.000002	 wd 0.0500	time 0.6111 (0.6314)	loss 0.7718 (1.1181)	grad_norm 2.5261 (2.5943)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:01:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:09:29 lr 0.000002	 wd 0.0500	time 0.5955 (0.6310)	loss 0.8288 (1.1172)	grad_norm 2.1577 (2.5902)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:02:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:08:25 lr 0.000002	 wd 0.0500	time 0.6140 (0.6306)	loss 0.7327 (1.1149)	grad_norm 2.2168 (2.5782)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:03:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:07:22 lr 0.000002	 wd 0.0500	time 0.5965 (0.6304)	loss 0.7946 (1.1166)	grad_norm 2.4840 (2.5816)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:04:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:06:19 lr 0.000002	 wd 0.0500	time 0.5947 (0.6301)	loss 1.4372 (1.1184)	grad_norm 2.0606 (2.5862)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:05:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:05:16 lr 0.000002	 wd 0.0500	time 0.5969 (0.6299)	loss 0.8934 (1.1185)	grad_norm 1.6737 (2.5770)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:06:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:04:13 lr 0.000002	 wd 0.0500	time 0.6044 (0.6296)	loss 1.2597 (1.1194)	grad_norm 1.9758 (2.6207)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:07:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:03:10 lr 0.000001	 wd 0.0500	time 0.5969 (0.6295)	loss 1.0592 (1.1197)	grad_norm 8.6118 (2.6073)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:08:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:02:07 lr 0.000001	 wd 0.0500	time 0.5991 (0.6293)	loss 1.3201 (1.1195)	grad_norm 2.0336 (2.5958)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:09:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:01:04 lr 0.000001	 wd 0.0500	time 0.5950 (0.6292)	loss 1.4705 (1.1190)	grad_norm 1.5562 (2.6038)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:10:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5967 (0.6290)	loss 1.0005 (1.1177)	grad_norm 3.9428 (2.5951)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:10:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 25 training takes 0:26:20
[2024-07-30 07:10:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.756 (10.756)	Loss 0.4871 (0.4871)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 20188MB
[2024-07-30 07:11:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.718 Acc@5 98.010
[2024-07-30 07:11:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 07:11:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.74%
[2024-07-30 07:11:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:34:14 lr 0.000001	 wd 0.0500	time 10.8931 (10.8931)	loss 1.0535 (1.0535)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:12:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:29:04 lr 0.000001	 wd 0.0500	time 0.5974 (0.7264)	loss 1.2760 (1.1271)	grad_norm 1.8904 (2.7724)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:13:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:25:53 lr 0.000001	 wd 0.0500	time 0.5944 (0.6751)	loss 1.0102 (1.1227)	grad_norm 2.3434 (2.5496)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:14:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:24:09 lr 0.000001	 wd 0.0500	time 0.5965 (0.6583)	loss 0.8903 (1.1105)	grad_norm 1.8987 (2.4989)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:15:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:22:45 lr 0.000001	 wd 0.0500	time 0.6058 (0.6497)	loss 1.4153 (1.1100)	grad_norm 3.5644 (2.5475)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:16:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:21:30 lr 0.000001	 wd 0.0500	time 0.5894 (0.6448)	loss 1.3129 (1.1184)	grad_norm 3.3080 (2.5414)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:17:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:20:20 lr 0.000001	 wd 0.0500	time 0.5986 (0.6416)	loss 0.7809 (1.1210)	grad_norm 1.5626 (2.4987)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:18:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:19:11 lr 0.000001	 wd 0.0500	time 0.6031 (0.6391)	loss 1.1795 (1.1167)	grad_norm 1.4936 (2.4857)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:19:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:18:04 lr 0.000001	 wd 0.0500	time 0.5856 (0.6374)	loss 1.2680 (1.1211)	grad_norm 2.0094 (2.4972)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:20:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:16:58 lr 0.000001	 wd 0.0500	time 0.5983 (0.6361)	loss 1.1830 (1.1214)	grad_norm 4.5123 (2.4915)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:21:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:15:53 lr 0.000001	 wd 0.0500	time 0.5887 (0.6351)	loss 0.9753 (1.1205)	grad_norm 2.4570 (2.4977)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:22:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:14:49 lr 0.000001	 wd 0.0500	time 0.5894 (0.6342)	loss 0.8168 (1.1193)	grad_norm 1.5836 (2.4875)	loss_scale 4096.0000 (2077.7620)	mem 20188MB
[2024-07-30 07:23:48 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:13:44 lr 0.000001	 wd 0.0500	time 0.5983 (0.6334)	loss 0.9274 (1.1198)	grad_norm 1.6958 (2.4658)	loss_scale 4096.0000 (2245.8085)	mem 20188MB
[2024-07-30 07:24:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:12:40 lr 0.000001	 wd 0.0500	time 0.5982 (0.6328)	loss 1.2029 (1.1218)	grad_norm 2.1086 (2.4982)	loss_scale 4096.0000 (2388.0215)	mem 20188MB
[2024-07-30 07:25:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:11:36 lr 0.000001	 wd 0.0500	time 0.5826 (0.6322)	loss 1.4007 (1.1236)	grad_norm 7.7021 (nan)	loss_scale 2048.0000 (2366.6752)	mem 20188MB
[2024-07-30 07:26:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:10:33 lr 0.000001	 wd 0.0500	time 0.6021 (0.6319)	loss 1.3159 (1.1253)	grad_norm 1.9231 (nan)	loss_scale 2048.0000 (2345.4444)	mem 20188MB
[2024-07-30 07:27:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:09:29 lr 0.000001	 wd 0.0500	time 0.5944 (0.6314)	loss 1.1228 (1.1240)	grad_norm 2.5421 (nan)	loss_scale 2048.0000 (2326.8657)	mem 20188MB
[2024-07-30 07:29:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:08:26 lr 0.000001	 wd 0.0500	time 0.5966 (0.6310)	loss 1.0995 (1.1208)	grad_norm 1.7366 (nan)	loss_scale 2048.0000 (2310.4715)	mem 20188MB
[2024-07-30 07:30:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:07:22 lr 0.000001	 wd 0.0500	time 0.5913 (0.6307)	loss 0.7525 (1.1199)	grad_norm 2.3437 (nan)	loss_scale 2048.0000 (2295.8978)	mem 20188MB
[2024-07-30 07:31:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:06:19 lr 0.000001	 wd 0.0500	time 0.5887 (0.6304)	loss 1.3019 (1.1215)	grad_norm 1.7752 (nan)	loss_scale 2048.0000 (2282.8574)	mem 20188MB
[2024-07-30 07:32:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:05:16 lr 0.000001	 wd 0.0500	time 0.6010 (0.6302)	loss 1.1344 (1.1203)	grad_norm 1.7813 (nan)	loss_scale 2048.0000 (2271.1204)	mem 20188MB
[2024-07-30 07:33:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:04:13 lr 0.000001	 wd 0.0500	time 0.5981 (0.6300)	loss 1.3039 (1.1198)	grad_norm 5.9341 (nan)	loss_scale 2048.0000 (2260.5007)	mem 20188MB
[2024-07-30 07:34:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:03:10 lr 0.000001	 wd 0.0500	time 0.5878 (0.6298)	loss 1.2484 (1.1181)	grad_norm 2.2680 (nan)	loss_scale 2048.0000 (2250.8460)	mem 20188MB
[2024-07-30 07:35:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:02:07 lr 0.000001	 wd 0.0500	time 0.5979 (0.6296)	loss 1.3535 (1.1184)	grad_norm 1.7779 (nan)	loss_scale 2048.0000 (2242.0304)	mem 20188MB
[2024-07-30 07:36:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:01:04 lr 0.000001	 wd 0.0500	time 0.5971 (0.6294)	loss 0.7353 (1.1189)	grad_norm 1.9952 (nan)	loss_scale 2048.0000 (2233.9492)	mem 20188MB
[2024-07-30 07:37:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5954 (0.6293)	loss 0.8012 (1.1190)	grad_norm 2.6293 (nan)	loss_scale 2048.0000 (2226.5142)	mem 20188MB
[2024-07-30 07:37:29 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 26 training takes 0:26:21
[2024-07-30 07:37:39 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 9.956 (9.956)	Loss 0.4719 (0.4719)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20188MB
[2024-07-30 07:38:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.732 Acc@5 98.032
[2024-07-30 07:38:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 07:38:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.74%
[2024-07-30 07:38:17 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:17:41 lr 0.000001	 wd 0.0500	time 10.4963 (10.4963)	loss 1.3396 (1.3396)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:39:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:29:10 lr 0.000001	 wd 0.0500	time 0.5768 (0.7288)	loss 1.2220 (1.1278)	grad_norm 1.5460 (2.3732)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:40:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:25:58 lr 0.000001	 wd 0.0500	time 0.5920 (0.6770)	loss 1.1859 (1.1329)	grad_norm 2.1485 (2.7905)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:41:24 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:24:11 lr 0.000001	 wd 0.0500	time 0.5982 (0.6592)	loss 1.2448 (1.1290)	grad_norm 2.4450 (2.8215)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:42:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:22:48 lr 0.000001	 wd 0.0500	time 0.5972 (0.6509)	loss 0.9419 (1.1284)	grad_norm 2.8501 (2.8043)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:43:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:21:32 lr 0.000001	 wd 0.0500	time 0.6054 (0.6458)	loss 0.7374 (1.1291)	grad_norm 2.3865 (2.7172)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:44:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:20:21 lr 0.000001	 wd 0.0500	time 0.6001 (0.6424)	loss 1.3391 (1.1285)	grad_norm 1.7237 (2.6253)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:45:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:19:13 lr 0.000001	 wd 0.0500	time 0.6269 (0.6399)	loss 0.7639 (1.1286)	grad_norm 2.1275 (2.6141)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:46:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:18:05 lr 0.000001	 wd 0.0500	time 0.5785 (0.6380)	loss 1.0840 (1.1285)	grad_norm 2.4671 (2.5886)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:47:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:16:59 lr 0.000001	 wd 0.0500	time 0.6127 (0.6366)	loss 1.2902 (1.1272)	grad_norm 1.5141 (2.5574)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:48:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:15:54 lr 0.000001	 wd 0.0500	time 0.5938 (0.6355)	loss 0.9089 (1.1232)	grad_norm 1.9784 (2.5624)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:49:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:14:49 lr 0.000001	 wd 0.0500	time 0.5992 (0.6347)	loss 1.1351 (1.1246)	grad_norm 2.8443 (2.6244)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:50:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:13:45 lr 0.000001	 wd 0.0500	time 0.5957 (0.6340)	loss 0.8287 (1.1242)	grad_norm 2.4373 (2.6299)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:51:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:12:41 lr 0.000001	 wd 0.0500	time 0.5902 (0.6332)	loss 0.7745 (1.1198)	grad_norm 2.6379 (2.6122)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:52:52 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:11:37 lr 0.000001	 wd 0.0500	time 0.5933 (0.6327)	loss 0.8666 (1.1213)	grad_norm 2.7646 (2.6004)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:53:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:10:33 lr 0.000001	 wd 0.0500	time 0.5907 (0.6322)	loss 1.4158 (1.1226)	grad_norm 4.0468 (2.5961)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:54:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:09:29 lr 0.000001	 wd 0.0500	time 0.6044 (0.6318)	loss 1.4784 (1.1240)	grad_norm 3.8109 (2.5910)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:56:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:08:26 lr 0.000001	 wd 0.0500	time 0.6006 (0.6315)	loss 1.3096 (1.1249)	grad_norm 2.1213 (2.5798)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:57:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:07:23 lr 0.000001	 wd 0.0500	time 0.5932 (0.6311)	loss 1.2993 (1.1241)	grad_norm 2.2952 (2.5671)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:58:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:06:19 lr 0.000001	 wd 0.0500	time 0.5924 (0.6308)	loss 1.3259 (1.1246)	grad_norm 2.1264 (2.5660)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 07:59:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:05:16 lr 0.000001	 wd 0.0500	time 0.5941 (0.6306)	loss 1.3318 (1.1251)	grad_norm 2.3150 (2.5623)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:00:10 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:04:13 lr 0.000001	 wd 0.0500	time 0.5945 (0.6304)	loss 1.4911 (1.1248)	grad_norm 2.4684 (2.5555)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:01:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:03:10 lr 0.000001	 wd 0.0500	time 0.5933 (0.6302)	loss 1.1860 (1.1255)	grad_norm 2.4655 (2.5584)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:02:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:02:07 lr 0.000001	 wd 0.0500	time 0.6185 (0.6300)	loss 1.2898 (1.1255)	grad_norm 1.7746 (2.5677)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:03:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:01:04 lr 0.000001	 wd 0.0500	time 0.6130 (0.6299)	loss 1.3239 (1.1268)	grad_norm 3.0219 (2.5792)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:04:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5904 (0.6296)	loss 1.1200 (1.1259)	grad_norm 2.7029 (2.5788)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:04:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 27 training takes 0:26:21
[2024-07-30 08:04:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.687 (10.687)	Loss 0.4602 (0.4602)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 20188MB
[2024-07-30 08:05:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.734 Acc@5 98.048
[2024-07-30 08:05:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 08:05:05 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.74%
[2024-07-30 08:05:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:12:59 lr 0.000001	 wd 0.0500	time 10.3835 (10.3835)	loss 0.7472 (0.7472)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:06:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:29:02 lr 0.000000	 wd 0.0500	time 0.5973 (0.7255)	loss 0.7286 (1.1456)	grad_norm 1.8728 (2.2847)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:07:21 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:25:53 lr 0.000000	 wd 0.0500	time 0.5919 (0.6750)	loss 1.3569 (1.1423)	grad_norm 2.3246 (2.4432)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:08:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:24:09 lr 0.000000	 wd 0.0500	time 0.5966 (0.6582)	loss 1.3797 (1.1339)	grad_norm 3.2099 (2.4156)	loss_scale 4096.0000 (2061.6080)	mem 20188MB
[2024-07-30 08:09:26 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:22:46 lr 0.000000	 wd 0.0500	time 0.5890 (0.6502)	loss 0.7281 (1.1253)	grad_norm 1.5821 (2.3891)	loss_scale 4096.0000 (2568.9377)	mem 20188MB
[2024-07-30 08:10:28 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:21:31 lr 0.000000	 wd 0.0500	time 0.5936 (0.6451)	loss 0.7998 (1.1306)	grad_norm 2.9135 (2.4229)	loss_scale 4096.0000 (2873.7405)	mem 20188MB
[2024-07-30 08:11:31 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:20:20 lr 0.000000	 wd 0.0500	time 0.5990 (0.6419)	loss 0.8666 (1.1235)	grad_norm 3.6612 (2.4070)	loss_scale 4096.0000 (3077.1115)	mem 20188MB
[2024-07-30 08:12:33 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:19:12 lr 0.000000	 wd 0.0500	time 0.5958 (0.6396)	loss 1.2117 (1.1204)	grad_norm 1.7334 (nan)	loss_scale 2048.0000 (3158.1854)	mem 20188MB
[2024-07-30 08:13:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:18:05 lr 0.000000	 wd 0.0500	time 0.6118 (0.6379)	loss 0.8094 (1.1218)	grad_norm 3.1917 (nan)	loss_scale 2048.0000 (3019.5855)	mem 20188MB
[2024-07-30 08:14:38 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:16:59 lr 0.000000	 wd 0.0500	time 0.6011 (0.6364)	loss 0.7610 (1.1186)	grad_norm 2.0285 (nan)	loss_scale 2048.0000 (2911.7514)	mem 20188MB
[2024-07-30 08:15:41 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:15:54 lr 0.000000	 wd 0.0500	time 0.5995 (0.6353)	loss 1.0055 (1.1199)	grad_norm 1.8857 (nan)	loss_scale 2048.0000 (2825.4625)	mem 20188MB
[2024-07-30 08:16:43 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:14:49 lr 0.000000	 wd 0.0500	time 0.5910 (0.6344)	loss 1.1387 (1.1209)	grad_norm 1.6527 (nan)	loss_scale 2048.0000 (2754.8483)	mem 20188MB
[2024-07-30 08:17:46 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:13:45 lr 0.000000	 wd 0.0500	time 0.5861 (0.6337)	loss 1.3322 (1.1215)	grad_norm 3.1031 (nan)	loss_scale 2048.0000 (2695.9933)	mem 20188MB
[2024-07-30 08:18:49 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:12:40 lr 0.000000	 wd 0.0500	time 0.6163 (0.6330)	loss 1.3527 (1.1234)	grad_norm 1.6514 (nan)	loss_scale 2048.0000 (2646.1860)	mem 20188MB
[2024-07-30 08:19:51 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:11:36 lr 0.000000	 wd 0.0500	time 0.6235 (0.6325)	loss 0.8745 (1.1221)	grad_norm 1.6988 (nan)	loss_scale 2048.0000 (2603.4889)	mem 20188MB
[2024-07-30 08:20:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:10:33 lr 0.000000	 wd 0.0500	time 0.5946 (0.6319)	loss 1.1751 (1.1219)	grad_norm 1.7347 (nan)	loss_scale 2048.0000 (2566.4810)	mem 20188MB
[2024-07-30 08:21:56 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:09:29 lr 0.000000	 wd 0.0500	time 0.5978 (0.6316)	loss 1.3863 (1.1241)	grad_norm 2.0708 (nan)	loss_scale 2048.0000 (2534.0962)	mem 20188MB
[2024-07-30 08:22:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:08:26 lr 0.000000	 wd 0.0500	time 0.5910 (0.6312)	loss 1.3344 (1.1239)	grad_norm 2.2649 (nan)	loss_scale 2048.0000 (2505.5191)	mem 20188MB
[2024-07-30 08:24:01 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:07:22 lr 0.000000	 wd 0.0500	time 0.5826 (0.6309)	loss 0.7291 (1.1234)	grad_norm 1.8341 (nan)	loss_scale 2048.0000 (2480.1155)	mem 20188MB
[2024-07-30 08:25:04 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:06:19 lr 0.000000	 wd 0.0500	time 0.6129 (0.6307)	loss 0.9851 (1.1209)	grad_norm 1.8710 (nan)	loss_scale 2048.0000 (2457.3845)	mem 20188MB
[2024-07-30 08:26:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:05:16 lr 0.000000	 wd 0.0500	time 0.5976 (0.6304)	loss 1.1289 (1.1210)	grad_norm 4.4313 (nan)	loss_scale 2048.0000 (2436.9255)	mem 20188MB
[2024-07-30 08:27:09 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:04:13 lr 0.000000	 wd 0.0500	time 0.5910 (0.6302)	loss 1.0088 (1.1215)	grad_norm 3.5311 (nan)	loss_scale 2048.0000 (2418.4141)	mem 20188MB
[2024-07-30 08:28:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:03:10 lr 0.000000	 wd 0.0500	time 0.5979 (0.6299)	loss 1.2745 (1.1209)	grad_norm 2.1930 (nan)	loss_scale 2048.0000 (2401.5847)	mem 20188MB
[2024-07-30 08:29:14 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:02:07 lr 0.000000	 wd 0.0500	time 0.6009 (0.6298)	loss 1.2599 (1.1208)	grad_norm 2.0028 (nan)	loss_scale 2048.0000 (2386.2182)	mem 20188MB
[2024-07-30 08:30:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:01:04 lr 0.000000	 wd 0.0500	time 0.5985 (0.6296)	loss 1.4975 (1.1199)	grad_norm 1.9340 (nan)	loss_scale 2048.0000 (2372.1316)	mem 20188MB
[2024-07-30 08:31:19 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.6100 (0.6294)	loss 1.0373 (1.1191)	grad_norm 2.0102 (nan)	loss_scale 2048.0000 (2359.1715)	mem 20188MB
[2024-07-30 08:31:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 28 training takes 0:26:19
[2024-07-30 08:31:36 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.818 (10.818)	Loss 0.4690 (0.4690)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 20188MB
[2024-07-30 08:32:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.714 Acc@5 98.034
[2024-07-30 08:32:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 08:32:02 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.74%
[2024-07-30 08:32:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:29:41 lr 0.000000	 wd 0.0500	time 10.7839 (10.7839)	loss 1.3009 (1.3009)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:33:15 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:29:02 lr 0.000000	 wd 0.0500	time 0.5808 (0.7255)	loss 1.3354 (1.1351)	grad_norm 4.9356 (2.3053)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:34:18 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:25:51 lr 0.000000	 wd 0.0500	time 0.5953 (0.6742)	loss 0.8007 (1.1363)	grad_norm 2.0781 (2.4032)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:35:20 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:24:07 lr 0.000000	 wd 0.0500	time 0.5854 (0.6575)	loss 0.6818 (1.1243)	grad_norm 2.7580 (2.3861)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:36:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:22:44 lr 0.000000	 wd 0.0500	time 0.5913 (0.6492)	loss 1.0131 (1.1205)	grad_norm 2.4358 (2.3362)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:37:25 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:21:29 lr 0.000000	 wd 0.0500	time 0.5948 (0.6443)	loss 1.1621 (1.1161)	grad_norm 2.4584 (2.4267)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:38:27 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:20:19 lr 0.000000	 wd 0.0500	time 0.5982 (0.6411)	loss 0.6917 (1.1181)	grad_norm 2.0733 (2.4679)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:39:30 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:19:11 lr 0.000000	 wd 0.0500	time 0.5906 (0.6388)	loss 1.1692 (1.1178)	grad_norm 2.7747 (2.4494)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:40:32 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:18:04 lr 0.000000	 wd 0.0500	time 0.6057 (0.6371)	loss 0.8254 (1.1192)	grad_norm 2.0952 (2.4646)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:41:35 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:16:58 lr 0.000000	 wd 0.0500	time 0.5945 (0.6357)	loss 1.3998 (1.1198)	grad_norm 3.9856 (2.4516)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:42:37 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:15:53 lr 0.000000	 wd 0.0500	time 0.5930 (0.6347)	loss 1.2438 (1.1205)	grad_norm 2.5382 (2.5351)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:43:40 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:14:48 lr 0.000000	 wd 0.0500	time 0.5949 (0.6338)	loss 1.2477 (1.1183)	grad_norm 8.9059 (2.5588)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:44:42 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:13:44 lr 0.000000	 wd 0.0500	time 0.5918 (0.6330)	loss 0.9509 (1.1173)	grad_norm 2.3123 (2.5361)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:45:45 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:12:40 lr 0.000000	 wd 0.0500	time 0.5975 (0.6324)	loss 1.1177 (1.1209)	grad_norm 2.8207 (2.5262)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:46:47 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:11:36 lr 0.000000	 wd 0.0500	time 0.5882 (0.6319)	loss 1.0737 (1.1161)	grad_norm 3.0404 (2.5131)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:47:50 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:10:32 lr 0.000000	 wd 0.0500	time 0.6269 (0.6315)	loss 0.9757 (1.1152)	grad_norm 1.7517 (2.4998)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:48:53 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:09:29 lr 0.000000	 wd 0.0500	time 0.6066 (0.6312)	loss 0.7705 (1.1179)	grad_norm 1.7863 (2.5166)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:49:55 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:08:25 lr 0.000000	 wd 0.0500	time 0.5956 (0.6308)	loss 1.1674 (1.1199)	grad_norm 1.8124 (2.5137)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:50:58 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:07:22 lr 0.000000	 wd 0.0500	time 0.6101 (0.6306)	loss 1.2620 (1.1192)	grad_norm 2.5176 (2.5092)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:52:00 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:06:19 lr 0.000000	 wd 0.0500	time 0.5949 (0.6303)	loss 0.8604 (1.1193)	grad_norm 2.9041 (2.5169)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:53:03 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:05:16 lr 0.000000	 wd 0.0500	time 0.5899 (0.6301)	loss 0.8920 (1.1212)	grad_norm 1.7771 (2.5280)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:54:06 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:04:13 lr 0.000000	 wd 0.0500	time 0.6152 (0.6299)	loss 0.8387 (1.1224)	grad_norm 2.0538 (2.5388)	loss_scale 2048.0000 (2048.0000)	mem 20188MB
[2024-07-30 08:55:08 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:03:10 lr 0.000000	 wd 0.0500	time 0.5887 (0.6297)	loss 0.8424 (1.1231)	grad_norm 1.6026 (2.5520)	loss_scale 4096.0000 (2070.3317)	mem 20188MB
[2024-07-30 08:56:11 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:02:07 lr 0.000000	 wd 0.0500	time 0.6009 (0.6295)	loss 0.7057 (1.1202)	grad_norm 1.8466 (2.5642)	loss_scale 4096.0000 (2158.3659)	mem 20188MB
[2024-07-30 08:57:13 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:01:04 lr 0.000000	 wd 0.0500	time 0.5951 (0.6293)	loss 1.1981 (1.1204)	grad_norm 3.3852 (2.5567)	loss_scale 4096.0000 (2239.0671)	mem 20188MB
[2024-07-30 08:58:16 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.5987 (0.6292)	loss 1.2019 (1.1199)	grad_norm 1.7966 (2.5555)	loss_scale 4096.0000 (2313.3147)	mem 20188MB
[2024-07-30 08:58:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 249): INFO EPOCH 29 training takes 0:26:20
[2024-07-30 08:58:22 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_29.pth saving......
[2024-07-30 08:58:23 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0/diffusion_ft_smt_l_sequence_cross0/ckpt_epoch_29.pth saved !!!
[2024-07-30 08:58:34 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 289): INFO Test: [0/98]	Time 10.331 (10.331)	Loss 0.4578 (0.4578)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 20188MB
[2024-07-30 08:58:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 296): INFO  * Acc@1 86.694 Acc@5 98.040
[2024-07-30 08:58:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-07-30 08:58:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 182): INFO Max accuracy: 86.74%
[2024-07-30 08:58:59 smt_diffusion_finetune_large_224_22kto1k_sequence_crosslayer0] (main.py 189): INFO Training time 13:25:31
