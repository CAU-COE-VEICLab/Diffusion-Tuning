[2024-08-01 23:59:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/config.json
[2024-08-01 23:59:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_stage3
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: stage3
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_step_stage3
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-01 23:59:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_step_stage3.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_step_stage3", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-02 00:00:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_stage3
[2024-08-02 00:00:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-02 00:00:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 113): INFO number of params: 19828456
[2024-08-02 00:00:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3, ignoring auto resume
[2024-08-02 00:00:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth for fine-tuning......
[2024-08-02 00:00:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-02 00:00:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_smt_l_step_stage2/ckpt_epoch_best.pth'
[2024-08-02 00:00:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 13.364 (13.364)	Loss 0.4563 (0.4563)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 2381MB
[2024-08-02 00:00:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.840 Acc@5 98.090
[2024-08-02 00:00:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 00:00:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 168): INFO Start training
[2024-08-02 00:00:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:41:37 lr 0.000000	 wd 0.0500	time 12.5090 (12.5090)	loss 1.4475 (1.4475)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17147MB
[2024-08-02 00:01:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:22:44 lr 0.000000	 wd 0.0500	time 0.4481 (0.5682)	loss 1.2480 (1.1540)	grad_norm 1.6170 (nan)	loss_scale 16384.0000 (23034.9307)	mem 17147MB
[2024-08-02 00:02:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:19:32 lr 0.000000	 wd 0.0500	time 0.4408 (0.5093)	loss 0.9924 (1.1485)	grad_norm 1.5636 (nan)	loss_scale 16384.0000 (19726.0100)	mem 17147MB
[2024-08-02 00:03:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:17:58 lr 0.000000	 wd 0.0500	time 0.4477 (0.4896)	loss 0.8604 (1.1102)	grad_norm 2.1295 (nan)	loss_scale 16384.0000 (18615.7076)	mem 17147MB
[2024-08-02 00:03:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:16:48 lr 0.000001	 wd 0.0500	time 0.4441 (0.4799)	loss 1.0550 (1.1175)	grad_norm 2.5609 (nan)	loss_scale 8192.0000 (16833.4364)	mem 17147MB
[2024-08-02 00:04:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:15:49 lr 0.000001	 wd 0.0500	time 0.4480 (0.4741)	loss 1.1384 (1.1193)	grad_norm 2.0473 (nan)	loss_scale 4096.0000 (14454.5469)	mem 17147MB
[2024-08-02 00:05:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:14:54 lr 0.000001	 wd 0.0500	time 0.4480 (0.4703)	loss 1.2306 (1.1180)	grad_norm 1.9262 (nan)	loss_scale 4096.0000 (12730.9950)	mem 17147MB
[2024-08-02 00:06:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:14:02 lr 0.000001	 wd 0.0500	time 0.4468 (0.4678)	loss 1.2283 (1.1204)	grad_norm 4.2191 (nan)	loss_scale 4096.0000 (11499.1840)	mem 17147MB
[2024-08-02 00:06:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:13:12 lr 0.000001	 wd 0.0500	time 0.4474 (0.4658)	loss 1.0513 (1.1192)	grad_norm 1.8646 (nan)	loss_scale 4096.0000 (10574.9413)	mem 17147MB
[2024-08-02 00:07:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:12:23 lr 0.000001	 wd 0.0500	time 0.4452 (0.4644)	loss 1.2694 (1.1197)	grad_norm 1.6968 (nan)	loss_scale 4096.0000 (9855.8579)	mem 17147MB
[2024-08-02 00:08:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:11:35 lr 0.000002	 wd 0.0500	time 0.4455 (0.4633)	loss 1.3912 (1.1171)	grad_norm 1.4340 (nan)	loss_scale 4096.0000 (9280.4476)	mem 17147MB
[2024-08-02 00:09:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:10:48 lr 0.000002	 wd 0.0500	time 0.4491 (0.4623)	loss 1.3063 (1.1185)	grad_norm 1.7901 (nan)	loss_scale 4096.0000 (8809.5622)	mem 17147MB
[2024-08-02 00:09:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:10:00 lr 0.000002	 wd 0.0500	time 0.4434 (0.4615)	loss 0.9760 (1.1180)	grad_norm 1.8232 (nan)	loss_scale 4096.0000 (8417.0924)	mem 17147MB
[2024-08-02 00:10:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:09:14 lr 0.000002	 wd 0.0500	time 0.4490 (0.4609)	loss 1.1548 (1.1196)	grad_norm 1.3369 (nan)	loss_scale 4096.0000 (8084.9562)	mem 17147MB
[2024-08-02 00:11:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:08:27 lr 0.000002	 wd 0.0500	time 0.4482 (0.4603)	loss 1.4189 (1.1206)	grad_norm 1.5209 (nan)	loss_scale 4096.0000 (7800.2341)	mem 17147MB
[2024-08-02 00:12:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:07:40 lr 0.000002	 wd 0.0500	time 0.4516 (0.4598)	loss 0.9613 (1.1237)	grad_norm 1.2443 (nan)	loss_scale 4096.0000 (7553.4497)	mem 17147MB
[2024-08-02 00:12:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:06:54 lr 0.000003	 wd 0.0500	time 0.4497 (0.4594)	loss 1.1818 (1.1210)	grad_norm 1.3103 (nan)	loss_scale 4096.0000 (7337.4941)	mem 17147MB
[2024-08-02 00:13:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:06:08 lr 0.000003	 wd 0.0500	time 0.4472 (0.4591)	loss 0.9405 (1.1205)	grad_norm 1.9081 (nan)	loss_scale 4096.0000 (7146.9300)	mem 17147MB
[2024-08-02 00:14:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:05:22 lr 0.000003	 wd 0.0500	time 0.4498 (0.4588)	loss 1.0264 (1.1205)	grad_norm 1.5446 (nan)	loss_scale 4096.0000 (6977.5280)	mem 17147MB
[2024-08-02 00:15:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:04:35 lr 0.000003	 wd 0.0500	time 0.4492 (0.4585)	loss 1.4715 (1.1203)	grad_norm 2.1374 (nan)	loss_scale 4096.0000 (6825.9484)	mem 17147MB
[2024-08-02 00:15:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:03:50 lr 0.000003	 wd 0.0500	time 0.4481 (0.4582)	loss 0.7954 (1.1194)	grad_norm 1.3829 (nan)	loss_scale 4096.0000 (6689.5192)	mem 17147MB
[2024-08-02 00:16:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:03:04 lr 0.000003	 wd 0.0500	time 0.4498 (0.4580)	loss 0.9573 (1.1182)	grad_norm 1.6566 (nan)	loss_scale 4096.0000 (6566.0771)	mem 17147MB
[2024-08-02 00:17:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:02:18 lr 0.000004	 wd 0.0500	time 0.4523 (0.4577)	loss 1.4335 (1.1174)	grad_norm 4.0274 (nan)	loss_scale 4096.0000 (6453.8519)	mem 17147MB
[2024-08-02 00:18:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:32 lr 0.000004	 wd 0.0500	time 0.4487 (0.4575)	loss 1.3592 (1.1172)	grad_norm 1.8766 (nan)	loss_scale 4096.0000 (6351.3811)	mem 17147MB
[2024-08-02 00:19:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:46 lr 0.000004	 wd 0.0500	time 0.4521 (0.4573)	loss 1.2690 (1.1167)	grad_norm 1.7290 (nan)	loss_scale 4096.0000 (6257.4461)	mem 17147MB
[2024-08-02 00:19:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.4443 (0.4571)	loss 0.9519 (1.1152)	grad_norm 1.8126 (nan)	loss_scale 4096.0000 (6171.0228)	mem 17147MB
[2024-08-02 00:19:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 0 training takes 0:19:06
[2024-08-02 00:19:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_0.pth saving......
[2024-08-02 00:19:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_0.pth saved !!!
[2024-08-02 00:20:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.110 (11.110)	Loss 0.4912 (0.4912)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 00:20:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.832 Acc@5 98.066
[2024-08-02 00:20:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 00:20:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.83%
[2024-08-02 00:20:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 00:20:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 00:20:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][0/2502]	eta 8:42:44 lr 0.000004	 wd 0.0500	time 12.5357 (12.5357)	loss 1.2872 (1.2872)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:21:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:22:46 lr 0.000004	 wd 0.0500	time 0.4444 (0.5689)	loss 1.2400 (1.1022)	grad_norm 2.2871 (2.0571)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:22:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:19:33 lr 0.000004	 wd 0.0500	time 0.4483 (0.5099)	loss 0.7470 (1.1037)	grad_norm 1.2924 (1.9550)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:22:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:17:59 lr 0.000004	 wd 0.0500	time 0.4460 (0.4901)	loss 0.8653 (1.1067)	grad_norm 1.7572 (1.9312)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:23:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:16:49 lr 0.000005	 wd 0.0500	time 0.4470 (0.4803)	loss 0.8800 (1.1210)	grad_norm 1.1606 (1.9808)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:24:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:15:50 lr 0.000005	 wd 0.0500	time 0.4497 (0.4746)	loss 1.4938 (1.1243)	grad_norm 2.0897 (1.9770)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:25:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:14:55 lr 0.000005	 wd 0.0500	time 0.4450 (0.4708)	loss 1.3961 (1.1237)	grad_norm 1.6490 (1.9511)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:25:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:14:03 lr 0.000005	 wd 0.0500	time 0.4504 (0.4682)	loss 0.8127 (1.1202)	grad_norm 1.7705 (1.9350)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:26:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:13:13 lr 0.000005	 wd 0.0500	time 0.4457 (0.4662)	loss 1.5124 (1.1197)	grad_norm 1.6333 (1.9199)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:27:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:12:24 lr 0.000005	 wd 0.0500	time 0.4470 (0.4646)	loss 1.0996 (1.1178)	grad_norm 1.8251 (1.9316)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:28:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:11:36 lr 0.000006	 wd 0.0500	time 0.4498 (0.4635)	loss 1.4187 (1.1147)	grad_norm 2.7727 (1.9641)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:28:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:10:48 lr 0.000006	 wd 0.0500	time 0.4444 (0.4625)	loss 1.4706 (1.1137)	grad_norm 1.6082 (1.9773)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:29:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:10:01 lr 0.000006	 wd 0.0500	time 0.4474 (0.4617)	loss 0.9007 (1.1120)	grad_norm 2.3127 (1.9725)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:30:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:09:14 lr 0.000006	 wd 0.0500	time 0.4500 (0.4610)	loss 1.1731 (1.1133)	grad_norm 1.7834 (1.9696)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:31:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:08:27 lr 0.000006	 wd 0.0500	time 0.4491 (0.4605)	loss 0.7714 (1.1133)	grad_norm 3.3364 (1.9611)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:31:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:07:40 lr 0.000006	 wd 0.0500	time 0.4487 (0.4600)	loss 1.3975 (1.1134)	grad_norm 1.9954 (1.9530)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:32:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:06:54 lr 0.000007	 wd 0.0500	time 0.4487 (0.4595)	loss 1.2946 (1.1144)	grad_norm 1.5237 (1.9653)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:33:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:06:08 lr 0.000007	 wd 0.0500	time 0.4479 (0.4592)	loss 0.8041 (1.1150)	grad_norm 1.7820 (1.9619)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:34:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:05:22 lr 0.000007	 wd 0.0500	time 0.4464 (0.4589)	loss 1.5815 (1.1142)	grad_norm 5.1699 (1.9873)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:34:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:04:36 lr 0.000007	 wd 0.0500	time 0.4479 (0.4586)	loss 1.4352 (1.1160)	grad_norm 1.7452 (2.0056)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:35:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:03:50 lr 0.000007	 wd 0.0500	time 0.4468 (0.4583)	loss 1.2304 (1.1164)	grad_norm 2.6758 (2.0067)	loss_scale 8192.0000 (4263.8521)	mem 17147MB
[2024-08-02 00:36:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:03:04 lr 0.000007	 wd 0.0500	time 0.4473 (0.4581)	loss 1.3422 (1.1182)	grad_norm 1.9441 (2.0016)	loss_scale 8192.0000 (4450.8177)	mem 17147MB
[2024-08-02 00:37:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:02:18 lr 0.000008	 wd 0.0500	time 0.4448 (0.4578)	loss 1.0813 (1.1186)	grad_norm inf (inf)	loss_scale 4096.0000 (4617.0722)	mem 17147MB
[2024-08-02 00:37:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.4485 (0.4576)	loss 0.7559 (1.1200)	grad_norm 3.4858 (inf)	loss_scale 4096.0000 (4594.4268)	mem 17147MB
[2024-08-02 00:38:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:46 lr 0.000008	 wd 0.0500	time 0.4510 (0.4574)	loss 1.2007 (1.1199)	grad_norm 1.2751 (inf)	loss_scale 4096.0000 (4573.6676)	mem 17147MB
[2024-08-02 00:39:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.4481 (0.4573)	loss 0.8959 (1.1197)	grad_norm 1.6983 (inf)	loss_scale 4096.0000 (4554.5686)	mem 17147MB
[2024-08-02 00:39:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 1 training takes 0:19:06
[2024-08-02 00:39:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.509 (11.509)	Loss 0.4851 (0.4851)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 00:40:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.788 Acc@5 98.070
[2024-08-02 00:40:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 00:40:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.83%
[2024-08-02 00:40:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][0/2502]	eta 8:29:59 lr 0.000008	 wd 0.0500	time 12.2300 (12.2300)	loss 1.1138 (1.1138)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:40:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:22:38 lr 0.000008	 wd 0.0500	time 0.4434 (0.5655)	loss 1.3885 (1.1196)	grad_norm 1.7046 (1.9927)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:41:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:19:29 lr 0.000008	 wd 0.0500	time 0.4421 (0.5082)	loss 1.2361 (1.1092)	grad_norm 1.0174 (1.9585)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:42:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:17:56 lr 0.000008	 wd 0.0500	time 0.4476 (0.4888)	loss 1.1855 (1.0979)	grad_norm 3.3409 (2.0863)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:43:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:16:47 lr 0.000009	 wd 0.0500	time 0.4459 (0.4793)	loss 1.3095 (1.1028)	grad_norm 1.2993 (2.0716)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:44:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:15:48 lr 0.000009	 wd 0.0500	time 0.4460 (0.4737)	loss 1.2837 (1.1015)	grad_norm 2.5448 (2.1005)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:44:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:14:54 lr 0.000009	 wd 0.0500	time 0.4466 (0.4701)	loss 1.2308 (1.1008)	grad_norm 1.7135 (2.0786)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:45:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:14:02 lr 0.000009	 wd 0.0500	time 0.4432 (0.4676)	loss 1.2787 (1.1049)	grad_norm 1.3634 (2.0359)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:46:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:13:12 lr 0.000009	 wd 0.0500	time 0.4475 (0.4657)	loss 1.1709 (1.1077)	grad_norm 1.6491 (2.0037)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:47:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:12:23 lr 0.000009	 wd 0.0500	time 0.4479 (0.4643)	loss 0.8085 (1.1112)	grad_norm 1.7377 (1.9971)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:47:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:11:35 lr 0.000010	 wd 0.0500	time 0.4479 (0.4632)	loss 0.8247 (1.1091)	grad_norm 2.8097 (1.9874)	loss_scale 4096.0000 (4096.0000)	mem 17147MB
[2024-08-02 00:48:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:10:48 lr 0.000010	 wd 0.0500	time 0.4467 (0.4623)	loss 1.3457 (1.1079)	grad_norm 1.8086 (nan)	loss_scale 2048.0000 (4040.1962)	mem 17147MB
[2024-08-02 00:49:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:10:00 lr 0.000010	 wd 0.0500	time 0.4506 (0.4615)	loss 0.9656 (1.1083)	grad_norm 1.9239 (nan)	loss_scale 2048.0000 (3874.3181)	mem 17147MB
[2024-08-02 00:50:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:09:13 lr 0.000010	 wd 0.0500	time 0.4478 (0.4608)	loss 1.3636 (1.1086)	grad_norm 1.3677 (nan)	loss_scale 2048.0000 (3733.9400)	mem 17147MB
[2024-08-02 00:50:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:08:27 lr 0.000010	 wd 0.0500	time 0.4505 (0.4603)	loss 1.1577 (1.1100)	grad_norm 2.4015 (nan)	loss_scale 2048.0000 (3613.6017)	mem 17147MB
[2024-08-02 00:51:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:07:40 lr 0.000010	 wd 0.0500	time 0.4475 (0.4598)	loss 1.3122 (1.1105)	grad_norm 1.5724 (nan)	loss_scale 2048.0000 (3509.2978)	mem 17147MB
[2024-08-02 00:52:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:06:54 lr 0.000011	 wd 0.0500	time 0.4511 (0.4594)	loss 0.9174 (1.1107)	grad_norm 1.5735 (nan)	loss_scale 2048.0000 (3418.0237)	mem 17147MB
[2024-08-02 00:53:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:06:08 lr 0.000011	 wd 0.0500	time 0.4479 (0.4590)	loss 1.1891 (1.1107)	grad_norm 1.6973 (nan)	loss_scale 2048.0000 (3337.4815)	mem 17147MB
[2024-08-02 00:53:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:05:21 lr 0.000011	 wd 0.0500	time 0.4468 (0.4586)	loss 1.1169 (1.1116)	grad_norm 1.8491 (nan)	loss_scale 2048.0000 (3265.8834)	mem 17147MB
[2024-08-02 00:54:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:04:35 lr 0.000011	 wd 0.0500	time 0.4496 (0.4583)	loss 1.2789 (1.1112)	grad_norm 1.3410 (nan)	loss_scale 2048.0000 (3201.8180)	mem 17147MB
[2024-08-02 00:55:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:03:49 lr 0.000011	 wd 0.0500	time 0.4461 (0.4580)	loss 0.9510 (1.1113)	grad_norm 1.3822 (nan)	loss_scale 2048.0000 (3144.1559)	mem 17147MB
[2024-08-02 00:56:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:03:04 lr 0.000011	 wd 0.0500	time 0.4506 (0.4578)	loss 0.9616 (1.1105)	grad_norm 1.8786 (nan)	loss_scale 2048.0000 (3091.9829)	mem 17147MB
[2024-08-02 00:56:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:02:18 lr 0.000012	 wd 0.0500	time 0.4488 (0.4576)	loss 1.2331 (1.1110)	grad_norm 1.5110 (nan)	loss_scale 2048.0000 (3044.5507)	mem 17147MB
[2024-08-02 00:57:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:32 lr 0.000012	 wd 0.0500	time 0.4517 (0.4574)	loss 1.0528 (1.1107)	grad_norm 1.4478 (nan)	loss_scale 2048.0000 (3001.2412)	mem 17147MB
[2024-08-02 00:58:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:46 lr 0.000012	 wd 0.0500	time 0.4482 (0.4572)	loss 0.7641 (1.1113)	grad_norm 1.7909 (nan)	loss_scale 2048.0000 (2961.5394)	mem 17147MB
[2024-08-02 00:59:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.4490 (0.4570)	loss 1.1552 (1.1112)	grad_norm 1.6202 (nan)	loss_scale 2048.0000 (2925.0124)	mem 17147MB
[2024-08-02 00:59:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 2 training takes 0:19:05
[2024-08-02 00:59:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.483 (11.483)	Loss 0.4685 (0.4685)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 00:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.864 Acc@5 98.064
[2024-08-02 00:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 00:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-02 00:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 00:59:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 00:59:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:46:40 lr 0.000012	 wd 0.0500	time 11.1912 (11.1912)	loss 0.6624 (0.6624)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:00:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:22:21 lr 0.000012	 wd 0.0500	time 0.4468 (0.5586)	loss 1.2220 (1.1477)	grad_norm 1.3839 (1.9546)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:01:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:19:21 lr 0.000012	 wd 0.0500	time 0.4464 (0.5046)	loss 1.4205 (1.1218)	grad_norm 1.7487 (1.9037)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:02:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:17:51 lr 0.000012	 wd 0.0500	time 0.4505 (0.4865)	loss 1.3951 (1.1225)	grad_norm 1.6272 (1.9197)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:02:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:16:44 lr 0.000013	 wd 0.0500	time 0.4457 (0.4777)	loss 1.3397 (1.1136)	grad_norm 1.5102 (nan)	loss_scale 1024.0000 (1879.4613)	mem 17147MB
[2024-08-02 01:03:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:15:45 lr 0.000013	 wd 0.0500	time 0.4493 (0.4724)	loss 0.7831 (1.1138)	grad_norm 1.7160 (nan)	loss_scale 1024.0000 (1708.7106)	mem 17147MB
[2024-08-02 01:04:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:14:51 lr 0.000013	 wd 0.0500	time 0.4493 (0.4689)	loss 1.0157 (1.1058)	grad_norm 1.6229 (nan)	loss_scale 1024.0000 (1594.7820)	mem 17147MB
[2024-08-02 01:05:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:14:00 lr 0.000013	 wd 0.0500	time 0.4498 (0.4666)	loss 1.3082 (1.1041)	grad_norm 2.1419 (nan)	loss_scale 1024.0000 (1513.3581)	mem 17147MB
[2024-08-02 01:05:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:13:11 lr 0.000013	 wd 0.0500	time 0.4503 (0.4648)	loss 0.9332 (1.0992)	grad_norm 1.3288 (nan)	loss_scale 1024.0000 (1452.2647)	mem 17147MB
[2024-08-02 01:06:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:12:22 lr 0.000013	 wd 0.0500	time 0.4424 (0.4635)	loss 1.5219 (1.1017)	grad_norm 1.8332 (nan)	loss_scale 1024.0000 (1404.7325)	mem 17147MB
[2024-08-02 01:07:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:11:34 lr 0.000014	 wd 0.0500	time 0.4513 (0.4624)	loss 1.2370 (1.1013)	grad_norm 1.4380 (nan)	loss_scale 1024.0000 (1366.6973)	mem 17147MB
[2024-08-02 01:08:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:10:47 lr 0.000014	 wd 0.0500	time 0.4505 (0.4616)	loss 0.7697 (1.1024)	grad_norm 3.2101 (nan)	loss_scale 1024.0000 (1335.5713)	mem 17147MB
[2024-08-02 01:08:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:10:00 lr 0.000014	 wd 0.0500	time 0.4489 (0.4609)	loss 1.1005 (1.1001)	grad_norm 1.9190 (nan)	loss_scale 1024.0000 (1309.6286)	mem 17147MB
[2024-08-02 01:09:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:09:13 lr 0.000014	 wd 0.0500	time 0.4488 (0.4603)	loss 1.2030 (1.0997)	grad_norm 1.5284 (nan)	loss_scale 1024.0000 (1287.6741)	mem 17147MB
[2024-08-02 01:10:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:08:26 lr 0.000014	 wd 0.0500	time 0.4506 (0.4598)	loss 1.1262 (1.1004)	grad_norm 1.0417 (nan)	loss_scale 1024.0000 (1268.8537)	mem 17147MB
[2024-08-02 01:11:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:07:40 lr 0.000014	 wd 0.0500	time 0.4503 (0.4593)	loss 1.3036 (1.0987)	grad_norm 1.3978 (nan)	loss_scale 1024.0000 (1252.5410)	mem 17147MB
[2024-08-02 01:11:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:06:54 lr 0.000015	 wd 0.0500	time 0.4482 (0.4590)	loss 0.6847 (1.0962)	grad_norm 1.4433 (nan)	loss_scale 1024.0000 (1238.2661)	mem 17147MB
[2024-08-02 01:12:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:06:07 lr 0.000015	 wd 0.0500	time 0.4491 (0.4587)	loss 0.9473 (1.0987)	grad_norm 1.4654 (nan)	loss_scale 1024.0000 (1225.6696)	mem 17147MB
[2024-08-02 01:13:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:05:21 lr 0.000015	 wd 0.0500	time 0.4515 (0.4584)	loss 1.1988 (1.1001)	grad_norm 1.2392 (nan)	loss_scale 1024.0000 (1214.4720)	mem 17147MB
[2024-08-02 01:14:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:04:35 lr 0.000015	 wd 0.0500	time 0.4512 (0.4582)	loss 1.2956 (1.0988)	grad_norm 1.1102 (nan)	loss_scale 1024.0000 (1204.4524)	mem 17147MB
[2024-08-02 01:14:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:03:49 lr 0.000015	 wd 0.0500	time 0.4485 (0.4579)	loss 1.1790 (1.0984)	grad_norm 1.2806 (nan)	loss_scale 1024.0000 (1195.4343)	mem 17147MB
[2024-08-02 01:15:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:03:04 lr 0.000015	 wd 0.0500	time 0.4488 (0.4577)	loss 0.7763 (1.0984)	grad_norm 1.9258 (nan)	loss_scale 1024.0000 (1187.2746)	mem 17147MB
[2024-08-02 01:16:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:18 lr 0.000016	 wd 0.0500	time 0.4465 (0.4575)	loss 0.7657 (1.0975)	grad_norm 1.4237 (nan)	loss_scale 1024.0000 (1179.8564)	mem 17147MB
[2024-08-02 01:17:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:32 lr 0.000016	 wd 0.0500	time 0.4497 (0.4573)	loss 0.8575 (1.0978)	grad_norm 1.4678 (nan)	loss_scale 1024.0000 (1173.0830)	mem 17147MB
[2024-08-02 01:18:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:46 lr 0.000016	 wd 0.0500	time 0.4474 (0.4572)	loss 1.2006 (1.0990)	grad_norm 1.9335 (nan)	loss_scale 1024.0000 (1166.8738)	mem 17147MB
[2024-08-02 01:18:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.4524 (0.4570)	loss 1.1625 (1.0990)	grad_norm 1.2120 (nan)	loss_scale 1024.0000 (1161.1611)	mem 17147MB
[2024-08-02 01:18:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 3 training takes 0:19:05
[2024-08-02 01:19:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.620 (12.620)	Loss 0.4771 (0.4771)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 01:19:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.836 Acc@5 98.078
[2024-08-02 01:19:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 01:19:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-02 01:19:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][0/2502]	eta 8:42:40 lr 0.000016	 wd 0.0500	time 12.5341 (12.5341)	loss 1.2071 (1.2071)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:20:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:22:51 lr 0.000016	 wd 0.0500	time 0.4487 (0.5708)	loss 0.8882 (1.1133)	grad_norm 1.6842 (2.5984)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:21:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:19:35 lr 0.000016	 wd 0.0500	time 0.4465 (0.5106)	loss 1.0690 (1.1021)	grad_norm 1.6374 (2.3156)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:21:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:18:00 lr 0.000016	 wd 0.0500	time 0.4434 (0.4905)	loss 0.7291 (1.0984)	grad_norm 1.9177 (2.2551)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:22:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:16:50 lr 0.000017	 wd 0.0500	time 0.4463 (0.4806)	loss 1.3089 (1.0958)	grad_norm 3.7726 (2.3876)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:23:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:15:50 lr 0.000017	 wd 0.0500	time 0.4487 (0.4748)	loss 1.1704 (1.0960)	grad_norm 2.3830 (2.3233)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:24:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:14:55 lr 0.000017	 wd 0.0500	time 0.4436 (0.4710)	loss 0.9523 (1.0969)	grad_norm 1.9055 (2.3494)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:24:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:14:03 lr 0.000017	 wd 0.0500	time 0.4487 (0.4683)	loss 0.9179 (1.0971)	grad_norm 2.0975 (2.3710)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:25:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:13:13 lr 0.000017	 wd 0.0500	time 0.4517 (0.4664)	loss 0.7100 (1.0990)	grad_norm 4.1280 (2.3416)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:26:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:12:24 lr 0.000017	 wd 0.0500	time 0.4467 (0.4649)	loss 0.7779 (1.0994)	grad_norm 1.8669 (2.3438)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:27:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:11:36 lr 0.000018	 wd 0.0500	time 0.4496 (0.4637)	loss 1.3901 (1.1002)	grad_norm 1.5066 (2.3153)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:27:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:10:48 lr 0.000018	 wd 0.0500	time 0.4467 (0.4628)	loss 1.3880 (1.1013)	grad_norm 1.4075 (2.2901)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:28:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:10:01 lr 0.000018	 wd 0.0500	time 0.4525 (0.4620)	loss 1.2124 (1.0993)	grad_norm 3.1605 (2.2734)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:29:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:09:14 lr 0.000018	 wd 0.0500	time 0.4503 (0.4613)	loss 0.9007 (1.0986)	grad_norm 1.3281 (2.2464)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:30:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:08:27 lr 0.000018	 wd 0.0500	time 0.4497 (0.4607)	loss 1.3461 (1.0986)	grad_norm 2.5093 (2.2501)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:30:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:07:41 lr 0.000018	 wd 0.0500	time 0.4513 (0.4602)	loss 1.1156 (1.0985)	grad_norm 1.7753 (2.2441)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:31:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:06:54 lr 0.000019	 wd 0.0500	time 0.4513 (0.4598)	loss 1.2677 (1.0971)	grad_norm 1.1592 (2.2313)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:32:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:06:08 lr 0.000019	 wd 0.0500	time 0.4513 (0.4594)	loss 1.0925 (1.0970)	grad_norm 1.2755 (2.2149)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:33:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:05:22 lr 0.000019	 wd 0.0500	time 0.4516 (0.4591)	loss 1.4305 (1.0982)	grad_norm 1.4274 (2.1904)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 01:33:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:04:36 lr 0.000019	 wd 0.0500	time 0.4498 (0.4588)	loss 1.2553 (1.0982)	grad_norm 2.1580 (2.1793)	loss_scale 2048.0000 (1060.6291)	mem 17147MB
[2024-08-02 01:34:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:03:50 lr 0.000019	 wd 0.0500	time 0.4465 (0.4585)	loss 0.7664 (1.0972)	grad_norm 1.4086 (2.1582)	loss_scale 2048.0000 (1109.9730)	mem 17147MB
[2024-08-02 01:35:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:03:04 lr 0.000019	 wd 0.0500	time 0.4494 (0.4583)	loss 0.8906 (1.0958)	grad_norm 5.6145 (2.1722)	loss_scale 2048.0000 (1154.6197)	mem 17147MB
[2024-08-02 01:36:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:02:18 lr 0.000020	 wd 0.0500	time 0.4419 (0.4580)	loss 0.8723 (1.0959)	grad_norm 1.7448 (2.1683)	loss_scale 2048.0000 (1195.2095)	mem 17147MB
[2024-08-02 01:36:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.4484 (0.4578)	loss 0.7329 (1.0960)	grad_norm 3.2040 (2.1589)	loss_scale 2048.0000 (1232.2712)	mem 17147MB
[2024-08-02 01:37:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:46 lr 0.000020	 wd 0.0500	time 0.4511 (0.4576)	loss 0.6750 (1.0953)	grad_norm 1.7324 (2.1543)	loss_scale 2048.0000 (1266.2457)	mem 17147MB
[2024-08-02 01:38:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.4459 (0.4575)	loss 1.0377 (1.0957)	grad_norm 1.4246 (2.1571)	loss_scale 2048.0000 (1297.5034)	mem 17147MB
[2024-08-02 01:38:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 4 training takes 0:19:07
[2024-08-02 01:38:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.271 (11.271)	Loss 0.4607 (0.4607)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 01:39:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.812 Acc@5 98.064
[2024-08-02 01:39:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 01:39:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-02 01:39:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:31:10 lr 0.000020	 wd 0.0500	time 12.2584 (12.2584)	loss 1.2960 (1.2960)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:40:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:22:39 lr 0.000020	 wd 0.0500	time 0.4451 (0.5661)	loss 0.9482 (1.1218)	grad_norm 1.9867 (3.0163)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:40:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:19:30 lr 0.000020	 wd 0.0500	time 0.4438 (0.5084)	loss 1.1088 (1.0895)	grad_norm 1.3620 (2.4773)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:41:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:17:56 lr 0.000020	 wd 0.0500	time 0.4505 (0.4890)	loss 0.7678 (1.0833)	grad_norm 1.6251 (2.2637)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:42:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:16:48 lr 0.000020	 wd 0.0500	time 0.4469 (0.4795)	loss 1.3769 (1.0906)	grad_norm 2.0666 (2.2456)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:43:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:15:48 lr 0.000020	 wd 0.0500	time 0.4476 (0.4740)	loss 0.9113 (1.0889)	grad_norm 2.2116 (2.1782)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:43:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:14:54 lr 0.000020	 wd 0.0500	time 0.4504 (0.4703)	loss 0.8428 (1.0891)	grad_norm 1.8089 (2.1810)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:44:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:14:03 lr 0.000020	 wd 0.0500	time 0.4478 (0.4678)	loss 0.8715 (1.0890)	grad_norm 1.5321 (2.1462)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:45:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:13:13 lr 0.000020	 wd 0.0500	time 0.4485 (0.4660)	loss 1.3939 (1.0868)	grad_norm 1.8496 (2.1116)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:46:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:12:24 lr 0.000020	 wd 0.0500	time 0.4467 (0.4646)	loss 0.9614 (1.0863)	grad_norm 1.9136 (2.1401)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:46:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:11:36 lr 0.000020	 wd 0.0500	time 0.4466 (0.4635)	loss 1.0599 (1.0850)	grad_norm 1.4775 (2.1408)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:47:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:10:48 lr 0.000020	 wd 0.0500	time 0.4445 (0.4625)	loss 1.1224 (1.0861)	grad_norm 2.4539 (2.1193)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:48:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:10:01 lr 0.000020	 wd 0.0500	time 0.4405 (0.4617)	loss 1.3616 (1.0852)	grad_norm 1.8272 (2.1129)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:49:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:09:14 lr 0.000020	 wd 0.0500	time 0.4494 (0.4611)	loss 0.7819 (1.0862)	grad_norm 2.9231 (2.1108)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:49:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:08:27 lr 0.000020	 wd 0.0500	time 0.4464 (0.4605)	loss 1.2263 (1.0897)	grad_norm 1.7208 (2.1011)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:50:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:07:40 lr 0.000020	 wd 0.0500	time 0.4515 (0.4600)	loss 0.7101 (1.0915)	grad_norm 1.5968 (2.0895)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:51:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:06:54 lr 0.000020	 wd 0.0500	time 0.4489 (0.4596)	loss 1.2591 (1.0946)	grad_norm 1.3845 (2.0880)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:52:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:06:08 lr 0.000020	 wd 0.0500	time 0.4507 (0.4592)	loss 0.7560 (1.0950)	grad_norm 1.9950 (2.0846)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:52:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:05:22 lr 0.000020	 wd 0.0500	time 0.4475 (0.4589)	loss 0.7732 (1.0926)	grad_norm 2.7433 (2.0854)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:53:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:04:36 lr 0.000020	 wd 0.0500	time 0.4463 (0.4586)	loss 1.0812 (1.0921)	grad_norm 1.5216 (2.0706)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:54:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:03:50 lr 0.000020	 wd 0.0500	time 0.4514 (0.4583)	loss 1.4817 (1.0930)	grad_norm 1.8923 (2.0605)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:55:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:03:04 lr 0.000020	 wd 0.0500	time 0.4529 (0.4580)	loss 1.4547 (1.0930)	grad_norm 2.0355 (2.0662)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:55:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:02:18 lr 0.000020	 wd 0.0500	time 0.4487 (0.4578)	loss 1.2212 (1.0934)	grad_norm 1.4068 (2.0586)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:56:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.4456 (0.4576)	loss 1.0912 (1.0922)	grad_norm 3.5257 (2.0641)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:57:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:46 lr 0.000020	 wd 0.0500	time 0.4507 (0.4574)	loss 0.8662 (1.0917)	grad_norm 1.3993 (2.0655)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:58:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.4461 (0.4572)	loss 1.2969 (1.0926)	grad_norm 1.8073 (2.0672)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:58:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 5 training takes 0:19:06
[2024-08-02 01:58:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.973 (12.973)	Loss 0.4751 (0.4751)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 01:58:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.820 Acc@5 98.084
[2024-08-02 01:58:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 01:58:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-02 01:58:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:42:16 lr 0.000020	 wd 0.0500	time 12.5247 (12.5247)	loss 1.1779 (1.1779)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 01:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:22:45 lr 0.000020	 wd 0.0500	time 0.4401 (0.5686)	loss 0.9111 (1.1194)	grad_norm 3.1803 (2.3077)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:00:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:19:32 lr 0.000020	 wd 0.0500	time 0.4447 (0.5094)	loss 0.8632 (1.1017)	grad_norm 2.5542 (2.1769)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:01:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:17:58 lr 0.000020	 wd 0.0500	time 0.4450 (0.4896)	loss 0.8731 (1.0980)	grad_norm 1.5808 (2.1506)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:01:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:16:48 lr 0.000020	 wd 0.0500	time 0.4484 (0.4799)	loss 0.6377 (1.0963)	grad_norm 2.5108 (2.0990)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:02:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:15:49 lr 0.000020	 wd 0.0500	time 0.4499 (0.4742)	loss 1.1973 (1.0968)	grad_norm 1.2330 (2.0947)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:03:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:14:54 lr 0.000020	 wd 0.0500	time 0.4476 (0.4705)	loss 0.7867 (1.0890)	grad_norm 3.8555 (2.1177)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:04:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:14:03 lr 0.000020	 wd 0.0500	time 0.4509 (0.4680)	loss 1.3268 (1.0915)	grad_norm 1.9288 (2.1640)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:04:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:13:13 lr 0.000020	 wd 0.0500	time 0.4469 (0.4660)	loss 0.7228 (1.0966)	grad_norm 1.5225 (2.1262)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 02:05:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:12:24 lr 0.000020	 wd 0.0500	time 0.4462 (0.4646)	loss 1.5356 (1.0922)	grad_norm 1.3294 (2.1272)	loss_scale 4096.0000 (2211.6582)	mem 17147MB
[2024-08-02 02:06:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:11:36 lr 0.000020	 wd 0.0500	time 0.4512 (0.4635)	loss 0.9591 (1.0920)	grad_norm 1.6473 (nan)	loss_scale 2048.0000 (2387.6284)	mem 17147MB
[2024-08-02 02:07:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:10:48 lr 0.000020	 wd 0.0500	time 0.4492 (0.4625)	loss 0.7515 (1.0876)	grad_norm 1.6417 (nan)	loss_scale 2048.0000 (2356.7811)	mem 17147MB
[2024-08-02 02:07:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:10:01 lr 0.000020	 wd 0.0500	time 0.4508 (0.4617)	loss 1.2275 (1.0845)	grad_norm 1.4392 (nan)	loss_scale 2048.0000 (2331.0708)	mem 17147MB
[2024-08-02 02:08:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:09:14 lr 0.000020	 wd 0.0500	time 0.4512 (0.4610)	loss 1.0528 (1.0836)	grad_norm 2.5228 (nan)	loss_scale 2048.0000 (2309.3128)	mem 17147MB
[2024-08-02 02:09:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:08:27 lr 0.000020	 wd 0.0500	time 0.4483 (0.4605)	loss 0.9336 (1.0830)	grad_norm 1.5060 (nan)	loss_scale 2048.0000 (2290.6610)	mem 17147MB
[2024-08-02 02:10:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:07:40 lr 0.000020	 wd 0.0500	time 0.4496 (0.4600)	loss 0.6414 (1.0828)	grad_norm 2.5945 (nan)	loss_scale 2048.0000 (2274.4943)	mem 17147MB
[2024-08-02 02:10:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:06:54 lr 0.000020	 wd 0.0500	time 0.4486 (0.4595)	loss 1.2703 (1.0808)	grad_norm 1.3077 (nan)	loss_scale 2048.0000 (2260.3473)	mem 17147MB
[2024-08-02 02:11:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:06:08 lr 0.000020	 wd 0.0500	time 0.4491 (0.4591)	loss 1.2913 (1.0791)	grad_norm 1.7223 (nan)	loss_scale 2048.0000 (2247.8636)	mem 17147MB
[2024-08-02 02:12:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:05:22 lr 0.000020	 wd 0.0500	time 0.4472 (0.4588)	loss 1.1250 (1.0793)	grad_norm 1.3839 (nan)	loss_scale 2048.0000 (2236.7662)	mem 17147MB
[2024-08-02 02:13:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:04:36 lr 0.000020	 wd 0.0500	time 0.4487 (0.4585)	loss 1.3111 (1.0815)	grad_norm 1.5581 (nan)	loss_scale 2048.0000 (2226.8364)	mem 17147MB
[2024-08-02 02:14:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:03:50 lr 0.000020	 wd 0.0500	time 0.4462 (0.4582)	loss 0.8672 (1.0821)	grad_norm 4.7328 (nan)	loss_scale 2048.0000 (2217.8991)	mem 17147MB
[2024-08-02 02:14:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:03:04 lr 0.000020	 wd 0.0500	time 0.4502 (0.4580)	loss 1.0510 (1.0825)	grad_norm 3.5561 (nan)	loss_scale 2048.0000 (2209.8125)	mem 17147MB
[2024-08-02 02:15:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:02:18 lr 0.000020	 wd 0.0500	time 0.4451 (0.4578)	loss 0.7517 (1.0820)	grad_norm 2.0254 (nan)	loss_scale 1024.0000 (2163.3803)	mem 17147MB
[2024-08-02 02:16:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.4504 (0.4575)	loss 0.8838 (1.0840)	grad_norm 2.0115 (nan)	loss_scale 1024.0000 (2113.8635)	mem 17147MB
[2024-08-02 02:17:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:46 lr 0.000020	 wd 0.0500	time 0.4498 (0.4574)	loss 1.3408 (1.0830)	grad_norm 3.7501 (nan)	loss_scale 1024.0000 (2068.4715)	mem 17147MB
[2024-08-02 02:17:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.4465 (0.4572)	loss 1.3404 (1.0824)	grad_norm 1.7982 (nan)	loss_scale 1024.0000 (2026.7093)	mem 17147MB
[2024-08-02 02:17:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 6 training takes 0:19:06
[2024-08-02 02:18:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.146 (12.146)	Loss 0.4717 (0.4717)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 02:18:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.862 Acc@5 98.052
[2024-08-02 02:18:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 02:18:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-02 02:18:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:18:30 lr 0.000020	 wd 0.0500	time 11.9547 (11.9547)	loss 0.8263 (0.8263)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:19:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:22:33 lr 0.000020	 wd 0.0500	time 0.4400 (0.5636)	loss 1.1230 (1.0673)	grad_norm 2.0429 (2.4307)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:20:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:19:27 lr 0.000020	 wd 0.0500	time 0.4467 (0.5071)	loss 1.0222 (1.0863)	grad_norm 1.6665 (2.2538)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:20:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:17:55 lr 0.000020	 wd 0.0500	time 0.4437 (0.4883)	loss 1.4104 (1.0850)	grad_norm 8.0067 (2.4942)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:21:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:16:46 lr 0.000020	 wd 0.0500	time 0.4387 (0.4790)	loss 0.7697 (1.0883)	grad_norm 1.7264 (2.3527)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:22:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:15:47 lr 0.000020	 wd 0.0500	time 0.4472 (0.4735)	loss 1.3098 (1.0869)	grad_norm 1.5945 (2.3047)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:23:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:14:53 lr 0.000020	 wd 0.0500	time 0.4429 (0.4700)	loss 1.1975 (1.0914)	grad_norm 1.9179 (2.2228)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:23:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:14:02 lr 0.000020	 wd 0.0500	time 0.4481 (0.4675)	loss 0.9975 (1.0859)	grad_norm 1.3372 (2.2172)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:24:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:13:12 lr 0.000020	 wd 0.0500	time 0.4511 (0.4657)	loss 0.8731 (1.0833)	grad_norm 2.3684 (2.1974)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:25:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:12:23 lr 0.000020	 wd 0.0500	time 0.4476 (0.4643)	loss 1.4337 (1.0862)	grad_norm 1.5419 (2.1912)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:26:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:11:35 lr 0.000020	 wd 0.0500	time 0.4478 (0.4632)	loss 0.7874 (1.0863)	grad_norm 1.4456 (2.1805)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:26:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:10:48 lr 0.000020	 wd 0.0500	time 0.4499 (0.4623)	loss 1.1708 (1.0843)	grad_norm 4.0351 (2.1934)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:27:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:10:00 lr 0.000020	 wd 0.0500	time 0.4481 (0.4615)	loss 1.1920 (1.0810)	grad_norm 1.9289 (2.1762)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:28:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:09:13 lr 0.000020	 wd 0.0500	time 0.4430 (0.4609)	loss 1.2352 (1.0811)	grad_norm 4.0700 (2.1653)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:29:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:08:27 lr 0.000019	 wd 0.0500	time 0.4480 (0.4603)	loss 1.1669 (1.0822)	grad_norm 2.3043 (2.1571)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:29:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:07:40 lr 0.000019	 wd 0.0500	time 0.4497 (0.4599)	loss 0.7461 (1.0832)	grad_norm 6.5626 (2.1417)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:30:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:06:54 lr 0.000019	 wd 0.0500	time 0.4502 (0.4594)	loss 1.1294 (1.0840)	grad_norm 1.9396 (2.1513)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:31:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:06:08 lr 0.000019	 wd 0.0500	time 0.4493 (0.4591)	loss 1.1804 (1.0836)	grad_norm 2.1753 (2.1436)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:32:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:05:22 lr 0.000019	 wd 0.0500	time 0.4493 (0.4588)	loss 0.7123 (1.0856)	grad_norm 1.4335 (2.1441)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:32:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:04:36 lr 0.000019	 wd 0.0500	time 0.4478 (0.4585)	loss 1.1105 (1.0861)	grad_norm 1.9302 (2.1305)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:33:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:03:50 lr 0.000019	 wd 0.0500	time 0.4426 (0.4582)	loss 1.1602 (1.0868)	grad_norm 2.2132 (2.1197)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:34:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:03:04 lr 0.000019	 wd 0.0500	time 0.4475 (0.4580)	loss 1.4151 (1.0892)	grad_norm 2.9401 (2.1229)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:35:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:02:18 lr 0.000019	 wd 0.0500	time 0.4487 (0.4578)	loss 1.1896 (1.0874)	grad_norm 2.0892 (2.1172)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:35:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:32 lr 0.000019	 wd 0.0500	time 0.4508 (0.4576)	loss 1.3478 (1.0868)	grad_norm 1.3934 (2.1042)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:36:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:46 lr 0.000019	 wd 0.0500	time 0.4498 (0.4574)	loss 1.2742 (1.0868)	grad_norm 1.4958 (2.1045)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:37:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.4429 (0.4572)	loss 1.4214 (1.0862)	grad_norm 1.5511 (2.1121)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:37:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 7 training takes 0:19:06
[2024-08-02 02:37:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.477 (12.477)	Loss 0.4492 (0.4492)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 02:38:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.800 Acc@5 98.078
[2024-08-02 02:38:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 02:38:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.86%
[2024-08-02 02:38:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:17:44 lr 0.000019	 wd 0.0500	time 11.9361 (11.9361)	loss 1.1726 (1.1726)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:39:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:22:37 lr 0.000019	 wd 0.0500	time 0.4467 (0.5652)	loss 0.8435 (1.1397)	grad_norm 1.5715 (2.2984)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:39:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:19:28 lr 0.000019	 wd 0.0500	time 0.4460 (0.5077)	loss 0.7589 (1.1161)	grad_norm 1.8963 (2.2215)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:40:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:17:55 lr 0.000019	 wd 0.0500	time 0.4471 (0.4884)	loss 1.3830 (1.1020)	grad_norm 1.4228 (2.1464)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:41:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:16:46 lr 0.000019	 wd 0.0500	time 0.4457 (0.4790)	loss 0.8143 (1.0998)	grad_norm 1.8902 (2.1144)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:42:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:15:48 lr 0.000019	 wd 0.0500	time 0.4494 (0.4735)	loss 0.7504 (1.0907)	grad_norm 1.5987 (2.0745)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:42:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:14:53 lr 0.000019	 wd 0.0500	time 0.4499 (0.4699)	loss 0.7994 (1.0906)	grad_norm 1.3826 (2.0486)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:43:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:14:02 lr 0.000019	 wd 0.0500	time 0.4452 (0.4675)	loss 1.2054 (1.0906)	grad_norm 1.7618 (2.0414)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:44:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:13:12 lr 0.000019	 wd 0.0500	time 0.4488 (0.4657)	loss 1.2627 (1.0925)	grad_norm 1.5122 (2.0571)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:45:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:12:23 lr 0.000019	 wd 0.0500	time 0.4444 (0.4644)	loss 0.8802 (1.0887)	grad_norm 2.7838 (2.0996)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:45:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:11:35 lr 0.000019	 wd 0.0500	time 0.4511 (0.4633)	loss 1.2699 (1.0906)	grad_norm 9.4814 (2.1528)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:46:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:10:48 lr 0.000019	 wd 0.0500	time 0.4448 (0.4624)	loss 1.2133 (1.0878)	grad_norm 1.7272 (2.1422)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:47:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:10:01 lr 0.000019	 wd 0.0500	time 0.4483 (0.4616)	loss 1.0096 (1.0885)	grad_norm 1.4050 (2.1356)	loss_scale 2048.0000 (1099.0308)	mem 17147MB
[2024-08-02 02:48:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:09:14 lr 0.000019	 wd 0.0500	time 0.4496 (0.4610)	loss 0.7830 (1.0882)	grad_norm 1.8563 (2.1986)	loss_scale 2048.0000 (1171.9723)	mem 17147MB
[2024-08-02 02:48:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:08:27 lr 0.000019	 wd 0.0500	time 0.4474 (0.4604)	loss 1.2573 (1.0895)	grad_norm 1.6642 (2.1977)	loss_scale 2048.0000 (1234.5011)	mem 17147MB
[2024-08-02 02:49:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:07:40 lr 0.000019	 wd 0.0500	time 0.4442 (0.4600)	loss 1.0121 (1.0903)	grad_norm 1.8381 (2.2061)	loss_scale 2048.0000 (1288.6982)	mem 17147MB
[2024-08-02 02:50:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:06:54 lr 0.000019	 wd 0.0500	time 0.4512 (0.4595)	loss 1.3099 (1.0921)	grad_norm 2.0940 (2.1900)	loss_scale 2048.0000 (1336.1249)	mem 17147MB
[2024-08-02 02:51:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:06:08 lr 0.000019	 wd 0.0500	time 0.4444 (0.4592)	loss 0.7311 (1.0941)	grad_norm 1.8859 (2.2114)	loss_scale 2048.0000 (1377.9753)	mem 17147MB
[2024-08-02 02:51:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:05:22 lr 0.000019	 wd 0.0500	time 0.4463 (0.4588)	loss 0.7240 (1.0924)	grad_norm 1.9497 (2.1969)	loss_scale 2048.0000 (1415.1782)	mem 17147MB
[2024-08-02 02:52:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:04:36 lr 0.000019	 wd 0.0500	time 0.4520 (0.4586)	loss 1.1865 (1.0938)	grad_norm 1.7463 (2.1860)	loss_scale 2048.0000 (1448.4671)	mem 17147MB
[2024-08-02 02:53:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:03:50 lr 0.000019	 wd 0.0500	time 0.4480 (0.4583)	loss 0.7214 (1.0927)	grad_norm 1.6775 (nan)	loss_scale 1024.0000 (1430.3248)	mem 17147MB
[2024-08-02 02:54:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:03:04 lr 0.000019	 wd 0.0500	time 0.4492 (0.4581)	loss 0.8508 (1.0934)	grad_norm 1.9107 (nan)	loss_scale 1024.0000 (1410.9852)	mem 17147MB
[2024-08-02 02:54:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:02:18 lr 0.000019	 wd 0.0500	time 0.4488 (0.4579)	loss 1.1437 (1.0921)	grad_norm 2.1309 (nan)	loss_scale 1024.0000 (1393.4030)	mem 17147MB
[2024-08-02 02:55:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:32 lr 0.000019	 wd 0.0500	time 0.4476 (0.4577)	loss 1.1888 (1.0905)	grad_norm 2.0428 (nan)	loss_scale 1024.0000 (1377.3490)	mem 17147MB
[2024-08-02 02:56:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:46 lr 0.000019	 wd 0.0500	time 0.4482 (0.4575)	loss 1.0183 (1.0907)	grad_norm 1.5105 (nan)	loss_scale 1024.0000 (1362.6322)	mem 17147MB
[2024-08-02 02:57:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.4498 (0.4573)	loss 0.7714 (1.0904)	grad_norm 2.9348 (nan)	loss_scale 1024.0000 (1349.0924)	mem 17147MB
[2024-08-02 02:57:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 8 training takes 0:19:06
[2024-08-02 02:57:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.825 (12.825)	Loss 0.4534 (0.4534)	Acc@1 93.359 (93.359)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 02:57:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.910 Acc@5 98.086
[2024-08-02 02:57:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 02:57:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-02 02:57:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 02:57:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 02:57:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][0/2502]	eta 8:41:10 lr 0.000019	 wd 0.0500	time 12.4983 (12.4983)	loss 1.2197 (1.2197)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:58:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:22:45 lr 0.000019	 wd 0.0500	time 0.4482 (0.5684)	loss 1.1645 (1.0753)	grad_norm 1.8934 (2.1157)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 02:59:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:19:32 lr 0.000019	 wd 0.0500	time 0.4497 (0.5094)	loss 0.8837 (1.0967)	grad_norm 1.5850 (1.9999)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:00:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:17:58 lr 0.000019	 wd 0.0500	time 0.4429 (0.4898)	loss 1.3571 (1.1024)	grad_norm 1.7750 (2.1711)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:00:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:16:48 lr 0.000019	 wd 0.0500	time 0.4465 (0.4799)	loss 0.8578 (1.0837)	grad_norm 2.2122 (2.1350)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:01:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:15:49 lr 0.000019	 wd 0.0500	time 0.4496 (0.4741)	loss 0.6391 (1.0793)	grad_norm 3.8156 (2.0923)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:02:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:14:54 lr 0.000019	 wd 0.0500	time 0.4485 (0.4704)	loss 0.7402 (1.0824)	grad_norm 2.3833 (2.0711)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:03:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:14:03 lr 0.000019	 wd 0.0500	time 0.4474 (0.4678)	loss 1.3447 (1.0851)	grad_norm 1.1028 (2.0696)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:03:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:13:12 lr 0.000019	 wd 0.0500	time 0.4443 (0.4658)	loss 1.3448 (1.0868)	grad_norm 1.2135 (2.0742)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:04:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:12:23 lr 0.000019	 wd 0.0500	time 0.4502 (0.4644)	loss 1.2867 (1.0872)	grad_norm 2.7569 (2.0535)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:05:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:11:35 lr 0.000019	 wd 0.0500	time 0.4460 (0.4632)	loss 1.2174 (1.0876)	grad_norm 1.5253 (2.0776)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:06:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:10:48 lr 0.000018	 wd 0.0500	time 0.4472 (0.4623)	loss 1.3279 (1.0836)	grad_norm 1.9450 (2.0827)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:06:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:10:00 lr 0.000018	 wd 0.0500	time 0.4457 (0.4615)	loss 0.9175 (1.0839)	grad_norm 4.6479 (2.0862)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:07:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:09:13 lr 0.000018	 wd 0.0500	time 0.4504 (0.4608)	loss 1.2768 (1.0849)	grad_norm 1.8672 (2.1076)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:08:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:08:27 lr 0.000018	 wd 0.0500	time 0.4496 (0.4603)	loss 0.8896 (1.0829)	grad_norm 2.0666 (2.1251)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:09:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:07:40 lr 0.000018	 wd 0.0500	time 0.4436 (0.4598)	loss 0.8145 (1.0854)	grad_norm 2.9559 (nan)	loss_scale 512.0000 (1006.2625)	mem 17147MB
[2024-08-02 03:10:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:06:54 lr 0.000018	 wd 0.0500	time 0.4500 (0.4594)	loss 0.8447 (1.0834)	grad_norm 1.6708 (nan)	loss_scale 512.0000 (975.3904)	mem 17147MB
[2024-08-02 03:10:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:06:08 lr 0.000018	 wd 0.0500	time 0.4487 (0.4591)	loss 1.3564 (1.0837)	grad_norm 1.8015 (nan)	loss_scale 512.0000 (948.1481)	mem 17147MB
[2024-08-02 03:11:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:05:22 lr 0.000018	 wd 0.0500	time 0.4484 (0.4588)	loss 1.1806 (1.0829)	grad_norm 1.5962 (nan)	loss_scale 512.0000 (923.9311)	mem 17147MB
[2024-08-02 03:12:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:04:35 lr 0.000018	 wd 0.0500	time 0.4494 (0.4585)	loss 1.3148 (1.0819)	grad_norm 1.7035 (nan)	loss_scale 512.0000 (902.2620)	mem 17147MB
[2024-08-02 03:13:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:03:50 lr 0.000018	 wd 0.0500	time 0.4460 (0.4582)	loss 1.1138 (1.0823)	grad_norm 8.1292 (nan)	loss_scale 512.0000 (882.7586)	mem 17147MB
[2024-08-02 03:13:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:03:04 lr 0.000018	 wd 0.0500	time 0.4510 (0.4580)	loss 1.2783 (1.0836)	grad_norm 2.1257 (nan)	loss_scale 512.0000 (865.1119)	mem 17147MB
[2024-08-02 03:14:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:18 lr 0.000018	 wd 0.0500	time 0.4488 (0.4577)	loss 1.1599 (1.0844)	grad_norm 2.5101 (nan)	loss_scale 512.0000 (849.0686)	mem 17147MB
[2024-08-02 03:15:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:32 lr 0.000018	 wd 0.0500	time 0.4481 (0.4575)	loss 0.7665 (1.0845)	grad_norm 2.9590 (nan)	loss_scale 512.0000 (834.4198)	mem 17147MB
[2024-08-02 03:16:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:46 lr 0.000018	 wd 0.0500	time 0.4494 (0.4574)	loss 1.0748 (1.0850)	grad_norm 1.9911 (nan)	loss_scale 512.0000 (820.9913)	mem 17147MB
[2024-08-02 03:16:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.4499 (0.4572)	loss 1.3178 (1.0845)	grad_norm 1.6004 (nan)	loss_scale 512.0000 (808.6365)	mem 17147MB
[2024-08-02 03:16:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 9 training takes 0:19:06
[2024-08-02 03:17:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.541 (12.541)	Loss 0.4553 (0.4553)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 03:17:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.856 Acc@5 98.096
[2024-08-02 03:17:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 03:17:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-02 03:17:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][0/2502]	eta 8:21:37 lr 0.000018	 wd 0.0500	time 12.0295 (12.0295)	loss 1.3271 (1.3271)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:18:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:22:35 lr 0.000018	 wd 0.0500	time 0.4483 (0.5645)	loss 0.7813 (1.0927)	grad_norm 2.4335 (2.1913)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:19:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:19:27 lr 0.000018	 wd 0.0500	time 0.4438 (0.5072)	loss 1.2867 (1.0804)	grad_norm 3.0650 (2.0321)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:19:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:17:55 lr 0.000018	 wd 0.0500	time 0.4459 (0.4882)	loss 1.2788 (1.0804)	grad_norm 2.2885 (2.0148)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:20:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:16:46 lr 0.000018	 wd 0.0500	time 0.4462 (0.4788)	loss 0.7151 (1.0879)	grad_norm 1.5672 (2.0216)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:21:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:15:47 lr 0.000018	 wd 0.0500	time 0.4494 (0.4733)	loss 0.6777 (1.0850)	grad_norm 1.3908 (2.1339)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:22:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:14:53 lr 0.000018	 wd 0.0500	time 0.4493 (0.4698)	loss 1.1674 (1.0915)	grad_norm 2.8484 (2.1519)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:22:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:14:02 lr 0.000018	 wd 0.0500	time 0.4467 (0.4673)	loss 1.2015 (1.0892)	grad_norm 1.4084 (2.1338)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:23:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:13:12 lr 0.000018	 wd 0.0500	time 0.4477 (0.4655)	loss 1.1951 (1.0917)	grad_norm 1.9936 (2.1460)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:24:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:12:23 lr 0.000018	 wd 0.0500	time 0.4510 (0.4640)	loss 1.2550 (1.0903)	grad_norm 5.4427 (2.1323)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:25:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:11:35 lr 0.000018	 wd 0.0500	time 0.4496 (0.4629)	loss 1.2296 (1.0914)	grad_norm 1.8876 (2.1046)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:25:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:10:47 lr 0.000018	 wd 0.0500	time 0.4448 (0.4620)	loss 0.9467 (1.0981)	grad_norm 2.8310 (2.1269)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:26:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:10:00 lr 0.000018	 wd 0.0500	time 0.4518 (0.4613)	loss 1.3697 (1.0951)	grad_norm 1.3704 (2.1396)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:27:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:09:13 lr 0.000018	 wd 0.0500	time 0.4474 (0.4606)	loss 1.1388 (1.0933)	grad_norm 3.1423 (2.1563)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:28:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:08:27 lr 0.000018	 wd 0.0500	time 0.4502 (0.4601)	loss 1.3633 (1.0920)	grad_norm 1.6607 (2.1586)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:28:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:07:40 lr 0.000018	 wd 0.0500	time 0.4435 (0.4596)	loss 1.0942 (1.0903)	grad_norm 1.7206 (2.1889)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:29:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:06:54 lr 0.000018	 wd 0.0500	time 0.4509 (0.4592)	loss 1.0872 (1.0886)	grad_norm 1.7779 (2.1915)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:30:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:06:07 lr 0.000018	 wd 0.0500	time 0.4484 (0.4588)	loss 1.2030 (1.0904)	grad_norm 1.8372 (2.2041)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:31:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:05:21 lr 0.000018	 wd 0.0500	time 0.4502 (0.4585)	loss 0.8931 (1.0887)	grad_norm 1.7557 (2.2063)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:31:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:04:35 lr 0.000018	 wd 0.0500	time 0.4502 (0.4582)	loss 1.0010 (1.0875)	grad_norm 3.2370 (2.2003)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:32:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:03:49 lr 0.000017	 wd 0.0500	time 0.4468 (0.4579)	loss 1.2225 (1.0905)	grad_norm 1.6358 (2.2012)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:33:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:03:03 lr 0.000017	 wd 0.0500	time 0.4480 (0.4576)	loss 0.9822 (1.0921)	grad_norm 1.6386 (2.1907)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:34:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:02:18 lr 0.000017	 wd 0.0500	time 0.4488 (0.4574)	loss 1.2003 (1.0930)	grad_norm 1.8947 (2.2636)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:34:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:32 lr 0.000017	 wd 0.0500	time 0.4486 (0.4572)	loss 1.1855 (1.0911)	grad_norm 2.2070 (2.2705)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:35:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:46 lr 0.000017	 wd 0.0500	time 0.4513 (0.4570)	loss 1.3319 (1.0898)	grad_norm 2.2608 (2.2583)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:36:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.4488 (0.4568)	loss 0.7641 (1.0889)	grad_norm 2.2236 (2.2550)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:36:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 10 training takes 0:19:05
[2024-08-02 03:36:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.848 (11.848)	Loss 0.4563 (0.4563)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 03:37:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.824 Acc@5 98.066
[2024-08-02 03:37:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-02 03:37:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-02 03:37:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:22:06 lr 0.000017	 wd 0.0500	time 12.0409 (12.0409)	loss 0.9036 (0.9036)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:38:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:22:36 lr 0.000017	 wd 0.0500	time 0.4451 (0.5646)	loss 1.2716 (1.0767)	grad_norm 1.5019 (2.1438)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:38:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:19:28 lr 0.000017	 wd 0.0500	time 0.4460 (0.5076)	loss 1.1716 (1.0864)	grad_norm 3.3095 (2.0886)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:39:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:17:55 lr 0.000017	 wd 0.0500	time 0.4464 (0.4883)	loss 0.8422 (1.0812)	grad_norm 1.3985 (2.1874)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:40:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:16:46 lr 0.000017	 wd 0.0500	time 0.4465 (0.4789)	loss 1.2941 (1.0831)	grad_norm 2.2870 (2.1609)	loss_scale 512.0000 (512.0000)	mem 17147MB
[2024-08-02 03:41:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:15:47 lr 0.000017	 wd 0.0500	time 0.4480 (0.4733)	loss 0.7202 (1.0817)	grad_norm 2.0533 (2.1671)	loss_scale 1024.0000 (569.2295)	mem 17147MB
[2024-08-02 03:41:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:14:53 lr 0.000017	 wd 0.0500	time 0.4485 (0.4697)	loss 1.4458 (1.0837)	grad_norm 2.9544 (2.1244)	loss_scale 1024.0000 (644.8985)	mem 17147MB
[2024-08-02 03:42:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:14:02 lr 0.000017	 wd 0.0500	time 0.4482 (0.4673)	loss 0.8517 (1.0809)	grad_norm 2.5065 (2.1281)	loss_scale 1024.0000 (698.9786)	mem 17147MB
[2024-08-02 03:43:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:13:12 lr 0.000017	 wd 0.0500	time 0.4524 (0.4655)	loss 1.3426 (1.0794)	grad_norm 1.2974 (2.0809)	loss_scale 1024.0000 (739.5556)	mem 17147MB
[2024-08-02 03:44:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:12:23 lr 0.000017	 wd 0.0500	time 0.4413 (0.4642)	loss 0.8158 (1.0811)	grad_norm 1.2664 (2.0625)	loss_scale 1024.0000 (771.1254)	mem 17147MB
[2024-08-02 03:44:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:11:35 lr 0.000017	 wd 0.0500	time 0.4532 (0.4630)	loss 1.1544 (1.0820)	grad_norm 1.5228 (2.0467)	loss_scale 1024.0000 (796.3876)	mem 17147MB
[2024-08-02 03:45:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:10:47 lr 0.000017	 wd 0.0500	time 0.4477 (0.4622)	loss 0.8709 (1.0803)	grad_norm 1.5066 (2.0260)	loss_scale 1024.0000 (817.0609)	mem 17147MB
[2024-08-02 03:46:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:10:00 lr 0.000017	 wd 0.0500	time 0.4462 (0.4614)	loss 0.9190 (1.0768)	grad_norm 1.8254 (2.0403)	loss_scale 1024.0000 (834.2914)	mem 17147MB
[2024-08-02 03:47:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:09:13 lr 0.000017	 wd 0.0500	time 0.4478 (0.4607)	loss 1.1710 (1.0774)	grad_norm 2.1053 (2.0494)	loss_scale 1024.0000 (848.8732)	mem 17147MB
[2024-08-02 03:47:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:08:27 lr 0.000017	 wd 0.0500	time 0.4475 (0.4601)	loss 0.8023 (1.0783)	grad_norm 1.8255 (2.0391)	loss_scale 1024.0000 (861.3733)	mem 17147MB
[2024-08-02 03:48:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:07:40 lr 0.000017	 wd 0.0500	time 0.4490 (0.4597)	loss 1.3186 (1.0782)	grad_norm 1.4995 (2.0330)	loss_scale 1024.0000 (872.2079)	mem 17147MB
[2024-08-02 03:49:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:06:54 lr 0.000017	 wd 0.0500	time 0.4502 (0.4593)	loss 1.2093 (1.0794)	grad_norm 1.4400 (2.0271)	loss_scale 1024.0000 (881.6889)	mem 17147MB
[2024-08-02 03:50:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:06:08 lr 0.000017	 wd 0.0500	time 0.4470 (0.4589)	loss 0.9617 (1.0801)	grad_norm 2.1292 (2.1901)	loss_scale 1024.0000 (890.0553)	mem 17147MB
[2024-08-02 03:50:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:05:21 lr 0.000017	 wd 0.0500	time 0.4520 (0.4586)	loss 0.7756 (1.0791)	grad_norm 1.5746 (2.1850)	loss_scale 1024.0000 (897.4925)	mem 17147MB
[2024-08-02 03:51:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:04:35 lr 0.000017	 wd 0.0500	time 0.4513 (0.4583)	loss 0.9924 (1.0777)	grad_norm 2.4745 (2.1708)	loss_scale 1024.0000 (904.1473)	mem 17147MB
[2024-08-02 03:52:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:03:49 lr 0.000017	 wd 0.0500	time 0.4461 (0.4580)	loss 1.1011 (1.0781)	grad_norm 1.3425 (2.1768)	loss_scale 1024.0000 (910.1369)	mem 17147MB
[2024-08-02 03:53:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:03:04 lr 0.000017	 wd 0.0500	time 0.4523 (0.4578)	loss 1.2882 (1.0777)	grad_norm 2.1893 (2.1787)	loss_scale 1024.0000 (915.5564)	mem 17147MB
[2024-08-02 03:53:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:18 lr 0.000017	 wd 0.0500	time 0.4484 (0.4576)	loss 0.8286 (1.0791)	grad_norm 2.4705 (2.1701)	loss_scale 1024.0000 (920.4834)	mem 17147MB
[2024-08-02 03:54:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:32 lr 0.000016	 wd 0.0500	time 0.4415 (0.4574)	loss 1.1073 (1.0775)	grad_norm 2.4015 (2.1588)	loss_scale 1024.0000 (924.9822)	mem 17147MB
[2024-08-02 03:55:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:46 lr 0.000016	 wd 0.0500	time 0.4514 (0.4572)	loss 1.2089 (1.0788)	grad_norm 1.3331 (2.1535)	loss_scale 1024.0000 (929.1062)	mem 17147MB
[2024-08-02 03:56:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.4477 (0.4570)	loss 0.7726 (1.0770)	grad_norm 1.2502 (2.1501)	loss_scale 1024.0000 (932.9004)	mem 17147MB
[2024-08-02 03:56:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 11 training takes 0:19:05
[2024-08-02 03:56:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.129 (12.129)	Loss 0.4485 (0.4485)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 03:56:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.860 Acc@5 98.052
[2024-08-02 03:56:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 03:56:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.91%
[2024-08-02 03:56:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:26:31 lr 0.000016	 wd 0.0500	time 12.1471 (12.1471)	loss 0.8620 (0.8620)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:57:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:22:38 lr 0.000016	 wd 0.0500	time 0.4443 (0.5655)	loss 1.1753 (1.0752)	grad_norm 1.5069 (2.1327)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:58:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:19:29 lr 0.000016	 wd 0.0500	time 0.4486 (0.5081)	loss 1.1289 (1.0739)	grad_norm 1.6172 (2.2634)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:59:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:17:56 lr 0.000016	 wd 0.0500	time 0.4462 (0.4888)	loss 0.8602 (1.0820)	grad_norm 1.9451 (2.1342)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 03:59:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:16:47 lr 0.000016	 wd 0.0500	time 0.4463 (0.4792)	loss 1.1548 (1.0774)	grad_norm 1.7353 (2.1008)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:00:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:15:48 lr 0.000016	 wd 0.0500	time 0.4495 (0.4737)	loss 1.3183 (1.0759)	grad_norm 1.1921 (2.0672)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:01:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:14:54 lr 0.000016	 wd 0.0500	time 0.4475 (0.4702)	loss 1.3362 (1.0736)	grad_norm 9.4022 (2.0839)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:02:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:14:02 lr 0.000016	 wd 0.0500	time 0.4453 (0.4678)	loss 1.1742 (1.0716)	grad_norm 1.9553 (2.3803)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:02:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:13:12 lr 0.000016	 wd 0.0500	time 0.4463 (0.4658)	loss 1.2797 (1.0727)	grad_norm 1.7300 (2.3648)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:03:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:12:24 lr 0.000016	 wd 0.0500	time 0.4482 (0.4644)	loss 1.3104 (1.0701)	grad_norm 1.8655 (2.3869)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:04:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:11:35 lr 0.000016	 wd 0.0500	time 0.4469 (0.4633)	loss 1.2125 (1.0738)	grad_norm 1.8101 (2.3520)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:05:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:10:48 lr 0.000016	 wd 0.0500	time 0.4420 (0.4623)	loss 0.8244 (1.0758)	grad_norm 3.0053 (2.3265)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:05:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:10:00 lr 0.000016	 wd 0.0500	time 0.4457 (0.4615)	loss 1.2252 (1.0748)	grad_norm 1.4509 (2.3056)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:06:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:09:13 lr 0.000016	 wd 0.0500	time 0.4469 (0.4609)	loss 1.0013 (1.0762)	grad_norm 1.7780 (2.3069)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:07:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:08:27 lr 0.000016	 wd 0.0500	time 0.4519 (0.4603)	loss 1.2701 (1.0770)	grad_norm 1.6231 (2.2900)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:08:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:07:40 lr 0.000016	 wd 0.0500	time 0.4502 (0.4598)	loss 0.7491 (1.0737)	grad_norm 6.7486 (2.2873)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:08:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:06:54 lr 0.000016	 wd 0.0500	time 0.4482 (0.4594)	loss 1.3317 (1.0711)	grad_norm 1.2993 (2.2782)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:09:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:06:08 lr 0.000016	 wd 0.0500	time 0.4433 (0.4590)	loss 1.1217 (1.0736)	grad_norm 1.9147 (2.2686)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:10:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:05:21 lr 0.000016	 wd 0.0500	time 0.4453 (0.4586)	loss 0.7684 (1.0725)	grad_norm 1.6868 (2.2738)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:11:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:04:35 lr 0.000016	 wd 0.0500	time 0.4442 (0.4583)	loss 1.1175 (1.0730)	grad_norm 1.7874 (2.2646)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:12:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:03:49 lr 0.000016	 wd 0.0500	time 0.4493 (0.4581)	loss 0.9335 (1.0740)	grad_norm 2.0853 (2.2590)	loss_scale 2048.0000 (1053.6812)	mem 17147MB
[2024-08-02 04:12:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:03:04 lr 0.000016	 wd 0.0500	time 0.4512 (0.4578)	loss 0.7768 (1.0732)	grad_norm 1.6854 (2.2559)	loss_scale 2048.0000 (1101.0071)	mem 17147MB
[2024-08-02 04:13:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:02:18 lr 0.000016	 wd 0.0500	time 0.4500 (0.4576)	loss 0.7300 (1.0746)	grad_norm 2.5770 (2.2429)	loss_scale 2048.0000 (1144.0327)	mem 17147MB
[2024-08-02 04:14:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:32 lr 0.000015	 wd 0.0500	time 0.4501 (0.4574)	loss 1.0256 (1.0748)	grad_norm 1.8938 (2.2296)	loss_scale 2048.0000 (1183.3186)	mem 17147MB
[2024-08-02 04:15:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:46 lr 0.000015	 wd 0.0500	time 0.4481 (0.4573)	loss 1.3154 (1.0753)	grad_norm 1.3218 (2.2182)	loss_scale 2048.0000 (1219.3319)	mem 17147MB
[2024-08-02 04:15:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.4494 (0.4571)	loss 0.7192 (1.0745)	grad_norm 1.4080 (2.2232)	loss_scale 2048.0000 (1252.4654)	mem 17147MB
[2024-08-02 04:15:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 12 training takes 0:19:06
[2024-08-02 04:16:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.149 (12.149)	Loss 0.4636 (0.4636)	Acc@1 93.555 (93.555)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 04:16:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.942 Acc@5 98.070
[2024-08-02 04:16:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 04:16:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 04:16:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 04:16:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 04:16:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:44:22 lr 0.000015	 wd 0.0500	time 11.1360 (11.1360)	loss 1.2429 (1.2429)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:17:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:22:12 lr 0.000015	 wd 0.0500	time 0.4449 (0.5548)	loss 1.3052 (1.1332)	grad_norm 0.9562 (1.8652)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:18:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:19:17 lr 0.000015	 wd 0.0500	time 0.4465 (0.5028)	loss 1.2842 (1.1190)	grad_norm 1.5516 (1.9632)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:18:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:17:49 lr 0.000015	 wd 0.0500	time 0.4465 (0.4855)	loss 1.3215 (1.1184)	grad_norm 1.3796 (2.0346)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:19:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:16:42 lr 0.000015	 wd 0.0500	time 0.4483 (0.4768)	loss 1.3696 (1.1030)	grad_norm 2.6910 (1.9994)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:20:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:15:44 lr 0.000015	 wd 0.0500	time 0.4461 (0.4716)	loss 1.3216 (1.0960)	grad_norm 2.1256 (2.0034)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:21:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:14:50 lr 0.000015	 wd 0.0500	time 0.4409 (0.4683)	loss 1.1699 (1.0895)	grad_norm 1.4812 (2.0157)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:21:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:13:59 lr 0.000015	 wd 0.0500	time 0.4460 (0.4660)	loss 1.0099 (1.0833)	grad_norm 1.8172 (2.0479)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:22:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:13:10 lr 0.000015	 wd 0.0500	time 0.4479 (0.4643)	loss 1.3465 (1.0831)	grad_norm 1.6243 (2.0562)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:23:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:12:21 lr 0.000015	 wd 0.0500	time 0.4487 (0.4630)	loss 1.0563 (1.0824)	grad_norm 1.7581 (2.0690)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:24:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:11:33 lr 0.000015	 wd 0.0500	time 0.4520 (0.4619)	loss 1.2102 (1.0815)	grad_norm 1.3353 (2.0693)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:24:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:10:46 lr 0.000015	 wd 0.0500	time 0.4455 (0.4612)	loss 1.2428 (1.0783)	grad_norm 2.5380 (2.1094)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:25:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:09:59 lr 0.000015	 wd 0.0500	time 0.4444 (0.4604)	loss 1.1585 (1.0823)	grad_norm 1.8620 (2.2306)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:26:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:09:12 lr 0.000015	 wd 0.0500	time 0.4503 (0.4598)	loss 0.8043 (1.0819)	grad_norm 2.3615 (2.2186)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:27:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:08:26 lr 0.000015	 wd 0.0500	time 0.4526 (0.4594)	loss 1.3032 (1.0806)	grad_norm 1.2643 (2.1973)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:27:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:07:39 lr 0.000015	 wd 0.0500	time 0.4455 (0.4589)	loss 1.1177 (1.0808)	grad_norm 3.4209 (2.1730)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:28:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:06:53 lr 0.000015	 wd 0.0500	time 0.4477 (0.4585)	loss 0.9405 (1.0804)	grad_norm 1.4868 (2.1565)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:29:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:06:07 lr 0.000015	 wd 0.0500	time 0.4492 (0.4582)	loss 1.0266 (1.0812)	grad_norm 2.3285 (2.1613)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:30:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:05:21 lr 0.000015	 wd 0.0500	time 0.4488 (0.4579)	loss 1.3231 (1.0816)	grad_norm 1.6906 (2.1609)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:30:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:04:35 lr 0.000015	 wd 0.0500	time 0.4452 (0.4577)	loss 1.0321 (1.0827)	grad_norm 1.3297 (2.1597)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:31:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:03:49 lr 0.000015	 wd 0.0500	time 0.4484 (0.4575)	loss 1.2641 (1.0826)	grad_norm 1.5582 (2.1857)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:32:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:03:03 lr 0.000014	 wd 0.0500	time 0.4444 (0.4572)	loss 1.3518 (1.0830)	grad_norm 1.7393 (2.1842)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:33:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:18 lr 0.000014	 wd 0.0500	time 0.4437 (0.4570)	loss 1.1105 (1.0815)	grad_norm 1.8634 (2.1722)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:33:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:32 lr 0.000014	 wd 0.0500	time 0.4525 (0.4569)	loss 1.2845 (1.0811)	grad_norm 1.4379 (2.1856)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:34:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:46 lr 0.000014	 wd 0.0500	time 0.4516 (0.4567)	loss 0.7923 (1.0838)	grad_norm 1.6345 (2.1813)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:35:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.4477 (0.4566)	loss 0.7840 (1.0822)	grad_norm 2.0277 (2.1632)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:35:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 13 training takes 0:19:04
[2024-08-02 04:35:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.529 (12.529)	Loss 0.4458 (0.4458)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 17147MB
[2024-08-02 04:36:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.930 Acc@5 98.096
[2024-08-02 04:36:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 04:36:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 04:36:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][0/2502]	eta 8:37:19 lr 0.000014	 wd 0.0500	time 12.4060 (12.4060)	loss 1.2751 (1.2751)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:37:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:22:45 lr 0.000014	 wd 0.0500	time 0.4476 (0.5686)	loss 0.9889 (1.0841)	grad_norm 2.0411 (2.0824)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:37:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:19:33 lr 0.000014	 wd 0.0500	time 0.4509 (0.5097)	loss 1.3198 (1.0911)	grad_norm 1.6158 (1.9647)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:38:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:17:58 lr 0.000014	 wd 0.0500	time 0.4515 (0.4900)	loss 0.8786 (1.0928)	grad_norm 2.5960 (1.9419)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:39:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:16:49 lr 0.000014	 wd 0.0500	time 0.4406 (0.4802)	loss 1.4954 (1.0941)	grad_norm 1.9036 (1.9014)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:40:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:15:49 lr 0.000014	 wd 0.0500	time 0.4423 (0.4744)	loss 0.9049 (1.0902)	grad_norm 2.3729 (2.0833)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:40:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:14:55 lr 0.000014	 wd 0.0500	time 0.4447 (0.4706)	loss 0.9245 (1.0874)	grad_norm 1.5096 (2.0922)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:41:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:14:03 lr 0.000014	 wd 0.0500	time 0.4485 (0.4680)	loss 1.1533 (1.0892)	grad_norm 1.8967 (2.1252)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:42:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:13:13 lr 0.000014	 wd 0.0500	time 0.4496 (0.4661)	loss 0.7411 (1.0845)	grad_norm 1.5941 (2.1530)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:43:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:12:24 lr 0.000014	 wd 0.0500	time 0.4485 (0.4647)	loss 0.7518 (1.0830)	grad_norm 1.4007 (2.1459)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 04:43:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:11:36 lr 0.000014	 wd 0.0500	time 0.4504 (0.4635)	loss 0.9301 (1.0812)	grad_norm 1.5520 (2.1204)	loss_scale 4096.0000 (2174.8492)	mem 17147MB
[2024-08-02 04:44:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:10:48 lr 0.000014	 wd 0.0500	time 0.4445 (0.4625)	loss 1.3126 (1.0819)	grad_norm 1.6854 (inf)	loss_scale 2048.0000 (2222.8520)	mem 17147MB
[2024-08-02 04:45:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:10:01 lr 0.000014	 wd 0.0500	time 0.4480 (0.4617)	loss 1.4309 (1.0815)	grad_norm 1.6397 (inf)	loss_scale 2048.0000 (2208.2931)	mem 17147MB
[2024-08-02 04:46:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:09:14 lr 0.000014	 wd 0.0500	time 0.4478 (0.4611)	loss 1.1063 (1.0849)	grad_norm 1.8595 (inf)	loss_scale 2048.0000 (2195.9723)	mem 17147MB
[2024-08-02 04:46:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:08:27 lr 0.000014	 wd 0.0500	time 0.4503 (0.4605)	loss 1.2308 (1.0851)	grad_norm 2.0152 (inf)	loss_scale 2048.0000 (2185.4104)	mem 17147MB
[2024-08-02 04:47:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:07:40 lr 0.000014	 wd 0.0500	time 0.4468 (0.4600)	loss 1.3556 (1.0839)	grad_norm 2.1393 (inf)	loss_scale 2048.0000 (2176.2558)	mem 17147MB
[2024-08-02 04:48:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:06:54 lr 0.000014	 wd 0.0500	time 0.4484 (0.4596)	loss 0.8490 (1.0830)	grad_norm 3.2698 (inf)	loss_scale 2048.0000 (2168.2448)	mem 17147MB
[2024-08-02 04:49:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:06:08 lr 0.000014	 wd 0.0500	time 0.4522 (0.4592)	loss 0.8006 (1.0825)	grad_norm 1.2340 (inf)	loss_scale 2048.0000 (2161.1758)	mem 17147MB
[2024-08-02 04:49:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:05:22 lr 0.000013	 wd 0.0500	time 0.4508 (0.4588)	loss 0.9563 (1.0836)	grad_norm 5.8519 (inf)	loss_scale 2048.0000 (2154.8917)	mem 17147MB
[2024-08-02 04:50:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:04:36 lr 0.000013	 wd 0.0500	time 0.4487 (0.4586)	loss 1.1699 (1.0844)	grad_norm 1.9357 (inf)	loss_scale 2048.0000 (2149.2688)	mem 17147MB
[2024-08-02 04:51:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:03:50 lr 0.000013	 wd 0.0500	time 0.4424 (0.4583)	loss 1.3326 (1.0831)	grad_norm 1.8174 (inf)	loss_scale 2048.0000 (2144.2079)	mem 17147MB
[2024-08-02 04:52:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:03:04 lr 0.000013	 wd 0.0500	time 0.4459 (0.4580)	loss 1.1932 (1.0823)	grad_norm 1.3473 (inf)	loss_scale 2048.0000 (2139.6287)	mem 17147MB
[2024-08-02 04:52:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:18 lr 0.000013	 wd 0.0500	time 0.4475 (0.4577)	loss 1.1636 (1.0817)	grad_norm 3.5794 (inf)	loss_scale 2048.0000 (2135.4657)	mem 17147MB
[2024-08-02 04:53:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:32 lr 0.000013	 wd 0.0500	time 0.4459 (0.4575)	loss 0.7632 (1.0812)	grad_norm 2.6959 (inf)	loss_scale 2048.0000 (2131.6645)	mem 17147MB
[2024-08-02 04:54:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:46 lr 0.000013	 wd 0.0500	time 0.4467 (0.4573)	loss 0.7208 (1.0806)	grad_norm 1.5349 (inf)	loss_scale 2048.0000 (2128.1799)	mem 17147MB
[2024-08-02 04:55:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.4472 (0.4571)	loss 1.2271 (1.0800)	grad_norm 1.8325 (nan)	loss_scale 1024.0000 (2097.1323)	mem 17147MB
[2024-08-02 04:55:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 14 training takes 0:19:06
[2024-08-02 04:55:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.568 (12.568)	Loss 0.4373 (0.4373)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 04:55:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.890 Acc@5 98.096
[2024-08-02 04:55:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 04:55:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 04:55:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][0/2502]	eta 8:34:57 lr 0.000013	 wd 0.0500	time 12.3490 (12.3490)	loss 1.1652 (1.1652)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:56:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:22:42 lr 0.000013	 wd 0.0500	time 0.4455 (0.5670)	loss 0.7316 (1.0773)	grad_norm 2.0191 (2.1768)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:57:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:19:31 lr 0.000013	 wd 0.0500	time 0.4446 (0.5088)	loss 1.4665 (1.0671)	grad_norm 1.2905 (2.1253)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:58:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:17:57 lr 0.000013	 wd 0.0500	time 0.4463 (0.4895)	loss 0.8409 (1.0664)	grad_norm 1.2140 (2.1099)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:58:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:16:48 lr 0.000013	 wd 0.0500	time 0.4439 (0.4798)	loss 0.7065 (1.0649)	grad_norm 1.7239 (2.1002)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 04:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:15:49 lr 0.000013	 wd 0.0500	time 0.4498 (0.4742)	loss 0.6898 (1.0738)	grad_norm 1.7902 (2.1195)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:00:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:14:54 lr 0.000013	 wd 0.0500	time 0.4479 (0.4705)	loss 1.2022 (1.0794)	grad_norm 1.8250 (2.1145)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:01:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:14:03 lr 0.000013	 wd 0.0500	time 0.4449 (0.4679)	loss 1.2719 (1.0822)	grad_norm 11.8707 (2.1609)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:01:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:13:13 lr 0.000013	 wd 0.0500	time 0.4494 (0.4660)	loss 1.1092 (1.0830)	grad_norm 1.7975 (2.1159)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:02:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:12:24 lr 0.000013	 wd 0.0500	time 0.4490 (0.4645)	loss 1.2942 (1.0844)	grad_norm 1.6032 (2.1162)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:03:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:11:35 lr 0.000013	 wd 0.0500	time 0.4501 (0.4634)	loss 0.7353 (1.0843)	grad_norm 1.3935 (2.1098)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:04:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:10:48 lr 0.000013	 wd 0.0500	time 0.4411 (0.4625)	loss 0.7970 (1.0851)	grad_norm 1.3765 (2.0981)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:04:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:10:01 lr 0.000013	 wd 0.0500	time 0.4475 (0.4617)	loss 0.8369 (1.0856)	grad_norm 2.0120 (2.0869)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:05:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:09:14 lr 0.000013	 wd 0.0500	time 0.4474 (0.4611)	loss 0.8574 (1.0868)	grad_norm 2.0601 (2.0896)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:06:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:08:27 lr 0.000012	 wd 0.0500	time 0.4501 (0.4605)	loss 0.7409 (1.0864)	grad_norm 2.7948 (2.1092)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:07:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:07:40 lr 0.000012	 wd 0.0500	time 0.4442 (0.4601)	loss 1.0138 (1.0826)	grad_norm 1.8631 (2.0975)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:07:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:06:54 lr 0.000012	 wd 0.0500	time 0.4502 (0.4596)	loss 0.8767 (1.0836)	grad_norm 2.0169 (2.0971)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:08:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:06:08 lr 0.000012	 wd 0.0500	time 0.4487 (0.4593)	loss 0.7501 (1.0839)	grad_norm 1.4861 (2.1032)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:09:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:05:22 lr 0.000012	 wd 0.0500	time 0.4489 (0.4589)	loss 1.5352 (1.0825)	grad_norm 2.8238 (2.1011)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:10:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:04:36 lr 0.000012	 wd 0.0500	time 0.4509 (0.4586)	loss 0.8456 (1.0817)	grad_norm 1.7889 (2.1006)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:11:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:03:50 lr 0.000012	 wd 0.0500	time 0.4430 (0.4583)	loss 0.8637 (1.0827)	grad_norm 6.1616 (2.1058)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:11:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:03:04 lr 0.000012	 wd 0.0500	time 0.4486 (0.4581)	loss 0.6850 (1.0814)	grad_norm 3.4051 (2.1122)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:12:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:18 lr 0.000012	 wd 0.0500	time 0.4477 (0.4579)	loss 0.8234 (1.0809)	grad_norm 2.0985 (2.1186)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:13:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:32 lr 0.000012	 wd 0.0500	time 0.4406 (0.4577)	loss 0.9271 (1.0813)	grad_norm 2.7977 (2.1379)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:14:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:46 lr 0.000012	 wd 0.0500	time 0.4486 (0.4575)	loss 1.2491 (1.0816)	grad_norm 2.5000 (2.1445)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:14:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.4483 (0.4573)	loss 1.3755 (1.0825)	grad_norm 3.1794 (2.1567)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:14:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 15 training takes 0:19:06
[2024-08-02 05:14:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_15.pth saving......
[2024-08-02 05:14:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_15.pth saved !!!
[2024-08-02 05:15:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 10.277 (10.277)	Loss 0.4807 (0.4807)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 05:15:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.932 Acc@5 98.072
[2024-08-02 05:15:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 05:15:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 05:15:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:01:11 lr 0.000012	 wd 0.0500	time 11.5393 (11.5393)	loss 1.1563 (1.1563)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:16:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:22:28 lr 0.000012	 wd 0.0500	time 0.4439 (0.5612)	loss 1.0156 (1.0935)	grad_norm 2.8964 (2.8893)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:17:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:19:24 lr 0.000012	 wd 0.0500	time 0.4478 (0.5058)	loss 1.1435 (1.0818)	grad_norm 1.4523 (2.4351)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:17:53 lr 0.000012	 wd 0.0500	time 0.4454 (0.4874)	loss 0.9862 (1.0744)	grad_norm 1.8589 (2.2843)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:18:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:16:45 lr 0.000012	 wd 0.0500	time 0.4484 (0.4781)	loss 0.7696 (1.0644)	grad_norm 2.2796 (2.3092)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:19:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:15:46 lr 0.000012	 wd 0.0500	time 0.4493 (0.4728)	loss 1.3373 (1.0688)	grad_norm 1.7161 (2.4332)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:20:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:14:52 lr 0.000012	 wd 0.0500	time 0.4476 (0.4694)	loss 1.3392 (1.0678)	grad_norm 1.5430 (2.3551)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:20:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:14:01 lr 0.000012	 wd 0.0500	time 0.4493 (0.4671)	loss 0.9433 (1.0699)	grad_norm 2.0825 (2.3247)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:21:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:13:11 lr 0.000012	 wd 0.0500	time 0.4485 (0.4653)	loss 1.2562 (1.0715)	grad_norm 1.4722 (2.2846)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:22:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:12:23 lr 0.000012	 wd 0.0500	time 0.4453 (0.4639)	loss 0.9852 (1.0700)	grad_norm 1.6596 (2.2735)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:23:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:11:35 lr 0.000011	 wd 0.0500	time 0.4437 (0.4629)	loss 1.2407 (1.0713)	grad_norm 1.7663 (2.2856)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:23:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:10:47 lr 0.000011	 wd 0.0500	time 0.4540 (0.4620)	loss 0.8504 (1.0764)	grad_norm 1.4512 (2.2570)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:24:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:10:00 lr 0.000011	 wd 0.0500	time 0.4506 (0.4613)	loss 0.8397 (1.0759)	grad_norm 1.9883 (2.2397)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:25:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:09:13 lr 0.000011	 wd 0.0500	time 0.4499 (0.4607)	loss 1.1884 (1.0789)	grad_norm 1.6868 (2.2325)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:26:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:08:27 lr 0.000011	 wd 0.0500	time 0.4478 (0.4601)	loss 1.1572 (1.0796)	grad_norm 2.6957 (2.2238)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:26:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:07:40 lr 0.000011	 wd 0.0500	time 0.4496 (0.4596)	loss 1.1098 (1.0815)	grad_norm 1.4738 (2.2451)	loss_scale 2048.0000 (1073.1193)	mem 17147MB
[2024-08-02 05:27:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:06:54 lr 0.000011	 wd 0.0500	time 0.4497 (0.4593)	loss 1.1740 (1.0817)	grad_norm 2.3723 (2.2249)	loss_scale 2048.0000 (1134.0112)	mem 17147MB
[2024-08-02 05:28:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:06:08 lr 0.000011	 wd 0.0500	time 0.4495 (0.4589)	loss 0.9830 (1.0813)	grad_norm 1.7686 (nan)	loss_scale 1024.0000 (1145.6038)	mem 17147MB
[2024-08-02 05:29:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:05:21 lr 0.000011	 wd 0.0500	time 0.4486 (0.4586)	loss 0.8551 (1.0802)	grad_norm 1.8066 (nan)	loss_scale 1024.0000 (1138.8517)	mem 17147MB
[2024-08-02 05:29:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:04:35 lr 0.000011	 wd 0.0500	time 0.4465 (0.4583)	loss 0.7730 (1.0809)	grad_norm 1.7673 (nan)	loss_scale 1024.0000 (1132.8101)	mem 17147MB
[2024-08-02 05:30:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:03:49 lr 0.000011	 wd 0.0500	time 0.4476 (0.4580)	loss 1.2404 (1.0815)	grad_norm 1.5377 (nan)	loss_scale 1024.0000 (1127.3723)	mem 17147MB
[2024-08-02 05:31:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:03:04 lr 0.000011	 wd 0.0500	time 0.4484 (0.4578)	loss 1.1864 (1.0823)	grad_norm 2.3907 (nan)	loss_scale 1024.0000 (1122.4522)	mem 17147MB
[2024-08-02 05:32:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:02:18 lr 0.000011	 wd 0.0500	time 0.4502 (0.4576)	loss 0.8595 (1.0821)	grad_norm 2.3151 (nan)	loss_scale 1024.0000 (1117.9791)	mem 17147MB
[2024-08-02 05:32:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:32 lr 0.000011	 wd 0.0500	time 0.4488 (0.4574)	loss 0.7435 (1.0804)	grad_norm 2.3755 (nan)	loss_scale 1024.0000 (1113.8948)	mem 17147MB
[2024-08-02 05:33:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:46 lr 0.000011	 wd 0.0500	time 0.4501 (0.4573)	loss 1.1612 (1.0799)	grad_norm 1.3324 (nan)	loss_scale 1024.0000 (1110.1508)	mem 17147MB
[2024-08-02 05:34:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.4484 (0.4571)	loss 0.7706 (1.0796)	grad_norm 1.7587 (nan)	loss_scale 1024.0000 (1106.7061)	mem 17147MB
[2024-08-02 05:34:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 16 training takes 0:19:06
[2024-08-02 05:34:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.458 (12.458)	Loss 0.4558 (0.4558)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 05:35:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.916 Acc@5 98.090
[2024-08-02 05:35:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 05:35:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 05:35:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:03:16 lr 0.000011	 wd 0.0500	time 11.5895 (11.5895)	loss 1.1814 (1.1814)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:36:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:22:24 lr 0.000011	 wd 0.0500	time 0.4469 (0.5599)	loss 1.1878 (1.0583)	grad_norm 1.3417 (2.3247)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:36:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:19:22 lr 0.000011	 wd 0.0500	time 0.4406 (0.5051)	loss 1.3239 (1.0619)	grad_norm 1.7080 (2.2061)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:37:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:17:52 lr 0.000011	 wd 0.0500	time 0.4423 (0.4868)	loss 1.3534 (1.0759)	grad_norm 1.6559 (2.0582)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:38:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:16:44 lr 0.000011	 wd 0.0500	time 0.4435 (0.4777)	loss 1.4576 (1.0721)	grad_norm 1.2735 (2.0357)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:39:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:15:45 lr 0.000010	 wd 0.0500	time 0.4476 (0.4724)	loss 1.2808 (1.0787)	grad_norm 1.7555 (2.0756)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:39:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:14:52 lr 0.000010	 wd 0.0500	time 0.4466 (0.4690)	loss 0.8246 (1.0801)	grad_norm 1.6314 (2.1156)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:40:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:14:01 lr 0.000010	 wd 0.0500	time 0.4502 (0.4667)	loss 1.2563 (1.0774)	grad_norm 1.3962 (2.0869)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:41:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:13:11 lr 0.000010	 wd 0.0500	time 0.4498 (0.4650)	loss 1.1365 (1.0771)	grad_norm 1.5898 (2.0921)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:42:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:12:22 lr 0.000010	 wd 0.0500	time 0.4482 (0.4636)	loss 1.2299 (1.0790)	grad_norm 1.3644 (2.1106)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:42:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:11:34 lr 0.000010	 wd 0.0500	time 0.4514 (0.4625)	loss 0.9355 (1.0776)	grad_norm 1.5405 (2.0766)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:43:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:10:47 lr 0.000010	 wd 0.0500	time 0.4442 (0.4616)	loss 0.9289 (1.0822)	grad_norm 1.7213 (2.1119)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:44:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:10:00 lr 0.000010	 wd 0.0500	time 0.4519 (0.4609)	loss 0.7848 (1.0824)	grad_norm 1.4516 (2.0987)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:45:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:09:13 lr 0.000010	 wd 0.0500	time 0.4497 (0.4602)	loss 1.4148 (1.0812)	grad_norm 1.8785 (2.0994)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:45:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:08:26 lr 0.000010	 wd 0.0500	time 0.4495 (0.4597)	loss 1.3673 (1.0813)	grad_norm 1.2952 (2.1045)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:46:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:07:40 lr 0.000010	 wd 0.0500	time 0.4480 (0.4593)	loss 1.1919 (1.0814)	grad_norm 3.4255 (2.1206)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:47:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:06:53 lr 0.000010	 wd 0.0500	time 0.4482 (0.4589)	loss 1.2599 (1.0800)	grad_norm 1.3367 (2.1148)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:48:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:06:07 lr 0.000010	 wd 0.0500	time 0.4470 (0.4585)	loss 1.4274 (1.0817)	grad_norm 1.6343 (2.1083)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:48:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:05:21 lr 0.000010	 wd 0.0500	time 0.4425 (0.4582)	loss 0.9097 (1.0818)	grad_norm 1.5866 (2.1024)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:49:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:04:35 lr 0.000010	 wd 0.0500	time 0.4522 (0.4579)	loss 1.1500 (1.0809)	grad_norm 1.5937 (2.1349)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:50:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:03:49 lr 0.000010	 wd 0.0500	time 0.4506 (0.4576)	loss 0.9321 (1.0817)	grad_norm 4.7835 (2.2221)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:51:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:03:03 lr 0.000010	 wd 0.0500	time 0.4435 (0.4574)	loss 1.3701 (1.0818)	grad_norm 1.9697 (2.2106)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:51:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:18 lr 0.000010	 wd 0.0500	time 0.4493 (0.4572)	loss 1.4017 (1.0825)	grad_norm 1.7379 (2.1920)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:52:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:32 lr 0.000010	 wd 0.0500	time 0.4471 (0.4570)	loss 0.7826 (1.0821)	grad_norm 1.6649 (2.1917)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:53:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:46 lr 0.000010	 wd 0.0500	time 0.4497 (0.4568)	loss 0.8171 (1.0807)	grad_norm 1.6142 (2.1799)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:54:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.4473 (0.4566)	loss 1.4098 (1.0809)	grad_norm 1.7760 (2.1926)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:54:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 17 training takes 0:19:04
[2024-08-02 05:54:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.981 (11.981)	Loss 0.4441 (0.4441)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 17147MB
[2024-08-02 05:54:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.920 Acc@5 98.126
[2024-08-02 05:54:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 05:54:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 05:54:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:35:15 lr 0.000009	 wd 0.0500	time 12.3565 (12.3565)	loss 1.4034 (1.4034)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:55:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:22:46 lr 0.000009	 wd 0.0500	time 0.4452 (0.5687)	loss 1.4008 (1.0902)	grad_norm 1.2656 (2.0104)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:56:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:19:32 lr 0.000009	 wd 0.0500	time 0.4422 (0.5093)	loss 0.9142 (1.0939)	grad_norm 2.0284 (1.9601)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:57:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:17:58 lr 0.000009	 wd 0.0500	time 0.4495 (0.4896)	loss 1.5050 (1.0946)	grad_norm 1.6801 (1.9682)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:57:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:16:48 lr 0.000009	 wd 0.0500	time 0.4483 (0.4799)	loss 0.6819 (1.0943)	grad_norm 2.4814 (2.0331)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:58:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:15:49 lr 0.000009	 wd 0.0500	time 0.4427 (0.4742)	loss 1.2878 (1.0904)	grad_norm 1.6681 (2.0981)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 05:59:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:14:54 lr 0.000009	 wd 0.0500	time 0.4495 (0.4705)	loss 1.4428 (1.0849)	grad_norm 6.5381 (2.1233)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:00:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:14:03 lr 0.000009	 wd 0.0500	time 0.4481 (0.4679)	loss 1.2889 (1.0879)	grad_norm 1.7115 (2.1720)	loss_scale 2048.0000 (1132.0970)	mem 17147MB
[2024-08-02 06:00:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:13:13 lr 0.000009	 wd 0.0500	time 0.4478 (0.4660)	loss 1.2921 (1.0918)	grad_norm 1.6662 (2.2487)	loss_scale 2048.0000 (1246.4419)	mem 17147MB
[2024-08-02 06:01:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:12:24 lr 0.000009	 wd 0.0500	time 0.4419 (0.4646)	loss 0.8972 (1.0902)	grad_norm 1.3375 (2.2278)	loss_scale 2048.0000 (1335.4051)	mem 17147MB
[2024-08-02 06:02:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:11:36 lr 0.000009	 wd 0.0500	time 0.4444 (0.4634)	loss 1.3291 (1.0883)	grad_norm 2.2569 (2.2036)	loss_scale 2048.0000 (1406.5934)	mem 17147MB
[2024-08-02 06:03:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:10:48 lr 0.000009	 wd 0.0500	time 0.4493 (0.4625)	loss 0.9423 (1.0898)	grad_norm 2.1483 (2.1784)	loss_scale 2048.0000 (1464.8501)	mem 17147MB
[2024-08-02 06:03:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:10:01 lr 0.000009	 wd 0.0500	time 0.4505 (0.4618)	loss 0.9221 (1.0846)	grad_norm 4.9613 (2.2062)	loss_scale 2048.0000 (1513.4055)	mem 17147MB
[2024-08-02 06:04:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:09:14 lr 0.000009	 wd 0.0500	time 0.4460 (0.4611)	loss 1.4255 (1.0844)	grad_norm 5.3966 (2.2102)	loss_scale 2048.0000 (1554.4965)	mem 17147MB
[2024-08-02 06:05:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:08:27 lr 0.000009	 wd 0.0500	time 0.4459 (0.4605)	loss 1.1934 (1.0851)	grad_norm 1.8314 (2.2006)	loss_scale 2048.0000 (1589.7216)	mem 17147MB
[2024-08-02 06:06:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:07:40 lr 0.000009	 wd 0.0500	time 0.4486 (0.4600)	loss 1.2948 (1.0852)	grad_norm 1.7166 (2.1927)	loss_scale 2048.0000 (1620.2532)	mem 17147MB
[2024-08-02 06:06:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:06:54 lr 0.000009	 wd 0.0500	time 0.4497 (0.4596)	loss 1.2526 (1.0869)	grad_norm 1.7979 (2.1773)	loss_scale 2048.0000 (1646.9706)	mem 17147MB
[2024-08-02 06:07:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:06:08 lr 0.000009	 wd 0.0500	time 0.4477 (0.4592)	loss 1.2067 (1.0883)	grad_norm 1.4287 (2.1716)	loss_scale 2048.0000 (1670.5467)	mem 17147MB
[2024-08-02 06:08:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:05:22 lr 0.000009	 wd 0.0500	time 0.4506 (0.4589)	loss 0.7379 (1.0886)	grad_norm 2.8262 (2.1654)	loss_scale 2048.0000 (1691.5047)	mem 17147MB
[2024-08-02 06:09:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:04:36 lr 0.000009	 wd 0.0500	time 0.4482 (0.4586)	loss 1.4271 (1.0887)	grad_norm 1.7981 (2.1956)	loss_scale 2048.0000 (1710.2578)	mem 17147MB
[2024-08-02 06:09:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:03:50 lr 0.000008	 wd 0.0500	time 0.4489 (0.4583)	loss 0.9236 (1.0881)	grad_norm 2.5637 (2.1901)	loss_scale 2048.0000 (1727.1364)	mem 17147MB
[2024-08-02 06:10:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:03:04 lr 0.000008	 wd 0.0500	time 0.4436 (0.4581)	loss 1.2219 (1.0878)	grad_norm 2.9889 (2.1974)	loss_scale 2048.0000 (1742.4084)	mem 17147MB
[2024-08-02 06:11:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:02:18 lr 0.000008	 wd 0.0500	time 0.4461 (0.4579)	loss 1.0202 (1.0880)	grad_norm 1.6271 (2.1887)	loss_scale 2048.0000 (1756.2926)	mem 17147MB
[2024-08-02 06:12:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.4517 (0.4577)	loss 1.2918 (1.0890)	grad_norm 2.0158 (2.1881)	loss_scale 2048.0000 (1768.9700)	mem 17147MB
[2024-08-02 06:13:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:46 lr 0.000008	 wd 0.0500	time 0.4493 (0.4574)	loss 1.3364 (1.0893)	grad_norm 2.0806 (2.1821)	loss_scale 2048.0000 (1780.5914)	mem 17147MB
[2024-08-02 06:13:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.4497 (0.4573)	loss 1.0810 (1.0897)	grad_norm 1.5650 (2.1804)	loss_scale 2048.0000 (1791.2835)	mem 17147MB
[2024-08-02 06:13:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 18 training takes 0:19:06
[2024-08-02 06:14:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.764 (11.764)	Loss 0.4546 (0.4546)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 17147MB
[2024-08-02 06:14:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.892 Acc@5 98.100
[2024-08-02 06:14:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 06:14:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 06:14:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:17:50 lr 0.000008	 wd 0.0500	time 11.9386 (11.9386)	loss 0.7938 (0.7938)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:15:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:22:41 lr 0.000008	 wd 0.0500	time 0.4465 (0.5667)	loss 1.4890 (1.1288)	grad_norm 1.6515 (2.2776)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:16:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:19:30 lr 0.000008	 wd 0.0500	time 0.4475 (0.5086)	loss 0.8357 (1.1004)	grad_norm 1.5945 (2.1180)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:16:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:17:57 lr 0.000008	 wd 0.0500	time 0.4457 (0.4892)	loss 1.3377 (1.0896)	grad_norm 1.9471 (2.0381)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:17:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:16:48 lr 0.000008	 wd 0.0500	time 0.4458 (0.4797)	loss 0.9641 (1.0817)	grad_norm 1.5197 (2.1459)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:18:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:15:49 lr 0.000008	 wd 0.0500	time 0.4486 (0.4742)	loss 1.3264 (1.0892)	grad_norm 2.6495 (2.1429)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:19:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:14:54 lr 0.000008	 wd 0.0500	time 0.4466 (0.4705)	loss 0.8422 (1.0822)	grad_norm 1.9143 (2.1558)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:19:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:14:03 lr 0.000008	 wd 0.0500	time 0.4485 (0.4681)	loss 1.0637 (1.0810)	grad_norm 1.1736 (2.2117)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:20:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:13:13 lr 0.000008	 wd 0.0500	time 0.4496 (0.4662)	loss 1.3188 (1.0821)	grad_norm 2.0650 (2.2356)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:21:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:12:24 lr 0.000008	 wd 0.0500	time 0.4490 (0.4648)	loss 1.1399 (1.0832)	grad_norm 1.9385 (2.2411)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:22:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:11:36 lr 0.000008	 wd 0.0500	time 0.4476 (0.4636)	loss 1.3070 (1.0836)	grad_norm 2.2580 (2.2315)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:22:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:10:48 lr 0.000008	 wd 0.0500	time 0.4492 (0.4627)	loss 1.0699 (1.0818)	grad_norm 1.8841 (2.2052)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:23:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:10:01 lr 0.000008	 wd 0.0500	time 0.4519 (0.4620)	loss 1.2836 (1.0803)	grad_norm 1.6370 (2.1983)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:24:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:09:14 lr 0.000008	 wd 0.0500	time 0.4495 (0.4613)	loss 0.8766 (1.0799)	grad_norm 2.4509 (2.1980)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:25:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:08:27 lr 0.000008	 wd 0.0500	time 0.4503 (0.4608)	loss 0.8415 (1.0797)	grad_norm 3.3720 (2.1824)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:25:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:07:41 lr 0.000008	 wd 0.0500	time 0.4508 (0.4603)	loss 1.3938 (1.0808)	grad_norm 1.7880 (2.1807)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:26:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:06:54 lr 0.000007	 wd 0.0500	time 0.4418 (0.4598)	loss 1.1463 (1.0822)	grad_norm 1.6348 (2.1709)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:27:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:06:08 lr 0.000007	 wd 0.0500	time 0.4485 (0.4595)	loss 0.7333 (1.0826)	grad_norm 1.7014 (2.1727)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:28:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:05:22 lr 0.000007	 wd 0.0500	time 0.4512 (0.4592)	loss 1.1893 (1.0845)	grad_norm 1.5697 (2.1720)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:28:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:04:36 lr 0.000007	 wd 0.0500	time 0.4511 (0.4589)	loss 1.0848 (1.0831)	grad_norm 2.1880 (2.1624)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:29:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:03:50 lr 0.000007	 wd 0.0500	time 0.4451 (0.4586)	loss 1.2126 (1.0840)	grad_norm 2.0506 (2.1809)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:30:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:03:04 lr 0.000007	 wd 0.0500	time 0.4465 (0.4583)	loss 1.5391 (1.0860)	grad_norm 1.0930 (2.1779)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:31:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:02:18 lr 0.000007	 wd 0.0500	time 0.4505 (0.4581)	loss 1.1885 (1.0853)	grad_norm 3.4267 (2.1789)	loss_scale 4096.0000 (2118.7169)	mem 17147MB
[2024-08-02 06:31:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:32 lr 0.000007	 wd 0.0500	time 0.4467 (0.4579)	loss 0.9349 (1.0850)	grad_norm 2.7626 (nan)	loss_scale 2048.0000 (2161.9261)	mem 17147MB
[2024-08-02 06:32:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:46 lr 0.000007	 wd 0.0500	time 0.4518 (0.4577)	loss 1.0372 (1.0843)	grad_norm 1.3488 (nan)	loss_scale 2048.0000 (2157.1812)	mem 17147MB
[2024-08-02 06:33:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.4491 (0.4576)	loss 0.7699 (1.0843)	grad_norm 3.8166 (nan)	loss_scale 2048.0000 (2152.8157)	mem 17147MB
[2024-08-02 06:33:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 19 training takes 0:19:07
[2024-08-02 06:33:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.683 (11.683)	Loss 0.4424 (0.4424)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 17147MB
[2024-08-02 06:34:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.910 Acc@5 98.114
[2024-08-02 06:34:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 06:34:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 06:34:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:56:52 lr 0.000007	 wd 0.0500	time 11.4359 (11.4359)	loss 1.2745 (1.2745)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:35:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:22:27 lr 0.000007	 wd 0.0500	time 0.4420 (0.5611)	loss 0.8742 (1.0928)	grad_norm 1.9394 (2.2495)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:35:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:19:24 lr 0.000007	 wd 0.0500	time 0.4490 (0.5058)	loss 1.2361 (1.0819)	grad_norm 1.9710 (2.1162)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 06:36:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:17:52 lr 0.000007	 wd 0.0500	time 0.4480 (0.4873)	loss 0.6855 (1.0913)	grad_norm 2.1880 (nan)	loss_scale 1024.0000 (1775.8405)	mem 17147MB
[2024-08-02 06:37:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:16:45 lr 0.000007	 wd 0.0500	time 0.4473 (0.4782)	loss 1.2295 (1.0889)	grad_norm 1.2867 (nan)	loss_scale 1024.0000 (1588.3491)	mem 17147MB
[2024-08-02 06:38:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:15:46 lr 0.000007	 wd 0.0500	time 0.4501 (0.4728)	loss 1.2348 (1.0881)	grad_norm 1.5493 (nan)	loss_scale 1024.0000 (1475.7046)	mem 17147MB
[2024-08-02 06:38:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:14:52 lr 0.000007	 wd 0.0500	time 0.4496 (0.4693)	loss 1.3020 (1.0920)	grad_norm 8.0859 (nan)	loss_scale 1024.0000 (1400.5458)	mem 17147MB
[2024-08-02 06:39:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:14:01 lr 0.000007	 wd 0.0500	time 0.4496 (0.4669)	loss 1.2064 (1.0905)	grad_norm 1.5371 (nan)	loss_scale 1024.0000 (1346.8302)	mem 17147MB
[2024-08-02 06:40:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:13:11 lr 0.000007	 wd 0.0500	time 0.4495 (0.4652)	loss 1.1105 (1.0923)	grad_norm 1.3176 (nan)	loss_scale 1024.0000 (1306.5268)	mem 17147MB
[2024-08-02 06:41:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:12:23 lr 0.000007	 wd 0.0500	time 0.4494 (0.4638)	loss 1.2841 (1.0907)	grad_norm 1.7494 (nan)	loss_scale 1024.0000 (1275.1698)	mem 17147MB
[2024-08-02 06:41:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:11:34 lr 0.000007	 wd 0.0500	time 0.4475 (0.4627)	loss 1.1847 (1.0922)	grad_norm 1.9777 (nan)	loss_scale 1024.0000 (1250.0779)	mem 17147MB
[2024-08-02 06:42:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:10:47 lr 0.000007	 wd 0.0500	time 0.4504 (0.4618)	loss 0.8675 (1.0901)	grad_norm 1.8663 (nan)	loss_scale 1024.0000 (1229.5441)	mem 17147MB
[2024-08-02 06:43:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:10:00 lr 0.000006	 wd 0.0500	time 0.4489 (0.4611)	loss 1.1683 (1.0871)	grad_norm 1.3465 (nan)	loss_scale 1024.0000 (1212.4296)	mem 17147MB
[2024-08-02 06:44:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:09:13 lr 0.000006	 wd 0.0500	time 0.4492 (0.4605)	loss 1.3236 (1.0871)	grad_norm 1.6148 (nan)	loss_scale 1024.0000 (1197.9462)	mem 17147MB
[2024-08-02 06:44:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:08:26 lr 0.000006	 wd 0.0500	time 0.4509 (0.4600)	loss 1.0096 (1.0891)	grad_norm 3.4466 (nan)	loss_scale 1024.0000 (1185.5303)	mem 17147MB
[2024-08-02 06:45:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:07:40 lr 0.000006	 wd 0.0500	time 0.4422 (0.4595)	loss 1.3024 (1.0905)	grad_norm 1.6005 (nan)	loss_scale 1024.0000 (1174.7688)	mem 17147MB
[2024-08-02 06:46:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:06:54 lr 0.000006	 wd 0.0500	time 0.4497 (0.4591)	loss 1.1534 (1.0922)	grad_norm 2.9456 (nan)	loss_scale 1024.0000 (1165.3517)	mem 17147MB
[2024-08-02 06:47:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:06:07 lr 0.000006	 wd 0.0500	time 0.4448 (0.4588)	loss 1.4396 (1.0929)	grad_norm 1.3566 (nan)	loss_scale 1024.0000 (1157.0417)	mem 17147MB
[2024-08-02 06:47:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:05:21 lr 0.000006	 wd 0.0500	time 0.4506 (0.4584)	loss 0.7601 (1.0910)	grad_norm 1.8140 (nan)	loss_scale 1024.0000 (1149.6546)	mem 17147MB
[2024-08-02 06:48:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:04:35 lr 0.000006	 wd 0.0500	time 0.4480 (0.4581)	loss 0.7601 (1.0915)	grad_norm 1.6556 (nan)	loss_scale 1024.0000 (1143.0447)	mem 17147MB
[2024-08-02 06:49:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:03:49 lr 0.000006	 wd 0.0500	time 0.4474 (0.4579)	loss 1.0518 (1.0911)	grad_norm 4.5882 (nan)	loss_scale 1024.0000 (1137.0955)	mem 17147MB
[2024-08-02 06:50:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:03:03 lr 0.000006	 wd 0.0500	time 0.4466 (0.4577)	loss 1.2426 (1.0914)	grad_norm 1.8573 (nan)	loss_scale 1024.0000 (1131.7125)	mem 17147MB
[2024-08-02 06:50:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:02:18 lr 0.000006	 wd 0.0500	time 0.4446 (0.4574)	loss 1.2842 (1.0928)	grad_norm 2.3411 (nan)	loss_scale 1024.0000 (1126.8187)	mem 17147MB
[2024-08-02 06:51:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:32 lr 0.000006	 wd 0.0500	time 0.4479 (0.4572)	loss 0.7819 (1.0942)	grad_norm 1.9767 (nan)	loss_scale 1024.0000 (1122.3503)	mem 17147MB
[2024-08-02 06:52:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:46 lr 0.000006	 wd 0.0500	time 0.4453 (0.4570)	loss 0.6994 (1.0938)	grad_norm 1.7390 (nan)	loss_scale 1024.0000 (1118.2541)	mem 17147MB
[2024-08-02 06:53:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.4464 (0.4568)	loss 1.0663 (1.0955)	grad_norm 1.4219 (nan)	loss_scale 1024.0000 (1114.4854)	mem 17147MB
[2024-08-02 06:53:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 20 training takes 0:19:06
[2024-08-02 06:53:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.570 (12.570)	Loss 0.4524 (0.4524)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 17147MB
[2024-08-02 06:53:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.880 Acc@5 98.110
[2024-08-02 06:53:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 06:53:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 06:53:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:38:16 lr 0.000006	 wd 0.0500	time 12.4285 (12.4285)	loss 0.8779 (0.8779)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:54:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:22:43 lr 0.000006	 wd 0.0500	time 0.4492 (0.5676)	loss 1.2693 (1.0987)	grad_norm 1.6336 (2.0108)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:55:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:19:31 lr 0.000006	 wd 0.0500	time 0.4438 (0.5090)	loss 1.2439 (1.1014)	grad_norm 1.6744 (1.9702)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:56:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:17:57 lr 0.000006	 wd 0.0500	time 0.4477 (0.4893)	loss 1.0488 (1.0964)	grad_norm 2.6754 (2.0214)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:56:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:16:48 lr 0.000006	 wd 0.0500	time 0.4408 (0.4796)	loss 1.3904 (1.0870)	grad_norm 1.8924 (2.0437)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:57:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:15:48 lr 0.000006	 wd 0.0500	time 0.4483 (0.4739)	loss 0.7647 (1.0828)	grad_norm 1.7724 (2.0667)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:58:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:14:54 lr 0.000006	 wd 0.0500	time 0.4480 (0.4702)	loss 1.2414 (1.0791)	grad_norm 1.8561 (2.0724)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:59:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:14:02 lr 0.000006	 wd 0.0500	time 0.4442 (0.4676)	loss 1.1678 (1.0813)	grad_norm 1.6420 (2.0784)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 06:59:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:13:12 lr 0.000006	 wd 0.0500	time 0.4493 (0.4657)	loss 1.2243 (1.0804)	grad_norm 2.2394 (2.0985)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:00:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:12:23 lr 0.000005	 wd 0.0500	time 0.4446 (0.4643)	loss 1.3027 (1.0840)	grad_norm 6.5673 (2.0897)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:01:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:11:35 lr 0.000005	 wd 0.0500	time 0.4466 (0.4632)	loss 1.2901 (1.0875)	grad_norm 1.1875 (2.1057)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:02:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:10:48 lr 0.000005	 wd 0.0500	time 0.4485 (0.4622)	loss 0.7775 (1.0911)	grad_norm 1.5512 (2.1104)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:03:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:10:00 lr 0.000005	 wd 0.0500	time 0.4432 (0.4615)	loss 1.2336 (1.0876)	grad_norm 2.1707 (2.0956)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:03:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:09:13 lr 0.000005	 wd 0.0500	time 0.4511 (0.4608)	loss 1.2443 (1.0877)	grad_norm 2.8605 (2.1122)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:04:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:08:27 lr 0.000005	 wd 0.0500	time 0.4491 (0.4603)	loss 0.7611 (1.0857)	grad_norm 4.6325 (2.1026)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:05:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:07:40 lr 0.000005	 wd 0.0500	time 0.4505 (0.4598)	loss 0.7706 (1.0872)	grad_norm 2.1426 (2.1191)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:06:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:06:54 lr 0.000005	 wd 0.0500	time 0.4488 (0.4594)	loss 1.0879 (1.0839)	grad_norm 3.1515 (2.1410)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:06:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:06:08 lr 0.000005	 wd 0.0500	time 0.4476 (0.4590)	loss 1.3030 (1.0839)	grad_norm 1.4056 (2.1245)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:07:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:05:21 lr 0.000005	 wd 0.0500	time 0.4486 (0.4587)	loss 0.7977 (1.0842)	grad_norm 1.9733 (2.1347)	loss_scale 2048.0000 (1070.6230)	mem 17147MB
[2024-08-02 07:08:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:04:35 lr 0.000005	 wd 0.0500	time 0.4513 (0.4584)	loss 0.7025 (1.0856)	grad_norm 1.6762 (2.1501)	loss_scale 2048.0000 (1122.0368)	mem 17147MB
[2024-08-02 07:09:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:03:49 lr 0.000005	 wd 0.0500	time 0.4494 (0.4581)	loss 1.0659 (1.0853)	grad_norm 1.8404 (2.1529)	loss_scale 2048.0000 (1168.3118)	mem 17147MB
[2024-08-02 07:09:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:03:04 lr 0.000005	 wd 0.0500	time 0.4505 (0.4579)	loss 1.1703 (1.0848)	grad_norm 2.6219 (2.1587)	loss_scale 2048.0000 (1210.1818)	mem 17147MB
[2024-08-02 07:10:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:02:18 lr 0.000005	 wd 0.0500	time 0.4484 (0.4577)	loss 1.3463 (1.0832)	grad_norm 1.9495 (2.1599)	loss_scale 2048.0000 (1248.2472)	mem 17147MB
[2024-08-02 07:11:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:32 lr 0.000005	 wd 0.0500	time 0.4508 (0.4575)	loss 1.3315 (1.0853)	grad_norm 1.5902 (2.1617)	loss_scale 2048.0000 (1283.0039)	mem 17147MB
[2024-08-02 07:12:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:46 lr 0.000005	 wd 0.0500	time 0.4505 (0.4573)	loss 1.0522 (1.0862)	grad_norm 1.5476 (2.1596)	loss_scale 2048.0000 (1314.8655)	mem 17147MB
[2024-08-02 07:12:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.4470 (0.4571)	loss 1.3559 (1.0858)	grad_norm 1.6133 (2.1863)	loss_scale 2048.0000 (1344.1791)	mem 17147MB
[2024-08-02 07:12:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 21 training takes 0:19:06
[2024-08-02 07:13:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.235 (12.235)	Loss 0.4629 (0.4629)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 17147MB
[2024-08-02 07:13:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.928 Acc@5 98.098
[2024-08-02 07:13:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 07:13:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 07:13:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:28:48 lr 0.000005	 wd 0.0500	time 12.2015 (12.2015)	loss 0.9578 (0.9578)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:14:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:22:38 lr 0.000005	 wd 0.0500	time 0.4459 (0.5658)	loss 1.2474 (1.0965)	grad_norm 1.7532 (2.1644)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:15:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:19:29 lr 0.000005	 wd 0.0500	time 0.4458 (0.5080)	loss 0.6405 (1.1074)	grad_norm 2.1123 (2.1485)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:15:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:17:56 lr 0.000005	 wd 0.0500	time 0.4484 (0.4887)	loss 1.4603 (1.1087)	grad_norm 1.4756 (2.2907)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:16:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:16:47 lr 0.000005	 wd 0.0500	time 0.4497 (0.4792)	loss 0.7303 (1.1094)	grad_norm 1.3508 (2.3308)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:17:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:15:47 lr 0.000005	 wd 0.0500	time 0.4457 (0.4735)	loss 1.0388 (1.1054)	grad_norm 1.6927 (2.2653)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:18:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:14:53 lr 0.000005	 wd 0.0500	time 0.4443 (0.4698)	loss 0.8269 (1.1050)	grad_norm 1.5462 (2.2529)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:18:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:14:02 lr 0.000005	 wd 0.0500	time 0.4497 (0.4674)	loss 1.3370 (1.0990)	grad_norm 1.3360 (2.2264)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:19:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:13:12 lr 0.000004	 wd 0.0500	time 0.4440 (0.4655)	loss 0.9905 (1.0990)	grad_norm 1.3392 (2.2100)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:20:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:12:23 lr 0.000004	 wd 0.0500	time 0.4496 (0.4641)	loss 1.0583 (1.0992)	grad_norm 1.8973 (2.1902)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:21:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:11:35 lr 0.000004	 wd 0.0500	time 0.4502 (0.4629)	loss 1.2895 (1.0997)	grad_norm 1.0596 (2.2459)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 07:21:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:10:47 lr 0.000004	 wd 0.0500	time 0.4481 (0.4620)	loss 0.7419 (1.0958)	grad_norm 1.8478 (nan)	loss_scale 1024.0000 (2033.1190)	mem 17147MB
[2024-08-02 07:22:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:10:00 lr 0.000004	 wd 0.0500	time 0.4479 (0.4613)	loss 1.2409 (1.0966)	grad_norm 1.5097 (nan)	loss_scale 1024.0000 (1949.0958)	mem 17147MB
[2024-08-02 07:23:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:09:13 lr 0.000004	 wd 0.0500	time 0.4473 (0.4606)	loss 0.7678 (1.0908)	grad_norm 3.3443 (nan)	loss_scale 1024.0000 (1877.9892)	mem 17147MB
[2024-08-02 07:24:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:08:26 lr 0.000004	 wd 0.0500	time 0.4518 (0.4601)	loss 0.9778 (1.0861)	grad_norm 2.4925 (nan)	loss_scale 1024.0000 (1817.0335)	mem 17147MB
[2024-08-02 07:24:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:07:40 lr 0.000004	 wd 0.0500	time 0.4464 (0.4596)	loss 1.0713 (1.0856)	grad_norm 2.0787 (nan)	loss_scale 1024.0000 (1764.1999)	mem 17147MB
[2024-08-02 07:25:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:06:54 lr 0.000004	 wd 0.0500	time 0.4477 (0.4592)	loss 1.2995 (1.0851)	grad_norm 1.4498 (nan)	loss_scale 1024.0000 (1717.9663)	mem 17147MB
[2024-08-02 07:26:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:06:07 lr 0.000004	 wd 0.0500	time 0.4479 (0.4588)	loss 0.8918 (1.0833)	grad_norm 1.4725 (nan)	loss_scale 1024.0000 (1677.1687)	mem 17147MB
[2024-08-02 07:27:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:05:21 lr 0.000004	 wd 0.0500	time 0.4511 (0.4585)	loss 1.1281 (1.0820)	grad_norm 2.9241 (nan)	loss_scale 1024.0000 (1640.9017)	mem 17147MB
[2024-08-02 07:28:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:04:35 lr 0.000004	 wd 0.0500	time 0.4492 (0.4582)	loss 0.9237 (1.0830)	grad_norm 1.7485 (nan)	loss_scale 1024.0000 (1608.4503)	mem 17147MB
[2024-08-02 07:28:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:03:49 lr 0.000004	 wd 0.0500	time 0.4504 (0.4580)	loss 0.8645 (1.0843)	grad_norm 1.2615 (nan)	loss_scale 1024.0000 (1579.2424)	mem 17147MB
[2024-08-02 07:29:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:03:04 lr 0.000004	 wd 0.0500	time 0.4479 (0.4578)	loss 1.2096 (1.0854)	grad_norm 1.6597 (nan)	loss_scale 1024.0000 (1552.8149)	mem 17147MB
[2024-08-02 07:30:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:18 lr 0.000004	 wd 0.0500	time 0.4480 (0.4576)	loss 1.0430 (1.0853)	grad_norm 1.6955 (nan)	loss_scale 1024.0000 (1528.7887)	mem 17147MB
[2024-08-02 07:31:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:32 lr 0.000004	 wd 0.0500	time 0.4504 (0.4574)	loss 1.1518 (1.0857)	grad_norm 1.3455 (nan)	loss_scale 1024.0000 (1506.8509)	mem 17147MB
[2024-08-02 07:31:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:46 lr 0.000004	 wd 0.0500	time 0.4463 (0.4572)	loss 1.3722 (1.0874)	grad_norm 53.5279 (nan)	loss_scale 1024.0000 (1486.7405)	mem 17147MB
[2024-08-02 07:32:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.4503 (0.4570)	loss 0.8567 (1.0877)	grad_norm 4.4206 (nan)	loss_scale 1024.0000 (1468.2383)	mem 17147MB
[2024-08-02 07:32:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 22 training takes 0:19:07
[2024-08-02 07:32:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.518 (11.518)	Loss 0.4465 (0.4465)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 17147MB
[2024-08-02 07:33:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.892 Acc@5 98.112
[2024-08-02 07:33:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 07:33:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 07:33:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:43:15 lr 0.000004	 wd 0.0500	time 12.5484 (12.5484)	loss 0.7546 (0.7546)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:34:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:22:44 lr 0.000004	 wd 0.0500	time 0.4446 (0.5683)	loss 1.3232 (1.0835)	grad_norm 2.1846 (2.0732)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:34:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:19:32 lr 0.000004	 wd 0.0500	time 0.4489 (0.5092)	loss 0.8861 (1.0927)	grad_norm 1.9129 (2.1957)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:35:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:17:57 lr 0.000004	 wd 0.0500	time 0.4460 (0.4895)	loss 1.2925 (1.0820)	grad_norm 1.5039 (2.2894)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:36:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:16:48 lr 0.000004	 wd 0.0500	time 0.4489 (0.4798)	loss 0.8666 (1.0889)	grad_norm 1.6984 (2.2053)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:37:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:15:49 lr 0.000004	 wd 0.0500	time 0.4471 (0.4740)	loss 0.8611 (1.0890)	grad_norm 1.5667 (2.1792)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:37:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:14:54 lr 0.000004	 wd 0.0500	time 0.4514 (0.4703)	loss 1.2913 (1.0882)	grad_norm 2.3777 (2.1825)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:38:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:14:02 lr 0.000004	 wd 0.0500	time 0.4494 (0.4677)	loss 1.2105 (1.0909)	grad_norm 14.7907 (2.2099)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:39:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:13:12 lr 0.000003	 wd 0.0500	time 0.4444 (0.4658)	loss 1.2997 (1.0934)	grad_norm 1.8813 (2.2036)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:40:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:12:24 lr 0.000003	 wd 0.0500	time 0.4488 (0.4644)	loss 1.0967 (1.0945)	grad_norm 1.4264 (2.2328)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:40:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:11:35 lr 0.000003	 wd 0.0500	time 0.4488 (0.4633)	loss 1.1263 (1.0907)	grad_norm 1.3550 (2.2499)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:41:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:10:48 lr 0.000003	 wd 0.0500	time 0.4503 (0.4623)	loss 0.9705 (1.0927)	grad_norm 1.7394 (2.2192)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:42:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:10:00 lr 0.000003	 wd 0.0500	time 0.4455 (0.4615)	loss 1.2855 (1.0951)	grad_norm 1.7806 (2.2336)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:43:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:09:13 lr 0.000003	 wd 0.0500	time 0.4503 (0.4609)	loss 1.1733 (1.0961)	grad_norm 2.0659 (2.2295)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:43:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:08:27 lr 0.000003	 wd 0.0500	time 0.4463 (0.4603)	loss 1.3292 (1.0960)	grad_norm 1.7423 (2.2502)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:44:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:07:40 lr 0.000003	 wd 0.0500	time 0.4477 (0.4598)	loss 1.3047 (1.0944)	grad_norm 3.7978 (2.2533)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:45:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:06:54 lr 0.000003	 wd 0.0500	time 0.4442 (0.4594)	loss 1.1258 (1.0972)	grad_norm 2.3740 (2.2593)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:46:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:06:08 lr 0.000003	 wd 0.0500	time 0.4510 (0.4590)	loss 0.6875 (1.0947)	grad_norm 1.7348 (2.2680)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:47:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:05:22 lr 0.000003	 wd 0.0500	time 0.4511 (0.4587)	loss 1.3293 (1.0942)	grad_norm 3.1705 (2.2676)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:47:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:04:35 lr 0.000003	 wd 0.0500	time 0.4523 (0.4584)	loss 1.2921 (1.0937)	grad_norm 1.7994 (2.2690)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:48:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:03:49 lr 0.000003	 wd 0.0500	time 0.4491 (0.4581)	loss 0.8045 (1.0928)	grad_norm 10.4626 (2.2643)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:49:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:03:04 lr 0.000003	 wd 0.0500	time 0.4502 (0.4579)	loss 1.2169 (1.0935)	grad_norm 2.8232 (2.2564)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:50:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:02:18 lr 0.000003	 wd 0.0500	time 0.4499 (0.4577)	loss 1.2521 (1.0925)	grad_norm 1.6781 (2.2479)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:50:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:32 lr 0.000003	 wd 0.0500	time 0.4485 (0.4575)	loss 1.3909 (1.0921)	grad_norm 1.4756 (2.2440)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:51:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:46 lr 0.000003	 wd 0.0500	time 0.4518 (0.4573)	loss 0.9174 (1.0919)	grad_norm 2.0424 (2.2427)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:52:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.4491 (0.4571)	loss 0.8316 (1.0907)	grad_norm 1.8980 (2.2433)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:52:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 23 training takes 0:19:09
[2024-08-02 07:52:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.692 (11.692)	Loss 0.4644 (0.4644)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 07:53:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.916 Acc@5 98.110
[2024-08-02 07:53:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 07:53:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 07:53:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:13:22 lr 0.000003	 wd 0.0500	time 11.8315 (11.8315)	loss 1.2106 (1.2106)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 07:54:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:22:27 lr 0.000003	 wd 0.0500	time 0.4448 (0.5611)	loss 0.9255 (1.1058)	grad_norm 1.8040 (2.1859)	loss_scale 2048.0000 (1226.7723)	mem 17147MB
[2024-08-02 07:54:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:19:24 lr 0.000003	 wd 0.0500	time 0.4458 (0.5057)	loss 1.1222 (1.0964)	grad_norm 1.7996 (2.2373)	loss_scale 2048.0000 (1635.3433)	mem 17147MB
[2024-08-02 07:55:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:17:53 lr 0.000003	 wd 0.0500	time 0.4483 (0.4873)	loss 1.2959 (1.1074)	grad_norm 2.2377 (2.4505)	loss_scale 2048.0000 (1772.4385)	mem 17147MB
[2024-08-02 07:56:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:16:44 lr 0.000003	 wd 0.0500	time 0.4430 (0.4781)	loss 0.6529 (1.1049)	grad_norm 2.1565 (2.4686)	loss_scale 2048.0000 (1841.1571)	mem 17147MB
[2024-08-02 07:57:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:15:46 lr 0.000003	 wd 0.0500	time 0.4484 (0.4727)	loss 1.2537 (1.1055)	grad_norm 1.4713 (2.3589)	loss_scale 2048.0000 (1882.4431)	mem 17147MB
[2024-08-02 07:57:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:14:52 lr 0.000003	 wd 0.0500	time 0.4474 (0.4692)	loss 0.8131 (1.1021)	grad_norm 1.6849 (2.3197)	loss_scale 2048.0000 (1909.9900)	mem 17147MB
[2024-08-02 07:58:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:14:01 lr 0.000003	 wd 0.0500	time 0.4439 (0.4668)	loss 1.2421 (1.1004)	grad_norm 2.0292 (2.2803)	loss_scale 2048.0000 (1929.6776)	mem 17147MB
[2024-08-02 07:59:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:13:11 lr 0.000003	 wd 0.0500	time 0.4498 (0.4649)	loss 1.2262 (1.1013)	grad_norm 1.2350 (2.2528)	loss_scale 2048.0000 (1944.4494)	mem 17147MB
[2024-08-02 08:00:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:12:22 lr 0.000003	 wd 0.0500	time 0.4507 (0.4635)	loss 0.8813 (1.1057)	grad_norm 1.9292 (2.2460)	loss_scale 2048.0000 (1955.9423)	mem 17147MB
[2024-08-02 08:00:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:11:34 lr 0.000003	 wd 0.0500	time 0.4493 (0.4625)	loss 0.8374 (1.1031)	grad_norm 1.4587 (2.2499)	loss_scale 2048.0000 (1965.1389)	mem 17147MB
[2024-08-02 08:01:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:10:47 lr 0.000003	 wd 0.0500	time 0.4486 (0.4617)	loss 0.8468 (1.1016)	grad_norm 1.5639 (2.2192)	loss_scale 2048.0000 (1972.6649)	mem 17147MB
[2024-08-02 08:02:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:10:00 lr 0.000002	 wd 0.0500	time 0.4499 (0.4610)	loss 0.9229 (1.0991)	grad_norm 5.6302 (2.2231)	loss_scale 2048.0000 (1978.9376)	mem 17147MB
[2024-08-02 08:03:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:09:13 lr 0.000002	 wd 0.0500	time 0.4466 (0.4605)	loss 0.8360 (1.0970)	grad_norm 1.5880 (2.2364)	loss_scale 2048.0000 (1984.2460)	mem 17147MB
[2024-08-02 08:03:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:08:26 lr 0.000002	 wd 0.0500	time 0.4504 (0.4600)	loss 1.4021 (1.0982)	grad_norm 3.0963 (2.2393)	loss_scale 2048.0000 (1988.7966)	mem 17147MB
[2024-08-02 08:04:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:07:40 lr 0.000002	 wd 0.0500	time 0.4515 (0.4595)	loss 1.1857 (1.0948)	grad_norm 1.4415 (2.2305)	loss_scale 2048.0000 (1992.7408)	mem 17147MB
[2024-08-02 08:05:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:06:54 lr 0.000002	 wd 0.0500	time 0.4487 (0.4591)	loss 1.2411 (1.0936)	grad_norm 2.2935 (2.2388)	loss_scale 2048.0000 (1996.1924)	mem 17147MB
[2024-08-02 08:06:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:06:07 lr 0.000002	 wd 0.0500	time 0.4497 (0.4588)	loss 1.5302 (1.0950)	grad_norm 1.6927 (2.2386)	loss_scale 2048.0000 (1999.2381)	mem 17147MB
[2024-08-02 08:06:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:05:21 lr 0.000002	 wd 0.0500	time 0.4490 (0.4585)	loss 1.1333 (1.0954)	grad_norm 3.0522 (2.2493)	loss_scale 2048.0000 (2001.9456)	mem 17147MB
[2024-08-02 08:07:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:04:35 lr 0.000002	 wd 0.0500	time 0.4502 (0.4582)	loss 0.8005 (1.0960)	grad_norm 1.9966 (2.2433)	loss_scale 2048.0000 (2004.3682)	mem 17147MB
[2024-08-02 08:08:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:03:49 lr 0.000002	 wd 0.0500	time 0.4493 (0.4580)	loss 1.2188 (1.0957)	grad_norm 1.5073 (2.2357)	loss_scale 2048.0000 (2006.5487)	mem 17147MB
[2024-08-02 08:09:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:03:04 lr 0.000002	 wd 0.0500	time 0.4475 (0.4578)	loss 0.9628 (1.0954)	grad_norm 1.6279 (2.2326)	loss_scale 2048.0000 (2008.5217)	mem 17147MB
[2024-08-02 08:09:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:02:18 lr 0.000002	 wd 0.0500	time 0.4453 (0.4576)	loss 0.8898 (1.0971)	grad_norm 3.3744 (2.2300)	loss_scale 2048.0000 (2010.3153)	mem 17147MB
[2024-08-02 08:10:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:32 lr 0.000002	 wd 0.0500	time 0.4486 (0.4574)	loss 1.2530 (1.0981)	grad_norm 2.5690 (2.2172)	loss_scale 2048.0000 (2011.9531)	mem 17147MB
[2024-08-02 08:11:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:46 lr 0.000002	 wd 0.0500	time 0.4472 (0.4572)	loss 1.3536 (1.0959)	grad_norm 1.3185 (2.2270)	loss_scale 2048.0000 (2013.4544)	mem 17147MB
[2024-08-02 08:12:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.4495 (0.4571)	loss 0.7453 (1.0962)	grad_norm 1.9653 (2.2385)	loss_scale 2048.0000 (2014.8357)	mem 17147MB
[2024-08-02 08:12:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 24 training takes 0:19:09
[2024-08-02 08:12:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.853 (11.853)	Loss 0.4478 (0.4478)	Acc@1 93.359 (93.359)	Acc@5 99.023 (99.023)	Mem 17147MB
[2024-08-02 08:12:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.894 Acc@5 98.130
[2024-08-02 08:12:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 08:12:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 08:13:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:14:33 lr 0.000002	 wd 0.0500	time 11.8599 (11.8599)	loss 1.2780 (1.2780)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:13:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:22:33 lr 0.000002	 wd 0.0500	time 0.4479 (0.5634)	loss 1.1522 (1.1597)	grad_norm 2.2451 (2.0196)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:14:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:19:27 lr 0.000002	 wd 0.0500	time 0.4457 (0.5070)	loss 1.4063 (1.1286)	grad_norm 1.8418 (2.1226)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:15:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:17:54 lr 0.000002	 wd 0.0500	time 0.4468 (0.4882)	loss 1.1551 (1.1128)	grad_norm 1.5924 (2.0653)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:16:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:16:46 lr 0.000002	 wd 0.0500	time 0.4459 (0.4787)	loss 1.1196 (1.1090)	grad_norm 1.4915 (2.1617)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:16:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:15:47 lr 0.000002	 wd 0.0500	time 0.4442 (0.4733)	loss 1.2549 (1.1064)	grad_norm 2.1841 (2.1564)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:17:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:14:53 lr 0.000002	 wd 0.0500	time 0.4447 (0.4696)	loss 0.8761 (1.0977)	grad_norm 2.9128 (2.1673)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:18:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:14:01 lr 0.000002	 wd 0.0500	time 0.4446 (0.4671)	loss 0.7493 (1.0993)	grad_norm 3.2718 (2.1436)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:19:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:13:11 lr 0.000002	 wd 0.0500	time 0.4468 (0.4652)	loss 1.3147 (1.0972)	grad_norm 2.1345 (2.1501)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:19:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:12:23 lr 0.000002	 wd 0.0500	time 0.4469 (0.4638)	loss 1.2169 (1.0973)	grad_norm 1.9023 (2.1482)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:20:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:11:34 lr 0.000002	 wd 0.0500	time 0.4500 (0.4627)	loss 1.5311 (1.0978)	grad_norm 2.2259 (2.1371)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:21:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:10:47 lr 0.000002	 wd 0.0500	time 0.4440 (0.4618)	loss 1.3642 (1.0997)	grad_norm 2.2413 (2.1512)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:22:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:10:00 lr 0.000002	 wd 0.0500	time 0.4434 (0.4610)	loss 0.9359 (1.0963)	grad_norm 1.6677 (2.1608)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:22:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:09:13 lr 0.000002	 wd 0.0500	time 0.4493 (0.4604)	loss 0.8955 (1.0959)	grad_norm 2.2679 (2.1816)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:23:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:08:26 lr 0.000002	 wd 0.0500	time 0.4497 (0.4598)	loss 0.9595 (1.0959)	grad_norm 1.4625 (2.1836)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:24:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:07:40 lr 0.000002	 wd 0.0500	time 0.4490 (0.4593)	loss 0.7491 (1.0953)	grad_norm 1.9622 (2.1883)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:25:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:06:53 lr 0.000002	 wd 0.0500	time 0.4486 (0.4589)	loss 0.8184 (1.0945)	grad_norm 2.9599 (2.1906)	loss_scale 4096.0000 (2076.1424)	mem 17147MB
[2024-08-02 08:25:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:06:07 lr 0.000002	 wd 0.0500	time 0.4505 (0.4585)	loss 0.7155 (1.0922)	grad_norm 1.2802 (2.2040)	loss_scale 4096.0000 (2194.8877)	mem 17147MB
[2024-08-02 08:26:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:05:21 lr 0.000002	 wd 0.0500	time 0.4506 (0.4582)	loss 0.7910 (1.0938)	grad_norm 1.9643 (2.2028)	loss_scale 4096.0000 (2300.4464)	mem 17147MB
[2024-08-02 08:27:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:04:35 lr 0.000002	 wd 0.0500	time 0.4513 (0.4579)	loss 1.4085 (1.0956)	grad_norm 1.7062 (2.2311)	loss_scale 4096.0000 (2394.8995)	mem 17147MB
[2024-08-02 08:28:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:03:49 lr 0.000002	 wd 0.0500	time 0.4515 (0.4576)	loss 0.8781 (1.0957)	grad_norm 1.3803 (2.2254)	loss_scale 4096.0000 (2479.9120)	mem 17147MB
[2024-08-02 08:28:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:03:03 lr 0.000002	 wd 0.0500	time 0.4499 (0.4574)	loss 1.2378 (1.0965)	grad_norm 2.0049 (2.2068)	loss_scale 4096.0000 (2556.8320)	mem 17147MB
[2024-08-02 08:29:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:02:18 lr 0.000001	 wd 0.0500	time 0.4479 (0.4572)	loss 1.0598 (1.0969)	grad_norm 1.6806 (2.1957)	loss_scale 4096.0000 (2626.7624)	mem 17147MB
[2024-08-02 08:30:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:32 lr 0.000001	 wd 0.0500	time 0.4509 (0.4571)	loss 1.3120 (1.0968)	grad_norm 1.5557 (2.1986)	loss_scale 4096.0000 (2690.6145)	mem 17147MB
[2024-08-02 08:31:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:46 lr 0.000001	 wd 0.0500	time 0.4502 (0.4569)	loss 1.4332 (1.0964)	grad_norm 1.3871 (nan)	loss_scale 2048.0000 (2740.6181)	mem 17147MB
[2024-08-02 08:31:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.4448 (0.4567)	loss 0.9817 (1.0951)	grad_norm 2.4808 (nan)	loss_scale 2048.0000 (2712.9244)	mem 17147MB
[2024-08-02 08:32:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 25 training takes 0:19:08
[2024-08-02 08:32:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.301 (11.301)	Loss 0.4692 (0.4692)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 08:32:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.930 Acc@5 98.092
[2024-08-02 08:32:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 08:32:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 08:32:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:43:06 lr 0.000001	 wd 0.0500	time 11.1057 (11.1057)	loss 1.0382 (1.0382)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:33:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:22:21 lr 0.000001	 wd 0.0500	time 0.4433 (0.5583)	loss 1.2578 (1.1040)	grad_norm 1.4499 (2.2285)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:34:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:19:20 lr 0.000001	 wd 0.0500	time 0.4433 (0.5041)	loss 0.9929 (1.0991)	grad_norm 1.9117 (2.3122)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:35:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:17:50 lr 0.000001	 wd 0.0500	time 0.4484 (0.4862)	loss 0.8613 (1.0873)	grad_norm 1.9024 (2.2269)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:35:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:16:43 lr 0.000001	 wd 0.0500	time 0.4397 (0.4773)	loss 1.4020 (1.0875)	grad_norm 1.5044 (2.1877)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:36:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:15:45 lr 0.000001	 wd 0.0500	time 0.4470 (0.4721)	loss 1.2915 (1.0961)	grad_norm 1.9283 (2.1723)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:37:20 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:14:51 lr 0.000001	 wd 0.0500	time 0.4458 (0.4687)	loss 0.7551 (1.0986)	grad_norm 1.2796 (2.1378)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:38:06 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:14:00 lr 0.000001	 wd 0.0500	time 0.4499 (0.4663)	loss 1.1671 (1.0946)	grad_norm 1.2259 (2.1770)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:38:51 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:13:10 lr 0.000001	 wd 0.0500	time 0.4499 (0.4646)	loss 1.2512 (1.0990)	grad_norm 1.7730 (2.1599)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:39:36 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:12:22 lr 0.000001	 wd 0.0500	time 0.4471 (0.4633)	loss 1.1519 (1.0993)	grad_norm 1.3162 (2.1484)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:40:21 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:11:34 lr 0.000001	 wd 0.0500	time 0.4450 (0.4622)	loss 0.9866 (1.0984)	grad_norm 2.2599 (2.1181)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:41:07 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:10:46 lr 0.000001	 wd 0.0500	time 0.4507 (0.4613)	loss 0.8067 (1.0972)	grad_norm 1.4313 (2.1138)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:41:52 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:09:59 lr 0.000001	 wd 0.0500	time 0.4502 (0.4606)	loss 0.9015 (1.0976)	grad_norm 1.3891 (2.1364)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:42:37 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:09:12 lr 0.000001	 wd 0.0500	time 0.4439 (0.4599)	loss 1.2030 (1.0996)	grad_norm 2.0213 (2.1233)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:43:22 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:08:26 lr 0.000001	 wd 0.0500	time 0.4449 (0.4594)	loss 1.3588 (1.1015)	grad_norm 1.1766 (2.1288)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:44:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:07:39 lr 0.000001	 wd 0.0500	time 0.4438 (0.4589)	loss 1.2927 (1.1031)	grad_norm 1.8215 (2.1196)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:44:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:06:53 lr 0.000001	 wd 0.0500	time 0.4481 (0.4585)	loss 1.1058 (1.1019)	grad_norm 1.8725 (2.1239)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:45:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:06:07 lr 0.000001	 wd 0.0500	time 0.4497 (0.4582)	loss 1.0861 (1.0986)	grad_norm 2.5534 (2.1159)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:46:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:05:21 lr 0.000001	 wd 0.0500	time 0.4488 (0.4578)	loss 0.7274 (1.0978)	grad_norm 2.9204 (2.1268)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:47:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:04:35 lr 0.000001	 wd 0.0500	time 0.4458 (0.4575)	loss 1.2992 (1.0994)	grad_norm 1.6150 (2.1505)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:47:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:03:49 lr 0.000001	 wd 0.0500	time 0.4482 (0.4573)	loss 1.1235 (1.0981)	grad_norm 2.7779 (2.1753)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:48:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:03:03 lr 0.000001	 wd 0.0500	time 0.4487 (0.4570)	loss 1.2901 (1.0976)	grad_norm 2.0850 (2.1699)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:49:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:02:17 lr 0.000001	 wd 0.0500	time 0.4467 (0.4568)	loss 1.2167 (1.0959)	grad_norm 2.1005 (2.2336)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:50:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:32 lr 0.000001	 wd 0.0500	time 0.4489 (0.4566)	loss 1.3356 (1.0962)	grad_norm 1.2670 (2.2249)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:50:55 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:46 lr 0.000001	 wd 0.0500	time 0.4488 (0.4565)	loss 0.7178 (1.0968)	grad_norm 1.6257 (2.2170)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:51:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.4449 (0.4563)	loss 0.7946 (1.0968)	grad_norm 2.4125 (2.2201)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:51:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 26 training takes 0:19:08
[2024-08-02 08:51:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.020 (12.020)	Loss 0.4548 (0.4548)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 08:52:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.892 Acc@5 98.112
[2024-08-02 08:52:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 08:52:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 08:52:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:47:39 lr 0.000001	 wd 0.0500	time 12.6537 (12.6537)	loss 1.3182 (1.3182)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:53:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:22:51 lr 0.000001	 wd 0.0500	time 0.4465 (0.5709)	loss 1.1772 (1.1052)	grad_norm 1.8396 (2.0462)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:54:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:19:35 lr 0.000001	 wd 0.0500	time 0.4484 (0.5108)	loss 1.1633 (1.1108)	grad_norm 1.8871 (2.1019)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:54:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:18:00 lr 0.000001	 wd 0.0500	time 0.4455 (0.4907)	loss 1.2475 (1.1079)	grad_norm 1.6180 (2.1422)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:55:38 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:16:50 lr 0.000001	 wd 0.0500	time 0.4469 (0.4806)	loss 0.9001 (1.1067)	grad_norm 2.0901 (2.2092)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:56:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:15:50 lr 0.000001	 wd 0.0500	time 0.4489 (0.4748)	loss 0.7196 (1.1073)	grad_norm 2.6011 (2.2905)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:57:08 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:14:55 lr 0.000001	 wd 0.0500	time 0.4486 (0.4710)	loss 1.3337 (1.1068)	grad_norm 1.4855 (2.2367)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:57:53 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:14:03 lr 0.000001	 wd 0.0500	time 0.4500 (0.4683)	loss 0.7527 (1.1069)	grad_norm 2.3293 (2.1834)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:58:39 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:13:13 lr 0.000001	 wd 0.0500	time 0.4464 (0.4663)	loss 1.0715 (1.1068)	grad_norm 1.4120 (2.1573)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 08:59:24 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:12:24 lr 0.000001	 wd 0.0500	time 0.4456 (0.4648)	loss 1.2683 (1.1052)	grad_norm 1.2641 (2.1643)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:00:09 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:11:36 lr 0.000001	 wd 0.0500	time 0.4459 (0.4636)	loss 0.8712 (1.1011)	grad_norm 1.4800 (2.2061)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:00:54 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:10:48 lr 0.000001	 wd 0.0500	time 0.4463 (0.4626)	loss 1.1092 (1.1027)	grad_norm 1.9390 (2.2122)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:01:40 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:10:01 lr 0.000001	 wd 0.0500	time 0.4505 (0.4618)	loss 0.7972 (1.1024)	grad_norm 1.5061 (2.2498)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:02:25 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:09:14 lr 0.000001	 wd 0.0500	time 0.4475 (0.4611)	loss 0.7535 (1.0981)	grad_norm 1.7820 (2.2295)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:03:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:08:27 lr 0.000001	 wd 0.0500	time 0.4481 (0.4605)	loss 0.8500 (1.0995)	grad_norm 1.7392 (2.2523)	loss_scale 4096.0000 (2068.4654)	mem 17147MB
[2024-08-02 09:03:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:07:40 lr 0.000001	 wd 0.0500	time 0.4478 (0.4600)	loss 1.3810 (1.1009)	grad_norm 1.7681 (nan)	loss_scale 2048.0000 (2162.6116)	mem 17147MB
[2024-08-02 09:04:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:06:54 lr 0.000001	 wd 0.0500	time 0.4498 (0.4596)	loss 1.4604 (1.1022)	grad_norm 2.9334 (nan)	loss_scale 2048.0000 (2155.4528)	mem 17147MB
[2024-08-02 09:05:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:06:08 lr 0.000001	 wd 0.0500	time 0.4458 (0.4592)	loss 1.2939 (1.1031)	grad_norm 1.6965 (nan)	loss_scale 2048.0000 (2149.1358)	mem 17147MB
[2024-08-02 09:06:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:05:22 lr 0.000001	 wd 0.0500	time 0.4484 (0.4588)	loss 1.2634 (1.1022)	grad_norm 2.2338 (nan)	loss_scale 2048.0000 (2143.5203)	mem 17147MB
[2024-08-02 09:06:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:04:36 lr 0.000001	 wd 0.0500	time 0.4500 (0.4585)	loss 1.2885 (1.1028)	grad_norm 4.1833 (nan)	loss_scale 2048.0000 (2138.4955)	mem 17147MB
[2024-08-02 09:07:42 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:03:50 lr 0.000001	 wd 0.0500	time 0.4474 (0.4582)	loss 1.3099 (1.1033)	grad_norm 2.4048 (nan)	loss_scale 2048.0000 (2133.9730)	mem 17147MB
[2024-08-02 09:08:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:03:04 lr 0.000001	 wd 0.0500	time 0.4465 (0.4580)	loss 1.4556 (1.1028)	grad_norm 7.0044 (nan)	loss_scale 2048.0000 (2129.8810)	mem 17147MB
[2024-08-02 09:09:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:02:18 lr 0.000001	 wd 0.0500	time 0.4500 (0.4578)	loss 1.1863 (1.1035)	grad_norm 8.1585 (nan)	loss_scale 2048.0000 (2126.1608)	mem 17147MB
[2024-08-02 09:09:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:32 lr 0.000001	 wd 0.0500	time 0.4480 (0.4576)	loss 1.2589 (1.1035)	grad_norm 1.8192 (nan)	loss_scale 2048.0000 (2122.7640)	mem 17147MB
[2024-08-02 09:10:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:46 lr 0.000001	 wd 0.0500	time 0.4485 (0.4574)	loss 1.2922 (1.1048)	grad_norm 2.3087 (nan)	loss_scale 2048.0000 (2119.6501)	mem 17147MB
[2024-08-02 09:11:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.4482 (0.4572)	loss 1.1263 (1.1039)	grad_norm 1.8597 (nan)	loss_scale 2048.0000 (2116.7853)	mem 17147MB
[2024-08-02 09:11:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 27 training takes 0:19:10
[2024-08-02 09:11:47 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.987 (11.987)	Loss 0.4434 (0.4434)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 09:12:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.934 Acc@5 98.102
[2024-08-02 09:12:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 09:12:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.94%
[2024-08-02 09:12:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][0/2502]	eta 8:09:47 lr 0.000001	 wd 0.0500	time 11.7455 (11.7455)	loss 0.7487 (0.7487)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:13:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:22:31 lr 0.000000	 wd 0.0500	time 0.4394 (0.5625)	loss 0.7096 (1.1246)	grad_norm 2.2751 (2.4037)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:13:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:19:25 lr 0.000000	 wd 0.0500	time 0.4440 (0.5064)	loss 1.3284 (1.1214)	grad_norm 2.2614 (2.2012)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:14:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:17:53 lr 0.000000	 wd 0.0500	time 0.4441 (0.4877)	loss 1.3563 (1.1132)	grad_norm 2.0517 (2.2150)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:15:26 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:16:45 lr 0.000000	 wd 0.0500	time 0.4466 (0.4783)	loss 0.7274 (1.1048)	grad_norm 1.1396 (2.1542)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:16:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:15:46 lr 0.000000	 wd 0.0500	time 0.4454 (0.4730)	loss 0.7731 (1.1094)	grad_norm 1.5310 (2.2514)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:16:56 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:14:52 lr 0.000000	 wd 0.0500	time 0.4443 (0.4694)	loss 0.8491 (1.1023)	grad_norm 1.7155 (2.1970)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:17:41 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:14:01 lr 0.000000	 wd 0.0500	time 0.4464 (0.4670)	loss 1.1831 (1.0991)	grad_norm 1.6651 (2.1716)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:18:27 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:13:11 lr 0.000000	 wd 0.0500	time 0.4510 (0.4651)	loss 0.7723 (1.1005)	grad_norm 1.8112 (2.1751)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:19:12 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:12:22 lr 0.000000	 wd 0.0500	time 0.4471 (0.4638)	loss 0.7087 (1.0973)	grad_norm 4.2975 (2.1735)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:19:57 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:11:35 lr 0.000000	 wd 0.0500	time 0.4500 (0.4627)	loss 0.9720 (1.0983)	grad_norm 2.4968 (2.1854)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:20:43 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:10:47 lr 0.000000	 wd 0.0500	time 0.4485 (0.4619)	loss 1.1160 (1.0995)	grad_norm 1.5557 (2.1735)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:21:28 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:10:00 lr 0.000000	 wd 0.0500	time 0.4506 (0.4611)	loss 1.3216 (1.1000)	grad_norm 11.8725 (2.1936)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:22:13 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:09:13 lr 0.000000	 wd 0.0500	time 0.4481 (0.4604)	loss 1.3046 (1.1018)	grad_norm 1.5492 (2.2377)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:22:58 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:08:26 lr 0.000000	 wd 0.0500	time 0.4475 (0.4599)	loss 0.8503 (1.1005)	grad_norm 1.1465 (2.2298)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:23:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:07:40 lr 0.000000	 wd 0.0500	time 0.4501 (0.4594)	loss 1.1859 (1.1003)	grad_norm 1.7235 (2.2091)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:24:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:06:53 lr 0.000000	 wd 0.0500	time 0.4494 (0.4590)	loss 1.3731 (1.1025)	grad_norm 4.1264 (2.2084)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:25:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:06:07 lr 0.000000	 wd 0.0500	time 0.4502 (0.4586)	loss 1.3041 (1.1023)	grad_norm 1.9437 (2.2090)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:26:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:05:21 lr 0.000000	 wd 0.0500	time 0.4507 (0.4583)	loss 0.6985 (1.1019)	grad_norm 1.7599 (2.2183)	loss_scale 2048.0000 (2048.0000)	mem 17147MB
[2024-08-02 09:26:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:04:35 lr 0.000000	 wd 0.0500	time 0.4497 (0.4581)	loss 0.9556 (1.0994)	grad_norm 1.8553 (nan)	loss_scale 1024.0000 (2012.4482)	mem 17147MB
[2024-08-02 09:27:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:03:49 lr 0.000000	 wd 0.0500	time 0.4513 (0.4578)	loss 1.1020 (1.0995)	grad_norm 1.7818 (nan)	loss_scale 1024.0000 (1963.0505)	mem 17147MB
[2024-08-02 09:28:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:03:03 lr 0.000000	 wd 0.0500	time 0.4500 (0.4576)	loss 0.9975 (1.1000)	grad_norm 1.9732 (nan)	loss_scale 1024.0000 (1918.3551)	mem 17147MB
[2024-08-02 09:29:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:02:18 lr 0.000000	 wd 0.0500	time 0.4461 (0.4573)	loss 1.2562 (1.0993)	grad_norm 1.8316 (nan)	loss_scale 1024.0000 (1877.7210)	mem 17147MB
[2024-08-02 09:29:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:32 lr 0.000000	 wd 0.0500	time 0.4505 (0.4571)	loss 1.2438 (1.0991)	grad_norm 1.4463 (nan)	loss_scale 1024.0000 (1840.6189)	mem 17147MB
[2024-08-02 09:30:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:46 lr 0.000000	 wd 0.0500	time 0.4461 (0.4570)	loss 1.4762 (1.0983)	grad_norm 1.6774 (nan)	loss_scale 1024.0000 (1806.6072)	mem 17147MB
[2024-08-02 09:31:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.4471 (0.4568)	loss 1.0139 (1.0974)	grad_norm 3.4831 (nan)	loss_scale 1024.0000 (1775.3155)	mem 17147MB
[2024-08-02 09:31:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 28 training takes 0:19:09
[2024-08-02 09:31:35 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.579 (11.579)	Loss 0.4502 (0.4502)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 09:32:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.946 Acc@5 98.130
[2024-08-02 09:32:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-02 09:32:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.95%
[2024-08-02 09:32:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 09:32:03 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 09:32:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:55:39 lr 0.000000	 wd 0.0500	time 11.4068 (11.4068)	loss 1.2843 (1.2843)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:32:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:22:18 lr 0.000000	 wd 0.0500	time 0.4482 (0.5574)	loss 1.2940 (1.1130)	grad_norm 4.6294 (1.9642)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:33:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:19:19 lr 0.000000	 wd 0.0500	time 0.4459 (0.5039)	loss 0.7776 (1.1136)	grad_norm 1.6608 (2.0106)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:34:29 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:17:50 lr 0.000000	 wd 0.0500	time 0.4454 (0.4860)	loss 0.6804 (1.1022)	grad_norm 2.4488 (2.1958)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:35:14 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:16:42 lr 0.000000	 wd 0.0500	time 0.4427 (0.4771)	loss 1.0073 (1.0984)	grad_norm 1.9546 (2.2009)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:35:59 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:15:44 lr 0.000000	 wd 0.0500	time 0.4460 (0.4719)	loss 1.1373 (1.0938)	grad_norm 3.1310 (2.2992)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:36:44 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:14:51 lr 0.000000	 wd 0.0500	time 0.4441 (0.4685)	loss 0.6865 (1.0961)	grad_norm 1.8191 (2.3360)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:37:30 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:14:00 lr 0.000000	 wd 0.0500	time 0.4494 (0.4662)	loss 1.1399 (1.0959)	grad_norm 2.0704 (2.2710)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:38:15 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:13:10 lr 0.000000	 wd 0.0500	time 0.4507 (0.4645)	loss 0.8212 (1.0972)	grad_norm 2.0381 (2.2563)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:39:00 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:12:21 lr 0.000000	 wd 0.0500	time 0.4451 (0.4632)	loss 1.3830 (1.0978)	grad_norm 2.6287 (2.2682)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:39:45 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:11:34 lr 0.000000	 wd 0.0500	time 0.4475 (0.4621)	loss 1.2342 (1.0983)	grad_norm 1.8363 (2.3052)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:40:31 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:10:46 lr 0.000000	 wd 0.0500	time 0.4458 (0.4613)	loss 1.2297 (1.0961)	grad_norm 2.1809 (2.3682)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:41:16 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:09:59 lr 0.000000	 wd 0.0500	time 0.4513 (0.4605)	loss 0.9401 (1.0952)	grad_norm 2.0185 (2.3582)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:42:01 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:09:12 lr 0.000000	 wd 0.0500	time 0.4445 (0.4599)	loss 1.0738 (1.0987)	grad_norm 2.7329 (2.3456)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:42:46 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:08:26 lr 0.000000	 wd 0.0500	time 0.4500 (0.4594)	loss 1.0641 (1.0939)	grad_norm 2.3456 (2.3245)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:43:32 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:07:39 lr 0.000000	 wd 0.0500	time 0.4483 (0.4590)	loss 0.9624 (1.0932)	grad_norm 2.0069 (2.3013)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:44:17 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:06:53 lr 0.000000	 wd 0.0500	time 0.4502 (0.4586)	loss 0.7570 (1.0959)	grad_norm 1.5120 (2.2868)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:45:02 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:06:07 lr 0.000000	 wd 0.0500	time 0.4499 (0.4582)	loss 1.1533 (1.0979)	grad_norm 1.8945 (2.2702)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:45:48 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:05:21 lr 0.000000	 wd 0.0500	time 0.4507 (0.4580)	loss 1.2240 (1.0971)	grad_norm 1.9425 (2.2535)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:46:33 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:04:35 lr 0.000000	 wd 0.0500	time 0.4478 (0.4577)	loss 0.8496 (1.0973)	grad_norm 1.8062 (2.2503)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:47:18 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:03:49 lr 0.000000	 wd 0.0500	time 0.4505 (0.4575)	loss 0.8778 (1.0992)	grad_norm 1.5541 (2.2520)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:48:04 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:03:03 lr 0.000000	 wd 0.0500	time 0.4446 (0.4573)	loss 0.8037 (1.1004)	grad_norm 2.2213 (2.2509)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:48:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:02:18 lr 0.000000	 wd 0.0500	time 0.4475 (0.4571)	loss 0.8171 (1.1010)	grad_norm 1.5673 (2.2555)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:49:34 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:32 lr 0.000000	 wd 0.0500	time 0.4492 (0.4569)	loss 0.6870 (1.0981)	grad_norm 1.6069 (2.2492)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:50:19 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:46 lr 0.000000	 wd 0.0500	time 0.4440 (0.4567)	loss 1.1809 (1.0983)	grad_norm 6.7941 (2.2429)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:51:05 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.4468 (0.4565)	loss 1.1916 (1.0978)	grad_norm 4.8430 (2.2376)	loss_scale 1024.0000 (1024.0000)	mem 17147MB
[2024-08-02 09:51:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 29 training takes 0:19:07
[2024-08-02 09:51:10 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_29.pth saving......
[2024-08-02 09:51:11 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_29.pth saved !!!
[2024-08-02 09:51:23 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.882 (11.882)	Loss 0.4409 (0.4409)	Acc@1 93.359 (93.359)	Acc@5 98.828 (98.828)	Mem 17147MB
[2024-08-02 09:51:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.968 Acc@5 98.104
[2024-08-02 09:51:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 87.0%
[2024-08-02 09:51:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.97%
[2024-08-02 09:51:49 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-08-02 09:51:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-08-02 09:51:50 smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 189): INFO Training time 9:51:08
