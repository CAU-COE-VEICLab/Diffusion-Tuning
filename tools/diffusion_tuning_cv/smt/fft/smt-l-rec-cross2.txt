[2024-08-06 13:04:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/config.json
[2024-08-06 13:04:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: fullfinetune
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_step_cross2
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: false
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-06 13:04:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_step_cross2.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_step_cross2", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-06 13:04:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft
[2024-08-06 13:04:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-06 13:04:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 113): INFO number of params: 80620264
[2024-08-06 13:04:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2, ignoring auto resume
[2024-08-06 13:04:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth for fine-tuning......
[2024-08-06 13:04:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-06 13:04:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer1/diffusion_ft_smt_l_step_cross1/ckpt_epoch_best.pth'
[2024-08-06 13:04:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.866 (12.866)	Loss 0.4578 (0.4578)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 2730MB
[2024-08-06 13:05:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.882 Acc@5 98.042
[2024-08-06 13:05:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 13:05:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 168): INFO Start training
[2024-08-06 13:05:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:23:20 lr 0.000000	 wd 0.0500	time 13.5093 (13.5093)	loss 1.4295 (1.4295)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 23350MB
[2024-08-06 13:06:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:41:01 lr 0.000000	 wd 0.0500	time 0.8311 (1.0247)	loss 1.2163 (1.1321)	grad_norm 2.7361 (nan)	loss_scale 8192.0000 (18006.1782)	mem 23350MB
[2024-08-06 13:08:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:36:54 lr 0.000000	 wd 0.0500	time 0.8972 (0.9620)	loss 0.9804 (1.1271)	grad_norm 2.6672 (nan)	loss_scale 8192.0000 (13123.5025)	mem 23350MB
[2024-08-06 13:09:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:34:30 lr 0.000000	 wd 0.0500	time 0.8346 (0.9405)	loss 0.8485 (1.0890)	grad_norm 2.9821 (nan)	loss_scale 8192.0000 (11485.1296)	mem 23350MB
[2024-08-06 13:11:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:32:33 lr 0.000001	 wd 0.0500	time 0.8154 (0.9295)	loss 1.0472 (1.0961)	grad_norm 2.3714 (nan)	loss_scale 8192.0000 (10663.9002)	mem 23350MB
[2024-08-06 13:12:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:30:49 lr 0.000001	 wd 0.0500	time 0.7929 (0.9238)	loss 1.1127 (1.0974)	grad_norm 2.2809 (nan)	loss_scale 8192.0000 (10170.5070)	mem 23350MB
[2024-08-06 13:14:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:29:10 lr 0.000001	 wd 0.0500	time 0.9010 (0.9201)	loss 1.2043 (1.0962)	grad_norm 2.4329 (nan)	loss_scale 8192.0000 (9841.3045)	mem 23350MB
[2024-08-06 13:15:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:27:32 lr 0.000001	 wd 0.0500	time 0.8554 (0.9171)	loss 1.2064 (1.0988)	grad_norm 2.7999 (nan)	loss_scale 4096.0000 (9208.6961)	mem 23350MB
[2024-08-06 13:17:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:25:56 lr 0.000001	 wd 0.0500	time 0.8442 (0.9147)	loss 1.0162 (1.0976)	grad_norm 2.3191 (nan)	loss_scale 4096.0000 (8570.4070)	mem 23350MB
[2024-08-06 13:18:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:24:22 lr 0.000001	 wd 0.0500	time 0.8174 (0.9128)	loss 1.2402 (1.0979)	grad_norm 4.0470 (nan)	loss_scale 2048.0000 (7964.6970)	mem 23350MB
[2024-08-06 13:20:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:22:48 lr 0.000002	 wd 0.0500	time 0.8770 (0.9111)	loss 1.3694 (1.0952)	grad_norm 2.5289 (nan)	loss_scale 2048.0000 (7373.6184)	mem 23350MB
[2024-08-06 13:21:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:21:16 lr 0.000002	 wd 0.0500	time 0.8473 (0.9103)	loss 1.2722 (1.0966)	grad_norm 3.3868 (nan)	loss_scale 2048.0000 (6889.9110)	mem 23350MB
[2024-08-06 13:23:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:19:43 lr 0.000002	 wd 0.0500	time 0.8396 (0.9093)	loss 0.9593 (1.0961)	grad_norm 2.5287 (nan)	loss_scale 2048.0000 (6486.7544)	mem 23350MB
[2024-08-06 13:24:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:18:11 lr 0.000002	 wd 0.0500	time 0.8031 (0.9084)	loss 1.1519 (1.0978)	grad_norm 2.4194 (nan)	loss_scale 2048.0000 (6145.5742)	mem 23350MB
[2024-08-06 13:26:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:16:40 lr 0.000002	 wd 0.0500	time 0.8647 (0.9075)	loss 1.3762 (1.0988)	grad_norm 2.0146 (nan)	loss_scale 2048.0000 (5853.0992)	mem 23350MB
[2024-08-06 13:27:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:15:08 lr 0.000002	 wd 0.0500	time 0.8487 (0.9070)	loss 0.9305 (1.1019)	grad_norm 2.4458 (nan)	loss_scale 2048.0000 (5599.5949)	mem 23350MB
[2024-08-06 13:29:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:13:37 lr 0.000003	 wd 0.0500	time 0.8779 (0.9064)	loss 1.1640 (1.0993)	grad_norm 2.2309 (nan)	loss_scale 2048.0000 (5377.7589)	mem 23350MB
[2024-08-06 13:30:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:12:06 lr 0.000003	 wd 0.0500	time 0.8342 (0.9062)	loss 0.9200 (1.0988)	grad_norm 3.0034 (nan)	loss_scale 2048.0000 (5182.0059)	mem 23350MB
[2024-08-06 13:32:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:10:35 lr 0.000003	 wd 0.0500	time 0.8522 (0.9058)	loss 1.0184 (1.0989)	grad_norm 2.7132 (nan)	loss_scale 2048.0000 (5007.9911)	mem 23350MB
[2024-08-06 13:33:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:09:05 lr 0.000003	 wd 0.0500	time 0.8298 (0.9054)	loss 1.4355 (1.0986)	grad_norm 2.2730 (nan)	loss_scale 2048.0000 (4852.2841)	mem 23350MB
[2024-08-06 13:35:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:07:34 lr 0.000003	 wd 0.0500	time 0.8158 (0.9051)	loss 0.7603 (1.0976)	grad_norm 2.4982 (nan)	loss_scale 2048.0000 (4712.1399)	mem 23350MB
[2024-08-06 13:36:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:06:03 lr 0.000003	 wd 0.0500	time 0.8365 (0.9049)	loss 0.9365 (1.0964)	grad_norm 3.0172 (nan)	loss_scale 2048.0000 (4585.3365)	mem 23350MB
[2024-08-06 13:38:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:04:33 lr 0.000004	 wd 0.0500	time 0.7852 (0.9045)	loss 1.4146 (1.0955)	grad_norm 1.9377 (nan)	loss_scale 2048.0000 (4470.0554)	mem 23350MB
[2024-08-06 13:39:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:03:02 lr 0.000004	 wd 0.0500	time 0.8388 (0.9043)	loss 1.3193 (1.0954)	grad_norm 2.1803 (nan)	loss_scale 2048.0000 (4364.7944)	mem 23350MB
[2024-08-06 13:41:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:32 lr 0.000004	 wd 0.0500	time 0.8592 (0.9041)	loss 1.2332 (1.0948)	grad_norm 2.1467 (nan)	loss_scale 2048.0000 (4268.3015)	mem 23350MB
[2024-08-06 13:42:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.8371 (0.9038)	loss 0.9460 (1.0933)	grad_norm 2.5129 (nan)	loss_scale 2048.0000 (4179.5250)	mem 23350MB
[2024-08-06 13:42:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 0 training takes 0:37:43
[2024-08-06 13:42:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_0.pth saving......
[2024-08-06 13:43:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_0.pth saved !!!
[2024-08-06 13:43:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.678 (11.678)	Loss 0.4746 (0.4746)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-06 13:43:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.892 Acc@5 98.062
[2024-08-06 13:43:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 13:43:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 13:43:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_best.pth saving......
[2024-08-06 13:43:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_best.pth saved !!!
[2024-08-06 13:43:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:57:34 lr 0.000004	 wd 0.0500	time 11.4527 (11.4527)	loss 1.2702 (1.2702)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:45:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:40:08 lr 0.000004	 wd 0.0500	time 0.8430 (1.0026)	loss 1.2174 (1.0798)	grad_norm 2.5140 (3.2711)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:46:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:36:28 lr 0.000004	 wd 0.0500	time 0.8517 (0.9508)	loss 0.7259 (1.0820)	grad_norm 4.9104 (3.1818)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:48:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:34:14 lr 0.000004	 wd 0.0500	time 0.8414 (0.9329)	loss 0.8573 (1.0847)	grad_norm 2.3671 (3.2153)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:49:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:32:22 lr 0.000005	 wd 0.0500	time 0.8387 (0.9240)	loss 0.8532 (1.0990)	grad_norm 2.3464 (3.2096)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:51:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:30:42 lr 0.000005	 wd 0.0500	time 0.8388 (0.9201)	loss 1.4626 (1.1022)	grad_norm 2.8605 (3.2280)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:52:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:29:03 lr 0.000005	 wd 0.0500	time 0.8556 (0.9165)	loss 1.3665 (1.1013)	grad_norm 3.0277 (3.1727)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:54:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:27:27 lr 0.000005	 wd 0.0500	time 0.8514 (0.9140)	loss 0.7907 (1.0981)	grad_norm 3.7736 (3.1605)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:55:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:25:52 lr 0.000005	 wd 0.0500	time 0.8554 (0.9121)	loss 1.4751 (1.0974)	grad_norm 3.1071 (3.1257)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:57:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:24:18 lr 0.000005	 wd 0.0500	time 0.8351 (0.9107)	loss 1.1030 (1.0956)	grad_norm 4.9154 (3.1285)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 13:58:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:22:46 lr 0.000006	 wd 0.0500	time 0.8447 (0.9096)	loss 1.3869 (1.0923)	grad_norm 3.3934 (3.1385)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 14:00:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:21:13 lr 0.000006	 wd 0.0500	time 0.8370 (0.9084)	loss 1.4441 (1.0913)	grad_norm 2.7746 (3.1330)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 14:01:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:19:41 lr 0.000006	 wd 0.0500	time 0.8470 (0.9075)	loss 0.8924 (1.0895)	grad_norm 2.4991 (3.1374)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 14:03:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:18:10 lr 0.000006	 wd 0.0500	time 0.8517 (0.9070)	loss 1.1639 (1.0908)	grad_norm 3.8233 (3.1040)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 14:04:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:16:38 lr 0.000006	 wd 0.0500	time 0.8682 (0.9064)	loss 0.7577 (1.0909)	grad_norm 3.6889 (inf)	loss_scale 1024.0000 (2008.5310)	mem 23350MB
[2024-08-06 14:06:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:15:07 lr 0.000006	 wd 0.0500	time 0.8380 (0.9060)	loss 1.3443 (1.0909)	grad_norm 3.8881 (inf)	loss_scale 1024.0000 (1942.9394)	mem 23350MB
[2024-08-06 14:07:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:13:36 lr 0.000007	 wd 0.0500	time 0.8526 (0.9054)	loss 1.2614 (1.0920)	grad_norm 2.8664 (inf)	loss_scale 1024.0000 (1885.5415)	mem 23350MB
[2024-08-06 14:09:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:12:05 lr 0.000007	 wd 0.0500	time 0.8355 (0.9048)	loss 0.7908 (1.0926)	grad_norm 3.2271 (inf)	loss_scale 1024.0000 (1834.8924)	mem 23350MB
[2024-08-06 14:10:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:10:34 lr 0.000007	 wd 0.0500	time 0.8390 (0.9044)	loss 1.5430 (1.0918)	grad_norm 2.3157 (inf)	loss_scale 1024.0000 (1789.8679)	mem 23350MB
[2024-08-06 14:12:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:09:04 lr 0.000007	 wd 0.0500	time 0.8373 (0.9043)	loss 1.4146 (1.0934)	grad_norm 2.3439 (inf)	loss_scale 1024.0000 (1749.5802)	mem 23350MB
[2024-08-06 14:13:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:07:33 lr 0.000007	 wd 0.0500	time 0.8935 (0.9040)	loss 1.2030 (1.0939)	grad_norm 2.7369 (inf)	loss_scale 1024.0000 (1713.3193)	mem 23350MB
[2024-08-06 14:15:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:06:03 lr 0.000007	 wd 0.0500	time 0.8337 (0.9036)	loss 1.3157 (1.0958)	grad_norm 3.4339 (inf)	loss_scale 1024.0000 (1680.5102)	mem 23350MB
[2024-08-06 14:16:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:04:32 lr 0.000008	 wd 0.0500	time 0.8339 (0.9033)	loss 1.0683 (1.0962)	grad_norm 2.5379 (inf)	loss_scale 1024.0000 (1650.6824)	mem 23350MB
[2024-08-06 14:18:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.8441 (0.9031)	loss 0.7269 (1.0976)	grad_norm 3.4927 (inf)	loss_scale 1024.0000 (1623.4472)	mem 23350MB
[2024-08-06 14:19:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.8338 (0.9030)	loss 1.1565 (1.0975)	grad_norm 2.3142 (inf)	loss_scale 1024.0000 (1598.4806)	mem 23350MB
[2024-08-06 14:21:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.8396 (0.9028)	loss 0.8624 (1.0972)	grad_norm 2.0985 (inf)	loss_scale 1024.0000 (1575.5106)	mem 23350MB
[2024-08-06 14:21:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 1 training takes 0:37:41
[2024-08-06 14:21:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.781 (11.781)	Loss 0.4683 (0.4683)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-06 14:21:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.856 Acc@5 98.034
[2024-08-06 14:21:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-06 14:21:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 14:22:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][0/2502]	eta 8:43:11 lr 0.000008	 wd 0.0500	time 12.5465 (12.5465)	loss 1.0763 (1.0763)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:23:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:40:32 lr 0.000008	 wd 0.0500	time 0.8211 (1.0129)	loss 1.3742 (1.0964)	grad_norm 2.4844 (3.1427)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:25:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:36:38 lr 0.000008	 wd 0.0500	time 0.8341 (0.9551)	loss 1.1962 (1.0869)	grad_norm 1.8918 (3.1016)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:26:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:34:21 lr 0.000008	 wd 0.0500	time 0.8470 (0.9361)	loss 1.1724 (1.0762)	grad_norm 2.6842 (3.0135)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:28:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:32:29 lr 0.000009	 wd 0.0500	time 0.8504 (0.9273)	loss 1.2733 (1.0807)	grad_norm 2.4658 (3.1150)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:29:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:30:44 lr 0.000009	 wd 0.0500	time 0.8329 (0.9216)	loss 1.2514 (1.0794)	grad_norm 2.6608 (3.2251)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:31:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:29:05 lr 0.000009	 wd 0.0500	time 0.8544 (0.9177)	loss 1.1940 (1.0783)	grad_norm 2.3426 (3.1618)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:32:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:27:28 lr 0.000009	 wd 0.0500	time 0.8388 (0.9147)	loss 1.2688 (1.0826)	grad_norm 10.7986 (3.1266)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:34:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:25:53 lr 0.000009	 wd 0.0500	time 0.8521 (0.9130)	loss 1.1419 (1.0856)	grad_norm 2.5082 (3.0963)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:35:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:24:19 lr 0.000009	 wd 0.0500	time 0.8556 (0.9113)	loss 0.7861 (1.0891)	grad_norm 3.3802 (3.0843)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:36:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:22:46 lr 0.000010	 wd 0.0500	time 0.8507 (0.9099)	loss 0.8328 (1.0870)	grad_norm 2.8309 (3.0784)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:38:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:21:14 lr 0.000010	 wd 0.0500	time 0.8339 (0.9089)	loss 1.3268 (1.0860)	grad_norm 2.3947 (3.0767)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:39:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:19:41 lr 0.000010	 wd 0.0500	time 0.8391 (0.9078)	loss 0.9334 (1.0865)	grad_norm 2.7956 (3.0979)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:41:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:18:10 lr 0.000010	 wd 0.0500	time 0.8386 (0.9073)	loss 1.3326 (1.0868)	grad_norm 3.0741 (3.1070)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:42:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:16:39 lr 0.000010	 wd 0.0500	time 0.8467 (0.9067)	loss 1.1496 (1.0881)	grad_norm 2.9094 (3.0908)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:44:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:15:07 lr 0.000010	 wd 0.0500	time 0.8510 (0.9061)	loss 1.2885 (1.0886)	grad_norm 3.4873 (3.0678)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:45:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:13:36 lr 0.000011	 wd 0.0500	time 0.8339 (0.9056)	loss 0.9001 (1.0888)	grad_norm 2.5025 (3.0778)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:47:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:12:05 lr 0.000011	 wd 0.0500	time 0.8998 (0.9052)	loss 1.1722 (1.0888)	grad_norm 2.9928 (3.0983)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:48:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:10:35 lr 0.000011	 wd 0.0500	time 0.8448 (0.9050)	loss 1.0929 (1.0897)	grad_norm 2.7790 (3.0955)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:50:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:09:04 lr 0.000011	 wd 0.0500	time 0.8822 (0.9048)	loss 1.2724 (1.0893)	grad_norm 2.7147 (3.0830)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:51:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:07:34 lr 0.000011	 wd 0.0500	time 0.8472 (0.9044)	loss 0.9294 (1.0894)	grad_norm 2.1821 (3.0812)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:53:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:06:03 lr 0.000011	 wd 0.0500	time 0.8488 (0.9042)	loss 0.9441 (1.0885)	grad_norm 2.9117 (3.0847)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:54:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:04:32 lr 0.000012	 wd 0.0500	time 0.8463 (0.9038)	loss 1.1826 (1.0890)	grad_norm 4.6582 (3.0781)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:56:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:03:02 lr 0.000012	 wd 0.0500	time 0.8519 (0.9036)	loss 1.0474 (1.0888)	grad_norm 3.0190 (3.0587)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:57:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:32 lr 0.000012	 wd 0.0500	time 0.7864 (0.9034)	loss 0.7492 (1.0894)	grad_norm 2.2596 (3.0617)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:59:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.8347 (0.9032)	loss 1.1396 (1.0893)	grad_norm 2.2229 (3.0425)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 14:59:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 2 training takes 0:37:42
[2024-08-06 14:59:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.494 (11.494)	Loss 0.4521 (0.4521)	Acc@1 93.555 (93.555)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-06 15:00:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.818 Acc@5 98.050
[2024-08-06 15:00:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 15:00:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 15:00:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:33:24 lr 0.000012	 wd 0.0500	time 12.3118 (12.3118)	loss 0.6528 (0.6528)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:01:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:40:29 lr 0.000012	 wd 0.0500	time 0.8501 (1.0116)	loss 1.2006 (1.1253)	grad_norm 4.2029 (2.9915)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:03:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:36:42 lr 0.000012	 wd 0.0500	time 0.8954 (0.9568)	loss 1.4011 (1.0997)	grad_norm 2.5361 (2.9496)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:04:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:34:24 lr 0.000012	 wd 0.0500	time 0.8513 (0.9374)	loss 1.3704 (1.1009)	grad_norm 2.3327 (2.9639)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:06:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:32:29 lr 0.000013	 wd 0.0500	time 0.8780 (0.9275)	loss 1.3145 (1.0919)	grad_norm 3.4542 (3.0138)	loss_scale 2048.0000 (1172.1097)	mem 23350MB
[2024-08-06 15:07:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:30:44 lr 0.000013	 wd 0.0500	time 0.8441 (0.9214)	loss 0.7630 (1.0921)	grad_norm 2.4786 (3.0071)	loss_scale 2048.0000 (1346.9381)	mem 23350MB
[2024-08-06 15:09:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:29:06 lr 0.000013	 wd 0.0500	time 0.8741 (0.9181)	loss 1.0043 (1.0841)	grad_norm 2.3373 (2.9743)	loss_scale 2048.0000 (1463.5874)	mem 23350MB
[2024-08-06 15:10:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:27:28 lr 0.000013	 wd 0.0500	time 0.8421 (0.9149)	loss 1.2625 (1.0826)	grad_norm 3.5745 (2.9493)	loss_scale 2048.0000 (1546.9558)	mem 23350MB
[2024-08-06 15:12:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:25:53 lr 0.000013	 wd 0.0500	time 0.8433 (0.9128)	loss 0.8791 (1.0774)	grad_norm 2.5828 (2.9620)	loss_scale 2048.0000 (1609.5081)	mem 23350MB
[2024-08-06 15:13:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:24:19 lr 0.000013	 wd 0.0500	time 0.8485 (0.9112)	loss 1.5066 (1.0798)	grad_norm 3.3472 (2.9579)	loss_scale 2048.0000 (1658.1754)	mem 23350MB
[2024-08-06 15:15:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:22:46 lr 0.000014	 wd 0.0500	time 0.8133 (0.9100)	loss 1.2102 (1.0792)	grad_norm 2.7203 (2.9409)	loss_scale 2048.0000 (1697.1189)	mem 23350MB
[2024-08-06 15:16:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:21:14 lr 0.000014	 wd 0.0500	time 0.8484 (0.9091)	loss 0.7352 (1.0802)	grad_norm 2.6913 (2.9542)	loss_scale 2048.0000 (1728.9882)	mem 23350MB
[2024-08-06 15:18:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:19:42 lr 0.000014	 wd 0.0500	time 0.8867 (0.9086)	loss 1.1004 (1.0778)	grad_norm 2.3948 (2.9569)	loss_scale 2048.0000 (1755.5504)	mem 23350MB
[2024-08-06 15:19:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:18:11 lr 0.000014	 wd 0.0500	time 0.8421 (0.9078)	loss 1.1935 (1.0774)	grad_norm 2.1246 (2.9429)	loss_scale 2048.0000 (1778.0292)	mem 23350MB
[2024-08-06 15:21:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:16:39 lr 0.000014	 wd 0.0500	time 0.7850 (0.9070)	loss 1.1332 (1.0779)	grad_norm 1.9480 (2.9451)	loss_scale 2048.0000 (1797.2991)	mem 23350MB
[2024-08-06 15:22:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:15:08 lr 0.000014	 wd 0.0500	time 0.8404 (0.9065)	loss 1.3355 (1.0765)	grad_norm 2.2076 (2.9421)	loss_scale 2048.0000 (1814.0013)	mem 23350MB
[2024-08-06 15:24:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:13:37 lr 0.000015	 wd 0.0500	time 0.8485 (0.9058)	loss 0.6675 (1.0739)	grad_norm 3.1411 (2.9521)	loss_scale 2048.0000 (1828.6171)	mem 23350MB
[2024-08-06 15:25:44 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:12:06 lr 0.000015	 wd 0.0500	time 0.8372 (0.9056)	loss 0.9368 (1.0765)	grad_norm 2.1327 (2.9973)	loss_scale 2048.0000 (1841.5144)	mem 23350MB
[2024-08-06 15:27:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:10:35 lr 0.000015	 wd 0.0500	time 0.8492 (0.9051)	loss 1.1934 (1.0779)	grad_norm 2.2114 (nan)	loss_scale 1024.0000 (1848.4309)	mem 23350MB
[2024-08-06 15:28:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:09:04 lr 0.000015	 wd 0.0500	time 0.8729 (0.9047)	loss 1.2918 (1.0765)	grad_norm 2.2534 (nan)	loss_scale 1024.0000 (1805.0626)	mem 23350MB
[2024-08-06 15:30:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:07:34 lr 0.000015	 wd 0.0500	time 0.8508 (0.9044)	loss 1.1696 (1.0762)	grad_norm 2.4675 (nan)	loss_scale 1024.0000 (1766.0290)	mem 23350MB
[2024-08-06 15:31:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:06:03 lr 0.000015	 wd 0.0500	time 0.8339 (0.9040)	loss 0.7601 (1.0763)	grad_norm 2.4650 (nan)	loss_scale 1024.0000 (1730.7111)	mem 23350MB
[2024-08-06 15:33:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:04:32 lr 0.000016	 wd 0.0500	time 0.9027 (0.9038)	loss 0.7536 (1.0754)	grad_norm 2.4501 (nan)	loss_scale 1024.0000 (1698.6025)	mem 23350MB
[2024-08-06 15:34:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:03:02 lr 0.000016	 wd 0.0500	time 0.7895 (0.9035)	loss 0.8139 (1.0757)	grad_norm 2.0624 (nan)	loss_scale 1024.0000 (1669.2847)	mem 23350MB
[2024-08-06 15:36:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:32 lr 0.000016	 wd 0.0500	time 0.8499 (0.9032)	loss 1.1555 (1.0769)	grad_norm 2.8793 (nan)	loss_scale 1024.0000 (1642.4090)	mem 23350MB
[2024-08-06 15:37:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8423 (0.9029)	loss 1.1344 (1.0769)	grad_norm 2.5990 (nan)	loss_scale 1024.0000 (1617.6825)	mem 23350MB
[2024-08-06 15:37:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 3 training takes 0:37:41
[2024-08-06 15:37:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.782 (12.782)	Loss 0.4619 (0.4619)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-06 15:38:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.816 Acc@5 98.016
[2024-08-06 15:38:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 15:38:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 15:38:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][0/2502]	eta 8:08:40 lr 0.000016	 wd 0.0500	time 11.7189 (11.7189)	loss 1.1856 (1.1856)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:40:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:40:18 lr 0.000016	 wd 0.0500	time 0.8452 (1.0069)	loss 0.8430 (1.0921)	grad_norm 2.9433 (3.9800)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:41:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:36:32 lr 0.000016	 wd 0.0500	time 0.8953 (0.9525)	loss 1.0311 (1.0793)	grad_norm 2.5693 (3.4914)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:43:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:34:15 lr 0.000016	 wd 0.0500	time 0.8467 (0.9337)	loss 0.7310 (1.0756)	grad_norm 2.6295 (3.6634)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:44:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:32:23 lr 0.000017	 wd 0.0500	time 0.8296 (0.9246)	loss 1.2883 (1.0730)	grad_norm 3.2706 (3.4852)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:46:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:30:41 lr 0.000017	 wd 0.0500	time 0.8356 (0.9199)	loss 1.1173 (1.0734)	grad_norm 2.5094 (3.4365)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:47:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:29:03 lr 0.000017	 wd 0.0500	time 0.7820 (0.9167)	loss 0.9381 (1.0744)	grad_norm 2.7080 (3.3007)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:49:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:27:27 lr 0.000017	 wd 0.0500	time 0.8443 (0.9141)	loss 0.9147 (1.0747)	grad_norm 2.6781 (3.2502)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:50:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:25:52 lr 0.000017	 wd 0.0500	time 0.8458 (0.9122)	loss 0.6932 (1.0767)	grad_norm 10.5275 (3.2128)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:51:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:24:18 lr 0.000017	 wd 0.0500	time 0.8499 (0.9104)	loss 0.7624 (1.0770)	grad_norm 2.4344 (3.2844)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:53:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:22:45 lr 0.000018	 wd 0.0500	time 0.9057 (0.9092)	loss 1.3606 (1.0781)	grad_norm 2.1026 (3.2543)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:54:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:21:13 lr 0.000018	 wd 0.0500	time 0.7894 (0.9084)	loss 1.3668 (1.0794)	grad_norm 2.1444 (3.2571)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:56:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:19:41 lr 0.000018	 wd 0.0500	time 0.8500 (0.9078)	loss 1.2175 (1.0775)	grad_norm 4.9606 (3.2318)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:57:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:18:10 lr 0.000018	 wd 0.0500	time 0.8463 (0.9070)	loss 0.8848 (1.0771)	grad_norm 2.1516 (3.2221)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 15:59:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:16:38 lr 0.000018	 wd 0.0500	time 0.8497 (0.9065)	loss 1.3237 (1.0773)	grad_norm 2.4009 (3.2306)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:00:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:15:07 lr 0.000018	 wd 0.0500	time 0.8340 (0.9058)	loss 1.0804 (1.0773)	grad_norm 5.2928 (3.2228)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:02:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:13:36 lr 0.000019	 wd 0.0500	time 0.8397 (0.9056)	loss 1.2363 (1.0758)	grad_norm 2.3178 (3.2382)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:03:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:12:06 lr 0.000019	 wd 0.0500	time 0.8494 (0.9054)	loss 1.0611 (1.0757)	grad_norm 2.1110 (3.2414)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:05:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:10:35 lr 0.000019	 wd 0.0500	time 0.8583 (0.9050)	loss 1.4209 (1.0768)	grad_norm 2.3266 (3.2750)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:06:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:09:04 lr 0.000019	 wd 0.0500	time 0.8413 (0.9047)	loss 1.2339 (1.0769)	grad_norm 2.2597 (3.2740)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:08:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:07:33 lr 0.000019	 wd 0.0500	time 0.8559 (0.9044)	loss 0.7523 (1.0759)	grad_norm 2.4935 (3.2478)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:09:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:06:03 lr 0.000019	 wd 0.0500	time 0.8449 (0.9041)	loss 0.8701 (1.0746)	grad_norm 2.3790 (3.2413)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:11:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:04:32 lr 0.000020	 wd 0.0500	time 0.8499 (0.9039)	loss 0.8753 (1.0748)	grad_norm 2.5666 (3.2204)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:12:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:03:02 lr 0.000020	 wd 0.0500	time 0.8431 (0.9037)	loss 0.7117 (1.0749)	grad_norm 2.7947 (3.2083)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:14:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.9022 (0.9035)	loss 0.6349 (1.0742)	grad_norm 3.0448 (3.1965)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:15:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8360 (0.9033)	loss 1.0130 (1.0747)	grad_norm 3.2467 (3.1835)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:16:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 4 training takes 0:37:42
[2024-08-06 16:16:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.747 (12.747)	Loss 0.4531 (0.4531)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-06 16:16:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.808 Acc@5 98.024
[2024-08-06 16:16:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 16:16:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 16:16:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][0/2502]	eta 9:03:11 lr 0.000020	 wd 0.0500	time 13.0263 (13.0263)	loss 1.2659 (1.2659)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:18:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:40:56 lr 0.000020	 wd 0.0500	time 0.8252 (1.0228)	loss 0.9327 (1.0974)	grad_norm 2.3576 (3.6503)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:19:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:36:52 lr 0.000020	 wd 0.0500	time 0.8429 (0.9610)	loss 1.1142 (1.0672)	grad_norm 2.2581 (3.5344)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:21:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:34:31 lr 0.000020	 wd 0.0500	time 0.8507 (0.9405)	loss 0.7440 (1.0620)	grad_norm 2.9043 (3.4296)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:22:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:32:35 lr 0.000020	 wd 0.0500	time 0.8396 (0.9302)	loss 1.3664 (1.0693)	grad_norm 2.9833 (3.3308)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:24:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:30:50 lr 0.000020	 wd 0.0500	time 0.8416 (0.9243)	loss 0.8852 (1.0677)	grad_norm 5.0792 (3.3205)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:25:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:29:09 lr 0.000020	 wd 0.0500	time 0.8493 (0.9198)	loss 0.8330 (1.0679)	grad_norm 2.5331 (3.3874)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:27:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:27:32 lr 0.000020	 wd 0.0500	time 0.8337 (0.9170)	loss 0.8581 (1.0678)	grad_norm 2.3763 (3.4078)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 16:28:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:25:56 lr 0.000020	 wd 0.0500	time 0.8539 (0.9146)	loss 1.3613 (1.0659)	grad_norm 2.4971 (3.3514)	loss_scale 2048.0000 (1039.3408)	mem 23350MB
[2024-08-06 16:30:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:24:22 lr 0.000020	 wd 0.0500	time 0.8488 (0.9129)	loss 0.9421 (1.0653)	grad_norm 2.5738 (3.3105)	loss_scale 2048.0000 (1151.2897)	mem 23350MB
[2024-08-06 16:31:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:22:49 lr 0.000020	 wd 0.0500	time 0.8853 (0.9117)	loss 1.0374 (1.0642)	grad_norm 2.1672 (3.3399)	loss_scale 2048.0000 (1240.8711)	mem 23350MB
[2024-08-06 16:33:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:21:16 lr 0.000020	 wd 0.0500	time 0.8388 (0.9106)	loss 1.1035 (1.0654)	grad_norm 2.7072 (3.3564)	loss_scale 2048.0000 (1314.1798)	mem 23350MB
[2024-08-06 16:34:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:19:44 lr 0.000020	 wd 0.0500	time 0.8321 (0.9097)	loss 1.3362 (1.0647)	grad_norm 3.9358 (3.3320)	loss_scale 2048.0000 (1375.2806)	mem 23350MB
[2024-08-06 16:36:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:18:12 lr 0.000020	 wd 0.0500	time 0.8526 (0.9090)	loss 0.7425 (1.0659)	grad_norm 2.8067 (3.3606)	loss_scale 2048.0000 (1426.9885)	mem 23350MB
[2024-08-06 16:37:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:16:40 lr 0.000020	 wd 0.0500	time 0.8464 (0.9083)	loss 1.1814 (1.0694)	grad_norm 2.7159 (3.3335)	loss_scale 2048.0000 (1471.3148)	mem 23350MB
[2024-08-06 16:39:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:15:09 lr 0.000020	 wd 0.0500	time 0.8387 (0.9077)	loss 0.6981 (1.0712)	grad_norm 4.1731 (3.3736)	loss_scale 2048.0000 (1509.7348)	mem 23350MB
[2024-08-06 16:40:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:13:38 lr 0.000020	 wd 0.0500	time 0.8416 (0.9072)	loss 1.2493 (1.0744)	grad_norm 2.4289 (3.3472)	loss_scale 2048.0000 (1543.3554)	mem 23350MB
[2024-08-06 16:42:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:12:07 lr 0.000020	 wd 0.0500	time 0.8557 (0.9068)	loss 0.7375 (1.0748)	grad_norm 2.9216 (3.3641)	loss_scale 2048.0000 (1573.0229)	mem 23350MB
[2024-08-06 16:43:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:10:36 lr 0.000020	 wd 0.0500	time 0.8498 (0.9064)	loss 0.7594 (1.0725)	grad_norm 3.8729 (3.3425)	loss_scale 2048.0000 (1599.3959)	mem 23350MB
[2024-08-06 16:45:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:09:05 lr 0.000020	 wd 0.0500	time 0.8536 (0.9060)	loss 1.0775 (1.0721)	grad_norm 2.5256 (3.3221)	loss_scale 2048.0000 (1622.9942)	mem 23350MB
[2024-08-06 16:46:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:07:34 lr 0.000020	 wd 0.0500	time 0.8920 (0.9058)	loss 1.4548 (1.0731)	grad_norm 2.6036 (3.3296)	loss_scale 2048.0000 (1644.2339)	mem 23350MB
[2024-08-06 16:48:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:06:03 lr 0.000020	 wd 0.0500	time 0.8354 (0.9054)	loss 1.4254 (1.0731)	grad_norm 2.7902 (3.3482)	loss_scale 2048.0000 (1663.4517)	mem 23350MB
[2024-08-06 16:49:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:04:33 lr 0.000020	 wd 0.0500	time 0.8726 (0.9051)	loss 1.1602 (1.0735)	grad_norm 3.1413 (3.3525)	loss_scale 2048.0000 (1680.9232)	mem 23350MB
[2024-08-06 16:51:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:03:02 lr 0.000020	 wd 0.0500	time 0.8369 (0.9049)	loss 1.0805 (1.0725)	grad_norm 2.2509 (3.3566)	loss_scale 2048.0000 (1696.8761)	mem 23350MB
[2024-08-06 16:52:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8491 (0.9046)	loss 0.8502 (1.0720)	grad_norm 2.5767 (3.3765)	loss_scale 2048.0000 (1711.5002)	mem 23350MB
[2024-08-06 16:54:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8529 (0.9044)	loss 1.2867 (1.0730)	grad_norm 2.8944 (3.3713)	loss_scale 2048.0000 (1724.9548)	mem 23350MB
[2024-08-06 16:54:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 5 training takes 0:37:45
[2024-08-06 16:54:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.927 (12.927)	Loss 0.4756 (0.4756)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-06 16:54:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.702 Acc@5 97.998
[2024-08-06 16:54:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 16:54:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 16:55:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:50:33 lr 0.000020	 wd 0.0500	time 12.7234 (12.7234)	loss 1.1520 (1.1520)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 16:56:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:40:30 lr 0.000020	 wd 0.0500	time 0.8540 (1.0117)	loss 0.8869 (1.0995)	grad_norm 2.3972 (3.6715)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 16:58:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:36:39 lr 0.000020	 wd 0.0500	time 0.8837 (0.9554)	loss 0.8334 (1.0831)	grad_norm 4.1801 (3.5027)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 16:59:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:34:22 lr 0.000020	 wd 0.0500	time 0.8499 (0.9369)	loss 0.8526 (1.0782)	grad_norm 2.2805 (3.4392)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:01:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:32:32 lr 0.000020	 wd 0.0500	time 0.8515 (0.9288)	loss 0.6294 (1.0769)	grad_norm 2.9609 (3.3724)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:02:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:30:47 lr 0.000020	 wd 0.0500	time 0.8604 (0.9228)	loss 1.1863 (1.0779)	grad_norm 9.8686 (3.3199)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:04:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:29:06 lr 0.000020	 wd 0.0500	time 0.8391 (0.9184)	loss 0.7624 (1.0705)	grad_norm 2.4378 (3.3415)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:05:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:27:29 lr 0.000020	 wd 0.0500	time 0.8420 (0.9154)	loss 1.2987 (1.0726)	grad_norm 2.7428 (3.2979)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:07:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:25:54 lr 0.000020	 wd 0.0500	time 0.8401 (0.9133)	loss 0.7015 (1.0775)	grad_norm 2.7869 (3.4073)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:08:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:24:20 lr 0.000020	 wd 0.0500	time 0.8434 (0.9118)	loss 1.5243 (1.0727)	grad_norm 2.8648 (3.4292)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:10:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:22:47 lr 0.000020	 wd 0.0500	time 0.8500 (0.9105)	loss 0.9521 (1.0727)	grad_norm 3.7310 (3.4199)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:11:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:21:14 lr 0.000020	 wd 0.0500	time 0.8466 (0.9094)	loss 0.7606 (1.0685)	grad_norm 3.2925 (3.4155)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:13:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:19:42 lr 0.000020	 wd 0.0500	time 0.8548 (0.9084)	loss 1.2176 (1.0656)	grad_norm 2.6676 (3.3865)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:14:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:18:11 lr 0.000020	 wd 0.0500	time 0.8471 (0.9077)	loss 1.0418 (1.0646)	grad_norm 3.0527 (3.3575)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:16:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:16:39 lr 0.000020	 wd 0.0500	time 0.8923 (0.9073)	loss 0.9135 (1.0640)	grad_norm 3.2229 (3.3396)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:17:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:15:08 lr 0.000020	 wd 0.0500	time 0.8947 (0.9067)	loss 0.6493 (1.0641)	grad_norm 3.5573 (3.4265)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:19:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:13:37 lr 0.000020	 wd 0.0500	time 0.8370 (0.9062)	loss 1.2430 (1.0621)	grad_norm 1.8349 (3.3939)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:20:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:12:06 lr 0.000020	 wd 0.0500	time 0.8589 (0.9057)	loss 1.2885 (1.0605)	grad_norm 2.5687 (3.3817)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:22:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:10:35 lr 0.000020	 wd 0.0500	time 0.8479 (0.9053)	loss 1.1000 (1.0607)	grad_norm 2.3365 (3.3619)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:23:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:09:04 lr 0.000020	 wd 0.0500	time 0.8487 (0.9049)	loss 1.3202 (1.0630)	grad_norm 3.0221 (3.3393)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:25:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:07:34 lr 0.000020	 wd 0.0500	time 0.8571 (0.9045)	loss 0.8692 (1.0636)	grad_norm 2.3634 (3.3498)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:26:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:06:03 lr 0.000020	 wd 0.0500	time 0.8454 (0.9041)	loss 1.0381 (1.0640)	grad_norm 2.6909 (3.3215)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:28:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:04:32 lr 0.000020	 wd 0.0500	time 0.8424 (0.9039)	loss 0.7337 (1.0634)	grad_norm 4.5557 (3.3169)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 17:29:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:03:02 lr 0.000020	 wd 0.0500	time 0.8539 (0.9036)	loss 0.8816 (1.0655)	grad_norm 3.4487 (3.3057)	loss_scale 4096.0000 (2060.4607)	mem 23350MB
[2024-08-06 17:31:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:32 lr 0.000020	 wd 0.0500	time 0.8558 (0.9036)	loss 1.3280 (1.0647)	grad_norm 2.6000 (3.3178)	loss_scale 4096.0000 (2145.2395)	mem 23350MB
[2024-08-06 17:32:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.8319 (0.9033)	loss 1.3292 (1.0641)	grad_norm 2.7237 (3.3233)	loss_scale 4096.0000 (2223.2387)	mem 23350MB
[2024-08-06 17:32:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 6 training takes 0:37:42
[2024-08-06 17:32:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.169 (12.169)	Loss 0.4788 (0.4788)	Acc@1 92.578 (92.578)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-06 17:33:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.750 Acc@5 97.938
[2024-08-06 17:33:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 17:33:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 17:33:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:49:13 lr 0.000020	 wd 0.0500	time 12.6914 (12.6914)	loss 0.7877 (0.7877)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:34:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:40:45 lr 0.000020	 wd 0.0500	time 0.7835 (1.0180)	loss 1.1117 (1.0465)	grad_norm 2.4299 (3.2681)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:36:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:36:47 lr 0.000020	 wd 0.0500	time 0.8445 (0.9588)	loss 0.9916 (1.0678)	grad_norm 2.7293 (3.4646)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:37:52 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:34:27 lr 0.000020	 wd 0.0500	time 0.7882 (0.9390)	loss 1.3922 (1.0662)	grad_norm 2.7574 (3.4967)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:39:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:32:31 lr 0.000020	 wd 0.0500	time 0.8580 (0.9284)	loss 0.7597 (1.0692)	grad_norm 2.3003 (3.3685)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:40:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:30:45 lr 0.000020	 wd 0.0500	time 0.8396 (0.9219)	loss 1.2889 (1.0673)	grad_norm 2.0684 (3.3061)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:42:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:29:05 lr 0.000020	 wd 0.0500	time 0.8339 (0.9178)	loss 1.1871 (1.0724)	grad_norm 3.8985 (3.2165)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:43:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:27:28 lr 0.000020	 wd 0.0500	time 0.8516 (0.9149)	loss 0.9893 (1.0672)	grad_norm 2.2981 (3.2005)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:45:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:25:54 lr 0.000020	 wd 0.0500	time 0.8545 (0.9134)	loss 0.8516 (1.0649)	grad_norm 2.4407 (3.1495)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:46:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:24:20 lr 0.000020	 wd 0.0500	time 0.8341 (0.9118)	loss 1.4202 (1.0683)	grad_norm 2.1099 (3.1094)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:48:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:22:47 lr 0.000020	 wd 0.0500	time 0.8338 (0.9103)	loss 0.7676 (1.0681)	grad_norm 2.2015 (3.0714)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:49:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:21:14 lr 0.000020	 wd 0.0500	time 0.8475 (0.9090)	loss 1.1413 (1.0662)	grad_norm 2.7228 (3.0756)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:51:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:19:42 lr 0.000020	 wd 0.0500	time 0.8336 (0.9080)	loss 1.1843 (1.0630)	grad_norm 2.4859 (3.0598)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:52:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:18:10 lr 0.000020	 wd 0.0500	time 0.8529 (0.9073)	loss 1.2147 (1.0630)	grad_norm 2.6833 (3.0642)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:54:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:16:39 lr 0.000019	 wd 0.0500	time 0.8578 (0.9066)	loss 1.1282 (1.0642)	grad_norm 2.2878 (3.0381)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:55:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:15:07 lr 0.000019	 wd 0.0500	time 0.8337 (0.9060)	loss 0.7154 (1.0652)	grad_norm 2.4356 (3.0679)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:57:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:13:36 lr 0.000019	 wd 0.0500	time 0.9029 (0.9056)	loss 1.1372 (1.0660)	grad_norm 2.7807 (3.0859)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-06 17:58:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:12:05 lr 0.000019	 wd 0.0500	time 0.8500 (0.9051)	loss 1.1569 (1.0658)	grad_norm 3.6696 (inf)	loss_scale 2048.0000 (4011.7202)	mem 23350MB
[2024-08-06 18:00:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:10:35 lr 0.000019	 wd 0.0500	time 0.8587 (0.9048)	loss 0.6940 (1.0679)	grad_norm 2.0626 (inf)	loss_scale 2048.0000 (3902.6852)	mem 23350MB
[2024-08-06 18:01:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:09:04 lr 0.000019	 wd 0.0500	time 0.8528 (0.9045)	loss 1.0944 (1.0684)	grad_norm 2.3516 (inf)	loss_scale 2048.0000 (3805.1215)	mem 23350MB
[2024-08-06 18:03:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:07:33 lr 0.000019	 wd 0.0500	time 0.8366 (0.9042)	loss 1.1425 (1.0691)	grad_norm 2.4292 (inf)	loss_scale 2048.0000 (3717.3093)	mem 23350MB
[2024-08-06 18:04:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:06:03 lr 0.000019	 wd 0.0500	time 0.8220 (0.9040)	loss 1.3917 (1.0716)	grad_norm 3.4065 (inf)	loss_scale 2048.0000 (3637.8563)	mem 23350MB
[2024-08-06 18:06:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:04:32 lr 0.000019	 wd 0.0500	time 0.8519 (0.9038)	loss 1.1771 (1.0698)	grad_norm 4.0432 (inf)	loss_scale 2048.0000 (3565.6229)	mem 23350MB
[2024-08-06 18:07:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:03:02 lr 0.000019	 wd 0.0500	time 0.8374 (0.9035)	loss 1.3246 (1.0693)	grad_norm 2.5875 (inf)	loss_scale 2048.0000 (3499.6680)	mem 23350MB
[2024-08-06 18:09:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:32 lr 0.000019	 wd 0.0500	time 0.8450 (0.9032)	loss 1.2684 (1.0694)	grad_norm 2.2035 (inf)	loss_scale 2048.0000 (3439.2070)	mem 23350MB
[2024-08-06 18:10:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8927 (0.9028)	loss 1.3796 (1.0688)	grad_norm 2.5634 (inf)	loss_scale 2048.0000 (3383.5810)	mem 23350MB
[2024-08-06 18:10:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 7 training takes 0:37:41
[2024-08-06 18:11:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.527 (12.527)	Loss 0.4424 (0.4424)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-06 18:11:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.642 Acc@5 97.948
[2024-08-06 18:11:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-06 18:11:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 18:11:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:24:50 lr 0.000019	 wd 0.0500	time 12.1065 (12.1065)	loss 1.1498 (1.1498)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:13:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:40:29 lr 0.000019	 wd 0.0500	time 0.8455 (1.0114)	loss 0.8207 (1.1198)	grad_norm 2.7398 (2.8796)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:14:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:36:43 lr 0.000019	 wd 0.0500	time 0.8455 (0.9574)	loss 0.7357 (1.0973)	grad_norm 3.5859 (2.9237)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:16:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:34:25 lr 0.000019	 wd 0.0500	time 0.8499 (0.9380)	loss 1.3803 (1.0837)	grad_norm 2.3538 (2.8550)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:17:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:32:30 lr 0.000019	 wd 0.0500	time 0.9012 (0.9278)	loss 0.7800 (1.0816)	grad_norm 2.4930 (2.9842)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:19:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:30:45 lr 0.000019	 wd 0.0500	time 0.8402 (0.9219)	loss 0.7220 (1.0721)	grad_norm 2.3588 (2.9811)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:20:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:29:06 lr 0.000019	 wd 0.0500	time 0.8522 (0.9182)	loss 0.7635 (1.0718)	grad_norm 2.7725 (3.0170)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:22:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:27:29 lr 0.000019	 wd 0.0500	time 0.8458 (0.9152)	loss 1.2036 (1.0720)	grad_norm 7.9881 (3.0162)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:23:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:25:54 lr 0.000019	 wd 0.0500	time 0.7990 (0.9131)	loss 1.2700 (1.0741)	grad_norm 2.7303 (2.9965)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:25:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:24:20 lr 0.000019	 wd 0.0500	time 0.8458 (0.9114)	loss 0.8407 (1.0702)	grad_norm 2.4885 (3.0242)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 18:26:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:22:47 lr 0.000019	 wd 0.0500	time 0.8568 (0.9102)	loss 1.2592 (1.0721)	grad_norm 3.2995 (inf)	loss_scale 1024.0000 (1982.5295)	mem 23350MB
[2024-08-06 18:28:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:21:14 lr 0.000019	 wd 0.0500	time 0.8915 (0.9093)	loss 1.2138 (1.0693)	grad_norm 1.8898 (inf)	loss_scale 1024.0000 (1895.4696)	mem 23350MB
[2024-08-06 18:29:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:19:42 lr 0.000019	 wd 0.0500	time 0.8449 (0.9084)	loss 1.0020 (1.0699)	grad_norm 2.4930 (inf)	loss_scale 1024.0000 (1822.9076)	mem 23350MB
[2024-08-06 18:31:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:18:10 lr 0.000019	 wd 0.0500	time 0.8701 (0.9076)	loss 0.7728 (1.0697)	grad_norm 6.1243 (inf)	loss_scale 1024.0000 (1761.5004)	mem 23350MB
[2024-08-06 18:32:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:16:39 lr 0.000019	 wd 0.0500	time 0.8490 (0.9069)	loss 1.2354 (1.0711)	grad_norm 2.6924 (inf)	loss_scale 1024.0000 (1708.8594)	mem 23350MB
[2024-08-06 18:34:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:15:08 lr 0.000019	 wd 0.0500	time 0.8453 (0.9064)	loss 0.9826 (1.0720)	grad_norm 2.6286 (inf)	loss_scale 1024.0000 (1663.2325)	mem 23350MB
[2024-08-06 18:35:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:13:37 lr 0.000019	 wd 0.0500	time 0.8361 (0.9059)	loss 1.2870 (1.0739)	grad_norm 2.4741 (inf)	loss_scale 1024.0000 (1623.3054)	mem 23350MB
[2024-08-06 18:37:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:12:06 lr 0.000019	 wd 0.0500	time 0.8313 (0.9054)	loss 0.6948 (1.0760)	grad_norm 2.7475 (inf)	loss_scale 1024.0000 (1588.0729)	mem 23350MB
[2024-08-06 18:38:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:10:35 lr 0.000019	 wd 0.0500	time 0.8408 (0.9050)	loss 0.7110 (1.0744)	grad_norm 3.4963 (inf)	loss_scale 1024.0000 (1556.7529)	mem 23350MB
[2024-08-06 18:40:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:09:04 lr 0.000019	 wd 0.0500	time 0.8481 (0.9046)	loss 1.1775 (1.0761)	grad_norm 2.7065 (inf)	loss_scale 1024.0000 (1528.7280)	mem 23350MB
[2024-08-06 18:41:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:07:33 lr 0.000019	 wd 0.0500	time 0.8453 (0.9042)	loss 0.7143 (1.0750)	grad_norm 2.2237 (inf)	loss_scale 1024.0000 (1503.5042)	mem 23350MB
[2024-08-06 18:43:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:06:03 lr 0.000019	 wd 0.0500	time 0.8372 (0.9040)	loss 0.8527 (1.0757)	grad_norm 3.0180 (inf)	loss_scale 1024.0000 (1480.6816)	mem 23350MB
[2024-08-06 18:44:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:04:32 lr 0.000019	 wd 0.0500	time 0.8534 (0.9039)	loss 1.1151 (1.0746)	grad_norm 15.1791 (inf)	loss_scale 1024.0000 (1459.9328)	mem 23350MB
[2024-08-06 18:46:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:03:02 lr 0.000019	 wd 0.0500	time 0.8387 (0.9036)	loss 1.2067 (1.0733)	grad_norm 2.6273 (inf)	loss_scale 1024.0000 (1440.9874)	mem 23350MB
[2024-08-06 18:47:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:32 lr 0.000019	 wd 0.0500	time 0.8464 (0.9034)	loss 1.0038 (1.0734)	grad_norm 2.9462 (inf)	loss_scale 1024.0000 (1423.6202)	mem 23350MB
[2024-08-06 18:49:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.8386 (0.9031)	loss 0.7533 (1.0731)	grad_norm 3.0280 (inf)	loss_scale 1024.0000 (1407.6417)	mem 23350MB
[2024-08-06 18:49:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 8 training takes 0:37:42
[2024-08-06 18:49:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.331 (11.331)	Loss 0.4514 (0.4514)	Acc@1 93.164 (93.164)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-06 18:49:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.790 Acc@5 98.014
[2024-08-06 18:49:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 18:49:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 18:49:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][0/2502]	eta 8:22:08 lr 0.000019	 wd 0.0500	time 12.0416 (12.0416)	loss 1.2176 (1.2176)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 18:51:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:40:22 lr 0.000019	 wd 0.0500	time 0.8466 (1.0087)	loss 1.1901 (1.0553)	grad_norm 3.0416 (2.9727)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 18:52:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:36:35 lr 0.000019	 wd 0.0500	time 0.8489 (0.9537)	loss 0.8657 (1.0788)	grad_norm 2.2337 (3.0200)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 18:54:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:34:17 lr 0.000019	 wd 0.0500	time 0.8524 (0.9346)	loss 1.3427 (1.0850)	grad_norm 5.1765 (2.9888)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 18:55:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:32:26 lr 0.000019	 wd 0.0500	time 0.8423 (0.9260)	loss 0.8430 (1.0674)	grad_norm 2.3827 (2.9753)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 18:57:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:30:43 lr 0.000019	 wd 0.0500	time 0.9355 (0.9210)	loss 0.6231 (1.0626)	grad_norm 2.4848 (2.9474)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 18:58:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:29:04 lr 0.000019	 wd 0.0500	time 0.8429 (0.9173)	loss 0.7264 (1.0656)	grad_norm 5.0340 (2.9940)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:00:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:27:28 lr 0.000019	 wd 0.0500	time 0.8945 (0.9149)	loss 1.3338 (1.0684)	grad_norm 1.9520 (2.9448)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:01:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:25:53 lr 0.000019	 wd 0.0500	time 0.7953 (0.9126)	loss 1.3325 (1.0700)	grad_norm 2.8515 (2.9700)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:03:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:24:19 lr 0.000019	 wd 0.0500	time 0.8453 (0.9111)	loss 1.2537 (1.0705)	grad_norm 2.4703 (2.9630)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:04:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:22:47 lr 0.000019	 wd 0.0500	time 0.8511 (0.9102)	loss 1.2188 (1.0710)	grad_norm 2.4124 (2.9745)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:06:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:21:14 lr 0.000018	 wd 0.0500	time 0.8509 (0.9093)	loss 1.2915 (1.0672)	grad_norm 3.5389 (2.9960)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:07:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:19:42 lr 0.000018	 wd 0.0500	time 0.8457 (0.9084)	loss 0.9033 (1.0674)	grad_norm 2.6640 (3.0183)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:09:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:18:11 lr 0.000018	 wd 0.0500	time 0.8406 (0.9078)	loss 1.2697 (1.0685)	grad_norm 2.0966 (3.1181)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:10:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:16:39 lr 0.000018	 wd 0.0500	time 0.8314 (0.9072)	loss 0.8746 (1.0666)	grad_norm 2.4780 (3.1341)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:12:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:15:08 lr 0.000018	 wd 0.0500	time 0.8434 (0.9069)	loss 0.8041 (1.0692)	grad_norm 2.3159 (3.1180)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:13:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:13:37 lr 0.000018	 wd 0.0500	time 0.8215 (0.9065)	loss 0.8741 (1.0671)	grad_norm 2.9442 (3.1477)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:15:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:12:06 lr 0.000018	 wd 0.0500	time 0.8512 (0.9062)	loss 1.3107 (1.0674)	grad_norm 3.6585 (3.1391)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:16:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:10:35 lr 0.000018	 wd 0.0500	time 0.8506 (0.9058)	loss 1.1996 (1.0666)	grad_norm 2.8207 (3.1474)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:18:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:09:05 lr 0.000018	 wd 0.0500	time 0.8428 (0.9054)	loss 1.2762 (1.0656)	grad_norm 2.6260 (3.1473)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:19:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:07:34 lr 0.000018	 wd 0.0500	time 0.8510 (0.9052)	loss 1.0787 (1.0659)	grad_norm 2.4929 (3.1580)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:21:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:06:03 lr 0.000018	 wd 0.0500	time 0.8430 (0.9048)	loss 1.2448 (1.0671)	grad_norm 2.6513 (3.1406)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:22:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:04:33 lr 0.000018	 wd 0.0500	time 0.8495 (0.9045)	loss 1.1548 (1.0680)	grad_norm 2.4467 (3.1272)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:24:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:03:02 lr 0.000018	 wd 0.0500	time 0.8265 (0.9044)	loss 0.7499 (1.0680)	grad_norm 2.6765 (3.1272)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:25:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:32 lr 0.000018	 wd 0.0500	time 0.8467 (0.9042)	loss 1.0742 (1.0686)	grad_norm 2.8926 (3.1311)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 19:27:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.9081 (0.9040)	loss 1.2828 (1.0682)	grad_norm 2.9790 (3.1227)	loss_scale 2048.0000 (1051.0228)	mem 23350MB
[2024-08-06 19:27:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 9 training takes 0:37:44
[2024-08-06 19:27:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.017 (12.017)	Loss 0.4509 (0.4509)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-06 19:27:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.756 Acc@5 98.004
[2024-08-06 19:27:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 19:27:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 19:28:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][0/2502]	eta 9:00:13 lr 0.000018	 wd 0.0500	time 12.9549 (12.9549)	loss 1.3063 (1.3063)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:29:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:40:49 lr 0.000018	 wd 0.0500	time 0.8481 (1.0200)	loss 0.7879 (1.0725)	grad_norm 11.6713 (3.8304)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:31:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:36:50 lr 0.000018	 wd 0.0500	time 0.8429 (0.9601)	loss 1.2744 (1.0618)	grad_norm 2.5960 (3.4914)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:32:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:34:28 lr 0.000018	 wd 0.0500	time 0.8438 (0.9395)	loss 1.2573 (1.0626)	grad_norm 3.4295 (3.6301)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:34:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:32:32 lr 0.000018	 wd 0.0500	time 0.8624 (0.9290)	loss 0.6637 (1.0698)	grad_norm 3.9714 (3.7033)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:35:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:30:49 lr 0.000018	 wd 0.0500	time 0.8492 (0.9236)	loss 0.6746 (1.0676)	grad_norm 2.5158 (3.7847)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:37:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:29:07 lr 0.000018	 wd 0.0500	time 0.8474 (0.9190)	loss 1.1331 (1.0743)	grad_norm 3.7979 (3.7145)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:38:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:27:30 lr 0.000018	 wd 0.0500	time 0.8310 (0.9159)	loss 1.2128 (1.0723)	grad_norm 2.0521 (3.5948)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:40:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:25:55 lr 0.000018	 wd 0.0500	time 0.8474 (0.9140)	loss 1.1546 (1.0748)	grad_norm 2.5947 (3.6632)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:41:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:24:22 lr 0.000018	 wd 0.0500	time 0.8368 (0.9128)	loss 1.2274 (1.0736)	grad_norm 2.4713 (3.6500)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:43:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:22:49 lr 0.000018	 wd 0.0500	time 0.9069 (0.9115)	loss 1.2087 (1.0750)	grad_norm 2.2417 (3.5864)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:44:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:21:16 lr 0.000018	 wd 0.0500	time 0.8788 (0.9103)	loss 0.9226 (1.0816)	grad_norm 11.9786 (3.5425)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:46:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:19:44 lr 0.000018	 wd 0.0500	time 0.8428 (0.9094)	loss 1.3575 (1.0786)	grad_norm 2.7198 (3.5578)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:47:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:18:12 lr 0.000018	 wd 0.0500	time 0.8228 (0.9087)	loss 1.1176 (1.0769)	grad_norm 2.9389 (3.5738)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:49:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:16:40 lr 0.000018	 wd 0.0500	time 0.8505 (0.9079)	loss 1.3491 (1.0756)	grad_norm 2.5434 (3.5182)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:50:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:15:09 lr 0.000018	 wd 0.0500	time 0.8398 (0.9073)	loss 1.0573 (1.0739)	grad_norm 3.0026 (3.5004)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:52:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:13:37 lr 0.000018	 wd 0.0500	time 0.8399 (0.9068)	loss 1.0679 (1.0723)	grad_norm 2.9934 (3.4598)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:53:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:12:06 lr 0.000018	 wd 0.0500	time 0.8510 (0.9063)	loss 1.2019 (1.0741)	grad_norm 2.7132 (3.4269)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:55:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:10:36 lr 0.000018	 wd 0.0500	time 0.8499 (0.9060)	loss 0.8855 (1.0726)	grad_norm 2.3578 (3.4213)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:56:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:09:05 lr 0.000018	 wd 0.0500	time 0.8868 (0.9058)	loss 0.9651 (1.0714)	grad_norm 2.5681 (3.4489)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:58:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:07:34 lr 0.000017	 wd 0.0500	time 0.8521 (0.9054)	loss 1.1947 (1.0745)	grad_norm 2.4390 (3.4184)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 19:59:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:06:03 lr 0.000017	 wd 0.0500	time 0.8357 (0.9050)	loss 0.9677 (1.0761)	grad_norm 2.8853 (3.3900)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:01:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:04:33 lr 0.000017	 wd 0.0500	time 0.8431 (0.9047)	loss 1.1840 (1.0771)	grad_norm 2.4346 (3.3696)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:02:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:03:02 lr 0.000017	 wd 0.0500	time 0.8306 (0.9044)	loss 1.2015 (1.0751)	grad_norm 2.8511 (3.3511)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:04:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:32 lr 0.000017	 wd 0.0500	time 0.8483 (0.9042)	loss 1.3315 (1.0740)	grad_norm 3.0483 (3.3563)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:05:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.8465 (0.9038)	loss 0.7284 (1.0731)	grad_norm 4.4342 (3.3450)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:05:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 10 training takes 0:37:43
[2024-08-06 20:05:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.043 (12.043)	Loss 0.4668 (0.4668)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-06 20:06:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.738 Acc@5 97.950
[2024-08-06 20:06:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 20:06:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 20:06:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:07:04 lr 0.000017	 wd 0.0500	time 11.6806 (11.6806)	loss 0.8874 (0.8874)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:07:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:40:15 lr 0.000017	 wd 0.0500	time 0.9026 (1.0057)	loss 1.2479 (1.0617)	grad_norm 2.1429 (3.0285)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:09:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:36:32 lr 0.000017	 wd 0.0500	time 0.8492 (0.9526)	loss 1.1598 (1.0704)	grad_norm 11.3335 (3.1860)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:10:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:34:21 lr 0.000017	 wd 0.0500	time 0.8495 (0.9361)	loss 0.8090 (1.0646)	grad_norm 2.1510 (3.3707)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:12:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:32:29 lr 0.000017	 wd 0.0500	time 0.8541 (0.9273)	loss 1.2763 (1.0668)	grad_norm 3.2058 (3.3686)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:13:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:30:45 lr 0.000017	 wd 0.0500	time 0.8403 (0.9219)	loss 0.7045 (1.0659)	grad_norm 2.3978 (3.2947)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:15:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:29:06 lr 0.000017	 wd 0.0500	time 0.8441 (0.9183)	loss 1.4056 (1.0683)	grad_norm 3.4946 (3.2777)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:16:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:27:29 lr 0.000017	 wd 0.0500	time 0.8526 (0.9154)	loss 0.8401 (1.0654)	grad_norm 15.3787 (3.2633)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:18:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:25:54 lr 0.000017	 wd 0.0500	time 0.8478 (0.9133)	loss 1.3462 (1.0640)	grad_norm 2.3406 (3.2819)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:19:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:24:20 lr 0.000017	 wd 0.0500	time 0.8398 (0.9117)	loss 0.8080 (1.0655)	grad_norm 2.5942 (3.2540)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:21:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:22:47 lr 0.000017	 wd 0.0500	time 0.8388 (0.9102)	loss 1.1520 (1.0662)	grad_norm 2.8115 (3.2227)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:22:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:21:14 lr 0.000017	 wd 0.0500	time 0.8459 (0.9090)	loss 0.8727 (1.0645)	grad_norm 4.4830 (3.1615)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:24:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:19:42 lr 0.000017	 wd 0.0500	time 0.8375 (0.9084)	loss 0.8970 (1.0610)	grad_norm 3.3054 (3.1785)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:25:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:18:11 lr 0.000017	 wd 0.0500	time 0.8549 (0.9078)	loss 1.1891 (1.0616)	grad_norm 2.7744 (3.1630)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:27:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:16:39 lr 0.000017	 wd 0.0500	time 0.8540 (0.9071)	loss 0.7668 (1.0626)	grad_norm 2.7926 (3.1386)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:28:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:15:08 lr 0.000017	 wd 0.0500	time 0.8512 (0.9066)	loss 1.3402 (1.0627)	grad_norm 7.1003 (3.1494)	loss_scale 4096.0000 (2143.5097)	mem 23350MB
[2024-08-06 20:30:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:13:37 lr 0.000017	 wd 0.0500	time 0.8572 (0.9061)	loss 1.1943 (1.0640)	grad_norm 2.2554 (3.1505)	loss_scale 4096.0000 (2265.4641)	mem 23350MB
[2024-08-06 20:31:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:12:06 lr 0.000017	 wd 0.0500	time 0.8492 (0.9058)	loss 0.9490 (1.0647)	grad_norm 2.2576 (3.1274)	loss_scale 4096.0000 (2373.0794)	mem 23350MB
[2024-08-06 20:33:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:10:35 lr 0.000017	 wd 0.0500	time 0.8501 (0.9053)	loss 0.7540 (1.0636)	grad_norm 2.3913 (3.1541)	loss_scale 4096.0000 (2468.7440)	mem 23350MB
[2024-08-06 20:34:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:09:04 lr 0.000017	 wd 0.0500	time 0.8425 (0.9049)	loss 0.9898 (1.0621)	grad_norm 3.2167 (3.1481)	loss_scale 4096.0000 (2554.3440)	mem 23350MB
[2024-08-06 20:36:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:07:34 lr 0.000017	 wd 0.0500	time 0.8410 (0.9046)	loss 1.1397 (1.0627)	grad_norm 2.3165 (3.1320)	loss_scale 4096.0000 (2631.3883)	mem 23350MB
[2024-08-06 20:37:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:06:03 lr 0.000017	 wd 0.0500	time 0.8325 (0.9043)	loss 1.2426 (1.0622)	grad_norm 2.2508 (3.1199)	loss_scale 4096.0000 (2701.0985)	mem 23350MB
[2024-08-06 20:39:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:04:33 lr 0.000017	 wd 0.0500	time 0.8366 (0.9041)	loss 0.8321 (1.0636)	grad_norm 2.6214 (3.1422)	loss_scale 4096.0000 (2764.4743)	mem 23350MB
[2024-08-06 20:40:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:03:02 lr 0.000016	 wd 0.0500	time 0.9064 (0.9041)	loss 1.0981 (1.0622)	grad_norm 2.8457 (3.1388)	loss_scale 4096.0000 (2822.3416)	mem 23350MB
[2024-08-06 20:42:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:32 lr 0.000016	 wd 0.0500	time 0.8486 (0.9039)	loss 1.1905 (1.0634)	grad_norm 2.0980 (3.1254)	loss_scale 4096.0000 (2875.3886)	mem 23350MB
[2024-08-06 20:43:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8368 (0.9035)	loss 0.7349 (1.0617)	grad_norm 1.9586 (inf)	loss_scale 2048.0000 (2861.9592)	mem 23350MB
[2024-08-06 20:43:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 11 training takes 0:37:43
[2024-08-06 20:44:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.632 (12.632)	Loss 0.4607 (0.4607)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-06 20:44:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.678 Acc@5 97.968
[2024-08-06 20:44:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 20:44:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 20:44:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:34:08 lr 0.000016	 wd 0.0500	time 12.3294 (12.3294)	loss 0.8082 (0.8082)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:46:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:40:32 lr 0.000016	 wd 0.0500	time 0.8568 (1.0127)	loss 1.1854 (1.0595)	grad_norm 2.3786 (3.1576)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:47:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:36:44 lr 0.000016	 wd 0.0500	time 0.8584 (0.9576)	loss 1.1159 (1.0579)	grad_norm 2.4326 (3.1279)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:49:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:34:26 lr 0.000016	 wd 0.0500	time 0.8494 (0.9383)	loss 0.8521 (1.0660)	grad_norm 2.7103 (3.0627)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:50:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:32:30 lr 0.000016	 wd 0.0500	time 0.7861 (0.9280)	loss 1.1488 (1.0615)	grad_norm 5.0145 (3.0888)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:52:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:30:45 lr 0.000016	 wd 0.0500	time 0.8366 (0.9219)	loss 1.2875 (1.0601)	grad_norm 1.9719 (3.0103)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:53:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:29:05 lr 0.000016	 wd 0.0500	time 0.8480 (0.9178)	loss 1.3268 (1.0575)	grad_norm 2.9604 (3.0743)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:55:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:27:29 lr 0.000016	 wd 0.0500	time 0.8344 (0.9155)	loss 1.1582 (1.0554)	grad_norm 3.1072 (3.1195)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:56:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:25:54 lr 0.000016	 wd 0.0500	time 0.8559 (0.9133)	loss 1.2559 (1.0567)	grad_norm 4.4046 (3.1028)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:58:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:24:20 lr 0.000016	 wd 0.0500	time 0.8521 (0.9116)	loss 1.2899 (1.0543)	grad_norm 2.1493 (3.0769)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 20:59:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:22:47 lr 0.000016	 wd 0.0500	time 0.8367 (0.9104)	loss 1.1971 (1.0579)	grad_norm 2.3001 (3.0728)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:01:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:21:15 lr 0.000016	 wd 0.0500	time 0.8395 (0.9094)	loss 0.8327 (1.0601)	grad_norm 2.8405 (3.0924)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:02:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:19:43 lr 0.000016	 wd 0.0500	time 0.8490 (0.9089)	loss 1.1901 (1.0593)	grad_norm 2.6332 (3.0664)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:04:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:18:11 lr 0.000016	 wd 0.0500	time 0.8848 (0.9082)	loss 0.9664 (1.0607)	grad_norm 2.3320 (3.0477)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:05:42 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:16:40 lr 0.000016	 wd 0.0500	time 0.8427 (0.9076)	loss 1.2559 (1.0616)	grad_norm 2.3939 (3.0391)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:07:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:15:08 lr 0.000016	 wd 0.0500	time 0.8483 (0.9069)	loss 0.7393 (1.0584)	grad_norm 4.0734 (3.0451)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:08:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:13:37 lr 0.000016	 wd 0.0500	time 0.8514 (0.9064)	loss 1.3275 (1.0560)	grad_norm 2.2289 (3.0640)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:10:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:12:06 lr 0.000016	 wd 0.0500	time 0.8409 (0.9062)	loss 1.1144 (1.0585)	grad_norm 2.8591 (3.0448)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:11:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:10:35 lr 0.000016	 wd 0.0500	time 0.8508 (0.9058)	loss 0.7247 (1.0572)	grad_norm 2.1463 (3.0732)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 21:13:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:09:05 lr 0.000016	 wd 0.0500	time 0.8496 (0.9054)	loss 1.1390 (1.0578)	grad_norm 2.7349 (inf)	loss_scale 1024.0000 (2009.2162)	mem 23350MB
[2024-08-06 21:14:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:07:34 lr 0.000016	 wd 0.0500	time 0.8419 (0.9051)	loss 0.9181 (1.0587)	grad_norm 14.5763 (inf)	loss_scale 1024.0000 (1959.9800)	mem 23350MB
[2024-08-06 21:16:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:06:03 lr 0.000016	 wd 0.0500	time 0.9045 (0.9048)	loss 0.7241 (1.0578)	grad_norm 2.6050 (inf)	loss_scale 1024.0000 (1915.4307)	mem 23350MB
[2024-08-06 21:17:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:04:33 lr 0.000016	 wd 0.0500	time 0.8413 (0.9045)	loss 0.7143 (1.0594)	grad_norm 3.0969 (inf)	loss_scale 1024.0000 (1874.9296)	mem 23350MB
[2024-08-06 21:19:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:03:02 lr 0.000015	 wd 0.0500	time 0.8472 (0.9042)	loss 1.0007 (1.0596)	grad_norm 3.2403 (inf)	loss_scale 1024.0000 (1837.9487)	mem 23350MB
[2024-08-06 21:20:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:32 lr 0.000015	 wd 0.0500	time 0.8625 (0.9040)	loss 1.2950 (1.0601)	grad_norm 2.7643 (inf)	loss_scale 1024.0000 (1804.0483)	mem 23350MB
[2024-08-06 21:22:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.8402 (0.9037)	loss 0.6784 (1.0593)	grad_norm 2.6554 (inf)	loss_scale 1024.0000 (1772.8589)	mem 23350MB
[2024-08-06 21:22:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 12 training takes 0:37:43
[2024-08-06 21:22:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.806 (11.806)	Loss 0.4663 (0.4663)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-06 21:22:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.784 Acc@5 97.962
[2024-08-06 21:22:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 21:22:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 21:22:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:50:53 lr 0.000015	 wd 0.0500	time 12.7311 (12.7311)	loss 1.2396 (1.2396)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:24:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:40:43 lr 0.000015	 wd 0.0500	time 0.8996 (1.0174)	loss 1.2869 (1.1170)	grad_norm 1.8529 (2.9481)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:25:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:36:46 lr 0.000015	 wd 0.0500	time 0.8059 (0.9585)	loss 1.2899 (1.1017)	grad_norm 2.4894 (2.9921)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:27:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:34:26 lr 0.000015	 wd 0.0500	time 0.8879 (0.9385)	loss 1.2918 (1.1016)	grad_norm 2.5570 (2.9609)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:28:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:32:31 lr 0.000015	 wd 0.0500	time 0.8494 (0.9285)	loss 1.3593 (1.0860)	grad_norm 2.9179 (3.0076)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:30:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:30:46 lr 0.000015	 wd 0.0500	time 0.8382 (0.9224)	loss 1.2843 (1.0797)	grad_norm 3.5431 (2.9481)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:31:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:29:07 lr 0.000015	 wd 0.0500	time 0.8406 (0.9186)	loss 1.1610 (1.0737)	grad_norm 2.1017 (2.9861)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:33:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:27:30 lr 0.000015	 wd 0.0500	time 0.8571 (0.9162)	loss 1.0307 (1.0678)	grad_norm 3.8160 (3.0242)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:34:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:25:56 lr 0.000015	 wd 0.0500	time 0.8530 (0.9142)	loss 1.3173 (1.0677)	grad_norm 2.8385 (2.9954)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:36:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:24:21 lr 0.000015	 wd 0.0500	time 0.8440 (0.9124)	loss 1.0793 (1.0672)	grad_norm 3.3579 (2.9728)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:37:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:22:48 lr 0.000015	 wd 0.0500	time 0.9004 (0.9108)	loss 1.2096 (1.0664)	grad_norm 3.0714 (2.9699)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:39:29 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:21:16 lr 0.000015	 wd 0.0500	time 0.7965 (0.9102)	loss 1.2236 (1.0630)	grad_norm 2.1819 (2.9368)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:40:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:19:43 lr 0.000015	 wd 0.0500	time 0.8436 (0.9093)	loss 1.1389 (1.0671)	grad_norm 5.3699 (2.9294)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:42:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:18:12 lr 0.000015	 wd 0.0500	time 0.8346 (0.9085)	loss 0.8001 (1.0667)	grad_norm 2.4623 (2.9280)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:43:58 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:16:40 lr 0.000015	 wd 0.0500	time 0.8472 (0.9077)	loss 1.2849 (1.0654)	grad_norm 2.2106 (2.9260)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:45:28 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:15:08 lr 0.000015	 wd 0.0500	time 0.8424 (0.9069)	loss 1.1062 (1.0656)	grad_norm 4.2486 (2.9191)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:46:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:13:37 lr 0.000015	 wd 0.0500	time 0.8392 (0.9063)	loss 0.9196 (1.0653)	grad_norm 2.2693 (2.9206)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:48:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:12:06 lr 0.000015	 wd 0.0500	time 0.8486 (0.9059)	loss 1.0358 (1.0662)	grad_norm 2.8294 (2.9392)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:49:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:10:35 lr 0.000015	 wd 0.0500	time 0.8396 (0.9056)	loss 1.3151 (1.0665)	grad_norm 3.1213 (2.9428)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:51:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:09:04 lr 0.000015	 wd 0.0500	time 0.8973 (0.9053)	loss 1.0474 (1.0677)	grad_norm 2.6821 (2.9353)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:52:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:07:34 lr 0.000015	 wd 0.0500	time 0.8468 (0.9049)	loss 1.2415 (1.0676)	grad_norm 2.9723 (2.9631)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:54:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:06:03 lr 0.000014	 wd 0.0500	time 0.8569 (0.9047)	loss 1.3210 (1.0679)	grad_norm 2.1764 (2.9767)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:55:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:04:33 lr 0.000014	 wd 0.0500	time 0.8592 (0.9043)	loss 1.1212 (1.0664)	grad_norm 4.4152 (2.9732)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:57:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:03:02 lr 0.000014	 wd 0.0500	time 0.8428 (0.9041)	loss 1.2853 (1.0662)	grad_norm 2.8591 (2.9654)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 21:58:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:32 lr 0.000014	 wd 0.0500	time 0.8415 (0.9039)	loss 0.7773 (1.0688)	grad_norm 2.2148 (2.9597)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:00:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.8816 (0.9037)	loss 0.7882 (1.0672)	grad_norm 2.7649 (2.9703)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:00:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 13 training takes 0:37:43
[2024-08-06 22:00:43 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.773 (12.773)	Loss 0.4543 (0.4543)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-06 22:01:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.710 Acc@5 98.018
[2024-08-06 22:01:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 22:01:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 22:01:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][0/2502]	eta 8:59:14 lr 0.000014	 wd 0.0500	time 12.9313 (12.9313)	loss 1.2326 (1.2326)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:02:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:40:45 lr 0.000014	 wd 0.0500	time 0.8477 (1.0183)	loss 0.9404 (1.0689)	grad_norm 2.5159 (2.7248)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:04:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:36:46 lr 0.000014	 wd 0.0500	time 0.8520 (0.9583)	loss 1.2834 (1.0750)	grad_norm 2.3340 (2.7713)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:05:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:34:27 lr 0.000014	 wd 0.0500	time 0.8595 (0.9389)	loss 0.8486 (1.0767)	grad_norm 5.0294 (2.8317)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:07:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:32:34 lr 0.000014	 wd 0.0500	time 0.8393 (0.9297)	loss 1.4534 (1.0782)	grad_norm 2.4576 (2.8406)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:08:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:30:50 lr 0.000014	 wd 0.0500	time 0.8564 (0.9242)	loss 0.8726 (1.0738)	grad_norm 3.1167 (2.8455)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:10:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:29:09 lr 0.000014	 wd 0.0500	time 0.8431 (0.9200)	loss 0.9155 (1.0712)	grad_norm 2.2888 (2.8178)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:11:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:27:32 lr 0.000014	 wd 0.0500	time 0.8513 (0.9169)	loss 1.1236 (1.0729)	grad_norm 2.7668 (2.8542)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:13:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:25:56 lr 0.000014	 wd 0.0500	time 0.8543 (0.9145)	loss 0.7332 (1.0687)	grad_norm 2.6483 (2.8643)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 22:14:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:24:22 lr 0.000014	 wd 0.0500	time 0.8458 (0.9130)	loss 0.7391 (1.0676)	grad_norm 3.1358 (2.8714)	loss_scale 2048.0000 (1110.3751)	mem 23350MB
[2024-08-06 22:16:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:22:48 lr 0.000014	 wd 0.0500	time 0.8332 (0.9113)	loss 0.9255 (1.0660)	grad_norm 2.4167 (2.9470)	loss_scale 2048.0000 (1204.0440)	mem 23350MB
[2024-08-06 22:17:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:21:15 lr 0.000014	 wd 0.0500	time 0.8858 (0.9101)	loss 1.3052 (1.0668)	grad_norm 2.7111 (2.9369)	loss_scale 2048.0000 (1280.6975)	mem 23350MB
[2024-08-06 22:19:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:19:43 lr 0.000014	 wd 0.0500	time 0.8393 (0.9091)	loss 1.4325 (1.0664)	grad_norm 2.8862 (2.9257)	loss_scale 2048.0000 (1344.5862)	mem 23350MB
[2024-08-06 22:20:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:18:11 lr 0.000014	 wd 0.0500	time 0.8487 (0.9083)	loss 1.1072 (1.0699)	grad_norm 2.4911 (2.9469)	loss_scale 2048.0000 (1398.6533)	mem 23350MB
[2024-08-06 22:22:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:16:40 lr 0.000014	 wd 0.0500	time 0.8464 (0.9079)	loss 1.2180 (1.0705)	grad_norm 2.2038 (3.0207)	loss_scale 2048.0000 (1445.0021)	mem 23350MB
[2024-08-06 22:23:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:15:09 lr 0.000014	 wd 0.0500	time 0.8313 (0.9074)	loss 1.3509 (1.0690)	grad_norm 2.7778 (3.0134)	loss_scale 2048.0000 (1485.1752)	mem 23350MB
[2024-08-06 22:25:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:13:38 lr 0.000014	 wd 0.0500	time 0.8566 (0.9070)	loss 0.8278 (1.0681)	grad_norm 2.6118 (3.0199)	loss_scale 2048.0000 (1520.3298)	mem 23350MB
[2024-08-06 22:26:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:12:07 lr 0.000014	 wd 0.0500	time 0.8489 (0.9066)	loss 0.7888 (1.0676)	grad_norm 2.2021 (3.0372)	loss_scale 2048.0000 (1551.3510)	mem 23350MB
[2024-08-06 22:28:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:10:36 lr 0.000013	 wd 0.0500	time 0.8450 (0.9061)	loss 0.9582 (1.0688)	grad_norm 2.6791 (3.0331)	loss_scale 2048.0000 (1578.9273)	mem 23350MB
[2024-08-06 22:29:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:09:05 lr 0.000013	 wd 0.0500	time 0.8485 (0.9057)	loss 1.1785 (1.0694)	grad_norm 3.6019 (3.0345)	loss_scale 2048.0000 (1603.6023)	mem 23350MB
[2024-08-06 22:31:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:07:34 lr 0.000013	 wd 0.0500	time 0.8328 (0.9053)	loss 1.2997 (1.0681)	grad_norm 3.3953 (3.0517)	loss_scale 2048.0000 (1625.8111)	mem 23350MB
[2024-08-06 22:32:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:06:03 lr 0.000013	 wd 0.0500	time 0.8426 (0.9050)	loss 1.1841 (1.0672)	grad_norm 2.6849 (3.0589)	loss_scale 2048.0000 (1645.9058)	mem 23350MB
[2024-08-06 22:34:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:04:33 lr 0.000013	 wd 0.0500	time 0.7934 (0.9047)	loss 1.1750 (1.0666)	grad_norm 2.5627 (3.0518)	loss_scale 2048.0000 (1664.1745)	mem 23350MB
[2024-08-06 22:35:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:03:02 lr 0.000013	 wd 0.0500	time 0.7916 (0.9045)	loss 0.7906 (1.0661)	grad_norm 2.9743 (3.0396)	loss_scale 2048.0000 (1680.8553)	mem 23350MB
[2024-08-06 22:37:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:32 lr 0.000013	 wd 0.0500	time 0.8424 (0.9044)	loss 0.6989 (1.0656)	grad_norm 3.0286 (3.0376)	loss_scale 2048.0000 (1696.1466)	mem 23350MB
[2024-08-06 22:38:45 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.8454 (0.9042)	loss 1.2373 (1.0649)	grad_norm 2.3442 (3.0279)	loss_scale 2048.0000 (1710.2151)	mem 23350MB
[2024-08-06 22:38:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 14 training takes 0:37:44
[2024-08-06 22:39:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.390 (12.390)	Loss 0.4431 (0.4431)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23350MB
[2024-08-06 22:39:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.844 Acc@5 97.948
[2024-08-06 22:39:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-06 22:39:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 22:39:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][0/2502]	eta 8:31:29 lr 0.000013	 wd 0.0500	time 12.2658 (12.2658)	loss 1.1730 (1.1730)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 22:41:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:40:20 lr 0.000013	 wd 0.0500	time 0.8499 (1.0077)	loss 0.7149 (1.0629)	grad_norm 3.8838 (2.9110)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 22:42:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:36:32 lr 0.000013	 wd 0.0500	time 0.8351 (0.9525)	loss 1.4364 (1.0529)	grad_norm 2.7365 (2.9334)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 22:44:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:34:17 lr 0.000013	 wd 0.0500	time 0.8601 (0.9345)	loss 0.8243 (1.0519)	grad_norm 3.3942 (nan)	loss_scale 1024.0000 (1782.6445)	mem 23350MB
[2024-08-06 22:45:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:32:25 lr 0.000013	 wd 0.0500	time 0.7859 (0.9256)	loss 0.7145 (1.0507)	grad_norm 2.5998 (nan)	loss_scale 1024.0000 (1593.4564)	mem 23350MB
[2024-08-06 22:47:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:30:41 lr 0.000013	 wd 0.0500	time 0.7898 (0.9199)	loss 0.6672 (1.0594)	grad_norm 2.5531 (nan)	loss_scale 1024.0000 (1479.7924)	mem 23350MB
[2024-08-06 22:48:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:29:01 lr 0.000013	 wd 0.0500	time 0.8490 (0.9158)	loss 1.1830 (1.0647)	grad_norm 2.4379 (nan)	loss_scale 1024.0000 (1403.9534)	mem 23350MB
[2024-08-06 22:50:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:27:26 lr 0.000013	 wd 0.0500	time 0.8745 (0.9136)	loss 1.2298 (1.0670)	grad_norm 3.2030 (nan)	loss_scale 1024.0000 (1349.7518)	mem 23350MB
[2024-08-06 22:51:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:25:52 lr 0.000013	 wd 0.0500	time 0.8367 (0.9119)	loss 1.0966 (1.0679)	grad_norm 2.4254 (nan)	loss_scale 1024.0000 (1309.0836)	mem 23350MB
[2024-08-06 22:53:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:24:18 lr 0.000013	 wd 0.0500	time 0.8496 (0.9104)	loss 1.2868 (1.0693)	grad_norm 2.4149 (nan)	loss_scale 1024.0000 (1277.4428)	mem 23350MB
[2024-08-06 22:54:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:22:45 lr 0.000013	 wd 0.0500	time 0.8452 (0.9089)	loss 0.7238 (1.0690)	grad_norm 2.3478 (nan)	loss_scale 1024.0000 (1252.1239)	mem 23350MB
[2024-08-06 22:56:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:21:12 lr 0.000013	 wd 0.0500	time 0.9009 (0.9080)	loss 0.7891 (1.0699)	grad_norm 2.1685 (nan)	loss_scale 1024.0000 (1231.4042)	mem 23350MB
[2024-08-06 22:57:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:19:41 lr 0.000013	 wd 0.0500	time 0.9020 (0.9075)	loss 0.8113 (1.0704)	grad_norm 4.2242 (nan)	loss_scale 1024.0000 (1214.1349)	mem 23350MB
[2024-08-06 22:59:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:18:10 lr 0.000013	 wd 0.0500	time 0.8480 (0.9070)	loss 0.8353 (1.0716)	grad_norm 2.9064 (nan)	loss_scale 1024.0000 (1199.5204)	mem 23350MB
[2024-08-06 23:00:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:16:38 lr 0.000012	 wd 0.0500	time 0.8503 (0.9064)	loss 0.7243 (1.0714)	grad_norm 2.6897 (nan)	loss_scale 1024.0000 (1186.9921)	mem 23350MB
[2024-08-06 23:02:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:15:07 lr 0.000012	 wd 0.0500	time 0.7907 (0.9059)	loss 1.0107 (1.0676)	grad_norm 2.6467 (nan)	loss_scale 1024.0000 (1176.1332)	mem 23350MB
[2024-08-06 23:03:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:13:36 lr 0.000012	 wd 0.0500	time 0.8840 (0.9055)	loss 0.8354 (1.0685)	grad_norm 4.2140 (nan)	loss_scale 1024.0000 (1166.6309)	mem 23350MB
[2024-08-06 23:05:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:12:05 lr 0.000012	 wd 0.0500	time 0.8440 (0.9052)	loss 0.7258 (1.0689)	grad_norm 4.3613 (nan)	loss_scale 1024.0000 (1158.2457)	mem 23350MB
[2024-08-06 23:06:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:10:35 lr 0.000012	 wd 0.0500	time 0.8464 (0.9051)	loss 1.5435 (1.0675)	grad_norm 2.7540 (nan)	loss_scale 1024.0000 (1150.7918)	mem 23350MB
[2024-08-06 23:08:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:09:04 lr 0.000012	 wd 0.0500	time 0.8537 (0.9048)	loss 0.8083 (1.0668)	grad_norm 2.4047 (nan)	loss_scale 1024.0000 (1144.1220)	mem 23350MB
[2024-08-06 23:09:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:07:34 lr 0.000012	 wd 0.0500	time 0.8482 (0.9044)	loss 0.8389 (1.0677)	grad_norm 2.1783 (nan)	loss_scale 1024.0000 (1138.1189)	mem 23350MB
[2024-08-06 23:11:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:06:03 lr 0.000012	 wd 0.0500	time 0.8554 (0.9041)	loss 0.6981 (1.0665)	grad_norm 2.6161 (nan)	loss_scale 1024.0000 (1132.6873)	mem 23350MB
[2024-08-06 23:12:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:04:32 lr 0.000012	 wd 0.0500	time 0.8437 (0.9038)	loss 0.7897 (1.0660)	grad_norm 2.9177 (nan)	loss_scale 1024.0000 (1127.7492)	mem 23350MB
[2024-08-06 23:14:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:03:02 lr 0.000012	 wd 0.0500	time 0.8959 (0.9037)	loss 0.9026 (1.0665)	grad_norm 2.5857 (nan)	loss_scale 1024.0000 (1123.2403)	mem 23350MB
[2024-08-06 23:15:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:32 lr 0.000012	 wd 0.0500	time 0.8562 (0.9034)	loss 1.2338 (1.0669)	grad_norm 5.4334 (nan)	loss_scale 1024.0000 (1119.1070)	mem 23350MB
[2024-08-06 23:17:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.8340 (0.9031)	loss 1.3628 (1.0677)	grad_norm 2.9931 (nan)	loss_scale 1024.0000 (1115.3043)	mem 23350MB
[2024-08-06 23:17:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 15 training takes 0:37:41
[2024-08-06 23:17:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_15.pth saving......
[2024-08-06 23:17:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_15.pth saved !!!
[2024-08-06 23:17:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.006 (12.006)	Loss 0.4883 (0.4883)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 23350MB
[2024-08-06 23:17:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.736 Acc@5 97.958
[2024-08-06 23:17:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 23:17:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 23:17:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:31:44 lr 0.000012	 wd 0.0500	time 12.2721 (12.2721)	loss 1.1625 (1.1625)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:19:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:40:32 lr 0.000012	 wd 0.0500	time 0.8558 (1.0128)	loss 0.9641 (1.0760)	grad_norm 2.2660 (3.5514)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:20:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:36:47 lr 0.000012	 wd 0.0500	time 0.7884 (0.9588)	loss 1.1424 (1.0662)	grad_norm 2.4094 (3.3605)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:22:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:34:32 lr 0.000012	 wd 0.0500	time 0.7708 (0.9412)	loss 0.9601 (1.0588)	grad_norm 2.6384 (3.3938)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:23:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:32:35 lr 0.000012	 wd 0.0500	time 0.8478 (0.9305)	loss 0.7607 (1.0490)	grad_norm 2.4423 (3.4910)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:25:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:30:49 lr 0.000012	 wd 0.0500	time 0.8366 (0.9241)	loss 1.3332 (1.0536)	grad_norm 7.1461 (3.4800)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:26:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:29:09 lr 0.000012	 wd 0.0500	time 0.8568 (0.9198)	loss 1.3221 (1.0523)	grad_norm 2.2165 (3.4804)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:28:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:27:32 lr 0.000012	 wd 0.0500	time 0.8370 (0.9171)	loss 0.9492 (1.0546)	grad_norm 2.4646 (3.4643)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:29:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:25:56 lr 0.000012	 wd 0.0500	time 0.8384 (0.9145)	loss 1.2220 (1.0561)	grad_norm 3.0511 (3.4155)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:31:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:24:22 lr 0.000012	 wd 0.0500	time 0.8365 (0.9128)	loss 0.9984 (1.0544)	grad_norm 3.0639 (3.3890)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:32:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:22:48 lr 0.000011	 wd 0.0500	time 0.7806 (0.9113)	loss 1.2423 (1.0557)	grad_norm 2.3493 (3.3535)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:34:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:21:15 lr 0.000011	 wd 0.0500	time 0.8508 (0.9100)	loss 0.8539 (1.0607)	grad_norm 2.3959 (3.3283)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:35:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:19:43 lr 0.000011	 wd 0.0500	time 0.8458 (0.9093)	loss 0.8318 (1.0601)	grad_norm 3.5178 (3.3683)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:37:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:18:12 lr 0.000011	 wd 0.0500	time 0.9024 (0.9086)	loss 1.1758 (1.0631)	grad_norm 2.3276 (3.3274)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:38:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:16:40 lr 0.000011	 wd 0.0500	time 0.8882 (0.9080)	loss 1.1773 (1.0639)	grad_norm 2.5882 (3.3030)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:40:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:15:09 lr 0.000011	 wd 0.0500	time 0.8463 (0.9074)	loss 1.1304 (1.0658)	grad_norm 2.1859 (3.3460)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:41:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:13:37 lr 0.000011	 wd 0.0500	time 0.8519 (0.9068)	loss 1.2038 (1.0662)	grad_norm 2.5699 (3.3276)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:43:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:12:06 lr 0.000011	 wd 0.0500	time 0.8447 (0.9062)	loss 0.9625 (1.0658)	grad_norm 2.6989 (3.3264)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-06 23:44:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:10:35 lr 0.000011	 wd 0.0500	time 0.7887 (0.9058)	loss 0.8271 (1.0649)	grad_norm 4.8086 (3.2992)	loss_scale 2048.0000 (1069.4858)	mem 23350MB
[2024-08-06 23:46:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:09:04 lr 0.000011	 wd 0.0500	time 0.8486 (0.9052)	loss 0.7492 (1.0657)	grad_norm 2.5839 (3.2813)	loss_scale 2048.0000 (1120.9595)	mem 23350MB
[2024-08-06 23:47:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:07:34 lr 0.000011	 wd 0.0500	time 0.8470 (0.9049)	loss 1.2286 (1.0664)	grad_norm 2.6606 (3.2791)	loss_scale 2048.0000 (1167.2884)	mem 23350MB
[2024-08-06 23:49:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:06:03 lr 0.000011	 wd 0.0500	time 0.8380 (0.9047)	loss 1.1901 (1.0672)	grad_norm 2.0807 (3.2658)	loss_scale 2048.0000 (1209.2070)	mem 23350MB
[2024-08-06 23:50:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:04:33 lr 0.000011	 wd 0.0500	time 0.8466 (0.9046)	loss 0.8279 (1.0671)	grad_norm 2.7471 (3.2548)	loss_scale 2048.0000 (1247.3167)	mem 23350MB
[2024-08-06 23:52:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:03:02 lr 0.000011	 wd 0.0500	time 0.8512 (0.9043)	loss 0.7268 (1.0654)	grad_norm 4.1310 (3.2416)	loss_scale 2048.0000 (1282.1139)	mem 23350MB
[2024-08-06 23:53:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:32 lr 0.000011	 wd 0.0500	time 0.8870 (0.9040)	loss 1.1262 (1.0649)	grad_norm 2.1052 (3.2464)	loss_scale 2048.0000 (1314.0125)	mem 23350MB
[2024-08-06 23:55:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.7954 (0.9037)	loss 0.7622 (1.0646)	grad_norm 3.0781 (3.2755)	loss_scale 2048.0000 (1343.3603)	mem 23350MB
[2024-08-06 23:55:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 16 training takes 0:37:43
[2024-08-06 23:55:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.811 (11.811)	Loss 0.4648 (0.4648)	Acc@1 92.969 (92.969)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-06 23:55:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.740 Acc@5 97.932
[2024-08-06 23:55:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-06 23:55:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-06 23:56:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:56:22 lr 0.000011	 wd 0.0500	time 11.4237 (11.4237)	loss 1.1735 (1.1735)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 23:57:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:40:08 lr 0.000011	 wd 0.0500	time 0.8392 (1.0029)	loss 1.1745 (1.0428)	grad_norm 3.5371 (3.3011)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-06 23:59:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:36:29 lr 0.000011	 wd 0.0500	time 0.8364 (0.9512)	loss 1.2847 (1.0455)	grad_norm 2.3042 (3.0381)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:00:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:34:14 lr 0.000011	 wd 0.0500	time 0.8407 (0.9331)	loss 1.3366 (1.0601)	grad_norm 2.6433 (3.0209)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:02:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:32:23 lr 0.000011	 wd 0.0500	time 0.9054 (0.9244)	loss 1.4352 (1.0560)	grad_norm 5.1175 (3.0137)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:03:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:30:40 lr 0.000010	 wd 0.0500	time 0.8374 (0.9194)	loss 1.2997 (1.0628)	grad_norm 86.5796 (3.3611)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:05:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:29:03 lr 0.000010	 wd 0.0500	time 0.8489 (0.9165)	loss 0.8007 (1.0638)	grad_norm 2.8771 (3.2746)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:06:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:27:27 lr 0.000010	 wd 0.0500	time 0.8505 (0.9140)	loss 1.2534 (1.0610)	grad_norm 2.1366 (3.2196)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:08:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:25:52 lr 0.000010	 wd 0.0500	time 0.8545 (0.9119)	loss 1.1096 (1.0610)	grad_norm 3.7258 (3.2233)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:09:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:24:18 lr 0.000010	 wd 0.0500	time 0.8597 (0.9104)	loss 1.2094 (1.0629)	grad_norm 3.7461 (3.2199)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:11:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:22:45 lr 0.000010	 wd 0.0500	time 0.8560 (0.9091)	loss 0.9120 (1.0617)	grad_norm 2.7475 (3.2730)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:12:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:21:12 lr 0.000010	 wd 0.0500	time 0.8383 (0.9080)	loss 0.8976 (1.0663)	grad_norm 2.3500 (3.2446)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:14:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:19:40 lr 0.000010	 wd 0.0500	time 0.8440 (0.9070)	loss 0.7747 (1.0666)	grad_norm 3.9683 (3.2415)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:15:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:18:09 lr 0.000010	 wd 0.0500	time 0.8437 (0.9062)	loss 1.3924 (1.0655)	grad_norm 4.1621 (3.2639)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:17:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:16:38 lr 0.000010	 wd 0.0500	time 0.8377 (0.9057)	loss 1.3526 (1.0656)	grad_norm 2.1200 (3.2438)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:18:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:15:07 lr 0.000010	 wd 0.0500	time 0.8573 (0.9053)	loss 1.2279 (1.0656)	grad_norm 4.2390 (3.2641)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:20:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:13:36 lr 0.000010	 wd 0.0500	time 0.8331 (0.9051)	loss 1.2837 (1.0643)	grad_norm 2.6019 (3.2447)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:21:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:12:05 lr 0.000010	 wd 0.0500	time 0.8477 (0.9047)	loss 1.4160 (1.0661)	grad_norm 2.2253 (3.2286)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:23:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:10:34 lr 0.000010	 wd 0.0500	time 0.8389 (0.9045)	loss 0.9032 (1.0662)	grad_norm 10.6078 (3.2127)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:24:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:09:04 lr 0.000010	 wd 0.0500	time 0.8453 (0.9041)	loss 1.1385 (1.0653)	grad_norm 2.8360 (3.2100)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:26:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:07:33 lr 0.000010	 wd 0.0500	time 0.8564 (0.9039)	loss 0.9029 (1.0661)	grad_norm 3.1467 (3.2227)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:27:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:06:03 lr 0.000010	 wd 0.0500	time 0.8531 (0.9036)	loss 1.3630 (1.0662)	grad_norm 2.3955 (3.2433)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:29:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:04:32 lr 0.000010	 wd 0.0500	time 0.8444 (0.9035)	loss 1.3803 (1.0669)	grad_norm 2.4635 (3.2258)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:30:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:03:02 lr 0.000010	 wd 0.0500	time 0.8452 (0.9032)	loss 0.7663 (1.0666)	grad_norm 2.5912 (3.2205)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:32:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:32 lr 0.000010	 wd 0.0500	time 0.8399 (0.9029)	loss 0.7730 (1.0652)	grad_norm 2.1180 (3.2045)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:33:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.8307 (0.9027)	loss 1.3968 (1.0653)	grad_norm 2.4337 (3.2019)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:33:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 17 training takes 0:37:40
[2024-08-07 00:33:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.148 (12.148)	Loss 0.4434 (0.4434)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-07 00:34:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.714 Acc@5 97.946
[2024-08-07 00:34:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-07 00:34:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 00:34:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:25:43 lr 0.000009	 wd 0.0500	time 12.1277 (12.1277)	loss 1.3842 (1.3842)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:35:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:40:31 lr 0.000009	 wd 0.0500	time 0.8330 (1.0124)	loss 1.4145 (1.0745)	grad_norm 2.1967 (3.0070)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:37:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:36:41 lr 0.000009	 wd 0.0500	time 0.8888 (0.9563)	loss 0.8855 (1.0774)	grad_norm 2.6744 (2.8467)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:38:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:34:22 lr 0.000009	 wd 0.0500	time 0.8484 (0.9367)	loss 1.4683 (1.0788)	grad_norm 3.1223 (2.9644)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:40:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:32:28 lr 0.000009	 wd 0.0500	time 0.8861 (0.9268)	loss 0.6826 (1.0782)	grad_norm 2.9260 (2.8979)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:41:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:30:43 lr 0.000009	 wd 0.0500	time 0.8477 (0.9206)	loss 1.2880 (1.0746)	grad_norm 2.1683 (2.9271)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:43:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:29:04 lr 0.000009	 wd 0.0500	time 0.8398 (0.9171)	loss 1.4113 (1.0692)	grad_norm 2.5358 (2.9296)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:44:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:27:28 lr 0.000009	 wd 0.0500	time 0.8363 (0.9147)	loss 1.2812 (1.0722)	grad_norm 3.5994 (2.9111)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 00:46:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:25:53 lr 0.000009	 wd 0.0500	time 0.9532 (0.9128)	loss 1.2690 (1.0760)	grad_norm 2.4972 (2.9492)	loss_scale 4096.0000 (2262.7715)	mem 23350MB
[2024-08-07 00:47:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:24:20 lr 0.000009	 wd 0.0500	time 0.8436 (0.9116)	loss 0.9022 (1.0745)	grad_norm 1.9673 (3.0552)	loss_scale 4096.0000 (2466.2375)	mem 23350MB
[2024-08-07 00:49:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:22:47 lr 0.000009	 wd 0.0500	time 0.8050 (0.9107)	loss 1.3140 (1.0729)	grad_norm 2.5743 (3.0647)	loss_scale 4096.0000 (2629.0509)	mem 23350MB
[2024-08-07 00:50:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:21:15 lr 0.000009	 wd 0.0500	time 0.8514 (0.9095)	loss 0.9060 (1.0743)	grad_norm 2.4697 (3.0433)	loss_scale 4096.0000 (2762.2888)	mem 23350MB
[2024-08-07 00:52:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:19:42 lr 0.000009	 wd 0.0500	time 0.8230 (0.9085)	loss 0.9105 (1.0691)	grad_norm 2.3212 (3.0332)	loss_scale 4096.0000 (2873.3389)	mem 23350MB
[2024-08-07 00:53:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:18:10 lr 0.000009	 wd 0.0500	time 0.8969 (0.9075)	loss 1.4125 (1.0688)	grad_norm 3.8828 (3.1218)	loss_scale 4096.0000 (2967.3174)	mem 23350MB
[2024-08-07 00:55:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:16:39 lr 0.000009	 wd 0.0500	time 0.9078 (0.9070)	loss 1.1557 (1.0696)	grad_norm 4.5536 (3.1332)	loss_scale 4096.0000 (3047.8801)	mem 23350MB
[2024-08-07 00:56:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:15:08 lr 0.000009	 wd 0.0500	time 0.8970 (0.9064)	loss 1.2340 (1.0697)	grad_norm 1.9974 (3.1126)	loss_scale 4096.0000 (3117.7082)	mem 23350MB
[2024-08-07 00:58:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:13:37 lr 0.000009	 wd 0.0500	time 0.8487 (0.9059)	loss 1.2350 (1.0713)	grad_norm 2.7125 (3.1133)	loss_scale 4096.0000 (3178.8132)	mem 23350MB
[2024-08-07 00:59:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:12:06 lr 0.000009	 wd 0.0500	time 0.8497 (0.9053)	loss 1.2071 (1.0727)	grad_norm 2.1674 (3.1145)	loss_scale 4096.0000 (3232.7337)	mem 23350MB
[2024-08-07 01:01:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:10:35 lr 0.000009	 wd 0.0500	time 0.7848 (0.9050)	loss 0.7202 (1.0731)	grad_norm 2.2737 (3.1006)	loss_scale 4096.0000 (3280.6663)	mem 23350MB
[2024-08-07 01:02:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:09:04 lr 0.000009	 wd 0.0500	time 0.8400 (0.9048)	loss 1.4058 (1.0730)	grad_norm 2.1958 (inf)	loss_scale 2048.0000 (3269.6896)	mem 23350MB
[2024-08-07 01:04:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:07:33 lr 0.000008	 wd 0.0500	time 0.8559 (0.9043)	loss 0.8790 (1.0721)	grad_norm 3.0051 (inf)	loss_scale 2048.0000 (3208.6357)	mem 23350MB
[2024-08-07 01:05:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:06:03 lr 0.000008	 wd 0.0500	time 0.8431 (0.9040)	loss 1.2107 (1.0718)	grad_norm 2.2419 (inf)	loss_scale 2048.0000 (3153.3936)	mem 23350MB
[2024-08-07 01:07:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:04:32 lr 0.000008	 wd 0.0500	time 0.8365 (0.9037)	loss 1.0135 (1.0720)	grad_norm 2.1530 (inf)	loss_scale 2048.0000 (3103.1713)	mem 23350MB
[2024-08-07 01:08:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:03:02 lr 0.000008	 wd 0.0500	time 0.8544 (0.9034)	loss 1.2622 (1.0730)	grad_norm 1.9221 (inf)	loss_scale 2048.0000 (3057.3142)	mem 23350MB
[2024-08-07 01:10:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:32 lr 0.000008	 wd 0.0500	time 0.9043 (0.9033)	loss 1.3161 (1.0732)	grad_norm 2.5685 (inf)	loss_scale 2048.0000 (3015.2770)	mem 23350MB
[2024-08-07 01:11:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.8434 (0.9029)	loss 1.0625 (1.0735)	grad_norm 2.3655 (inf)	loss_scale 2048.0000 (2976.6014)	mem 23350MB
[2024-08-07 01:11:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 18 training takes 0:37:41
[2024-08-07 01:12:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.137 (12.137)	Loss 0.4583 (0.4583)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 01:12:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.796 Acc@5 97.970
[2024-08-07 01:12:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 01:12:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 01:12:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:55:20 lr 0.000008	 wd 0.0500	time 12.8380 (12.8380)	loss 0.7610 (0.7610)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:14:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:40:40 lr 0.000008	 wd 0.0500	time 0.8729 (1.0158)	loss 1.4730 (1.1116)	grad_norm 2.5182 (2.7502)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:15:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:36:45 lr 0.000008	 wd 0.0500	time 0.8412 (0.9581)	loss 0.8281 (1.0836)	grad_norm 2.8141 (3.1198)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:17:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:34:28 lr 0.000008	 wd 0.0500	time 0.8522 (0.9393)	loss 1.3403 (1.0734)	grad_norm 2.2636 (3.0747)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:18:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:32:35 lr 0.000008	 wd 0.0500	time 0.8508 (0.9304)	loss 0.9596 (1.0647)	grad_norm 2.9523 (2.9572)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:20:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:30:50 lr 0.000008	 wd 0.0500	time 0.8470 (0.9242)	loss 1.2875 (1.0720)	grad_norm 2.5564 (2.9635)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:21:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:29:09 lr 0.000008	 wd 0.0500	time 0.9031 (0.9199)	loss 0.8345 (1.0652)	grad_norm 6.4053 (3.0364)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:23:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:27:32 lr 0.000008	 wd 0.0500	time 0.8475 (0.9170)	loss 1.0345 (1.0639)	grad_norm 2.1542 (3.0954)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:24:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:25:56 lr 0.000008	 wd 0.0500	time 0.8463 (0.9148)	loss 1.3262 (1.0649)	grad_norm 2.7561 (3.1297)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:26:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:24:22 lr 0.000008	 wd 0.0500	time 0.8370 (0.9131)	loss 1.1250 (1.0661)	grad_norm 2.6050 (3.1168)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:27:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:22:49 lr 0.000008	 wd 0.0500	time 0.8514 (0.9116)	loss 1.2802 (1.0663)	grad_norm 2.4047 (3.1194)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:29:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:21:16 lr 0.000008	 wd 0.0500	time 0.8565 (0.9104)	loss 1.0614 (1.0644)	grad_norm 3.0144 (3.0906)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:30:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:19:43 lr 0.000008	 wd 0.0500	time 0.8477 (0.9093)	loss 1.2706 (1.0629)	grad_norm 2.3466 (3.0845)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:32:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:18:12 lr 0.000008	 wd 0.0500	time 0.9370 (0.9087)	loss 0.8428 (1.0624)	grad_norm 6.5799 (3.0590)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:33:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:16:40 lr 0.000008	 wd 0.0500	time 0.8555 (0.9081)	loss 0.8207 (1.0623)	grad_norm 2.9476 (3.0632)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:35:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:15:09 lr 0.000008	 wd 0.0500	time 0.8520 (0.9075)	loss 1.3760 (1.0633)	grad_norm 2.4362 (3.0671)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:36:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:13:37 lr 0.000007	 wd 0.0500	time 0.8476 (0.9068)	loss 1.1388 (1.0647)	grad_norm 2.4441 (3.0398)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:38:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:12:06 lr 0.000007	 wd 0.0500	time 0.8487 (0.9062)	loss 0.7006 (1.0652)	grad_norm 2.5272 (3.0226)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:39:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:10:35 lr 0.000007	 wd 0.0500	time 0.8393 (0.9057)	loss 1.1767 (1.0669)	grad_norm 2.4273 (3.0361)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:41:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:09:04 lr 0.000007	 wd 0.0500	time 0.8400 (0.9053)	loss 1.0801 (1.0655)	grad_norm 4.1812 (3.0339)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:42:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:07:34 lr 0.000007	 wd 0.0500	time 0.8540 (0.9050)	loss 1.1773 (1.0665)	grad_norm 2.3941 (3.0381)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:44:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:06:03 lr 0.000007	 wd 0.0500	time 0.8390 (0.9046)	loss 1.5284 (1.0686)	grad_norm 2.3213 (3.0568)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:45:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:04:33 lr 0.000007	 wd 0.0500	time 0.8382 (0.9042)	loss 1.1825 (1.0680)	grad_norm 8.3618 (3.0626)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:47:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:03:02 lr 0.000007	 wd 0.0500	time 0.9092 (0.9042)	loss 0.9047 (1.0677)	grad_norm 3.7139 (3.0551)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:48:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:32 lr 0.000007	 wd 0.0500	time 0.8487 (0.9040)	loss 1.0202 (1.0670)	grad_norm 2.3069 (3.0457)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:50:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.8504 (0.9037)	loss 0.7515 (1.0670)	grad_norm 2.7539 (3.0567)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:50:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 19 training takes 0:37:43
[2024-08-07 01:50:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 10.895 (10.895)	Loss 0.4465 (0.4465)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 01:50:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.806 Acc@5 97.956
[2024-08-07 01:50:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 01:50:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 01:50:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][0/2502]	eta 8:36:13 lr 0.000007	 wd 0.0500	time 12.3793 (12.3793)	loss 1.2557 (1.2557)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:52:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:40:30 lr 0.000007	 wd 0.0500	time 0.8372 (1.0120)	loss 0.8482 (1.0758)	grad_norm 13.6492 (2.9870)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:53:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:36:40 lr 0.000007	 wd 0.0500	time 0.8635 (0.9560)	loss 1.2214 (1.0647)	grad_norm 41.0704 (3.3095)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:55:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:34:22 lr 0.000007	 wd 0.0500	time 0.8531 (0.9366)	loss 0.6799 (1.0741)	grad_norm 3.6137 (3.3126)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:56:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:32:27 lr 0.000007	 wd 0.0500	time 0.8528 (0.9267)	loss 1.1839 (1.0719)	grad_norm 6.9577 (3.2774)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:58:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:30:44 lr 0.000007	 wd 0.0500	time 0.8334 (0.9212)	loss 1.2219 (1.0708)	grad_norm 2.7796 (3.2187)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 01:59:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:29:04 lr 0.000007	 wd 0.0500	time 0.8380 (0.9172)	loss 1.2867 (1.0747)	grad_norm 2.7487 (3.2195)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 02:01:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:27:28 lr 0.000007	 wd 0.0500	time 0.8439 (0.9149)	loss 1.1852 (1.0730)	grad_norm 2.4517 (3.1939)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 02:02:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:25:53 lr 0.000007	 wd 0.0500	time 0.8393 (0.9130)	loss 1.1228 (1.0747)	grad_norm 2.2428 (3.1656)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 02:04:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:24:19 lr 0.000007	 wd 0.0500	time 0.8460 (0.9114)	loss 1.2628 (1.0731)	grad_norm 3.6270 (3.1157)	loss_scale 4096.0000 (2170.7436)	mem 23350MB
[2024-08-07 02:05:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:22:46 lr 0.000007	 wd 0.0500	time 0.8340 (0.9100)	loss 1.1665 (1.0745)	grad_norm 2.8909 (3.1392)	loss_scale 4096.0000 (2363.0769)	mem 23350MB
[2024-08-07 02:07:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:21:14 lr 0.000007	 wd 0.0500	time 0.8915 (0.9091)	loss 0.8385 (1.0726)	grad_norm 2.6245 (3.1236)	loss_scale 4096.0000 (2520.4723)	mem 23350MB
[2024-08-07 02:08:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:19:42 lr 0.000006	 wd 0.0500	time 0.8524 (0.9080)	loss 1.1442 (1.0697)	grad_norm 2.6560 (3.0933)	loss_scale 4096.0000 (2651.6570)	mem 23350MB
[2024-08-07 02:10:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:18:10 lr 0.000006	 wd 0.0500	time 0.8824 (0.9074)	loss 1.3090 (1.0696)	grad_norm 2.7391 (3.0758)	loss_scale 4096.0000 (2762.6749)	mem 23350MB
[2024-08-07 02:11:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:16:39 lr 0.000006	 wd 0.0500	time 0.8563 (0.9067)	loss 1.0024 (1.0717)	grad_norm 4.0781 (3.0730)	loss_scale 4096.0000 (2857.8444)	mem 23350MB
[2024-08-07 02:13:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:15:07 lr 0.000006	 wd 0.0500	time 0.8465 (0.9062)	loss 1.3087 (1.0732)	grad_norm 3.4073 (3.0717)	loss_scale 4096.0000 (2940.3331)	mem 23350MB
[2024-08-07 02:14:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:13:37 lr 0.000006	 wd 0.0500	time 0.9137 (0.9058)	loss 1.1295 (1.0749)	grad_norm 4.3101 (3.0679)	loss_scale 4096.0000 (3012.5172)	mem 23350MB
[2024-08-07 02:16:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:12:06 lr 0.000006	 wd 0.0500	time 0.8439 (0.9054)	loss 1.4311 (1.0755)	grad_norm 2.9090 (nan)	loss_scale 1024.0000 (2958.2222)	mem 23350MB
[2024-08-07 02:17:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:10:35 lr 0.000006	 wd 0.0500	time 0.8486 (0.9050)	loss 0.7472 (1.0736)	grad_norm 4.2593 (nan)	loss_scale 1024.0000 (2850.8251)	mem 23350MB
[2024-08-07 02:19:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:09:04 lr 0.000006	 wd 0.0500	time 0.8445 (0.9046)	loss 0.7362 (1.0741)	grad_norm 2.2746 (nan)	loss_scale 1024.0000 (2754.7270)	mem 23350MB
[2024-08-07 02:20:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:07:33 lr 0.000006	 wd 0.0500	time 0.8867 (0.9043)	loss 1.0097 (1.0736)	grad_norm 3.1595 (nan)	loss_scale 1024.0000 (2668.2339)	mem 23350MB
[2024-08-07 02:22:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:06:03 lr 0.000006	 wd 0.0500	time 0.8027 (0.9039)	loss 1.2067 (1.0740)	grad_norm 5.1701 (nan)	loss_scale 1024.0000 (2589.9743)	mem 23350MB
[2024-08-07 02:23:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:04:32 lr 0.000006	 wd 0.0500	time 0.8355 (0.9036)	loss 1.2558 (1.0754)	grad_norm 2.3396 (nan)	loss_scale 1024.0000 (2518.8260)	mem 23350MB
[2024-08-07 02:25:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:03:02 lr 0.000006	 wd 0.0500	time 0.8539 (0.9034)	loss 0.7705 (1.0769)	grad_norm 2.1362 (nan)	loss_scale 1024.0000 (2453.8618)	mem 23350MB
[2024-08-07 02:26:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:32 lr 0.000006	 wd 0.0500	time 0.8471 (0.9032)	loss 0.6772 (1.0765)	grad_norm 3.6533 (nan)	loss_scale 1024.0000 (2394.3090)	mem 23350MB
[2024-08-07 02:28:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.8612 (0.9030)	loss 1.0430 (1.0783)	grad_norm 3.6967 (nan)	loss_scale 1024.0000 (2339.5186)	mem 23350MB
[2024-08-07 02:28:21 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 20 training takes 0:37:42
[2024-08-07 02:28:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.967 (11.967)	Loss 0.4585 (0.4585)	Acc@1 93.164 (93.164)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-07 02:28:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.718 Acc@5 97.956
[2024-08-07 02:28:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-07 02:28:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 02:29:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:32:42 lr 0.000006	 wd 0.0500	time 12.2953 (12.2953)	loss 0.8618 (0.8618)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:30:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:40:34 lr 0.000006	 wd 0.0500	time 0.8913 (1.0135)	loss 1.2634 (1.0811)	grad_norm 9.6851 (4.0214)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:32:08 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:36:42 lr 0.000006	 wd 0.0500	time 0.8347 (0.9568)	loss 1.2301 (1.0830)	grad_norm 5.4061 (3.4533)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:33:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:34:23 lr 0.000006	 wd 0.0500	time 0.8345 (0.9373)	loss 1.0376 (1.0774)	grad_norm 4.2330 (3.2290)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:35:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:32:30 lr 0.000006	 wd 0.0500	time 0.9600 (0.9279)	loss 1.3596 (1.0685)	grad_norm 2.5501 (3.3400)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:36:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:30:45 lr 0.000006	 wd 0.0500	time 0.8805 (0.9220)	loss 0.7502 (1.0647)	grad_norm 2.8276 (3.5577)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:38:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:29:05 lr 0.000006	 wd 0.0500	time 0.8472 (0.9177)	loss 1.2031 (1.0609)	grad_norm 2.6878 (3.4681)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:39:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:27:29 lr 0.000006	 wd 0.0500	time 0.7933 (0.9155)	loss 1.1695 (1.0628)	grad_norm 2.1786 (3.3769)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:41:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:25:54 lr 0.000006	 wd 0.0500	time 0.8458 (0.9132)	loss 1.2182 (1.0619)	grad_norm 2.5122 (3.3127)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:42:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:24:20 lr 0.000005	 wd 0.0500	time 0.8407 (0.9116)	loss 1.2644 (1.0656)	grad_norm 2.5664 (3.3018)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:44:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:22:47 lr 0.000005	 wd 0.0500	time 0.8301 (0.9103)	loss 1.2680 (1.0687)	grad_norm 2.3745 (3.2784)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:45:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:21:15 lr 0.000005	 wd 0.0500	time 0.8307 (0.9096)	loss 0.7724 (1.0724)	grad_norm 2.5799 (3.2821)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:47:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:19:43 lr 0.000005	 wd 0.0500	time 0.8474 (0.9086)	loss 1.2150 (1.0690)	grad_norm 3.1081 (3.3093)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:48:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:18:11 lr 0.000005	 wd 0.0500	time 0.8230 (0.9078)	loss 1.1895 (1.0692)	grad_norm 2.1701 (3.2662)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:50:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:16:39 lr 0.000005	 wd 0.0500	time 0.8460 (0.9070)	loss 0.7488 (1.0673)	grad_norm 2.4099 (3.2272)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:51:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:15:08 lr 0.000005	 wd 0.0500	time 0.8539 (0.9063)	loss 0.7656 (1.0688)	grad_norm 2.3678 (3.2161)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:53:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:13:37 lr 0.000005	 wd 0.0500	time 0.8280 (0.9058)	loss 1.0722 (1.0656)	grad_norm 2.7048 (3.1892)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:54:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:12:06 lr 0.000005	 wd 0.0500	time 0.8391 (0.9053)	loss 1.2846 (1.0656)	grad_norm 2.3470 (3.2004)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:56:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:10:35 lr 0.000005	 wd 0.0500	time 0.8375 (0.9048)	loss 0.7642 (1.0659)	grad_norm 2.9527 (3.1889)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:57:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:09:04 lr 0.000005	 wd 0.0500	time 0.8484 (0.9045)	loss 0.6818 (1.0672)	grad_norm 2.8866 (3.1682)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 02:59:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:07:33 lr 0.000005	 wd 0.0500	time 0.8553 (0.9043)	loss 1.0476 (1.0667)	grad_norm 2.4654 (3.1670)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 03:00:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:06:03 lr 0.000005	 wd 0.0500	time 0.8367 (0.9041)	loss 1.1690 (1.0662)	grad_norm 4.6265 (nan)	loss_scale 512.0000 (1022.5378)	mem 23350MB
[2024-08-07 03:02:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:04:32 lr 0.000005	 wd 0.0500	time 0.8488 (0.9038)	loss 1.3083 (1.0645)	grad_norm 2.2363 (nan)	loss_scale 512.0000 (999.3421)	mem 23350MB
[2024-08-07 03:03:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:03:02 lr 0.000005	 wd 0.0500	time 0.8382 (0.9034)	loss 1.3015 (1.0666)	grad_norm 5.7347 (nan)	loss_scale 512.0000 (978.1625)	mem 23350MB
[2024-08-07 03:05:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:32 lr 0.000005	 wd 0.0500	time 0.8461 (0.9032)	loss 1.0330 (1.0676)	grad_norm 2.7247 (nan)	loss_scale 512.0000 (958.7472)	mem 23350MB
[2024-08-07 03:06:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.8491 (0.9029)	loss 1.3254 (1.0671)	grad_norm 2.6047 (nan)	loss_scale 512.0000 (940.8844)	mem 23350MB
[2024-08-07 03:06:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 21 training takes 0:37:42
[2024-08-07 03:06:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.437 (11.437)	Loss 0.4680 (0.4680)	Acc@1 93.164 (93.164)	Acc@5 99.219 (99.219)	Mem 23350MB
[2024-08-07 03:07:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.790 Acc@5 97.932
[2024-08-07 03:07:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 03:07:14 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 03:07:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:23:00 lr 0.000005	 wd 0.0500	time 12.0624 (12.0624)	loss 0.9415 (0.9415)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:08:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:40:25 lr 0.000005	 wd 0.0500	time 0.8513 (1.0097)	loss 1.2324 (1.0769)	grad_norm 2.5321 (2.6646)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:10:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:36:34 lr 0.000005	 wd 0.0500	time 0.8954 (0.9532)	loss 0.6307 (1.0889)	grad_norm 4.3986 (2.9076)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:11:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:34:19 lr 0.000005	 wd 0.0500	time 0.8409 (0.9352)	loss 1.4338 (1.0905)	grad_norm 2.4140 (2.9074)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:13:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:32:27 lr 0.000005	 wd 0.0500	time 0.8417 (0.9266)	loss 0.7176 (1.0913)	grad_norm 2.3357 (3.0414)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:14:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:30:44 lr 0.000005	 wd 0.0500	time 0.8549 (0.9214)	loss 1.0198 (1.0872)	grad_norm 2.9309 (2.9877)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:16:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:29:06 lr 0.000005	 wd 0.0500	time 0.8502 (0.9181)	loss 0.7842 (1.0864)	grad_norm 2.7921 (2.9796)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:17:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:27:29 lr 0.000005	 wd 0.0500	time 0.8586 (0.9155)	loss 1.3344 (1.0802)	grad_norm 2.3165 (2.9641)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:19:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:25:54 lr 0.000004	 wd 0.0500	time 0.7922 (0.9135)	loss 0.9476 (1.0800)	grad_norm 2.1416 (2.9927)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:20:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:24:20 lr 0.000004	 wd 0.0500	time 0.8427 (0.9119)	loss 1.0681 (1.0802)	grad_norm 2.4495 (3.1470)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:22:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:22:47 lr 0.000004	 wd 0.0500	time 0.7838 (0.9106)	loss 1.2752 (1.0807)	grad_norm 2.0721 (3.1883)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:23:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:21:15 lr 0.000004	 wd 0.0500	time 0.8488 (0.9096)	loss 0.7469 (1.0767)	grad_norm 2.8322 (3.1669)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:25:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:19:42 lr 0.000004	 wd 0.0500	time 0.8496 (0.9086)	loss 1.2324 (1.0775)	grad_norm 3.2285 (3.1740)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:26:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:18:11 lr 0.000004	 wd 0.0500	time 0.8405 (0.9081)	loss 0.7368 (1.0719)	grad_norm 2.7521 (3.1464)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:28:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:16:40 lr 0.000004	 wd 0.0500	time 0.8507 (0.9077)	loss 0.9651 (1.0672)	grad_norm 2.5814 (3.1580)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:29:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:15:08 lr 0.000004	 wd 0.0500	time 0.8365 (0.9072)	loss 1.0588 (1.0668)	grad_norm 7.5946 (3.1650)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:31:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:13:37 lr 0.000004	 wd 0.0500	time 0.8375 (0.9066)	loss 1.2589 (1.0664)	grad_norm 2.4445 (3.1487)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:32:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:12:06 lr 0.000004	 wd 0.0500	time 0.8503 (0.9060)	loss 0.8619 (1.0645)	grad_norm 2.8308 (3.1299)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:34:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:10:35 lr 0.000004	 wd 0.0500	time 0.8505 (0.9056)	loss 1.0962 (1.0632)	grad_norm 2.3722 (3.1360)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:35:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:09:05 lr 0.000004	 wd 0.0500	time 0.8356 (0.9053)	loss 0.8921 (1.0641)	grad_norm 2.1524 (3.1121)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:37:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:07:34 lr 0.000004	 wd 0.0500	time 0.8525 (0.9049)	loss 0.8399 (1.0654)	grad_norm 2.0555 (3.1236)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:38:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:06:03 lr 0.000004	 wd 0.0500	time 0.8319 (0.9046)	loss 1.1864 (1.0665)	grad_norm 2.4444 (3.1187)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:40:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:04:33 lr 0.000004	 wd 0.0500	time 0.8503 (0.9042)	loss 1.0146 (1.0663)	grad_norm 11.3001 (3.1435)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:41:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:03:02 lr 0.000004	 wd 0.0500	time 0.8956 (0.9039)	loss 1.1272 (1.0669)	grad_norm 2.6654 (3.1489)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:43:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:32 lr 0.000004	 wd 0.0500	time 0.8528 (0.9039)	loss 1.3525 (1.0685)	grad_norm 3.1410 (3.1388)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:44:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.8396 (0.9036)	loss 0.8291 (1.0688)	grad_norm 2.5372 (3.1311)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:45:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 22 training takes 0:37:46
[2024-08-07 03:45:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.636 (12.636)	Loss 0.4536 (0.4536)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 03:45:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.744 Acc@5 97.970
[2024-08-07 03:45:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-07 03:45:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 03:45:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:09:20 lr 0.000004	 wd 0.0500	time 11.7347 (11.7347)	loss 0.7460 (0.7460)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:47:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:40:17 lr 0.000004	 wd 0.0500	time 0.8300 (1.0063)	loss 1.2099 (1.0524)	grad_norm 2.3686 (3.4781)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:48:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:36:33 lr 0.000004	 wd 0.0500	time 0.8466 (0.9527)	loss 0.8701 (1.0624)	grad_norm 3.0525 (3.3992)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:50:19 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:34:17 lr 0.000004	 wd 0.0500	time 0.8437 (0.9342)	loss 1.2571 (1.0527)	grad_norm 2.4911 (3.2285)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:51:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:32:24 lr 0.000004	 wd 0.0500	time 0.8448 (0.9251)	loss 0.8287 (1.0600)	grad_norm 2.9748 (3.4612)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:53:18 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:30:40 lr 0.000004	 wd 0.0500	time 0.8538 (0.9194)	loss 0.8170 (1.0605)	grad_norm 2.3580 (3.3730)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:54:48 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:29:01 lr 0.000004	 wd 0.0500	time 0.8382 (0.9155)	loss 1.2588 (1.0602)	grad_norm 4.8999 (3.4384)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:56:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:27:24 lr 0.000004	 wd 0.0500	time 0.8447 (0.9126)	loss 1.1769 (1.0630)	grad_norm 3.1346 (3.4000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:57:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:25:50 lr 0.000003	 wd 0.0500	time 0.8337 (0.9111)	loss 1.2794 (1.0657)	grad_norm 2.6661 (3.3438)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 03:59:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:24:17 lr 0.000003	 wd 0.0500	time 0.8542 (0.9099)	loss 1.0522 (1.0666)	grad_norm 2.0767 (3.4151)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 04:00:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:22:45 lr 0.000003	 wd 0.0500	time 0.8704 (0.9089)	loss 1.1033 (1.0628)	grad_norm 2.1524 (3.3811)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 04:02:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:21:13 lr 0.000003	 wd 0.0500	time 0.8770 (0.9080)	loss 0.9490 (1.0648)	grad_norm 2.6125 (3.3931)	loss_scale 1024.0000 (516.6503)	mem 23350MB
[2024-08-07 04:03:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:19:40 lr 0.000003	 wd 0.0500	time 0.8404 (0.9071)	loss 1.2514 (1.0673)	grad_norm 2.7172 (3.3565)	loss_scale 1024.0000 (558.8943)	mem 23350MB
[2024-08-07 04:05:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:18:09 lr 0.000003	 wd 0.0500	time 0.8502 (0.9064)	loss 1.1467 (1.0685)	grad_norm 2.1945 (3.3219)	loss_scale 1024.0000 (594.6441)	mem 23350MB
[2024-08-07 04:06:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:16:38 lr 0.000003	 wd 0.0500	time 0.8435 (0.9058)	loss 1.2839 (1.0685)	grad_norm 2.6994 (3.3745)	loss_scale 1024.0000 (625.2905)	mem 23350MB
[2024-08-07 04:08:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:15:07 lr 0.000003	 wd 0.0500	time 0.8858 (0.9054)	loss 1.2681 (1.0669)	grad_norm 8.6004 (3.3708)	loss_scale 1024.0000 (651.8534)	mem 23350MB
[2024-08-07 04:09:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:13:36 lr 0.000003	 wd 0.0500	time 0.8815 (0.9051)	loss 1.0889 (1.0696)	grad_norm 2.7223 (3.3338)	loss_scale 1024.0000 (675.0981)	mem 23350MB
[2024-08-07 04:11:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:12:05 lr 0.000003	 wd 0.0500	time 0.8393 (0.9048)	loss 0.6684 (1.0672)	grad_norm 29.7215 (3.3407)	loss_scale 1024.0000 (695.6096)	mem 23350MB
[2024-08-07 04:12:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:10:35 lr 0.000003	 wd 0.0500	time 0.7864 (0.9046)	loss 1.2973 (1.0667)	grad_norm 2.6042 (3.3325)	loss_scale 1024.0000 (713.8434)	mem 23350MB
[2024-08-07 04:14:17 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:09:04 lr 0.000003	 wd 0.0500	time 0.8868 (0.9044)	loss 1.2881 (1.0661)	grad_norm 3.0498 (3.3358)	loss_scale 1024.0000 (730.1589)	mem 23350MB
[2024-08-07 04:15:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:07:33 lr 0.000003	 wd 0.0500	time 0.8465 (0.9041)	loss 0.7743 (1.0653)	grad_norm 3.5848 (3.3182)	loss_scale 1024.0000 (744.8436)	mem 23350MB
[2024-08-07 04:17:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:06:03 lr 0.000003	 wd 0.0500	time 0.8451 (0.9038)	loss 1.2174 (1.0661)	grad_norm 2.5992 (3.2963)	loss_scale 1024.0000 (758.1304)	mem 23350MB
[2024-08-07 04:18:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:04:32 lr 0.000003	 wd 0.0500	time 0.8552 (0.9035)	loss 1.2308 (1.0651)	grad_norm 2.2903 (3.2807)	loss_scale 1024.0000 (770.2099)	mem 23350MB
[2024-08-07 04:20:16 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:03:02 lr 0.000003	 wd 0.0500	time 0.8405 (0.9032)	loss 1.3756 (1.0645)	grad_norm 2.4086 (3.2784)	loss_scale 1024.0000 (781.2395)	mem 23350MB
[2024-08-07 04:21:46 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:32 lr 0.000003	 wd 0.0500	time 0.8501 (0.9030)	loss 0.8755 (1.0643)	grad_norm 2.1901 (3.3160)	loss_scale 1024.0000 (791.3503)	mem 23350MB
[2024-08-07 04:23:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.8670 (0.9028)	loss 0.8091 (1.0632)	grad_norm 2.3401 (3.3081)	loss_scale 1024.0000 (800.6525)	mem 23350MB
[2024-08-07 04:23:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 23 training takes 0:37:44
[2024-08-07 04:23:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.896 (11.896)	Loss 0.4707 (0.4707)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 04:24:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.756 Acc@5 97.956
[2024-08-07 04:24:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 04:24:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 04:24:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:08:10 lr 0.000003	 wd 0.0500	time 11.7070 (11.7070)	loss 1.1898 (1.1898)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:25:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:40:17 lr 0.000003	 wd 0.0500	time 0.8287 (1.0065)	loss 0.8897 (1.0797)	grad_norm 2.8375 (3.3366)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:27:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:36:32 lr 0.000003	 wd 0.0500	time 0.9368 (0.9526)	loss 1.1083 (1.0698)	grad_norm 2.4229 (3.5350)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:28:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:34:20 lr 0.000003	 wd 0.0500	time 0.8593 (0.9358)	loss 1.2723 (1.0811)	grad_norm 2.6134 (3.4059)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:30:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:32:27 lr 0.000003	 wd 0.0500	time 0.8498 (0.9264)	loss 0.6425 (1.0787)	grad_norm 2.6280 (3.2368)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:31:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:30:44 lr 0.000003	 wd 0.0500	time 0.8533 (0.9212)	loss 1.2196 (1.0791)	grad_norm 2.8204 (3.1660)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:33:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:29:05 lr 0.000003	 wd 0.0500	time 0.8437 (0.9177)	loss 0.7902 (1.0757)	grad_norm 3.0948 (3.1320)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:34:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:27:28 lr 0.000003	 wd 0.0500	time 0.8488 (0.9148)	loss 1.2323 (1.0743)	grad_norm 2.5545 (3.0931)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:36:11 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:25:53 lr 0.000003	 wd 0.0500	time 0.8381 (0.9128)	loss 1.1946 (1.0748)	grad_norm 2.2491 (3.0357)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:37:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:24:19 lr 0.000003	 wd 0.0500	time 0.7859 (0.9110)	loss 0.8480 (1.0791)	grad_norm 2.4004 (3.0879)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:39:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:22:46 lr 0.000003	 wd 0.0500	time 0.8479 (0.9095)	loss 0.7937 (1.0764)	grad_norm 2.1890 (3.1670)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:40:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:21:13 lr 0.000003	 wd 0.0500	time 0.8455 (0.9085)	loss 0.8070 (1.0749)	grad_norm 2.6798 (3.1231)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:42:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:19:41 lr 0.000002	 wd 0.0500	time 0.9357 (0.9078)	loss 0.9006 (1.0726)	grad_norm 2.8409 (3.1445)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:43:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:18:10 lr 0.000002	 wd 0.0500	time 0.8111 (0.9072)	loss 0.8086 (1.0705)	grad_norm 8.3493 (3.1523)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:45:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:16:39 lr 0.000002	 wd 0.0500	time 0.8354 (0.9067)	loss 1.3887 (1.0716)	grad_norm 2.9056 (3.1398)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:46:40 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:15:07 lr 0.000002	 wd 0.0500	time 0.8479 (0.9062)	loss 1.1401 (1.0682)	grad_norm 2.4530 (3.1588)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:48:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:13:36 lr 0.000002	 wd 0.0500	time 0.8481 (0.9056)	loss 1.2084 (1.0671)	grad_norm 2.6471 (3.1394)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:49:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:12:05 lr 0.000002	 wd 0.0500	time 0.8451 (0.9052)	loss 1.5153 (1.0685)	grad_norm 3.6216 (3.1671)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:51:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:10:35 lr 0.000002	 wd 0.0500	time 0.8340 (0.9048)	loss 1.1089 (1.0689)	grad_norm 6.0716 (3.1589)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:52:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:09:04 lr 0.000002	 wd 0.0500	time 0.8433 (0.9043)	loss 0.7945 (1.0694)	grad_norm 2.2718 (3.1465)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:54:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:07:33 lr 0.000002	 wd 0.0500	time 0.8419 (0.9041)	loss 1.1967 (1.0692)	grad_norm 3.4919 (3.1420)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:55:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:06:03 lr 0.000002	 wd 0.0500	time 0.8468 (0.9037)	loss 0.9365 (1.0690)	grad_norm 2.4486 (3.1419)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:57:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:04:32 lr 0.000002	 wd 0.0500	time 0.8880 (0.9038)	loss 0.8603 (1.0707)	grad_norm 3.1770 (3.1544)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 04:58:39 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:03:02 lr 0.000002	 wd 0.0500	time 0.8480 (0.9036)	loss 1.2202 (1.0718)	grad_norm 3.3468 (3.1526)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 05:00:09 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:32 lr 0.000002	 wd 0.0500	time 0.8454 (0.9034)	loss 1.2922 (1.0696)	grad_norm 2.6543 (3.1424)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 05:01:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.8331 (0.9031)	loss 0.7115 (1.0700)	grad_norm 3.1957 (3.1483)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 05:01:47 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 24 training takes 0:37:47
[2024-08-07 05:01:59 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.979 (11.979)	Loss 0.4561 (0.4561)	Acc@1 92.578 (92.578)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 05:02:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.772 Acc@5 97.964
[2024-08-07 05:02:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 05:02:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 05:02:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:30:02 lr 0.000002	 wd 0.0500	time 12.2314 (12.2314)	loss 1.2523 (1.2523)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23350MB
[2024-08-07 05:04:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:40:19 lr 0.000002	 wd 0.0500	time 0.8442 (1.0072)	loss 1.1345 (1.1356)	grad_norm 3.2706 (2.9264)	loss_scale 2048.0000 (1165.9406)	mem 23350MB
[2024-08-07 05:05:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:36:34 lr 0.000002	 wd 0.0500	time 0.7908 (0.9532)	loss 1.3716 (1.1036)	grad_norm 2.2795 (3.1490)	loss_scale 2048.0000 (1604.7761)	mem 23350MB
[2024-08-07 05:07:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:34:18 lr 0.000002	 wd 0.0500	time 0.8717 (0.9350)	loss 1.1074 (1.0884)	grad_norm 2.2355 (3.0824)	loss_scale 2048.0000 (1752.0266)	mem 23350MB
[2024-08-07 05:08:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:32:26 lr 0.000002	 wd 0.0500	time 0.8521 (0.9261)	loss 1.0940 (1.0843)	grad_norm 2.3579 (3.0484)	loss_scale 2048.0000 (1825.8354)	mem 23350MB
[2024-08-07 05:10:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:30:44 lr 0.000002	 wd 0.0500	time 0.8566 (0.9212)	loss 1.2287 (1.0819)	grad_norm 2.5293 (3.0268)	loss_scale 2048.0000 (1870.1796)	mem 23350MB
[2024-08-07 05:11:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:29:05 lr 0.000002	 wd 0.0500	time 0.8517 (0.9179)	loss 0.8519 (1.0727)	grad_norm 2.1624 (3.0202)	loss_scale 2048.0000 (1899.7671)	mem 23350MB
[2024-08-07 05:13:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:27:29 lr 0.000002	 wd 0.0500	time 0.8470 (0.9156)	loss 0.7337 (1.0748)	grad_norm 2.2919 (3.0938)	loss_scale 2048.0000 (1920.9130)	mem 23350MB
[2024-08-07 05:14:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:25:54 lr 0.000002	 wd 0.0500	time 0.8479 (0.9135)	loss 1.2902 (1.0728)	grad_norm 2.5913 (3.1139)	loss_scale 2048.0000 (1936.7790)	mem 23350MB
[2024-08-07 05:16:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:24:20 lr 0.000002	 wd 0.0500	time 0.8467 (0.9119)	loss 1.1796 (1.0728)	grad_norm 4.9584 (3.3587)	loss_scale 2048.0000 (1949.1232)	mem 23350MB
[2024-08-07 05:17:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:22:47 lr 0.000002	 wd 0.0500	time 0.8455 (0.9107)	loss 1.4884 (1.0732)	grad_norm 3.4212 (3.3276)	loss_scale 2048.0000 (1959.0010)	mem 23350MB
[2024-08-07 05:19:07 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:21:14 lr 0.000002	 wd 0.0500	time 0.8292 (0.9094)	loss 1.3533 (1.0751)	grad_norm 3.8373 (3.2738)	loss_scale 2048.0000 (1967.0845)	mem 23350MB
[2024-08-07 05:20:37 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:19:42 lr 0.000002	 wd 0.0500	time 0.8544 (0.9086)	loss 0.8756 (1.0719)	grad_norm 2.3484 (3.2486)	loss_scale 2048.0000 (1973.8218)	mem 23350MB
[2024-08-07 05:22:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:18:10 lr 0.000002	 wd 0.0500	time 0.8438 (0.9076)	loss 0.8473 (1.0716)	grad_norm 2.1745 (3.2441)	loss_scale 2048.0000 (1979.5234)	mem 23350MB
[2024-08-07 05:23:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:16:39 lr 0.000002	 wd 0.0500	time 0.8883 (0.9069)	loss 0.9717 (1.0715)	grad_norm 4.3199 (3.2199)	loss_scale 2048.0000 (1984.4111)	mem 23350MB
[2024-08-07 05:25:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:15:08 lr 0.000002	 wd 0.0500	time 0.8471 (0.9063)	loss 0.7462 (1.0711)	grad_norm 2.2930 (3.1952)	loss_scale 2048.0000 (1988.6476)	mem 23350MB
[2024-08-07 05:26:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:13:37 lr 0.000002	 wd 0.0500	time 0.8385 (0.9059)	loss 0.7784 (1.0703)	grad_norm 2.6984 (3.1982)	loss_scale 2048.0000 (1992.3548)	mem 23350MB
[2024-08-07 05:28:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:12:06 lr 0.000002	 wd 0.0500	time 0.8403 (0.9056)	loss 0.6964 (1.0679)	grad_norm 2.1601 (3.1759)	loss_scale 2048.0000 (1995.6261)	mem 23350MB
[2024-08-07 05:29:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:10:35 lr 0.000002	 wd 0.0500	time 0.8516 (0.9051)	loss 0.7566 (1.0697)	grad_norm 4.6835 (3.1879)	loss_scale 2048.0000 (1998.5341)	mem 23350MB
[2024-08-07 05:31:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:09:04 lr 0.000002	 wd 0.0500	time 0.8415 (0.9049)	loss 1.3855 (1.0715)	grad_norm 4.2028 (3.2138)	loss_scale 2048.0000 (2001.1362)	mem 23350MB
[2024-08-07 05:32:36 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:07:34 lr 0.000002	 wd 0.0500	time 0.8303 (0.9047)	loss 0.8505 (1.0716)	grad_norm 2.0631 (3.2609)	loss_scale 2048.0000 (2003.4783)	mem 23350MB
[2024-08-07 05:34:06 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:06:03 lr 0.000002	 wd 0.0500	time 0.8410 (0.9044)	loss 1.1945 (1.0724)	grad_norm 2.3714 (3.2647)	loss_scale 2048.0000 (2005.5973)	mem 23350MB
[2024-08-07 05:35:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:04:33 lr 0.000001	 wd 0.0500	time 0.8475 (0.9041)	loss 1.0185 (1.0728)	grad_norm 2.7968 (3.2662)	loss_scale 2048.0000 (2007.5239)	mem 23350MB
[2024-08-07 05:37:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:03:02 lr 0.000001	 wd 0.0500	time 0.8076 (0.9038)	loss 1.2737 (1.0727)	grad_norm 1.9260 (3.2401)	loss_scale 2048.0000 (2009.2829)	mem 23350MB
[2024-08-07 05:38:35 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:01:32 lr 0.000001	 wd 0.0500	time 0.8504 (0.9037)	loss 1.4013 (1.0722)	grad_norm 2.6154 (3.2296)	loss_scale 2048.0000 (2010.8955)	mem 23350MB
[2024-08-07 05:40:05 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.8333 (0.9034)	loss 0.9324 (1.0708)	grad_norm 2.8998 (3.2423)	loss_scale 2048.0000 (2012.3790)	mem 23350MB
[2024-08-07 05:40:12 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 25 training takes 0:37:46
[2024-08-07 05:40:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.075 (12.075)	Loss 0.4744 (0.4744)	Acc@1 92.383 (92.383)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-07 05:40:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.712 Acc@5 97.960
[2024-08-07 05:40:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-07 05:40:51 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 05:41:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:54:24 lr 0.000001	 wd 0.0500	time 12.8157 (12.8157)	loss 1.0310 (1.0310)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:42:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:40:51 lr 0.000001	 wd 0.0500	time 0.8337 (1.0206)	loss 1.2458 (1.0809)	grad_norm 3.1629 (3.1237)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:44:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:36:50 lr 0.000001	 wd 0.0500	time 0.8417 (0.9603)	loss 0.9642 (1.0761)	grad_norm 6.7673 (3.1029)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:45:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:34:29 lr 0.000001	 wd 0.0500	time 0.8567 (0.9399)	loss 0.8284 (1.0636)	grad_norm 2.5885 (3.0509)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:47:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:32:34 lr 0.000001	 wd 0.0500	time 0.8506 (0.9297)	loss 1.3762 (1.0637)	grad_norm 2.5029 (2.9940)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:48:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:30:48 lr 0.000001	 wd 0.0500	time 0.8795 (0.9233)	loss 1.2635 (1.0722)	grad_norm 2.3951 (2.9866)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:50:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:29:08 lr 0.000001	 wd 0.0500	time 0.8473 (0.9194)	loss 0.7056 (1.0742)	grad_norm 2.0350 (3.0331)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:51:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:27:31 lr 0.000001	 wd 0.0500	time 0.7825 (0.9164)	loss 1.1283 (1.0704)	grad_norm 3.6125 (3.0039)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:53:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:25:56 lr 0.000001	 wd 0.0500	time 0.8929 (0.9144)	loss 1.2260 (1.0748)	grad_norm 2.0532 (2.9759)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:54:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:24:22 lr 0.000001	 wd 0.0500	time 0.8594 (0.9130)	loss 1.1433 (1.0751)	grad_norm 3.7820 (2.9985)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:56:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:22:49 lr 0.000001	 wd 0.0500	time 0.9102 (0.9120)	loss 0.9517 (1.0743)	grad_norm 2.7389 (2.9945)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:57:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:21:17 lr 0.000001	 wd 0.0500	time 0.8510 (0.9111)	loss 0.7841 (1.0732)	grad_norm 2.1841 (2.9762)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 05:59:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:19:45 lr 0.000001	 wd 0.0500	time 0.8865 (0.9102)	loss 0.8829 (1.0737)	grad_norm 2.9335 (2.9698)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 06:00:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:18:12 lr 0.000001	 wd 0.0500	time 0.8633 (0.9093)	loss 1.1773 (1.0757)	grad_norm 2.8543 (2.9623)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 06:02:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:16:41 lr 0.000001	 wd 0.0500	time 0.8515 (0.9087)	loss 1.3430 (1.0775)	grad_norm 2.4440 (2.9608)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 06:03:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:15:09 lr 0.000001	 wd 0.0500	time 0.8438 (0.9079)	loss 1.2557 (1.0793)	grad_norm 2.2479 (2.9657)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 06:05:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:13:38 lr 0.000001	 wd 0.0500	time 0.8498 (0.9073)	loss 1.0739 (1.0781)	grad_norm 2.5403 (2.9679)	loss_scale 4096.0000 (2068.4672)	mem 23350MB
[2024-08-07 06:06:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:12:07 lr 0.000001	 wd 0.0500	time 0.9017 (0.9067)	loss 1.0612 (1.0748)	grad_norm 2.2549 (2.9834)	loss_scale 4096.0000 (2187.6637)	mem 23350MB
[2024-08-07 06:08:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:10:36 lr 0.000001	 wd 0.0500	time 0.8388 (0.9064)	loss 0.7042 (1.0739)	grad_norm 2.9714 (3.0100)	loss_scale 4096.0000 (2293.6235)	mem 23350MB
[2024-08-07 06:09:34 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:09:05 lr 0.000001	 wd 0.0500	time 0.8381 (0.9060)	loss 1.2718 (1.0754)	grad_norm 2.8462 (3.0140)	loss_scale 4096.0000 (2388.4356)	mem 23350MB
[2024-08-07 06:11:04 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:07:34 lr 0.000001	 wd 0.0500	time 0.7875 (0.9057)	loss 1.0590 (1.0740)	grad_norm 4.3028 (3.0092)	loss_scale 4096.0000 (2473.7711)	mem 23350MB
[2024-08-07 06:12:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:06:03 lr 0.000001	 wd 0.0500	time 0.8403 (0.9053)	loss 1.2653 (1.0736)	grad_norm 2.3668 (3.0131)	loss_scale 4096.0000 (2550.9833)	mem 23350MB
[2024-08-07 06:14:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:04:33 lr 0.000001	 wd 0.0500	time 0.8330 (0.9049)	loss 1.1935 (1.0719)	grad_norm 2.4834 (3.0097)	loss_scale 4096.0000 (2621.1795)	mem 23350MB
[2024-08-07 06:15:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:03:02 lr 0.000001	 wd 0.0500	time 0.8476 (0.9047)	loss 1.3112 (1.0722)	grad_norm 2.1725 (3.0042)	loss_scale 4096.0000 (2685.2742)	mem 23350MB
[2024-08-07 06:17:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:01:32 lr 0.000001	 wd 0.0500	time 0.8439 (0.9044)	loss 0.6908 (1.0728)	grad_norm 2.4200 (3.0077)	loss_scale 4096.0000 (2744.0300)	mem 23350MB
[2024-08-07 06:18:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.8486 (0.9042)	loss 0.7659 (1.0728)	grad_norm 5.2797 (3.0012)	loss_scale 4096.0000 (2798.0872)	mem 23350MB
[2024-08-07 06:18:41 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 26 training takes 0:37:49
[2024-08-07 06:18:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.169 (12.169)	Loss 0.4602 (0.4602)	Acc@1 92.773 (92.773)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 06:19:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.802 Acc@5 97.952
[2024-08-07 06:19:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 06:19:20 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 06:19:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:50:25 lr 0.000001	 wd 0.0500	time 12.7201 (12.7201)	loss 1.2697 (1.2697)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-07 06:21:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:40:34 lr 0.000001	 wd 0.0500	time 0.8844 (1.0135)	loss 1.1517 (1.0829)	grad_norm 2.3298 (2.9787)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-07 06:22:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:36:41 lr 0.000001	 wd 0.0500	time 0.8388 (0.9565)	loss 1.1575 (1.0882)	grad_norm 2.4591 (3.0430)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-07 06:24:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:34:24 lr 0.000001	 wd 0.0500	time 0.8360 (0.9374)	loss 1.2124 (1.0844)	grad_norm 2.2747 (3.0832)	loss_scale 4096.0000 (4096.0000)	mem 23350MB
[2024-08-07 06:25:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:32:31 lr 0.000001	 wd 0.0500	time 0.8486 (0.9284)	loss 0.8857 (1.0834)	grad_norm 3.3180 (nan)	loss_scale 2048.0000 (3830.4239)	mem 23350MB
[2024-08-07 06:27:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:30:48 lr 0.000001	 wd 0.0500	time 0.8576 (0.9234)	loss 0.7010 (1.0843)	grad_norm 3.0443 (nan)	loss_scale 2048.0000 (3474.6507)	mem 23350MB
[2024-08-07 06:28:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:29:08 lr 0.000001	 wd 0.0500	time 0.8481 (0.9194)	loss 1.3233 (1.0835)	grad_norm 2.1518 (nan)	loss_scale 2048.0000 (3237.2712)	mem 23350MB
[2024-08-07 06:30:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:27:31 lr 0.000001	 wd 0.0500	time 0.8541 (0.9166)	loss 0.7144 (1.0839)	grad_norm 2.4992 (nan)	loss_scale 2048.0000 (3067.6177)	mem 23350MB
[2024-08-07 06:31:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:25:56 lr 0.000001	 wd 0.0500	time 0.8475 (0.9142)	loss 1.0463 (1.0839)	grad_norm 2.3805 (nan)	loss_scale 2048.0000 (2940.3246)	mem 23350MB
[2024-08-07 06:33:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:24:22 lr 0.000001	 wd 0.0500	time 0.8409 (0.9126)	loss 1.2391 (1.0824)	grad_norm 2.2883 (nan)	loss_scale 2048.0000 (2841.2875)	mem 23350MB
[2024-08-07 06:34:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:22:48 lr 0.000001	 wd 0.0500	time 0.8803 (0.9114)	loss 0.8586 (1.0780)	grad_norm 2.4488 (nan)	loss_scale 2048.0000 (2762.0380)	mem 23350MB
[2024-08-07 06:36:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:21:16 lr 0.000001	 wd 0.0500	time 0.8580 (0.9103)	loss 1.0844 (1.0794)	grad_norm 2.4371 (nan)	loss_scale 2048.0000 (2697.1844)	mem 23350MB
[2024-08-07 06:37:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:19:43 lr 0.000001	 wd 0.0500	time 0.9609 (0.9092)	loss 0.7776 (1.0791)	grad_norm 2.3182 (nan)	loss_scale 2048.0000 (2643.1307)	mem 23350MB
[2024-08-07 06:39:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:18:12 lr 0.000001	 wd 0.0500	time 0.8601 (0.9085)	loss 0.7323 (1.0748)	grad_norm 4.0866 (nan)	loss_scale 2048.0000 (2597.3866)	mem 23350MB
[2024-08-07 06:40:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:16:40 lr 0.000001	 wd 0.0500	time 0.8416 (0.9080)	loss 0.8321 (1.0763)	grad_norm 2.6895 (nan)	loss_scale 2048.0000 (2558.1727)	mem 23350MB
[2024-08-07 06:42:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:15:09 lr 0.000001	 wd 0.0500	time 0.8477 (0.9075)	loss 1.3340 (1.0776)	grad_norm 2.9752 (nan)	loss_scale 2048.0000 (2524.1839)	mem 23350MB
[2024-08-07 06:43:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:13:38 lr 0.000001	 wd 0.0500	time 0.8360 (0.9071)	loss 1.4533 (1.0788)	grad_norm 2.5578 (nan)	loss_scale 2048.0000 (2494.4410)	mem 23350MB
[2024-08-07 06:45:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:12:07 lr 0.000001	 wd 0.0500	time 0.9013 (0.9067)	loss 1.2594 (1.0798)	grad_norm 2.3825 (nan)	loss_scale 2048.0000 (2468.1952)	mem 23350MB
[2024-08-07 06:46:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:10:36 lr 0.000001	 wd 0.0500	time 0.7867 (0.9063)	loss 1.2251 (1.0791)	grad_norm 4.5922 (nan)	loss_scale 2048.0000 (2444.8640)	mem 23350MB
[2024-08-07 06:48:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:09:05 lr 0.000001	 wd 0.0500	time 0.8050 (0.9060)	loss 1.2538 (1.0796)	grad_norm 2.4974 (nan)	loss_scale 2048.0000 (2423.9874)	mem 23350MB
[2024-08-07 06:49:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:07:34 lr 0.000001	 wd 0.0500	time 0.8424 (0.9056)	loss 1.2832 (1.0801)	grad_norm 2.0845 (nan)	loss_scale 2048.0000 (2405.1974)	mem 23350MB
[2024-08-07 06:51:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:06:03 lr 0.000001	 wd 0.0500	time 0.8482 (0.9051)	loss 1.3664 (1.0795)	grad_norm 3.1963 (nan)	loss_scale 2048.0000 (2388.1961)	mem 23350MB
[2024-08-07 06:52:32 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:04:33 lr 0.000001	 wd 0.0500	time 0.8424 (0.9050)	loss 1.1424 (1.0801)	grad_norm 3.0795 (nan)	loss_scale 2048.0000 (2372.7397)	mem 23350MB
[2024-08-07 06:54:02 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:03:02 lr 0.000001	 wd 0.0500	time 0.8504 (0.9048)	loss 1.2663 (1.0802)	grad_norm 2.2093 (nan)	loss_scale 2048.0000 (2358.6267)	mem 23350MB
[2024-08-07 06:55:33 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:01:32 lr 0.000001	 wd 0.0500	time 0.8519 (0.9049)	loss 1.2632 (1.0815)	grad_norm 3.3704 (nan)	loss_scale 2048.0000 (2345.6893)	mem 23350MB
[2024-08-07 06:57:03 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.8472 (0.9045)	loss 1.0781 (1.0805)	grad_norm 21.9791 (nan)	loss_scale 2048.0000 (2333.7865)	mem 23350MB
[2024-08-07 06:57:10 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 27 training takes 0:37:49
[2024-08-07 06:57:22 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.956 (11.956)	Loss 0.4473 (0.4473)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 06:57:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.800 Acc@5 97.962
[2024-08-07 06:57:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 06:57:49 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 06:58:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][0/2502]	eta 8:10:42 lr 0.000001	 wd 0.0500	time 11.7675 (11.7675)	loss 0.7223 (0.7223)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 06:59:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:40:20 lr 0.000000	 wd 0.0500	time 0.8394 (1.0076)	loss 0.6942 (1.1026)	grad_norm 2.5766 (3.1563)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:01:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:36:35 lr 0.000000	 wd 0.0500	time 0.8512 (0.9538)	loss 1.3060 (1.0979)	grad_norm 2.5736 (2.9413)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:02:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:34:19 lr 0.000000	 wd 0.0500	time 0.8390 (0.9354)	loss 1.3292 (1.0896)	grad_norm 3.8467 (2.9277)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:04:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:32:28 lr 0.000000	 wd 0.0500	time 0.8518 (0.9271)	loss 0.6994 (1.0813)	grad_norm 2.1288 (2.9468)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:05:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:30:44 lr 0.000000	 wd 0.0500	time 0.8728 (0.9214)	loss 0.7614 (1.0856)	grad_norm 2.3206 (2.9411)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:07:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:29:04 lr 0.000000	 wd 0.0500	time 0.8395 (0.9174)	loss 0.8367 (1.0782)	grad_norm 4.5148 (3.0678)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:08:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:27:29 lr 0.000000	 wd 0.0500	time 0.8530 (0.9152)	loss 1.1726 (1.0751)	grad_norm 2.1528 (3.1354)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:10:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:25:54 lr 0.000000	 wd 0.0500	time 0.8410 (0.9136)	loss 0.7601 (1.0766)	grad_norm 3.4677 (3.1334)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:11:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:24:21 lr 0.000000	 wd 0.0500	time 0.8389 (0.9120)	loss 0.6553 (1.0731)	grad_norm 3.7825 (3.0796)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:13:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:22:48 lr 0.000000	 wd 0.0500	time 0.8418 (0.9108)	loss 0.9551 (1.0741)	grad_norm 2.3135 (3.0746)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:14:31 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:21:15 lr 0.000000	 wd 0.0500	time 0.8533 (0.9099)	loss 1.1006 (1.0752)	grad_norm 2.4063 (3.0662)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:16:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:19:43 lr 0.000000	 wd 0.0500	time 0.8492 (0.9088)	loss 1.2871 (1.0758)	grad_norm 2.6338 (3.0913)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:17:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:18:11 lr 0.000000	 wd 0.0500	time 0.7864 (0.9081)	loss 1.2742 (1.0775)	grad_norm 1.9181 (3.0526)	loss_scale 2048.0000 (2048.0000)	mem 23350MB
[2024-08-07 07:19:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:16:40 lr 0.000000	 wd 0.0500	time 0.8613 (0.9075)	loss 0.8398 (1.0763)	grad_norm 1.8420 (nan)	loss_scale 1024.0000 (2005.6074)	mem 23350MB
[2024-08-07 07:20:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:15:08 lr 0.000000	 wd 0.0500	time 0.8944 (0.9069)	loss 1.1558 (1.0762)	grad_norm 2.1663 (nan)	loss_scale 1024.0000 (1940.2105)	mem 23350MB
[2024-08-07 07:22:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:13:37 lr 0.000000	 wd 0.0500	time 0.8451 (0.9067)	loss 1.3259 (1.0785)	grad_norm 2.3273 (nan)	loss_scale 1024.0000 (1882.9831)	mem 23350MB
[2024-08-07 07:23:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:12:06 lr 0.000000	 wd 0.0500	time 0.8456 (0.9062)	loss 1.2941 (1.0783)	grad_norm 2.4547 (nan)	loss_scale 1024.0000 (1832.4844)	mem 23350MB
[2024-08-07 07:25:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:10:35 lr 0.000000	 wd 0.0500	time 0.8377 (0.9058)	loss 0.6606 (1.0780)	grad_norm 2.4655 (nan)	loss_scale 512.0000 (1773.9478)	mem 23350MB
[2024-08-07 07:26:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:09:05 lr 0.000000	 wd 0.0500	time 0.8449 (0.9055)	loss 0.9316 (1.0755)	grad_norm 2.9467 (nan)	loss_scale 512.0000 (1707.5644)	mem 23350MB
[2024-08-07 07:28:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:07:34 lr 0.000000	 wd 0.0500	time 0.7862 (0.9052)	loss 1.0724 (1.0756)	grad_norm 2.9918 (nan)	loss_scale 512.0000 (1647.8161)	mem 23350MB
[2024-08-07 07:29:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:06:03 lr 0.000000	 wd 0.0500	time 0.8470 (0.9050)	loss 0.9536 (1.0762)	grad_norm 3.1998 (nan)	loss_scale 512.0000 (1593.7554)	mem 23350MB
[2024-08-07 07:31:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:04:33 lr 0.000000	 wd 0.0500	time 0.8935 (0.9048)	loss 1.2318 (1.0756)	grad_norm 3.2299 (nan)	loss_scale 512.0000 (1544.6070)	mem 23350MB
[2024-08-07 07:32:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:03:02 lr 0.000000	 wd 0.0500	time 0.8405 (0.9044)	loss 1.2065 (1.0754)	grad_norm 3.0572 (nan)	loss_scale 512.0000 (1499.7306)	mem 23350MB
[2024-08-07 07:34:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:01:32 lr 0.000000	 wd 0.0500	time 0.8471 (0.9042)	loss 1.4280 (1.0745)	grad_norm 2.9986 (nan)	loss_scale 512.0000 (1458.5923)	mem 23350MB
[2024-08-07 07:35:30 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.7819 (0.9039)	loss 0.9984 (1.0736)	grad_norm 2.1742 (nan)	loss_scale 512.0000 (1420.7437)	mem 23350MB
[2024-08-07 07:35:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 28 training takes 0:37:48
[2024-08-07 07:35:50 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 12.035 (12.035)	Loss 0.4543 (0.4543)	Acc@1 92.773 (92.773)	Acc@5 98.828 (98.828)	Mem 23350MB
[2024-08-07 07:36:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.798 Acc@5 97.974
[2024-08-07 07:36:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 07:36:15 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 07:36:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:15:45 lr 0.000000	 wd 0.0500	time 11.8889 (11.8889)	loss 1.2300 (1.2300)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:37:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:40:23 lr 0.000000	 wd 0.0500	time 0.8559 (1.0089)	loss 1.2711 (1.0890)	grad_norm 2.5317 (2.9828)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:39:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:36:40 lr 0.000000	 wd 0.0500	time 0.8466 (0.9561)	loss 0.7490 (1.0899)	grad_norm 2.6028 (2.9743)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:40:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:34:22 lr 0.000000	 wd 0.0500	time 0.8776 (0.9367)	loss 0.6546 (1.0783)	grad_norm 2.3543 (2.9939)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:42:27 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:32:29 lr 0.000000	 wd 0.0500	time 0.8385 (0.9273)	loss 0.9891 (1.0742)	grad_norm 2.4946 (3.0538)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:43:57 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:30:45 lr 0.000000	 wd 0.0500	time 0.8411 (0.9216)	loss 1.1113 (1.0700)	grad_norm 2.0947 (3.0506)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:45:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:29:04 lr 0.000000	 wd 0.0500	time 0.8388 (0.9174)	loss 0.6735 (1.0727)	grad_norm 2.6483 (3.1410)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:46:56 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:27:28 lr 0.000000	 wd 0.0500	time 0.8329 (0.9147)	loss 1.1605 (1.0725)	grad_norm 3.2320 (3.0945)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:48:26 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:25:52 lr 0.000000	 wd 0.0500	time 0.8491 (0.9123)	loss 0.8108 (1.0737)	grad_norm 2.2419 (3.0500)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:49:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:24:18 lr 0.000000	 wd 0.0500	time 0.8383 (0.9105)	loss 1.3556 (1.0741)	grad_norm 3.3485 (3.2239)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:51:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:22:45 lr 0.000000	 wd 0.0500	time 0.8265 (0.9092)	loss 1.2010 (1.0746)	grad_norm 3.2828 (3.5405)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:52:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:21:13 lr 0.000000	 wd 0.0500	time 0.8847 (0.9080)	loss 1.2184 (1.0723)	grad_norm 2.0484 (3.4938)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:54:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:19:41 lr 0.000000	 wd 0.0500	time 0.8488 (0.9076)	loss 0.9387 (1.0713)	grad_norm 2.3103 (3.4856)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:55:55 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:18:10 lr 0.000000	 wd 0.0500	time 0.8507 (0.9070)	loss 1.0395 (1.0749)	grad_norm 2.7399 (3.4728)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:57:25 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:16:38 lr 0.000000	 wd 0.0500	time 0.8283 (0.9063)	loss 1.0451 (1.0701)	grad_norm 14.7070 (3.4751)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 07:58:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:15:07 lr 0.000000	 wd 0.0500	time 0.8486 (0.9058)	loss 0.9561 (1.0693)	grad_norm 2.4428 (3.5143)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:00:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:13:36 lr 0.000000	 wd 0.0500	time 0.8569 (0.9052)	loss 0.7396 (1.0720)	grad_norm 2.3287 (3.5594)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:01:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:12:05 lr 0.000000	 wd 0.0500	time 0.8455 (0.9047)	loss 1.1342 (1.0743)	grad_norm 4.1023 (3.5317)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:03:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:10:34 lr 0.000000	 wd 0.0500	time 0.8447 (0.9045)	loss 1.2091 (1.0734)	grad_norm 3.4497 (3.5018)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:04:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:09:04 lr 0.000000	 wd 0.0500	time 0.8414 (0.9042)	loss 0.8226 (1.0735)	grad_norm 2.7401 (3.4687)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:06:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:07:33 lr 0.000000	 wd 0.0500	time 0.8520 (0.9041)	loss 0.8450 (1.0754)	grad_norm 2.1245 (3.5309)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:07:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:06:03 lr 0.000000	 wd 0.0500	time 0.9043 (0.9038)	loss 0.7811 (1.0766)	grad_norm 2.0748 (3.5002)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:09:24 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:04:32 lr 0.000000	 wd 0.0500	time 0.8978 (0.9037)	loss 0.8050 (1.0771)	grad_norm 2.5059 (3.4722)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:10:54 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:03:02 lr 0.000000	 wd 0.0500	time 0.8258 (0.9034)	loss 0.6766 (1.0742)	grad_norm 3.3767 (3.4402)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:12:23 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:01:32 lr 0.000000	 wd 0.0500	time 0.8406 (0.9032)	loss 1.1792 (1.0745)	grad_norm 2.9594 (3.4181)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:13:53 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.8484 (0.9029)	loss 1.1540 (1.0739)	grad_norm 3.1334 (3.4649)	loss_scale 512.0000 (512.0000)	mem 23350MB
[2024-08-07 08:14:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 249): INFO EPOCH 29 training takes 0:37:44
[2024-08-07 08:14:00 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_29.pth saving......
[2024-08-07 08:14:01 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft/diffusion_ft_smt_l_step_cross2/ckpt_epoch_29.pth saved !!!
[2024-08-07 08:14:13 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 289): INFO Test: [0/98]	Time 11.423 (11.423)	Loss 0.4456 (0.4456)	Acc@1 92.969 (92.969)	Acc@5 99.023 (99.023)	Mem 23350MB
[2024-08-07 08:14:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 296): INFO  * Acc@1 86.826 Acc@5 97.968
[2024-08-07 08:14:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-07 08:14:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 182): INFO Max accuracy: 86.89%
[2024-08-07 08:14:38 smt_diffusion_finetune_large_224_22kto1k_step_crosslayer2-full-ft] (main.py 189): INFO Training time 19:09:22
