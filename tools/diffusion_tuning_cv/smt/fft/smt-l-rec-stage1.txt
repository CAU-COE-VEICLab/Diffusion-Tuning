[2024-07-31 21:57:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/config.json
[2024-07-31 21:57:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_stage1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: stage1
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_step_stage1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-31 21:57:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_step_stage1.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_step_stage1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-31 21:57:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_stage1
[2024-07-31 21:57:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-31 21:57:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 113): INFO number of params: 5400424
[2024-07-31 21:57:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1, ignoring auto resume
[2024-07-31 21:57:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth for fine-tuning......
[2024-07-31 21:57:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-31 21:57:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth'
[2024-07-31 21:57:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 13.090 (13.090)	Loss 0.4961 (0.4961)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 2335MB
[2024-07-31 21:58:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.706 Acc@5 97.674
[2024-07-31 21:58:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 21:58:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 168): INFO Start training
[2024-07-31 21:58:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:59:07 lr 0.000000	 wd 0.0500	time 12.9285 (12.9285)	loss 1.5612 (1.5612)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 18344MB
[2024-07-31 21:59:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:23:11 lr 0.000000	 wd 0.0500	time 0.4445 (0.5792)	loss 1.3456 (1.2173)	grad_norm 1.8285 (nan)	loss_scale 8192.0000 (17357.3069)	mem 18344MB
[2024-07-31 21:59:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:19:52 lr 0.000000	 wd 0.0500	time 0.4455 (0.5181)	loss 1.0752 (1.2109)	grad_norm 2.0987 (nan)	loss_scale 8192.0000 (12797.4527)	mem 18344MB
[2024-07-31 22:00:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:18:16 lr 0.000000	 wd 0.0500	time 0.4493 (0.4977)	loss 0.8883 (1.1707)	grad_norm 2.2739 (nan)	loss_scale 8192.0000 (11267.4020)	mem 18344MB
[2024-07-31 22:01:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:17:05 lr 0.000001	 wd 0.0500	time 0.4500 (0.4877)	loss 1.1178 (1.1789)	grad_norm 2.2977 (nan)	loss_scale 8192.0000 (10500.4688)	mem 18344MB
[2024-07-31 22:02:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:16:04 lr 0.000001	 wd 0.0500	time 0.4496 (0.4818)	loss 1.1986 (1.1812)	grad_norm 1.3609 (nan)	loss_scale 8192.0000 (10039.6966)	mem 18344MB
[2024-07-31 22:02:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:15:09 lr 0.000001	 wd 0.0500	time 0.4479 (0.4779)	loss 1.3095 (1.1793)	grad_norm 1.8038 (nan)	loss_scale 8192.0000 (9732.2596)	mem 18344MB
[2024-07-31 22:03:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:14:16 lr 0.000001	 wd 0.0500	time 0.4478 (0.4752)	loss 1.3224 (1.1819)	grad_norm 3.7847 (nan)	loss_scale 4096.0000 (8939.9144)	mem 18344MB
[2024-07-31 22:04:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:13:25 lr 0.000001	 wd 0.0500	time 0.4489 (0.4733)	loss 1.1101 (1.1803)	grad_norm 1.6659 (nan)	loss_scale 4096.0000 (8335.1810)	mem 18344MB
[2024-07-31 22:05:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:12:35 lr 0.000001	 wd 0.0500	time 0.4507 (0.4719)	loss 1.3574 (1.1807)	grad_norm 2.1263 (nan)	loss_scale 4096.0000 (7864.6837)	mem 18344MB
[2024-07-31 22:06:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:11:47 lr 0.000002	 wd 0.0500	time 0.4520 (0.4707)	loss 1.4527 (1.1785)	grad_norm 1.8387 (nan)	loss_scale 4096.0000 (7488.1918)	mem 18344MB
[2024-07-31 22:06:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:10:58 lr 0.000002	 wd 0.0500	time 0.4495 (0.4698)	loss 1.3591 (1.1803)	grad_norm 2.4330 (nan)	loss_scale 4096.0000 (7180.0908)	mem 18344MB
[2024-07-31 22:07:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:10:10 lr 0.000002	 wd 0.0500	time 0.4517 (0.4691)	loss 1.0180 (1.1798)	grad_norm 1.6654 (nan)	loss_scale 4096.0000 (6923.2973)	mem 18344MB
[2024-07-31 22:08:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:09:23 lr 0.000002	 wd 0.0500	time 0.4526 (0.4684)	loss 1.2260 (1.1815)	grad_norm 1.9203 (nan)	loss_scale 4096.0000 (6705.9800)	mem 18344MB
[2024-07-31 22:09:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:08:35 lr 0.000002	 wd 0.0500	time 0.4538 (0.4679)	loss 1.4535 (1.1827)	grad_norm 2.0884 (nan)	loss_scale 4096.0000 (6519.6859)	mem 18344MB
[2024-07-31 22:09:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:07:48 lr 0.000002	 wd 0.0500	time 0.4509 (0.4674)	loss 1.0477 (1.1861)	grad_norm 1.9225 (nan)	loss_scale 4096.0000 (6358.2145)	mem 18344MB
[2024-07-31 22:10:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:07:01 lr 0.000003	 wd 0.0500	time 0.4513 (0.4670)	loss 1.2353 (1.1835)	grad_norm 1.5459 (nan)	loss_scale 4096.0000 (6216.9144)	mem 18344MB
[2024-07-31 22:11:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:06:14 lr 0.000003	 wd 0.0500	time 0.4554 (0.4666)	loss 1.0224 (1.1831)	grad_norm 3.0141 (nan)	loss_scale 4096.0000 (6092.2281)	mem 18344MB
[2024-07-31 22:12:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:05:27 lr 0.000003	 wd 0.0500	time 0.4528 (0.4663)	loss 1.1070 (1.1831)	grad_norm 1.5877 (nan)	loss_scale 4096.0000 (5981.3881)	mem 18344MB
[2024-07-31 22:12:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:04:40 lr 0.000003	 wd 0.0500	time 0.4506 (0.4660)	loss 1.5304 (1.1829)	grad_norm 1.9600 (nan)	loss_scale 4096.0000 (5882.2094)	mem 18344MB
[2024-07-31 22:13:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:03:53 lr 0.000003	 wd 0.0500	time 0.4518 (0.4658)	loss 0.8529 (1.1820)	grad_norm 1.4116 (nan)	loss_scale 4096.0000 (5792.9435)	mem 18344MB
[2024-07-31 22:14:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:03:07 lr 0.000003	 wd 0.0500	time 0.4509 (0.4655)	loss 1.0334 (1.1809)	grad_norm 1.7082 (nan)	loss_scale 4096.0000 (5712.1752)	mem 18344MB
[2024-07-31 22:15:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:02:20 lr 0.000004	 wd 0.0500	time 0.4520 (0.4653)	loss 1.5425 (1.1801)	grad_norm 1.8389 (nan)	loss_scale 4096.0000 (5638.7460)	mem 18344MB
[2024-07-31 22:16:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:33 lr 0.000004	 wd 0.0500	time 0.4519 (0.4651)	loss 1.4266 (1.1801)	grad_norm 3.3746 (nan)	loss_scale 4096.0000 (5571.6993)	mem 18344MB
[2024-07-31 22:16:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:47 lr 0.000004	 wd 0.0500	time 0.4549 (0.4649)	loss 1.3748 (1.1798)	grad_norm 1.4537 (nan)	loss_scale 4096.0000 (5510.2374)	mem 18344MB
[2024-07-31 22:17:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.4481 (0.4647)	loss 1.0302 (1.1782)	grad_norm 1.6706 (nan)	loss_scale 2048.0000 (5430.7621)	mem 18344MB
[2024-07-31 22:17:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 0 training takes 0:19:25
[2024-07-31 22:17:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_0.pth saving......
[2024-07-31 22:17:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_0.pth saved !!!
[2024-07-31 22:17:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.395 (11.395)	Loss 0.5327 (0.5327)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 18344MB
[2024-07-31 22:18:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.660 Acc@5 97.668
[2024-07-31 22:18:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 22:18:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.66%
[2024-07-31 22:18:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 22:18:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 22:18:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:59:20 lr 0.000004	 wd 0.0500	time 11.4951 (11.4951)	loss 1.3457 (1.3457)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:19:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:22:37 lr 0.000004	 wd 0.0500	time 0.4475 (0.5651)	loss 1.2785 (1.1650)	grad_norm 2.6306 (2.1637)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:19:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:19:36 lr 0.000004	 wd 0.0500	time 0.4479 (0.5112)	loss 0.8339 (1.1676)	grad_norm 1.4513 (1.9832)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:20:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:18:06 lr 0.000004	 wd 0.0500	time 0.4492 (0.4933)	loss 0.9223 (1.1727)	grad_norm 1.6712 (2.0527)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:21:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:16:58 lr 0.000005	 wd 0.0500	time 0.4491 (0.4845)	loss 0.9526 (1.1883)	grad_norm 1.3584 (2.0846)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:22:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:15:59 lr 0.000005	 wd 0.0500	time 0.4500 (0.4792)	loss 1.5556 (1.1912)	grad_norm 2.1115 (2.1222)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:22:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:15:05 lr 0.000005	 wd 0.0500	time 0.4519 (0.4758)	loss 1.4746 (1.1907)	grad_norm 1.5067 (2.1857)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:23:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:14:13 lr 0.000005	 wd 0.0500	time 0.4522 (0.4735)	loss 0.8719 (1.1871)	grad_norm 2.0891 (2.1754)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:24:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:13:23 lr 0.000005	 wd 0.0500	time 0.4493 (0.4719)	loss 1.5851 (1.1865)	grad_norm 1.9196 (2.2400)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:25:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:12:34 lr 0.000005	 wd 0.0500	time 0.4470 (0.4707)	loss 1.1783 (1.1846)	grad_norm 1.6042 (2.2081)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:26:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:11:45 lr 0.000006	 wd 0.0500	time 0.4507 (0.4696)	loss 1.4814 (1.1814)	grad_norm 2.0534 (2.1977)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:26:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:10:57 lr 0.000006	 wd 0.0500	time 0.4509 (0.4688)	loss 1.5357 (1.1800)	grad_norm 1.2514 (2.2184)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:27:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:10:09 lr 0.000006	 wd 0.0500	time 0.4547 (0.4682)	loss 0.9401 (1.1782)	grad_norm 1.6631 (2.1998)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:28:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:09:22 lr 0.000006	 wd 0.0500	time 0.4440 (0.4676)	loss 1.2432 (1.1795)	grad_norm 2.0728 (2.1798)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:29:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:08:34 lr 0.000006	 wd 0.0500	time 0.4502 (0.4671)	loss 0.8186 (1.1795)	grad_norm 1.5275 (2.1799)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:29:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:07:47 lr 0.000006	 wd 0.0500	time 0.4498 (0.4667)	loss 1.4952 (1.1797)	grad_norm 1.3597 (2.1788)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:30:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:07:00 lr 0.000007	 wd 0.0500	time 0.4523 (0.4663)	loss 1.3603 (1.1807)	grad_norm 2.1550 (2.1833)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:31:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:06:13 lr 0.000007	 wd 0.0500	time 0.4536 (0.4660)	loss 0.8455 (1.1815)	grad_norm 4.1578 (2.1766)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:32:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:05:26 lr 0.000007	 wd 0.0500	time 0.4530 (0.4657)	loss 1.6661 (1.1808)	grad_norm 2.2791 (2.1659)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:32:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:04:40 lr 0.000007	 wd 0.0500	time 0.4561 (0.4655)	loss 1.4892 (1.1827)	grad_norm 1.4071 (2.1731)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:33:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:03:53 lr 0.000007	 wd 0.0500	time 0.4501 (0.4652)	loss 1.2855 (1.1833)	grad_norm 1.8428 (2.1804)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:34:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:03:06 lr 0.000007	 wd 0.0500	time 0.4513 (0.4650)	loss 1.4227 (1.1855)	grad_norm 1.6800 (2.1866)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:35:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:02:20 lr 0.000008	 wd 0.0500	time 0.4541 (0.4649)	loss 1.1597 (1.1860)	grad_norm 2.3372 (2.1760)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:35:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:33 lr 0.000008	 wd 0.0500	time 0.4519 (0.4647)	loss 0.8478 (1.1876)	grad_norm 1.5848 (2.1901)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:36:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:47 lr 0.000008	 wd 0.0500	time 0.4513 (0.4645)	loss 1.2359 (1.1875)	grad_norm 1.6335 (2.1863)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:37:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.4507 (0.4644)	loss 0.9474 (1.1876)	grad_norm 1.8347 (2.1800)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:37:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 1 training takes 0:19:24
[2024-07-31 22:37:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.293 (12.293)	Loss 0.5264 (0.5264)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 18344MB
[2024-07-31 22:38:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.676 Acc@5 97.682
[2024-07-31 22:38:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 22:38:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-31 22:38:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 22:38:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 22:38:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:52:50 lr 0.000008	 wd 0.0500	time 11.3392 (11.3392)	loss 1.1692 (1.1692)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:39:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:22:34 lr 0.000008	 wd 0.0500	time 0.4466 (0.5639)	loss 1.5023 (1.1927)	grad_norm 1.4047 (2.2012)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:39:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:19:35 lr 0.000008	 wd 0.0500	time 0.4487 (0.5107)	loss 1.3141 (1.1820)	grad_norm 1.3495 (2.2065)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:40:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:18:05 lr 0.000008	 wd 0.0500	time 0.4376 (0.4930)	loss 1.2680 (1.1711)	grad_norm 3.9928 (2.1174)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:41:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:16:58 lr 0.000009	 wd 0.0500	time 0.4524 (0.4843)	loss 1.3993 (1.1748)	grad_norm 1.8188 (2.0327)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:42:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:15:59 lr 0.000009	 wd 0.0500	time 0.4464 (0.4793)	loss 1.3640 (1.1729)	grad_norm 1.4698 (2.0408)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:42:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:15:05 lr 0.000009	 wd 0.0500	time 0.4509 (0.4760)	loss 1.2802 (1.1719)	grad_norm 1.5550 (2.1626)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:43:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:14:13 lr 0.000009	 wd 0.0500	time 0.4431 (0.4737)	loss 1.3650 (1.1762)	grad_norm 4.3839 (2.1625)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:44:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:13:23 lr 0.000009	 wd 0.0500	time 0.4525 (0.4721)	loss 1.2610 (1.1794)	grad_norm 1.5167 (2.1581)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:45:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:12:34 lr 0.000009	 wd 0.0500	time 0.4479 (0.4708)	loss 0.8365 (1.1832)	grad_norm 1.8674 (2.1641)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:45:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:11:45 lr 0.000010	 wd 0.0500	time 0.4516 (0.4698)	loss 0.8899 (1.1807)	grad_norm 2.6583 (2.1408)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:46:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:10:57 lr 0.000010	 wd 0.0500	time 0.4405 (0.4690)	loss 1.4242 (1.1796)	grad_norm 1.7659 (2.1846)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:47:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:10:09 lr 0.000010	 wd 0.0500	time 0.4527 (0.4683)	loss 1.0373 (1.1803)	grad_norm 3.1033 (2.1996)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:48:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:09:22 lr 0.000010	 wd 0.0500	time 0.4529 (0.4677)	loss 1.4448 (1.1809)	grad_norm 1.3386 (2.1858)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:49:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:08:34 lr 0.000010	 wd 0.0500	time 0.4491 (0.4672)	loss 1.2489 (1.1824)	grad_norm 4.6172 (2.1974)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 22:49:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:07:47 lr 0.000010	 wd 0.0500	time 0.4499 (0.4668)	loss 1.4219 (1.1830)	grad_norm 2.1000 (2.1794)	loss_scale 4096.0000 (2091.6616)	mem 18344MB
[2024-07-31 22:50:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:07:00 lr 0.000011	 wd 0.0500	time 0.4534 (0.4665)	loss 0.9623 (1.1833)	grad_norm 1.6440 (2.1875)	loss_scale 4096.0000 (2216.8545)	mem 18344MB
[2024-07-31 22:51:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:06:13 lr 0.000011	 wd 0.0500	time 0.4524 (0.4662)	loss 1.2536 (1.1834)	grad_norm 1.6509 (2.1872)	loss_scale 4096.0000 (2327.3275)	mem 18344MB
[2024-07-31 22:52:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:05:27 lr 0.000011	 wd 0.0500	time 0.4513 (0.4659)	loss 1.1879 (1.1844)	grad_norm 1.9407 (2.1956)	loss_scale 4096.0000 (2425.5325)	mem 18344MB
[2024-07-31 22:52:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:04:40 lr 0.000011	 wd 0.0500	time 0.4529 (0.4656)	loss 1.3791 (1.1842)	grad_norm 2.4625 (2.2106)	loss_scale 4096.0000 (2513.4056)	mem 18344MB
[2024-07-31 22:53:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:03:53 lr 0.000011	 wd 0.0500	time 0.4523 (0.4654)	loss 1.0076 (1.1844)	grad_norm 1.5185 (2.1961)	loss_scale 4096.0000 (2592.4958)	mem 18344MB
[2024-07-31 22:54:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:03:07 lr 0.000011	 wd 0.0500	time 0.4511 (0.4652)	loss 1.0262 (1.1836)	grad_norm 2.4193 (2.1877)	loss_scale 4096.0000 (2664.0571)	mem 18344MB
[2024-07-31 22:55:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:02:20 lr 0.000012	 wd 0.0500	time 0.4516 (0.4650)	loss 1.3169 (1.1841)	grad_norm 5.6980 (2.1958)	loss_scale 4096.0000 (2729.1159)	mem 18344MB
[2024-07-31 22:55:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:33 lr 0.000012	 wd 0.0500	time 0.4536 (0.4648)	loss 1.1524 (1.1840)	grad_norm 1.2645 (2.1854)	loss_scale 4096.0000 (2788.5198)	mem 18344MB
[2024-07-31 22:56:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:47 lr 0.000012	 wd 0.0500	time 0.4517 (0.4647)	loss 0.8070 (1.1847)	grad_norm 1.3480 (2.1784)	loss_scale 4096.0000 (2842.9754)	mem 18344MB
[2024-07-31 22:57:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.4512 (0.4645)	loss 1.2419 (1.1846)	grad_norm 1.4081 (2.1758)	loss_scale 4096.0000 (2893.0764)	mem 18344MB
[2024-07-31 22:57:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 2 training takes 0:19:24
[2024-07-31 22:57:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.072 (12.072)	Loss 0.5063 (0.5063)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-07-31 22:58:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.750 Acc@5 97.666
[2024-07-31 22:58:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 22:58:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 22:58:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-07-31 22:58:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-07-31 22:58:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:40:00 lr 0.000012	 wd 0.0500	time 11.0315 (11.0315)	loss 0.7004 (0.7004)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 22:59:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:22:41 lr 0.000012	 wd 0.0500	time 0.4454 (0.5669)	loss 1.2962 (1.2237)	grad_norm 1.7183 (1.9486)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 22:59:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:19:38 lr 0.000012	 wd 0.0500	time 0.4499 (0.5122)	loss 1.5205 (1.1976)	grad_norm 3.0178 (2.0742)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:00:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:18:07 lr 0.000012	 wd 0.0500	time 0.4503 (0.4939)	loss 1.4757 (1.1981)	grad_norm 1.3522 (2.0971)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:01:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:16:59 lr 0.000013	 wd 0.0500	time 0.4472 (0.4850)	loss 1.4353 (1.1895)	grad_norm 1.5921 (2.1496)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:02:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:16:00 lr 0.000013	 wd 0.0500	time 0.4477 (0.4797)	loss 0.8362 (1.1893)	grad_norm 6.9842 (2.1428)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:02:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:15:06 lr 0.000013	 wd 0.0500	time 0.4504 (0.4763)	loss 1.0914 (1.1816)	grad_norm 1.6696 (2.1214)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:03:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:14:14 lr 0.000013	 wd 0.0500	time 0.4422 (0.4740)	loss 1.4024 (1.1794)	grad_norm 2.1289 (2.1145)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:04:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:13:23 lr 0.000013	 wd 0.0500	time 0.4481 (0.4723)	loss 0.9898 (1.1742)	grad_norm 1.6486 (2.0997)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:05:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:12:34 lr 0.000013	 wd 0.0500	time 0.4506 (0.4710)	loss 1.6042 (1.1770)	grad_norm 2.8554 (2.0846)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:05:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:11:45 lr 0.000014	 wd 0.0500	time 0.4520 (0.4700)	loss 1.2921 (1.1770)	grad_norm 1.1863 (2.0771)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:06:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:10:57 lr 0.000014	 wd 0.0500	time 0.4503 (0.4691)	loss 0.8442 (1.1783)	grad_norm 1.6595 (2.1066)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:07:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:10:09 lr 0.000014	 wd 0.0500	time 0.4398 (0.4684)	loss 1.2087 (1.1764)	grad_norm 1.9199 (2.1151)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:08:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:09:22 lr 0.000014	 wd 0.0500	time 0.4463 (0.4679)	loss 1.2755 (1.1763)	grad_norm 1.6374 (2.1085)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:09:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:08:35 lr 0.000014	 wd 0.0500	time 0.4538 (0.4674)	loss 1.2108 (1.1770)	grad_norm 1.1455 (2.0942)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:09:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:07:47 lr 0.000014	 wd 0.0500	time 0.4506 (0.4669)	loss 1.4573 (1.1755)	grad_norm 1.5414 (2.0826)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:10:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:07:00 lr 0.000015	 wd 0.0500	time 0.4501 (0.4666)	loss 0.7315 (1.1730)	grad_norm 2.3080 (2.0784)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:11:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:06:13 lr 0.000015	 wd 0.0500	time 0.4501 (0.4662)	loss 1.0268 (1.1758)	grad_norm 1.3123 (2.0851)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:12:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:05:27 lr 0.000015	 wd 0.0500	time 0.4525 (0.4659)	loss 1.2756 (1.1773)	grad_norm 2.1225 (2.0842)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:12:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:04:40 lr 0.000015	 wd 0.0500	time 0.4529 (0.4656)	loss 1.3979 (1.1758)	grad_norm 1.7629 (2.0808)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:13:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:03:53 lr 0.000015	 wd 0.0500	time 0.4516 (0.4654)	loss 1.2599 (1.1753)	grad_norm 1.3485 (2.0676)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:14:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:03:07 lr 0.000015	 wd 0.0500	time 0.4540 (0.4652)	loss 0.8262 (1.1753)	grad_norm 1.2941 (2.0714)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-07-31 23:15:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:20 lr 0.000016	 wd 0.0500	time 0.4548 (0.4650)	loss 0.8176 (1.1745)	grad_norm 1.4191 (nan)	loss_scale 2048.0000 (4064.3635)	mem 18344MB
[2024-07-31 23:15:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:33 lr 0.000016	 wd 0.0500	time 0.4543 (0.4648)	loss 0.9373 (1.1751)	grad_norm 1.7912 (nan)	loss_scale 2048.0000 (3976.7336)	mem 18344MB
[2024-07-31 23:16:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:47 lr 0.000016	 wd 0.0500	time 0.4484 (0.4646)	loss 1.3356 (1.1764)	grad_norm 1.9947 (nan)	loss_scale 2048.0000 (3896.4032)	mem 18344MB
[2024-07-31 23:17:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.4504 (0.4645)	loss 1.2549 (1.1765)	grad_norm 1.5403 (nan)	loss_scale 2048.0000 (3822.4966)	mem 18344MB
[2024-07-31 23:17:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 3 training takes 0:19:24
[2024-07-31 23:17:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.764 (11.764)	Loss 0.5190 (0.5190)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-07-31 23:18:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.746 Acc@5 97.704
[2024-07-31 23:18:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 23:18:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 23:18:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:54:36 lr 0.000016	 wd 0.0500	time 11.3814 (11.3814)	loss 1.2649 (1.2649)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:19:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:22:51 lr 0.000016	 wd 0.0500	time 0.4483 (0.5709)	loss 0.9568 (1.1937)	grad_norm 2.0783 (2.1368)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:19:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:19:43 lr 0.000016	 wd 0.0500	time 0.4478 (0.5143)	loss 1.0993 (1.1834)	grad_norm 1.7496 (1.9957)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:20:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:18:10 lr 0.000016	 wd 0.0500	time 0.4492 (0.4954)	loss 0.7571 (1.1793)	grad_norm 2.3797 (2.0386)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:21:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:17:01 lr 0.000017	 wd 0.0500	time 0.4467 (0.4861)	loss 1.3791 (1.1766)	grad_norm 1.5947 (2.1426)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:22:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:16:02 lr 0.000017	 wd 0.0500	time 0.4473 (0.4806)	loss 1.2336 (1.1766)	grad_norm 1.6797 (2.1326)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:22:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:15:07 lr 0.000017	 wd 0.0500	time 0.4504 (0.4770)	loss 1.0124 (1.1775)	grad_norm 1.4662 (2.0941)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:23:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:14:15 lr 0.000017	 wd 0.0500	time 0.4520 (0.4746)	loss 0.9839 (1.1784)	grad_norm 1.5994 (2.0585)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:24:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:13:24 lr 0.000017	 wd 0.0500	time 0.4486 (0.4728)	loss 0.7791 (1.1805)	grad_norm 1.9889 (2.0603)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:25:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:12:35 lr 0.000017	 wd 0.0500	time 0.4506 (0.4714)	loss 0.8927 (1.1814)	grad_norm 1.5369 (2.0646)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:25:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:11:46 lr 0.000018	 wd 0.0500	time 0.4487 (0.4703)	loss 1.4976 (1.1823)	grad_norm 1.4206 (2.0584)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:26:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:10:58 lr 0.000018	 wd 0.0500	time 0.4526 (0.4694)	loss 1.4701 (1.1837)	grad_norm 1.6233 (2.0611)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:27:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:10:10 lr 0.000018	 wd 0.0500	time 0.4510 (0.4687)	loss 1.2956 (1.1818)	grad_norm 1.6857 (2.0480)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:28:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:09:22 lr 0.000018	 wd 0.0500	time 0.4493 (0.4681)	loss 0.9569 (1.1811)	grad_norm 2.4507 (2.0516)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:28:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:08:35 lr 0.000018	 wd 0.0500	time 0.4528 (0.4675)	loss 1.4326 (1.1814)	grad_norm 2.3421 (2.0506)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:29:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:07:48 lr 0.000018	 wd 0.0500	time 0.4520 (0.4671)	loss 1.2125 (1.1815)	grad_norm 1.4209 (2.0458)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:30:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:07:00 lr 0.000019	 wd 0.0500	time 0.4528 (0.4667)	loss 1.3620 (1.1798)	grad_norm 1.2347 (2.0610)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:31:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:06:14 lr 0.000019	 wd 0.0500	time 0.4505 (0.4663)	loss 1.1699 (1.1799)	grad_norm 1.5714 (2.0783)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:32:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:05:27 lr 0.000019	 wd 0.0500	time 0.4512 (0.4660)	loss 1.5331 (1.1815)	grad_norm 1.5401 (2.0803)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:32:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:04:40 lr 0.000019	 wd 0.0500	time 0.4471 (0.4658)	loss 1.3717 (1.1818)	grad_norm 1.6295 (2.0967)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:33:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:03:53 lr 0.000019	 wd 0.0500	time 0.4489 (0.4655)	loss 0.8172 (1.1806)	grad_norm 1.8908 (2.0942)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:34:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:03:07 lr 0.000019	 wd 0.0500	time 0.4535 (0.4653)	loss 0.9344 (1.1794)	grad_norm 1.4277 (2.1043)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:35:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:02:20 lr 0.000020	 wd 0.0500	time 0.4492 (0.4651)	loss 0.9083 (1.1796)	grad_norm 2.6474 (2.0991)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:35:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:33 lr 0.000020	 wd 0.0500	time 0.4512 (0.4649)	loss 0.8245 (1.1799)	grad_norm 3.5906 (2.0993)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:36:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:47 lr 0.000020	 wd 0.0500	time 0.4500 (0.4648)	loss 0.7594 (1.1792)	grad_norm 1.7667 (2.1116)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:37:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.4514 (0.4646)	loss 1.1253 (1.1797)	grad_norm 1.7249 (2.1076)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:37:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 4 training takes 0:19:24
[2024-07-31 23:37:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.170 (12.170)	Loss 0.5068 (0.5068)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 18344MB
[2024-07-31 23:38:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.708 Acc@5 97.710
[2024-07-31 23:38:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 23:38:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 23:38:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:06:18 lr 0.000020	 wd 0.0500	time 11.6621 (11.6621)	loss 1.4091 (1.4091)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:39:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:22:49 lr 0.000020	 wd 0.0500	time 0.4449 (0.5700)	loss 1.0126 (1.2103)	grad_norm 1.6246 (2.2330)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:39:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:19:42 lr 0.000020	 wd 0.0500	time 0.4460 (0.5139)	loss 1.2281 (1.1747)	grad_norm 2.1475 (2.0330)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:40:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:18:09 lr 0.000020	 wd 0.0500	time 0.4490 (0.4950)	loss 0.8401 (1.1688)	grad_norm 2.4575 (2.1335)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:41:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:17:01 lr 0.000020	 wd 0.0500	time 0.4477 (0.4858)	loss 1.4930 (1.1764)	grad_norm 1.8105 (2.1196)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:42:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:16:01 lr 0.000020	 wd 0.0500	time 0.4466 (0.4804)	loss 1.0254 (1.1740)	grad_norm 2.0050 (2.2939)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:42:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:15:07 lr 0.000020	 wd 0.0500	time 0.4493 (0.4769)	loss 0.9096 (1.1748)	grad_norm 2.1939 (2.2517)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:43:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:14:15 lr 0.000020	 wd 0.0500	time 0.4514 (0.4745)	loss 0.9203 (1.1750)	grad_norm 1.4396 (2.2334)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:44:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:13:24 lr 0.000020	 wd 0.0500	time 0.4581 (0.4728)	loss 1.4861 (1.1728)	grad_norm 7.5456 (2.2222)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:45:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:12:35 lr 0.000020	 wd 0.0500	time 0.4516 (0.4714)	loss 1.0253 (1.1717)	grad_norm 1.6960 (2.2338)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:45:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:11:46 lr 0.000020	 wd 0.0500	time 0.4573 (0.4704)	loss 1.1751 (1.1706)	grad_norm 2.4285 (2.2177)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:46:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:10:58 lr 0.000020	 wd 0.0500	time 0.4492 (0.4695)	loss 1.1961 (1.1721)	grad_norm 1.9437 (2.2562)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:47:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:10:10 lr 0.000020	 wd 0.0500	time 0.4525 (0.4688)	loss 1.5048 (1.1713)	grad_norm 1.7931 (2.2314)	loss_scale 4096.0000 (2112.7993)	mem 18344MB
[2024-07-31 23:48:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:09:22 lr 0.000020	 wd 0.0500	time 0.4545 (0.4682)	loss 0.8218 (1.1724)	grad_norm 1.6312 (2.2010)	loss_scale 4096.0000 (2265.2360)	mem 18344MB
[2024-07-31 23:48:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:08:35 lr 0.000020	 wd 0.0500	time 0.4527 (0.4677)	loss 1.3266 (1.1762)	grad_norm 1.7191 (2.2084)	loss_scale 4096.0000 (2395.9115)	mem 18344MB
[2024-07-31 23:49:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:07:48 lr 0.000020	 wd 0.0500	time 0.4514 (0.4672)	loss 0.7823 (1.1779)	grad_norm 4.8228 (2.2054)	loss_scale 4096.0000 (2509.1752)	mem 18344MB
[2024-07-31 23:50:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:07:01 lr 0.000020	 wd 0.0500	time 0.4500 (0.4668)	loss 1.3870 (1.1811)	grad_norm 1.8084 (2.1926)	loss_scale 4096.0000 (2608.2898)	mem 18344MB
[2024-07-31 23:51:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:06:14 lr 0.000020	 wd 0.0500	time 0.4551 (0.4665)	loss 0.8373 (1.1814)	grad_norm 2.8585 (2.1969)	loss_scale 4096.0000 (2695.7507)	mem 18344MB
[2024-07-31 23:52:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:05:27 lr 0.000020	 wd 0.0500	time 0.4508 (0.4662)	loss 0.8292 (1.1789)	grad_norm 1.5498 (2.2050)	loss_scale 4096.0000 (2773.4992)	mem 18344MB
[2024-07-31 23:52:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:04:40 lr 0.000020	 wd 0.0500	time 0.4513 (0.4659)	loss 1.1497 (1.1783)	grad_norm 3.1804 (2.2260)	loss_scale 4096.0000 (2843.0679)	mem 18344MB
[2024-07-31 23:53:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:03:53 lr 0.000020	 wd 0.0500	time 0.4522 (0.4657)	loss 1.5910 (1.1794)	grad_norm 2.3343 (2.2223)	loss_scale 4096.0000 (2905.6832)	mem 18344MB
[2024-07-31 23:54:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:03:07 lr 0.000020	 wd 0.0500	time 0.4491 (0.4654)	loss 1.5810 (1.1794)	grad_norm 1.8683 (nan)	loss_scale 2048.0000 (2870.7092)	mem 18344MB
[2024-07-31 23:55:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:02:20 lr 0.000020	 wd 0.0500	time 0.4521 (0.4652)	loss 1.3080 (1.1799)	grad_norm 1.2452 (nan)	loss_scale 2048.0000 (2833.3303)	mem 18344MB
[2024-07-31 23:55:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:33 lr 0.000020	 wd 0.0500	time 0.4481 (0.4650)	loss 1.1543 (1.1785)	grad_norm 2.1179 (nan)	loss_scale 2048.0000 (2799.2003)	mem 18344MB
[2024-07-31 23:56:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:47 lr 0.000020	 wd 0.0500	time 0.4523 (0.4649)	loss 0.9197 (1.1780)	grad_norm 1.8135 (nan)	loss_scale 2048.0000 (2767.9134)	mem 18344MB
[2024-07-31 23:57:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.4500 (0.4647)	loss 1.4013 (1.1790)	grad_norm 1.5535 (nan)	loss_scale 2048.0000 (2739.1283)	mem 18344MB
[2024-07-31 23:57:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 5 training takes 0:19:25
[2024-07-31 23:57:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.388 (12.388)	Loss 0.5083 (0.5083)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-07-31 23:58:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.718 Acc@5 97.730
[2024-07-31 23:58:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 23:58:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.75%
[2024-07-31 23:58:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:10:08 lr 0.000020	 wd 0.0500	time 11.7542 (11.7542)	loss 1.2187 (1.2187)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:58:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:22:45 lr 0.000020	 wd 0.0500	time 0.4492 (0.5684)	loss 0.9737 (1.2077)	grad_norm 1.5551 (2.3505)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-07-31 23:59:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:19:40 lr 0.000020	 wd 0.0500	time 0.4491 (0.5129)	loss 0.9201 (1.1900)	grad_norm 1.8057 (2.1977)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:00:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:18:08 lr 0.000020	 wd 0.0500	time 0.4485 (0.4945)	loss 0.9681 (1.1865)	grad_norm 1.4905 (2.1386)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:01:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:17:00 lr 0.000020	 wd 0.0500	time 0.4512 (0.4853)	loss 0.6980 (1.1850)	grad_norm 2.0043 (2.1623)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:02:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:16:00 lr 0.000020	 wd 0.0500	time 0.4525 (0.4800)	loss 1.2990 (1.1861)	grad_norm 1.6671 (2.2273)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:02:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:15:06 lr 0.000020	 wd 0.0500	time 0.4527 (0.4766)	loss 0.8841 (1.1777)	grad_norm 1.5907 (2.1851)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:03:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:14:14 lr 0.000020	 wd 0.0500	time 0.4519 (0.4742)	loss 1.4138 (1.1801)	grad_norm 1.3644 (2.1312)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:04:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:13:24 lr 0.000020	 wd 0.0500	time 0.4527 (0.4725)	loss 0.7879 (1.1854)	grad_norm 3.3783 (2.1222)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:05:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:12:34 lr 0.000020	 wd 0.0500	time 0.4541 (0.4711)	loss 1.6363 (1.1813)	grad_norm 2.0970 (inf)	loss_scale 1024.0000 (1941.1676)	mem 18344MB
[2024-08-01 00:05:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:11:46 lr 0.000020	 wd 0.0500	time 0.4517 (0.4701)	loss 1.0483 (1.1810)	grad_norm 1.7391 (inf)	loss_scale 1024.0000 (1849.5425)	mem 18344MB
[2024-08-01 00:06:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:10:57 lr 0.000020	 wd 0.0500	time 0.4516 (0.4692)	loss 0.8153 (1.1764)	grad_norm 1.8525 (inf)	loss_scale 1024.0000 (1774.5613)	mem 18344MB
[2024-08-01 00:07:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:10:10 lr 0.000020	 wd 0.0500	time 0.4484 (0.4685)	loss 1.2968 (1.1732)	grad_norm 1.6239 (inf)	loss_scale 1024.0000 (1712.0666)	mem 18344MB
[2024-08-01 00:08:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:09:22 lr 0.000020	 wd 0.0500	time 0.4516 (0.4679)	loss 1.1417 (1.1720)	grad_norm 4.0500 (inf)	loss_scale 1024.0000 (1659.1791)	mem 18344MB
[2024-08-01 00:08:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:08:35 lr 0.000020	 wd 0.0500	time 0.4521 (0.4674)	loss 1.0195 (1.1714)	grad_norm 1.7098 (inf)	loss_scale 1024.0000 (1613.8415)	mem 18344MB
[2024-08-01 00:09:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:07:47 lr 0.000020	 wd 0.0500	time 0.4522 (0.4670)	loss 0.7050 (1.1713)	grad_norm 1.2922 (inf)	loss_scale 1024.0000 (1574.5450)	mem 18344MB
[2024-08-01 00:10:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:07:00 lr 0.000020	 wd 0.0500	time 0.4509 (0.4666)	loss 1.3807 (1.1691)	grad_norm 2.8712 (inf)	loss_scale 1024.0000 (1540.1574)	mem 18344MB
[2024-08-01 00:11:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:06:13 lr 0.000020	 wd 0.0500	time 0.4508 (0.4663)	loss 1.3989 (1.1672)	grad_norm 1.3621 (inf)	loss_scale 1024.0000 (1509.8131)	mem 18344MB
[2024-08-01 00:12:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:05:27 lr 0.000020	 wd 0.0500	time 0.4501 (0.4660)	loss 1.2553 (1.1675)	grad_norm 1.4339 (inf)	loss_scale 1024.0000 (1482.8384)	mem 18344MB
[2024-08-01 00:12:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:04:40 lr 0.000020	 wd 0.0500	time 0.4485 (0.4657)	loss 1.4306 (1.1696)	grad_norm 1.6814 (inf)	loss_scale 1024.0000 (1458.7017)	mem 18344MB
[2024-08-01 00:13:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:03:53 lr 0.000020	 wd 0.0500	time 0.4530 (0.4655)	loss 0.9533 (1.1704)	grad_norm 3.9039 (inf)	loss_scale 1024.0000 (1436.9775)	mem 18344MB
[2024-08-01 00:14:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:03:07 lr 0.000020	 wd 0.0500	time 0.4493 (0.4653)	loss 1.1198 (1.1708)	grad_norm 2.4605 (inf)	loss_scale 1024.0000 (1417.3213)	mem 18344MB
[2024-08-01 00:15:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:02:20 lr 0.000020	 wd 0.0500	time 0.4537 (0.4651)	loss 0.8225 (1.1702)	grad_norm 1.9567 (inf)	loss_scale 1024.0000 (1399.4512)	mem 18344MB
[2024-08-01 00:15:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:33 lr 0.000020	 wd 0.0500	time 0.4544 (0.4649)	loss 0.9826 (1.1724)	grad_norm 1.8479 (inf)	loss_scale 1024.0000 (1383.1343)	mem 18344MB
[2024-08-01 00:16:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:47 lr 0.000020	 wd 0.0500	time 0.4507 (0.4647)	loss 1.4282 (1.1712)	grad_norm 1.6498 (inf)	loss_scale 1024.0000 (1368.1766)	mem 18344MB
[2024-08-01 00:17:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.4504 (0.4645)	loss 1.4281 (1.1707)	grad_norm 1.5990 (inf)	loss_scale 1024.0000 (1354.4150)	mem 18344MB
[2024-08-01 00:17:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 6 training takes 0:19:24
[2024-08-01 00:17:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 13.000 (13.000)	Loss 0.5190 (0.5190)	Acc@1 93.359 (93.359)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 00:17:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.772 Acc@5 97.714
[2024-08-01 00:17:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 00:17:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.77%
[2024-08-01 00:17:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 00:18:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 00:18:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:11:36 lr 0.000020	 wd 0.0500	time 11.7891 (11.7891)	loss 0.9452 (0.9452)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:18:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:22:48 lr 0.000020	 wd 0.0500	time 0.4535 (0.5697)	loss 1.2525 (1.1535)	grad_norm 1.5439 (2.0892)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:19:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:19:42 lr 0.000020	 wd 0.0500	time 0.4495 (0.5136)	loss 1.1387 (1.1740)	grad_norm 1.4572 (1.9745)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:20:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:18:09 lr 0.000020	 wd 0.0500	time 0.4484 (0.4948)	loss 1.4936 (1.1736)	grad_norm 2.0543 (2.0227)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:21:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:17:00 lr 0.000020	 wd 0.0500	time 0.4506 (0.4856)	loss 0.8661 (1.1789)	grad_norm 1.4072 (2.2216)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:22:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:16:01 lr 0.000020	 wd 0.0500	time 0.4394 (0.4802)	loss 1.4249 (1.1767)	grad_norm 1.3517 (2.1775)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:22:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:15:06 lr 0.000020	 wd 0.0500	time 0.4497 (0.4767)	loss 1.2754 (1.1814)	grad_norm 1.8631 (2.2600)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:23:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:14:14 lr 0.000020	 wd 0.0500	time 0.4505 (0.4744)	loss 1.0937 (1.1759)	grad_norm 1.3192 (2.2986)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:24:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:13:24 lr 0.000020	 wd 0.0500	time 0.4512 (0.4726)	loss 0.9151 (1.1735)	grad_norm 1.6126 (2.2804)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:25:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:12:35 lr 0.000020	 wd 0.0500	time 0.4508 (0.4713)	loss 1.5215 (1.1766)	grad_norm 1.3703 (2.3435)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:25:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:11:46 lr 0.000020	 wd 0.0500	time 0.4511 (0.4702)	loss 0.8369 (1.1765)	grad_norm 1.7105 (2.3300)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:26:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:10:58 lr 0.000020	 wd 0.0500	time 0.4520 (0.4694)	loss 1.2277 (1.1744)	grad_norm 2.4483 (2.3606)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:27:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:10:10 lr 0.000020	 wd 0.0500	time 0.4513 (0.4687)	loss 1.3174 (1.1708)	grad_norm 2.6399 (2.3237)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:28:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:09:22 lr 0.000020	 wd 0.0500	time 0.4514 (0.4681)	loss 1.3054 (1.1707)	grad_norm 1.6689 (2.3095)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:28:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:08:35 lr 0.000019	 wd 0.0500	time 0.4511 (0.4675)	loss 1.2553 (1.1720)	grad_norm 1.5419 (2.3078)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:29:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:07:48 lr 0.000019	 wd 0.0500	time 0.4525 (0.4671)	loss 0.8113 (1.1729)	grad_norm 1.3083 (2.2810)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:30:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:07:00 lr 0.000019	 wd 0.0500	time 0.4511 (0.4667)	loss 1.2207 (1.1736)	grad_norm 1.6163 (2.2745)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:31:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:06:13 lr 0.000019	 wd 0.0500	time 0.4524 (0.4663)	loss 1.2435 (1.1736)	grad_norm 2.4544 (2.2522)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:31:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:05:27 lr 0.000019	 wd 0.0500	time 0.4542 (0.4660)	loss 0.7565 (1.1757)	grad_norm 2.9014 (2.2387)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:32:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:04:40 lr 0.000019	 wd 0.0500	time 0.4504 (0.4657)	loss 1.1956 (1.1763)	grad_norm 1.5109 (2.2262)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:33:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:03:53 lr 0.000019	 wd 0.0500	time 0.4532 (0.4655)	loss 1.3119 (1.1769)	grad_norm 1.4737 (2.2472)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:34:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:03:07 lr 0.000019	 wd 0.0500	time 0.4526 (0.4652)	loss 1.5530 (1.1794)	grad_norm 1.9918 (2.2495)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:35:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:02:20 lr 0.000019	 wd 0.0500	time 0.4541 (0.4650)	loss 1.2844 (1.1773)	grad_norm 1.9323 (2.2493)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:35:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:33 lr 0.000019	 wd 0.0500	time 0.4492 (0.4649)	loss 1.4895 (1.1767)	grad_norm 1.4549 (2.2596)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:36:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:47 lr 0.000019	 wd 0.0500	time 0.4492 (0.4647)	loss 1.3510 (1.1766)	grad_norm 1.5278 (2.2534)	loss_scale 2048.0000 (1064.9429)	mem 18344MB
[2024-08-01 00:37:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.4510 (0.4645)	loss 1.5286 (1.1759)	grad_norm 1.4867 (2.2409)	loss_scale 2048.0000 (1104.2495)	mem 18344MB
[2024-08-01 00:37:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 7 training takes 0:19:24
[2024-08-01 00:37:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.451 (12.451)	Loss 0.4927 (0.4927)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-08-01 00:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.746 Acc@5 97.720
[2024-08-01 00:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-01 00:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.77%
[2024-08-01 00:38:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:56:56 lr 0.000019	 wd 0.0500	time 12.8762 (12.8762)	loss 1.2595 (1.2595)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:38:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:23:13 lr 0.000019	 wd 0.0500	time 0.4476 (0.5803)	loss 0.8985 (1.2363)	grad_norm 1.7880 (2.0671)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:39:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:19:54 lr 0.000019	 wd 0.0500	time 0.4462 (0.5188)	loss 0.8821 (1.2089)	grad_norm 3.9653 (2.1282)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:40:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:18:17 lr 0.000019	 wd 0.0500	time 0.4383 (0.4983)	loss 1.4723 (1.1934)	grad_norm 2.4310 (2.1270)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:41:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:17:06 lr 0.000019	 wd 0.0500	time 0.4488 (0.4882)	loss 0.9053 (1.1903)	grad_norm 1.5930 (2.1388)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:42:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:16:05 lr 0.000019	 wd 0.0500	time 0.4495 (0.4823)	loss 0.8269 (1.1805)	grad_norm 1.6329 (2.1240)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:42:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:15:10 lr 0.000019	 wd 0.0500	time 0.4481 (0.4785)	loss 0.8597 (1.1802)	grad_norm 1.1828 (2.1173)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:43:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:14:17 lr 0.000019	 wd 0.0500	time 0.4503 (0.4758)	loss 1.3419 (1.1798)	grad_norm 2.0957 (2.1051)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:44:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:13:26 lr 0.000019	 wd 0.0500	time 0.4503 (0.4739)	loss 1.3282 (1.1821)	grad_norm 1.3304 (2.0984)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:45:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:12:36 lr 0.000019	 wd 0.0500	time 0.4509 (0.4724)	loss 0.9457 (1.1787)	grad_norm 3.0607 (2.1137)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:45:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:11:47 lr 0.000019	 wd 0.0500	time 0.4491 (0.4713)	loss 1.3649 (1.1807)	grad_norm 2.2860 (2.1046)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:46:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:10:59 lr 0.000019	 wd 0.0500	time 0.4482 (0.4703)	loss 1.3228 (1.1778)	grad_norm 1.5946 (2.1259)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:47:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:10:11 lr 0.000019	 wd 0.0500	time 0.4418 (0.4695)	loss 1.0793 (1.1785)	grad_norm 1.5048 (2.1206)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:48:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:09:23 lr 0.000019	 wd 0.0500	time 0.4409 (0.4689)	loss 0.8308 (1.1785)	grad_norm 2.1088 (2.1943)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:48:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:08:36 lr 0.000019	 wd 0.0500	time 0.4463 (0.4683)	loss 1.2969 (1.1796)	grad_norm 1.6955 (2.1686)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:49:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:07:48 lr 0.000019	 wd 0.0500	time 0.4497 (0.4678)	loss 1.1264 (1.1803)	grad_norm 1.5629 (2.1818)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:50:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:07:01 lr 0.000019	 wd 0.0500	time 0.4524 (0.4674)	loss 1.4123 (1.1822)	grad_norm 1.9527 (2.1808)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:51:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:06:14 lr 0.000019	 wd 0.0500	time 0.4527 (0.4670)	loss 0.7979 (1.1846)	grad_norm 1.7939 (2.1934)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 00:51:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:05:27 lr 0.000019	 wd 0.0500	time 0.4485 (0.4667)	loss 0.8114 (1.1831)	grad_norm 2.0134 (inf)	loss_scale 1024.0000 (2017.2971)	mem 18344MB
[2024-08-01 00:52:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:04:40 lr 0.000019	 wd 0.0500	time 0.4505 (0.4664)	loss 1.2799 (1.1849)	grad_norm 1.8138 (inf)	loss_scale 1024.0000 (1965.0458)	mem 18344MB
[2024-08-01 00:53:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:03:53 lr 0.000019	 wd 0.0500	time 0.4521 (0.4661)	loss 0.7685 (1.1836)	grad_norm 1.4569 (inf)	loss_scale 1024.0000 (1918.0170)	mem 18344MB
[2024-08-01 00:54:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:03:07 lr 0.000019	 wd 0.0500	time 0.4485 (0.4658)	loss 0.8937 (1.1844)	grad_norm 1.9032 (inf)	loss_scale 1024.0000 (1875.4650)	mem 18344MB
[2024-08-01 00:55:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:02:20 lr 0.000019	 wd 0.0500	time 0.4503 (0.4656)	loss 1.2227 (1.1830)	grad_norm 1.6760 (inf)	loss_scale 1024.0000 (1836.7796)	mem 18344MB
[2024-08-01 00:55:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:34 lr 0.000019	 wd 0.0500	time 0.4521 (0.4654)	loss 1.2812 (1.1813)	grad_norm 2.5681 (inf)	loss_scale 1024.0000 (1801.4568)	mem 18344MB
[2024-08-01 00:56:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:47 lr 0.000019	 wd 0.0500	time 0.4495 (0.4652)	loss 1.1013 (1.1812)	grad_norm 2.5945 (inf)	loss_scale 1024.0000 (1769.0762)	mem 18344MB
[2024-08-01 00:57:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.4525 (0.4650)	loss 0.9235 (1.1808)	grad_norm 1.8574 (inf)	loss_scale 1024.0000 (1739.2851)	mem 18344MB
[2024-08-01 00:57:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 8 training takes 0:19:25
[2024-08-01 00:57:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.881 (11.881)	Loss 0.4917 (0.4917)	Acc@1 93.555 (93.555)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 00:57:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.778 Acc@5 97.728
[2024-08-01 00:57:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 00:57:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.78%
[2024-08-01 00:57:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 00:57:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 00:58:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:50:36 lr 0.000019	 wd 0.0500	time 11.2858 (11.2858)	loss 1.3662 (1.3662)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:58:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:22:32 lr 0.000019	 wd 0.0500	time 0.4482 (0.5632)	loss 1.2699 (1.1687)	grad_norm 2.0868 (3.2767)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 00:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:19:34 lr 0.000019	 wd 0.0500	time 0.4482 (0.5103)	loss 0.9896 (1.1910)	grad_norm 1.6618 (2.6142)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:00:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:18:04 lr 0.000019	 wd 0.0500	time 0.4421 (0.4926)	loss 1.4358 (1.1960)	grad_norm 1.6276 (2.5903)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:01:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:16:57 lr 0.000019	 wd 0.0500	time 0.4503 (0.4839)	loss 0.9296 (1.1761)	grad_norm 1.4273 (2.4419)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:01:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:15:58 lr 0.000019	 wd 0.0500	time 0.4507 (0.4788)	loss 0.7133 (1.1715)	grad_norm 1.8834 (2.3317)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:02:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:15:04 lr 0.000019	 wd 0.0500	time 0.4490 (0.4755)	loss 0.7909 (1.1742)	grad_norm 1.9743 (2.3247)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:03:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:14:12 lr 0.000019	 wd 0.0500	time 0.4522 (0.4733)	loss 1.4442 (1.1768)	grad_norm 1.1223 (2.3444)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:04:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:13:22 lr 0.000019	 wd 0.0500	time 0.4499 (0.4717)	loss 1.4268 (1.1787)	grad_norm 1.6104 (2.3075)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:05:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:12:33 lr 0.000019	 wd 0.0500	time 0.4408 (0.4704)	loss 1.3850 (1.1787)	grad_norm 1.7857 (2.2686)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:05:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:11:45 lr 0.000019	 wd 0.0500	time 0.4521 (0.4694)	loss 1.2937 (1.1793)	grad_norm 2.5711 (2.2475)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:06:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:10:56 lr 0.000018	 wd 0.0500	time 0.4522 (0.4686)	loss 1.4224 (1.1750)	grad_norm 2.7872 (2.2237)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:07:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:10:09 lr 0.000018	 wd 0.0500	time 0.4511 (0.4680)	loss 0.9756 (1.1753)	grad_norm 1.3998 (2.2279)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:08:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:09:21 lr 0.000018	 wd 0.0500	time 0.4497 (0.4674)	loss 1.3598 (1.1763)	grad_norm 1.5234 (2.2065)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:08:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:08:34 lr 0.000018	 wd 0.0500	time 0.4515 (0.4670)	loss 0.9421 (1.1744)	grad_norm 1.6094 (2.2376)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:09:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:07:47 lr 0.000018	 wd 0.0500	time 0.4481 (0.4666)	loss 0.9274 (1.1771)	grad_norm 1.9946 (2.2741)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:10:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:07:00 lr 0.000018	 wd 0.0500	time 0.4521 (0.4662)	loss 0.9440 (1.1751)	grad_norm 1.6598 (2.2720)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:11:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:06:13 lr 0.000018	 wd 0.0500	time 0.4500 (0.4659)	loss 1.4435 (1.1753)	grad_norm 1.4893 (2.2755)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:11:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:05:26 lr 0.000018	 wd 0.0500	time 0.4510 (0.4656)	loss 1.3097 (1.1747)	grad_norm 2.5608 (2.2973)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:12:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:04:40 lr 0.000018	 wd 0.0500	time 0.4502 (0.4654)	loss 1.4190 (1.1736)	grad_norm 1.8141 (2.3072)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:13:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:03:53 lr 0.000018	 wd 0.0500	time 0.4516 (0.4651)	loss 1.2401 (1.1740)	grad_norm 1.4244 (2.2959)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:14:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:03:06 lr 0.000018	 wd 0.0500	time 0.4551 (0.4650)	loss 1.4035 (1.1751)	grad_norm 2.6081 (2.2851)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:15:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:20 lr 0.000018	 wd 0.0500	time 0.4491 (0.4648)	loss 1.2476 (1.1761)	grad_norm 3.9364 (2.2797)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:15:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:33 lr 0.000018	 wd 0.0500	time 0.4416 (0.4646)	loss 0.8248 (1.1761)	grad_norm 2.1615 (2.2758)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:16:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:47 lr 0.000018	 wd 0.0500	time 0.4516 (0.4644)	loss 1.1442 (1.1765)	grad_norm 1.9137 (2.2796)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:17:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.4510 (0.4643)	loss 1.4102 (1.1760)	grad_norm 1.7421 (2.2767)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:17:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 9 training takes 0:19:23
[2024-08-01 01:17:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.872 (11.872)	Loss 0.5073 (0.5073)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 01:17:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.782 Acc@5 97.720
[2024-08-01 01:17:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 01:17:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.78%
[2024-08-01 01:17:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 01:17:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 01:18:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:46:43 lr 0.000018	 wd 0.0500	time 11.1924 (11.1924)	loss 1.4379 (1.4379)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:18:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:22:31 lr 0.000018	 wd 0.0500	time 0.4464 (0.5625)	loss 0.8128 (1.1831)	grad_norm 3.1435 (2.2974)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:19:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:19:33 lr 0.000018	 wd 0.0500	time 0.4483 (0.5100)	loss 1.3829 (1.1731)	grad_norm 1.6693 (2.3978)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:20:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:18:04 lr 0.000018	 wd 0.0500	time 0.4473 (0.4925)	loss 1.3648 (1.1733)	grad_norm 5.1942 (2.3593)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:21:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:16:57 lr 0.000018	 wd 0.0500	time 0.4492 (0.4839)	loss 0.7832 (1.1813)	grad_norm 5.0737 (2.3432)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:21:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:15:58 lr 0.000018	 wd 0.0500	time 0.4506 (0.4789)	loss 0.7451 (1.1784)	grad_norm 1.4350 (2.3925)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:22:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:15:04 lr 0.000018	 wd 0.0500	time 0.4505 (0.4757)	loss 1.2493 (1.1847)	grad_norm 1.6015 (2.3060)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:23:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:14:13 lr 0.000018	 wd 0.0500	time 0.4487 (0.4735)	loss 1.2879 (1.1819)	grad_norm 1.9592 (2.2572)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 01:24:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:13:23 lr 0.000018	 wd 0.0500	time 0.4511 (0.4718)	loss 1.2949 (1.1847)	grad_norm 1.5853 (2.2258)	loss_scale 2048.0000 (1098.1473)	mem 18344MB
[2024-08-01 01:25:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:12:33 lr 0.000018	 wd 0.0500	time 0.4533 (0.4706)	loss 1.3519 (1.1831)	grad_norm 1.7590 (2.2275)	loss_scale 2048.0000 (1203.5694)	mem 18344MB
[2024-08-01 01:25:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:11:45 lr 0.000018	 wd 0.0500	time 0.4502 (0.4696)	loss 1.3109 (1.1839)	grad_norm 1.5211 (2.2111)	loss_scale 2048.0000 (1287.9281)	mem 18344MB
[2024-08-01 01:26:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:10:57 lr 0.000018	 wd 0.0500	time 0.4512 (0.4688)	loss 1.0653 (1.1914)	grad_norm 3.1319 (2.2111)	loss_scale 2048.0000 (1356.9628)	mem 18344MB
[2024-08-01 01:27:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:10:09 lr 0.000018	 wd 0.0500	time 0.4420 (0.4682)	loss 1.4581 (1.1882)	grad_norm 1.6134 (2.1939)	loss_scale 2048.0000 (1414.5012)	mem 18344MB
[2024-08-01 01:28:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:09:22 lr 0.000018	 wd 0.0500	time 0.4517 (0.4676)	loss 1.2074 (1.1860)	grad_norm 2.0835 (2.1885)	loss_scale 2048.0000 (1463.1945)	mem 18344MB
[2024-08-01 01:28:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:08:34 lr 0.000018	 wd 0.0500	time 0.4496 (0.4671)	loss 1.4589 (1.1844)	grad_norm 1.2577 (2.1909)	loss_scale 2048.0000 (1504.9365)	mem 18344MB
[2024-08-01 01:29:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:07:47 lr 0.000018	 wd 0.0500	time 0.4530 (0.4667)	loss 1.2244 (1.1826)	grad_norm 1.6854 (2.1876)	loss_scale 2048.0000 (1541.1166)	mem 18344MB
[2024-08-01 01:30:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:07:00 lr 0.000018	 wd 0.0500	time 0.4536 (0.4664)	loss 1.1712 (1.1807)	grad_norm 2.2520 (2.1999)	loss_scale 2048.0000 (1572.7770)	mem 18344MB
[2024-08-01 01:31:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:06:13 lr 0.000018	 wd 0.0500	time 0.4501 (0.4660)	loss 1.3121 (1.1826)	grad_norm 1.8931 (2.2061)	loss_scale 2048.0000 (1600.7149)	mem 18344MB
[2024-08-01 01:31:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:05:26 lr 0.000018	 wd 0.0500	time 0.4525 (0.4657)	loss 0.9821 (1.1810)	grad_norm 1.3680 (2.2071)	loss_scale 2048.0000 (1625.5502)	mem 18344MB
[2024-08-01 01:32:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:04:40 lr 0.000018	 wd 0.0500	time 0.4431 (0.4655)	loss 1.0768 (1.1797)	grad_norm 1.9306 (2.1880)	loss_scale 2048.0000 (1647.7728)	mem 18344MB
[2024-08-01 01:33:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:03:53 lr 0.000017	 wd 0.0500	time 0.4426 (0.4652)	loss 1.3267 (1.1828)	grad_norm 3.8671 (2.2252)	loss_scale 2048.0000 (1667.7741)	mem 18344MB
[2024-08-01 01:34:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:03:06 lr 0.000017	 wd 0.0500	time 0.4530 (0.4650)	loss 1.1070 (1.1844)	grad_norm 3.1151 (2.2269)	loss_scale 2048.0000 (1685.8715)	mem 18344MB
[2024-08-01 01:34:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:02:20 lr 0.000017	 wd 0.0500	time 0.4516 (0.4648)	loss 1.3118 (1.1855)	grad_norm 1.9076 (2.2231)	loss_scale 2048.0000 (1702.3244)	mem 18344MB
[2024-08-01 01:35:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:33 lr 0.000017	 wd 0.0500	time 0.4492 (0.4647)	loss 1.2982 (1.1833)	grad_norm 1.9258 (2.2315)	loss_scale 2048.0000 (1717.3472)	mem 18344MB
[2024-08-01 01:36:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:47 lr 0.000017	 wd 0.0500	time 0.4513 (0.4645)	loss 1.4620 (1.1820)	grad_norm 1.9802 (2.2212)	loss_scale 2048.0000 (1731.1187)	mem 18344MB
[2024-08-01 01:37:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.4519 (0.4644)	loss 0.8622 (1.1811)	grad_norm 2.1966 (2.2650)	loss_scale 2048.0000 (1743.7889)	mem 18344MB
[2024-08-01 01:37:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 10 training takes 0:19:24
[2024-08-01 01:37:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.023 (12.023)	Loss 0.4971 (0.4971)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 01:37:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.786 Acc@5 97.764
[2024-08-01 01:37:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 01:37:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.79%
[2024-08-01 01:37:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 01:37:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 01:38:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:51:03 lr 0.000017	 wd 0.0500	time 11.2964 (11.2964)	loss 0.9548 (0.9548)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:38:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:22:33 lr 0.000017	 wd 0.0500	time 0.4453 (0.5637)	loss 1.3753 (1.1689)	grad_norm 1.4242 (1.9084)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:39:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:19:35 lr 0.000017	 wd 0.0500	time 0.4435 (0.5105)	loss 1.2789 (1.1793)	grad_norm 1.7905 (2.0345)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:40:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:18:05 lr 0.000017	 wd 0.0500	time 0.4463 (0.4929)	loss 0.9177 (1.1718)	grad_norm 1.6068 (2.1000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:41:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:16:57 lr 0.000017	 wd 0.0500	time 0.4470 (0.4842)	loss 1.4309 (1.1748)	grad_norm 1.4331 (2.2677)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:41:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:15:59 lr 0.000017	 wd 0.0500	time 0.4545 (0.4791)	loss 0.7667 (1.1736)	grad_norm 2.5787 (2.2378)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:42:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:15:05 lr 0.000017	 wd 0.0500	time 0.4496 (0.4759)	loss 1.5632 (1.1756)	grad_norm 1.9711 (2.2016)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:43:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:14:13 lr 0.000017	 wd 0.0500	time 0.4514 (0.4737)	loss 0.9308 (1.1726)	grad_norm 1.7521 (2.2138)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:44:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:13:23 lr 0.000017	 wd 0.0500	time 0.4519 (0.4721)	loss 1.4398 (1.1711)	grad_norm 1.8402 (2.1920)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:44:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:12:34 lr 0.000017	 wd 0.0500	time 0.4492 (0.4708)	loss 0.8988 (1.1734)	grad_norm 1.4344 (2.1690)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:45:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:11:45 lr 0.000017	 wd 0.0500	time 0.4516 (0.4698)	loss 1.2621 (1.1743)	grad_norm 1.6878 (2.1919)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:46:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:10:57 lr 0.000017	 wd 0.0500	time 0.4501 (0.4690)	loss 0.9181 (1.1724)	grad_norm 1.7739 (2.2075)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:47:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:10:09 lr 0.000017	 wd 0.0500	time 0.4503 (0.4683)	loss 1.0087 (1.1685)	grad_norm 2.3577 (2.1857)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:48:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:09:22 lr 0.000017	 wd 0.0500	time 0.4495 (0.4677)	loss 1.2587 (1.1690)	grad_norm 1.5848 (2.1735)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:48:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:08:34 lr 0.000017	 wd 0.0500	time 0.4519 (0.4672)	loss 0.8961 (1.1698)	grad_norm 2.0233 (2.1858)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:49:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:07:47 lr 0.000017	 wd 0.0500	time 0.4511 (0.4668)	loss 1.4205 (1.1694)	grad_norm 1.7593 (2.2076)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:50:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:07:00 lr 0.000017	 wd 0.0500	time 0.4509 (0.4664)	loss 1.3080 (1.1707)	grad_norm 1.3596 (2.2307)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:51:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:06:13 lr 0.000017	 wd 0.0500	time 0.4506 (0.4661)	loss 1.0612 (1.1713)	grad_norm 3.0351 (2.2301)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:51:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:05:27 lr 0.000017	 wd 0.0500	time 0.4499 (0.4658)	loss 0.8770 (1.1703)	grad_norm 1.6758 (2.2159)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:52:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:04:40 lr 0.000017	 wd 0.0500	time 0.4493 (0.4656)	loss 1.0534 (1.1689)	grad_norm 1.3518 (2.2323)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:53:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:03:53 lr 0.000017	 wd 0.0500	time 0.4513 (0.4654)	loss 1.2525 (1.1693)	grad_norm 1.1585 (2.2320)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:54:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:03:06 lr 0.000017	 wd 0.0500	time 0.4495 (0.4652)	loss 1.4341 (1.1690)	grad_norm 1.4298 (2.2115)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:54:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:20 lr 0.000017	 wd 0.0500	time 0.4522 (0.4650)	loss 0.8949 (1.1704)	grad_norm 2.4322 (2.2121)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 01:55:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:33 lr 0.000016	 wd 0.0500	time 0.4519 (0.4648)	loss 1.1739 (1.1688)	grad_norm 1.6527 (2.1983)	loss_scale 4096.0000 (2101.4029)	mem 18344MB
[2024-08-01 01:56:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:47 lr 0.000016	 wd 0.0500	time 0.4503 (0.4646)	loss 1.3142 (1.1701)	grad_norm 1.4654 (2.1963)	loss_scale 4096.0000 (2184.4765)	mem 18344MB
[2024-08-01 01:57:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.4493 (0.4645)	loss 0.8546 (1.1681)	grad_norm 1.4363 (2.1975)	loss_scale 4096.0000 (2260.9068)	mem 18344MB
[2024-08-01 01:57:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 11 training takes 0:19:24
[2024-08-01 01:57:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.063 (12.063)	Loss 0.4917 (0.4917)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 01:57:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.816 Acc@5 97.718
[2024-08-01 01:57:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 01:57:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.82%
[2024-08-01 01:57:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 01:57:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 01:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:46:53 lr 0.000016	 wd 0.0500	time 11.1963 (11.1963)	loss 0.9403 (0.9403)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 01:58:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:22:31 lr 0.000016	 wd 0.0500	time 0.4476 (0.5627)	loss 1.2981 (1.1653)	grad_norm 1.5353 (1.9200)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 01:59:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:19:34 lr 0.000016	 wd 0.0500	time 0.4448 (0.5101)	loss 1.2233 (1.1633)	grad_norm 1.8959 (1.9439)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 02:00:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:18:04 lr 0.000016	 wd 0.0500	time 0.4490 (0.4926)	loss 0.9290 (1.1741)	grad_norm 1.7500 (1.9726)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 02:01:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:16:57 lr 0.000016	 wd 0.0500	time 0.4460 (0.4839)	loss 1.2391 (1.1684)	grad_norm 2.9705 (1.9930)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 02:01:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:15:58 lr 0.000016	 wd 0.0500	time 0.4486 (0.4788)	loss 1.4168 (1.1665)	grad_norm 1.4192 (nan)	loss_scale 2048.0000 (4014.2435)	mem 18344MB
[2024-08-01 02:02:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:15:04 lr 0.000016	 wd 0.0500	time 0.4518 (0.4755)	loss 1.4191 (1.1639)	grad_norm 9.5753 (nan)	loss_scale 2048.0000 (3687.0815)	mem 18344MB
[2024-08-01 02:03:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:14:13 lr 0.000016	 wd 0.0500	time 0.4527 (0.4734)	loss 1.2862 (1.1620)	grad_norm 2.0479 (nan)	loss_scale 2048.0000 (3453.2611)	mem 18344MB
[2024-08-01 02:04:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:13:23 lr 0.000016	 wd 0.0500	time 0.4550 (0.4718)	loss 1.3981 (1.1632)	grad_norm 1.9164 (nan)	loss_scale 2048.0000 (3277.8227)	mem 18344MB
[2024-08-01 02:04:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:12:33 lr 0.000016	 wd 0.0500	time 0.4418 (0.4706)	loss 1.4031 (1.1605)	grad_norm 1.6017 (nan)	loss_scale 2048.0000 (3141.3274)	mem 18344MB
[2024-08-01 02:05:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:11:45 lr 0.000016	 wd 0.0500	time 0.4512 (0.4696)	loss 1.2954 (1.1647)	grad_norm 1.4015 (nan)	loss_scale 2048.0000 (3032.1039)	mem 18344MB
[2024-08-01 02:06:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:10:57 lr 0.000016	 wd 0.0500	time 0.4522 (0.4688)	loss 0.9520 (1.1669)	grad_norm 3.4184 (nan)	loss_scale 2048.0000 (2942.7212)	mem 18344MB
[2024-08-01 02:07:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:10:09 lr 0.000016	 wd 0.0500	time 0.4502 (0.4682)	loss 1.3429 (1.1654)	grad_norm 1.8589 (nan)	loss_scale 2048.0000 (2868.2231)	mem 18344MB
[2024-08-01 02:08:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:09:22 lr 0.000016	 wd 0.0500	time 0.4494 (0.4676)	loss 1.0555 (1.1669)	grad_norm 2.0548 (nan)	loss_scale 2048.0000 (2805.1776)	mem 18344MB
[2024-08-01 02:08:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:08:34 lr 0.000016	 wd 0.0500	time 0.4383 (0.4672)	loss 1.4227 (1.1678)	grad_norm 2.9015 (nan)	loss_scale 2048.0000 (2751.1320)	mem 18344MB
[2024-08-01 02:09:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:07:47 lr 0.000016	 wd 0.0500	time 0.4514 (0.4668)	loss 0.8438 (1.1644)	grad_norm 1.6791 (nan)	loss_scale 2048.0000 (2704.2878)	mem 18344MB
[2024-08-01 02:10:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:07:00 lr 0.000016	 wd 0.0500	time 0.4523 (0.4664)	loss 1.4399 (1.1619)	grad_norm 1.2087 (nan)	loss_scale 2048.0000 (2663.2954)	mem 18344MB
[2024-08-01 02:11:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:06:13 lr 0.000016	 wd 0.0500	time 0.4530 (0.4661)	loss 1.2026 (1.1645)	grad_norm 2.9850 (nan)	loss_scale 2048.0000 (2627.1229)	mem 18344MB
[2024-08-01 02:11:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:05:26 lr 0.000016	 wd 0.0500	time 0.4502 (0.4658)	loss 0.8285 (1.1634)	grad_norm 2.1677 (nan)	loss_scale 2048.0000 (2594.9672)	mem 18344MB
[2024-08-01 02:12:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:04:40 lr 0.000016	 wd 0.0500	time 0.4520 (0.4655)	loss 1.2057 (1.1638)	grad_norm 1.6163 (nan)	loss_scale 2048.0000 (2566.1946)	mem 18344MB
[2024-08-01 02:13:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:03:53 lr 0.000016	 wd 0.0500	time 0.4502 (0.4653)	loss 1.0295 (1.1649)	grad_norm 3.4794 (nan)	loss_scale 2048.0000 (2540.2979)	mem 18344MB
[2024-08-01 02:14:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:03:06 lr 0.000016	 wd 0.0500	time 0.4537 (0.4651)	loss 0.8378 (1.1640)	grad_norm 1.4954 (nan)	loss_scale 2048.0000 (2516.8663)	mem 18344MB
[2024-08-01 02:14:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:02:20 lr 0.000016	 wd 0.0500	time 0.4544 (0.4649)	loss 0.7988 (1.1653)	grad_norm 2.5331 (nan)	loss_scale 2048.0000 (2495.5638)	mem 18344MB
[2024-08-01 02:15:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:33 lr 0.000015	 wd 0.0500	time 0.4523 (0.4647)	loss 1.0773 (1.1658)	grad_norm 1.8632 (nan)	loss_scale 2048.0000 (2476.1130)	mem 18344MB
[2024-08-01 02:16:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:47 lr 0.000015	 wd 0.0500	time 0.4534 (0.4646)	loss 1.3830 (1.1664)	grad_norm 2.1263 (nan)	loss_scale 2048.0000 (2458.2824)	mem 18344MB
[2024-08-01 02:17:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.4547 (0.4644)	loss 0.8014 (1.1658)	grad_norm 2.1844 (nan)	loss_scale 2048.0000 (2441.8776)	mem 18344MB
[2024-08-01 02:17:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 12 training takes 0:19:24
[2024-08-01 02:17:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.276 (12.276)	Loss 0.5122 (0.5122)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 02:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.818 Acc@5 97.756
[2024-08-01 02:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 02:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.82%
[2024-08-01 02:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 02:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 02:18:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:52:31 lr 0.000015	 wd 0.0500	time 11.3316 (11.3316)	loss 1.3243 (1.3243)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 02:18:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:22:34 lr 0.000015	 wd 0.0500	time 0.4460 (0.5637)	loss 1.4216 (1.2284)	grad_norm 1.1299 (nan)	loss_scale 1024.0000 (1297.7426)	mem 18344MB
[2024-08-01 02:19:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:19:35 lr 0.000015	 wd 0.0500	time 0.4478 (0.5105)	loss 1.4275 (1.2129)	grad_norm 1.4128 (nan)	loss_scale 1024.0000 (1161.5522)	mem 18344MB
[2024-08-01 02:20:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:18:05 lr 0.000015	 wd 0.0500	time 0.4482 (0.4928)	loss 1.4269 (1.2126)	grad_norm 2.0465 (nan)	loss_scale 1024.0000 (1115.8538)	mem 18344MB
[2024-08-01 02:21:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:16:57 lr 0.000015	 wd 0.0500	time 0.4456 (0.4841)	loss 1.4598 (1.1959)	grad_norm 1.9651 (nan)	loss_scale 1024.0000 (1092.9476)	mem 18344MB
[2024-08-01 02:21:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:15:59 lr 0.000015	 wd 0.0500	time 0.4508 (0.4790)	loss 1.3868 (1.1887)	grad_norm 2.7488 (nan)	loss_scale 1024.0000 (1079.1856)	mem 18344MB
[2024-08-01 02:22:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:15:04 lr 0.000015	 wd 0.0500	time 0.4506 (0.4758)	loss 1.2691 (1.1822)	grad_norm 2.1787 (nan)	loss_scale 1024.0000 (1070.0033)	mem 18344MB
[2024-08-01 02:23:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:14:13 lr 0.000015	 wd 0.0500	time 0.4525 (0.4735)	loss 1.0824 (1.1753)	grad_norm 1.9395 (nan)	loss_scale 1024.0000 (1063.4408)	mem 18344MB
[2024-08-01 02:24:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:13:23 lr 0.000015	 wd 0.0500	time 0.4511 (0.4718)	loss 1.4684 (1.1749)	grad_norm 1.7540 (nan)	loss_scale 1024.0000 (1058.5169)	mem 18344MB
[2024-08-01 02:24:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:12:33 lr 0.000015	 wd 0.0500	time 0.4518 (0.4706)	loss 1.1658 (1.1745)	grad_norm 1.8863 (nan)	loss_scale 1024.0000 (1054.6859)	mem 18344MB
[2024-08-01 02:25:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:11:45 lr 0.000015	 wd 0.0500	time 0.4500 (0.4696)	loss 1.2923 (1.1738)	grad_norm 1.7005 (nan)	loss_scale 1024.0000 (1051.6204)	mem 18344MB
[2024-08-01 02:26:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:10:57 lr 0.000015	 wd 0.0500	time 0.4507 (0.4688)	loss 1.3481 (1.1702)	grad_norm 1.7258 (nan)	loss_scale 1024.0000 (1049.1117)	mem 18344MB
[2024-08-01 02:27:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:10:09 lr 0.000015	 wd 0.0500	time 0.4518 (0.4682)	loss 1.2708 (1.1744)	grad_norm 1.8218 (nan)	loss_scale 1024.0000 (1047.0208)	mem 18344MB
[2024-08-01 02:27:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:09:22 lr 0.000015	 wd 0.0500	time 0.4493 (0.4676)	loss 0.8719 (1.1741)	grad_norm 1.5710 (nan)	loss_scale 1024.0000 (1045.2513)	mem 18344MB
[2024-08-01 02:28:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:08:34 lr 0.000015	 wd 0.0500	time 0.4520 (0.4671)	loss 1.4334 (1.1727)	grad_norm 1.8830 (nan)	loss_scale 1024.0000 (1043.7345)	mem 18344MB
[2024-08-01 02:29:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:07:47 lr 0.000015	 wd 0.0500	time 0.4516 (0.4667)	loss 1.2079 (1.1730)	grad_norm 2.9829 (nan)	loss_scale 1024.0000 (1042.4197)	mem 18344MB
[2024-08-01 02:30:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:07:00 lr 0.000015	 wd 0.0500	time 0.4416 (0.4664)	loss 0.9928 (1.1727)	grad_norm 1.5039 (nan)	loss_scale 1024.0000 (1041.2692)	mem 18344MB
[2024-08-01 02:31:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:06:13 lr 0.000015	 wd 0.0500	time 0.4508 (0.4660)	loss 1.0930 (1.1735)	grad_norm 1.4204 (nan)	loss_scale 1024.0000 (1040.2540)	mem 18344MB
[2024-08-01 02:31:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:05:26 lr 0.000015	 wd 0.0500	time 0.4494 (0.4657)	loss 1.4351 (1.1738)	grad_norm 1.5576 (nan)	loss_scale 1024.0000 (1039.3515)	mem 18344MB
[2024-08-01 02:32:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:04:40 lr 0.000015	 wd 0.0500	time 0.4517 (0.4655)	loss 1.1221 (1.1750)	grad_norm 1.8161 (nan)	loss_scale 1024.0000 (1038.5439)	mem 18344MB
[2024-08-01 02:33:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:03:53 lr 0.000015	 wd 0.0500	time 0.4556 (0.4652)	loss 1.3782 (1.1747)	grad_norm 2.3814 (nan)	loss_scale 1024.0000 (1037.8171)	mem 18344MB
[2024-08-01 02:34:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:03:06 lr 0.000014	 wd 0.0500	time 0.4522 (0.4650)	loss 1.4617 (1.1751)	grad_norm 1.8846 (nan)	loss_scale 1024.0000 (1037.1594)	mem 18344MB
[2024-08-01 02:34:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:20 lr 0.000014	 wd 0.0500	time 0.4525 (0.4648)	loss 1.1974 (1.1736)	grad_norm 2.5805 (nan)	loss_scale 1024.0000 (1036.5616)	mem 18344MB
[2024-08-01 02:35:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:33 lr 0.000014	 wd 0.0500	time 0.4516 (0.4646)	loss 1.3700 (1.1732)	grad_norm 1.8391 (nan)	loss_scale 1024.0000 (1036.0156)	mem 18344MB
[2024-08-01 02:36:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:47 lr 0.000014	 wd 0.0500	time 0.4518 (0.4644)	loss 0.8822 (1.1762)	grad_norm 1.2500 (nan)	loss_scale 1024.0000 (1035.5152)	mem 18344MB
[2024-08-01 02:37:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.4529 (0.4643)	loss 0.8397 (1.1742)	grad_norm 1.7356 (nan)	loss_scale 1024.0000 (1035.0548)	mem 18344MB
[2024-08-01 02:37:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 13 training takes 0:19:24
[2024-08-01 02:37:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.949 (11.949)	Loss 0.4858 (0.4858)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-08-01 02:37:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.898 Acc@5 97.760
[2024-08-01 02:37:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 02:37:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 02:37:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 02:37:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 02:37:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:52:39 lr 0.000014	 wd 0.0500	time 11.3347 (11.3347)	loss 1.3324 (1.3324)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:38:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:22:38 lr 0.000014	 wd 0.0500	time 0.4506 (0.5657)	loss 1.0968 (1.1811)	grad_norm 1.7582 (2.2350)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:39:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:19:37 lr 0.000014	 wd 0.0500	time 0.4462 (0.5115)	loss 1.3935 (1.1857)	grad_norm 1.7975 (2.3486)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:40:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:18:06 lr 0.000014	 wd 0.0500	time 0.4493 (0.4935)	loss 0.9701 (1.1876)	grad_norm 1.7470 (2.2417)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:41:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:16:58 lr 0.000014	 wd 0.0500	time 0.4488 (0.4846)	loss 1.6446 (1.1885)	grad_norm 4.0244 (2.2251)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:41:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:15:59 lr 0.000014	 wd 0.0500	time 0.4501 (0.4794)	loss 0.9756 (1.1838)	grad_norm 2.2610 (2.1645)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:42:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:15:05 lr 0.000014	 wd 0.0500	time 0.4537 (0.4760)	loss 0.9892 (1.1810)	grad_norm 1.9538 (2.1171)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:43:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:14:13 lr 0.000014	 wd 0.0500	time 0.4509 (0.4737)	loss 1.2306 (1.1829)	grad_norm 1.9868 (2.1788)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:44:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:13:23 lr 0.000014	 wd 0.0500	time 0.4508 (0.4720)	loss 0.7646 (1.1772)	grad_norm 1.7818 (2.1397)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:44:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:12:34 lr 0.000014	 wd 0.0500	time 0.4505 (0.4707)	loss 0.8209 (1.1760)	grad_norm 1.2679 (2.1668)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:45:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:11:45 lr 0.000014	 wd 0.0500	time 0.4514 (0.4697)	loss 1.0225 (1.1737)	grad_norm 1.8518 (2.1472)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:46:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:10:57 lr 0.000014	 wd 0.0500	time 0.4524 (0.4689)	loss 1.3839 (1.1744)	grad_norm 1.7183 (2.1398)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:47:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:10:09 lr 0.000014	 wd 0.0500	time 0.4512 (0.4682)	loss 1.5459 (1.1741)	grad_norm 1.7821 (2.1409)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:47:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:09:22 lr 0.000014	 wd 0.0500	time 0.4519 (0.4676)	loss 1.2222 (1.1778)	grad_norm 2.6844 (2.2438)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:48:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:08:34 lr 0.000014	 wd 0.0500	time 0.4525 (0.4672)	loss 1.3402 (1.1781)	grad_norm 2.4860 (2.2856)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:49:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:07:47 lr 0.000014	 wd 0.0500	time 0.4508 (0.4667)	loss 1.4568 (1.1768)	grad_norm 1.5425 (2.2567)	loss_scale 1024.0000 (1024.0000)	mem 18344MB
[2024-08-01 02:50:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:07:00 lr 0.000014	 wd 0.0500	time 0.4515 (0.4664)	loss 0.9314 (1.1759)	grad_norm 1.6764 (2.2449)	loss_scale 2048.0000 (1072.6096)	mem 18344MB
[2024-08-01 02:51:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:06:13 lr 0.000014	 wd 0.0500	time 0.4524 (0.4660)	loss 0.8914 (1.1751)	grad_norm 1.3190 (2.2611)	loss_scale 2048.0000 (1129.9518)	mem 18344MB
[2024-08-01 02:51:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:05:26 lr 0.000013	 wd 0.0500	time 0.4520 (0.4657)	loss 1.0599 (1.1762)	grad_norm 1.5718 (2.2563)	loss_scale 2048.0000 (1180.9262)	mem 18344MB
[2024-08-01 02:52:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:04:40 lr 0.000013	 wd 0.0500	time 0.4552 (0.4655)	loss 1.2454 (1.1771)	grad_norm 2.6322 (2.2602)	loss_scale 2048.0000 (1226.5376)	mem 18344MB
[2024-08-01 02:53:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:03:53 lr 0.000013	 wd 0.0500	time 0.4508 (0.4652)	loss 1.4303 (1.1756)	grad_norm 1.7711 (2.2428)	loss_scale 2048.0000 (1267.5902)	mem 18344MB
[2024-08-01 02:54:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:03:06 lr 0.000013	 wd 0.0500	time 0.4535 (0.4650)	loss 1.3008 (1.1749)	grad_norm 1.5319 (2.2317)	loss_scale 2048.0000 (1304.7349)	mem 18344MB
[2024-08-01 02:54:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:20 lr 0.000013	 wd 0.0500	time 0.4504 (0.4648)	loss 1.2667 (1.1741)	grad_norm 2.5630 (2.2365)	loss_scale 2048.0000 (1338.5043)	mem 18344MB
[2024-08-01 02:55:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:33 lr 0.000013	 wd 0.0500	time 0.4530 (0.4647)	loss 0.8430 (1.1734)	grad_norm 1.6566 (2.2431)	loss_scale 2048.0000 (1369.3385)	mem 18344MB
[2024-08-01 02:56:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:47 lr 0.000013	 wd 0.0500	time 0.4553 (0.4645)	loss 0.8275 (1.1728)	grad_norm 1.6792 (2.2312)	loss_scale 2048.0000 (1397.6043)	mem 18344MB
[2024-08-01 02:57:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.4496 (0.4643)	loss 1.3199 (1.1719)	grad_norm 2.0696 (2.2191)	loss_scale 2048.0000 (1423.6098)	mem 18344MB
[2024-08-01 02:57:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 14 training takes 0:19:24
[2024-08-01 02:57:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.358 (12.358)	Loss 0.4751 (0.4751)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 02:57:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.792 Acc@5 97.784
[2024-08-01 02:57:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 02:57:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 02:57:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:45:17 lr 0.000013	 wd 0.0500	time 11.1579 (11.1579)	loss 1.3025 (1.3025)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 02:58:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:22:37 lr 0.000013	 wd 0.0500	time 0.4474 (0.5653)	loss 0.7892 (1.1696)	grad_norm 2.0050 (2.1620)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 02:59:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:19:37 lr 0.000013	 wd 0.0500	time 0.4486 (0.5114)	loss 1.5569 (1.1595)	grad_norm 1.3122 (2.0796)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:00:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:18:06 lr 0.000013	 wd 0.0500	time 0.4485 (0.4934)	loss 0.9145 (1.1577)	grad_norm 1.4217 (2.0003)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:01:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:16:58 lr 0.000013	 wd 0.0500	time 0.4461 (0.4845)	loss 0.7701 (1.1562)	grad_norm 1.3734 (2.0638)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:01:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:15:59 lr 0.000013	 wd 0.0500	time 0.4568 (0.4794)	loss 0.7471 (1.1661)	grad_norm 1.4199 (2.0447)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:02:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:15:05 lr 0.000013	 wd 0.0500	time 0.4517 (0.4761)	loss 1.2932 (1.1707)	grad_norm 1.6537 (2.1104)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:03:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:14:13 lr 0.000013	 wd 0.0500	time 0.4525 (0.4738)	loss 1.3573 (1.1729)	grad_norm 1.8489 (2.1158)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:04:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:13:23 lr 0.000013	 wd 0.0500	time 0.4506 (0.4722)	loss 1.1921 (1.1739)	grad_norm 1.9302 (2.2243)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:04:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:12:34 lr 0.000013	 wd 0.0500	time 0.4536 (0.4709)	loss 1.3765 (1.1753)	grad_norm 3.1550 (2.1871)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:05:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:11:45 lr 0.000013	 wd 0.0500	time 0.4515 (0.4699)	loss 0.7902 (1.1752)	grad_norm 2.3735 (2.1743)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:06:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:10:57 lr 0.000013	 wd 0.0500	time 0.4515 (0.4691)	loss 0.8732 (1.1758)	grad_norm 1.1716 (2.1718)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:07:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:10:09 lr 0.000013	 wd 0.0500	time 0.4528 (0.4684)	loss 0.8815 (1.1759)	grad_norm 1.9787 (2.2098)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:07:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:09:22 lr 0.000013	 wd 0.0500	time 0.4497 (0.4678)	loss 0.9368 (1.1770)	grad_norm 1.5072 (2.2093)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:08:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:08:34 lr 0.000012	 wd 0.0500	time 0.4502 (0.4673)	loss 0.8411 (1.1771)	grad_norm 2.3845 (2.1832)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:09:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:07:47 lr 0.000012	 wd 0.0500	time 0.4527 (0.4668)	loss 1.0829 (1.1729)	grad_norm 1.6033 (2.2002)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:10:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:07:00 lr 0.000012	 wd 0.0500	time 0.4538 (0.4665)	loss 0.9324 (1.1738)	grad_norm 4.0689 (2.1841)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:10:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:06:13 lr 0.000012	 wd 0.0500	time 0.4508 (0.4661)	loss 0.7868 (1.1743)	grad_norm 2.7149 (2.1895)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:11:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:05:27 lr 0.000012	 wd 0.0500	time 0.4506 (0.4658)	loss 1.6748 (1.1726)	grad_norm 1.8190 (2.2165)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:12:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:04:40 lr 0.000012	 wd 0.0500	time 0.4521 (0.4656)	loss 0.9317 (1.1716)	grad_norm 3.3784 (2.2079)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:13:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:03:53 lr 0.000012	 wd 0.0500	time 0.4516 (0.4653)	loss 0.9511 (1.1727)	grad_norm 2.1779 (2.2000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:14:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:03:06 lr 0.000012	 wd 0.0500	time 0.4484 (0.4651)	loss 0.7503 (1.1714)	grad_norm 1.4809 (2.1848)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:14:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:20 lr 0.000012	 wd 0.0500	time 0.4526 (0.4649)	loss 0.9168 (1.1710)	grad_norm 2.4327 (2.2025)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:15:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:33 lr 0.000012	 wd 0.0500	time 0.4498 (0.4647)	loss 1.0297 (1.1715)	grad_norm 2.0132 (2.2019)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:16:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:47 lr 0.000012	 wd 0.0500	time 0.4500 (0.4646)	loss 1.3674 (1.1718)	grad_norm 2.2870 (2.1877)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:17:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.4511 (0.4644)	loss 1.5045 (1.1728)	grad_norm 1.6175 (2.1906)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:17:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 15 training takes 0:19:24
[2024-08-01 03:17:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_15.pth saving......
[2024-08-01 03:17:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_15.pth saved !!!
[2024-08-01 03:17:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 10.974 (10.974)	Loss 0.5264 (0.5264)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-08-01 03:17:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.808 Acc@5 97.744
[2024-08-01 03:17:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 03:17:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 03:17:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:32:09 lr 0.000012	 wd 0.0500	time 12.2819 (12.2819)	loss 1.2364 (1.2364)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:18:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:22:56 lr 0.000012	 wd 0.0500	time 0.4504 (0.5732)	loss 1.1496 (1.1824)	grad_norm 1.5144 (2.0696)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:19:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:19:46 lr 0.000012	 wd 0.0500	time 0.4476 (0.5154)	loss 1.2571 (1.1719)	grad_norm 1.7207 (2.1089)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:20:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:18:12 lr 0.000012	 wd 0.0500	time 0.4493 (0.4961)	loss 1.0577 (1.1660)	grad_norm 1.7669 (2.3653)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:20:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:17:02 lr 0.000012	 wd 0.0500	time 0.4483 (0.4866)	loss 0.8573 (1.1553)	grad_norm 1.4396 (2.2951)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:21:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:16:02 lr 0.000012	 wd 0.0500	time 0.4471 (0.4810)	loss 1.4666 (1.1596)	grad_norm 1.5386 (2.3042)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:22:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:15:08 lr 0.000012	 wd 0.0500	time 0.4497 (0.4774)	loss 1.3911 (1.1581)	grad_norm 1.5367 (2.3140)	loss_scale 4096.0000 (2320.6123)	mem 18344MB
[2024-08-01 03:23:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:14:15 lr 0.000012	 wd 0.0500	time 0.4516 (0.4749)	loss 1.0346 (1.1602)	grad_norm 1.4890 (2.2552)	loss_scale 4096.0000 (2573.8773)	mem 18344MB
[2024-08-01 03:24:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:13:25 lr 0.000012	 wd 0.0500	time 0.4505 (0.4731)	loss 1.3598 (1.1619)	grad_norm 1.5969 (2.2182)	loss_scale 4096.0000 (2763.9051)	mem 18344MB
[2024-08-01 03:24:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:12:35 lr 0.000012	 wd 0.0500	time 0.4482 (0.4717)	loss 1.0917 (1.1601)	grad_norm 1.6823 (2.2106)	loss_scale 4096.0000 (2911.7514)	mem 18344MB
[2024-08-01 03:25:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:11:46 lr 0.000011	 wd 0.0500	time 0.4524 (0.4706)	loss 1.3374 (1.1611)	grad_norm 1.1918 (2.2271)	loss_scale 4096.0000 (3030.0579)	mem 18344MB
[2024-08-01 03:26:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:10:58 lr 0.000011	 wd 0.0500	time 0.4511 (0.4698)	loss 0.9088 (1.1666)	grad_norm 2.0756 (2.2120)	loss_scale 4096.0000 (3126.8738)	mem 18344MB
[2024-08-01 03:27:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:10:10 lr 0.000011	 wd 0.0500	time 0.4529 (0.4690)	loss 0.9094 (1.1661)	grad_norm 2.2123 (2.1993)	loss_scale 4096.0000 (3207.5670)	mem 18344MB
[2024-08-01 03:27:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:09:23 lr 0.000011	 wd 0.0500	time 0.4530 (0.4684)	loss 1.3155 (1.1693)	grad_norm 1.5051 (2.1696)	loss_scale 4096.0000 (3275.8555)	mem 18344MB
[2024-08-01 03:28:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:08:35 lr 0.000011	 wd 0.0500	time 0.4524 (0.4679)	loss 1.2469 (1.1700)	grad_norm 1.3641 (2.1464)	loss_scale 4096.0000 (3334.3954)	mem 18344MB
[2024-08-01 03:29:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:07:48 lr 0.000011	 wd 0.0500	time 0.4505 (0.4674)	loss 1.2003 (1.1720)	grad_norm 2.0079 (2.1626)	loss_scale 4096.0000 (3385.1352)	mem 18344MB
[2024-08-01 03:30:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:07:01 lr 0.000011	 wd 0.0500	time 0.4537 (0.4670)	loss 1.2608 (1.1723)	grad_norm 1.7337 (2.1561)	loss_scale 4096.0000 (3429.5365)	mem 18344MB
[2024-08-01 03:30:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:06:14 lr 0.000011	 wd 0.0500	time 0.4527 (0.4667)	loss 1.0949 (1.1717)	grad_norm 1.5259 (2.1463)	loss_scale 4096.0000 (3468.7172)	mem 18344MB
[2024-08-01 03:31:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:05:27 lr 0.000011	 wd 0.0500	time 0.4559 (0.4664)	loss 0.9226 (1.1706)	grad_norm 1.7405 (2.1347)	loss_scale 4096.0000 (3503.5469)	mem 18344MB
[2024-08-01 03:32:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:04:40 lr 0.000011	 wd 0.0500	time 0.4484 (0.4660)	loss 0.8981 (1.1712)	grad_norm 1.7360 (nan)	loss_scale 2048.0000 (3495.9285)	mem 18344MB
[2024-08-01 03:33:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:03:53 lr 0.000011	 wd 0.0500	time 0.4516 (0.4658)	loss 1.3276 (1.1718)	grad_norm 2.1019 (nan)	loss_scale 2048.0000 (3423.5682)	mem 18344MB
[2024-08-01 03:34:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:03:07 lr 0.000011	 wd 0.0500	time 0.4516 (0.4656)	loss 1.2642 (1.1724)	grad_norm 1.5038 (nan)	loss_scale 2048.0000 (3358.0961)	mem 18344MB
[2024-08-01 03:34:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:02:20 lr 0.000011	 wd 0.0500	time 0.4476 (0.4653)	loss 0.9261 (1.1720)	grad_norm 2.9729 (nan)	loss_scale 2048.0000 (3298.5734)	mem 18344MB
[2024-08-01 03:35:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:33 lr 0.000011	 wd 0.0500	time 0.4542 (0.4652)	loss 0.8352 (1.1702)	grad_norm 3.0478 (nan)	loss_scale 2048.0000 (3244.2243)	mem 18344MB
[2024-08-01 03:36:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:47 lr 0.000011	 wd 0.0500	time 0.4546 (0.4650)	loss 1.2788 (1.1696)	grad_norm 1.2492 (nan)	loss_scale 2048.0000 (3194.4023)	mem 18344MB
[2024-08-01 03:37:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.4522 (0.4648)	loss 0.8299 (1.1691)	grad_norm 1.7859 (nan)	loss_scale 2048.0000 (3148.5646)	mem 18344MB
[2024-08-01 03:37:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 16 training takes 0:19:25
[2024-08-01 03:37:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.639 (12.639)	Loss 0.5005 (0.5005)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 03:37:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.856 Acc@5 97.786
[2024-08-01 03:37:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 03:37:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 03:37:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:54:31 lr 0.000011	 wd 0.0500	time 11.3794 (11.3794)	loss 1.2933 (1.2933)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:38:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:22:50 lr 0.000011	 wd 0.0500	time 0.4469 (0.5704)	loss 1.2889 (1.1459)	grad_norm 1.4937 (1.9360)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:39:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:19:43 lr 0.000011	 wd 0.0500	time 0.4353 (0.5139)	loss 1.3837 (1.1516)	grad_norm 5.2462 (2.1485)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:40:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:18:10 lr 0.000011	 wd 0.0500	time 0.4471 (0.4951)	loss 1.4240 (1.1663)	grad_norm 2.0810 (2.1048)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:40:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:17:01 lr 0.000011	 wd 0.0500	time 0.4471 (0.4859)	loss 1.5286 (1.1616)	grad_norm 4.1359 (2.0820)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:41:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:16:01 lr 0.000010	 wd 0.0500	time 0.4509 (0.4804)	loss 1.3622 (1.1684)	grad_norm 1.8838 (2.1112)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:42:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:15:07 lr 0.000010	 wd 0.0500	time 0.4508 (0.4769)	loss 0.8833 (1.1696)	grad_norm 1.7096 (2.0506)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:43:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:14:15 lr 0.000010	 wd 0.0500	time 0.4490 (0.4745)	loss 1.3605 (1.1669)	grad_norm 1.2672 (2.0382)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:44:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:13:24 lr 0.000010	 wd 0.0500	time 0.4486 (0.4728)	loss 1.2298 (1.1664)	grad_norm 2.0890 (2.0878)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:44:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:12:35 lr 0.000010	 wd 0.0500	time 0.4499 (0.4715)	loss 1.3389 (1.1686)	grad_norm 1.2314 (2.0807)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:45:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:11:46 lr 0.000010	 wd 0.0500	time 0.4494 (0.4704)	loss 0.9932 (1.1670)	grad_norm 1.5912 (2.1880)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:46:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:10:58 lr 0.000010	 wd 0.0500	time 0.4518 (0.4696)	loss 0.9992 (1.1720)	grad_norm 1.3062 (2.1822)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:47:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:10:10 lr 0.000010	 wd 0.0500	time 0.4533 (0.4688)	loss 0.8636 (1.1718)	grad_norm 2.0866 (2.1882)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:47:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:09:22 lr 0.000010	 wd 0.0500	time 0.4530 (0.4682)	loss 1.5074 (1.1705)	grad_norm 1.8832 (2.1685)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:48:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:08:35 lr 0.000010	 wd 0.0500	time 0.4522 (0.4677)	loss 1.4325 (1.1708)	grad_norm 1.5714 (2.1468)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:49:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:07:48 lr 0.000010	 wd 0.0500	time 0.4392 (0.4672)	loss 1.3762 (1.1710)	grad_norm 2.1727 (2.1376)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:50:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:07:01 lr 0.000010	 wd 0.0500	time 0.4522 (0.4669)	loss 1.3490 (1.1693)	grad_norm 1.5984 (2.1220)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:50:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:06:14 lr 0.000010	 wd 0.0500	time 0.4538 (0.4666)	loss 1.5574 (1.1713)	grad_norm 1.9610 (2.1100)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:51:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:05:27 lr 0.000010	 wd 0.0500	time 0.4534 (0.4663)	loss 0.9924 (1.1712)	grad_norm 1.6805 (2.1027)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:52:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:04:40 lr 0.000010	 wd 0.0500	time 0.4523 (0.4660)	loss 1.2706 (1.1703)	grad_norm 6.2064 (2.1055)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:53:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:03:53 lr 0.000010	 wd 0.0500	time 0.4520 (0.4657)	loss 1.0067 (1.1711)	grad_norm 2.5692 (2.1031)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:53:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:03:07 lr 0.000010	 wd 0.0500	time 0.4512 (0.4655)	loss 1.4825 (1.1712)	grad_norm 2.0754 (2.0955)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:54:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:20 lr 0.000010	 wd 0.0500	time 0.4494 (0.4653)	loss 1.5230 (1.1718)	grad_norm 3.2873 (2.1107)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:55:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:33 lr 0.000010	 wd 0.0500	time 0.4527 (0.4651)	loss 0.8532 (1.1712)	grad_norm 1.4093 (2.0985)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:56:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:47 lr 0.000010	 wd 0.0500	time 0.4517 (0.4649)	loss 0.9230 (1.1699)	grad_norm 3.2486 (2.0940)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:57:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.4516 (0.4648)	loss 1.5316 (1.1700)	grad_norm 1.4528 (2.0995)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:57:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 17 training takes 0:19:25
[2024-08-01 03:57:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.879 (11.879)	Loss 0.4897 (0.4897)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 03:57:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.848 Acc@5 97.770
[2024-08-01 03:57:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 03:57:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 03:57:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:45:58 lr 0.000009	 wd 0.0500	time 11.1746 (11.1746)	loss 1.5457 (1.5457)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:58:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:22:41 lr 0.000009	 wd 0.0500	time 0.4467 (0.5668)	loss 1.5160 (1.1806)	grad_norm 2.0319 (2.0770)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 03:59:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:19:39 lr 0.000009	 wd 0.0500	time 0.4466 (0.5122)	loss 1.0090 (1.1831)	grad_norm 1.9071 (2.1095)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:00:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:18:07 lr 0.000009	 wd 0.0500	time 0.4491 (0.4941)	loss 1.6071 (1.1848)	grad_norm 2.6508 (2.1923)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:00:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:16:59 lr 0.000009	 wd 0.0500	time 0.4480 (0.4852)	loss 0.7458 (1.1844)	grad_norm 2.1088 (2.1434)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:01:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:16:00 lr 0.000009	 wd 0.0500	time 0.4501 (0.4798)	loss 1.4085 (1.1807)	grad_norm 1.4635 (2.1230)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:02:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:15:06 lr 0.000009	 wd 0.0500	time 0.4495 (0.4765)	loss 1.6067 (1.1746)	grad_norm 1.6056 (2.0766)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:03:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:14:14 lr 0.000009	 wd 0.0500	time 0.4530 (0.4742)	loss 1.4103 (1.1774)	grad_norm 1.7027 (2.1060)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:03:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:13:24 lr 0.000009	 wd 0.0500	time 0.4509 (0.4725)	loss 1.3559 (1.1819)	grad_norm 1.5618 (2.0743)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:04:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:12:34 lr 0.000009	 wd 0.0500	time 0.4502 (0.4711)	loss 0.9745 (1.1803)	grad_norm 1.4016 (2.0988)	loss_scale 4096.0000 (2138.9212)	mem 18344MB
[2024-08-01 04:05:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:11:46 lr 0.000009	 wd 0.0500	time 0.4504 (0.4701)	loss 1.4173 (1.1778)	grad_norm 2.0754 (2.0748)	loss_scale 4096.0000 (2334.4336)	mem 18344MB
[2024-08-01 04:06:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:10:57 lr 0.000009	 wd 0.0500	time 0.4444 (0.4693)	loss 1.0383 (1.1794)	grad_norm 2.8083 (2.0610)	loss_scale 4096.0000 (2494.4305)	mem 18344MB
[2024-08-01 04:07:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:10:10 lr 0.000009	 wd 0.0500	time 0.4504 (0.4686)	loss 0.9828 (1.1739)	grad_norm 1.5639 (2.0656)	loss_scale 4096.0000 (2627.7835)	mem 18344MB
[2024-08-01 04:07:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:09:22 lr 0.000009	 wd 0.0500	time 0.4520 (0.4680)	loss 1.5044 (1.1737)	grad_norm 2.0221 (2.0675)	loss_scale 4096.0000 (2740.6364)	mem 18344MB
[2024-08-01 04:08:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:08:35 lr 0.000009	 wd 0.0500	time 0.4494 (0.4675)	loss 1.2832 (1.1744)	grad_norm 1.7810 (2.0565)	loss_scale 4096.0000 (2837.3790)	mem 18344MB
[2024-08-01 04:09:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:07:48 lr 0.000009	 wd 0.0500	time 0.4519 (0.4671)	loss 1.3985 (1.1748)	grad_norm 1.2963 (2.0656)	loss_scale 4096.0000 (2921.2312)	mem 18344MB
[2024-08-01 04:10:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:07:01 lr 0.000009	 wd 0.0500	time 0.4514 (0.4668)	loss 1.3516 (1.1762)	grad_norm 4.2334 (2.0817)	loss_scale 4096.0000 (2994.6084)	mem 18344MB
[2024-08-01 04:10:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:06:14 lr 0.000009	 wd 0.0500	time 0.4503 (0.4664)	loss 1.2930 (1.1776)	grad_norm 1.5274 (2.0820)	loss_scale 4096.0000 (3059.3580)	mem 18344MB
[2024-08-01 04:11:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:05:27 lr 0.000009	 wd 0.0500	time 0.4527 (0.4661)	loss 0.7820 (1.1778)	grad_norm 1.9644 (2.0829)	loss_scale 4096.0000 (3116.9173)	mem 18344MB
[2024-08-01 04:12:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:04:40 lr 0.000009	 wd 0.0500	time 0.4491 (0.4658)	loss 1.5391 (1.1778)	grad_norm 1.4363 (2.1102)	loss_scale 4096.0000 (3168.4208)	mem 18344MB
[2024-08-01 04:13:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:03:53 lr 0.000008	 wd 0.0500	time 0.4514 (0.4656)	loss 1.0104 (1.1772)	grad_norm 2.4255 (2.1117)	loss_scale 4096.0000 (3214.7766)	mem 18344MB
[2024-08-01 04:13:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:03:07 lr 0.000008	 wd 0.0500	time 0.4491 (0.4653)	loss 1.3246 (1.1769)	grad_norm 1.5391 (2.1189)	loss_scale 4096.0000 (3256.7197)	mem 18344MB
[2024-08-01 04:14:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:02:20 lr 0.000008	 wd 0.0500	time 0.4511 (0.4651)	loss 1.0684 (1.1769)	grad_norm 2.5279 (2.1133)	loss_scale 4096.0000 (3294.8514)	mem 18344MB
[2024-08-01 04:15:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:33 lr 0.000008	 wd 0.0500	time 0.4516 (0.4650)	loss 1.3624 (1.1780)	grad_norm 1.7149 (2.1113)	loss_scale 4096.0000 (3329.6688)	mem 18344MB
[2024-08-01 04:16:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:47 lr 0.000008	 wd 0.0500	time 0.4498 (0.4648)	loss 1.4542 (1.1782)	grad_norm 2.0484 (2.0983)	loss_scale 4096.0000 (3361.5860)	mem 18344MB
[2024-08-01 04:17:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.4478 (0.4646)	loss 1.2008 (1.1784)	grad_norm 1.3754 (2.1050)	loss_scale 4096.0000 (3390.9508)	mem 18344MB
[2024-08-01 04:17:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 18 training takes 0:19:24
[2024-08-01 04:17:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.017 (12.017)	Loss 0.5010 (0.5010)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 04:17:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.846 Acc@5 97.746
[2024-08-01 04:17:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 04:17:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 04:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:17:01 lr 0.000008	 wd 0.0500	time 11.9191 (11.9191)	loss 0.8880 (0.8880)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 04:18:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:22:50 lr 0.000008	 wd 0.0500	time 0.4474 (0.5704)	loss 1.5925 (1.2188)	grad_norm 1.4833 (2.4116)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 04:19:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:19:43 lr 0.000008	 wd 0.0500	time 0.4450 (0.5139)	loss 0.9079 (1.1879)	grad_norm 1.5037 (2.1408)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 04:20:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:18:10 lr 0.000008	 wd 0.0500	time 0.4480 (0.4952)	loss 1.4376 (1.1782)	grad_norm 1.2550 (2.0480)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 04:20:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:17:01 lr 0.000008	 wd 0.0500	time 0.4500 (0.4860)	loss 0.9859 (1.1690)	grad_norm 15.1392 (2.2165)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 04:21:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:16:02 lr 0.000008	 wd 0.0500	time 0.4517 (0.4805)	loss 1.4011 (1.1772)	grad_norm 1.6534 (nan)	loss_scale 2048.0000 (3957.0140)	mem 18344MB
[2024-08-01 04:22:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:15:07 lr 0.000008	 wd 0.0500	time 0.4502 (0.4770)	loss 0.9000 (1.1689)	grad_norm 1.6236 (nan)	loss_scale 2048.0000 (3639.3744)	mem 18344MB
[2024-08-01 04:23:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:14:15 lr 0.000008	 wd 0.0500	time 0.4505 (0.4746)	loss 1.1658 (1.1676)	grad_norm 2.0272 (nan)	loss_scale 2048.0000 (3412.3595)	mem 18344MB
[2024-08-01 04:23:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:13:24 lr 0.000008	 wd 0.0500	time 0.4523 (0.4728)	loss 1.4092 (1.1692)	grad_norm 2.4823 (nan)	loss_scale 2048.0000 (3242.0275)	mem 18344MB
[2024-08-01 04:24:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:12:35 lr 0.000008	 wd 0.0500	time 0.4536 (0.4715)	loss 1.1849 (1.1701)	grad_norm 1.3406 (nan)	loss_scale 2048.0000 (3109.5050)	mem 18344MB
[2024-08-01 04:25:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:11:46 lr 0.000008	 wd 0.0500	time 0.4517 (0.4704)	loss 1.4104 (1.1702)	grad_norm 2.6858 (nan)	loss_scale 2048.0000 (3003.4605)	mem 18344MB
[2024-08-01 04:26:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:10:58 lr 0.000008	 wd 0.0500	time 0.4514 (0.4695)	loss 1.1531 (1.1688)	grad_norm 1.6186 (nan)	loss_scale 2048.0000 (2916.6794)	mem 18344MB
[2024-08-01 04:27:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:10:10 lr 0.000008	 wd 0.0500	time 0.4523 (0.4687)	loss 1.3624 (1.1673)	grad_norm 1.5415 (nan)	loss_scale 2048.0000 (2844.3497)	mem 18344MB
[2024-08-01 04:27:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:09:22 lr 0.000008	 wd 0.0500	time 0.4528 (0.4681)	loss 0.9477 (1.1668)	grad_norm 3.6557 (nan)	loss_scale 2048.0000 (2783.1391)	mem 18344MB
[2024-08-01 04:28:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:08:35 lr 0.000008	 wd 0.0500	time 0.4492 (0.4676)	loss 0.9351 (1.1668)	grad_norm 1.8186 (nan)	loss_scale 2048.0000 (2730.6667)	mem 18344MB
[2024-08-01 04:29:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:07:48 lr 0.000008	 wd 0.0500	time 0.4522 (0.4672)	loss 1.5070 (1.1677)	grad_norm 2.8546 (nan)	loss_scale 2048.0000 (2685.1859)	mem 18344MB
[2024-08-01 04:30:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:07:01 lr 0.000007	 wd 0.0500	time 0.4529 (0.4668)	loss 1.2276 (1.1693)	grad_norm 1.5312 (nan)	loss_scale 2048.0000 (2645.3866)	mem 18344MB
[2024-08-01 04:30:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:06:14 lr 0.000007	 wd 0.0500	time 0.4539 (0.4664)	loss 0.8067 (1.1696)	grad_norm 1.3550 (nan)	loss_scale 2048.0000 (2610.2669)	mem 18344MB
[2024-08-01 04:31:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:05:27 lr 0.000007	 wd 0.0500	time 0.4493 (0.4661)	loss 1.3091 (1.1716)	grad_norm 1.3994 (nan)	loss_scale 2048.0000 (2579.0472)	mem 18344MB
[2024-08-01 04:32:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:04:40 lr 0.000007	 wd 0.0500	time 0.4527 (0.4658)	loss 1.1785 (1.1702)	grad_norm 2.4006 (nan)	loss_scale 2048.0000 (2551.1120)	mem 18344MB
[2024-08-01 04:33:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:03:53 lr 0.000007	 wd 0.0500	time 0.4531 (0.4656)	loss 1.3296 (1.1712)	grad_norm 1.6015 (nan)	loss_scale 2048.0000 (2525.9690)	mem 18344MB
[2024-08-01 04:33:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:03:07 lr 0.000007	 wd 0.0500	time 0.4536 (0.4653)	loss 1.6351 (1.1732)	grad_norm 1.0679 (nan)	loss_scale 2048.0000 (2503.2194)	mem 18344MB
[2024-08-01 04:34:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:02:20 lr 0.000007	 wd 0.0500	time 0.4534 (0.4651)	loss 1.2487 (1.1725)	grad_norm 5.1355 (nan)	loss_scale 2048.0000 (2482.5370)	mem 18344MB
[2024-08-01 04:35:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:33 lr 0.000007	 wd 0.0500	time 0.4521 (0.4650)	loss 1.0129 (1.1721)	grad_norm 3.0164 (nan)	loss_scale 2048.0000 (2463.6523)	mem 18344MB
[2024-08-01 04:36:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:47 lr 0.000007	 wd 0.0500	time 0.4506 (0.4648)	loss 1.1370 (1.1713)	grad_norm 3.9162 (nan)	loss_scale 2048.0000 (2446.3407)	mem 18344MB
[2024-08-01 04:37:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.4527 (0.4646)	loss 0.8366 (1.1712)	grad_norm 1.5789 (nan)	loss_scale 2048.0000 (2430.4134)	mem 18344MB
[2024-08-01 04:37:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 19 training takes 0:19:25
[2024-08-01 04:37:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.171 (12.171)	Loss 0.4863 (0.4863)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 04:37:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.848 Acc@5 97.754
[2024-08-01 04:37:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 04:37:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 04:37:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][0/2502]	eta 8:20:10 lr 0.000007	 wd 0.0500	time 11.9947 (11.9947)	loss 1.3885 (1.3885)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:38:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:22:53 lr 0.000007	 wd 0.0500	time 0.4481 (0.5716)	loss 0.9766 (1.1772)	grad_norm 2.2864 (2.1248)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:39:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:19:44 lr 0.000007	 wd 0.0500	time 0.4467 (0.5145)	loss 1.3130 (1.1675)	grad_norm 2.0241 (2.3127)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:40:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:18:11 lr 0.000007	 wd 0.0500	time 0.4459 (0.4955)	loss 0.7367 (1.1779)	grad_norm 1.7418 (2.2075)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:40:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:17:01 lr 0.000007	 wd 0.0500	time 0.4471 (0.4861)	loss 1.3220 (1.1753)	grad_norm 1.3427 (2.4304)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:41:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:16:02 lr 0.000007	 wd 0.0500	time 0.4465 (0.4806)	loss 1.2948 (1.1743)	grad_norm 1.4772 (2.3488)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:42:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:15:07 lr 0.000007	 wd 0.0500	time 0.4492 (0.4771)	loss 1.3768 (1.1787)	grad_norm 2.8807 (2.3294)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:43:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:14:15 lr 0.000007	 wd 0.0500	time 0.4506 (0.4747)	loss 1.3340 (1.1769)	grad_norm 2.9083 (2.3072)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:43:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:13:24 lr 0.000007	 wd 0.0500	time 0.4515 (0.4729)	loss 1.1930 (1.1789)	grad_norm 1.4163 (2.2767)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:44:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:12:35 lr 0.000007	 wd 0.0500	time 0.4510 (0.4715)	loss 1.3951 (1.1776)	grad_norm 1.5422 (2.2502)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:45:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:11:46 lr 0.000007	 wd 0.0500	time 0.4525 (0.4704)	loss 1.2599 (1.1793)	grad_norm 1.4441 (2.2480)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:46:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:10:58 lr 0.000007	 wd 0.0500	time 0.4523 (0.4696)	loss 0.9556 (1.1772)	grad_norm 2.6260 (2.2172)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:46:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:10:10 lr 0.000006	 wd 0.0500	time 0.4595 (0.4688)	loss 1.2632 (1.1741)	grad_norm 3.5864 (2.2803)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:47:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:09:22 lr 0.000006	 wd 0.0500	time 0.4515 (0.4682)	loss 1.3957 (1.1740)	grad_norm 1.2295 (2.2954)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:48:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:08:35 lr 0.000006	 wd 0.0500	time 0.4518 (0.4677)	loss 1.0822 (1.1761)	grad_norm 2.8678 (2.2815)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:49:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:07:48 lr 0.000006	 wd 0.0500	time 0.4505 (0.4672)	loss 1.3940 (1.1776)	grad_norm 2.9626 (2.2858)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:50:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:07:01 lr 0.000006	 wd 0.0500	time 0.4503 (0.4668)	loss 1.2498 (1.1797)	grad_norm 4.7448 (2.2881)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:50:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:06:14 lr 0.000006	 wd 0.0500	time 0.4507 (0.4665)	loss 1.5336 (1.1804)	grad_norm 1.6031 (2.2792)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:51:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:05:27 lr 0.000006	 wd 0.0500	time 0.4505 (0.4662)	loss 0.8020 (1.1781)	grad_norm 1.7655 (2.2743)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:52:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:04:40 lr 0.000006	 wd 0.0500	time 0.4504 (0.4659)	loss 0.8320 (1.1788)	grad_norm 1.4575 (2.2702)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 04:53:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:03:53 lr 0.000006	 wd 0.0500	time 0.4520 (0.4656)	loss 1.1088 (1.1782)	grad_norm 2.0763 (2.2697)	loss_scale 4096.0000 (2084.8456)	mem 18344MB
[2024-08-01 04:53:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:03:07 lr 0.000006	 wd 0.0500	time 0.4479 (0.4654)	loss 1.3600 (1.1785)	grad_norm 1.3353 (2.2507)	loss_scale 4096.0000 (2180.5693)	mem 18344MB
[2024-08-01 04:54:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:02:20 lr 0.000006	 wd 0.0500	time 0.4508 (0.4652)	loss 1.3690 (1.1797)	grad_norm 1.7737 (2.2444)	loss_scale 4096.0000 (2267.5947)	mem 18344MB
[2024-08-01 04:55:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:33 lr 0.000006	 wd 0.0500	time 0.4509 (0.4650)	loss 0.8377 (1.1810)	grad_norm 1.5128 (2.2271)	loss_scale 4096.0000 (2347.0561)	mem 18344MB
[2024-08-01 04:56:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:47 lr 0.000006	 wd 0.0500	time 0.4525 (0.4649)	loss 0.7406 (1.1805)	grad_norm 1.7910 (2.2168)	loss_scale 4096.0000 (2419.8984)	mem 18344MB
[2024-08-01 04:56:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.4526 (0.4647)	loss 1.1537 (1.1824)	grad_norm 2.5794 (2.2063)	loss_scale 4096.0000 (2486.9156)	mem 18344MB
[2024-08-01 04:57:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 20 training takes 0:19:26
[2024-08-01 04:57:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.257 (11.257)	Loss 0.4951 (0.4951)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-08-01 04:57:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.866 Acc@5 97.794
[2024-08-01 04:57:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 04:57:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 04:57:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:19:17 lr 0.000006	 wd 0.0500	time 11.9734 (11.9734)	loss 0.9056 (0.9056)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 04:58:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:22:49 lr 0.000006	 wd 0.0500	time 0.4466 (0.5702)	loss 1.3949 (1.1840)	grad_norm 7.2207 (2.4134)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 04:59:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:19:42 lr 0.000006	 wd 0.0500	time 0.4464 (0.5138)	loss 1.3373 (1.1870)	grad_norm 1.6638 (2.3658)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:00:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:18:10 lr 0.000006	 wd 0.0500	time 0.4506 (0.4951)	loss 1.1344 (1.1819)	grad_norm 2.1822 (2.2905)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:00:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:17:01 lr 0.000006	 wd 0.0500	time 0.4487 (0.4858)	loss 1.4924 (1.1726)	grad_norm 2.2101 (2.2431)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:01:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:16:01 lr 0.000006	 wd 0.0500	time 0.4527 (0.4804)	loss 0.8631 (1.1678)	grad_norm 1.6124 (2.2637)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:02:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:15:07 lr 0.000006	 wd 0.0500	time 0.4522 (0.4769)	loss 1.3396 (1.1640)	grad_norm 1.8177 (2.2341)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:03:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:14:15 lr 0.000006	 wd 0.0500	time 0.4515 (0.4746)	loss 1.3084 (1.1660)	grad_norm 1.4293 (2.3105)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:03:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:13:24 lr 0.000006	 wd 0.0500	time 0.4477 (0.4728)	loss 1.3109 (1.1653)	grad_norm 2.0432 (2.2595)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:04:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:12:35 lr 0.000005	 wd 0.0500	time 0.4527 (0.4715)	loss 1.3823 (1.1694)	grad_norm 2.4187 (2.2213)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:05:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:11:46 lr 0.000005	 wd 0.0500	time 0.4490 (0.4704)	loss 1.4173 (1.1733)	grad_norm 1.2628 (2.2035)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:06:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:10:58 lr 0.000005	 wd 0.0500	time 0.4500 (0.4695)	loss 0.8315 (1.1768)	grad_norm 1.6391 (2.2341)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:07:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:10:10 lr 0.000005	 wd 0.0500	time 0.4476 (0.4688)	loss 1.3278 (1.1732)	grad_norm 1.9644 (2.2438)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:07:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:09:22 lr 0.000005	 wd 0.0500	time 0.4522 (0.4682)	loss 1.3317 (1.1733)	grad_norm 1.4472 (2.2253)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:08:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:08:35 lr 0.000005	 wd 0.0500	time 0.4531 (0.4677)	loss 0.8547 (1.1712)	grad_norm 5.6815 (2.2281)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:09:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:07:48 lr 0.000005	 wd 0.0500	time 0.4537 (0.4672)	loss 0.8033 (1.1725)	grad_norm 1.4805 (2.2243)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:10:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:07:01 lr 0.000005	 wd 0.0500	time 0.4521 (0.4668)	loss 1.1166 (1.1692)	grad_norm 2.4044 (2.2466)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:10:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:06:14 lr 0.000005	 wd 0.0500	time 0.4557 (0.4665)	loss 1.4044 (1.1692)	grad_norm 1.4234 (2.2383)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:11:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:05:27 lr 0.000005	 wd 0.0500	time 0.4525 (0.4662)	loss 0.8417 (1.1692)	grad_norm 2.0384 (2.2176)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:12:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:04:40 lr 0.000005	 wd 0.0500	time 0.4493 (0.4659)	loss 0.7418 (1.1708)	grad_norm 1.5161 (2.1990)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:13:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:03:53 lr 0.000005	 wd 0.0500	time 0.4493 (0.4656)	loss 1.1608 (1.1702)	grad_norm 1.5100 (2.2014)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:13:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:03:07 lr 0.000005	 wd 0.0500	time 0.4489 (0.4654)	loss 1.2893 (1.1698)	grad_norm 2.1052 (2.1968)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:14:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:02:20 lr 0.000005	 wd 0.0500	time 0.4518 (0.4652)	loss 1.4436 (1.1682)	grad_norm 3.5370 (2.1930)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:15:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:33 lr 0.000005	 wd 0.0500	time 0.4540 (0.4650)	loss 1.4476 (1.1705)	grad_norm 2.1455 (2.1784)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:16:13 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:47 lr 0.000005	 wd 0.0500	time 0.4538 (0.4649)	loss 1.1401 (1.1715)	grad_norm 1.5893 (2.1964)	loss_scale 4096.0000 (4096.0000)	mem 18344MB
[2024-08-01 05:16:59 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.4545 (0.4647)	loss 1.4805 (1.1710)	grad_norm 2.2428 (nan)	loss_scale 2048.0000 (4035.4034)	mem 18344MB
[2024-08-01 05:17:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 21 training takes 0:19:26
[2024-08-01 05:17:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.149 (12.149)	Loss 0.5112 (0.5112)	Acc@1 93.359 (93.359)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 05:17:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.862 Acc@5 97.734
[2024-08-01 05:17:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 05:17:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 05:17:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:15:11 lr 0.000005	 wd 0.0500	time 11.8750 (11.8750)	loss 1.0259 (1.0259)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:18:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:22:48 lr 0.000005	 wd 0.0500	time 0.4450 (0.5699)	loss 1.3543 (1.1818)	grad_norm 1.8721 (1.8547)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:19:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:19:42 lr 0.000005	 wd 0.0500	time 0.4463 (0.5136)	loss 0.7241 (1.1933)	grad_norm 1.5667 (1.9151)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:20:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:18:09 lr 0.000005	 wd 0.0500	time 0.4482 (0.4950)	loss 1.5675 (1.1950)	grad_norm 1.6569 (2.0106)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:20:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:17:01 lr 0.000005	 wd 0.0500	time 0.4466 (0.4858)	loss 0.7850 (1.1950)	grad_norm 1.3001 (2.0396)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:21:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:16:01 lr 0.000005	 wd 0.0500	time 0.4504 (0.4804)	loss 1.1340 (1.1913)	grad_norm 1.9131 (2.0446)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:22:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:15:07 lr 0.000005	 wd 0.0500	time 0.4502 (0.4770)	loss 0.8709 (1.1907)	grad_norm 2.0515 (2.0877)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:23:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:14:15 lr 0.000005	 wd 0.0500	time 0.4491 (0.4746)	loss 1.4419 (1.1843)	grad_norm 1.9090 (2.0818)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:23:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:13:24 lr 0.000004	 wd 0.0500	time 0.4507 (0.4728)	loss 1.0705 (1.1846)	grad_norm 1.3572 (2.1045)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:24:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:12:35 lr 0.000004	 wd 0.0500	time 0.4537 (0.4715)	loss 1.1384 (1.1843)	grad_norm 2.6589 (2.1001)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:25:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:11:46 lr 0.000004	 wd 0.0500	time 0.4475 (0.4704)	loss 1.4148 (1.1849)	grad_norm 2.3975 (2.0849)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:26:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:10:58 lr 0.000004	 wd 0.0500	time 0.4492 (0.4696)	loss 0.7887 (1.1808)	grad_norm 1.4962 (2.0863)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:27:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:10:10 lr 0.000004	 wd 0.0500	time 0.4521 (0.4688)	loss 1.2980 (1.1814)	grad_norm 1.2908 (2.0698)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:27:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:09:22 lr 0.000004	 wd 0.0500	time 0.4513 (0.4682)	loss 0.8674 (1.1752)	grad_norm 2.1035 (2.0602)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:28:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:08:35 lr 0.000004	 wd 0.0500	time 0.4518 (0.4677)	loss 1.0429 (1.1702)	grad_norm 1.7616 (2.0897)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:29:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:07:48 lr 0.000004	 wd 0.0500	time 0.4519 (0.4673)	loss 1.1593 (1.1697)	grad_norm 1.3667 (2.0832)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:30:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:07:01 lr 0.000004	 wd 0.0500	time 0.4481 (0.4669)	loss 1.3928 (1.1691)	grad_norm 1.2843 (2.0959)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:30:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:06:14 lr 0.000004	 wd 0.0500	time 0.4526 (0.4665)	loss 0.9681 (1.1669)	grad_norm 1.7322 (2.0981)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:31:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:05:27 lr 0.000004	 wd 0.0500	time 0.4518 (0.4662)	loss 1.2585 (1.1656)	grad_norm 1.9584 (2.0885)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:32:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:04:40 lr 0.000004	 wd 0.0500	time 0.4541 (0.4659)	loss 0.9535 (1.1664)	grad_norm 1.4082 (2.0933)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:33:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:03:53 lr 0.000004	 wd 0.0500	time 0.4512 (0.4656)	loss 0.9162 (1.1677)	grad_norm 1.2270 (2.0860)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:33:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:03:07 lr 0.000004	 wd 0.0500	time 0.4538 (0.4654)	loss 1.2991 (1.1687)	grad_norm 1.4633 (2.1045)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:34:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:20 lr 0.000004	 wd 0.0500	time 0.4506 (0.4652)	loss 1.1258 (1.1685)	grad_norm 1.7402 (2.1061)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:35:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:33 lr 0.000004	 wd 0.0500	time 0.4522 (0.4650)	loss 1.2436 (1.1689)	grad_norm 1.3648 (2.1139)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:36:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:47 lr 0.000004	 wd 0.0500	time 0.4515 (0.4648)	loss 1.4764 (1.1705)	grad_norm 1.6671 (2.0977)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:37:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.4514 (0.4647)	loss 0.9396 (1.1707)	grad_norm 1.3360 (2.0881)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:37:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 22 training takes 0:19:27
[2024-08-01 05:37:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.777 (11.777)	Loss 0.4944 (0.4944)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 05:37:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.860 Acc@5 97.754
[2024-08-01 05:37:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 05:37:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 05:37:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:16:42 lr 0.000004	 wd 0.0500	time 11.9116 (11.9116)	loss 0.7852 (0.7852)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:38:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:22:52 lr 0.000004	 wd 0.0500	time 0.4442 (0.5713)	loss 1.4020 (1.1610)	grad_norm 1.7731 (2.0504)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:39:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:19:44 lr 0.000004	 wd 0.0500	time 0.4487 (0.5143)	loss 0.9232 (1.1693)	grad_norm 2.1223 (1.9714)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:40:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:18:10 lr 0.000004	 wd 0.0500	time 0.4507 (0.4954)	loss 1.3765 (1.1581)	grad_norm 2.1410 (2.1266)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:40:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:17:01 lr 0.000004	 wd 0.0500	time 0.4452 (0.4860)	loss 0.9380 (1.1641)	grad_norm 3.5945 (2.0623)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:41:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:16:01 lr 0.000004	 wd 0.0500	time 0.4487 (0.4805)	loss 0.9247 (1.1644)	grad_norm 2.7849 (2.0407)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:42:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:15:07 lr 0.000004	 wd 0.0500	time 0.4504 (0.4769)	loss 1.4106 (1.1633)	grad_norm 4.0515 (2.1514)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:43:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:14:15 lr 0.000004	 wd 0.0500	time 0.4496 (0.4745)	loss 1.2644 (1.1657)	grad_norm 1.8750 (2.1472)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:44:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:13:24 lr 0.000003	 wd 0.0500	time 0.4532 (0.4728)	loss 1.3894 (1.1681)	grad_norm 1.4053 (2.2130)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:44:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:12:35 lr 0.000003	 wd 0.0500	time 0.4514 (0.4715)	loss 1.1479 (1.1690)	grad_norm 3.5601 (2.2389)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:45:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:11:46 lr 0.000003	 wd 0.0500	time 0.4490 (0.4704)	loss 1.1966 (1.1650)	grad_norm 1.2829 (2.2584)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:46:19 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:10:58 lr 0.000003	 wd 0.0500	time 0.4487 (0.4695)	loss 1.0261 (1.1670)	grad_norm 1.6693 (2.2165)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:47:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:10:10 lr 0.000003	 wd 0.0500	time 0.4514 (0.4688)	loss 1.3549 (1.1697)	grad_norm 1.8694 (2.1891)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:47:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:09:22 lr 0.000003	 wd 0.0500	time 0.4499 (0.4682)	loss 1.2701 (1.1706)	grad_norm 1.7604 (2.1772)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:48:37 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:08:35 lr 0.000003	 wd 0.0500	time 0.4515 (0.4676)	loss 1.3843 (1.1707)	grad_norm 3.2803 (2.1784)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:49:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:07:48 lr 0.000003	 wd 0.0500	time 0.4555 (0.4672)	loss 1.3892 (1.1692)	grad_norm 1.8288 (2.1800)	loss_scale 4096.0000 (2154.4250)	mem 18344MB
[2024-08-01 05:50:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:07:01 lr 0.000003	 wd 0.0500	time 0.4540 (0.4668)	loss 1.2278 (1.1722)	grad_norm 3.1060 (2.2054)	loss_scale 4096.0000 (2275.6977)	mem 18344MB
[2024-08-01 05:50:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:06:14 lr 0.000003	 wd 0.0500	time 0.4510 (0.4664)	loss 0.7472 (1.1693)	grad_norm 2.3860 (2.2139)	loss_scale 4096.0000 (2382.7113)	mem 18344MB
[2024-08-01 05:51:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:05:27 lr 0.000003	 wd 0.0500	time 0.4498 (0.4662)	loss 1.4246 (1.1687)	grad_norm 5.9639 (2.2128)	loss_scale 4096.0000 (2477.8412)	mem 18344MB
[2024-08-01 05:52:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:04:40 lr 0.000003	 wd 0.0500	time 0.4500 (0.4659)	loss 1.3786 (1.1681)	grad_norm 1.7568 (2.2027)	loss_scale 4096.0000 (2562.9627)	mem 18344MB
[2024-08-01 05:53:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:03:53 lr 0.000003	 wd 0.0500	time 0.4516 (0.4656)	loss 0.8334 (1.1673)	grad_norm 2.1527 (2.1872)	loss_scale 4096.0000 (2639.5762)	mem 18344MB
[2024-08-01 05:54:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:03:07 lr 0.000003	 wd 0.0500	time 0.4560 (0.4654)	loss 1.2709 (1.1681)	grad_norm 1.9900 (2.1707)	loss_scale 4096.0000 (2708.8967)	mem 18344MB
[2024-08-01 05:54:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:02:20 lr 0.000003	 wd 0.0500	time 0.4522 (0.4652)	loss 1.3443 (1.1671)	grad_norm 1.7370 (inf)	loss_scale 2048.0000 (2727.2549)	mem 18344MB
[2024-08-01 05:55:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:33 lr 0.000003	 wd 0.0500	time 0.4519 (0.4650)	loss 1.4475 (1.1666)	grad_norm 1.3684 (inf)	loss_scale 2048.0000 (2697.7349)	mem 18344MB
[2024-08-01 05:56:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:47 lr 0.000003	 wd 0.0500	time 0.4532 (0.4648)	loss 1.0032 (1.1663)	grad_norm 1.6287 (inf)	loss_scale 2048.0000 (2670.6739)	mem 18344MB
[2024-08-01 05:57:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.4503 (0.4647)	loss 0.8781 (1.1651)	grad_norm 4.0244 (inf)	loss_scale 2048.0000 (2645.7769)	mem 18344MB
[2024-08-01 05:57:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 23 training takes 0:19:28
[2024-08-01 05:57:23 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.869 (11.869)	Loss 0.5122 (0.5122)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-08-01 05:57:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.850 Acc@5 97.746
[2024-08-01 05:57:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 05:57:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 05:58:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:28:56 lr 0.000003	 wd 0.0500	time 12.2048 (12.2048)	loss 1.2905 (1.2905)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:58:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:22:54 lr 0.000003	 wd 0.0500	time 0.4508 (0.5721)	loss 0.9851 (1.1810)	grad_norm 1.5296 (2.2145)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 05:59:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:19:44 lr 0.000003	 wd 0.0500	time 0.4486 (0.5146)	loss 1.1474 (1.1715)	grad_norm 1.3317 (2.0932)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:00:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:18:11 lr 0.000003	 wd 0.0500	time 0.4522 (0.4956)	loss 1.3828 (1.1838)	grad_norm 1.4601 (2.0714)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:01:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:17:01 lr 0.000003	 wd 0.0500	time 0.4460 (0.4862)	loss 0.7032 (1.1819)	grad_norm 1.9157 (2.1086)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:01:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:16:02 lr 0.000003	 wd 0.0500	time 0.4496 (0.4806)	loss 1.3409 (1.1827)	grad_norm 1.7175 (2.0731)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:02:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:15:07 lr 0.000003	 wd 0.0500	time 0.4512 (0.4770)	loss 0.8795 (1.1780)	grad_norm 2.1151 (2.0394)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:03:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:14:15 lr 0.000003	 wd 0.0500	time 0.4511 (0.4746)	loss 1.3503 (1.1756)	grad_norm 2.0385 (2.0082)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:04:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:13:24 lr 0.000003	 wd 0.0500	time 0.4524 (0.4728)	loss 1.2901 (1.1767)	grad_norm 1.3460 (1.9909)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:04:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:12:35 lr 0.000003	 wd 0.0500	time 0.4502 (0.4714)	loss 0.9391 (1.1810)	grad_norm 1.6655 (2.0002)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:05:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:11:46 lr 0.000003	 wd 0.0500	time 0.4543 (0.4704)	loss 0.8811 (1.1784)	grad_norm 1.3369 (2.0291)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:06:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:10:58 lr 0.000003	 wd 0.0500	time 0.4514 (0.4695)	loss 0.8791 (1.1767)	grad_norm 1.1905 (2.0420)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:07:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:10:10 lr 0.000002	 wd 0.0500	time 0.4510 (0.4688)	loss 0.9576 (1.1739)	grad_norm 1.5624 (2.0317)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:07:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:09:22 lr 0.000002	 wd 0.0500	time 0.4523 (0.4681)	loss 0.8291 (1.1714)	grad_norm 1.4952 (2.0331)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:08:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:08:35 lr 0.000002	 wd 0.0500	time 0.4520 (0.4676)	loss 1.4659 (1.1727)	grad_norm 1.9936 (2.0524)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:09:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:07:48 lr 0.000002	 wd 0.0500	time 0.4506 (0.4671)	loss 1.2644 (1.1690)	grad_norm 2.6256 (2.0995)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:10:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:07:01 lr 0.000002	 wd 0.0500	time 0.4543 (0.4667)	loss 1.3067 (1.1677)	grad_norm 1.7747 (2.0848)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:11:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:06:14 lr 0.000002	 wd 0.0500	time 0.4506 (0.4664)	loss 1.6032 (1.1691)	grad_norm 1.5523 (2.0722)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:11:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:05:27 lr 0.000002	 wd 0.0500	time 0.4620 (0.4661)	loss 1.1918 (1.1695)	grad_norm 3.3084 (2.0636)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:12:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:04:40 lr 0.000002	 wd 0.0500	time 0.4524 (0.4658)	loss 0.8755 (1.1703)	grad_norm 2.0518 (2.0772)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:13:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:03:53 lr 0.000002	 wd 0.0500	time 0.4529 (0.4655)	loss 1.2909 (1.1699)	grad_norm 1.5836 (2.0802)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:14:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:03:07 lr 0.000002	 wd 0.0500	time 0.4533 (0.4653)	loss 1.0415 (1.1696)	grad_norm 1.5124 (2.0805)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:14:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:02:20 lr 0.000002	 wd 0.0500	time 0.4500 (0.4651)	loss 0.9626 (1.1712)	grad_norm 2.1248 (2.0778)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:15:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:33 lr 0.000002	 wd 0.0500	time 0.4519 (0.4649)	loss 1.3245 (1.1723)	grad_norm 2.4058 (2.0741)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:16:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:47 lr 0.000002	 wd 0.0500	time 0.4515 (0.4647)	loss 1.4127 (1.1700)	grad_norm 1.6454 (2.0740)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:17:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.4515 (0.4646)	loss 0.8050 (1.1703)	grad_norm 1.8597 (2.0743)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:17:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 24 training takes 0:19:26
[2024-08-01 06:17:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.845 (12.845)	Loss 0.4951 (0.4951)	Acc@1 93.359 (93.359)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 06:17:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.856 Acc@5 97.758
[2024-08-01 06:17:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 06:17:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 06:18:05 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:05:04 lr 0.000002	 wd 0.0500	time 11.6325 (11.6325)	loss 1.3608 (1.3608)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:18:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:22:48 lr 0.000002	 wd 0.0500	time 0.4489 (0.5698)	loss 1.2333 (1.2376)	grad_norm 2.0709 (2.7662)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:19:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:19:42 lr 0.000002	 wd 0.0500	time 0.4507 (0.5135)	loss 1.4346 (1.2021)	grad_norm 2.9973 (2.4379)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:20:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:18:09 lr 0.000002	 wd 0.0500	time 0.4481 (0.4948)	loss 1.1952 (1.1857)	grad_norm 1.4473 (2.2054)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:21:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:17:00 lr 0.000002	 wd 0.0500	time 0.4488 (0.4856)	loss 1.2258 (1.1833)	grad_norm 1.4447 (2.1918)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:21:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:16:01 lr 0.000002	 wd 0.0500	time 0.4463 (0.4801)	loss 1.3267 (1.1810)	grad_norm 1.2977 (2.2515)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:22:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:15:06 lr 0.000002	 wd 0.0500	time 0.4477 (0.4766)	loss 0.9196 (1.1718)	grad_norm 2.7629 (2.2649)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:23:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:14:14 lr 0.000002	 wd 0.0500	time 0.4492 (0.4742)	loss 0.8019 (1.1733)	grad_norm 2.0039 (2.2988)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:24:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:13:24 lr 0.000002	 wd 0.0500	time 0.4474 (0.4724)	loss 1.4061 (1.1709)	grad_norm 2.6548 (2.3705)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:24:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:12:34 lr 0.000002	 wd 0.0500	time 0.4521 (0.4711)	loss 1.3063 (1.1711)	grad_norm 1.3679 (2.3206)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:25:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:11:45 lr 0.000002	 wd 0.0500	time 0.4509 (0.4700)	loss 1.6202 (1.1719)	grad_norm 1.7303 (2.2848)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:26:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:10:57 lr 0.000002	 wd 0.0500	time 0.4498 (0.4692)	loss 1.4709 (1.1740)	grad_norm 1.7150 (2.2472)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:27:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:10:09 lr 0.000002	 wd 0.0500	time 0.4511 (0.4685)	loss 0.9737 (1.1706)	grad_norm 1.4499 (2.2266)	loss_scale 4096.0000 (2136.6728)	mem 18344MB
[2024-08-01 06:28:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:09:22 lr 0.000002	 wd 0.0500	time 0.4504 (0.4679)	loss 0.9508 (1.1699)	grad_norm 3.8517 (2.2126)	loss_scale 4096.0000 (2287.2744)	mem 18344MB
[2024-08-01 06:28:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:08:35 lr 0.000002	 wd 0.0500	time 0.4535 (0.4674)	loss 1.0124 (1.1698)	grad_norm 2.8067 (2.2161)	loss_scale 4096.0000 (2416.3769)	mem 18344MB
[2024-08-01 06:29:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:07:47 lr 0.000002	 wd 0.0500	time 0.4505 (0.4670)	loss 0.8092 (1.1693)	grad_norm 1.5051 (2.1920)	loss_scale 4096.0000 (2528.2771)	mem 18344MB
[2024-08-01 06:30:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:07:00 lr 0.000002	 wd 0.0500	time 0.4503 (0.4666)	loss 0.8556 (1.1682)	grad_norm 2.4196 (2.1810)	loss_scale 4096.0000 (2626.1986)	mem 18344MB
[2024-08-01 06:31:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:06:13 lr 0.000002	 wd 0.0500	time 0.4501 (0.4663)	loss 0.7803 (1.1659)	grad_norm 1.2918 (2.1692)	loss_scale 4096.0000 (2712.6067)	mem 18344MB
[2024-08-01 06:31:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:05:27 lr 0.000002	 wd 0.0500	time 0.4492 (0.4660)	loss 0.8547 (1.1677)	grad_norm 1.2862 (inf)	loss_scale 2048.0000 (2677.9789)	mem 18344MB
[2024-08-01 06:32:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:04:40 lr 0.000002	 wd 0.0500	time 0.4514 (0.4657)	loss 1.4876 (1.1696)	grad_norm 1.6039 (inf)	loss_scale 2048.0000 (2644.8396)	mem 18344MB
[2024-08-01 06:33:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:03:53 lr 0.000002	 wd 0.0500	time 0.4527 (0.4654)	loss 0.9620 (1.1697)	grad_norm 1.2646 (inf)	loss_scale 2048.0000 (2615.0125)	mem 18344MB
[2024-08-01 06:34:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:03:07 lr 0.000002	 wd 0.0500	time 0.4532 (0.4652)	loss 1.3493 (1.1706)	grad_norm 1.5353 (inf)	loss_scale 2048.0000 (2588.0248)	mem 18344MB
[2024-08-01 06:34:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:02:20 lr 0.000001	 wd 0.0500	time 0.4547 (0.4650)	loss 1.0791 (1.1710)	grad_norm 1.5584 (inf)	loss_scale 2048.0000 (2563.4893)	mem 18344MB
[2024-08-01 06:35:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:33 lr 0.000001	 wd 0.0500	time 0.4512 (0.4648)	loss 1.3459 (1.1709)	grad_norm 1.2231 (inf)	loss_scale 2048.0000 (2541.0865)	mem 18344MB
[2024-08-01 06:36:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:47 lr 0.000001	 wd 0.0500	time 0.4514 (0.4646)	loss 1.5547 (1.1704)	grad_norm 1.8617 (inf)	loss_scale 2048.0000 (2520.5498)	mem 18344MB
[2024-08-01 06:37:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.4499 (0.4645)	loss 1.0226 (1.1691)	grad_norm 1.6647 (inf)	loss_scale 2048.0000 (2501.6553)	mem 18344MB
[2024-08-01 06:37:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 25 training takes 0:19:26
[2024-08-01 06:37:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.448 (12.448)	Loss 0.5176 (0.5176)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-08-01 06:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.870 Acc@5 97.746
[2024-08-01 06:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 06:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 06:38:09 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:44:24 lr 0.000001	 wd 0.0500	time 11.1370 (11.1370)	loss 1.1065 (1.1065)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:38:55 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:22:49 lr 0.000001	 wd 0.0500	time 0.4454 (0.5700)	loss 1.3606 (1.1797)	grad_norm 1.7701 (1.8250)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:39:41 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:19:42 lr 0.000001	 wd 0.0500	time 0.4472 (0.5135)	loss 1.0921 (1.1750)	grad_norm 2.6425 (1.8710)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:40:27 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:18:09 lr 0.000001	 wd 0.0500	time 0.4489 (0.4947)	loss 0.9284 (1.1619)	grad_norm 1.9102 (1.9565)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:41:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:17:00 lr 0.000001	 wd 0.0500	time 0.4527 (0.4855)	loss 1.4712 (1.1618)	grad_norm 1.5874 (1.9705)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:41:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:16:01 lr 0.000001	 wd 0.0500	time 0.4498 (0.4801)	loss 1.3491 (1.1700)	grad_norm 2.5169 (1.9614)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:42:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:15:06 lr 0.000001	 wd 0.0500	time 0.4509 (0.4766)	loss 0.8025 (1.1724)	grad_norm 1.5913 (1.9837)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:43:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:14:14 lr 0.000001	 wd 0.0500	time 0.4506 (0.4742)	loss 1.2615 (1.1683)	grad_norm 1.2520 (1.9803)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:44:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:13:24 lr 0.000001	 wd 0.0500	time 0.4518 (0.4725)	loss 1.3526 (1.1726)	grad_norm 1.5514 (1.9661)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:45:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:12:34 lr 0.000001	 wd 0.0500	time 0.4472 (0.4712)	loss 1.2658 (1.1730)	grad_norm 1.7079 (2.0171)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:45:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:11:46 lr 0.000001	 wd 0.0500	time 0.4548 (0.4702)	loss 1.0174 (1.1720)	grad_norm 3.1760 (2.0749)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:46:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:10:57 lr 0.000001	 wd 0.0500	time 0.4499 (0.4693)	loss 0.8738 (1.1708)	grad_norm 1.2849 (2.0462)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:47:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:10:10 lr 0.000001	 wd 0.0500	time 0.4491 (0.4686)	loss 0.9676 (1.1715)	grad_norm 2.5356 (2.0753)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:48:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:09:22 lr 0.000001	 wd 0.0500	time 0.4523 (0.4680)	loss 1.3129 (1.1736)	grad_norm 2.0392 (2.0832)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:48:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:08:35 lr 0.000001	 wd 0.0500	time 0.4516 (0.4675)	loss 1.4455 (1.1753)	grad_norm 1.4970 (2.0685)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:49:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:07:47 lr 0.000001	 wd 0.0500	time 0.4412 (0.4670)	loss 1.3871 (1.1771)	grad_norm 2.0420 (2.0640)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:50:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:07:00 lr 0.000001	 wd 0.0500	time 0.4538 (0.4666)	loss 1.1637 (1.1757)	grad_norm 1.8991 (2.0843)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:51:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:06:13 lr 0.000001	 wd 0.0500	time 0.4514 (0.4663)	loss 1.1572 (1.1725)	grad_norm 2.2133 (2.1029)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:51:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:05:27 lr 0.000001	 wd 0.0500	time 0.4505 (0.4660)	loss 0.8033 (1.1717)	grad_norm 1.9738 (2.1085)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:52:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:04:40 lr 0.000001	 wd 0.0500	time 0.4520 (0.4658)	loss 1.3813 (1.1734)	grad_norm 1.4665 (2.0962)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:53:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:03:53 lr 0.000001	 wd 0.0500	time 0.4493 (0.4655)	loss 1.2201 (1.1722)	grad_norm 2.0206 (2.0982)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:54:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:03:07 lr 0.000001	 wd 0.0500	time 0.4528 (0.4653)	loss 1.3437 (1.1715)	grad_norm 2.3193 (2.0923)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:55:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:02:20 lr 0.000001	 wd 0.0500	time 0.4517 (0.4651)	loss 1.3113 (1.1699)	grad_norm 1.3225 (2.0935)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:55:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:33 lr 0.000001	 wd 0.0500	time 0.4534 (0.4649)	loss 1.3959 (1.1700)	grad_norm 1.8566 (2.0976)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:56:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:47 lr 0.000001	 wd 0.0500	time 0.4493 (0.4648)	loss 0.7686 (1.1705)	grad_norm 2.0561 (2.0959)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:57:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.4542 (0.4646)	loss 0.8483 (1.1707)	grad_norm 1.8604 (2.0973)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:57:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 26 training takes 0:19:28
[2024-08-01 06:57:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 11.818 (11.818)	Loss 0.5015 (0.5015)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 06:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.884 Acc@5 97.756
[2024-08-01 06:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 06:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 06:58:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:04:16 lr 0.000001	 wd 0.0500	time 11.6134 (11.6134)	loss 1.4100 (1.4100)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:59:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:22:55 lr 0.000001	 wd 0.0500	time 0.4492 (0.5726)	loss 1.2939 (1.1765)	grad_norm 1.5511 (1.7652)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 06:59:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:19:45 lr 0.000001	 wd 0.0500	time 0.4479 (0.5150)	loss 1.2475 (1.1839)	grad_norm 1.3646 (1.9737)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:00:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:18:11 lr 0.000001	 wd 0.0500	time 0.4470 (0.4957)	loss 1.3285 (1.1808)	grad_norm 1.4523 (1.9490)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:01:18 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:17:01 lr 0.000001	 wd 0.0500	time 0.4485 (0.4862)	loss 0.9918 (1.1802)	grad_norm 2.0465 (2.0380)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:02:04 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:16:02 lr 0.000001	 wd 0.0500	time 0.4477 (0.4806)	loss 0.7784 (1.1810)	grad_norm 1.6730 (2.0071)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:02:50 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:15:07 lr 0.000001	 wd 0.0500	time 0.4498 (0.4770)	loss 1.3789 (1.1802)	grad_norm 1.3582 (2.0339)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:03:36 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:14:14 lr 0.000001	 wd 0.0500	time 0.4500 (0.4745)	loss 0.7852 (1.1801)	grad_norm 1.7946 (2.0107)	loss_scale 4096.0000 (2053.8431)	mem 18344MB
[2024-08-01 07:04:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:13:24 lr 0.000001	 wd 0.0500	time 0.4497 (0.4727)	loss 1.1444 (1.1801)	grad_norm 1.3096 (2.0399)	loss_scale 4096.0000 (2308.7940)	mem 18344MB
[2024-08-01 07:05:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:12:35 lr 0.000001	 wd 0.0500	time 0.4493 (0.4714)	loss 1.3362 (1.1789)	grad_norm 1.1030 (2.0066)	loss_scale 4096.0000 (2507.1521)	mem 18344MB
[2024-08-01 07:05:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:11:46 lr 0.000001	 wd 0.0500	time 0.4481 (0.4703)	loss 0.9575 (1.1746)	grad_norm 1.4614 (2.0129)	loss_scale 4096.0000 (2665.8781)	mem 18344MB
[2024-08-01 07:06:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:10:58 lr 0.000001	 wd 0.0500	time 0.4520 (0.4695)	loss 1.1803 (1.1761)	grad_norm 1.4611 (2.0237)	loss_scale 4096.0000 (2795.7711)	mem 18344MB
[2024-08-01 07:07:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:10:10 lr 0.000001	 wd 0.0500	time 0.4516 (0.4687)	loss 0.8775 (1.1758)	grad_norm 3.4437 (2.0365)	loss_scale 4096.0000 (2904.0333)	mem 18344MB
[2024-08-01 07:08:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:09:22 lr 0.000001	 wd 0.0500	time 0.4513 (0.4681)	loss 0.8352 (1.1714)	grad_norm 2.0266 (2.0437)	loss_scale 4096.0000 (2995.6526)	mem 18344MB
[2024-08-01 07:08:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:08:35 lr 0.000001	 wd 0.0500	time 0.4504 (0.4676)	loss 0.8961 (1.1728)	grad_norm 1.3842 (2.0480)	loss_scale 4096.0000 (3074.1927)	mem 18344MB
[2024-08-01 07:09:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:07:48 lr 0.000001	 wd 0.0500	time 0.4512 (0.4672)	loss 1.4788 (1.1740)	grad_norm 1.6595 (2.0570)	loss_scale 4096.0000 (3142.2678)	mem 18344MB
[2024-08-01 07:10:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:07:01 lr 0.000001	 wd 0.0500	time 0.4527 (0.4668)	loss 1.5339 (1.1755)	grad_norm 2.1393 (2.0450)	loss_scale 4096.0000 (3201.8389)	mem 18344MB
[2024-08-01 07:11:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:06:14 lr 0.000001	 wd 0.0500	time 0.4533 (0.4664)	loss 1.3717 (1.1765)	grad_norm 2.0014 (2.0548)	loss_scale 4096.0000 (3254.4056)	mem 18344MB
[2024-08-01 07:12:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:05:27 lr 0.000001	 wd 0.0500	time 0.4514 (0.4661)	loss 1.3761 (1.1757)	grad_norm 1.7171 (2.0619)	loss_scale 4096.0000 (3301.1349)	mem 18344MB
[2024-08-01 07:12:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:04:40 lr 0.000001	 wd 0.0500	time 0.4508 (0.4658)	loss 1.3927 (1.1761)	grad_norm 2.5309 (nan)	loss_scale 2048.0000 (3252.4524)	mem 18344MB
[2024-08-01 07:13:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:03:53 lr 0.000001	 wd 0.0500	time 0.4517 (0.4656)	loss 1.4025 (1.1765)	grad_norm 1.6130 (nan)	loss_scale 2048.0000 (3192.2599)	mem 18344MB
[2024-08-01 07:14:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:03:07 lr 0.000001	 wd 0.0500	time 0.4557 (0.4654)	loss 1.5556 (1.1762)	grad_norm 2.0457 (nan)	loss_scale 2048.0000 (3137.7972)	mem 18344MB
[2024-08-01 07:15:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:02:20 lr 0.000001	 wd 0.0500	time 0.4499 (0.4651)	loss 1.2635 (1.1769)	grad_norm 1.3599 (nan)	loss_scale 2048.0000 (3088.2835)	mem 18344MB
[2024-08-01 07:15:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:33 lr 0.000001	 wd 0.0500	time 0.4492 (0.4650)	loss 1.3256 (1.1769)	grad_norm 1.7547 (nan)	loss_scale 2048.0000 (3043.0734)	mem 18344MB
[2024-08-01 07:16:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:47 lr 0.000001	 wd 0.0500	time 0.4507 (0.4648)	loss 1.3826 (1.1783)	grad_norm 2.4115 (nan)	loss_scale 2048.0000 (3001.6293)	mem 18344MB
[2024-08-01 07:17:25 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.4499 (0.4646)	loss 1.1787 (1.1774)	grad_norm 2.0242 (nan)	loss_scale 2048.0000 (2963.4994)	mem 18344MB
[2024-08-01 07:17:32 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 27 training takes 0:19:28
[2024-08-01 07:17:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.520 (12.520)	Loss 0.4902 (0.4902)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 07:18:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.902 Acc@5 97.770
[2024-08-01 07:18:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 07:18:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 07:18:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saving......
[2024-08-01 07:18:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-01 07:18:22 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:19:27 lr 0.000001	 wd 0.0500	time 10.5385 (10.5385)	loss 0.7733 (0.7733)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:19:08 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:22:30 lr 0.000000	 wd 0.0500	time 0.4474 (0.5622)	loss 0.7892 (1.1978)	grad_norm 2.0263 (1.9028)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:19:54 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:19:33 lr 0.000000	 wd 0.0500	time 0.4477 (0.5097)	loss 1.3988 (1.1927)	grad_norm 1.3423 (1.9194)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:20:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:18:03 lr 0.000000	 wd 0.0500	time 0.4494 (0.4922)	loss 1.4427 (1.1856)	grad_norm 3.7715 (1.9123)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:21:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:16:56 lr 0.000000	 wd 0.0500	time 0.4488 (0.4836)	loss 0.7944 (1.1770)	grad_norm 1.4600 (2.0316)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:22:11 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:15:58 lr 0.000000	 wd 0.0500	time 0.4484 (0.4785)	loss 0.8196 (1.1824)	grad_norm 1.8163 (2.1065)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:22:57 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:15:03 lr 0.000000	 wd 0.0500	time 0.4514 (0.4753)	loss 0.8987 (1.1750)	grad_norm 1.6294 (2.1061)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:23:43 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:14:12 lr 0.000000	 wd 0.0500	time 0.4543 (0.4730)	loss 1.2286 (1.1716)	grad_norm 2.6594 (2.0883)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:24:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:13:22 lr 0.000000	 wd 0.0500	time 0.4542 (0.4715)	loss 0.8545 (1.1731)	grad_norm 1.7778 (2.0871)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:25:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:12:33 lr 0.000000	 wd 0.0500	time 0.4496 (0.4703)	loss 0.7908 (1.1696)	grad_norm 2.4096 (2.0485)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:26:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:11:44 lr 0.000000	 wd 0.0500	time 0.4521 (0.4693)	loss 1.0598 (1.1711)	grad_norm 2.2611 (2.1174)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:26:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:10:56 lr 0.000000	 wd 0.0500	time 0.4517 (0.4685)	loss 1.2093 (1.1725)	grad_norm 1.3712 (2.1183)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:27:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:10:09 lr 0.000000	 wd 0.0500	time 0.4531 (0.4679)	loss 1.3980 (1.1732)	grad_norm 2.6957 (2.1222)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:28:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:09:21 lr 0.000000	 wd 0.0500	time 0.4502 (0.4674)	loss 1.4367 (1.1753)	grad_norm 2.3849 (2.1214)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:29:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:08:34 lr 0.000000	 wd 0.0500	time 0.4523 (0.4669)	loss 0.8837 (1.1739)	grad_norm 1.4601 (2.0964)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:29:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:07:47 lr 0.000000	 wd 0.0500	time 0.4506 (0.4665)	loss 1.1917 (1.1736)	grad_norm 1.2939 (2.1011)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:30:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:07:00 lr 0.000000	 wd 0.0500	time 0.4550 (0.4662)	loss 1.4512 (1.1759)	grad_norm 1.7621 (2.2074)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:31:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:06:13 lr 0.000000	 wd 0.0500	time 0.4504 (0.4659)	loss 1.4039 (1.1758)	grad_norm 1.8146 (2.2350)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:32:10 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:05:26 lr 0.000000	 wd 0.0500	time 0.4493 (0.4656)	loss 0.7869 (1.1755)	grad_norm 1.6395 (2.2193)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:32:56 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:04:40 lr 0.000000	 wd 0.0500	time 0.4504 (0.4653)	loss 1.0436 (1.1728)	grad_norm 1.8051 (2.2015)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:33:42 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:03:53 lr 0.000000	 wd 0.0500	time 0.4496 (0.4651)	loss 1.1626 (1.1728)	grad_norm 2.3989 (2.2222)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:34:28 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:03:06 lr 0.000000	 wd 0.0500	time 0.4500 (0.4649)	loss 1.0531 (1.1734)	grad_norm 1.8274 (2.2131)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:35:15 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:02:20 lr 0.000000	 wd 0.0500	time 0.4498 (0.4647)	loss 1.3336 (1.1729)	grad_norm 3.5751 (2.2076)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:36:01 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:33 lr 0.000000	 wd 0.0500	time 0.4419 (0.4646)	loss 1.3426 (1.1727)	grad_norm 2.1611 (2.2023)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:36:47 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:47 lr 0.000000	 wd 0.0500	time 0.4501 (0.4644)	loss 1.5699 (1.1718)	grad_norm 2.9132 (2.1902)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:37:33 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.4502 (0.4642)	loss 1.0675 (1.1710)	grad_norm 1.3100 (2.1955)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:37:38 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 28 training takes 0:19:26
[2024-08-01 07:37:51 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 12.497 (12.497)	Loss 0.4971 (0.4971)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 18344MB
[2024-08-01 07:38:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.872 Acc@5 97.760
[2024-08-01 07:38:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 07:38:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 07:38:29 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:25:27 lr 0.000000	 wd 0.0500	time 12.1215 (12.1215)	loss 1.3856 (1.3856)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:39:14 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:22:53 lr 0.000000	 wd 0.0500	time 0.4438 (0.5718)	loss 1.4314 (1.1876)	grad_norm 2.0418 (2.5462)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:40:00 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:19:44 lr 0.000000	 wd 0.0500	time 0.4477 (0.5146)	loss 0.8220 (1.1877)	grad_norm 2.0731 (2.2492)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:40:46 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:18:11 lr 0.000000	 wd 0.0500	time 0.4476 (0.4956)	loss 0.7217 (1.1758)	grad_norm 10.3566 (2.2288)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:41:31 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:17:01 lr 0.000000	 wd 0.0500	time 0.4501 (0.4861)	loss 1.0540 (1.1721)	grad_norm 1.9530 (2.2143)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:42:17 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:16:02 lr 0.000000	 wd 0.0500	time 0.4486 (0.4806)	loss 1.2305 (1.1679)	grad_norm 1.5721 (2.2231)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:43:03 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:15:07 lr 0.000000	 wd 0.0500	time 0.4498 (0.4770)	loss 0.7256 (1.1699)	grad_norm 2.5449 (2.1902)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:43:49 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:14:15 lr 0.000000	 wd 0.0500	time 0.4495 (0.4745)	loss 1.1798 (1.1699)	grad_norm 1.9832 (2.1546)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:44:35 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:13:24 lr 0.000000	 wd 0.0500	time 0.4535 (0.4728)	loss 0.8709 (1.1714)	grad_norm 1.6701 (2.1498)	loss_scale 2048.0000 (2048.0000)	mem 18344MB
[2024-08-01 07:45:21 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:12:35 lr 0.000000	 wd 0.0500	time 0.4517 (0.4715)	loss 1.4484 (1.1726)	grad_norm 3.4603 (2.1453)	loss_scale 4096.0000 (2248.0266)	mem 18344MB
[2024-08-01 07:46:07 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:11:46 lr 0.000000	 wd 0.0500	time 0.4514 (0.4704)	loss 1.3157 (1.1727)	grad_norm 3.0895 (2.1397)	loss_scale 4096.0000 (2432.6394)	mem 18344MB
[2024-08-01 07:46:53 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:10:58 lr 0.000000	 wd 0.0500	time 0.4539 (0.4695)	loss 1.3039 (1.1705)	grad_norm 1.4788 (2.1328)	loss_scale 4096.0000 (2583.7166)	mem 18344MB
[2024-08-01 07:47:40 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:10:10 lr 0.000000	 wd 0.0500	time 0.4532 (0.4688)	loss 0.9726 (1.1694)	grad_norm 1.5584 (2.1178)	loss_scale 4096.0000 (2709.6353)	mem 18344MB
[2024-08-01 07:48:26 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:09:22 lr 0.000000	 wd 0.0500	time 0.4510 (0.4682)	loss 1.2231 (1.1731)	grad_norm 1.8925 (2.1072)	loss_scale 4096.0000 (2816.1968)	mem 18344MB
[2024-08-01 07:49:12 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:08:35 lr 0.000000	 wd 0.0500	time 0.4484 (0.4677)	loss 1.1141 (1.1681)	grad_norm 2.3494 (2.1441)	loss_scale 4096.0000 (2907.5460)	mem 18344MB
[2024-08-01 07:49:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:07:48 lr 0.000000	 wd 0.0500	time 0.4482 (0.4672)	loss 0.9995 (1.1672)	grad_norm 2.4181 (2.1322)	loss_scale 4096.0000 (2986.7235)	mem 18344MB
[2024-08-01 07:50:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:07:01 lr 0.000000	 wd 0.0500	time 0.4527 (0.4668)	loss 0.8243 (1.1699)	grad_norm 1.3817 (2.1257)	loss_scale 4096.0000 (3056.0100)	mem 18344MB
[2024-08-01 07:51:30 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:06:14 lr 0.000000	 wd 0.0500	time 0.4503 (0.4665)	loss 1.2406 (1.1721)	grad_norm 1.9521 (2.1257)	loss_scale 4096.0000 (3117.1499)	mem 18344MB
[2024-08-01 07:52:16 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:05:27 lr 0.000000	 wd 0.0500	time 0.4524 (0.4662)	loss 1.3153 (1.1713)	grad_norm 1.7842 (2.1178)	loss_scale 4096.0000 (3171.5003)	mem 18344MB
[2024-08-01 07:53:02 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:04:40 lr 0.000000	 wd 0.0500	time 0.4522 (0.4659)	loss 0.8966 (1.1715)	grad_norm 2.2531 (2.1245)	loss_scale 4096.0000 (3220.1326)	mem 18344MB
[2024-08-01 07:53:48 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:03:53 lr 0.000000	 wd 0.0500	time 0.4511 (0.4656)	loss 0.9054 (1.1735)	grad_norm 2.3649 (2.1150)	loss_scale 4096.0000 (3263.9040)	mem 18344MB
[2024-08-01 07:54:34 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:03:07 lr 0.000000	 wd 0.0500	time 0.4531 (0.4654)	loss 0.8643 (1.1747)	grad_norm 1.5547 (2.1269)	loss_scale 4096.0000 (3303.5088)	mem 18344MB
[2024-08-01 07:55:20 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:02:20 lr 0.000000	 wd 0.0500	time 0.4496 (0.4652)	loss 0.8572 (1.1753)	grad_norm 1.6328 (2.1305)	loss_scale 4096.0000 (3339.5148)	mem 18344MB
[2024-08-01 07:56:06 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:33 lr 0.000000	 wd 0.0500	time 0.4484 (0.4650)	loss 0.7296 (1.1724)	grad_norm 2.0084 (2.1284)	loss_scale 4096.0000 (3372.3911)	mem 18344MB
[2024-08-01 07:56:52 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:47 lr 0.000000	 wd 0.0500	time 0.4521 (0.4648)	loss 1.2828 (1.1726)	grad_norm 1.4151 (2.1400)	loss_scale 4096.0000 (3402.5289)	mem 18344MB
[2024-08-01 07:57:39 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.4521 (0.4646)	loss 1.2649 (1.1720)	grad_norm 2.0530 (2.1335)	loss_scale 4096.0000 (3430.2567)	mem 18344MB
[2024-08-01 07:57:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 249): INFO EPOCH 29 training takes 0:19:27
[2024-08-01 07:57:44 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_29.pth saving......
[2024-08-01 07:57:45 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_smt_l_step_stage1/ckpt_epoch_29.pth saved !!!
[2024-08-01 07:57:58 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 289): INFO Test: [0/98]	Time 13.000 (13.000)	Loss 0.4868 (0.4868)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 18344MB
[2024-08-01 07:58:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 296): INFO  * Acc@1 85.882 Acc@5 97.766
[2024-08-01 07:58:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 07:58:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-01 07:58:24 smt_diffusion_finetune_large_224_22kto1k_step_stage1] (main.py 189): INFO Training time 10:00:12
