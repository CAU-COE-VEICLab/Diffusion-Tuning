[2024-08-02 10:29:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/config.json
[2024-08-02 10:29:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_sequence1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_stage1
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_sequence_stage1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-02 10:29:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_sequence_stage1.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_sequence_stage1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-02 10:29:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_sequence1
[2024-08-02 10:29:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-02 10:29:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 113): INFO number of params: 6400744
[2024-08-02 10:29:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1, ignoring auto resume
[2024-08-02 10:29:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth for fine-tuning......
[2024-08-02 10:29:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-02 10:29:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth'
[2024-08-02 10:29:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 14.057 (14.057)	Loss 0.4961 (0.4961)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 2342MB
[2024-08-02 10:30:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.706 Acc@5 97.674
[2024-08-02 10:30:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 10:30:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 168): INFO Start training
[2024-08-02 10:30:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:23:51 lr 0.000000	 wd 0.0500	time 13.5217 (13.5217)	loss 1.5612 (1.5612)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 20413MB
[2024-08-02 10:31:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:28:01 lr 0.000000	 wd 0.0500	time 0.5514 (0.7000)	loss 1.3456 (1.2173)	grad_norm 1.8943 (nan)	loss_scale 16384.0000 (27901.4653)	mem 20413MB
[2024-08-02 10:32:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:24:23 lr 0.000000	 wd 0.0500	time 0.5624 (0.6356)	loss 1.0751 (1.2108)	grad_norm 3.3835 (nan)	loss_scale 8192.0000 (20215.0846)	mem 20413MB
[2024-08-02 10:33:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:22:33 lr 0.000000	 wd 0.0500	time 0.5570 (0.6145)	loss 0.8883 (1.1707)	grad_norm 2.0643 (nan)	loss_scale 8192.0000 (16220.7043)	mem 20413MB
[2024-08-02 10:34:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:21:09 lr 0.000001	 wd 0.0500	time 0.5592 (0.6039)	loss 1.1179 (1.1789)	grad_norm 2.2842 (nan)	loss_scale 4096.0000 (14014.2444)	mem 20413MB
[2024-08-02 10:35:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:19:56 lr 0.000001	 wd 0.0500	time 0.5541 (0.5977)	loss 1.1980 (1.1812)	grad_norm 2.2343 (nan)	loss_scale 4096.0000 (12034.5549)	mem 20413MB
[2024-08-02 10:36:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:18:49 lr 0.000001	 wd 0.0500	time 0.5597 (0.5938)	loss 1.3095 (1.1793)	grad_norm 2.7120 (nan)	loss_scale 4096.0000 (10713.6639)	mem 20413MB
[2024-08-02 10:37:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:17:44 lr 0.000001	 wd 0.0500	time 0.5525 (0.5909)	loss 1.3220 (1.1819)	grad_norm 2.6778 (nan)	loss_scale 4096.0000 (9769.6320)	mem 20413MB
[2024-08-02 10:38:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:16:42 lr 0.000001	 wd 0.0500	time 0.5534 (0.5888)	loss 1.1096 (1.1803)	grad_norm 1.8473 (nan)	loss_scale 4096.0000 (9061.3134)	mem 20413MB
[2024-08-02 10:39:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:15:40 lr 0.000001	 wd 0.0500	time 0.5564 (0.5871)	loss 1.3570 (1.1807)	grad_norm 7.2652 (nan)	loss_scale 4096.0000 (8510.2242)	mem 20413MB
[2024-08-02 10:39:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:14:39 lr 0.000002	 wd 0.0500	time 0.5518 (0.5858)	loss 1.4528 (1.1785)	grad_norm 1.6175 (nan)	loss_scale 4096.0000 (8069.2428)	mem 20413MB
[2024-08-02 10:40:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:13:39 lr 0.000002	 wd 0.0500	time 0.5564 (0.5847)	loss 1.3581 (1.1803)	grad_norm 2.2054 (nan)	loss_scale 4096.0000 (7708.3669)	mem 20413MB
[2024-08-02 10:41:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:12:40 lr 0.000002	 wd 0.0500	time 0.5613 (0.5838)	loss 1.0181 (1.1798)	grad_norm 2.0115 (nan)	loss_scale 4096.0000 (7407.5870)	mem 20413MB
[2024-08-02 10:42:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:11:40 lr 0.000002	 wd 0.0500	time 0.5542 (0.5831)	loss 1.2257 (1.1815)	grad_norm 2.2163 (nan)	loss_scale 4096.0000 (7153.0453)	mem 20413MB
[2024-08-02 10:43:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:10:41 lr 0.000002	 wd 0.0500	time 0.5606 (0.5825)	loss 1.4539 (1.1827)	grad_norm 2.0539 (nan)	loss_scale 4096.0000 (6934.8408)	mem 20413MB
[2024-08-02 10:44:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:09:43 lr 0.000002	 wd 0.0500	time 0.5592 (0.5819)	loss 1.0473 (1.1861)	grad_norm 2.7381 (nan)	loss_scale 4096.0000 (6745.7109)	mem 20413MB
[2024-08-02 10:45:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:08:44 lr 0.000003	 wd 0.0500	time 0.5551 (0.5815)	loss 1.2328 (1.1835)	grad_norm 1.4781 (nan)	loss_scale 4096.0000 (6580.2074)	mem 20413MB
[2024-08-02 10:46:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:07:45 lr 0.000003	 wd 0.0500	time 0.5518 (0.5810)	loss 1.0213 (1.1831)	grad_norm 2.4608 (nan)	loss_scale 4096.0000 (6434.1634)	mem 20413MB
[2024-08-02 10:47:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:06:47 lr 0.000003	 wd 0.0500	time 0.5573 (0.5807)	loss 1.1065 (1.1831)	grad_norm 2.3920 (nan)	loss_scale 4096.0000 (6304.3376)	mem 20413MB
[2024-08-02 10:48:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:05:49 lr 0.000003	 wd 0.0500	time 0.5587 (0.5803)	loss 1.5282 (1.1828)	grad_norm 1.7549 (nan)	loss_scale 4096.0000 (6188.1704)	mem 20413MB
[2024-08-02 10:49:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:04:51 lr 0.000003	 wd 0.0500	time 0.5549 (0.5800)	loss 0.8532 (1.1820)	grad_norm 1.7034 (nan)	loss_scale 4096.0000 (6083.6142)	mem 20413MB
[2024-08-02 10:50:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:03:53 lr 0.000003	 wd 0.0500	time 0.5630 (0.5798)	loss 1.0339 (1.1809)	grad_norm 2.8752 (nan)	loss_scale 4096.0000 (5989.0109)	mem 20413MB
[2024-08-02 10:51:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:02:55 lr 0.000004	 wd 0.0500	time 0.5566 (0.5795)	loss 1.5456 (1.1801)	grad_norm 1.8369 (nan)	loss_scale 4096.0000 (5903.0041)	mem 20413MB
[2024-08-02 10:52:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:57 lr 0.000004	 wd 0.0500	time 0.5643 (0.5793)	loss 1.4266 (1.1801)	grad_norm 2.8970 (nan)	loss_scale 4096.0000 (5824.4728)	mem 20413MB
[2024-08-02 10:53:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:59 lr 0.000004	 wd 0.0500	time 0.5611 (0.5791)	loss 1.3776 (1.1798)	grad_norm 1.6190 (nan)	loss_scale 4096.0000 (5752.4831)	mem 20413MB
[2024-08-02 10:54:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.5417 (0.5788)	loss 1.0318 (1.1782)	grad_norm 2.0716 (nan)	loss_scale 4096.0000 (5686.2503)	mem 20413MB
[2024-08-02 10:54:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 0 training takes 0:24:10
[2024-08-02 10:54:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_0.pth saving......
[2024-08-02 10:54:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_0.pth saved !!!
[2024-08-02 10:54:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.866 (11.866)	Loss 0.5332 (0.5332)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 20413MB
[2024-08-02 10:54:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.682 Acc@5 97.656
[2024-08-02 10:54:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 10:54:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.68%
[2024-08-02 10:54:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 10:54:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 10:55:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:21:18 lr 0.000004	 wd 0.0500	time 10.5828 (10.5828)	loss 1.3447 (1.3447)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 10:56:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:27:11 lr 0.000004	 wd 0.0500	time 0.5503 (0.6792)	loss 1.2804 (1.1650)	grad_norm 1.9524 (2.6514)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 10:57:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:24:00 lr 0.000004	 wd 0.0500	time 0.5588 (0.6259)	loss 0.8356 (1.1677)	grad_norm 5.1550 (inf)	loss_scale 2048.0000 (3545.7910)	mem 20413MB
[2024-08-02 10:57:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:22:19 lr 0.000004	 wd 0.0500	time 0.5567 (0.6082)	loss 0.9218 (1.1727)	grad_norm 1.6893 (inf)	loss_scale 2048.0000 (3048.1860)	mem 20413MB
[2024-08-02 10:58:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:21:00 lr 0.000005	 wd 0.0500	time 0.5478 (0.5995)	loss 0.9540 (1.1883)	grad_norm 1.4792 (inf)	loss_scale 2048.0000 (2798.7631)	mem 20413MB
[2024-08-02 10:59:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:19:49 lr 0.000005	 wd 0.0500	time 0.5571 (0.5943)	loss 1.5540 (1.1911)	grad_norm 2.5039 (inf)	loss_scale 2048.0000 (2648.9102)	mem 20413MB
[2024-08-02 11:00:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:18:43 lr 0.000005	 wd 0.0500	time 0.5587 (0.5908)	loss 1.4724 (1.1907)	grad_norm 1.7895 (inf)	loss_scale 2048.0000 (2548.9251)	mem 20413MB
[2024-08-02 11:01:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:17:40 lr 0.000005	 wd 0.0500	time 0.5625 (0.5884)	loss 0.8697 (1.1871)	grad_norm 2.6447 (inf)	loss_scale 2048.0000 (2477.4665)	mem 20413MB
[2024-08-02 11:02:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:16:38 lr 0.000005	 wd 0.0500	time 0.5529 (0.5866)	loss 1.5860 (1.1865)	grad_norm 1.8796 (inf)	loss_scale 2048.0000 (2423.8502)	mem 20413MB
[2024-08-02 11:03:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:15:37 lr 0.000005	 wd 0.0500	time 0.5618 (0.5852)	loss 1.1766 (1.1847)	grad_norm 1.6352 (inf)	loss_scale 2048.0000 (2382.1354)	mem 20413MB
[2024-08-02 11:04:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:14:37 lr 0.000006	 wd 0.0500	time 0.5584 (0.5842)	loss 1.4820 (1.1814)	grad_norm 2.2979 (inf)	loss_scale 2048.0000 (2348.7552)	mem 20413MB
[2024-08-02 11:05:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:13:37 lr 0.000006	 wd 0.0500	time 0.5592 (0.5833)	loss 1.5390 (1.1800)	grad_norm 1.8339 (inf)	loss_scale 2048.0000 (2321.4387)	mem 20413MB
[2024-08-02 11:06:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:12:38 lr 0.000006	 wd 0.0500	time 0.5625 (0.5825)	loss 0.9370 (1.1783)	grad_norm 2.4412 (inf)	loss_scale 2048.0000 (2298.6711)	mem 20413MB
[2024-08-02 11:07:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:11:39 lr 0.000006	 wd 0.0500	time 0.5593 (0.5819)	loss 1.2424 (1.1796)	grad_norm 3.1266 (inf)	loss_scale 2048.0000 (2279.4035)	mem 20413MB
[2024-08-02 11:08:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:10:40 lr 0.000006	 wd 0.0500	time 0.5614 (0.5813)	loss 0.8206 (1.1796)	grad_norm 2.6181 (inf)	loss_scale 2048.0000 (2262.8865)	mem 20413MB
[2024-08-02 11:09:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:09:42 lr 0.000006	 wd 0.0500	time 0.5598 (0.5809)	loss 1.4967 (1.1798)	grad_norm 2.6831 (inf)	loss_scale 2048.0000 (2248.5703)	mem 20413MB
[2024-08-02 11:10:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:08:43 lr 0.000007	 wd 0.0500	time 0.5567 (0.5804)	loss 1.3590 (1.1808)	grad_norm 3.6888 (inf)	loss_scale 2048.0000 (2236.0425)	mem 20413MB
[2024-08-02 11:11:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:07:45 lr 0.000007	 wd 0.0500	time 0.5514 (0.5800)	loss 0.8446 (1.1816)	grad_norm 2.0431 (inf)	loss_scale 2048.0000 (2224.9877)	mem 20413MB
[2024-08-02 11:12:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:06:46 lr 0.000007	 wd 0.0500	time 0.5539 (0.5797)	loss 1.6626 (1.1809)	grad_norm 2.6462 (inf)	loss_scale 2048.0000 (2215.1605)	mem 20413MB
[2024-08-02 11:13:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:05:48 lr 0.000007	 wd 0.0500	time 0.5612 (0.5794)	loss 1.4920 (1.1827)	grad_norm 1.5623 (inf)	loss_scale 2048.0000 (2206.3672)	mem 20413MB
[2024-08-02 11:14:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:04:50 lr 0.000007	 wd 0.0500	time 0.5667 (0.5792)	loss 1.2870 (1.1833)	grad_norm 2.7468 (inf)	loss_scale 2048.0000 (2198.4528)	mem 20413MB
[2024-08-02 11:15:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:03:52 lr 0.000007	 wd 0.0500	time 0.5654 (0.5789)	loss 1.4235 (1.1855)	grad_norm 2.1992 (inf)	loss_scale 2048.0000 (2191.2918)	mem 20413MB
[2024-08-02 11:16:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:02:54 lr 0.000008	 wd 0.0500	time 0.5573 (0.5787)	loss 1.1617 (1.1860)	grad_norm 1.7503 (inf)	loss_scale 2048.0000 (2184.7815)	mem 20413MB
[2024-08-02 11:17:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:56 lr 0.000008	 wd 0.0500	time 0.5608 (0.5785)	loss 0.8518 (1.1876)	grad_norm 1.6939 (inf)	loss_scale 2048.0000 (2178.8370)	mem 20413MB
[2024-08-02 11:18:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:58 lr 0.000008	 wd 0.0500	time 0.5542 (0.5783)	loss 1.2432 (1.1876)	grad_norm 5.8201 (inf)	loss_scale 2048.0000 (2173.3878)	mem 20413MB
[2024-08-02 11:19:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.5531 (0.5781)	loss 0.9480 (1.1876)	grad_norm 1.4655 (inf)	loss_scale 2048.0000 (2168.3743)	mem 20413MB
[2024-08-02 11:19:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 1 training takes 0:24:08
[2024-08-02 11:19:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.787 (12.787)	Loss 0.5288 (0.5288)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 20413MB
[2024-08-02 11:19:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.654 Acc@5 97.650
[2024-08-02 11:19:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 11:19:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.68%
[2024-08-02 11:19:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][0/2502]	eta 8:49:55 lr 0.000008	 wd 0.0500	time 12.7080 (12.7080)	loss 1.1658 (1.1658)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:20:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:27:41 lr 0.000008	 wd 0.0500	time 0.5579 (0.6917)	loss 1.5052 (1.1922)	grad_norm 1.6103 (2.2632)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:21:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:24:15 lr 0.000008	 wd 0.0500	time 0.5569 (0.6321)	loss 1.3164 (1.1819)	grad_norm 1.7312 (2.4085)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:22:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:22:28 lr 0.000008	 wd 0.0500	time 0.5531 (0.6124)	loss 1.2610 (1.1711)	grad_norm 2.4839 (2.4263)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:23:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:21:06 lr 0.000009	 wd 0.0500	time 0.5581 (0.6026)	loss 1.3961 (1.1749)	grad_norm 2.1497 (2.3516)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:24:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:19:54 lr 0.000009	 wd 0.0500	time 0.5606 (0.5968)	loss 1.3610 (1.1729)	grad_norm 1.9257 (2.4811)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:25:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:18:47 lr 0.000009	 wd 0.0500	time 0.5578 (0.5930)	loss 1.2817 (1.1720)	grad_norm 1.9916 (2.5867)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:26:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:17:43 lr 0.000009	 wd 0.0500	time 0.5593 (0.5902)	loss 1.3647 (1.1764)	grad_norm 2.3971 (2.6303)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:27:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:16:41 lr 0.000009	 wd 0.0500	time 0.5536 (0.5881)	loss 1.2604 (1.1796)	grad_norm 1.4976 (2.5821)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:28:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:15:39 lr 0.000009	 wd 0.0500	time 0.5587 (0.5865)	loss 0.8418 (1.1833)	grad_norm 6.7241 (2.5405)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:29:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:14:39 lr 0.000010	 wd 0.0500	time 0.5595 (0.5853)	loss 0.8926 (1.1809)	grad_norm 2.5448 (2.4968)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:30:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:13:39 lr 0.000010	 wd 0.0500	time 0.5600 (0.5843)	loss 1.4246 (1.1797)	grad_norm 4.7003 (2.4924)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:31:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:12:39 lr 0.000010	 wd 0.0500	time 0.5540 (0.5834)	loss 1.0375 (1.1805)	grad_norm 5.3661 (2.4899)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:32:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:11:40 lr 0.000010	 wd 0.0500	time 0.5518 (0.5826)	loss 1.4490 (1.1811)	grad_norm 1.6293 (2.4730)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 11:33:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:10:41 lr 0.000010	 wd 0.0500	time 0.5520 (0.5820)	loss 1.2468 (1.1825)	grad_norm 2.1348 (inf)	loss_scale 1024.0000 (2001.2220)	mem 20413MB
[2024-08-02 11:34:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:09:42 lr 0.000010	 wd 0.0500	time 0.5588 (0.5815)	loss 1.4206 (1.1831)	grad_norm 2.1822 (inf)	loss_scale 1024.0000 (1936.1173)	mem 20413MB
[2024-08-02 11:35:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:08:44 lr 0.000011	 wd 0.0500	time 0.5490 (0.5810)	loss 0.9635 (1.1833)	grad_norm 2.1135 (inf)	loss_scale 1024.0000 (1879.1455)	mem 20413MB
[2024-08-02 11:36:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:07:45 lr 0.000011	 wd 0.0500	time 0.5542 (0.5806)	loss 1.2568 (1.1834)	grad_norm 1.9489 (inf)	loss_scale 1024.0000 (1828.8724)	mem 20413MB
[2024-08-02 11:37:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:06:47 lr 0.000011	 wd 0.0500	time 0.5508 (0.5802)	loss 1.1917 (1.1845)	grad_norm 2.6648 (inf)	loss_scale 1024.0000 (1784.1821)	mem 20413MB
[2024-08-02 11:38:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:05:49 lr 0.000011	 wd 0.0500	time 0.5620 (0.5799)	loss 1.3731 (1.1843)	grad_norm 2.4422 (inf)	loss_scale 1024.0000 (1744.1936)	mem 20413MB
[2024-08-02 11:38:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:04:50 lr 0.000011	 wd 0.0500	time 0.5547 (0.5796)	loss 1.0053 (1.1845)	grad_norm 1.6616 (inf)	loss_scale 1024.0000 (1708.2019)	mem 20413MB
[2024-08-02 11:39:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:03:52 lr 0.000011	 wd 0.0500	time 0.5545 (0.5793)	loss 1.0279 (1.1837)	grad_norm 1.8231 (inf)	loss_scale 1024.0000 (1675.6364)	mem 20413MB
[2024-08-02 11:40:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:02:54 lr 0.000012	 wd 0.0500	time 0.5627 (0.5791)	loss 1.3154 (1.1842)	grad_norm 1.7601 (inf)	loss_scale 1024.0000 (1646.0300)	mem 20413MB
[2024-08-02 11:41:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:56 lr 0.000012	 wd 0.0500	time 0.5566 (0.5789)	loss 1.1467 (1.1841)	grad_norm 2.4254 (inf)	loss_scale 1024.0000 (1618.9970)	mem 20413MB
[2024-08-02 11:42:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:59 lr 0.000012	 wd 0.0500	time 0.5573 (0.5787)	loss 0.8044 (1.1848)	grad_norm 1.9763 (inf)	loss_scale 1024.0000 (1594.2157)	mem 20413MB
[2024-08-02 11:43:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.5575 (0.5784)	loss 1.2440 (1.1847)	grad_norm 1.9291 (inf)	loss_scale 1024.0000 (1571.4162)	mem 20413MB
[2024-08-02 11:43:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 2 training takes 0:24:09
[2024-08-02 11:44:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.118 (12.118)	Loss 0.5068 (0.5068)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 11:44:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.736 Acc@5 97.672
[2024-08-02 11:44:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 11:44:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-02 11:44:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 11:44:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 11:44:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][0/2502]	eta 8:27:15 lr 0.000012	 wd 0.0500	time 12.1645 (12.1645)	loss 0.7010 (0.7010)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:45:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:27:27 lr 0.000012	 wd 0.0500	time 0.5538 (0.6860)	loss 1.2873 (1.2240)	grad_norm 2.4442 (2.2574)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:46:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:24:07 lr 0.000012	 wd 0.0500	time 0.5583 (0.6290)	loss 1.5250 (1.1979)	grad_norm 1.7704 (2.3784)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:47:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:22:23 lr 0.000012	 wd 0.0500	time 0.5552 (0.6102)	loss 1.4819 (1.1982)	grad_norm 1.7633 (2.4264)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:48:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:21:02 lr 0.000013	 wd 0.0500	time 0.5588 (0.6008)	loss 1.4306 (1.1897)	grad_norm 2.3577 (2.4306)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:49:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:19:51 lr 0.000013	 wd 0.0500	time 0.5605 (0.5953)	loss 0.8317 (1.1895)	grad_norm 2.0754 (2.3876)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:50:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:18:45 lr 0.000013	 wd 0.0500	time 0.5493 (0.5918)	loss 1.0887 (1.1817)	grad_norm 1.9651 (2.3794)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:51:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:17:41 lr 0.000013	 wd 0.0500	time 0.5582 (0.5893)	loss 1.4064 (1.1796)	grad_norm 2.1963 (2.4339)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:52:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:16:39 lr 0.000013	 wd 0.0500	time 0.5588 (0.5874)	loss 0.9919 (1.1745)	grad_norm 1.9162 (2.5618)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:53:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:15:38 lr 0.000013	 wd 0.0500	time 0.5584 (0.5859)	loss 1.6020 (1.1773)	grad_norm 2.4118 (2.5517)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:54:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:14:38 lr 0.000014	 wd 0.0500	time 0.5506 (0.5847)	loss 1.2943 (1.1774)	grad_norm 1.3512 (2.5381)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:55:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:13:38 lr 0.000014	 wd 0.0500	time 0.5652 (0.5837)	loss 0.8422 (1.1786)	grad_norm 1.9601 (2.5426)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:56:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:12:38 lr 0.000014	 wd 0.0500	time 0.5607 (0.5829)	loss 1.2110 (1.1767)	grad_norm 2.6223 (2.5260)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:57:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:11:39 lr 0.000014	 wd 0.0500	time 0.5574 (0.5822)	loss 1.2801 (1.1766)	grad_norm 2.3132 (2.4995)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:57:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:10:40 lr 0.000014	 wd 0.0500	time 0.5482 (0.5816)	loss 1.2122 (1.1773)	grad_norm 1.8542 (2.4954)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:58:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:09:42 lr 0.000014	 wd 0.0500	time 0.5546 (0.5811)	loss 1.4558 (1.1757)	grad_norm 1.6021 (2.4815)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 11:59:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:08:43 lr 0.000015	 wd 0.0500	time 0.5560 (0.5807)	loss 0.7323 (1.1732)	grad_norm 1.9505 (2.4606)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:00:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:07:45 lr 0.000015	 wd 0.0500	time 0.5562 (0.5803)	loss 1.0230 (1.1760)	grad_norm 1.7606 (2.4606)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:01:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:06:47 lr 0.000015	 wd 0.0500	time 0.5691 (0.5800)	loss 1.2648 (1.1774)	grad_norm 1.4282 (2.4683)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:02:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:05:48 lr 0.000015	 wd 0.0500	time 0.5583 (0.5797)	loss 1.4062 (1.1759)	grad_norm 1.5448 (2.4787)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:03:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:04:50 lr 0.000015	 wd 0.0500	time 0.5599 (0.5794)	loss 1.2636 (1.1755)	grad_norm 1.5396 (2.4570)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:04:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:03:52 lr 0.000015	 wd 0.0500	time 0.5559 (0.5792)	loss 0.8254 (1.1755)	grad_norm 2.4346 (2.4409)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:05:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:54 lr 0.000016	 wd 0.0500	time 0.5663 (0.5789)	loss 0.8116 (1.1747)	grad_norm 1.8954 (2.4268)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:06:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:56 lr 0.000016	 wd 0.0500	time 0.5527 (0.5787)	loss 0.9446 (1.1752)	grad_norm 1.8087 (2.4411)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:07:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:59 lr 0.000016	 wd 0.0500	time 0.5614 (0.5786)	loss 1.3340 (1.1766)	grad_norm 19.1182 (2.4550)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:08:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5591 (0.5784)	loss 1.2663 (1.1767)	grad_norm 3.6817 (2.4743)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:08:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 3 training takes 0:24:09
[2024-08-02 12:08:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.376 (12.376)	Loss 0.5190 (0.5190)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 20413MB
[2024-08-02 12:09:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.732 Acc@5 97.696
[2024-08-02 12:09:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 12:09:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-02 12:09:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][0/2502]	eta 8:38:06 lr 0.000016	 wd 0.0500	time 12.4248 (12.4248)	loss 1.2693 (1.2693)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:10:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:27:34 lr 0.000016	 wd 0.0500	time 0.5525 (0.6887)	loss 0.9574 (1.1946)	grad_norm 1.7687 (2.1711)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:11:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:24:11 lr 0.000016	 wd 0.0500	time 0.5511 (0.6306)	loss 1.0967 (1.1837)	grad_norm 1.6245 (2.3949)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:12:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:22:25 lr 0.000016	 wd 0.0500	time 0.5565 (0.6112)	loss 0.7559 (1.1798)	grad_norm 1.8169 (2.3913)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 12:13:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:21:04 lr 0.000017	 wd 0.0500	time 0.5538 (0.6016)	loss 1.3784 (1.1771)	grad_norm 1.6743 (2.3912)	loss_scale 2048.0000 (1197.6459)	mem 20413MB
[2024-08-02 12:14:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:19:53 lr 0.000017	 wd 0.0500	time 0.5560 (0.5960)	loss 1.2330 (1.1771)	grad_norm 9.2933 (2.3840)	loss_scale 2048.0000 (1367.3772)	mem 20413MB
[2024-08-02 12:15:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:18:46 lr 0.000017	 wd 0.0500	time 0.5582 (0.5923)	loss 1.0142 (1.1781)	grad_norm 1.7492 (2.3401)	loss_scale 2048.0000 (1480.6256)	mem 20413MB
[2024-08-02 12:15:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:17:42 lr 0.000017	 wd 0.0500	time 0.5521 (0.5897)	loss 0.9769 (1.1789)	grad_norm 1.9475 (2.4141)	loss_scale 2048.0000 (1561.5635)	mem 20413MB
[2024-08-02 12:16:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:16:40 lr 0.000017	 wd 0.0500	time 0.5587 (0.5876)	loss 0.7801 (1.1811)	grad_norm 2.4245 (2.4177)	loss_scale 2048.0000 (1622.2921)	mem 20413MB
[2024-08-02 12:17:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:15:38 lr 0.000017	 wd 0.0500	time 0.5577 (0.5861)	loss 0.8944 (1.1819)	grad_norm 1.4765 (2.3971)	loss_scale 2048.0000 (1669.5405)	mem 20413MB
[2024-08-02 12:18:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:14:38 lr 0.000018	 wd 0.0500	time 0.5551 (0.5848)	loss 1.4960 (1.1829)	grad_norm 1.7728 (2.4248)	loss_scale 2048.0000 (1707.3487)	mem 20413MB
[2024-08-02 12:19:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:13:38 lr 0.000018	 wd 0.0500	time 0.5550 (0.5838)	loss 1.4736 (1.1843)	grad_norm 1.8344 (2.4124)	loss_scale 2048.0000 (1738.2888)	mem 20413MB
[2024-08-02 12:20:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:12:39 lr 0.000018	 wd 0.0500	time 0.5615 (0.5830)	loss 1.2999 (1.1825)	grad_norm 1.5132 (2.4000)	loss_scale 2048.0000 (1764.0766)	mem 20413MB
[2024-08-02 12:21:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:11:39 lr 0.000018	 wd 0.0500	time 0.5511 (0.5823)	loss 0.9552 (1.1819)	grad_norm 1.8677 (2.4157)	loss_scale 2048.0000 (1785.9001)	mem 20413MB
[2024-08-02 12:22:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:10:40 lr 0.000018	 wd 0.0500	time 0.5527 (0.5817)	loss 1.4362 (1.1821)	grad_norm 2.3956 (2.4004)	loss_scale 2048.0000 (1804.6081)	mem 20413MB
[2024-08-02 12:23:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:09:42 lr 0.000018	 wd 0.0500	time 0.5587 (0.5812)	loss 1.2092 (1.1821)	grad_norm 2.0140 (2.3776)	loss_scale 2048.0000 (1820.8235)	mem 20413MB
[2024-08-02 12:24:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:08:43 lr 0.000019	 wd 0.0500	time 0.5546 (0.5807)	loss 1.3657 (1.1804)	grad_norm 1.5949 (2.3682)	loss_scale 2048.0000 (1835.0131)	mem 20413MB
[2024-08-02 12:25:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:07:45 lr 0.000019	 wd 0.0500	time 0.5565 (0.5803)	loss 1.1734 (1.1804)	grad_norm 2.3320 (2.3623)	loss_scale 2048.0000 (1847.5344)	mem 20413MB
[2024-08-02 12:26:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:06:47 lr 0.000019	 wd 0.0500	time 0.5607 (0.5799)	loss 1.5324 (1.1821)	grad_norm 2.1075 (2.3564)	loss_scale 2048.0000 (1858.6652)	mem 20413MB
[2024-08-02 12:27:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:05:48 lr 0.000019	 wd 0.0500	time 0.5513 (0.5796)	loss 1.3736 (1.1823)	grad_norm 1.5009 (2.3464)	loss_scale 2048.0000 (1868.6249)	mem 20413MB
[2024-08-02 12:28:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:04:50 lr 0.000019	 wd 0.0500	time 0.5589 (0.5793)	loss 0.8207 (1.1812)	grad_norm 2.0608 (2.3452)	loss_scale 2048.0000 (1877.5892)	mem 20413MB
[2024-08-02 12:29:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:03:52 lr 0.000019	 wd 0.0500	time 0.5608 (0.5791)	loss 0.9356 (1.1799)	grad_norm 1.6055 (2.3652)	loss_scale 2048.0000 (1885.7001)	mem 20413MB
[2024-08-02 12:30:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:02:54 lr 0.000020	 wd 0.0500	time 0.5576 (0.5788)	loss 0.9086 (1.1801)	grad_norm 1.8344 (2.3834)	loss_scale 2048.0000 (1893.0741)	mem 20413MB
[2024-08-02 12:31:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:56 lr 0.000020	 wd 0.0500	time 0.5613 (0.5786)	loss 0.8268 (1.1804)	grad_norm 2.2544 (2.4118)	loss_scale 2048.0000 (1899.8070)	mem 20413MB
[2024-08-02 12:32:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:59 lr 0.000020	 wd 0.0500	time 0.5523 (0.5784)	loss 0.7599 (1.1797)	grad_norm 2.0767 (2.4113)	loss_scale 2048.0000 (1905.9792)	mem 20413MB
[2024-08-02 12:33:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5557 (0.5782)	loss 1.1345 (1.1802)	grad_norm 1.8077 (2.4140)	loss_scale 2048.0000 (1911.6577)	mem 20413MB
[2024-08-02 12:33:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 4 training takes 0:24:09
[2024-08-02 12:33:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.775 (11.775)	Loss 0.5083 (0.5083)	Acc@1 93.555 (93.555)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 12:33:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.704 Acc@5 97.682
[2024-08-02 12:33:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 12:33:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-02 12:34:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:19:43 lr 0.000020	 wd 0.0500	time 11.9839 (11.9839)	loss 1.4124 (1.4124)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:34:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:27:23 lr 0.000020	 wd 0.0500	time 0.5633 (0.6842)	loss 1.0169 (1.2118)	grad_norm 2.8161 (2.6337)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:35:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:24:06 lr 0.000020	 wd 0.0500	time 0.5577 (0.6282)	loss 1.2138 (1.1754)	grad_norm 1.6229 (2.3347)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:36:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:22:22 lr 0.000020	 wd 0.0500	time 0.5557 (0.6098)	loss 0.8444 (1.1692)	grad_norm 1.9369 (2.3282)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:37:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:21:02 lr 0.000020	 wd 0.0500	time 0.5493 (0.6005)	loss 1.4941 (1.1771)	grad_norm 4.7576 (2.4284)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:38:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:19:51 lr 0.000020	 wd 0.0500	time 0.5544 (0.5952)	loss 1.0342 (1.1747)	grad_norm 2.6662 (2.4774)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:39:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:18:45 lr 0.000020	 wd 0.0500	time 0.5617 (0.5918)	loss 0.9131 (1.1755)	grad_norm 6.7121 (2.5381)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:40:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:17:41 lr 0.000020	 wd 0.0500	time 0.5538 (0.5893)	loss 0.9281 (1.1759)	grad_norm 1.5372 (2.5645)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:41:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:16:39 lr 0.000020	 wd 0.0500	time 0.5590 (0.5873)	loss 1.4822 (1.1737)	grad_norm 1.8269 (2.5128)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:42:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:15:38 lr 0.000020	 wd 0.0500	time 0.5598 (0.5859)	loss 1.0266 (1.1725)	grad_norm 1.7894 (2.4813)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:43:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:14:38 lr 0.000020	 wd 0.0500	time 0.5571 (0.5847)	loss 1.1661 (1.1713)	grad_norm 2.0809 (2.4666)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:44:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:13:38 lr 0.000020	 wd 0.0500	time 0.5641 (0.5839)	loss 1.1905 (1.1729)	grad_norm 1.8813 (2.4481)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:45:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:12:39 lr 0.000020	 wd 0.0500	time 0.5722 (0.5831)	loss 1.5028 (1.1720)	grad_norm 2.3250 (2.4310)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:46:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:11:40 lr 0.000020	 wd 0.0500	time 0.5552 (0.5824)	loss 0.8221 (1.1731)	grad_norm 2.0409 (2.4281)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:47:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:10:41 lr 0.000020	 wd 0.0500	time 0.5547 (0.5819)	loss 1.3241 (1.1769)	grad_norm 2.0185 (2.4236)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:48:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:09:42 lr 0.000020	 wd 0.0500	time 0.5532 (0.5814)	loss 0.7789 (1.1787)	grad_norm 7.0314 (2.4117)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:49:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:08:44 lr 0.000020	 wd 0.0500	time 0.5595 (0.5809)	loss 1.3863 (1.1819)	grad_norm 1.7247 (2.3974)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:50:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:07:45 lr 0.000020	 wd 0.0500	time 0.5620 (0.5805)	loss 0.8343 (1.1821)	grad_norm 2.4018 (2.3976)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:51:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:06:47 lr 0.000020	 wd 0.0500	time 0.5542 (0.5802)	loss 0.8243 (1.1797)	grad_norm 3.6589 (2.3867)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 12:52:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:05:49 lr 0.000020	 wd 0.0500	time 0.5591 (0.5799)	loss 1.1578 (1.1790)	grad_norm 1.4816 (2.3689)	loss_scale 4096.0000 (2123.4129)	mem 20413MB
[2024-08-02 12:53:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:04:50 lr 0.000020	 wd 0.0500	time 0.5569 (0.5796)	loss 1.5891 (1.1801)	grad_norm 8.6698 (2.4217)	loss_scale 4096.0000 (2221.9930)	mem 20413MB
[2024-08-02 12:54:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:03:52 lr 0.000020	 wd 0.0500	time 0.5552 (0.5794)	loss 1.5805 (1.1800)	grad_norm 2.0623 (2.4120)	loss_scale 4096.0000 (2311.1890)	mem 20413MB
[2024-08-02 12:55:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:02:54 lr 0.000020	 wd 0.0500	time 0.5563 (0.5791)	loss 1.3048 (1.1805)	grad_norm 1.3866 (2.4037)	loss_scale 4096.0000 (2392.2799)	mem 20413MB
[2024-08-02 12:56:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:56 lr 0.000020	 wd 0.0500	time 0.5556 (0.5789)	loss 1.1540 (1.1791)	grad_norm 3.2498 (2.4117)	loss_scale 4096.0000 (2466.3225)	mem 20413MB
[2024-08-02 12:56:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:59 lr 0.000020	 wd 0.0500	time 0.5614 (0.5787)	loss 0.9168 (1.1786)	grad_norm 2.0667 (2.4028)	loss_scale 4096.0000 (2534.1974)	mem 20413MB
[2024-08-02 12:57:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5575 (0.5785)	loss 1.4044 (1.1796)	grad_norm 1.8837 (2.4033)	loss_scale 4096.0000 (2596.6445)	mem 20413MB
[2024-08-02 12:57:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 5 training takes 0:24:09
[2024-08-02 12:58:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.470 (12.470)	Loss 0.5098 (0.5098)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 12:58:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.708 Acc@5 97.734
[2024-08-02 12:58:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 12:58:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.74%
[2024-08-02 12:58:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:38:30 lr 0.000020	 wd 0.0500	time 12.4344 (12.4344)	loss 1.2133 (1.2133)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 12:59:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:27:34 lr 0.000020	 wd 0.0500	time 0.5508 (0.6886)	loss 0.9782 (1.2078)	grad_norm 1.5093 (2.6662)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:00:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:24:11 lr 0.000020	 wd 0.0500	time 0.5604 (0.6304)	loss 0.9260 (1.1904)	grad_norm 2.0273 (2.6524)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:01:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:22:25 lr 0.000020	 wd 0.0500	time 0.5588 (0.6112)	loss 0.9679 (1.1868)	grad_norm 1.8656 (2.7022)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:02:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:21:05 lr 0.000020	 wd 0.0500	time 0.5603 (0.6018)	loss 0.7009 (1.1855)	grad_norm 2.7330 (2.6553)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:03:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:19:53 lr 0.000020	 wd 0.0500	time 0.5526 (0.5963)	loss 1.2988 (1.1865)	grad_norm 1.5211 (2.5632)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:04:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:18:46 lr 0.000020	 wd 0.0500	time 0.5604 (0.5925)	loss 0.8954 (1.1782)	grad_norm 2.3616 (2.5011)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:05:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:17:43 lr 0.000020	 wd 0.0500	time 0.5638 (0.5899)	loss 1.4156 (1.1808)	grad_norm 1.7977 (2.4762)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:06:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:16:40 lr 0.000020	 wd 0.0500	time 0.5512 (0.5880)	loss 0.7779 (1.1861)	grad_norm 2.2171 (2.4305)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:07:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:15:39 lr 0.000020	 wd 0.0500	time 0.5560 (0.5864)	loss 1.6360 (1.1821)	grad_norm 2.0773 (2.4809)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:08:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:14:38 lr 0.000020	 wd 0.0500	time 0.5598 (0.5851)	loss 1.0511 (1.1818)	grad_norm 3.6338 (2.4574)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:09:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:13:38 lr 0.000020	 wd 0.0500	time 0.5583 (0.5842)	loss 0.8239 (1.1772)	grad_norm 6.8526 (2.4519)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:10:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:12:39 lr 0.000020	 wd 0.0500	time 0.5571 (0.5833)	loss 1.3023 (1.1742)	grad_norm 2.0651 (2.4330)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:11:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:11:40 lr 0.000020	 wd 0.0500	time 0.5595 (0.5826)	loss 1.1455 (1.1730)	grad_norm 1.4878 (2.4240)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:12:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:10:41 lr 0.000020	 wd 0.0500	time 0.5554 (0.5820)	loss 1.0190 (1.1723)	grad_norm 2.5657 (2.4028)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:13:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:09:42 lr 0.000020	 wd 0.0500	time 0.5600 (0.5815)	loss 0.7050 (1.1722)	grad_norm 1.5188 (2.3913)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:14:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:08:44 lr 0.000020	 wd 0.0500	time 0.5617 (0.5811)	loss 1.3890 (1.1700)	grad_norm 3.3502 (2.3979)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:14:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:07:45 lr 0.000020	 wd 0.0500	time 0.5504 (0.5807)	loss 1.4002 (1.1681)	grad_norm 1.5032 (2.3955)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:15:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:06:47 lr 0.000020	 wd 0.0500	time 0.5564 (0.5803)	loss 1.2578 (1.1684)	grad_norm 1.6085 (2.3935)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:16:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:05:49 lr 0.000020	 wd 0.0500	time 0.5626 (0.5800)	loss 1.4331 (1.1706)	grad_norm 2.1048 (2.4168)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:17:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:04:51 lr 0.000020	 wd 0.0500	time 0.5546 (0.5797)	loss 0.9528 (1.1713)	grad_norm 1.6231 (2.4120)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:18:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:03:52 lr 0.000020	 wd 0.0500	time 0.5587 (0.5795)	loss 1.1179 (1.1717)	grad_norm 1.7945 (2.3981)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:19:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:02:54 lr 0.000020	 wd 0.0500	time 0.5759 (0.5792)	loss 0.8199 (1.1711)	grad_norm 2.4448 (2.4158)	loss_scale 4096.0000 (4096.0000)	mem 20413MB
[2024-08-02 13:20:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:56 lr 0.000020	 wd 0.0500	time 0.5497 (0.5790)	loss 0.9898 (1.1733)	grad_norm 2.5181 (inf)	loss_scale 2048.0000 (4042.5971)	mem 20413MB
[2024-08-02 13:21:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:59 lr 0.000020	 wd 0.0500	time 0.5582 (0.5788)	loss 1.4281 (1.1721)	grad_norm 3.6497 (inf)	loss_scale 2048.0000 (3959.5235)	mem 20413MB
[2024-08-02 13:22:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5596 (0.5786)	loss 1.4256 (1.1716)	grad_norm 1.7243 (inf)	loss_scale 2048.0000 (3883.0932)	mem 20413MB
[2024-08-02 13:22:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 6 training takes 0:24:10
[2024-08-02 13:22:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.367 (12.367)	Loss 0.5190 (0.5190)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 13:23:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.768 Acc@5 97.728
[2024-08-02 13:23:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 13:23:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.77%
[2024-08-02 13:23:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 13:23:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 13:23:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:40:00 lr 0.000020	 wd 0.0500	time 11.0313 (11.0313)	loss 0.9564 (0.9564)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:24:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:27:01 lr 0.000020	 wd 0.0500	time 0.5540 (0.6751)	loss 1.2520 (1.1541)	grad_norm 1.7311 (2.2965)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:25:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:23:55 lr 0.000020	 wd 0.0500	time 0.5642 (0.6237)	loss 1.1412 (1.1751)	grad_norm 1.9123 (2.2414)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:26:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:22:16 lr 0.000020	 wd 0.0500	time 0.5591 (0.6069)	loss 1.4854 (1.1744)	grad_norm 4.1548 (2.3221)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:27:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:20:57 lr 0.000020	 wd 0.0500	time 0.5504 (0.5985)	loss 0.8766 (1.1797)	grad_norm 3.2924 (2.3223)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:28:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:19:48 lr 0.000020	 wd 0.0500	time 0.5501 (0.5935)	loss 1.4229 (1.1775)	grad_norm 1.8825 (2.3225)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:29:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:18:42 lr 0.000020	 wd 0.0500	time 0.5580 (0.5903)	loss 1.2835 (1.1821)	grad_norm 2.7879 (2.3413)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:30:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:17:39 lr 0.000020	 wd 0.0500	time 0.5630 (0.5880)	loss 1.0967 (1.1765)	grad_norm 2.6260 (2.4270)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:31:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:16:37 lr 0.000020	 wd 0.0500	time 0.5513 (0.5862)	loss 0.9085 (1.1740)	grad_norm 2.2545 (2.4192)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:32:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:15:36 lr 0.000020	 wd 0.0500	time 0.5590 (0.5849)	loss 1.5240 (1.1771)	grad_norm 1.6868 (2.4085)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:32:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:14:36 lr 0.000020	 wd 0.0500	time 0.5603 (0.5838)	loss 0.8423 (1.1771)	grad_norm 5.2287 (2.3937)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:33:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:13:37 lr 0.000020	 wd 0.0500	time 0.5597 (0.5829)	loss 1.2304 (1.1751)	grad_norm 2.3101 (2.4050)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:34:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:12:38 lr 0.000020	 wd 0.0500	time 0.5442 (0.5822)	loss 1.3134 (1.1716)	grad_norm 5.1081 (2.4045)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:35:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:11:39 lr 0.000020	 wd 0.0500	time 0.5485 (0.5816)	loss 1.2967 (1.1715)	grad_norm 2.0334 (2.4378)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:36:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:10:40 lr 0.000019	 wd 0.0500	time 0.5521 (0.5811)	loss 1.2546 (1.1728)	grad_norm 1.7214 (2.4194)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:37:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:09:41 lr 0.000019	 wd 0.0500	time 0.5482 (0.5806)	loss 0.8152 (1.1737)	grad_norm 1.6040 (2.4418)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:38:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:08:43 lr 0.000019	 wd 0.0500	time 0.5532 (0.5802)	loss 1.2292 (1.1744)	grad_norm 1.8965 (2.4770)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:39:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:07:45 lr 0.000019	 wd 0.0500	time 0.5675 (0.5798)	loss 1.2487 (1.1744)	grad_norm 1.7735 (2.4688)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:40:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:06:46 lr 0.000019	 wd 0.0500	time 0.5596 (0.5795)	loss 0.7559 (1.1765)	grad_norm 2.2019 (2.4569)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:41:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:05:48 lr 0.000019	 wd 0.0500	time 0.5502 (0.5793)	loss 1.2039 (1.1771)	grad_norm 2.1206 (2.4774)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:42:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:04:50 lr 0.000019	 wd 0.0500	time 0.5566 (0.5791)	loss 1.3148 (1.1777)	grad_norm 2.0928 (2.5153)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:43:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:03:52 lr 0.000019	 wd 0.0500	time 0.5514 (0.5788)	loss 1.5642 (1.1802)	grad_norm 2.7716 (2.5407)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:44:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:02:54 lr 0.000019	 wd 0.0500	time 0.5587 (0.5786)	loss 1.2908 (1.1781)	grad_norm 2.3599 (2.5333)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:45:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:56 lr 0.000019	 wd 0.0500	time 0.5579 (0.5784)	loss 1.4920 (1.1775)	grad_norm 1.4522 (2.5202)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:46:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:58 lr 0.000019	 wd 0.0500	time 0.5489 (0.5783)	loss 1.3564 (1.1774)	grad_norm 3.5137 (2.5042)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:47:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5601 (0.5781)	loss 1.5312 (1.1767)	grad_norm 1.6071 (2.4890)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:47:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 7 training takes 0:24:08
[2024-08-02 13:47:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.485 (11.485)	Loss 0.4961 (0.4961)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 13:47:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.778 Acc@5 97.732
[2024-08-02 13:47:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 13:47:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.78%
[2024-08-02 13:47:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 13:47:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 13:48:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:27:18 lr 0.000019	 wd 0.0500	time 12.1657 (12.1657)	loss 1.2640 (1.2640)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:49:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:27:28 lr 0.000019	 wd 0.0500	time 0.5542 (0.6862)	loss 0.8978 (1.2365)	grad_norm 2.0383 (2.5389)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:50:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:24:09 lr 0.000019	 wd 0.0500	time 0.5572 (0.6295)	loss 0.8785 (1.2095)	grad_norm 3.0429 (2.5452)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:51:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:22:24 lr 0.000019	 wd 0.0500	time 0.5463 (0.6107)	loss 1.4723 (1.1945)	grad_norm 2.1674 (2.4913)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:51:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:21:03 lr 0.000019	 wd 0.0500	time 0.5590 (0.6012)	loss 0.9091 (1.1913)	grad_norm 2.8034 (2.4919)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:52:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:19:52 lr 0.000019	 wd 0.0500	time 0.5561 (0.5956)	loss 0.8183 (1.1815)	grad_norm 2.1834 (2.5333)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:53:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:18:46 lr 0.000019	 wd 0.0500	time 0.5544 (0.5921)	loss 0.8569 (1.1811)	grad_norm 1.6322 (2.4813)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:54:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:17:42 lr 0.000019	 wd 0.0500	time 0.5611 (0.5896)	loss 1.3431 (1.1807)	grad_norm 2.0051 (2.4328)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:55:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:16:40 lr 0.000019	 wd 0.0500	time 0.5617 (0.5877)	loss 1.3216 (1.1830)	grad_norm 1.3715 (2.4044)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:56:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:15:39 lr 0.000019	 wd 0.0500	time 0.5520 (0.5862)	loss 0.9418 (1.1795)	grad_norm 1.7483 (2.3901)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:57:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:14:38 lr 0.000019	 wd 0.0500	time 0.5663 (0.5850)	loss 1.3656 (1.1816)	grad_norm 1.9057 (2.3790)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:58:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:13:38 lr 0.000019	 wd 0.0500	time 0.5584 (0.5841)	loss 1.3207 (1.1786)	grad_norm 1.7856 (2.3724)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 13:59:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:12:39 lr 0.000019	 wd 0.0500	time 0.5604 (0.5833)	loss 1.0869 (1.1793)	grad_norm 2.2903 (2.3957)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:00:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:11:40 lr 0.000019	 wd 0.0500	time 0.5561 (0.5825)	loss 0.8371 (1.1792)	grad_norm 2.1327 (2.3887)	loss_scale 4096.0000 (2148.7471)	mem 20413MB
[2024-08-02 14:01:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:10:41 lr 0.000019	 wd 0.0500	time 0.5566 (0.5820)	loss 1.2946 (1.1803)	grad_norm 1.7936 (2.3853)	loss_scale 4096.0000 (2287.7373)	mem 20413MB
[2024-08-02 14:02:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:09:42 lr 0.000019	 wd 0.0500	time 0.5601 (0.5815)	loss 1.1198 (1.1810)	grad_norm 6.4015 (2.3795)	loss_scale 4096.0000 (2408.2079)	mem 20413MB
[2024-08-02 14:03:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:08:44 lr 0.000019	 wd 0.0500	time 0.5570 (0.5811)	loss 1.4148 (1.1829)	grad_norm 1.8659 (2.3737)	loss_scale 4096.0000 (2513.6290)	mem 20413MB
[2024-08-02 14:04:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:07:45 lr 0.000019	 wd 0.0500	time 0.5612 (0.5807)	loss 0.7939 (1.1853)	grad_norm 2.1674 (inf)	loss_scale 2048.0000 (2584.9830)	mem 20413MB
[2024-08-02 14:05:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:06:47 lr 0.000019	 wd 0.0500	time 0.5507 (0.5803)	loss 0.8133 (1.1838)	grad_norm 2.2765 (inf)	loss_scale 2048.0000 (2555.1671)	mem 20413MB
[2024-08-02 14:06:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:05:49 lr 0.000019	 wd 0.0500	time 0.5510 (0.5800)	loss 1.2867 (1.1856)	grad_norm 2.6686 (inf)	loss_scale 2048.0000 (2528.4882)	mem 20413MB
[2024-08-02 14:07:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:04:51 lr 0.000019	 wd 0.0500	time 0.5627 (0.5798)	loss 0.7668 (1.1844)	grad_norm 1.8038 (inf)	loss_scale 2048.0000 (2504.4758)	mem 20413MB
[2024-08-02 14:08:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:03:52 lr 0.000019	 wd 0.0500	time 0.5570 (0.5795)	loss 0.9011 (1.1852)	grad_norm 2.1013 (inf)	loss_scale 2048.0000 (2482.7492)	mem 20413MB
[2024-08-02 14:09:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:02:54 lr 0.000019	 wd 0.0500	time 0.5541 (0.5793)	loss 1.2270 (1.1838)	grad_norm 3.6551 (inf)	loss_scale 2048.0000 (2462.9968)	mem 20413MB
[2024-08-02 14:10:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:56 lr 0.000019	 wd 0.0500	time 0.5610 (0.5791)	loss 1.2912 (1.1821)	grad_norm 1.5864 (inf)	loss_scale 2048.0000 (2444.9613)	mem 20413MB
[2024-08-02 14:11:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:59 lr 0.000019	 wd 0.0500	time 0.5474 (0.5789)	loss 1.1001 (1.1820)	grad_norm 2.0957 (inf)	loss_scale 2048.0000 (2428.4282)	mem 20413MB
[2024-08-02 14:12:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5426 (0.5787)	loss 0.9220 (1.1816)	grad_norm 4.0592 (inf)	loss_scale 2048.0000 (2413.2171)	mem 20413MB
[2024-08-02 14:12:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 8 training takes 0:24:10
[2024-08-02 14:12:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.903 (11.903)	Loss 0.4927 (0.4927)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 14:12:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.780 Acc@5 97.742
[2024-08-02 14:12:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 14:12:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.78%
[2024-08-02 14:12:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 14:12:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 14:12:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:50:28 lr 0.000019	 wd 0.0500	time 11.2825 (11.2825)	loss 1.3670 (1.3670)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:13:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:27:07 lr 0.000019	 wd 0.0500	time 0.5578 (0.6777)	loss 1.2858 (1.1697)	grad_norm 2.6012 (4.7533)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:14:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:23:59 lr 0.000019	 wd 0.0500	time 0.5592 (0.6253)	loss 0.9900 (1.1918)	grad_norm 2.0724 (3.5302)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:15:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:22:18 lr 0.000019	 wd 0.0500	time 0.5524 (0.6079)	loss 1.4332 (1.1968)	grad_norm 3.0502 (3.0796)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:16:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:20:59 lr 0.000019	 wd 0.0500	time 0.5630 (0.5992)	loss 0.9248 (1.1769)	grad_norm 2.0170 (2.8451)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:17:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:19:49 lr 0.000019	 wd 0.0500	time 0.5579 (0.5941)	loss 0.7135 (1.1721)	grad_norm 2.2951 (2.7177)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:18:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:18:43 lr 0.000019	 wd 0.0500	time 0.5612 (0.5907)	loss 0.7865 (1.1747)	grad_norm 2.2556 (2.6087)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:19:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:17:40 lr 0.000019	 wd 0.0500	time 0.5576 (0.5884)	loss 1.4478 (1.1772)	grad_norm 1.2391 (2.5398)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:20:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:16:38 lr 0.000019	 wd 0.0500	time 0.5677 (0.5866)	loss 1.4335 (1.1792)	grad_norm 1.7212 (2.5177)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:21:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:15:37 lr 0.000019	 wd 0.0500	time 0.5626 (0.5852)	loss 1.3818 (1.1792)	grad_norm 1.5194 (2.5288)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:22:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:14:37 lr 0.000019	 wd 0.0500	time 0.5539 (0.5841)	loss 1.2839 (1.1797)	grad_norm 1.3338 (2.5052)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:23:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:13:37 lr 0.000018	 wd 0.0500	time 0.5576 (0.5832)	loss 1.4310 (1.1755)	grad_norm 1.7178 (2.4902)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:24:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:12:38 lr 0.000018	 wd 0.0500	time 0.5618 (0.5825)	loss 0.9705 (1.1758)	grad_norm 4.8584 (2.4903)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:25:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:11:39 lr 0.000018	 wd 0.0500	time 0.5534 (0.5819)	loss 1.3612 (1.1767)	grad_norm 1.9210 (2.4680)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:26:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:10:40 lr 0.000018	 wd 0.0500	time 0.5562 (0.5814)	loss 0.9436 (1.1749)	grad_norm 1.6263 (2.5142)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:27:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:09:42 lr 0.000018	 wd 0.0500	time 0.5580 (0.5809)	loss 0.9224 (1.1777)	grad_norm 1.7504 (2.4880)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:28:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:08:43 lr 0.000018	 wd 0.0500	time 0.5475 (0.5805)	loss 0.9433 (1.1757)	grad_norm 3.1365 (2.4811)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:29:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:07:45 lr 0.000018	 wd 0.0500	time 0.5616 (0.5802)	loss 1.4460 (1.1759)	grad_norm 2.8897 (2.4648)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:30:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:06:47 lr 0.000018	 wd 0.0500	time 0.5600 (0.5798)	loss 1.3194 (1.1752)	grad_norm 1.9195 (2.4446)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:31:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:05:48 lr 0.000018	 wd 0.0500	time 0.5516 (0.5795)	loss 1.4154 (1.1742)	grad_norm 1.5169 (2.4311)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:32:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:04:50 lr 0.000018	 wd 0.0500	time 0.5598 (0.5792)	loss 1.2515 (1.1746)	grad_norm 1.7236 (2.4441)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:32:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:03:52 lr 0.000018	 wd 0.0500	time 0.5540 (0.5790)	loss 1.4058 (1.1757)	grad_norm 1.9416 (2.4307)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:33:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:54 lr 0.000018	 wd 0.0500	time 0.5560 (0.5788)	loss 1.2345 (1.1767)	grad_norm 2.0208 (2.4154)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:34:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:56 lr 0.000018	 wd 0.0500	time 0.5582 (0.5786)	loss 0.8247 (1.1767)	grad_norm 1.8695 (2.4181)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:35:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:58 lr 0.000018	 wd 0.0500	time 0.5520 (0.5784)	loss 1.1445 (1.1771)	grad_norm 2.3216 (2.4439)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:36:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.5611 (0.5782)	loss 1.4134 (1.1766)	grad_norm 2.3647 (2.4456)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:36:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 9 training takes 0:24:09
[2024-08-02 14:37:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.490 (12.490)	Loss 0.5039 (0.5039)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 14:37:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.802 Acc@5 97.748
[2024-08-02 14:37:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 14:37:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.80%
[2024-08-02 14:37:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 14:37:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 14:37:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:56:49 lr 0.000018	 wd 0.0500	time 11.4347 (11.4347)	loss 1.4303 (1.4303)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:38:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:27:11 lr 0.000018	 wd 0.0500	time 0.5552 (0.6791)	loss 0.8109 (1.1836)	grad_norm 2.3912 (2.3171)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:39:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:24:00 lr 0.000018	 wd 0.0500	time 0.5574 (0.6258)	loss 1.3943 (1.1743)	grad_norm 1.7893 (2.4720)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:40:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:22:19 lr 0.000018	 wd 0.0500	time 0.5584 (0.6081)	loss 1.3777 (1.1746)	grad_norm 5.4597 (2.4973)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:41:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:20:59 lr 0.000018	 wd 0.0500	time 0.5484 (0.5993)	loss 0.7771 (1.1824)	grad_norm 1.9819 (2.5172)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:42:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:19:49 lr 0.000018	 wd 0.0500	time 0.5583 (0.5942)	loss 0.7526 (1.1796)	grad_norm 1.3393 (2.5257)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:43:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:18:43 lr 0.000018	 wd 0.0500	time 0.5575 (0.5908)	loss 1.2570 (1.1858)	grad_norm 1.7658 (2.4958)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 14:44:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:17:40 lr 0.000018	 wd 0.0500	time 0.5648 (0.5885)	loss 1.3015 (1.1829)	grad_norm 1.7403 (2.4462)	loss_scale 4096.0000 (2112.2739)	mem 20413MB
[2024-08-02 14:45:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:16:38 lr 0.000018	 wd 0.0500	time 0.5592 (0.5866)	loss 1.2989 (1.1855)	grad_norm 1.7615 (2.4662)	loss_scale 4096.0000 (2359.9301)	mem 20413MB
[2024-08-02 14:46:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:15:37 lr 0.000018	 wd 0.0500	time 0.5455 (0.5853)	loss 1.3572 (1.1839)	grad_norm 1.9315 (2.4647)	loss_scale 4096.0000 (2552.6127)	mem 20413MB
[2024-08-02 14:47:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:14:37 lr 0.000018	 wd 0.0500	time 0.5467 (0.5841)	loss 1.3252 (1.1848)	grad_norm 2.3000 (2.4368)	loss_scale 4096.0000 (2706.7972)	mem 20413MB
[2024-08-02 14:48:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:13:37 lr 0.000018	 wd 0.0500	time 0.5652 (0.5833)	loss 1.0609 (1.1922)	grad_norm 2.4066 (2.4473)	loss_scale 4096.0000 (2832.9737)	mem 20413MB
[2024-08-02 14:49:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:12:38 lr 0.000018	 wd 0.0500	time 0.5561 (0.5825)	loss 1.4665 (1.1891)	grad_norm 1.9099 (2.5001)	loss_scale 4096.0000 (2938.1382)	mem 20413MB
[2024-08-02 14:50:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:11:39 lr 0.000018	 wd 0.0500	time 0.5564 (0.5819)	loss 1.2140 (1.1868)	grad_norm 3.6331 (2.4942)	loss_scale 4096.0000 (3027.1360)	mem 20413MB
[2024-08-02 14:50:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:10:40 lr 0.000018	 wd 0.0500	time 0.5569 (0.5813)	loss 1.4510 (1.1852)	grad_norm 1.6711 (2.5446)	loss_scale 4096.0000 (3103.4290)	mem 20413MB
[2024-08-02 14:51:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:09:42 lr 0.000018	 wd 0.0500	time 0.5622 (0.5809)	loss 1.2192 (1.1833)	grad_norm 3.1447 (inf)	loss_scale 2048.0000 (3074.0466)	mem 20413MB
[2024-08-02 14:52:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:08:43 lr 0.000018	 wd 0.0500	time 0.5552 (0.5804)	loss 1.1640 (1.1815)	grad_norm 3.1222 (inf)	loss_scale 2048.0000 (3009.9588)	mem 20413MB
[2024-08-02 14:53:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:07:45 lr 0.000018	 wd 0.0500	time 0.5684 (0.5801)	loss 1.3131 (1.1835)	grad_norm 3.1998 (inf)	loss_scale 2048.0000 (2953.4062)	mem 20413MB
[2024-08-02 14:54:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:06:46 lr 0.000018	 wd 0.0500	time 0.5467 (0.5797)	loss 0.9862 (1.1819)	grad_norm 2.2458 (inf)	loss_scale 2048.0000 (2903.1338)	mem 20413MB
[2024-08-02 14:55:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:05:48 lr 0.000018	 wd 0.0500	time 0.5570 (0.5794)	loss 1.0803 (1.1806)	grad_norm 2.9364 (inf)	loss_scale 2048.0000 (2858.1504)	mem 20413MB
[2024-08-02 14:56:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:04:50 lr 0.000017	 wd 0.0500	time 0.5586 (0.5792)	loss 1.3220 (1.1837)	grad_norm 2.5614 (inf)	loss_scale 2048.0000 (2817.6632)	mem 20413MB
[2024-08-02 14:57:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:03:52 lr 0.000017	 wd 0.0500	time 0.5520 (0.5790)	loss 1.1036 (1.1852)	grad_norm 3.2170 (inf)	loss_scale 2048.0000 (2781.0300)	mem 20413MB
[2024-08-02 14:58:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:02:54 lr 0.000017	 wd 0.0500	time 0.5604 (0.5787)	loss 1.3146 (1.1863)	grad_norm 1.7344 (inf)	loss_scale 2048.0000 (2747.7256)	mem 20413MB
[2024-08-02 14:59:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:56 lr 0.000017	 wd 0.0500	time 0.5553 (0.5785)	loss 1.2970 (1.1841)	grad_norm 2.4144 (inf)	loss_scale 2048.0000 (2717.3159)	mem 20413MB
[2024-08-02 15:00:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:58 lr 0.000017	 wd 0.0500	time 0.5582 (0.5784)	loss 1.4677 (1.1829)	grad_norm 2.2872 (inf)	loss_scale 2048.0000 (2689.4394)	mem 20413MB
[2024-08-02 15:01:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.5423 (0.5782)	loss 0.8643 (1.1820)	grad_norm 1.8566 (inf)	loss_scale 2048.0000 (2663.7921)	mem 20413MB
[2024-08-02 15:01:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 10 training takes 0:24:09
[2024-08-02 15:01:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.159 (12.159)	Loss 0.4976 (0.4976)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 15:02:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.834 Acc@5 97.760
[2024-08-02 15:02:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 15:02:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.83%
[2024-08-02 15:02:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 15:02:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 15:02:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:42:56 lr 0.000017	 wd 0.0500	time 11.1018 (11.1018)	loss 0.9614 (0.9614)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:03:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:27:07 lr 0.000017	 wd 0.0500	time 0.5542 (0.6776)	loss 1.3626 (1.1695)	grad_norm 2.9410 (2.5808)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:04:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:23:58 lr 0.000017	 wd 0.0500	time 0.5626 (0.6251)	loss 1.2816 (1.1805)	grad_norm 2.1415 (2.4814)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:05:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:22:18 lr 0.000017	 wd 0.0500	time 0.5563 (0.6076)	loss 0.9194 (1.1732)	grad_norm 2.6225 (2.3667)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:06:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:20:59 lr 0.000017	 wd 0.0500	time 0.5570 (0.5990)	loss 1.4392 (1.1758)	grad_norm 3.2445 (2.4163)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:07:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:19:49 lr 0.000017	 wd 0.0500	time 0.5610 (0.5940)	loss 0.7749 (1.1744)	grad_norm 1.7065 (2.3622)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:08:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:18:43 lr 0.000017	 wd 0.0500	time 0.5596 (0.5907)	loss 1.5702 (1.1763)	grad_norm 2.0947 (2.3581)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:09:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:17:40 lr 0.000017	 wd 0.0500	time 0.5693 (0.5884)	loss 0.9336 (1.1733)	grad_norm 2.8945 (2.3575)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:09:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:16:38 lr 0.000017	 wd 0.0500	time 0.5520 (0.5865)	loss 1.4593 (1.1718)	grad_norm 2.2748 (2.3118)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:10:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:15:37 lr 0.000017	 wd 0.0500	time 0.5592 (0.5851)	loss 0.8952 (1.1740)	grad_norm 1.5789 (2.2932)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:11:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:14:37 lr 0.000017	 wd 0.0500	time 0.5604 (0.5840)	loss 1.2737 (1.1749)	grad_norm 6.1707 (2.3361)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:12:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:13:37 lr 0.000017	 wd 0.0500	time 0.5520 (0.5831)	loss 0.9133 (1.1731)	grad_norm 3.0674 (2.3266)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:13:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:12:38 lr 0.000017	 wd 0.0500	time 0.5610 (0.5824)	loss 1.0177 (1.1692)	grad_norm 2.5605 (2.3260)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:14:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:11:39 lr 0.000017	 wd 0.0500	time 0.5562 (0.5817)	loss 1.2801 (1.1697)	grad_norm 2.3927 (2.3253)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:15:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:10:40 lr 0.000017	 wd 0.0500	time 0.5576 (0.5811)	loss 0.8954 (1.1706)	grad_norm 2.6303 (2.3327)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:16:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:09:41 lr 0.000017	 wd 0.0500	time 0.5602 (0.5806)	loss 1.4229 (1.1701)	grad_norm 2.7735 (2.3326)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:17:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:08:43 lr 0.000017	 wd 0.0500	time 0.5613 (0.5802)	loss 1.3052 (1.1715)	grad_norm 1.3939 (2.3295)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:18:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:07:45 lr 0.000017	 wd 0.0500	time 0.5552 (0.5799)	loss 1.0639 (1.1720)	grad_norm 2.0023 (2.3237)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:19:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:06:46 lr 0.000017	 wd 0.0500	time 0.5527 (0.5796)	loss 0.8778 (1.1711)	grad_norm 1.8313 (2.3590)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:20:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:05:48 lr 0.000017	 wd 0.0500	time 0.5554 (0.5793)	loss 1.0394 (1.1696)	grad_norm 1.9589 (2.4144)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:21:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:04:50 lr 0.000017	 wd 0.0500	time 0.5621 (0.5790)	loss 1.2452 (1.1700)	grad_norm 4.0918 (2.4295)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:22:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:03:52 lr 0.000017	 wd 0.0500	time 0.5471 (0.5788)	loss 1.4370 (1.1697)	grad_norm 1.7327 (2.4243)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:23:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:54 lr 0.000017	 wd 0.0500	time 0.5589 (0.5786)	loss 0.8967 (1.1711)	grad_norm 1.9386 (2.4560)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:24:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:56 lr 0.000016	 wd 0.0500	time 0.5605 (0.5784)	loss 1.1713 (1.1695)	grad_norm 2.4998 (2.4415)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:25:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:58 lr 0.000016	 wd 0.0500	time 0.5578 (0.5783)	loss 1.2988 (1.1708)	grad_norm 1.4790 (2.4390)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:26:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5613 (0.5781)	loss 0.8584 (1.1688)	grad_norm 2.0589 (2.4285)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:26:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 11 training takes 0:24:08
[2024-08-02 15:26:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.334 (12.334)	Loss 0.4946 (0.4946)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 15:26:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.738 Acc@5 97.756
[2024-08-02 15:26:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-08-02 15:26:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.83%
[2024-08-02 15:27:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:05:23 lr 0.000016	 wd 0.0500	time 11.6400 (11.6400)	loss 0.9388 (0.9388)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:27:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:27:24 lr 0.000016	 wd 0.0500	time 0.5540 (0.6847)	loss 1.3167 (1.1669)	grad_norm 1.5235 (2.1641)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:28:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:24:07 lr 0.000016	 wd 0.0500	time 0.5576 (0.6286)	loss 1.2279 (1.1650)	grad_norm 2.6491 (2.2732)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:29:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:22:23 lr 0.000016	 wd 0.0500	time 0.5573 (0.6103)	loss 0.9258 (1.1755)	grad_norm 1.9657 (2.3323)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:30:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:21:03 lr 0.000016	 wd 0.0500	time 0.5579 (0.6011)	loss 1.2421 (1.1695)	grad_norm 1.6498 (2.4329)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:31:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:19:52 lr 0.000016	 wd 0.0500	time 0.5596 (0.5956)	loss 1.4055 (1.1676)	grad_norm 1.5091 (2.4229)	loss_scale 4096.0000 (2350.4990)	mem 20413MB
[2024-08-02 15:32:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:18:45 lr 0.000016	 wd 0.0500	time 0.5543 (0.5919)	loss 1.4151 (1.1649)	grad_norm 2.5785 (2.3908)	loss_scale 4096.0000 (2640.9318)	mem 20413MB
[2024-08-02 15:33:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:17:41 lr 0.000016	 wd 0.0500	time 0.5584 (0.5893)	loss 1.2854 (1.1629)	grad_norm 2.6004 (2.3724)	loss_scale 4096.0000 (2848.5021)	mem 20413MB
[2024-08-02 15:34:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:16:39 lr 0.000016	 wd 0.0500	time 0.5587 (0.5874)	loss 1.3974 (1.1642)	grad_norm 1.6664 (2.3472)	loss_scale 4096.0000 (3004.2447)	mem 20413MB
[2024-08-02 15:35:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:15:38 lr 0.000016	 wd 0.0500	time 0.5574 (0.5859)	loss 1.3968 (1.1615)	grad_norm 2.1640 (2.3308)	loss_scale 4096.0000 (3125.4162)	mem 20413MB
[2024-08-02 15:36:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:14:38 lr 0.000016	 wd 0.0500	time 0.5636 (0.5848)	loss 1.2988 (1.1658)	grad_norm 1.5966 (2.3230)	loss_scale 4096.0000 (3222.3776)	mem 20413MB
[2024-08-02 15:37:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:13:38 lr 0.000016	 wd 0.0500	time 0.5582 (0.5839)	loss 0.9540 (1.1679)	grad_norm 2.1727 (2.3379)	loss_scale 4096.0000 (3301.7257)	mem 20413MB
[2024-08-02 15:38:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:12:39 lr 0.000016	 wd 0.0500	time 0.5639 (0.5831)	loss 1.3428 (1.1664)	grad_norm 1.6037 (2.3237)	loss_scale 4096.0000 (3367.8601)	mem 20413MB
[2024-08-02 15:39:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:11:40 lr 0.000016	 wd 0.0500	time 0.5509 (0.5824)	loss 1.0602 (1.1678)	grad_norm 1.4601 (2.3368)	loss_scale 4096.0000 (3423.8278)	mem 20413MB
[2024-08-02 15:40:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:10:41 lr 0.000016	 wd 0.0500	time 0.5580 (0.5819)	loss 1.4331 (1.1688)	grad_norm 1.8178 (2.3400)	loss_scale 4096.0000 (3471.8059)	mem 20413MB
[2024-08-02 15:41:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:09:42 lr 0.000016	 wd 0.0500	time 0.5619 (0.5814)	loss 0.8411 (1.1654)	grad_norm 1.6923 (2.3437)	loss_scale 4096.0000 (3513.3911)	mem 20413MB
[2024-08-02 15:42:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:08:44 lr 0.000016	 wd 0.0500	time 0.5568 (0.5809)	loss 1.4350 (1.1628)	grad_norm 1.9650 (2.3423)	loss_scale 4096.0000 (3549.7814)	mem 20413MB
[2024-08-02 15:43:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:07:45 lr 0.000016	 wd 0.0500	time 0.5599 (0.5806)	loss 1.2085 (1.1654)	grad_norm 2.2097 (2.3466)	loss_scale 4096.0000 (3581.8930)	mem 20413MB
[2024-08-02 15:44:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:06:47 lr 0.000016	 wd 0.0500	time 0.5548 (0.5802)	loss 0.8295 (1.1643)	grad_norm 2.2626 (2.3572)	loss_scale 4096.0000 (3610.4386)	mem 20413MB
[2024-08-02 15:45:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:05:49 lr 0.000016	 wd 0.0500	time 0.5539 (0.5799)	loss 1.2046 (1.1647)	grad_norm 1.6871 (2.3456)	loss_scale 4096.0000 (3635.9811)	mem 20413MB
[2024-08-02 15:46:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:04:50 lr 0.000016	 wd 0.0500	time 0.5587 (0.5796)	loss 1.0405 (1.1657)	grad_norm 4.3058 (2.3486)	loss_scale 4096.0000 (3658.9705)	mem 20413MB
[2024-08-02 15:47:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:03:52 lr 0.000016	 wd 0.0500	time 0.5492 (0.5794)	loss 0.8310 (1.1649)	grad_norm 1.9951 (inf)	loss_scale 2048.0000 (3619.3356)	mem 20413MB
[2024-08-02 15:48:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:02:54 lr 0.000016	 wd 0.0500	time 0.5558 (0.5792)	loss 0.7968 (1.1662)	grad_norm 3.2341 (inf)	loss_scale 2048.0000 (3547.9437)	mem 20413MB
[2024-08-02 15:49:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:56 lr 0.000015	 wd 0.0500	time 0.5639 (0.5790)	loss 1.0742 (1.1667)	grad_norm 1.9232 (inf)	loss_scale 2048.0000 (3482.7571)	mem 20413MB
[2024-08-02 15:49:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:59 lr 0.000015	 wd 0.0500	time 0.5574 (0.5788)	loss 1.3755 (1.1674)	grad_norm 2.0170 (inf)	loss_scale 2048.0000 (3423.0004)	mem 20413MB
[2024-08-02 15:50:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.5526 (0.5785)	loss 0.8012 (1.1667)	grad_norm 1.9254 (inf)	loss_scale 2048.0000 (3368.0224)	mem 20413MB
[2024-08-02 15:51:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 12 training takes 0:24:09
[2024-08-02 15:51:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.181 (12.181)	Loss 0.5146 (0.5146)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 15:51:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.788 Acc@5 97.748
[2024-08-02 15:51:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 15:51:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.83%
[2024-08-02 15:51:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:17:25 lr 0.000015	 wd 0.0500	time 11.9288 (11.9288)	loss 1.3159 (1.3159)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:52:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:27:27 lr 0.000015	 wd 0.0500	time 0.5636 (0.6859)	loss 1.4275 (1.2287)	grad_norm 1.1856 (2.2015)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:53:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:24:08 lr 0.000015	 wd 0.0500	time 0.5470 (0.6291)	loss 1.4258 (1.2132)	grad_norm 1.3996 (2.2819)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:54:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:22:23 lr 0.000015	 wd 0.0500	time 0.5563 (0.6102)	loss 1.4210 (1.2131)	grad_norm 1.7577 (2.2785)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:55:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:21:03 lr 0.000015	 wd 0.0500	time 0.5607 (0.6010)	loss 1.4511 (1.1963)	grad_norm 1.7571 (2.4643)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:56:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:19:52 lr 0.000015	 wd 0.0500	time 0.5533 (0.5954)	loss 1.3955 (1.1893)	grad_norm 2.8727 (2.4134)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:57:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:18:45 lr 0.000015	 wd 0.0500	time 0.5561 (0.5919)	loss 1.2789 (1.1828)	grad_norm 1.8894 (2.4314)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:58:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:17:42 lr 0.000015	 wd 0.0500	time 0.5587 (0.5893)	loss 1.1033 (1.1760)	grad_norm 2.0630 (2.4247)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 15:59:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:16:39 lr 0.000015	 wd 0.0500	time 0.5543 (0.5874)	loss 1.4633 (1.1757)	grad_norm 1.6465 (2.4190)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:00:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:15:38 lr 0.000015	 wd 0.0500	time 0.5614 (0.5859)	loss 1.1763 (1.1753)	grad_norm 1.9564 (2.4004)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:01:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:14:38 lr 0.000015	 wd 0.0500	time 0.5669 (0.5848)	loss 1.3022 (1.1746)	grad_norm 3.1858 (2.4406)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:02:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:13:38 lr 0.000015	 wd 0.0500	time 0.5492 (0.5839)	loss 1.3531 (1.1711)	grad_norm 1.9151 (2.4674)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:03:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:12:39 lr 0.000015	 wd 0.0500	time 0.5629 (0.5830)	loss 1.2743 (1.1752)	grad_norm 1.8854 (2.4309)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:04:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:11:39 lr 0.000015	 wd 0.0500	time 0.5497 (0.5823)	loss 0.8698 (1.1749)	grad_norm 2.6731 (2.4603)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:05:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:10:41 lr 0.000015	 wd 0.0500	time 0.5611 (0.5817)	loss 1.4291 (1.1734)	grad_norm 1.7559 (2.4740)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:06:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:09:42 lr 0.000015	 wd 0.0500	time 0.5635 (0.5812)	loss 1.2180 (1.1737)	grad_norm 2.3278 (2.6032)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:07:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:08:43 lr 0.000015	 wd 0.0500	time 0.5592 (0.5807)	loss 0.9899 (1.1734)	grad_norm 1.5590 (2.5789)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:08:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:07:45 lr 0.000015	 wd 0.0500	time 0.5559 (0.5803)	loss 1.0995 (1.1742)	grad_norm 2.4730 (2.5589)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:08:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:06:47 lr 0.000015	 wd 0.0500	time 0.5493 (0.5800)	loss 1.4319 (1.1745)	grad_norm 1.9565 (2.5374)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:09:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:05:48 lr 0.000015	 wd 0.0500	time 0.5504 (0.5797)	loss 1.1213 (1.1757)	grad_norm 2.0320 (2.5119)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:10:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:04:50 lr 0.000015	 wd 0.0500	time 0.5509 (0.5794)	loss 1.3794 (1.1754)	grad_norm 1.9658 (2.5021)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:11:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:03:52 lr 0.000014	 wd 0.0500	time 0.5611 (0.5792)	loss 1.4541 (1.1758)	grad_norm 1.6016 (2.5128)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:12:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:54 lr 0.000014	 wd 0.0500	time 0.5517 (0.5790)	loss 1.1984 (1.1743)	grad_norm 2.8048 (2.5002)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:13:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:56 lr 0.000014	 wd 0.0500	time 0.5568 (0.5788)	loss 1.3672 (1.1739)	grad_norm 2.1831 (2.4968)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:14:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:59 lr 0.000014	 wd 0.0500	time 0.5579 (0.5786)	loss 0.8875 (1.1769)	grad_norm 1.5171 (2.4835)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:15:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.5625 (0.5784)	loss 0.8365 (1.1748)	grad_norm 1.9445 (2.4707)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:15:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 13 training takes 0:24:09
[2024-08-02 16:15:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.736 (11.736)	Loss 0.4871 (0.4871)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 16:16:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.828 Acc@5 97.768
[2024-08-02 16:16:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 16:16:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.83%
[2024-08-02 16:16:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:39:00 lr 0.000014	 wd 0.0500	time 11.0074 (11.0074)	loss 1.3384 (1.3384)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:17:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:27:32 lr 0.000014	 wd 0.0500	time 0.5593 (0.6879)	loss 1.0831 (1.1811)	grad_norm 2.1355 (2.5250)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 16:18:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:24:10 lr 0.000014	 wd 0.0500	time 0.5519 (0.6299)	loss 1.3960 (1.1860)	grad_norm 1.4725 (nan)	loss_scale 1024.0000 (1966.4876)	mem 20413MB
[2024-08-02 16:19:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:22:25 lr 0.000014	 wd 0.0500	time 0.5592 (0.6110)	loss 0.9728 (1.1878)	grad_norm 2.3386 (nan)	loss_scale 1024.0000 (1653.3688)	mem 20413MB
[2024-08-02 16:20:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:21:04 lr 0.000014	 wd 0.0500	time 0.5562 (0.6017)	loss 1.6443 (1.1888)	grad_norm 1.8482 (nan)	loss_scale 1024.0000 (1496.4190)	mem 20413MB
[2024-08-02 16:21:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:19:53 lr 0.000014	 wd 0.0500	time 0.5543 (0.5961)	loss 0.9709 (1.1842)	grad_norm 4.0220 (nan)	loss_scale 1024.0000 (1402.1238)	mem 20413MB
[2024-08-02 16:22:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:18:46 lr 0.000014	 wd 0.0500	time 0.5594 (0.5923)	loss 0.9940 (1.1815)	grad_norm 1.9608 (nan)	loss_scale 1024.0000 (1339.2080)	mem 20413MB
[2024-08-02 16:23:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:17:42 lr 0.000014	 wd 0.0500	time 0.5520 (0.5897)	loss 1.2279 (1.1835)	grad_norm 1.7972 (nan)	loss_scale 1024.0000 (1294.2425)	mem 20413MB
[2024-08-02 16:24:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:16:40 lr 0.000014	 wd 0.0500	time 0.5474 (0.5878)	loss 0.7694 (1.1778)	grad_norm 2.1655 (nan)	loss_scale 1024.0000 (1260.5044)	mem 20413MB
[2024-08-02 16:25:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:15:39 lr 0.000014	 wd 0.0500	time 0.5514 (0.5863)	loss 0.8284 (1.1765)	grad_norm 1.7866 (nan)	loss_scale 1024.0000 (1234.2553)	mem 20413MB
[2024-08-02 16:26:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:14:38 lr 0.000014	 wd 0.0500	time 0.5628 (0.5850)	loss 1.0186 (1.1742)	grad_norm 1.5463 (nan)	loss_scale 1024.0000 (1213.2507)	mem 20413MB
[2024-08-02 16:26:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:13:38 lr 0.000014	 wd 0.0500	time 0.5541 (0.5840)	loss 1.3800 (1.1748)	grad_norm 1.5229 (nan)	loss_scale 1024.0000 (1196.0618)	mem 20413MB
[2024-08-02 16:27:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:12:39 lr 0.000014	 wd 0.0500	time 0.5665 (0.5832)	loss 1.5507 (1.1746)	grad_norm 2.0795 (nan)	loss_scale 1024.0000 (1181.7352)	mem 20413MB
[2024-08-02 16:28:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:11:40 lr 0.000014	 wd 0.0500	time 0.5590 (0.5825)	loss 1.2277 (1.1781)	grad_norm 5.9878 (nan)	loss_scale 1024.0000 (1169.6111)	mem 20413MB
[2024-08-02 16:29:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:10:41 lr 0.000014	 wd 0.0500	time 0.5602 (0.5819)	loss 1.3298 (1.1784)	grad_norm 9.2739 (nan)	loss_scale 1024.0000 (1159.2177)	mem 20413MB
[2024-08-02 16:30:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:09:42 lr 0.000014	 wd 0.0500	time 0.5553 (0.5814)	loss 1.4644 (1.1772)	grad_norm 2.8573 (nan)	loss_scale 1024.0000 (1150.2092)	mem 20413MB
[2024-08-02 16:31:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:08:44 lr 0.000014	 wd 0.0500	time 0.5511 (0.5810)	loss 0.9290 (1.1763)	grad_norm 2.6712 (nan)	loss_scale 1024.0000 (1142.3260)	mem 20413MB
[2024-08-02 16:32:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:07:45 lr 0.000014	 wd 0.0500	time 0.5582 (0.5807)	loss 0.8868 (1.1755)	grad_norm 3.4741 (nan)	loss_scale 1024.0000 (1135.3698)	mem 20413MB
[2024-08-02 16:33:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:06:47 lr 0.000013	 wd 0.0500	time 0.5588 (0.5803)	loss 1.0583 (1.1766)	grad_norm 1.9462 (nan)	loss_scale 1024.0000 (1129.1860)	mem 20413MB
[2024-08-02 16:34:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:05:49 lr 0.000013	 wd 0.0500	time 0.5529 (0.5800)	loss 1.2586 (1.1775)	grad_norm 3.7286 (nan)	loss_scale 1024.0000 (1123.6528)	mem 20413MB
[2024-08-02 16:35:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:04:51 lr 0.000013	 wd 0.0500	time 0.5640 (0.5797)	loss 1.4360 (1.1760)	grad_norm 2.4967 (nan)	loss_scale 1024.0000 (1118.6727)	mem 20413MB
[2024-08-02 16:36:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:03:52 lr 0.000013	 wd 0.0500	time 0.5595 (0.5795)	loss 1.3036 (1.1753)	grad_norm 1.7396 (nan)	loss_scale 1024.0000 (1114.1666)	mem 20413MB
[2024-08-02 16:37:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:54 lr 0.000013	 wd 0.0500	time 0.5525 (0.5792)	loss 1.2665 (1.1745)	grad_norm 5.3475 (nan)	loss_scale 1024.0000 (1110.0700)	mem 20413MB
[2024-08-02 16:38:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:56 lr 0.000013	 wd 0.0500	time 0.5503 (0.5790)	loss 0.8320 (1.1739)	grad_norm 2.1290 (nan)	loss_scale 1024.0000 (1106.3294)	mem 20413MB
[2024-08-02 16:39:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:59 lr 0.000013	 wd 0.0500	time 0.5532 (0.5788)	loss 0.8244 (1.1733)	grad_norm 2.4465 (nan)	loss_scale 1024.0000 (1102.9005)	mem 20413MB
[2024-08-02 16:40:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.5559 (0.5786)	loss 1.3305 (1.1724)	grad_norm 2.0030 (nan)	loss_scale 1024.0000 (1099.7457)	mem 20413MB
[2024-08-02 16:40:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 14 training takes 0:24:10
[2024-08-02 16:40:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.803 (12.803)	Loss 0.4761 (0.4761)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 16:40:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.812 Acc@5 97.782
[2024-08-02 16:40:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 16:40:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.83%
[2024-08-02 16:41:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:47:02 lr 0.000013	 wd 0.0500	time 11.1999 (11.1999)	loss 1.3108 (1.3108)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:42:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:27:21 lr 0.000013	 wd 0.0500	time 0.5574 (0.6836)	loss 0.7860 (1.1707)	grad_norm 2.2893 (3.6132)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:43:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:24:06 lr 0.000013	 wd 0.0500	time 0.5584 (0.6282)	loss 1.5489 (1.1602)	grad_norm 1.7482 (3.0261)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:44:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:22:22 lr 0.000013	 wd 0.0500	time 0.5614 (0.6097)	loss 0.9168 (1.1583)	grad_norm 1.6981 (3.2108)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:45:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:21:02 lr 0.000013	 wd 0.0500	time 0.5457 (0.6005)	loss 0.7775 (1.1565)	grad_norm 1.9907 (3.0130)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:45:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:19:51 lr 0.000013	 wd 0.0500	time 0.5629 (0.5951)	loss 0.7477 (1.1665)	grad_norm 1.7490 (2.9097)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:46:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:18:45 lr 0.000013	 wd 0.0500	time 0.5555 (0.5916)	loss 1.2967 (1.1712)	grad_norm 2.9209 (2.8594)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:47:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:17:41 lr 0.000013	 wd 0.0500	time 0.5532 (0.5891)	loss 1.3509 (1.1734)	grad_norm 1.9717 (2.7780)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:48:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:16:39 lr 0.000013	 wd 0.0500	time 0.5581 (0.5872)	loss 1.1951 (1.1744)	grad_norm 4.1880 (2.7322)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:49:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:15:38 lr 0.000013	 wd 0.0500	time 0.5533 (0.5857)	loss 1.3755 (1.1758)	grad_norm 2.9281 (2.6669)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:50:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:14:37 lr 0.000013	 wd 0.0500	time 0.5544 (0.5845)	loss 0.7898 (1.1757)	grad_norm 3.8748 (2.6336)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:51:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:13:38 lr 0.000013	 wd 0.0500	time 0.5579 (0.5836)	loss 0.8724 (1.1763)	grad_norm 2.4047 (2.5831)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:52:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:12:38 lr 0.000013	 wd 0.0500	time 0.5553 (0.5827)	loss 0.8834 (1.1764)	grad_norm 4.6373 (2.5851)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:53:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:11:39 lr 0.000013	 wd 0.0500	time 0.5545 (0.5821)	loss 0.9396 (1.1775)	grad_norm 1.9739 (2.5580)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:54:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:10:40 lr 0.000012	 wd 0.0500	time 0.5532 (0.5815)	loss 0.8476 (1.1775)	grad_norm 1.7831 (2.5448)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:55:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:09:42 lr 0.000012	 wd 0.0500	time 0.5630 (0.5811)	loss 1.0860 (1.1733)	grad_norm 1.9277 (2.5232)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:56:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:08:43 lr 0.000012	 wd 0.0500	time 0.5632 (0.5806)	loss 0.9378 (1.1743)	grad_norm 2.3003 (2.5074)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 16:57:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:07:45 lr 0.000012	 wd 0.0500	time 0.5496 (0.5802)	loss 0.7861 (1.1748)	grad_norm 1.8863 (2.5487)	loss_scale 2048.0000 (1034.8360)	mem 20413MB
[2024-08-02 16:58:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:06:47 lr 0.000012	 wd 0.0500	time 0.5543 (0.5799)	loss 1.6953 (1.1731)	grad_norm 1.6876 (2.5306)	loss_scale 2048.0000 (1091.0916)	mem 20413MB
[2024-08-02 16:59:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:05:48 lr 0.000012	 wd 0.0500	time 0.5620 (0.5797)	loss 0.9370 (1.1722)	grad_norm 44.1093 (2.5933)	loss_scale 2048.0000 (1141.4287)	mem 20413MB
[2024-08-02 17:00:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:04:50 lr 0.000012	 wd 0.0500	time 0.5592 (0.5794)	loss 0.9517 (1.1733)	grad_norm 1.6890 (2.5962)	loss_scale 2048.0000 (1186.7346)	mem 20413MB
[2024-08-02 17:01:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:03:52 lr 0.000012	 wd 0.0500	time 0.5605 (0.5792)	loss 0.7495 (1.1720)	grad_norm 2.0178 (2.6174)	loss_scale 2048.0000 (1227.7277)	mem 20413MB
[2024-08-02 17:02:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:54 lr 0.000012	 wd 0.0500	time 0.5583 (0.5790)	loss 0.9113 (1.1715)	grad_norm 2.2175 (2.6054)	loss_scale 2048.0000 (1264.9959)	mem 20413MB
[2024-08-02 17:03:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:56 lr 0.000012	 wd 0.0500	time 0.5562 (0.5788)	loss 1.0350 (1.1720)	grad_norm 2.4516 (2.5894)	loss_scale 2048.0000 (1299.0248)	mem 20413MB
[2024-08-02 17:04:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:59 lr 0.000012	 wd 0.0500	time 0.5620 (0.5786)	loss 1.3712 (1.1723)	grad_norm 1.6646 (2.5779)	loss_scale 2048.0000 (1330.2191)	mem 20413MB
[2024-08-02 17:05:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.5599 (0.5784)	loss 1.5027 (1.1733)	grad_norm 1.9853 (2.5673)	loss_scale 2048.0000 (1358.9188)	mem 20413MB
[2024-08-02 17:05:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 15 training takes 0:24:09
[2024-08-02 17:05:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_15.pth saving......
[2024-08-02 17:05:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_15.pth saved !!!
[2024-08-02 17:05:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.100 (11.100)	Loss 0.5269 (0.5269)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 17:05:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.810 Acc@5 97.740
[2024-08-02 17:05:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 17:05:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.83%
[2024-08-02 17:05:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:37:25 lr 0.000012	 wd 0.0500	time 12.4083 (12.4083)	loss 1.2343 (1.2343)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:06:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:27:34 lr 0.000012	 wd 0.0500	time 0.5475 (0.6890)	loss 1.1417 (1.1835)	grad_norm 1.6630 (2.6950)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:07:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:24:12 lr 0.000012	 wd 0.0500	time 0.5569 (0.6308)	loss 1.2519 (1.1727)	grad_norm 1.6702 (2.5529)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:08:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:22:26 lr 0.000012	 wd 0.0500	time 0.5526 (0.6115)	loss 1.0546 (1.1665)	grad_norm 1.7356 (2.4101)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:09:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:21:05 lr 0.000012	 wd 0.0500	time 0.5412 (0.6021)	loss 0.8557 (1.1559)	grad_norm 2.8821 (2.3581)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:10:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:19:54 lr 0.000012	 wd 0.0500	time 0.5517 (0.5964)	loss 1.4747 (1.1602)	grad_norm 1.9779 (2.3559)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:11:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:18:47 lr 0.000012	 wd 0.0500	time 0.5534 (0.5927)	loss 1.3873 (1.1586)	grad_norm 1.8257 (2.3276)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:12:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:17:43 lr 0.000012	 wd 0.0500	time 0.5606 (0.5901)	loss 1.0392 (1.1606)	grad_norm 3.3048 (2.3821)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:13:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:16:40 lr 0.000012	 wd 0.0500	time 0.5531 (0.5881)	loss 1.3535 (1.1623)	grad_norm 1.7053 (2.4097)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:14:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:15:39 lr 0.000012	 wd 0.0500	time 0.5563 (0.5866)	loss 1.1038 (1.1604)	grad_norm 2.2580 (2.4069)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:15:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:14:39 lr 0.000011	 wd 0.0500	time 0.5586 (0.5854)	loss 1.3397 (1.1615)	grad_norm 1.7757 (2.4075)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:16:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:13:39 lr 0.000011	 wd 0.0500	time 0.5526 (0.5844)	loss 0.9117 (1.1669)	grad_norm 1.9229 (2.4216)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:17:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:12:39 lr 0.000011	 wd 0.0500	time 0.5577 (0.5835)	loss 0.9098 (1.1664)	grad_norm 2.3030 (2.4652)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:18:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:11:40 lr 0.000011	 wd 0.0500	time 0.5583 (0.5828)	loss 1.3230 (1.1697)	grad_norm 1.8120 (2.4803)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:19:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:10:41 lr 0.000011	 wd 0.0500	time 0.5639 (0.5823)	loss 1.2503 (1.1705)	grad_norm 2.3941 (2.4819)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:20:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:09:42 lr 0.000011	 wd 0.0500	time 0.5612 (0.5818)	loss 1.2031 (1.1725)	grad_norm 1.4074 (2.4741)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:21:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:08:44 lr 0.000011	 wd 0.0500	time 0.5584 (0.5813)	loss 1.2583 (1.1728)	grad_norm 1.9879 (2.4855)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:22:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:07:45 lr 0.000011	 wd 0.0500	time 0.5587 (0.5809)	loss 1.0981 (1.1722)	grad_norm 1.7651 (2.4675)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:23:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:06:47 lr 0.000011	 wd 0.0500	time 0.5505 (0.5806)	loss 0.9166 (1.1710)	grad_norm 1.9861 (2.4617)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:24:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:05:49 lr 0.000011	 wd 0.0500	time 0.5691 (0.5802)	loss 0.9046 (1.1717)	grad_norm 1.6950 (2.4532)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:25:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:04:51 lr 0.000011	 wd 0.0500	time 0.5576 (0.5800)	loss 1.3343 (1.1722)	grad_norm 1.5134 (2.4427)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:26:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:03:53 lr 0.000011	 wd 0.0500	time 0.5635 (0.5797)	loss 1.2625 (1.1729)	grad_norm 3.1887 (2.4582)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:26:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:02:54 lr 0.000011	 wd 0.0500	time 0.5610 (0.5794)	loss 0.9216 (1.1725)	grad_norm 2.7649 (2.4565)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:27:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:57 lr 0.000011	 wd 0.0500	time 0.5546 (0.5792)	loss 0.8363 (1.1707)	grad_norm 2.5111 (2.4388)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:28:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:59 lr 0.000011	 wd 0.0500	time 0.5603 (0.5790)	loss 1.2780 (1.1702)	grad_norm 1.5973 (2.4431)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 17:29:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.5596 (0.5788)	loss 0.8357 (1.1697)	grad_norm 2.6382 (nan)	loss_scale 1024.0000 (2042.2679)	mem 20413MB
[2024-08-02 17:29:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 16 training takes 0:24:10
[2024-08-02 17:30:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.064 (12.064)	Loss 0.4995 (0.4995)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 17:30:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.780 Acc@5 97.760
[2024-08-02 17:30:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 17:30:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.83%
[2024-08-02 17:30:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:35:58 lr 0.000011	 wd 0.0500	time 10.9348 (10.9348)	loss 1.3064 (1.3064)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:31:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:27:15 lr 0.000011	 wd 0.0500	time 0.5558 (0.6809)	loss 1.2939 (1.1463)	grad_norm 1.7994 (2.4072)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:32:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:24:02 lr 0.000011	 wd 0.0500	time 0.5498 (0.6266)	loss 1.3777 (1.1522)	grad_norm 1.8448 (2.3568)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:33:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:22:20 lr 0.000011	 wd 0.0500	time 0.5620 (0.6087)	loss 1.4280 (1.1669)	grad_norm 2.4809 (2.3677)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:34:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:21:00 lr 0.000011	 wd 0.0500	time 0.5578 (0.5998)	loss 1.5269 (1.1622)	grad_norm 1.6624 (2.3745)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:35:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:19:50 lr 0.000010	 wd 0.0500	time 0.5616 (0.5947)	loss 1.3606 (1.1691)	grad_norm 2.2556 (2.3972)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:36:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:18:44 lr 0.000010	 wd 0.0500	time 0.5572 (0.5911)	loss 0.8861 (1.1702)	grad_norm 1.7185 (2.3842)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:37:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:17:40 lr 0.000010	 wd 0.0500	time 0.5560 (0.5887)	loss 1.3468 (1.1673)	grad_norm 1.4308 (2.3187)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:38:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:16:38 lr 0.000010	 wd 0.0500	time 0.5623 (0.5868)	loss 1.2361 (1.1669)	grad_norm 2.3396 (2.3204)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:39:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:15:37 lr 0.000010	 wd 0.0500	time 0.5585 (0.5854)	loss 1.3407 (1.1690)	grad_norm 1.7711 (2.3136)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:40:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:14:37 lr 0.000010	 wd 0.0500	time 0.5692 (0.5843)	loss 0.9963 (1.1674)	grad_norm 1.6570 (2.3223)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:41:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:13:37 lr 0.000010	 wd 0.0500	time 0.5572 (0.5833)	loss 0.9979 (1.1723)	grad_norm 1.8248 (2.3196)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:42:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:12:38 lr 0.000010	 wd 0.0500	time 0.5513 (0.5825)	loss 0.8583 (1.1721)	grad_norm 2.8256 (2.3304)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:43:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:11:39 lr 0.000010	 wd 0.0500	time 0.5580 (0.5819)	loss 1.5141 (1.1708)	grad_norm 2.0120 (2.3530)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:44:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:10:40 lr 0.000010	 wd 0.0500	time 0.5614 (0.5814)	loss 1.4338 (1.1711)	grad_norm 1.5809 (2.3737)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:44:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:09:42 lr 0.000010	 wd 0.0500	time 0.5545 (0.5809)	loss 1.3784 (1.1713)	grad_norm 2.2463 (2.3987)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:45:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:08:43 lr 0.000010	 wd 0.0500	time 0.5622 (0.5805)	loss 1.3503 (1.1696)	grad_norm 1.7304 (2.3990)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:46:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:07:45 lr 0.000010	 wd 0.0500	time 0.5541 (0.5801)	loss 1.5564 (1.1715)	grad_norm 1.9122 (2.4515)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:47:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:06:46 lr 0.000010	 wd 0.0500	time 0.5575 (0.5797)	loss 0.9867 (1.1714)	grad_norm 2.7139 (2.4525)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:48:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:05:48 lr 0.000010	 wd 0.0500	time 0.5573 (0.5794)	loss 1.2725 (1.1706)	grad_norm 1.8131 (2.4563)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:49:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:04:50 lr 0.000010	 wd 0.0500	time 0.5611 (0.5791)	loss 1.0040 (1.1713)	grad_norm 2.4622 (2.4590)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:50:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:03:52 lr 0.000010	 wd 0.0500	time 0.5578 (0.5789)	loss 1.4904 (1.1714)	grad_norm 3.0733 (2.4803)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:51:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:54 lr 0.000010	 wd 0.0500	time 0.5590 (0.5786)	loss 1.5213 (1.1720)	grad_norm 2.3833 (2.4917)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:52:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:56 lr 0.000010	 wd 0.0500	time 0.5628 (0.5785)	loss 0.8576 (1.1714)	grad_norm 1.6108 (2.5118)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:53:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:58 lr 0.000010	 wd 0.0500	time 0.5585 (0.5783)	loss 0.9239 (1.1701)	grad_norm 2.5025 (2.5022)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:54:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.5609 (0.5781)	loss 1.5326 (1.1703)	grad_norm 1.7388 (2.4929)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:54:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 17 training takes 0:24:08
[2024-08-02 17:54:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.809 (11.809)	Loss 0.4897 (0.4897)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 17:55:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.836 Acc@5 97.782
[2024-08-02 17:55:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 17:55:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.84%
[2024-08-02 17:55:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 17:55:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 17:55:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:56:53 lr 0.000009	 wd 0.0500	time 11.4362 (11.4362)	loss 1.5458 (1.5458)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:56:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:27:11 lr 0.000009	 wd 0.0500	time 0.5534 (0.6790)	loss 1.5150 (1.1814)	grad_norm 1.6553 (2.1929)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:57:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:24:00 lr 0.000009	 wd 0.0500	time 0.5585 (0.6256)	loss 1.0077 (1.1830)	grad_norm 2.5796 (2.3397)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:58:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:22:19 lr 0.000009	 wd 0.0500	time 0.5586 (0.6081)	loss 1.6003 (1.1849)	grad_norm 1.9767 (2.3635)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 17:59:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:21:00 lr 0.000009	 wd 0.0500	time 0.5534 (0.5995)	loss 0.7455 (1.1846)	grad_norm 2.3071 (2.3498)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:00:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:19:49 lr 0.000009	 wd 0.0500	time 0.5585 (0.5943)	loss 1.4043 (1.1810)	grad_norm 1.7362 (2.3429)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:01:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:18:43 lr 0.000009	 wd 0.0500	time 0.5490 (0.5909)	loss 1.6194 (1.1750)	grad_norm 2.0292 (2.3813)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:02:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:17:40 lr 0.000009	 wd 0.0500	time 0.5589 (0.5885)	loss 1.3944 (1.1778)	grad_norm 1.6185 (2.4131)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:02:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:16:38 lr 0.000009	 wd 0.0500	time 0.5541 (0.5868)	loss 1.3553 (1.1823)	grad_norm 1.5743 (2.4699)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:03:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:15:37 lr 0.000009	 wd 0.0500	time 0.5602 (0.5854)	loss 0.9662 (1.1806)	grad_norm 1.3272 (2.4883)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:04:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:14:37 lr 0.000009	 wd 0.0500	time 0.5598 (0.5842)	loss 1.4228 (1.1781)	grad_norm 2.1777 (2.4832)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:05:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:13:37 lr 0.000009	 wd 0.0500	time 0.5583 (0.5833)	loss 1.0450 (1.1798)	grad_norm 2.3435 (2.4732)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:06:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:12:38 lr 0.000009	 wd 0.0500	time 0.5509 (0.5826)	loss 0.9813 (1.1742)	grad_norm 1.6473 (2.4819)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:07:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:11:39 lr 0.000009	 wd 0.0500	time 0.5571 (0.5819)	loss 1.5128 (1.1740)	grad_norm 2.5507 (2.4941)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:08:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:10:40 lr 0.000009	 wd 0.0500	time 0.5693 (0.5814)	loss 1.2813 (1.1747)	grad_norm 2.3792 (2.5217)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 18:09:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:09:42 lr 0.000009	 wd 0.0500	time 0.5578 (0.5810)	loss 1.3985 (1.1751)	grad_norm 4.4244 (2.4972)	loss_scale 2048.0000 (1036.2798)	mem 20413MB
[2024-08-02 18:10:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:08:43 lr 0.000009	 wd 0.0500	time 0.5594 (0.5805)	loss 1.3618 (1.1765)	grad_norm 2.2227 (2.4867)	loss_scale 2048.0000 (1099.4728)	mem 20413MB
[2024-08-02 18:11:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:07:45 lr 0.000009	 wd 0.0500	time 0.5587 (0.5802)	loss 1.3014 (1.1778)	grad_norm 1.5939 (2.4740)	loss_scale 2048.0000 (1155.2357)	mem 20413MB
[2024-08-02 18:12:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:06:47 lr 0.000009	 wd 0.0500	time 0.5591 (0.5798)	loss 0.7783 (1.1781)	grad_norm 2.4476 (2.4661)	loss_scale 2048.0000 (1204.8062)	mem 20413MB
[2024-08-02 18:13:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:05:48 lr 0.000009	 wd 0.0500	time 0.5608 (0.5795)	loss 1.5300 (1.1781)	grad_norm 1.2699 (2.4700)	loss_scale 2048.0000 (1249.1615)	mem 20413MB
[2024-08-02 18:14:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:04:50 lr 0.000008	 wd 0.0500	time 0.5531 (0.5793)	loss 1.0193 (1.1775)	grad_norm 3.4087 (2.4624)	loss_scale 2048.0000 (1289.0835)	mem 20413MB
[2024-08-02 18:15:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:03:52 lr 0.000008	 wd 0.0500	time 0.5619 (0.5790)	loss 1.3206 (1.1771)	grad_norm 1.8244 (2.4576)	loss_scale 2048.0000 (1325.2051)	mem 20413MB
[2024-08-02 18:16:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:02:54 lr 0.000008	 wd 0.0500	time 0.5584 (0.5788)	loss 1.0676 (1.1773)	grad_norm 2.0806 (2.4485)	loss_scale 2048.0000 (1358.0445)	mem 20413MB
[2024-08-02 18:17:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:56 lr 0.000008	 wd 0.0500	time 0.5573 (0.5786)	loss 1.3598 (1.1783)	grad_norm 2.2327 (2.4476)	loss_scale 2048.0000 (1388.0296)	mem 20413MB
[2024-08-02 18:18:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:59 lr 0.000008	 wd 0.0500	time 0.5561 (0.5785)	loss 1.4538 (1.1786)	grad_norm 1.9509 (2.4395)	loss_scale 2048.0000 (1415.5169)	mem 20413MB
[2024-08-02 18:19:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.5570 (0.5783)	loss 1.1943 (1.1788)	grad_norm 1.4572 (2.4698)	loss_scale 2048.0000 (1440.8061)	mem 20413MB
[2024-08-02 18:19:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 18 training takes 0:24:09
[2024-08-02 18:19:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.641 (11.641)	Loss 0.5024 (0.5024)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 18:19:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.834 Acc@5 97.760
[2024-08-02 18:19:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 18:19:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.84%
[2024-08-02 18:20:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:17:48 lr 0.000008	 wd 0.0500	time 11.9379 (11.9379)	loss 0.8887 (0.8887)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:21:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:27:22 lr 0.000008	 wd 0.0500	time 0.5530 (0.6838)	loss 1.5879 (1.2194)	grad_norm 2.8680 (2.4225)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:21:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:24:06 lr 0.000008	 wd 0.0500	time 0.5517 (0.6282)	loss 0.9102 (1.1885)	grad_norm 2.1789 (2.3553)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:22:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:22:22 lr 0.000008	 wd 0.0500	time 0.5566 (0.6098)	loss 1.4415 (1.1784)	grad_norm 2.4609 (2.3692)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:23:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:21:02 lr 0.000008	 wd 0.0500	time 0.5610 (0.6008)	loss 0.9901 (1.1692)	grad_norm 3.4497 (2.3613)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:24:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:19:51 lr 0.000008	 wd 0.0500	time 0.5658 (0.5954)	loss 1.3910 (1.1775)	grad_norm 4.2755 (2.4267)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:25:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:18:45 lr 0.000008	 wd 0.0500	time 0.5516 (0.5918)	loss 0.8969 (1.1690)	grad_norm 2.2191 (2.3846)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:26:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:17:41 lr 0.000008	 wd 0.0500	time 0.5513 (0.5893)	loss 1.1606 (1.1677)	grad_norm 1.7014 (2.3988)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:27:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:16:39 lr 0.000008	 wd 0.0500	time 0.5530 (0.5874)	loss 1.4106 (1.1694)	grad_norm 2.3614 (2.4536)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:28:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:15:38 lr 0.000008	 wd 0.0500	time 0.5570 (0.5859)	loss 1.1875 (1.1703)	grad_norm 2.7471 (2.4657)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:29:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:14:38 lr 0.000008	 wd 0.0500	time 0.5554 (0.5847)	loss 1.4063 (1.1703)	grad_norm 1.7172 (2.4458)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:30:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:13:38 lr 0.000008	 wd 0.0500	time 0.5593 (0.5838)	loss 1.1479 (1.1688)	grad_norm 1.9860 (2.4375)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:31:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:12:39 lr 0.000008	 wd 0.0500	time 0.5594 (0.5830)	loss 1.3651 (1.1673)	grad_norm 2.1396 (2.4571)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:32:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:11:39 lr 0.000008	 wd 0.0500	time 0.5567 (0.5823)	loss 0.9411 (1.1668)	grad_norm 3.8833 (2.4397)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:33:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:10:41 lr 0.000008	 wd 0.0500	time 0.5606 (0.5818)	loss 0.9325 (1.1669)	grad_norm 2.6354 (2.4430)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:34:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:09:42 lr 0.000008	 wd 0.0500	time 0.5695 (0.5813)	loss 1.5114 (1.1677)	grad_norm 2.5044 (2.4295)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:35:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:08:43 lr 0.000007	 wd 0.0500	time 0.5632 (0.5809)	loss 1.2357 (1.1694)	grad_norm 20.5592 (2.4685)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:36:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:07:45 lr 0.000007	 wd 0.0500	time 0.5541 (0.5806)	loss 0.8065 (1.1697)	grad_norm 2.2953 (2.4557)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:37:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:06:47 lr 0.000007	 wd 0.0500	time 0.5584 (0.5802)	loss 1.2906 (1.1716)	grad_norm 1.7772 (2.4538)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:38:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:05:49 lr 0.000007	 wd 0.0500	time 0.5632 (0.5799)	loss 1.1739 (1.1702)	grad_norm 3.6778 (2.4612)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:39:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:04:50 lr 0.000007	 wd 0.0500	time 0.5562 (0.5796)	loss 1.3381 (1.1712)	grad_norm 2.0443 (2.4686)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:40:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:03:52 lr 0.000007	 wd 0.0500	time 0.5613 (0.5794)	loss 1.6338 (1.1732)	grad_norm 1.5319 (2.4928)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:41:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:02:54 lr 0.000007	 wd 0.0500	time 0.5714 (0.5792)	loss 1.2542 (1.1725)	grad_norm 2.0420 (2.4813)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:42:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:56 lr 0.000007	 wd 0.0500	time 0.5574 (0.5790)	loss 1.0144 (1.1721)	grad_norm 8.9938 (2.4720)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:43:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:59 lr 0.000007	 wd 0.0500	time 0.5592 (0.5788)	loss 1.1321 (1.1713)	grad_norm 3.1525 (2.4730)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:43:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.5584 (0.5786)	loss 0.8370 (1.1712)	grad_norm 2.7973 (2.4579)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:44:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 19 training takes 0:24:10
[2024-08-02 18:44:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.502 (11.502)	Loss 0.4866 (0.4866)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 18:44:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.842 Acc@5 97.784
[2024-08-02 18:44:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 18:44:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.84%
[2024-08-02 18:44:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 18:44:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 18:44:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:55:05 lr 0.000007	 wd 0.0500	time 11.3931 (11.3931)	loss 1.3909 (1.3909)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:45:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:27:10 lr 0.000007	 wd 0.0500	time 0.5537 (0.6787)	loss 0.9774 (1.1777)	grad_norm 2.0705 (2.3923)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:46:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:24:00 lr 0.000007	 wd 0.0500	time 0.5438 (0.6256)	loss 1.3189 (1.1676)	grad_norm 3.1304 (2.3401)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:47:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:22:19 lr 0.000007	 wd 0.0500	time 0.5464 (0.6082)	loss 0.7370 (1.1779)	grad_norm 2.7837 (2.4299)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:48:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:21:00 lr 0.000007	 wd 0.0500	time 0.5583 (0.5996)	loss 1.3157 (1.1753)	grad_norm 1.6293 (2.5190)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 18:49:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:19:50 lr 0.000007	 wd 0.0500	time 0.5544 (0.5945)	loss 1.2970 (1.1743)	grad_norm 1.7407 (2.4379)	loss_scale 4096.0000 (2137.9321)	mem 20413MB
[2024-08-02 18:50:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:18:44 lr 0.000007	 wd 0.0500	time 0.5529 (0.5910)	loss 1.3743 (1.1788)	grad_norm 2.0574 (2.3930)	loss_scale 4096.0000 (2463.7338)	mem 20413MB
[2024-08-02 18:51:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:17:40 lr 0.000007	 wd 0.0500	time 0.5621 (0.5886)	loss 1.3326 (1.1770)	grad_norm 1.8225 (2.4163)	loss_scale 4096.0000 (2696.5820)	mem 20413MB
[2024-08-02 18:52:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:16:38 lr 0.000007	 wd 0.0500	time 0.5616 (0.5869)	loss 1.2016 (1.1790)	grad_norm 1.8791 (2.4068)	loss_scale 4096.0000 (2871.2909)	mem 20413MB
[2024-08-02 18:53:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:15:38 lr 0.000007	 wd 0.0500	time 0.5595 (0.5855)	loss 1.3980 (1.1777)	grad_norm 2.4139 (2.4083)	loss_scale 4096.0000 (3007.2186)	mem 20413MB
[2024-08-02 18:54:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:14:37 lr 0.000007	 wd 0.0500	time 0.5453 (0.5845)	loss 1.2581 (1.1795)	grad_norm 2.1134 (2.4022)	loss_scale 4096.0000 (3115.9880)	mem 20413MB
[2024-08-02 18:55:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:13:38 lr 0.000007	 wd 0.0500	time 0.5462 (0.5836)	loss 0.9571 (1.1774)	grad_norm 1.7234 (2.4000)	loss_scale 4096.0000 (3204.9991)	mem 20413MB
[2024-08-02 18:56:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:12:38 lr 0.000006	 wd 0.0500	time 0.5603 (0.5828)	loss 1.2580 (1.1743)	grad_norm 1.8938 (2.4111)	loss_scale 4096.0000 (3279.1873)	mem 20413MB
[2024-08-02 18:57:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:11:39 lr 0.000006	 wd 0.0500	time 0.5420 (0.5822)	loss 1.3943 (1.1741)	grad_norm 1.6064 (2.4029)	loss_scale 4096.0000 (3341.9708)	mem 20413MB
[2024-08-02 18:58:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:10:40 lr 0.000006	 wd 0.0500	time 0.5583 (0.5816)	loss 1.0849 (1.1763)	grad_norm 2.6847 (2.3819)	loss_scale 4096.0000 (3395.7916)	mem 20413MB
[2024-08-02 18:59:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:09:42 lr 0.000006	 wd 0.0500	time 0.5526 (0.5812)	loss 1.3984 (1.1778)	grad_norm 2.0741 (2.3991)	loss_scale 4096.0000 (3442.4410)	mem 20413MB
[2024-08-02 19:00:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:08:43 lr 0.000006	 wd 0.0500	time 0.5534 (0.5808)	loss 1.2496 (1.1799)	grad_norm 3.2774 (2.3891)	loss_scale 4096.0000 (3483.2630)	mem 20413MB
[2024-08-02 19:01:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:07:45 lr 0.000006	 wd 0.0500	time 0.5674 (0.5804)	loss 1.5373 (1.1806)	grad_norm 2.5742 (2.3933)	loss_scale 4096.0000 (3519.2851)	mem 20413MB
[2024-08-02 19:02:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:06:47 lr 0.000006	 wd 0.0500	time 0.5578 (0.5801)	loss 0.7992 (1.1783)	grad_norm 2.6287 (2.3762)	loss_scale 4096.0000 (3551.3071)	mem 20413MB
[2024-08-02 19:02:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:05:49 lr 0.000006	 wd 0.0500	time 0.5584 (0.5798)	loss 0.8404 (1.1790)	grad_norm 1.9010 (2.3750)	loss_scale 4096.0000 (3579.9600)	mem 20413MB
[2024-08-02 19:03:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:04:50 lr 0.000006	 wd 0.0500	time 0.5520 (0.5796)	loss 1.1145 (1.1784)	grad_norm 5.3317 (2.3964)	loss_scale 4096.0000 (3605.7491)	mem 20413MB
[2024-08-02 19:04:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:03:52 lr 0.000006	 wd 0.0500	time 0.5625 (0.5793)	loss 1.3589 (1.1787)	grad_norm 1.5080 (nan)	loss_scale 2048.0000 (3574.4960)	mem 20413MB
[2024-08-02 19:05:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:02:54 lr 0.000006	 wd 0.0500	time 0.5536 (0.5791)	loss 1.3763 (1.1799)	grad_norm 2.3587 (nan)	loss_scale 2048.0000 (3505.1413)	mem 20413MB
[2024-08-02 19:06:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:56 lr 0.000006	 wd 0.0500	time 0.5517 (0.5789)	loss 0.8333 (1.1812)	grad_norm 1.5582 (nan)	loss_scale 2048.0000 (3441.8149)	mem 20413MB
[2024-08-02 19:07:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:59 lr 0.000006	 wd 0.0500	time 0.5600 (0.5787)	loss 0.7368 (1.1808)	grad_norm 2.6168 (nan)	loss_scale 2048.0000 (3383.7634)	mem 20413MB
[2024-08-02 19:08:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.5620 (0.5785)	loss 1.1521 (1.1826)	grad_norm 2.3451 (nan)	loss_scale 2048.0000 (3330.3543)	mem 20413MB
[2024-08-02 19:08:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 20 training takes 0:24:10
[2024-08-02 19:08:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.126 (12.126)	Loss 0.4949 (0.4949)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 19:09:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.840 Acc@5 97.770
[2024-08-02 19:09:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 19:09:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.84%
[2024-08-02 19:09:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:25:29 lr 0.000006	 wd 0.0500	time 12.1222 (12.1222)	loss 0.8938 (0.8938)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:10:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:27:27 lr 0.000006	 wd 0.0500	time 0.5509 (0.6859)	loss 1.3913 (1.1840)	grad_norm 2.4013 (2.4497)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:11:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:24:07 lr 0.000006	 wd 0.0500	time 0.5608 (0.6290)	loss 1.3389 (1.1869)	grad_norm 1.8187 (2.5767)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:12:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:22:23 lr 0.000006	 wd 0.0500	time 0.5582 (0.6100)	loss 1.1400 (1.1824)	grad_norm 2.2921 (2.5088)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:13:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:21:02 lr 0.000006	 wd 0.0500	time 0.5586 (0.6007)	loss 1.4990 (1.1732)	grad_norm 2.7031 (2.5105)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:14:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:19:51 lr 0.000006	 wd 0.0500	time 0.5605 (0.5954)	loss 0.8635 (1.1682)	grad_norm 1.6827 (2.4264)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:15:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:18:45 lr 0.000006	 wd 0.0500	time 0.5555 (0.5918)	loss 1.3312 (1.1640)	grad_norm 2.6322 (2.5589)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:16:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:17:41 lr 0.000006	 wd 0.0500	time 0.5620 (0.5893)	loss 1.3097 (1.1659)	grad_norm 2.1003 (2.5670)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:17:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:16:39 lr 0.000006	 wd 0.0500	time 0.5636 (0.5874)	loss 1.3183 (1.1651)	grad_norm 1.9031 (2.5604)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:18:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:15:38 lr 0.000005	 wd 0.0500	time 0.5586 (0.5859)	loss 1.3806 (1.1691)	grad_norm 4.3727 (2.5237)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:19:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:14:38 lr 0.000005	 wd 0.0500	time 0.5526 (0.5848)	loss 1.4031 (1.1729)	grad_norm 1.3801 (2.4839)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:20:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:13:38 lr 0.000005	 wd 0.0500	time 0.5565 (0.5838)	loss 0.8397 (1.1765)	grad_norm 2.4004 (2.4625)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:21:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:12:39 lr 0.000005	 wd 0.0500	time 0.5582 (0.5830)	loss 1.3291 (1.1729)	grad_norm 2.1294 (2.4521)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:21:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:11:39 lr 0.000005	 wd 0.0500	time 0.5697 (0.5823)	loss 1.3266 (1.1731)	grad_norm 1.4718 (2.4375)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:22:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:10:41 lr 0.000005	 wd 0.0500	time 0.5609 (0.5818)	loss 0.8592 (1.1710)	grad_norm 2.3575 (2.4349)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:23:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:09:42 lr 0.000005	 wd 0.0500	time 0.5634 (0.5813)	loss 0.8011 (1.1723)	grad_norm 1.4999 (2.4405)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:24:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:08:43 lr 0.000005	 wd 0.0500	time 0.5596 (0.5808)	loss 1.1199 (1.1690)	grad_norm 3.1856 (2.4302)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:25:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:07:45 lr 0.000005	 wd 0.0500	time 0.5610 (0.5805)	loss 1.4139 (1.1690)	grad_norm 1.5183 (2.4385)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:26:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:06:47 lr 0.000005	 wd 0.0500	time 0.5542 (0.5801)	loss 0.8435 (1.1690)	grad_norm 3.6645 (2.4507)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:27:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:05:49 lr 0.000005	 wd 0.0500	time 0.5504 (0.5798)	loss 0.7441 (1.1706)	grad_norm 1.5894 (2.4549)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:28:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:04:50 lr 0.000005	 wd 0.0500	time 0.5513 (0.5796)	loss 1.1597 (1.1700)	grad_norm 1.9596 (2.4527)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:29:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:03:52 lr 0.000005	 wd 0.0500	time 0.5717 (0.5793)	loss 1.2955 (1.1697)	grad_norm 1.8511 (2.4506)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:30:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:02:54 lr 0.000005	 wd 0.0500	time 0.5585 (0.5791)	loss 1.4413 (1.1680)	grad_norm 2.5326 (2.4541)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:31:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:56 lr 0.000005	 wd 0.0500	time 0.5596 (0.5789)	loss 1.4376 (1.1703)	grad_norm 3.5496 (2.4449)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:32:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:59 lr 0.000005	 wd 0.0500	time 0.5568 (0.5787)	loss 1.1409 (1.1713)	grad_norm 1.8104 (2.4522)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:33:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.5557 (0.5785)	loss 1.4863 (1.1709)	grad_norm 2.3196 (2.4511)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:33:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 21 training takes 0:24:11
[2024-08-02 19:33:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.221 (12.221)	Loss 0.5107 (0.5107)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 19:34:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.858 Acc@5 97.750
[2024-08-02 19:34:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 19:34:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.86%
[2024-08-02 19:34:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 19:34:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 19:34:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:15:26 lr 0.000005	 wd 0.0500	time 10.4423 (10.4423)	loss 1.0326 (1.0326)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:35:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:26:52 lr 0.000005	 wd 0.0500	time 0.5528 (0.6713)	loss 1.3478 (1.1822)	grad_norm 1.7779 (2.5698)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:36:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:23:50 lr 0.000005	 wd 0.0500	time 0.5553 (0.6216)	loss 0.7210 (1.1934)	grad_norm 1.8817 (2.5548)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:37:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:22:12 lr 0.000005	 wd 0.0500	time 0.5530 (0.6053)	loss 1.5746 (1.1950)	grad_norm 1.5544 (2.6148)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:38:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:20:55 lr 0.000005	 wd 0.0500	time 0.5522 (0.5973)	loss 0.7833 (1.1951)	grad_norm 2.8899 (2.7404)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:39:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:19:46 lr 0.000005	 wd 0.0500	time 0.5600 (0.5926)	loss 1.1360 (1.1915)	grad_norm 2.1927 (2.6468)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:40:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:18:41 lr 0.000005	 wd 0.0500	time 0.5603 (0.5895)	loss 0.8673 (1.1908)	grad_norm 2.0515 (2.6051)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:41:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:17:38 lr 0.000005	 wd 0.0500	time 0.5536 (0.5872)	loss 1.4481 (1.1844)	grad_norm 1.7888 (2.5736)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:41:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:16:36 lr 0.000004	 wd 0.0500	time 0.5598 (0.5856)	loss 1.0746 (1.1846)	grad_norm 2.4012 (2.5354)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:42:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:15:36 lr 0.000004	 wd 0.0500	time 0.5691 (0.5844)	loss 1.1450 (1.1843)	grad_norm 9.1187 (2.5553)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:43:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:14:36 lr 0.000004	 wd 0.0500	time 0.5551 (0.5834)	loss 1.4118 (1.1849)	grad_norm 1.5660 (2.6182)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 19:44:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:13:36 lr 0.000004	 wd 0.0500	time 0.5608 (0.5826)	loss 0.7895 (1.1808)	grad_norm 1.8810 (2.6003)	loss_scale 4096.0000 (2159.6076)	mem 20413MB
[2024-08-02 19:45:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:12:37 lr 0.000004	 wd 0.0500	time 0.5583 (0.5819)	loss 1.2988 (1.1814)	grad_norm 2.3989 (2.5867)	loss_scale 4096.0000 (2320.8393)	mem 20413MB
[2024-08-02 19:46:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:11:38 lr 0.000004	 wd 0.0500	time 0.5596 (0.5813)	loss 0.8691 (1.1753)	grad_norm 1.5016 (2.5666)	loss_scale 4096.0000 (2457.2852)	mem 20413MB
[2024-08-02 19:47:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:10:40 lr 0.000004	 wd 0.0500	time 0.5521 (0.5808)	loss 1.0409 (1.1702)	grad_norm 2.1394 (2.5631)	loss_scale 4096.0000 (2574.2527)	mem 20413MB
[2024-08-02 19:48:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:09:41 lr 0.000004	 wd 0.0500	time 0.5597 (0.5804)	loss 1.1613 (1.1698)	grad_norm 1.5370 (2.5395)	loss_scale 4096.0000 (2675.6349)	mem 20413MB
[2024-08-02 19:49:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:08:43 lr 0.000004	 wd 0.0500	time 0.5696 (0.5800)	loss 1.3887 (1.1691)	grad_norm 1.5440 (2.5222)	loss_scale 4096.0000 (2764.3523)	mem 20413MB
[2024-08-02 19:50:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:07:44 lr 0.000004	 wd 0.0500	time 0.5561 (0.5797)	loss 0.9694 (1.1669)	grad_norm 1.9896 (nan)	loss_scale 2048.0000 (2731.8707)	mem 20413MB
[2024-08-02 19:51:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:06:46 lr 0.000004	 wd 0.0500	time 0.5605 (0.5794)	loss 1.2609 (1.1656)	grad_norm 2.0154 (nan)	loss_scale 2048.0000 (2693.8989)	mem 20413MB
[2024-08-02 19:52:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:05:48 lr 0.000004	 wd 0.0500	time 0.5485 (0.5791)	loss 0.9473 (1.1665)	grad_norm 3.3369 (nan)	loss_scale 2048.0000 (2659.9221)	mem 20413MB
[2024-08-02 19:53:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:04:50 lr 0.000004	 wd 0.0500	time 0.5578 (0.5788)	loss 0.9223 (1.1677)	grad_norm 1.8585 (nan)	loss_scale 2048.0000 (2629.3413)	mem 20413MB
[2024-08-02 19:54:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:03:52 lr 0.000004	 wd 0.0500	time 0.5561 (0.5786)	loss 1.3019 (1.1688)	grad_norm 1.9914 (nan)	loss_scale 2048.0000 (2601.6716)	mem 20413MB
[2024-08-02 19:55:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:54 lr 0.000004	 wd 0.0500	time 0.5545 (0.5784)	loss 1.1253 (1.1685)	grad_norm 9.8115 (nan)	loss_scale 2048.0000 (2576.5161)	mem 20413MB
[2024-08-02 19:56:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:56 lr 0.000004	 wd 0.0500	time 0.5593 (0.5782)	loss 1.2421 (1.1689)	grad_norm 1.2814 (nan)	loss_scale 2048.0000 (2553.5472)	mem 20413MB
[2024-08-02 19:57:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:58 lr 0.000004	 wd 0.0500	time 0.5566 (0.5781)	loss 1.4730 (1.1706)	grad_norm 2.2868 (nan)	loss_scale 2048.0000 (2532.4915)	mem 20413MB
[2024-08-02 19:58:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.5564 (0.5779)	loss 0.9443 (1.1707)	grad_norm 2.0944 (nan)	loss_scale 2048.0000 (2513.1196)	mem 20413MB
[2024-08-02 19:58:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 22 training takes 0:24:11
[2024-08-02 19:58:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.066 (12.066)	Loss 0.4944 (0.4944)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 19:58:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.830 Acc@5 97.766
[2024-08-02 19:58:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 19:58:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.86%
[2024-08-02 19:59:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:07:45 lr 0.000004	 wd 0.0500	time 11.6968 (11.6968)	loss 0.7854 (0.7854)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:00:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:27:27 lr 0.000004	 wd 0.0500	time 0.5541 (0.6858)	loss 1.3823 (1.1605)	grad_norm 2.3387 (2.0642)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:01:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:24:08 lr 0.000004	 wd 0.0500	time 0.5575 (0.6290)	loss 0.9189 (1.1686)	grad_norm 2.3817 (2.4337)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:02:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:22:23 lr 0.000004	 wd 0.0500	time 0.5536 (0.6103)	loss 1.3781 (1.1579)	grad_norm 1.9090 (2.2850)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:02:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:21:03 lr 0.000004	 wd 0.0500	time 0.5574 (0.6010)	loss 0.9422 (1.1641)	grad_norm 2.7613 (2.3375)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:03:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:19:52 lr 0.000004	 wd 0.0500	time 0.5415 (0.5955)	loss 0.9194 (1.1642)	grad_norm 1.6244 (2.3502)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:04:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:18:45 lr 0.000004	 wd 0.0500	time 0.5594 (0.5918)	loss 1.4124 (1.1631)	grad_norm 1.7471 (2.3675)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:05:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:17:42 lr 0.000004	 wd 0.0500	time 0.5608 (0.5894)	loss 1.2608 (1.1656)	grad_norm 2.5688 (2.3635)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:06:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:16:39 lr 0.000003	 wd 0.0500	time 0.5562 (0.5875)	loss 1.3968 (1.1680)	grad_norm 1.7848 (2.3534)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:07:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:15:38 lr 0.000003	 wd 0.0500	time 0.5591 (0.5860)	loss 1.1455 (1.1689)	grad_norm 1.2299 (2.3192)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:08:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:14:38 lr 0.000003	 wd 0.0500	time 0.5491 (0.5848)	loss 1.2024 (1.1650)	grad_norm 1.5342 (2.3772)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:09:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:13:38 lr 0.000003	 wd 0.0500	time 0.5572 (0.5839)	loss 1.0289 (1.1670)	grad_norm 10.9140 (2.3925)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:10:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:12:39 lr 0.000003	 wd 0.0500	time 0.5545 (0.5830)	loss 1.3560 (1.1696)	grad_norm 2.2815 (2.3806)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:11:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:11:39 lr 0.000003	 wd 0.0500	time 0.5566 (0.5823)	loss 1.2608 (1.1705)	grad_norm 4.2616 (2.3807)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:12:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:10:41 lr 0.000003	 wd 0.0500	time 0.5589 (0.5817)	loss 1.3895 (1.1707)	grad_norm 2.0150 (2.3602)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:13:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:09:42 lr 0.000003	 wd 0.0500	time 0.5520 (0.5813)	loss 1.3987 (1.1691)	grad_norm 1.7192 (2.3738)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:14:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:08:43 lr 0.000003	 wd 0.0500	time 0.5564 (0.5808)	loss 1.2257 (1.1722)	grad_norm 1.9401 (2.4327)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:15:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:07:45 lr 0.000003	 wd 0.0500	time 0.5629 (0.5805)	loss 0.7508 (1.1693)	grad_norm 2.2627 (2.4339)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:16:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:06:47 lr 0.000003	 wd 0.0500	time 0.5596 (0.5801)	loss 1.4241 (1.1687)	grad_norm 3.5866 (2.4276)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:17:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:05:49 lr 0.000003	 wd 0.0500	time 0.5626 (0.5798)	loss 1.3764 (1.1680)	grad_norm 2.2981 (2.4399)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:18:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:04:50 lr 0.000003	 wd 0.0500	time 0.5483 (0.5796)	loss 0.8324 (1.1673)	grad_norm 2.8762 (2.4426)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:19:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:03:52 lr 0.000003	 wd 0.0500	time 0.5612 (0.5793)	loss 1.2809 (1.1681)	grad_norm 2.6092 (2.4291)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:20:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:02:54 lr 0.000003	 wd 0.0500	time 0.5624 (0.5791)	loss 1.3429 (1.1670)	grad_norm 1.9246 (2.4271)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:21:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:56 lr 0.000003	 wd 0.0500	time 0.5503 (0.5788)	loss 1.4504 (1.1665)	grad_norm 2.3685 (2.4134)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:22:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:59 lr 0.000003	 wd 0.0500	time 0.5620 (0.5787)	loss 1.0046 (1.1662)	grad_norm 7.9551 (2.4225)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:23:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.5582 (0.5785)	loss 0.8803 (1.1651)	grad_norm 2.8068 (2.4168)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:23:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 23 training takes 0:24:14
[2024-08-02 20:23:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.520 (12.520)	Loss 0.5122 (0.5122)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 20:23:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.826 Acc@5 97.774
[2024-08-02 20:23:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-02 20:23:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.86%
[2024-08-02 20:24:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:13:02 lr 0.000003	 wd 0.0500	time 11.8235 (11.8235)	loss 1.2934 (1.2934)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:24:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:27:23 lr 0.000003	 wd 0.0500	time 0.5611 (0.6843)	loss 0.9856 (1.1807)	grad_norm 2.2439 (2.2277)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:25:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:24:06 lr 0.000003	 wd 0.0500	time 0.5583 (0.6283)	loss 1.1430 (1.1712)	grad_norm 1.5385 (2.2653)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:26:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:22:22 lr 0.000003	 wd 0.0500	time 0.5483 (0.6097)	loss 1.3744 (1.1838)	grad_norm 2.2477 (2.3968)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:27:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:21:02 lr 0.000003	 wd 0.0500	time 0.5509 (0.6005)	loss 0.7026 (1.1820)	grad_norm 4.8084 (2.4080)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:28:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:19:51 lr 0.000003	 wd 0.0500	time 0.5572 (0.5951)	loss 1.3373 (1.1827)	grad_norm 1.7094 (2.3873)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 20:29:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:18:44 lr 0.000003	 wd 0.0500	time 0.5551 (0.5914)	loss 0.8841 (1.1778)	grad_norm 2.2846 (inf)	loss_scale 1024.0000 (1935.5474)	mem 20413MB
[2024-08-02 20:30:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:17:41 lr 0.000003	 wd 0.0500	time 0.5629 (0.5889)	loss 1.3529 (1.1756)	grad_norm 2.0413 (inf)	loss_scale 1024.0000 (1805.5121)	mem 20413MB
[2024-08-02 20:31:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:16:39 lr 0.000003	 wd 0.0500	time 0.5411 (0.5870)	loss 1.2908 (1.1766)	grad_norm 1.5422 (inf)	loss_scale 1024.0000 (1707.9451)	mem 20413MB
[2024-08-02 20:32:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:15:38 lr 0.000003	 wd 0.0500	time 0.5597 (0.5856)	loss 0.9389 (1.1809)	grad_norm 1.8740 (inf)	loss_scale 1024.0000 (1632.0355)	mem 20413MB
[2024-08-02 20:33:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:14:37 lr 0.000003	 wd 0.0500	time 0.5572 (0.5844)	loss 0.8794 (1.1783)	grad_norm 1.7945 (inf)	loss_scale 1024.0000 (1571.2927)	mem 20413MB
[2024-08-02 20:34:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:13:38 lr 0.000003	 wd 0.0500	time 0.5622 (0.5836)	loss 0.8790 (1.1765)	grad_norm 1.6840 (inf)	loss_scale 1024.0000 (1521.5840)	mem 20413MB
[2024-08-02 20:35:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:12:38 lr 0.000002	 wd 0.0500	time 0.5615 (0.5828)	loss 0.9586 (1.1738)	grad_norm 2.1585 (inf)	loss_scale 1024.0000 (1480.1532)	mem 20413MB
[2024-08-02 20:36:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:11:39 lr 0.000002	 wd 0.0500	time 0.5597 (0.5821)	loss 0.8278 (1.1713)	grad_norm 2.3119 (inf)	loss_scale 1024.0000 (1445.0915)	mem 20413MB
[2024-08-02 20:37:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:10:40 lr 0.000002	 wd 0.0500	time 0.5568 (0.5815)	loss 1.4621 (1.1726)	grad_norm 1.9324 (inf)	loss_scale 1024.0000 (1415.0350)	mem 20413MB
[2024-08-02 20:38:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:09:42 lr 0.000002	 wd 0.0500	time 0.5743 (0.5811)	loss 1.2472 (1.1689)	grad_norm 1.9013 (inf)	loss_scale 1024.0000 (1388.9833)	mem 20413MB
[2024-08-02 20:39:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:08:43 lr 0.000002	 wd 0.0500	time 0.5576 (0.5806)	loss 1.2996 (1.1677)	grad_norm 2.6861 (inf)	loss_scale 1024.0000 (1366.1861)	mem 20413MB
[2024-08-02 20:40:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:07:45 lr 0.000002	 wd 0.0500	time 0.5624 (0.5802)	loss 1.6068 (1.1690)	grad_norm 1.8005 (inf)	loss_scale 1024.0000 (1346.0694)	mem 20413MB
[2024-08-02 20:41:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:06:47 lr 0.000002	 wd 0.0500	time 0.5575 (0.5799)	loss 1.1812 (1.1694)	grad_norm 3.3130 (inf)	loss_scale 1024.0000 (1328.1866)	mem 20413MB
[2024-08-02 20:42:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:05:48 lr 0.000002	 wd 0.0500	time 0.5408 (0.5796)	loss 0.8811 (1.1702)	grad_norm 2.0282 (inf)	loss_scale 1024.0000 (1312.1852)	mem 20413MB
[2024-08-02 20:43:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:04:50 lr 0.000002	 wd 0.0500	time 0.5601 (0.5793)	loss 1.2787 (1.1697)	grad_norm 3.6022 (inf)	loss_scale 1024.0000 (1297.7831)	mem 20413MB
[2024-08-02 20:44:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:03:52 lr 0.000002	 wd 0.0500	time 0.5532 (0.5791)	loss 1.0442 (1.1695)	grad_norm 1.5824 (inf)	loss_scale 1024.0000 (1284.7520)	mem 20413MB
[2024-08-02 20:45:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:02:54 lr 0.000002	 wd 0.0500	time 0.5650 (0.5789)	loss 0.9631 (1.1711)	grad_norm 2.1641 (inf)	loss_scale 1024.0000 (1272.9050)	mem 20413MB
[2024-08-02 20:46:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:56 lr 0.000002	 wd 0.0500	time 0.5549 (0.5786)	loss 1.3321 (1.1722)	grad_norm 2.5631 (inf)	loss_scale 1024.0000 (1262.0878)	mem 20413MB
[2024-08-02 20:46:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:59 lr 0.000002	 wd 0.0500	time 0.5550 (0.5785)	loss 1.4149 (1.1699)	grad_norm 1.5705 (inf)	loss_scale 1024.0000 (1252.1716)	mem 20413MB
[2024-08-02 20:47:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.5583 (0.5783)	loss 0.8050 (1.1702)	grad_norm 2.1987 (inf)	loss_scale 1024.0000 (1243.0484)	mem 20413MB
[2024-08-02 20:48:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 24 training takes 0:24:13
[2024-08-02 20:48:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.050 (12.050)	Loss 0.4951 (0.4951)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 20:48:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.888 Acc@5 97.792
[2024-08-02 20:48:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 20:48:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.89%
[2024-08-02 20:48:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 20:48:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 20:48:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:18:07 lr 0.000002	 wd 0.0500	time 11.9454 (11.9454)	loss 1.3634 (1.3634)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:49:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:27:22 lr 0.000002	 wd 0.0500	time 0.5545 (0.6839)	loss 1.2347 (1.2372)	grad_norm 2.7725 (2.3216)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:50:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:24:05 lr 0.000002	 wd 0.0500	time 0.5500 (0.6279)	loss 1.4366 (1.2014)	grad_norm 1.8601 (2.2988)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:51:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:22:21 lr 0.000002	 wd 0.0500	time 0.5521 (0.6093)	loss 1.1999 (1.1852)	grad_norm 1.9678 (2.3752)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:52:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:21:01 lr 0.000002	 wd 0.0500	time 0.5584 (0.6003)	loss 1.2234 (1.1828)	grad_norm 1.5496 (2.4302)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:53:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:19:50 lr 0.000002	 wd 0.0500	time 0.5605 (0.5949)	loss 1.3354 (1.1808)	grad_norm 1.6819 (2.3552)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:54:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:18:44 lr 0.000002	 wd 0.0500	time 0.5654 (0.5914)	loss 0.9234 (1.1715)	grad_norm 1.5333 (2.3818)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:55:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:17:41 lr 0.000002	 wd 0.0500	time 0.5578 (0.5889)	loss 0.8104 (1.1732)	grad_norm 2.3210 (2.3368)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:56:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:16:39 lr 0.000002	 wd 0.0500	time 0.5593 (0.5870)	loss 1.4085 (1.1709)	grad_norm 3.0700 (2.3236)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:57:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:15:38 lr 0.000002	 wd 0.0500	time 0.5596 (0.5856)	loss 1.3020 (1.1710)	grad_norm 2.1416 (2.3405)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:58:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:14:37 lr 0.000002	 wd 0.0500	time 0.5477 (0.5845)	loss 1.6189 (1.1718)	grad_norm 2.1753 (2.3380)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 20:59:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:13:38 lr 0.000002	 wd 0.0500	time 0.5580 (0.5836)	loss 1.4753 (1.1739)	grad_norm 1.9909 (2.3250)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:00:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:12:38 lr 0.000002	 wd 0.0500	time 0.5561 (0.5828)	loss 0.9709 (1.1705)	grad_norm 1.7993 (2.3454)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:01:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:11:39 lr 0.000002	 wd 0.0500	time 0.5600 (0.5821)	loss 0.9544 (1.1698)	grad_norm 2.3371 (2.3369)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:02:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:10:40 lr 0.000002	 wd 0.0500	time 0.5542 (0.5816)	loss 1.0094 (1.1697)	grad_norm 1.7653 (2.3587)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:03:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:09:42 lr 0.000002	 wd 0.0500	time 0.5527 (0.5811)	loss 0.8094 (1.1692)	grad_norm 1.9955 (2.3770)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:04:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:08:43 lr 0.000002	 wd 0.0500	time 0.5589 (0.5807)	loss 0.8473 (1.1682)	grad_norm 1.8799 (2.3751)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:05:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:07:45 lr 0.000002	 wd 0.0500	time 0.5602 (0.5803)	loss 0.7829 (1.1658)	grad_norm 1.6353 (2.3781)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:06:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:06:47 lr 0.000002	 wd 0.0500	time 0.5761 (0.5800)	loss 0.8600 (1.1675)	grad_norm 1.5279 (2.3746)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:07:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:05:48 lr 0.000002	 wd 0.0500	time 0.5570 (0.5797)	loss 1.4885 (1.1695)	grad_norm 2.1516 (2.3848)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:08:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:04:50 lr 0.000002	 wd 0.0500	time 0.5523 (0.5794)	loss 0.9557 (1.1696)	grad_norm 1.8891 (2.3881)	loss_scale 1024.0000 (1024.0000)	mem 20413MB
[2024-08-02 21:08:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:03:52 lr 0.000002	 wd 0.0500	time 0.5611 (0.5791)	loss 1.3488 (1.1705)	grad_norm 2.3155 (2.4029)	loss_scale 2048.0000 (1057.1423)	mem 20413MB
[2024-08-02 21:09:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:02:54 lr 0.000001	 wd 0.0500	time 0.5597 (0.5789)	loss 1.0782 (1.1709)	grad_norm 2.0701 (2.3991)	loss_scale 2048.0000 (1102.1608)	mem 20413MB
[2024-08-02 21:10:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:56 lr 0.000001	 wd 0.0500	time 0.5453 (0.5787)	loss 1.3433 (1.1707)	grad_norm 1.5001 (2.4547)	loss_scale 2048.0000 (1143.2664)	mem 20413MB
[2024-08-02 21:11:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:59 lr 0.000001	 wd 0.0500	time 0.5525 (0.5786)	loss 1.5581 (1.1703)	grad_norm 1.4840 (2.4522)	loss_scale 2048.0000 (1180.9479)	mem 20413MB
[2024-08-02 21:12:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5629 (0.5784)	loss 1.0302 (1.1689)	grad_norm 2.4050 (2.4619)	loss_scale 2048.0000 (1215.6162)	mem 20413MB
[2024-08-02 21:12:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 25 training takes 0:24:13
[2024-08-02 21:13:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.810 (11.810)	Loss 0.5195 (0.5195)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 21:13:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.862 Acc@5 97.758
[2024-08-02 21:13:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 21:13:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.89%
[2024-08-02 21:13:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:31:25 lr 0.000001	 wd 0.0500	time 12.2644 (12.2644)	loss 1.1097 (1.1097)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:14:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:27:29 lr 0.000001	 wd 0.0500	time 0.5528 (0.6865)	loss 1.3558 (1.1798)	grad_norm 1.6911 (2.2104)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:15:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:24:08 lr 0.000001	 wd 0.0500	time 0.5556 (0.6293)	loss 1.0874 (1.1751)	grad_norm 2.8594 (2.3224)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:16:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:22:24 lr 0.000001	 wd 0.0500	time 0.5414 (0.6105)	loss 0.9306 (1.1618)	grad_norm 1.8647 (2.3025)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:17:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:21:03 lr 0.000001	 wd 0.0500	time 0.5561 (0.6012)	loss 1.4765 (1.1615)	grad_norm 1.5311 (2.3881)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:18:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:19:52 lr 0.000001	 wd 0.0500	time 0.5536 (0.5957)	loss 1.3440 (1.1697)	grad_norm 2.1236 (2.3482)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:19:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:18:46 lr 0.000001	 wd 0.0500	time 0.5593 (0.5920)	loss 0.8057 (1.1721)	grad_norm 1.8141 (2.3668)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:20:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:17:42 lr 0.000001	 wd 0.0500	time 0.5597 (0.5894)	loss 1.2709 (1.1680)	grad_norm 1.2841 (2.3381)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:21:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:16:39 lr 0.000001	 wd 0.0500	time 0.5579 (0.5875)	loss 1.3553 (1.1724)	grad_norm 1.5095 (2.4307)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:22:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:15:38 lr 0.000001	 wd 0.0500	time 0.5648 (0.5860)	loss 1.2527 (1.1727)	grad_norm 1.9477 (2.3994)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:23:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:14:38 lr 0.000001	 wd 0.0500	time 0.5542 (0.5848)	loss 1.0195 (1.1718)	grad_norm 2.2883 (2.3749)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:24:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:13:38 lr 0.000001	 wd 0.0500	time 0.5504 (0.5838)	loss 0.8710 (1.1706)	grad_norm 1.8384 (2.3551)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:25:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:12:39 lr 0.000001	 wd 0.0500	time 0.5534 (0.5830)	loss 0.9643 (1.1713)	grad_norm 1.8949 (2.3444)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:26:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:11:39 lr 0.000001	 wd 0.0500	time 0.5547 (0.5823)	loss 1.3099 (1.1733)	grad_norm 9.5113 (2.3519)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:27:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:10:41 lr 0.000001	 wd 0.0500	time 0.5600 (0.5818)	loss 1.4502 (1.1751)	grad_norm 1.5642 (2.3325)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:28:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:09:42 lr 0.000001	 wd 0.0500	time 0.5470 (0.5813)	loss 1.3888 (1.1769)	grad_norm 1.8086 (2.3189)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:29:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:08:43 lr 0.000001	 wd 0.0500	time 0.5558 (0.5809)	loss 1.1657 (1.1755)	grad_norm 1.9514 (2.3834)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:30:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:07:45 lr 0.000001	 wd 0.0500	time 0.5548 (0.5805)	loss 1.1572 (1.1723)	grad_norm 1.5690 (2.3735)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:31:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:06:47 lr 0.000001	 wd 0.0500	time 0.5584 (0.5802)	loss 0.7994 (1.1715)	grad_norm 3.5681 (2.3753)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:31:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:05:49 lr 0.000001	 wd 0.0500	time 0.5569 (0.5799)	loss 1.3780 (1.1731)	grad_norm 1.7612 (2.3582)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:32:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:04:50 lr 0.000001	 wd 0.0500	time 0.5614 (0.5796)	loss 1.2162 (1.1719)	grad_norm 2.1120 (2.4533)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:33:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:03:52 lr 0.000001	 wd 0.0500	time 0.5607 (0.5793)	loss 1.3442 (1.1713)	grad_norm 3.1291 (2.4519)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:34:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:02:54 lr 0.000001	 wd 0.0500	time 0.5584 (0.5791)	loss 1.3039 (1.1696)	grad_norm 1.4815 (2.4387)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:35:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:56 lr 0.000001	 wd 0.0500	time 0.5516 (0.5789)	loss 1.3926 (1.1698)	grad_norm 1.4483 (2.4290)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:36:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:59 lr 0.000001	 wd 0.0500	time 0.5493 (0.5787)	loss 0.7749 (1.1703)	grad_norm 2.3051 (2.4252)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:37:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5583 (0.5785)	loss 0.8448 (1.1705)	grad_norm 2.0464 (2.4258)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:37:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 26 training takes 0:24:13
[2024-08-02 21:38:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.439 (12.439)	Loss 0.5015 (0.5015)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 21:38:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.902 Acc@5 97.774
[2024-08-02 21:38:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 21:38:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-02 21:38:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 21:38:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 21:38:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:00:06 lr 0.000001	 wd 0.0500	time 11.5133 (11.5133)	loss 1.4172 (1.4172)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:39:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:27:10 lr 0.000001	 wd 0.0500	time 0.5559 (0.6789)	loss 1.2946 (1.1769)	grad_norm 1.4599 (2.0604)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:40:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:23:59 lr 0.000001	 wd 0.0500	time 0.5549 (0.6253)	loss 1.2505 (1.1838)	grad_norm 2.0326 (2.1597)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:41:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:22:17 lr 0.000001	 wd 0.0500	time 0.5594 (0.6075)	loss 1.3224 (1.1807)	grad_norm 2.5169 (2.4054)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:42:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:20:58 lr 0.000001	 wd 0.0500	time 0.5590 (0.5989)	loss 0.9869 (1.1800)	grad_norm 2.1277 (2.5041)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:43:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:19:49 lr 0.000001	 wd 0.0500	time 0.5526 (0.5939)	loss 0.7735 (1.1809)	grad_norm 2.6072 (2.4751)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:44:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:18:43 lr 0.000001	 wd 0.0500	time 0.5550 (0.5906)	loss 1.3783 (1.1802)	grad_norm 1.5797 (2.4267)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:45:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:17:39 lr 0.000001	 wd 0.0500	time 0.5531 (0.5882)	loss 0.7926 (1.1801)	grad_norm 2.5155 (2.3980)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:46:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:16:38 lr 0.000001	 wd 0.0500	time 0.5577 (0.5864)	loss 1.1476 (1.1802)	grad_norm 1.6451 (2.4595)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:47:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:15:37 lr 0.000001	 wd 0.0500	time 0.5537 (0.5851)	loss 1.3348 (1.1790)	grad_norm 1.4515 (2.4351)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:48:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:14:37 lr 0.000001	 wd 0.0500	time 0.5597 (0.5840)	loss 0.9635 (1.1747)	grad_norm 1.5579 (2.4053)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 21:49:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:13:37 lr 0.000001	 wd 0.0500	time 0.5589 (0.5831)	loss 1.1831 (1.1762)	grad_norm 2.0110 (2.4196)	loss_scale 4096.0000 (2181.9292)	mem 20413MB
[2024-08-02 21:50:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:12:38 lr 0.000001	 wd 0.0500	time 0.5583 (0.5824)	loss 0.8721 (1.1758)	grad_norm 1.5811 (2.4335)	loss_scale 4096.0000 (2341.3022)	mem 20413MB
[2024-08-02 21:51:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:11:39 lr 0.000001	 wd 0.0500	time 0.5589 (0.5817)	loss 0.8434 (1.1714)	grad_norm 2.1801 (2.4613)	loss_scale 4096.0000 (2476.1752)	mem 20413MB
[2024-08-02 21:52:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:10:40 lr 0.000001	 wd 0.0500	time 0.5605 (0.5812)	loss 0.8986 (1.1727)	grad_norm 1.8707 (nan)	loss_scale 2048.0000 (2504.0857)	mem 20413MB
[2024-08-02 21:52:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:09:41 lr 0.000001	 wd 0.0500	time 0.5655 (0.5807)	loss 1.4764 (1.1740)	grad_norm 2.3132 (nan)	loss_scale 2048.0000 (2473.7002)	mem 20413MB
[2024-08-02 21:53:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:08:43 lr 0.000001	 wd 0.0500	time 0.5590 (0.5803)	loss 1.5355 (1.1754)	grad_norm 2.3561 (nan)	loss_scale 2048.0000 (2447.1106)	mem 20413MB
[2024-08-02 21:54:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:07:45 lr 0.000001	 wd 0.0500	time 0.5420 (0.5800)	loss 1.3667 (1.1764)	grad_norm 1.8546 (nan)	loss_scale 2048.0000 (2423.6473)	mem 20413MB
[2024-08-02 21:55:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:06:46 lr 0.000001	 wd 0.0500	time 0.5567 (0.5796)	loss 1.3788 (1.1756)	grad_norm 2.0778 (nan)	loss_scale 2048.0000 (2402.7896)	mem 20413MB
[2024-08-02 21:56:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:05:48 lr 0.000001	 wd 0.0500	time 0.5518 (0.5793)	loss 1.3944 (1.1761)	grad_norm 2.5280 (nan)	loss_scale 2048.0000 (2384.1262)	mem 20413MB
[2024-08-02 21:57:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:04:50 lr 0.000001	 wd 0.0500	time 0.5595 (0.5790)	loss 1.3966 (1.1764)	grad_norm 2.3752 (nan)	loss_scale 2048.0000 (2367.3283)	mem 20413MB
[2024-08-02 21:58:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:03:52 lr 0.000001	 wd 0.0500	time 0.5582 (0.5788)	loss 1.5590 (1.1760)	grad_norm 3.0041 (nan)	loss_scale 2048.0000 (2352.1295)	mem 20413MB
[2024-08-02 21:59:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:02:54 lr 0.000001	 wd 0.0500	time 0.5622 (0.5786)	loss 1.2580 (1.1768)	grad_norm 2.1084 (nan)	loss_scale 2048.0000 (2338.3117)	mem 20413MB
[2024-08-02 22:00:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:56 lr 0.000001	 wd 0.0500	time 0.5553 (0.5784)	loss 1.3280 (1.1768)	grad_norm 1.9694 (nan)	loss_scale 2048.0000 (2325.6949)	mem 20413MB
[2024-08-02 22:01:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:58 lr 0.000001	 wd 0.0500	time 0.5599 (0.5782)	loss 1.3905 (1.1781)	grad_norm 3.3851 (nan)	loss_scale 2048.0000 (2314.1291)	mem 20413MB
[2024-08-02 22:02:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5588 (0.5780)	loss 1.1783 (1.1772)	grad_norm 2.1946 (nan)	loss_scale 2048.0000 (2303.4882)	mem 20413MB
[2024-08-02 22:02:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 27 training takes 0:24:12
[2024-08-02 22:02:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.047 (12.047)	Loss 0.4915 (0.4915)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 22:03:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.880 Acc@5 97.782
[2024-08-02 22:03:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 22:03:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-02 22:03:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:46:51 lr 0.000001	 wd 0.0500	time 11.1958 (11.1958)	loss 0.7722 (0.7722)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:04:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:27:18 lr 0.000000	 wd 0.0500	time 0.5602 (0.6823)	loss 0.7945 (1.1981)	grad_norm 2.2270 (3.1112)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:05:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:24:03 lr 0.000000	 wd 0.0500	time 0.5582 (0.6271)	loss 1.3914 (1.1926)	grad_norm 1.6532 (2.6942)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:06:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:22:20 lr 0.000000	 wd 0.0500	time 0.5539 (0.6090)	loss 1.4392 (1.1854)	grad_norm 2.3578 (2.4701)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:07:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:21:00 lr 0.000000	 wd 0.0500	time 0.5602 (0.5999)	loss 0.7946 (1.1768)	grad_norm 1.7481 (2.4016)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:08:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:19:50 lr 0.000000	 wd 0.0500	time 0.5580 (0.5947)	loss 0.8162 (1.1821)	grad_norm 1.6211 (2.3924)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:09:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:18:44 lr 0.000000	 wd 0.0500	time 0.5554 (0.5913)	loss 0.8995 (1.1747)	grad_norm 2.7897 (2.3701)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:10:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:17:41 lr 0.000000	 wd 0.0500	time 0.5584 (0.5889)	loss 1.2283 (1.1713)	grad_norm 1.9705 (2.4875)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:11:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:16:39 lr 0.000000	 wd 0.0500	time 0.5580 (0.5870)	loss 0.8523 (1.1729)	grad_norm 2.3743 (2.4437)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:12:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:15:38 lr 0.000000	 wd 0.0500	time 0.5508 (0.5856)	loss 0.7961 (1.1694)	grad_norm 3.3057 (2.4650)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:13:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:14:37 lr 0.000000	 wd 0.0500	time 0.5581 (0.5844)	loss 1.0520 (1.1710)	grad_norm 1.7436 (2.4759)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:14:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:13:38 lr 0.000000	 wd 0.0500	time 0.5526 (0.5835)	loss 1.2071 (1.1722)	grad_norm 1.3883 (2.4566)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:14:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:12:38 lr 0.000000	 wd 0.0500	time 0.5556 (0.5827)	loss 1.4014 (1.1730)	grad_norm 1.7359 (2.4467)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:15:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:11:39 lr 0.000000	 wd 0.0500	time 0.5535 (0.5820)	loss 1.4421 (1.1750)	grad_norm 1.2505 (2.4342)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:16:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:10:40 lr 0.000000	 wd 0.0500	time 0.5564 (0.5814)	loss 0.8822 (1.1736)	grad_norm 1.6880 (2.4332)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:17:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:09:42 lr 0.000000	 wd 0.0500	time 0.5569 (0.5810)	loss 1.1906 (1.1733)	grad_norm 2.5528 (2.4129)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:18:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:08:43 lr 0.000000	 wd 0.0500	time 0.5549 (0.5805)	loss 1.4525 (1.1756)	grad_norm 1.6980 (2.4089)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:19:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:07:45 lr 0.000000	 wd 0.0500	time 0.5607 (0.5802)	loss 1.4081 (1.1755)	grad_norm 3.0461 (2.4462)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:20:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:06:47 lr 0.000000	 wd 0.0500	time 0.5571 (0.5798)	loss 0.7834 (1.1752)	grad_norm 1.8991 (2.4481)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:21:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:05:48 lr 0.000000	 wd 0.0500	time 0.5611 (0.5795)	loss 1.0314 (1.1725)	grad_norm 4.0286 (2.4346)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:22:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:04:50 lr 0.000000	 wd 0.0500	time 0.5610 (0.5793)	loss 1.1662 (1.1725)	grad_norm 2.2288 (2.4205)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:23:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:03:52 lr 0.000000	 wd 0.0500	time 0.5568 (0.5790)	loss 1.0540 (1.1731)	grad_norm 3.2209 (2.4168)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:24:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:02:54 lr 0.000000	 wd 0.0500	time 0.5600 (0.5788)	loss 1.3335 (1.1726)	grad_norm 3.0776 (2.4080)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:25:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:56 lr 0.000000	 wd 0.0500	time 0.5564 (0.5786)	loss 1.3437 (1.1725)	grad_norm 2.6860 (2.4007)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:26:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:59 lr 0.000000	 wd 0.0500	time 0.5578 (0.5785)	loss 1.5746 (1.1716)	grad_norm 1.6187 (2.3909)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:27:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.5590 (0.5783)	loss 1.0627 (1.1708)	grad_norm 4.1757 (2.3874)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:27:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 28 training takes 0:24:13
[2024-08-02 22:27:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 12.279 (12.279)	Loss 0.4990 (0.4990)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 20413MB
[2024-08-02 22:28:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.912 Acc@5 97.776
[2024-08-02 22:28:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 22:28:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.91%
[2024-08-02 22:28:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saving......
[2024-08-02 22:28:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth saved !!!
[2024-08-02 22:28:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:56:57 lr 0.000000	 wd 0.0500	time 11.4379 (11.4379)	loss 1.3796 (1.3796)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:29:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:27:09 lr 0.000000	 wd 0.0500	time 0.5568 (0.6782)	loss 1.4120 (1.1868)	grad_norm 2.1821 (2.2738)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:30:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:23:58 lr 0.000000	 wd 0.0500	time 0.5559 (0.6249)	loss 0.8163 (1.1869)	grad_norm 1.8033 (2.3107)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:31:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:22:17 lr 0.000000	 wd 0.0500	time 0.5558 (0.6075)	loss 0.7202 (1.1753)	grad_norm 1.6330 (2.2497)	loss_scale 2048.0000 (2048.0000)	mem 20413MB
[2024-08-02 22:32:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:20:58 lr 0.000000	 wd 0.0500	time 0.5548 (0.5987)	loss 1.0526 (1.1718)	grad_norm 2.2070 (2.3245)	loss_scale 4096.0000 (2374.8628)	mem 20413MB
[2024-08-02 22:33:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:19:48 lr 0.000000	 wd 0.0500	time 0.5603 (0.5937)	loss 1.2359 (1.1676)	grad_norm 1.7786 (2.4035)	loss_scale 4096.0000 (2718.4032)	mem 20413MB
[2024-08-02 22:34:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:18:43 lr 0.000000	 wd 0.0500	time 0.5608 (0.5904)	loss 0.7246 (1.1697)	grad_norm 1.8307 (2.3795)	loss_scale 4096.0000 (2947.6206)	mem 20413MB
[2024-08-02 22:35:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:17:39 lr 0.000000	 wd 0.0500	time 0.5536 (0.5882)	loss 1.1763 (1.1697)	grad_norm 4.5345 (2.3608)	loss_scale 4096.0000 (3111.4408)	mem 20413MB
[2024-08-02 22:36:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:16:38 lr 0.000000	 wd 0.0500	time 0.5571 (0.5864)	loss 0.8646 (1.1711)	grad_norm 1.8513 (2.3458)	loss_scale 4096.0000 (3234.3571)	mem 20413MB
[2024-08-02 22:36:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:15:37 lr 0.000000	 wd 0.0500	time 0.5611 (0.5850)	loss 1.4468 (1.1724)	grad_norm 1.6881 (nan)	loss_scale 2048.0000 (3120.8701)	mem 20413MB
[2024-08-02 22:37:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:14:37 lr 0.000000	 wd 0.0500	time 0.5607 (0.5839)	loss 1.3118 (1.1725)	grad_norm 2.3018 (nan)	loss_scale 2048.0000 (3013.6903)	mem 20413MB
[2024-08-02 22:38:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:13:37 lr 0.000000	 wd 0.0500	time 0.5605 (0.5831)	loss 1.3002 (1.1703)	grad_norm 2.2834 (nan)	loss_scale 2048.0000 (2925.9800)	mem 20413MB
[2024-08-02 22:39:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:12:38 lr 0.000000	 wd 0.0500	time 0.5509 (0.5823)	loss 0.9754 (1.1692)	grad_norm 1.5003 (nan)	loss_scale 2048.0000 (2852.8759)	mem 20413MB
[2024-08-02 22:40:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:11:39 lr 0.000000	 wd 0.0500	time 0.5556 (0.5817)	loss 1.2172 (1.1729)	grad_norm 4.5352 (nan)	loss_scale 2048.0000 (2791.0100)	mem 20413MB
[2024-08-02 22:41:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:10:40 lr 0.000000	 wd 0.0500	time 0.5424 (0.5811)	loss 1.1076 (1.1679)	grad_norm 3.7183 (nan)	loss_scale 2048.0000 (2737.9757)	mem 20413MB
[2024-08-02 22:42:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:09:41 lr 0.000000	 wd 0.0500	time 0.5548 (0.5807)	loss 0.9967 (1.1670)	grad_norm 1.5005 (nan)	loss_scale 2048.0000 (2692.0080)	mem 20413MB
[2024-08-02 22:43:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:08:43 lr 0.000000	 wd 0.0500	time 0.5558 (0.5802)	loss 0.8262 (1.1697)	grad_norm 1.8207 (nan)	loss_scale 2048.0000 (2651.7826)	mem 20413MB
[2024-08-02 22:44:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:07:45 lr 0.000000	 wd 0.0500	time 0.5575 (0.5799)	loss 1.2513 (1.1720)	grad_norm 2.2240 (nan)	loss_scale 2048.0000 (2616.2869)	mem 20413MB
[2024-08-02 22:45:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:06:46 lr 0.000000	 wd 0.0500	time 0.5540 (0.5795)	loss 1.3063 (1.1712)	grad_norm 1.9782 (nan)	loss_scale 2048.0000 (2584.7329)	mem 20413MB
[2024-08-02 22:46:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:05:48 lr 0.000000	 wd 0.0500	time 0.5620 (0.5793)	loss 0.8949 (1.1713)	grad_norm 4.5629 (nan)	loss_scale 2048.0000 (2556.4987)	mem 20413MB
[2024-08-02 22:47:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:04:50 lr 0.000000	 wd 0.0500	time 0.5581 (0.5791)	loss 0.9105 (1.1733)	grad_norm 1.4824 (nan)	loss_scale 2048.0000 (2531.0865)	mem 20413MB
[2024-08-02 22:48:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:03:52 lr 0.000000	 wd 0.0500	time 0.5534 (0.5789)	loss 0.8645 (1.1746)	grad_norm 4.8913 (nan)	loss_scale 2048.0000 (2508.0933)	mem 20413MB
[2024-08-02 22:49:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:02:54 lr 0.000000	 wd 0.0500	time 0.5576 (0.5786)	loss 0.8531 (1.1751)	grad_norm 1.6449 (nan)	loss_scale 2048.0000 (2487.1895)	mem 20413MB
[2024-08-02 22:50:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:56 lr 0.000000	 wd 0.0500	time 0.5557 (0.5785)	loss 0.7316 (1.1722)	grad_norm 3.5732 (nan)	loss_scale 2048.0000 (2468.1026)	mem 20413MB
[2024-08-02 22:51:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:58 lr 0.000000	 wd 0.0500	time 0.5586 (0.5783)	loss 1.2748 (1.1724)	grad_norm 3.2397 (nan)	loss_scale 2048.0000 (2450.6056)	mem 20413MB
[2024-08-02 22:52:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.5624 (0.5781)	loss 1.2670 (1.1718)	grad_norm 2.1711 (nan)	loss_scale 2048.0000 (2434.5078)	mem 20413MB
[2024-08-02 22:52:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 249): INFO EPOCH 29 training takes 0:24:13
[2024-08-02 22:52:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_29.pth saving......
[2024-08-02 22:52:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_29.pth saved !!!
[2024-08-02 22:52:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 289): INFO Test: [0/98]	Time 11.225 (11.225)	Loss 0.4888 (0.4888)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 20413MB
[2024-08-02 22:53:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 296): INFO  * Acc@1 85.876 Acc@5 97.792
[2024-08-02 22:53:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 22:53:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 182): INFO Max accuracy: 85.91%
[2024-08-02 22:53:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence1] (main.py 189): INFO Training time 12:22:50
