[2024-08-03 09:01:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/config.json
[2024-08-03 09:01:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_sequence2
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_stage2
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_sequence_stage2
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-03 09:01:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_sequence_stage2.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_sequence_stage2", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-03 09:01:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_sequence2
[2024-08-03 09:01:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-03 09:01:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 113): INFO number of params: 61613800
[2024-08-03 09:01:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2, ignoring auto resume
[2024-08-03 09:01:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth for fine-tuning......
[2024-08-03 09:01:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-03 09:01:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence1/diffusion_ft_smt_l_sequence_stage1/ckpt_epoch_best.pth'
[2024-08-03 09:02:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 13.324 (13.324)	Loss 0.4917 (0.4917)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 2657MB
[2024-08-03 09:02:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 85.892 Acc@5 97.788
[2024-08-03 09:02:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-03 09:02:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 168): INFO Start training
[2024-08-03 09:02:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:10:51 lr 0.000000	 wd 0.0500	time 13.2100 (13.2100)	loss 1.5439 (1.5439)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 23200MB
[2024-08-03 09:04:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:38:17 lr 0.000000	 wd 0.0500	time 0.8127 (0.9564)	loss 1.3176 (1.2071)	grad_norm 2.1649 (nan)	loss_scale 8192.0000 (10057.5050)	mem 23200MB
[2024-08-03 09:05:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:34:19 lr 0.000000	 wd 0.0500	time 0.7651 (0.8945)	loss 1.0567 (1.2013)	grad_norm 2.5206 (nan)	loss_scale 8192.0000 (9129.3930)	mem 23200MB
[2024-08-03 09:06:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:32:02 lr 0.000000	 wd 0.0500	time 0.7862 (0.8732)	loss 0.8817 (1.1616)	grad_norm 5.1991 (nan)	loss_scale 8192.0000 (8817.9668)	mem 23200MB
[2024-08-03 09:08:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:30:15 lr 0.000001	 wd 0.0500	time 0.7705 (0.8636)	loss 1.1014 (1.1695)	grad_norm 5.4268 (nan)	loss_scale 8192.0000 (8661.8653)	mem 23200MB
[2024-08-03 09:09:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:28:37 lr 0.000001	 wd 0.0500	time 0.7811 (0.8579)	loss 1.1896 (1.1715)	grad_norm 2.0119 (nan)	loss_scale 8192.0000 (8568.0798)	mem 23200MB
[2024-08-03 09:11:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:27:03 lr 0.000001	 wd 0.0500	time 0.7692 (0.8537)	loss 1.3051 (1.1697)	grad_norm 1.9969 (nan)	loss_scale 8192.0000 (8505.5042)	mem 23200MB
[2024-08-03 09:12:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:25:32 lr 0.000001	 wd 0.0500	time 0.7755 (0.8506)	loss 1.3018 (1.1721)	grad_norm 2.6838 (nan)	loss_scale 4096.0000 (8133.5692)	mem 23200MB
[2024-08-03 09:13:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:24:03 lr 0.000001	 wd 0.0500	time 0.7750 (0.8483)	loss 1.0981 (1.1705)	grad_norm 2.9284 (nan)	loss_scale 4096.0000 (7629.5031)	mem 23200MB
[2024-08-03 09:15:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:22:36 lr 0.000001	 wd 0.0500	time 0.7841 (0.8467)	loss 1.3416 (1.1708)	grad_norm 2.5415 (nan)	loss_scale 2048.0000 (7228.2353)	mem 23200MB
[2024-08-03 09:16:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:21:09 lr 0.000002	 wd 0.0500	time 0.7750 (0.8451)	loss 1.4442 (1.1684)	grad_norm 4.4203 (nan)	loss_scale 2048.0000 (6710.7293)	mem 23200MB
[2024-08-03 09:18:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:19:43 lr 0.000002	 wd 0.0500	time 0.7649 (0.8440)	loss 1.3510 (1.1699)	grad_norm 2.5046 (nan)	loss_scale 2048.0000 (6287.2298)	mem 23200MB
[2024-08-03 09:19:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:18:17 lr 0.000002	 wd 0.0500	time 0.8039 (0.8433)	loss 1.0149 (1.1695)	grad_norm 2.0660 (nan)	loss_scale 2048.0000 (5934.2548)	mem 23200MB
[2024-08-03 09:20:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:16:52 lr 0.000002	 wd 0.0500	time 0.7869 (0.8425)	loss 1.2148 (1.1710)	grad_norm 2.0201 (nan)	loss_scale 2048.0000 (5635.5419)	mem 23200MB
[2024-08-03 09:22:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:15:27 lr 0.000002	 wd 0.0500	time 0.7760 (0.8418)	loss 1.4481 (1.1721)	grad_norm 2.4568 (nan)	loss_scale 2048.0000 (5379.4718)	mem 23200MB
[2024-08-03 09:23:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:14:02 lr 0.000002	 wd 0.0500	time 0.7863 (0.8412)	loss 1.0294 (1.1753)	grad_norm 1.9273 (nan)	loss_scale 2048.0000 (5157.5217)	mem 23200MB
[2024-08-03 09:24:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:12:38 lr 0.000003	 wd 0.0500	time 0.7726 (0.8407)	loss 1.2419 (1.1726)	grad_norm 1.6404 (nan)	loss_scale 2048.0000 (4963.2979)	mem 23200MB
[2024-08-03 09:26:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:11:13 lr 0.000003	 wd 0.0500	time 0.7734 (0.8401)	loss 1.0037 (1.1720)	grad_norm 3.1440 (nan)	loss_scale 2048.0000 (4791.9106)	mem 23200MB
[2024-08-03 09:27:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:09:49 lr 0.000003	 wd 0.0500	time 0.7706 (0.8396)	loss 1.0915 (1.1718)	grad_norm 9.1811 (nan)	loss_scale 2048.0000 (4639.5558)	mem 23200MB
[2024-08-03 09:29:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:08:25 lr 0.000003	 wd 0.0500	time 0.8049 (0.8393)	loss 1.5115 (1.1716)	grad_norm 2.2787 (nan)	loss_scale 2048.0000 (4503.2299)	mem 23200MB
[2024-08-03 09:30:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:07:01 lr 0.000003	 wd 0.0500	time 0.7759 (0.8390)	loss 0.8453 (1.1706)	grad_norm 2.0706 (nan)	loss_scale 2048.0000 (4380.5297)	mem 23200MB
[2024-08-03 09:31:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:05:37 lr 0.000003	 wd 0.0500	time 0.8006 (0.8388)	loss 1.0256 (1.1694)	grad_norm 2.8289 (nan)	loss_scale 2048.0000 (4269.5098)	mem 23200MB
[2024-08-03 09:33:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:04:13 lr 0.000004	 wd 0.0500	time 0.7757 (0.8384)	loss 1.5361 (1.1685)	grad_norm 2.4155 (nan)	loss_scale 2048.0000 (4168.5779)	mem 23200MB
[2024-08-03 09:34:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:02:49 lr 0.000004	 wd 0.0500	time 0.7920 (0.8381)	loss 1.4016 (1.1685)	grad_norm 2.1307 (nan)	loss_scale 2048.0000 (4076.4189)	mem 23200MB
[2024-08-03 09:36:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:01:25 lr 0.000004	 wd 0.0500	time 0.7785 (0.8380)	loss 1.3496 (1.1681)	grad_norm 2.1106 (nan)	loss_scale 2048.0000 (3991.9367)	mem 23200MB
[2024-08-03 09:37:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.7922 (0.8378)	loss 1.0175 (1.1665)	grad_norm 2.4207 (nan)	loss_scale 2048.0000 (3914.2103)	mem 23200MB
[2024-08-03 09:37:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 0 training takes 0:34:58
[2024-08-03 09:37:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_0.pth saving......
[2024-08-03 09:37:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_0.pth saved !!!
[2024-08-03 09:37:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.182 (11.182)	Loss 0.5186 (0.5186)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 09:38:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.030 Acc@5 97.842
[2024-08-03 09:38:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-03 09:38:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.03%
[2024-08-03 09:38:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 09:38:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 09:38:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:58:33 lr 0.000004	 wd 0.0500	time 11.4761 (11.4761)	loss 1.3416 (1.3416)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 09:39:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:37:31 lr 0.000004	 wd 0.0500	time 0.8271 (0.9372)	loss 1.2683 (1.1519)	grad_norm 2.2924 (2.7646)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 09:41:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:33:56 lr 0.000004	 wd 0.0500	time 0.7771 (0.8848)	loss 0.8100 (1.1538)	grad_norm 2.5445 (2.7951)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 09:42:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:31:50 lr 0.000004	 wd 0.0500	time 0.7749 (0.8676)	loss 0.9087 (1.1582)	grad_norm 1.9856 (nan)	loss_scale 1024.0000 (1993.5681)	mem 23200MB
[2024-08-03 09:43:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:30:05 lr 0.000005	 wd 0.0500	time 0.7740 (0.8591)	loss 0.9244 (1.1738)	grad_norm 2.1583 (nan)	loss_scale 1024.0000 (1751.7805)	mem 23200MB
[2024-08-03 09:45:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:28:29 lr 0.000005	 wd 0.0500	time 0.7841 (0.8540)	loss 1.5341 (1.1762)	grad_norm 2.7639 (nan)	loss_scale 1024.0000 (1606.5150)	mem 23200MB
[2024-08-03 09:46:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:26:58 lr 0.000005	 wd 0.0500	time 0.7811 (0.8508)	loss 1.4466 (1.1757)	grad_norm 2.1329 (nan)	loss_scale 1024.0000 (1509.5907)	mem 23200MB
[2024-08-03 09:48:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:25:28 lr 0.000005	 wd 0.0500	time 0.7694 (0.8484)	loss 0.8574 (1.1720)	grad_norm 2.7896 (nan)	loss_scale 1024.0000 (1440.3195)	mem 23200MB
[2024-08-03 09:49:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:24:00 lr 0.000005	 wd 0.0500	time 0.7806 (0.8465)	loss 1.5726 (1.1716)	grad_norm 2.5109 (nan)	loss_scale 1024.0000 (1388.3446)	mem 23200MB
[2024-08-03 09:50:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:22:33 lr 0.000005	 wd 0.0500	time 0.7766 (0.8449)	loss 1.1595 (1.1696)	grad_norm 2.1539 (nan)	loss_scale 1024.0000 (1347.9068)	mem 23200MB
[2024-08-03 09:52:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:21:07 lr 0.000006	 wd 0.0500	time 0.7799 (0.8435)	loss 1.4679 (1.1661)	grad_norm 2.7514 (nan)	loss_scale 1024.0000 (1315.5485)	mem 23200MB
[2024-08-03 09:53:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:19:41 lr 0.000006	 wd 0.0500	time 0.7834 (0.8426)	loss 1.5159 (1.1646)	grad_norm 1.9512 (nan)	loss_scale 1024.0000 (1289.0681)	mem 23200MB
[2024-08-03 09:54:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:18:16 lr 0.000006	 wd 0.0500	time 0.7732 (0.8420)	loss 0.9213 (1.1629)	grad_norm 1.9716 (nan)	loss_scale 1024.0000 (1266.9975)	mem 23200MB
[2024-08-03 09:56:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:16:51 lr 0.000006	 wd 0.0500	time 0.7815 (0.8414)	loss 1.2392 (1.1640)	grad_norm 2.9520 (nan)	loss_scale 1024.0000 (1248.3198)	mem 23200MB
[2024-08-03 09:57:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:15:26 lr 0.000006	 wd 0.0500	time 0.7696 (0.8406)	loss 0.8119 (1.1639)	grad_norm 2.5013 (nan)	loss_scale 1024.0000 (1232.3084)	mem 23200MB
[2024-08-03 09:59:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:14:01 lr 0.000006	 wd 0.0500	time 0.7833 (0.8401)	loss 1.4684 (1.1640)	grad_norm 2.5142 (nan)	loss_scale 1024.0000 (1218.4304)	mem 23200MB
[2024-08-03 10:00:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:12:37 lr 0.000007	 wd 0.0500	time 0.7810 (0.8396)	loss 1.3529 (1.1650)	grad_norm 2.3591 (nan)	loss_scale 1024.0000 (1206.2861)	mem 23200MB
[2024-08-03 10:01:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:11:13 lr 0.000007	 wd 0.0500	time 0.8100 (0.8392)	loss 0.8368 (1.1657)	grad_norm 2.6785 (nan)	loss_scale 1024.0000 (1195.5697)	mem 23200MB
[2024-08-03 10:03:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:09:48 lr 0.000007	 wd 0.0500	time 0.7679 (0.8389)	loss 1.6499 (1.1649)	grad_norm 2.9174 (nan)	loss_scale 1024.0000 (1186.0433)	mem 23200MB
[2024-08-03 10:04:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:08:24 lr 0.000007	 wd 0.0500	time 0.7846 (0.8387)	loss 1.4755 (1.1668)	grad_norm 2.3655 (nan)	loss_scale 1024.0000 (1177.5192)	mem 23200MB
[2024-08-03 10:06:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:07:00 lr 0.000007	 wd 0.0500	time 0.7847 (0.8385)	loss 1.2628 (1.1673)	grad_norm 2.8546 (nan)	loss_scale 1024.0000 (1169.8471)	mem 23200MB
[2024-08-03 10:07:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:05:36 lr 0.000007	 wd 0.0500	time 0.7808 (0.8382)	loss 1.3903 (1.1693)	grad_norm 2.1693 (nan)	loss_scale 1024.0000 (1162.9053)	mem 23200MB
[2024-08-03 10:08:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:04:13 lr 0.000008	 wd 0.0500	time 0.7684 (0.8379)	loss 1.1373 (1.1697)	grad_norm 2.9914 (nan)	loss_scale 1024.0000 (1156.5943)	mem 23200MB
[2024-08-03 10:10:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.7655 (0.8377)	loss 0.8240 (1.1711)	grad_norm 1.9889 (nan)	loss_scale 1024.0000 (1150.8318)	mem 23200MB
[2024-08-03 10:11:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:01:25 lr 0.000008	 wd 0.0500	time 0.7814 (0.8375)	loss 1.2245 (1.1710)	grad_norm 2.9835 (nan)	loss_scale 1024.0000 (1145.5494)	mem 23200MB
[2024-08-03 10:13:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.8232 (0.8372)	loss 0.9327 (1.1709)	grad_norm 2.0567 (nan)	loss_scale 1024.0000 (1140.6893)	mem 23200MB
[2024-08-03 10:13:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 1 training takes 0:34:57
[2024-08-03 10:13:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.253 (12.253)	Loss 0.5156 (0.5156)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 10:13:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.094 Acc@5 97.882
[2024-08-03 10:13:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-03 10:13:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.09%
[2024-08-03 10:13:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 10:13:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 10:13:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:43:40 lr 0.000008	 wd 0.0500	time 11.1194 (11.1194)	loss 1.1450 (1.1450)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:15:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:37:21 lr 0.000008	 wd 0.0500	time 0.7830 (0.9332)	loss 1.4686 (1.1710)	grad_norm 2.2207 (2.8554)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:16:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:33:52 lr 0.000008	 wd 0.0500	time 0.7768 (0.8831)	loss 1.2835 (1.1615)	grad_norm 4.6309 (2.8514)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:17:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:31:46 lr 0.000008	 wd 0.0500	time 0.7856 (0.8659)	loss 1.2481 (1.1509)	grad_norm 3.6271 (2.7848)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:19:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:30:02 lr 0.000009	 wd 0.0500	time 0.7790 (0.8577)	loss 1.3752 (1.1551)	grad_norm 2.1169 (2.8439)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:20:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:28:27 lr 0.000009	 wd 0.0500	time 0.7682 (0.8530)	loss 1.3446 (1.1532)	grad_norm 2.4481 (2.9302)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:22:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:26:55 lr 0.000009	 wd 0.0500	time 0.7851 (0.8495)	loss 1.2532 (1.1522)	grad_norm 2.2484 (3.0018)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:23:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:25:26 lr 0.000009	 wd 0.0500	time 0.7806 (0.8472)	loss 1.3372 (1.1564)	grad_norm 2.3014 (2.9665)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:24:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:23:58 lr 0.000009	 wd 0.0500	time 0.7801 (0.8455)	loss 1.2301 (1.1593)	grad_norm 2.2970 (2.9127)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:26:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:22:32 lr 0.000009	 wd 0.0500	time 0.7663 (0.8444)	loss 0.8459 (1.1632)	grad_norm 6.6799 (2.9722)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:27:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:21:06 lr 0.000010	 wd 0.0500	time 0.7778 (0.8432)	loss 0.8797 (1.1607)	grad_norm 2.6817 (2.9519)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:29:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:19:40 lr 0.000010	 wd 0.0500	time 0.7752 (0.8422)	loss 1.3996 (1.1597)	grad_norm 2.5644 (2.9056)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:30:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:18:15 lr 0.000010	 wd 0.0500	time 0.8188 (0.8415)	loss 0.9949 (1.1602)	grad_norm 2.5601 (2.9410)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:31:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:16:50 lr 0.000010	 wd 0.0500	time 0.7814 (0.8410)	loss 1.4307 (1.1608)	grad_norm 2.1884 (2.9422)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:33:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:15:26 lr 0.000010	 wd 0.0500	time 0.7734 (0.8403)	loss 1.2034 (1.1621)	grad_norm 2.3392 (2.9210)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:34:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:14:01 lr 0.000010	 wd 0.0500	time 0.7777 (0.8399)	loss 1.3871 (1.1626)	grad_norm 2.2876 (2.8941)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:36:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:12:37 lr 0.000011	 wd 0.0500	time 0.7745 (0.8393)	loss 0.9548 (1.1627)	grad_norm 2.7930 (2.8975)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:37:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:11:12 lr 0.000011	 wd 0.0500	time 0.7885 (0.8388)	loss 1.2316 (1.1627)	grad_norm 2.8936 (2.8859)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 10:38:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:09:48 lr 0.000011	 wd 0.0500	time 0.8684 (0.8385)	loss 1.1450 (1.1637)	grad_norm 3.6389 (2.9487)	loss_scale 2048.0000 (1034.2343)	mem 23200MB
[2024-08-03 10:40:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:08:24 lr 0.000011	 wd 0.0500	time 0.7731 (0.8383)	loss 1.3550 (1.1634)	grad_norm 3.3373 (2.9524)	loss_scale 2048.0000 (1087.5623)	mem 23200MB
[2024-08-03 10:41:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:07:00 lr 0.000011	 wd 0.0500	time 0.7810 (0.8380)	loss 0.9871 (1.1637)	grad_norm 1.8512 (2.9550)	loss_scale 2048.0000 (1135.5602)	mem 23200MB
[2024-08-03 10:42:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:05:36 lr 0.000011	 wd 0.0500	time 0.7841 (0.8377)	loss 1.0046 (1.1629)	grad_norm 2.4161 (2.9462)	loss_scale 2048.0000 (1178.9891)	mem 23200MB
[2024-08-03 10:44:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:04:12 lr 0.000012	 wd 0.0500	time 0.7834 (0.8375)	loss 1.2864 (1.1633)	grad_norm 2.0787 (2.9475)	loss_scale 2048.0000 (1218.4716)	mem 23200MB
[2024-08-03 10:45:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:02:49 lr 0.000012	 wd 0.0500	time 0.7883 (0.8373)	loss 1.1108 (1.1631)	grad_norm 3.0169 (2.9465)	loss_scale 2048.0000 (1254.5224)	mem 23200MB
[2024-08-03 10:47:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:01:25 lr 0.000012	 wd 0.0500	time 0.7728 (0.8372)	loss 0.7931 (1.1637)	grad_norm 2.0470 (2.9416)	loss_scale 2048.0000 (1287.5702)	mem 23200MB
[2024-08-03 10:48:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.7774 (0.8370)	loss 1.2093 (1.1636)	grad_norm 2.1600 (2.9322)	loss_scale 2048.0000 (1317.9752)	mem 23200MB
[2024-08-03 10:48:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 2 training takes 0:34:56
[2024-08-03 10:48:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.233 (12.233)	Loss 0.4961 (0.4961)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 10:49:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.200 Acc@5 97.924
[2024-08-03 10:49:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 10:49:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-03 10:49:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 10:49:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 10:49:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:43:07 lr 0.000012	 wd 0.0500	time 11.1060 (11.1060)	loss 0.6842 (0.6842)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 10:50:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:37:21 lr 0.000012	 wd 0.0500	time 0.7700 (0.9330)	loss 1.2811 (1.2005)	grad_norm 2.3409 (3.4781)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 10:52:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:33:56 lr 0.000012	 wd 0.0500	time 0.7840 (0.8845)	loss 1.4857 (1.1748)	grad_norm 2.1885 (3.0323)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 10:53:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:31:51 lr 0.000012	 wd 0.0500	time 0.7808 (0.8680)	loss 1.4438 (1.1746)	grad_norm 3.5458 (2.9362)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 10:54:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:30:05 lr 0.000013	 wd 0.0500	time 0.7940 (0.8591)	loss 1.4036 (1.1656)	grad_norm 3.7409 (2.9862)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 10:56:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:28:29 lr 0.000013	 wd 0.0500	time 0.7815 (0.8541)	loss 0.8135 (1.1658)	grad_norm 3.0328 (2.9624)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 10:57:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:26:57 lr 0.000013	 wd 0.0500	time 0.7734 (0.8507)	loss 1.0748 (1.1581)	grad_norm 2.2240 (2.9772)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 10:59:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:25:27 lr 0.000013	 wd 0.0500	time 0.8153 (0.8479)	loss 1.3833 (1.1561)	grad_norm 2.9081 (2.9678)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:00:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:23:59 lr 0.000013	 wd 0.0500	time 0.7939 (0.8459)	loss 0.9844 (1.1510)	grad_norm 2.2631 (2.9866)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:01:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:22:32 lr 0.000013	 wd 0.0500	time 0.7929 (0.8443)	loss 1.5768 (1.1537)	grad_norm 3.9745 (2.9487)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:03:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:21:05 lr 0.000014	 wd 0.0500	time 0.7839 (0.8428)	loss 1.2690 (1.1537)	grad_norm 2.2053 (2.8997)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:04:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:19:40 lr 0.000014	 wd 0.0500	time 0.7959 (0.8419)	loss 0.8172 (1.1548)	grad_norm 2.2814 (2.9521)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:05:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:18:15 lr 0.000014	 wd 0.0500	time 0.7786 (0.8412)	loss 1.1791 (1.1529)	grad_norm 2.3936 (2.9258)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:07:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:16:50 lr 0.000014	 wd 0.0500	time 0.7786 (0.8406)	loss 1.2552 (1.1527)	grad_norm 2.2438 (2.9356)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:08:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:15:25 lr 0.000014	 wd 0.0500	time 0.7984 (0.8400)	loss 1.2121 (1.1534)	grad_norm 1.7062 (3.0338)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:10:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:14:01 lr 0.000014	 wd 0.0500	time 0.7679 (0.8395)	loss 1.4057 (1.1520)	grad_norm 2.2430 (2.9984)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:11:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:12:36 lr 0.000015	 wd 0.0500	time 0.7840 (0.8391)	loss 0.7252 (1.1495)	grad_norm 2.1477 (3.0237)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:12:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:11:12 lr 0.000015	 wd 0.0500	time 0.7866 (0.8387)	loss 1.0015 (1.1521)	grad_norm 2.3625 (3.0284)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:14:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:09:48 lr 0.000015	 wd 0.0500	time 0.7716 (0.8383)	loss 1.2455 (1.1534)	grad_norm 1.8255 (3.0202)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:15:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:08:24 lr 0.000015	 wd 0.0500	time 0.7688 (0.8379)	loss 1.3678 (1.1520)	grad_norm 2.3933 (3.0105)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:17:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:07:00 lr 0.000015	 wd 0.0500	time 0.7994 (0.8377)	loss 1.2375 (1.1517)	grad_norm 2.2616 (2.9913)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:18:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:05:36 lr 0.000015	 wd 0.0500	time 0.8277 (0.8375)	loss 0.8149 (1.1517)	grad_norm 2.1119 (2.9753)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:19:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:04:12 lr 0.000016	 wd 0.0500	time 0.8085 (0.8375)	loss 0.7997 (1.1509)	grad_norm 3.3170 (2.9557)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:21:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:02:49 lr 0.000016	 wd 0.0500	time 0.7619 (0.8373)	loss 0.9257 (1.1515)	grad_norm 3.5971 (2.9501)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 11:22:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:01:25 lr 0.000016	 wd 0.0500	time 0.7774 (0.8371)	loss 1.2680 (1.1527)	grad_norm 2.9564 (nan)	loss_scale 1024.0000 (2013.0279)	mem 23200MB
[2024-08-03 11:24:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8294 (0.8369)	loss 1.2379 (1.1528)	grad_norm 2.8016 (nan)	loss_scale 1024.0000 (1973.4826)	mem 23200MB
[2024-08-03 11:24:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 3 training takes 0:34:56
[2024-08-03 11:24:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.387 (12.387)	Loss 0.5073 (0.5073)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 11:24:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.196 Acc@5 97.962
[2024-08-03 11:24:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 11:24:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-03 11:24:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:55:04 lr 0.000016	 wd 0.0500	time 11.3927 (11.3927)	loss 1.2441 (1.2441)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:26:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:37:30 lr 0.000016	 wd 0.0500	time 0.7890 (0.9370)	loss 0.9296 (1.1650)	grad_norm 2.4226 (2.6145)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:27:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:33:58 lr 0.000016	 wd 0.0500	time 0.7629 (0.8854)	loss 1.0973 (1.1549)	grad_norm 15.4441 (2.9984)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:29:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:31:50 lr 0.000016	 wd 0.0500	time 0.7795 (0.8678)	loss 0.7631 (1.1520)	grad_norm 3.3455 (2.9134)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:30:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:30:07 lr 0.000017	 wd 0.0500	time 0.8468 (0.8600)	loss 1.3569 (1.1496)	grad_norm 2.6461 (2.8871)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:31:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:28:31 lr 0.000017	 wd 0.0500	time 0.7731 (0.8549)	loss 1.2072 (1.1500)	grad_norm 2.3565 (2.8944)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:33:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:27:00 lr 0.000017	 wd 0.0500	time 0.8190 (0.8521)	loss 0.9933 (1.1514)	grad_norm 2.4888 (2.8506)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:34:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:25:31 lr 0.000017	 wd 0.0500	time 0.7817 (0.8498)	loss 0.9853 (1.1521)	grad_norm 2.7137 (2.8295)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:35:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:24:02 lr 0.000017	 wd 0.0500	time 0.7694 (0.8477)	loss 0.7531 (1.1541)	grad_norm 1.9162 (2.7965)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:37:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:22:35 lr 0.000017	 wd 0.0500	time 0.7884 (0.8460)	loss 0.8616 (1.1548)	grad_norm 2.7081 (2.7900)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:38:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:21:08 lr 0.000018	 wd 0.0500	time 0.7871 (0.8447)	loss 1.4550 (1.1557)	grad_norm 5.7550 (2.7626)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:40:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:19:42 lr 0.000018	 wd 0.0500	time 0.7830 (0.8436)	loss 1.4373 (1.1570)	grad_norm 2.1025 (2.7553)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:41:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:18:17 lr 0.000018	 wd 0.0500	time 0.7900 (0.8427)	loss 1.2673 (1.1551)	grad_norm 2.2372 (2.7711)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:42:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:16:51 lr 0.000018	 wd 0.0500	time 0.7879 (0.8418)	loss 0.9563 (1.1545)	grad_norm 4.3467 (2.8660)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:44:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:15:26 lr 0.000018	 wd 0.0500	time 0.7983 (0.8411)	loss 1.3935 (1.1547)	grad_norm 2.3341 (2.8620)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:45:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:14:02 lr 0.000018	 wd 0.0500	time 0.7854 (0.8406)	loss 1.1845 (1.1548)	grad_norm 2.4440 (2.8521)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:47:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:12:38 lr 0.000019	 wd 0.0500	time 0.7718 (0.8404)	loss 1.3203 (1.1530)	grad_norm 2.6784 (2.8710)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:48:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:11:13 lr 0.000019	 wd 0.0500	time 0.7913 (0.8400)	loss 1.1342 (1.1530)	grad_norm 2.0953 (2.8687)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:49:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:09:49 lr 0.000019	 wd 0.0500	time 0.7869 (0.8395)	loss 1.5067 (1.1546)	grad_norm 8.6790 (2.8884)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:51:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:08:25 lr 0.000019	 wd 0.0500	time 0.7742 (0.8391)	loss 1.3487 (1.1547)	grad_norm 5.5133 (2.8708)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:52:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:07:01 lr 0.000019	 wd 0.0500	time 0.7848 (0.8388)	loss 0.7912 (1.1538)	grad_norm 2.8045 (2.8589)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:54:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:05:37 lr 0.000019	 wd 0.0500	time 0.7742 (0.8386)	loss 0.9218 (1.1525)	grad_norm 2.1019 (2.8779)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:55:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:04:13 lr 0.000020	 wd 0.0500	time 0.7788 (0.8383)	loss 0.9060 (1.1527)	grad_norm 2.6592 (2.8855)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:56:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.8072 (0.8380)	loss 0.8078 (1.1529)	grad_norm 2.0067 (2.8797)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:58:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:01:25 lr 0.000020	 wd 0.0500	time 0.7765 (0.8377)	loss 0.7515 (1.1522)	grad_norm 2.3837 (2.8756)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:59:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.7993 (0.8374)	loss 1.1002 (1.1527)	grad_norm 3.2327 (2.8918)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 11:59:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 4 training takes 0:34:57
[2024-08-03 11:59:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 10.886 (10.886)	Loss 0.4900 (0.4900)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 12:00:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.284 Acc@5 97.924
[2024-08-03 12:00:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.3%
[2024-08-03 12:00:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.28%
[2024-08-03 12:00:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 12:00:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 12:00:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:35:36 lr 0.000020	 wd 0.0500	time 10.9259 (10.9259)	loss 1.3709 (1.3709)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:01:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:37:22 lr 0.000020	 wd 0.0500	time 0.7799 (0.9336)	loss 0.9864 (1.1798)	grad_norm 2.2893 (2.9960)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:03:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:33:52 lr 0.000020	 wd 0.0500	time 0.7734 (0.8829)	loss 1.1964 (1.1462)	grad_norm 2.3663 (2.8058)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:04:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:31:47 lr 0.000020	 wd 0.0500	time 0.7810 (0.8663)	loss 0.8292 (1.1402)	grad_norm 2.2516 (2.7543)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:05:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:30:03 lr 0.000020	 wd 0.0500	time 0.7830 (0.8579)	loss 1.4906 (1.1478)	grad_norm 2.9856 (2.7930)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:07:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:28:28 lr 0.000020	 wd 0.0500	time 0.7821 (0.8533)	loss 1.0130 (1.1459)	grad_norm 3.1693 (2.7361)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:08:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:26:55 lr 0.000020	 wd 0.0500	time 0.7821 (0.8496)	loss 0.8977 (1.1467)	grad_norm 2.3397 (2.7350)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:10:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:25:27 lr 0.000020	 wd 0.0500	time 0.7876 (0.8475)	loss 0.9073 (1.1468)	grad_norm 2.3281 (2.7173)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:11:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:23:59 lr 0.000020	 wd 0.0500	time 0.7956 (0.8457)	loss 1.4630 (1.1444)	grad_norm 2.3749 (2.8752)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:12:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:22:32 lr 0.000020	 wd 0.0500	time 0.7886 (0.8446)	loss 1.0157 (1.1436)	grad_norm 3.9448 (2.8636)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:14:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:21:06 lr 0.000020	 wd 0.0500	time 0.7793 (0.8435)	loss 1.1399 (1.1426)	grad_norm 2.3402 (2.8480)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:15:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:19:41 lr 0.000020	 wd 0.0500	time 0.8203 (0.8426)	loss 1.1679 (1.1441)	grad_norm 6.9684 (2.8804)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:17:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:18:15 lr 0.000020	 wd 0.0500	time 0.7725 (0.8417)	loss 1.4606 (1.1432)	grad_norm 4.4298 (2.8938)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:18:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:16:51 lr 0.000020	 wd 0.0500	time 0.7460 (0.8411)	loss 0.8001 (1.1442)	grad_norm 2.3300 (2.8937)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 12:19:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:15:26 lr 0.000020	 wd 0.0500	time 0.7794 (0.8405)	loss 1.2830 (1.1478)	grad_norm 2.7151 (2.8962)	loss_scale 2048.0000 (1086.8580)	mem 23200MB
[2024-08-03 12:21:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:14:01 lr 0.000020	 wd 0.0500	time 0.7715 (0.8399)	loss 0.7636 (1.1495)	grad_norm 5.1144 (2.8943)	loss_scale 2048.0000 (1150.8914)	mem 23200MB
[2024-08-03 12:22:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:12:37 lr 0.000020	 wd 0.0500	time 0.7766 (0.8394)	loss 1.3270 (1.1527)	grad_norm 4.3861 (2.9038)	loss_scale 2048.0000 (1206.9257)	mem 23200MB
[2024-08-03 12:23:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:11:12 lr 0.000020	 wd 0.0500	time 0.7851 (0.8391)	loss 0.8161 (1.1529)	grad_norm 2.2801 (2.8846)	loss_scale 2048.0000 (1256.3715)	mem 23200MB
[2024-08-03 12:25:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:09:48 lr 0.000020	 wd 0.0500	time 0.7475 (0.8388)	loss 0.8182 (1.1505)	grad_norm 5.4985 (2.8886)	loss_scale 2048.0000 (1300.3265)	mem 23200MB
[2024-08-03 12:26:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:08:24 lr 0.000020	 wd 0.0500	time 0.7856 (0.8385)	loss 1.1355 (1.1499)	grad_norm 1.8873 (2.9005)	loss_scale 2048.0000 (1339.6570)	mem 23200MB
[2024-08-03 12:28:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:07:00 lr 0.000020	 wd 0.0500	time 0.7782 (0.8383)	loss 1.5592 (1.1509)	grad_norm 2.9667 (2.9123)	loss_scale 2048.0000 (1375.0565)	mem 23200MB
[2024-08-03 12:29:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:05:36 lr 0.000020	 wd 0.0500	time 0.7766 (0.8380)	loss 1.5327 (1.1509)	grad_norm 2.3817 (2.9264)	loss_scale 2048.0000 (1407.0861)	mem 23200MB
[2024-08-03 12:30:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:04:12 lr 0.000020	 wd 0.0500	time 0.7705 (0.8377)	loss 1.2545 (1.1513)	grad_norm 1.8152 (2.9189)	loss_scale 2048.0000 (1436.2054)	mem 23200MB
[2024-08-03 12:32:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.7767 (0.8375)	loss 1.1490 (1.1501)	grad_norm 3.5286 (2.9050)	loss_scale 2048.0000 (1462.7936)	mem 23200MB
[2024-08-03 12:33:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:01:25 lr 0.000020	 wd 0.0500	time 0.8309 (0.8373)	loss 0.9033 (1.1496)	grad_norm 2.1606 (2.9140)	loss_scale 2048.0000 (1487.1670)	mem 23200MB
[2024-08-03 12:35:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.7438 (0.8371)	loss 1.3638 (1.1505)	grad_norm 2.2300 (2.9044)	loss_scale 2048.0000 (1509.5914)	mem 23200MB
[2024-08-03 12:35:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 5 training takes 0:34:56
[2024-08-03 12:35:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.626 (12.626)	Loss 0.5029 (0.5029)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 12:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.218 Acc@5 97.962
[2024-08-03 12:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-03 12:35:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.28%
[2024-08-03 12:35:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:03:13 lr 0.000020	 wd 0.0500	time 11.5882 (11.5882)	loss 1.2001 (1.2001)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 12:37:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:37:51 lr 0.000020	 wd 0.0500	time 0.7431 (0.9458)	loss 0.9720 (1.1773)	grad_norm 2.2916 (2.8425)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 12:38:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:34:06 lr 0.000020	 wd 0.0500	time 0.7471 (0.8888)	loss 0.8979 (1.1594)	grad_norm 2.1037 (inf)	loss_scale 1024.0000 (1732.1393)	mem 23200MB
[2024-08-03 12:40:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:31:55 lr 0.000020	 wd 0.0500	time 0.7846 (0.8697)	loss 0.9298 (1.1557)	grad_norm 2.3151 (inf)	loss_scale 1024.0000 (1496.8771)	mem 23200MB
[2024-08-03 12:41:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:30:11 lr 0.000020	 wd 0.0500	time 0.7909 (0.8616)	loss 0.6895 (1.1543)	grad_norm 2.7574 (inf)	loss_scale 1024.0000 (1378.9526)	mem 23200MB
[2024-08-03 12:42:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:28:32 lr 0.000020	 wd 0.0500	time 0.7824 (0.8555)	loss 1.2605 (1.1549)	grad_norm 1.8939 (inf)	loss_scale 1024.0000 (1308.1038)	mem 23200MB
[2024-08-03 12:44:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:27:00 lr 0.000020	 wd 0.0500	time 0.8106 (0.8520)	loss 0.8848 (1.1469)	grad_norm 2.1753 (inf)	loss_scale 1024.0000 (1260.8319)	mem 23200MB
[2024-08-03 12:45:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:25:29 lr 0.000020	 wd 0.0500	time 0.7750 (0.8490)	loss 1.3770 (1.1490)	grad_norm 2.0194 (inf)	loss_scale 1024.0000 (1227.0471)	mem 23200MB
[2024-08-03 12:47:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:24:01 lr 0.000020	 wd 0.0500	time 0.7777 (0.8472)	loss 0.7635 (1.1541)	grad_norm 2.7698 (inf)	loss_scale 1024.0000 (1201.6979)	mem 23200MB
[2024-08-03 12:48:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:22:34 lr 0.000020	 wd 0.0500	time 0.7807 (0.8455)	loss 1.6018 (1.1497)	grad_norm 6.6456 (inf)	loss_scale 1024.0000 (1181.9756)	mem 23200MB
[2024-08-03 12:49:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:21:07 lr 0.000020	 wd 0.0500	time 0.7753 (0.8441)	loss 1.0125 (1.1495)	grad_norm 2.2520 (inf)	loss_scale 1024.0000 (1166.1938)	mem 23200MB
[2024-08-03 12:51:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:19:42 lr 0.000020	 wd 0.0500	time 0.7664 (0.8432)	loss 0.7922 (1.1449)	grad_norm 2.5705 (inf)	loss_scale 1024.0000 (1153.2788)	mem 23200MB
[2024-08-03 12:52:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:18:16 lr 0.000020	 wd 0.0500	time 0.7874 (0.8422)	loss 1.2841 (1.1418)	grad_norm 2.9316 (inf)	loss_scale 1024.0000 (1142.5146)	mem 23200MB
[2024-08-03 12:53:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:16:51 lr 0.000020	 wd 0.0500	time 0.7832 (0.8416)	loss 1.1213 (1.1409)	grad_norm 2.8114 (inf)	loss_scale 1024.0000 (1133.4051)	mem 23200MB
[2024-08-03 12:55:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:15:26 lr 0.000020	 wd 0.0500	time 0.7820 (0.8412)	loss 1.0138 (1.1402)	grad_norm 2.4380 (inf)	loss_scale 1024.0000 (1125.5960)	mem 23200MB
[2024-08-03 12:56:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:14:02 lr 0.000020	 wd 0.0500	time 0.8150 (0.8407)	loss 0.6857 (1.1401)	grad_norm 2.0107 (inf)	loss_scale 1024.0000 (1118.8274)	mem 23200MB
[2024-08-03 12:58:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:12:37 lr 0.000020	 wd 0.0500	time 0.8294 (0.8403)	loss 1.3311 (1.1379)	grad_norm 1.9698 (inf)	loss_scale 1024.0000 (1112.9044)	mem 23200MB
[2024-08-03 12:59:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:11:13 lr 0.000020	 wd 0.0500	time 0.7724 (0.8399)	loss 1.3499 (1.1360)	grad_norm 1.7423 (inf)	loss_scale 1024.0000 (1107.6778)	mem 23200MB
[2024-08-03 13:00:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:09:49 lr 0.000020	 wd 0.0500	time 0.7827 (0.8396)	loss 1.2054 (1.1362)	grad_norm 2.9041 (inf)	loss_scale 1024.0000 (1103.0316)	mem 23200MB
[2024-08-03 13:02:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:08:25 lr 0.000020	 wd 0.0500	time 0.7994 (0.8393)	loss 1.3986 (1.1384)	grad_norm 4.1233 (inf)	loss_scale 1024.0000 (1098.8743)	mem 23200MB
[2024-08-03 13:03:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:07:01 lr 0.000020	 wd 0.0500	time 0.8151 (0.8391)	loss 0.9090 (1.1391)	grad_norm 2.1777 (inf)	loss_scale 1024.0000 (1095.1324)	mem 23200MB
[2024-08-03 13:05:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:05:37 lr 0.000020	 wd 0.0500	time 0.7844 (0.8388)	loss 1.0959 (1.1394)	grad_norm 2.1079 (inf)	loss_scale 1024.0000 (1091.7468)	mem 23200MB
[2024-08-03 13:06:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:04:13 lr 0.000020	 wd 0.0500	time 0.7803 (0.8385)	loss 0.7979 (1.1388)	grad_norm 3.6704 (inf)	loss_scale 1024.0000 (1088.6688)	mem 23200MB
[2024-08-03 13:07:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:02:49 lr 0.000020	 wd 0.0500	time 0.8315 (0.8383)	loss 0.9444 (1.1409)	grad_norm 2.9134 (inf)	loss_scale 1024.0000 (1085.8583)	mem 23200MB
[2024-08-03 13:09:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:01:25 lr 0.000020	 wd 0.0500	time 0.8012 (0.8381)	loss 1.3964 (1.1399)	grad_norm 2.7701 (inf)	loss_scale 1024.0000 (1083.2820)	mem 23200MB
[2024-08-03 13:10:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.7677 (0.8378)	loss 1.4041 (1.1393)	grad_norm 2.1410 (inf)	loss_scale 1024.0000 (1080.9116)	mem 23200MB
[2024-08-03 13:10:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 6 training takes 0:34:58
[2024-08-03 13:10:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.248 (12.248)	Loss 0.5068 (0.5068)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 13:11:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.378 Acc@5 97.926
[2024-08-03 13:11:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-08-03 13:11:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.38%
[2024-08-03 13:11:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 13:11:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 13:11:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:55:18 lr 0.000020	 wd 0.0500	time 11.3984 (11.3984)	loss 0.9098 (0.9098)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:12:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:37:21 lr 0.000020	 wd 0.0500	time 0.7786 (0.9331)	loss 1.2198 (1.1221)	grad_norm 2.0922 (2.8680)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:14:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:33:51 lr 0.000020	 wd 0.0500	time 0.7618 (0.8826)	loss 1.0915 (1.1408)	grad_norm 2.3238 (2.7313)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:15:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:31:45 lr 0.000020	 wd 0.0500	time 0.7769 (0.8654)	loss 1.4726 (1.1408)	grad_norm 2.7782 (2.8379)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:16:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:30:01 lr 0.000020	 wd 0.0500	time 0.7968 (0.8573)	loss 0.8244 (1.1458)	grad_norm 2.3304 (2.8136)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:18:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:28:26 lr 0.000020	 wd 0.0500	time 0.7661 (0.8523)	loss 1.4009 (1.1437)	grad_norm 1.7919 (2.9902)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:19:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:26:55 lr 0.000020	 wd 0.0500	time 0.7863 (0.8493)	loss 1.2465 (1.1484)	grad_norm 3.4351 (3.0797)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:21:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:25:26 lr 0.000020	 wd 0.0500	time 0.8916 (0.8470)	loss 1.0414 (1.1428)	grad_norm 3.0724 (3.0474)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:22:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:23:58 lr 0.000020	 wd 0.0500	time 0.7992 (0.8453)	loss 0.9062 (1.1402)	grad_norm 3.7539 (3.0216)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:23:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:22:32 lr 0.000020	 wd 0.0500	time 0.7686 (0.8440)	loss 1.5030 (1.1433)	grad_norm 1.7465 (3.0333)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:25:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:21:06 lr 0.000020	 wd 0.0500	time 0.8245 (0.8429)	loss 0.8380 (1.1433)	grad_norm 2.0914 (3.0183)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:26:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:19:40 lr 0.000020	 wd 0.0500	time 0.7853 (0.8422)	loss 1.2058 (1.1414)	grad_norm 3.5495 (3.0463)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:28:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:18:15 lr 0.000020	 wd 0.0500	time 0.7858 (0.8413)	loss 1.2837 (1.1379)	grad_norm 3.1286 (3.0338)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:29:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:16:50 lr 0.000020	 wd 0.0500	time 0.7710 (0.8407)	loss 1.2839 (1.1379)	grad_norm 2.7065 (3.0579)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:30:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:15:25 lr 0.000019	 wd 0.0500	time 0.7650 (0.8400)	loss 1.1804 (1.1392)	grad_norm 3.6861 (3.0760)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:32:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:14:01 lr 0.000019	 wd 0.0500	time 0.7834 (0.8397)	loss 0.8287 (1.1402)	grad_norm 2.0921 (3.0440)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:33:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:12:36 lr 0.000019	 wd 0.0500	time 0.7796 (0.8392)	loss 1.1993 (1.1408)	grad_norm 3.2231 (3.0193)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:35:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:11:12 lr 0.000019	 wd 0.0500	time 0.7943 (0.8390)	loss 1.2398 (1.1406)	grad_norm 1.9897 (inf)	loss_scale 1024.0000 (1026.4080)	mem 23200MB
[2024-08-03 13:36:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:09:48 lr 0.000019	 wd 0.0500	time 0.7840 (0.8387)	loss 0.7376 (1.1426)	grad_norm 12.9991 (inf)	loss_scale 1024.0000 (1026.2743)	mem 23200MB
[2024-08-03 13:37:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:08:24 lr 0.000019	 wd 0.0500	time 0.8239 (0.8385)	loss 1.1555 (1.1432)	grad_norm 2.7471 (inf)	loss_scale 1024.0000 (1026.1547)	mem 23200MB
[2024-08-03 13:39:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:07:00 lr 0.000019	 wd 0.0500	time 0.7949 (0.8382)	loss 1.2411 (1.1437)	grad_norm 2.1165 (inf)	loss_scale 1024.0000 (1026.0470)	mem 23200MB
[2024-08-03 13:40:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:05:36 lr 0.000019	 wd 0.0500	time 0.7797 (0.8379)	loss 1.5173 (1.1462)	grad_norm 2.5889 (inf)	loss_scale 1024.0000 (1025.9495)	mem 23200MB
[2024-08-03 13:41:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:04:12 lr 0.000019	 wd 0.0500	time 0.7758 (0.8377)	loss 1.2352 (1.1443)	grad_norm 2.7109 (inf)	loss_scale 1024.0000 (1025.8610)	mem 23200MB
[2024-08-03 13:43:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:02:49 lr 0.000019	 wd 0.0500	time 0.7979 (0.8374)	loss 1.4288 (1.1437)	grad_norm 10.8609 (inf)	loss_scale 1024.0000 (1025.7801)	mem 23200MB
[2024-08-03 13:44:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:01:25 lr 0.000019	 wd 0.0500	time 0.7797 (0.8373)	loss 1.3048 (1.1437)	grad_norm 2.5973 (inf)	loss_scale 1024.0000 (1025.7060)	mem 23200MB
[2024-08-03 13:46:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.7866 (0.8372)	loss 1.4832 (1.1430)	grad_norm 2.3860 (inf)	loss_scale 1024.0000 (1025.6377)	mem 23200MB
[2024-08-03 13:46:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 7 training takes 0:34:57
[2024-08-03 13:46:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.706 (12.706)	Loss 0.4766 (0.4766)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 13:46:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.412 Acc@5 97.944
[2024-08-03 13:46:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.4%
[2024-08-03 13:46:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.41%
[2024-08-03 13:46:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 13:46:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 13:46:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:59:32 lr 0.000019	 wd 0.0500	time 11.4997 (11.4997)	loss 1.2386 (1.2386)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:48:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:37:40 lr 0.000019	 wd 0.0500	time 0.8696 (0.9410)	loss 0.8827 (1.1978)	grad_norm 2.1267 (2.7332)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:49:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:34:06 lr 0.000019	 wd 0.0500	time 0.7800 (0.8890)	loss 0.8342 (1.1730)	grad_norm 4.3633 (2.7056)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:51:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:31:57 lr 0.000019	 wd 0.0500	time 0.7807 (0.8707)	loss 1.4450 (1.1578)	grad_norm 2.9408 (2.7053)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:52:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:30:10 lr 0.000019	 wd 0.0500	time 0.7913 (0.8614)	loss 0.8702 (1.1551)	grad_norm 2.2727 (2.7719)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:53:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:28:33 lr 0.000019	 wd 0.0500	time 0.7673 (0.8558)	loss 0.8040 (1.1454)	grad_norm 2.4625 (2.8024)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:55:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:27:01 lr 0.000019	 wd 0.0500	time 0.7883 (0.8524)	loss 0.8137 (1.1454)	grad_norm 2.1541 (2.8278)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:56:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:25:31 lr 0.000019	 wd 0.0500	time 0.7941 (0.8498)	loss 1.2704 (1.1455)	grad_norm 2.0941 (2.8112)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:58:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:24:02 lr 0.000019	 wd 0.0500	time 0.7770 (0.8477)	loss 1.3034 (1.1476)	grad_norm 2.1177 (2.8002)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 13:59:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:22:35 lr 0.000019	 wd 0.0500	time 0.7741 (0.8459)	loss 0.9186 (1.1440)	grad_norm 6.2843 (2.8069)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:00:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:21:08 lr 0.000019	 wd 0.0500	time 0.7725 (0.8445)	loss 1.3258 (1.1460)	grad_norm 1.9868 (2.8123)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:02:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:19:43 lr 0.000019	 wd 0.0500	time 0.7846 (0.8439)	loss 1.2737 (1.1431)	grad_norm 1.8540 (2.8311)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:03:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:18:17 lr 0.000019	 wd 0.0500	time 0.7762 (0.8431)	loss 1.0535 (1.1439)	grad_norm 2.4523 (2.8401)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:05:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:16:52 lr 0.000019	 wd 0.0500	time 0.7834 (0.8422)	loss 0.8161 (1.1436)	grad_norm 2.9780 (2.9082)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:06:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:15:27 lr 0.000019	 wd 0.0500	time 0.7876 (0.8414)	loss 1.2821 (1.1447)	grad_norm 1.8819 (2.8925)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:07:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:14:02 lr 0.000019	 wd 0.0500	time 0.7463 (0.8410)	loss 1.0556 (1.1454)	grad_norm 3.9780 (2.8729)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:09:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:12:37 lr 0.000019	 wd 0.0500	time 0.7898 (0.8403)	loss 1.3725 (1.1472)	grad_norm 2.8669 (2.8500)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:10:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:11:13 lr 0.000019	 wd 0.0500	time 0.7842 (0.8399)	loss 0.7723 (1.1493)	grad_norm 3.0486 (2.8237)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:11:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:09:49 lr 0.000019	 wd 0.0500	time 0.7686 (0.8395)	loss 0.7736 (1.1477)	grad_norm 3.6134 (2.8086)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:13:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:08:25 lr 0.000019	 wd 0.0500	time 0.8135 (0.8392)	loss 1.2363 (1.1493)	grad_norm 2.2256 (2.8146)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:14:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:07:01 lr 0.000019	 wd 0.0500	time 0.7827 (0.8390)	loss 0.7448 (1.1481)	grad_norm 2.2044 (2.8120)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:16:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:05:37 lr 0.000019	 wd 0.0500	time 0.7784 (0.8390)	loss 0.9057 (1.1488)	grad_norm 2.4893 (2.8117)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:17:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:04:13 lr 0.000019	 wd 0.0500	time 0.7719 (0.8387)	loss 1.2036 (1.1475)	grad_norm 2.0784 (2.8418)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:18:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:02:49 lr 0.000019	 wd 0.0500	time 0.7801 (0.8385)	loss 1.2531 (1.1459)	grad_norm 2.1286 (2.8208)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:20:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:01:25 lr 0.000019	 wd 0.0500	time 0.7847 (0.8383)	loss 1.0545 (1.1459)	grad_norm 2.5662 (2.8235)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:21:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.7796 (0.8381)	loss 0.8618 (1.1455)	grad_norm 2.0676 (2.8300)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:21:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 8 training takes 0:34:59
[2024-08-03 14:21:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.506 (12.506)	Loss 0.4766 (0.4766)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 14:22:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.556 Acc@5 98.006
[2024-08-03 14:22:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-03 14:22:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.56%
[2024-08-03 14:22:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 14:22:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 14:22:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:36:29 lr 0.000019	 wd 0.0500	time 10.9472 (10.9472)	loss 1.3432 (1.3432)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:23:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:37:19 lr 0.000019	 wd 0.0500	time 0.7473 (0.9321)	loss 1.2313 (1.1306)	grad_norm 8.0044 (3.2123)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:25:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:33:50 lr 0.000019	 wd 0.0500	time 0.7749 (0.8823)	loss 0.9393 (1.1540)	grad_norm 6.2163 (3.0628)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:26:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:31:45 lr 0.000019	 wd 0.0500	time 0.7846 (0.8655)	loss 1.4091 (1.1582)	grad_norm 1.9265 (2.9073)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:28:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:30:02 lr 0.000019	 wd 0.0500	time 0.7701 (0.8573)	loss 0.9341 (1.1388)	grad_norm 2.4815 (2.8355)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:29:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:28:27 lr 0.000019	 wd 0.0500	time 0.7780 (0.8531)	loss 0.6728 (1.1341)	grad_norm 2.8524 (2.7715)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:30:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:26:55 lr 0.000019	 wd 0.0500	time 0.7613 (0.8495)	loss 0.7776 (1.1370)	grad_norm 2.2939 (2.7578)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 14:32:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:25:26 lr 0.000019	 wd 0.0500	time 0.7760 (0.8471)	loss 1.4142 (1.1396)	grad_norm 1.7272 (2.9607)	loss_scale 2048.0000 (1117.4893)	mem 23200MB
[2024-08-03 14:33:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:23:58 lr 0.000019	 wd 0.0500	time 0.7838 (0.8452)	loss 1.3880 (1.1412)	grad_norm 2.4920 (2.9488)	loss_scale 2048.0000 (1233.6579)	mem 23200MB
[2024-08-03 14:35:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:22:31 lr 0.000019	 wd 0.0500	time 0.7448 (0.8438)	loss 1.3358 (1.1414)	grad_norm 2.1150 (2.9409)	loss_scale 2048.0000 (1324.0400)	mem 23200MB
[2024-08-03 14:36:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:21:05 lr 0.000019	 wd 0.0500	time 0.7849 (0.8426)	loss 1.2699 (1.1418)	grad_norm 1.7367 (2.9505)	loss_scale 2048.0000 (1396.3636)	mem 23200MB
[2024-08-03 14:37:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:19:40 lr 0.000018	 wd 0.0500	time 0.7740 (0.8417)	loss 1.3777 (1.1379)	grad_norm 2.4940 (2.9353)	loss_scale 2048.0000 (1455.5495)	mem 23200MB
[2024-08-03 14:39:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:18:15 lr 0.000018	 wd 0.0500	time 0.7686 (0.8410)	loss 0.9734 (1.1382)	grad_norm 2.0265 (2.9337)	loss_scale 2048.0000 (1504.8793)	mem 23200MB
[2024-08-03 14:40:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:16:49 lr 0.000018	 wd 0.0500	time 0.7837 (0.8403)	loss 1.3173 (1.1391)	grad_norm 2.4573 (2.9497)	loss_scale 2048.0000 (1546.6257)	mem 23200MB
[2024-08-03 14:41:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:15:25 lr 0.000018	 wd 0.0500	time 0.7986 (0.8397)	loss 0.9182 (1.1372)	grad_norm 2.2314 (2.9491)	loss_scale 2048.0000 (1582.4126)	mem 23200MB
[2024-08-03 14:43:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:14:01 lr 0.000018	 wd 0.0500	time 0.7897 (0.8395)	loss 0.8739 (1.1398)	grad_norm 2.0704 (2.9421)	loss_scale 2048.0000 (1613.4310)	mem 23200MB
[2024-08-03 14:44:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:12:36 lr 0.000018	 wd 0.0500	time 0.7824 (0.8391)	loss 0.8982 (1.1378)	grad_norm 2.7378 (2.9437)	loss_scale 2048.0000 (1640.5746)	mem 23200MB
[2024-08-03 14:46:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:11:12 lr 0.000018	 wd 0.0500	time 0.7842 (0.8387)	loss 1.3872 (1.1380)	grad_norm 2.2753 (2.9445)	loss_scale 2048.0000 (1664.5267)	mem 23200MB
[2024-08-03 14:47:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:09:48 lr 0.000018	 wd 0.0500	time 0.7857 (0.8384)	loss 1.2892 (1.1372)	grad_norm 2.2236 (2.9442)	loss_scale 2048.0000 (1685.8190)	mem 23200MB
[2024-08-03 14:48:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:08:24 lr 0.000018	 wd 0.0500	time 0.7811 (0.8381)	loss 1.3739 (1.1361)	grad_norm 3.3012 (2.9499)	loss_scale 2048.0000 (1704.8711)	mem 23200MB
[2024-08-03 14:50:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:07:00 lr 0.000018	 wd 0.0500	time 0.7972 (0.8379)	loss 1.1545 (1.1366)	grad_norm 1.7933 (2.9595)	loss_scale 2048.0000 (1722.0190)	mem 23200MB
[2024-08-03 14:51:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:05:36 lr 0.000018	 wd 0.0500	time 0.8007 (0.8378)	loss 1.3335 (1.1378)	grad_norm 14.1039 (2.9681)	loss_scale 2048.0000 (1737.5345)	mem 23200MB
[2024-08-03 14:53:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:04:12 lr 0.000018	 wd 0.0500	time 0.7730 (0.8376)	loss 1.2008 (1.1387)	grad_norm 2.1541 (2.9474)	loss_scale 2048.0000 (1751.6402)	mem 23200MB
[2024-08-03 14:54:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:02:49 lr 0.000018	 wd 0.0500	time 0.7835 (0.8374)	loss 0.8184 (1.1386)	grad_norm 4.2578 (2.9350)	loss_scale 2048.0000 (1764.5198)	mem 23200MB
[2024-08-03 14:55:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:01:25 lr 0.000018	 wd 0.0500	time 0.7849 (0.8372)	loss 1.1099 (1.1391)	grad_norm 2.0240 (2.9218)	loss_scale 2048.0000 (1776.3265)	mem 23200MB
[2024-08-03 14:57:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.7819 (0.8370)	loss 1.3745 (1.1385)	grad_norm 2.6885 (2.9273)	loss_scale 2048.0000 (1787.1891)	mem 23200MB
[2024-08-03 14:57:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 9 training takes 0:34:56
[2024-08-03 14:57:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.424 (12.424)	Loss 0.4814 (0.4814)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 23200MB
[2024-08-03 14:57:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.582 Acc@5 98.014
[2024-08-03 14:57:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-03 14:57:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.58%
[2024-08-03 14:57:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 14:57:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 14:58:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:35:20 lr 0.000018	 wd 0.0500	time 10.9193 (10.9193)	loss 1.3898 (1.3898)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 14:59:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:37:16 lr 0.000018	 wd 0.0500	time 0.7835 (0.9311)	loss 0.8069 (1.1432)	grad_norm 3.2402 (2.7681)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:00:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:33:50 lr 0.000018	 wd 0.0500	time 0.7938 (0.8821)	loss 1.3577 (1.1337)	grad_norm 2.2226 (2.9445)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:02:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:31:47 lr 0.000018	 wd 0.0500	time 0.7764 (0.8662)	loss 1.3336 (1.1336)	grad_norm 3.2037 (2.8734)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:03:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:30:02 lr 0.000018	 wd 0.0500	time 0.7834 (0.8573)	loss 0.7552 (1.1413)	grad_norm 3.9649 (2.8000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:04:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:28:26 lr 0.000018	 wd 0.0500	time 0.7674 (0.8522)	loss 0.7320 (1.1391)	grad_norm 3.0954 (2.8017)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:06:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:26:55 lr 0.000018	 wd 0.0500	time 0.7946 (0.8494)	loss 1.2199 (1.1454)	grad_norm 2.5164 (2.8306)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:07:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:25:26 lr 0.000018	 wd 0.0500	time 0.7805 (0.8469)	loss 1.2886 (1.1430)	grad_norm 1.9354 (2.7818)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:09:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:23:59 lr 0.000018	 wd 0.0500	time 0.7829 (0.8456)	loss 1.2306 (1.1455)	grad_norm 2.6541 (2.7685)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:10:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:22:33 lr 0.000018	 wd 0.0500	time 0.7840 (0.8447)	loss 1.3003 (1.1439)	grad_norm 2.1975 (2.7837)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:11:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:21:07 lr 0.000018	 wd 0.0500	time 0.7882 (0.8436)	loss 1.3059 (1.1449)	grad_norm 4.0767 (2.7931)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:13:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:19:41 lr 0.000018	 wd 0.0500	time 0.7696 (0.8427)	loss 0.9985 (1.1518)	grad_norm 2.9209 (2.7939)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:14:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:18:16 lr 0.000018	 wd 0.0500	time 0.7713 (0.8420)	loss 1.4190 (1.1488)	grad_norm 3.3418 (2.8231)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:16:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:16:51 lr 0.000018	 wd 0.0500	time 0.7733 (0.8415)	loss 1.1862 (1.1468)	grad_norm 2.5609 (2.8053)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:17:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:15:26 lr 0.000018	 wd 0.0500	time 0.7735 (0.8407)	loss 1.4012 (1.1454)	grad_norm 2.3815 (2.8475)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:18:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:14:01 lr 0.000018	 wd 0.0500	time 0.7566 (0.8403)	loss 1.1564 (1.1436)	grad_norm 2.5137 (2.8302)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:20:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:12:37 lr 0.000018	 wd 0.0500	time 0.7853 (0.8399)	loss 1.1264 (1.1417)	grad_norm 2.5365 (2.8217)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:21:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:11:13 lr 0.000018	 wd 0.0500	time 0.7680 (0.8394)	loss 1.2481 (1.1435)	grad_norm 3.0204 (2.8290)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:23:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:09:49 lr 0.000018	 wd 0.0500	time 0.8172 (0.8390)	loss 0.9460 (1.1419)	grad_norm 2.2571 (2.9344)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:24:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:08:25 lr 0.000018	 wd 0.0500	time 0.7783 (0.8389)	loss 1.0557 (1.1407)	grad_norm 2.2294 (2.9392)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:25:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:07:00 lr 0.000017	 wd 0.0500	time 0.8155 (0.8386)	loss 1.2656 (1.1437)	grad_norm 4.7506 (2.9234)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:27:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:05:37 lr 0.000017	 wd 0.0500	time 0.7851 (0.8384)	loss 1.0348 (1.1453)	grad_norm 2.6959 (2.9107)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 15:28:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:04:13 lr 0.000017	 wd 0.0500	time 0.7711 (0.8382)	loss 1.2748 (1.1463)	grad_norm 2.4777 (2.9282)	loss_scale 4096.0000 (2109.4121)	mem 23200MB
[2024-08-03 15:30:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:02:49 lr 0.000017	 wd 0.0500	time 0.7866 (0.8380)	loss 1.2556 (1.1442)	grad_norm 2.8683 (2.9299)	loss_scale 4096.0000 (2195.7479)	mem 23200MB
[2024-08-03 15:31:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:01:25 lr 0.000017	 wd 0.0500	time 0.7813 (0.8378)	loss 1.4239 (1.1431)	grad_norm 2.4924 (2.9254)	loss_scale 4096.0000 (2274.8921)	mem 23200MB
[2024-08-03 15:32:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.7705 (0.8375)	loss 0.8014 (1.1421)	grad_norm 1.8543 (2.9238)	loss_scale 4096.0000 (2347.7073)	mem 23200MB
[2024-08-03 15:32:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 10 training takes 0:34:57
[2024-08-03 15:33:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.017 (12.017)	Loss 0.4836 (0.4836)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 15:33:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.594 Acc@5 98.036
[2024-08-03 15:33:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-03 15:33:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.59%
[2024-08-03 15:33:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 15:33:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 15:33:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:41:13 lr 0.000017	 wd 0.0500	time 11.0604 (11.0604)	loss 0.9505 (0.9505)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 23200MB
[2024-08-03 15:34:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:37:14 lr 0.000017	 wd 0.0500	time 0.7828 (0.9302)	loss 1.3232 (1.1289)	grad_norm 2.2261 (inf)	loss_scale 2048.0000 (2595.4851)	mem 23200MB
[2024-08-03 15:36:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:33:53 lr 0.000017	 wd 0.0500	time 0.7860 (0.8832)	loss 1.2398 (1.1394)	grad_norm 2.9194 (inf)	loss_scale 2048.0000 (2323.1045)	mem 23200MB
[2024-08-03 15:37:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:31:50 lr 0.000017	 wd 0.0500	time 0.7773 (0.8676)	loss 0.8830 (1.1323)	grad_norm 2.0631 (inf)	loss_scale 2048.0000 (2231.7076)	mem 23200MB
[2024-08-03 15:39:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:30:06 lr 0.000017	 wd 0.0500	time 0.7800 (0.8596)	loss 1.4069 (1.1346)	grad_norm 2.2707 (inf)	loss_scale 2048.0000 (2185.8953)	mem 23200MB
[2024-08-03 15:40:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:28:31 lr 0.000017	 wd 0.0500	time 0.7817 (0.8547)	loss 0.7508 (1.1332)	grad_norm 2.8968 (inf)	loss_scale 2048.0000 (2158.3713)	mem 23200MB
[2024-08-03 15:41:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:26:58 lr 0.000017	 wd 0.0500	time 0.7724 (0.8512)	loss 1.4939 (1.1352)	grad_norm 2.6683 (inf)	loss_scale 2048.0000 (2140.0067)	mem 23200MB
[2024-08-03 15:43:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:25:29 lr 0.000017	 wd 0.0500	time 0.8012 (0.8487)	loss 0.9149 (1.1323)	grad_norm 4.2064 (inf)	loss_scale 2048.0000 (2126.8816)	mem 23200MB
[2024-08-03 15:44:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:24:00 lr 0.000017	 wd 0.0500	time 0.7759 (0.8465)	loss 1.4267 (1.1309)	grad_norm 2.2218 (inf)	loss_scale 2048.0000 (2117.0337)	mem 23200MB
[2024-08-03 15:46:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:22:33 lr 0.000017	 wd 0.0500	time 0.7794 (0.8449)	loss 0.8745 (1.1332)	grad_norm 2.8495 (inf)	loss_scale 1024.0000 (2066.1842)	mem 23200MB
[2024-08-03 15:47:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:21:07 lr 0.000017	 wd 0.0500	time 0.7871 (0.8436)	loss 1.2441 (1.1342)	grad_norm 3.0897 (inf)	loss_scale 1024.0000 (1962.0699)	mem 23200MB
[2024-08-03 15:48:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:19:41 lr 0.000017	 wd 0.0500	time 0.7643 (0.8427)	loss 0.8961 (1.1323)	grad_norm 2.4075 (inf)	loss_scale 1024.0000 (1876.8683)	mem 23200MB
[2024-08-03 15:50:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:18:16 lr 0.000017	 wd 0.0500	time 0.8062 (0.8421)	loss 0.9651 (1.1285)	grad_norm 2.2242 (inf)	loss_scale 1024.0000 (1805.8551)	mem 23200MB
[2024-08-03 15:51:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:16:51 lr 0.000017	 wd 0.0500	time 0.7977 (0.8415)	loss 1.2587 (1.1291)	grad_norm 2.6068 (inf)	loss_scale 1024.0000 (1745.7586)	mem 23200MB
[2024-08-03 15:53:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:15:26 lr 0.000017	 wd 0.0500	time 0.7790 (0.8409)	loss 0.8516 (1.1301)	grad_norm 2.1572 (inf)	loss_scale 1024.0000 (1694.2413)	mem 23200MB
[2024-08-03 15:54:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:14:01 lr 0.000017	 wd 0.0500	time 0.7824 (0.8402)	loss 1.3764 (1.1298)	grad_norm 3.2775 (inf)	loss_scale 1024.0000 (1649.5883)	mem 23200MB
[2024-08-03 15:55:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:12:37 lr 0.000017	 wd 0.0500	time 0.7797 (0.8396)	loss 1.2478 (1.1311)	grad_norm 2.0932 (inf)	loss_scale 1024.0000 (1610.5134)	mem 23200MB
[2024-08-03 15:57:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:11:13 lr 0.000017	 wd 0.0500	time 0.7690 (0.8392)	loss 1.0084 (1.1318)	grad_norm 2.4162 (inf)	loss_scale 1024.0000 (1576.0329)	mem 23200MB
[2024-08-03 15:58:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:09:48 lr 0.000017	 wd 0.0500	time 0.7853 (0.8389)	loss 0.8451 (1.1308)	grad_norm 2.2783 (inf)	loss_scale 1024.0000 (1545.3815)	mem 23200MB
[2024-08-03 15:59:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:08:24 lr 0.000017	 wd 0.0500	time 0.8010 (0.8386)	loss 1.0335 (1.1293)	grad_norm 2.1304 (inf)	loss_scale 1024.0000 (1517.9548)	mem 23200MB
[2024-08-03 16:01:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:07:00 lr 0.000017	 wd 0.0500	time 0.7853 (0.8384)	loss 1.2151 (1.1297)	grad_norm 1.6846 (inf)	loss_scale 1024.0000 (1493.2694)	mem 23200MB
[2024-08-03 16:02:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:05:36 lr 0.000017	 wd 0.0500	time 0.8187 (0.8382)	loss 1.3661 (1.1293)	grad_norm 2.1964 (inf)	loss_scale 1024.0000 (1470.9338)	mem 23200MB
[2024-08-03 16:04:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:04:13 lr 0.000017	 wd 0.0500	time 0.7763 (0.8381)	loss 0.8685 (1.1307)	grad_norm 2.2910 (inf)	loss_scale 1024.0000 (1450.6279)	mem 23200MB
[2024-08-03 16:05:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:02:49 lr 0.000016	 wd 0.0500	time 0.7746 (0.8379)	loss 1.1353 (1.1292)	grad_norm 3.0044 (inf)	loss_scale 1024.0000 (1432.0869)	mem 23200MB
[2024-08-03 16:06:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:01:25 lr 0.000016	 wd 0.0500	time 0.7797 (0.8377)	loss 1.2369 (1.1303)	grad_norm 2.7234 (inf)	loss_scale 1024.0000 (1415.0904)	mem 23200MB
[2024-08-03 16:08:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.8095 (0.8374)	loss 0.8187 (1.1284)	grad_norm 2.4039 (inf)	loss_scale 1024.0000 (1399.4530)	mem 23200MB
[2024-08-03 16:08:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 11 training takes 0:34:57
[2024-08-03 16:08:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.221 (12.221)	Loss 0.4668 (0.4668)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 16:08:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.544 Acc@5 98.030
[2024-08-03 16:08:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.5%
[2024-08-03 16:08:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.59%
[2024-08-03 16:09:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][0/2502]	eta 9:05:26 lr 0.000016	 wd 0.0500	time 13.0800 (13.0800)	loss 0.9065 (0.9065)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:10:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:38:06 lr 0.000016	 wd 0.0500	time 0.7938 (0.9519)	loss 1.2468 (1.1241)	grad_norm 1.8737 (2.5078)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:11:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:34:14 lr 0.000016	 wd 0.0500	time 0.8192 (0.8925)	loss 1.1814 (1.1237)	grad_norm 2.5554 (2.6078)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:13:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:32:01 lr 0.000016	 wd 0.0500	time 0.7787 (0.8727)	loss 0.8955 (1.1332)	grad_norm 2.5439 (2.6415)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:14:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:30:13 lr 0.000016	 wd 0.0500	time 0.7641 (0.8629)	loss 1.1849 (1.1282)	grad_norm 2.0748 (2.7816)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:16:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:28:35 lr 0.000016	 wd 0.0500	time 0.8099 (0.8567)	loss 1.3719 (1.1264)	grad_norm 1.8187 (2.7793)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:17:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:27:03 lr 0.000016	 wd 0.0500	time 0.7794 (0.8537)	loss 1.3698 (1.1238)	grad_norm 3.2267 (2.7783)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:18:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:25:32 lr 0.000016	 wd 0.0500	time 0.7919 (0.8507)	loss 1.2351 (1.1216)	grad_norm 4.2834 (2.7630)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:20:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:24:03 lr 0.000016	 wd 0.0500	time 0.8144 (0.8484)	loss 1.3312 (1.1231)	grad_norm 2.6137 (2.7447)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:21:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:22:36 lr 0.000016	 wd 0.0500	time 0.7787 (0.8467)	loss 1.3454 (1.1204)	grad_norm 3.0626 (2.7315)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:23:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:21:09 lr 0.000016	 wd 0.0500	time 0.7459 (0.8452)	loss 1.2542 (1.1242)	grad_norm 1.9377 (2.7259)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:24:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:19:43 lr 0.000016	 wd 0.0500	time 0.7841 (0.8441)	loss 0.8904 (1.1261)	grad_norm 2.1736 (2.7446)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:25:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:18:17 lr 0.000016	 wd 0.0500	time 0.7809 (0.8433)	loss 1.2691 (1.1251)	grad_norm 1.9968 (2.8517)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:27:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:16:52 lr 0.000016	 wd 0.0500	time 0.7821 (0.8426)	loss 1.0353 (1.1267)	grad_norm 2.1470 (2.8804)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:28:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:15:27 lr 0.000016	 wd 0.0500	time 0.7796 (0.8417)	loss 1.3709 (1.1274)	grad_norm 3.1456 (2.8892)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:29:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:14:02 lr 0.000016	 wd 0.0500	time 0.7767 (0.8411)	loss 0.7993 (1.1241)	grad_norm 2.9941 (2.9244)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:31:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:12:38 lr 0.000016	 wd 0.0500	time 0.7720 (0.8409)	loss 1.3799 (1.1215)	grad_norm 2.3069 (2.9072)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:32:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:11:14 lr 0.000016	 wd 0.0500	time 0.7443 (0.8406)	loss 1.1929 (1.1240)	grad_norm 2.6500 (2.9562)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:34:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:09:49 lr 0.000016	 wd 0.0500	time 0.8075 (0.8400)	loss 0.8007 (1.1229)	grad_norm 2.7313 (2.9396)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:35:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:08:25 lr 0.000016	 wd 0.0500	time 0.7914 (0.8398)	loss 1.1940 (1.1234)	grad_norm 2.0493 (2.9348)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:36:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:07:01 lr 0.000016	 wd 0.0500	time 0.8312 (0.8394)	loss 0.9822 (1.1245)	grad_norm 7.8252 (2.9424)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:38:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:05:37 lr 0.000016	 wd 0.0500	time 0.7712 (0.8391)	loss 0.8082 (1.1236)	grad_norm 2.1812 (2.9337)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:39:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:04:13 lr 0.000016	 wd 0.0500	time 0.7934 (0.8389)	loss 0.7673 (1.1249)	grad_norm 3.5288 (2.9335)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:41:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:02:49 lr 0.000015	 wd 0.0500	time 0.7811 (0.8387)	loss 1.0572 (1.1252)	grad_norm 2.4791 (2.9425)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 16:42:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:01:25 lr 0.000015	 wd 0.0500	time 0.7670 (0.8385)	loss 1.3566 (1.1258)	grad_norm 2.1646 (2.9605)	loss_scale 2048.0000 (1041.0596)	mem 23200MB
[2024-08-03 16:43:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.7896 (0.8382)	loss 0.7548 (1.1251)	grad_norm 2.6063 (2.9478)	loss_scale 2048.0000 (1081.3211)	mem 23200MB
[2024-08-03 16:43:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 12 training takes 0:34:59
[2024-08-03 16:44:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.366 (11.366)	Loss 0.4888 (0.4888)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 16:44:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.606 Acc@5 98.022
[2024-08-03 16:44:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.6%
[2024-08-03 16:44:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.61%
[2024-08-03 16:44:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 16:44:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 16:44:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:32:44 lr 0.000015	 wd 0.0500	time 10.8571 (10.8571)	loss 1.2907 (1.2907)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:46:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:37:25 lr 0.000015	 wd 0.0500	time 0.7885 (0.9347)	loss 1.3625 (1.1843)	grad_norm 1.5779 (3.0096)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:47:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:33:54 lr 0.000015	 wd 0.0500	time 0.7737 (0.8838)	loss 1.3582 (1.1681)	grad_norm 3.9263 (2.9608)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:48:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:31:48 lr 0.000015	 wd 0.0500	time 0.8098 (0.8668)	loss 1.3656 (1.1672)	grad_norm 3.4763 (2.8539)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:50:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:30:03 lr 0.000015	 wd 0.0500	time 0.7824 (0.8579)	loss 1.4355 (1.1513)	grad_norm 3.3409 (2.7637)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:51:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:28:26 lr 0.000015	 wd 0.0500	time 0.7820 (0.8525)	loss 1.3644 (1.1446)	grad_norm 4.7780 (2.9157)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:53:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:26:54 lr 0.000015	 wd 0.0500	time 0.7787 (0.8491)	loss 1.2147 (1.1383)	grad_norm 2.2030 (2.9074)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:54:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:25:26 lr 0.000015	 wd 0.0500	time 0.7955 (0.8470)	loss 1.0567 (1.1318)	grad_norm 3.8330 (2.8687)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:55:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:23:58 lr 0.000015	 wd 0.0500	time 0.7850 (0.8452)	loss 1.3967 (1.1317)	grad_norm 2.5750 (2.8725)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:57:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:22:32 lr 0.000015	 wd 0.0500	time 0.7832 (0.8442)	loss 1.1388 (1.1313)	grad_norm 3.4062 (2.8830)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:58:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:21:06 lr 0.000015	 wd 0.0500	time 0.7780 (0.8432)	loss 1.3170 (1.1306)	grad_norm 2.1448 (2.8379)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 16:59:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:19:41 lr 0.000015	 wd 0.0500	time 0.7800 (0.8426)	loss 1.2985 (1.1273)	grad_norm 2.2070 (2.8273)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:01:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:18:15 lr 0.000015	 wd 0.0500	time 0.7814 (0.8418)	loss 1.2294 (1.1312)	grad_norm 2.1073 (2.8290)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:02:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:16:50 lr 0.000015	 wd 0.0500	time 0.7888 (0.8411)	loss 0.8486 (1.1309)	grad_norm 3.2084 (2.8365)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:04:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:15:26 lr 0.000015	 wd 0.0500	time 0.7828 (0.8404)	loss 1.3752 (1.1297)	grad_norm 2.5972 (2.8373)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:05:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:14:01 lr 0.000015	 wd 0.0500	time 0.7602 (0.8399)	loss 1.1634 (1.1299)	grad_norm 3.0526 (2.8424)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:06:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:12:37 lr 0.000015	 wd 0.0500	time 0.7741 (0.8394)	loss 0.9815 (1.1296)	grad_norm 2.3742 (2.8431)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:08:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:11:12 lr 0.000015	 wd 0.0500	time 0.7805 (0.8391)	loss 1.0687 (1.1303)	grad_norm 2.4645 (2.8288)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:09:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:09:48 lr 0.000015	 wd 0.0500	time 0.7679 (0.8387)	loss 1.3708 (1.1305)	grad_norm 2.1361 (2.8232)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:11:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:08:24 lr 0.000015	 wd 0.0500	time 0.7807 (0.8385)	loss 1.0850 (1.1317)	grad_norm 4.5938 (2.8286)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:12:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:07:00 lr 0.000015	 wd 0.0500	time 0.7757 (0.8383)	loss 1.2974 (1.1315)	grad_norm 2.4873 (2.8466)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:13:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:05:36 lr 0.000014	 wd 0.0500	time 0.7743 (0.8381)	loss 1.3954 (1.1319)	grad_norm 2.5380 (2.8370)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:15:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:04:13 lr 0.000014	 wd 0.0500	time 0.7963 (0.8379)	loss 1.1655 (1.1303)	grad_norm 6.3298 (2.8742)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:16:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:02:49 lr 0.000014	 wd 0.0500	time 0.7961 (0.8376)	loss 1.3439 (1.1300)	grad_norm 2.3089 (2.9100)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:18:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:01:25 lr 0.000014	 wd 0.0500	time 0.7720 (0.8374)	loss 0.8551 (1.1328)	grad_norm 1.9053 (2.9059)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:19:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.8038 (0.8372)	loss 0.8102 (1.1311)	grad_norm 2.1998 (2.9092)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:19:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 13 training takes 0:34:57
[2024-08-03 17:19:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.482 (12.482)	Loss 0.4648 (0.4648)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 17:20:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.698 Acc@5 98.024
[2024-08-03 17:20:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-03 17:20:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.70%
[2024-08-03 17:20:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 17:20:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 17:20:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:46:47 lr 0.000014	 wd 0.0500	time 11.1939 (11.1939)	loss 1.2970 (1.2970)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:21:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:37:29 lr 0.000014	 wd 0.0500	time 0.7837 (0.9364)	loss 1.0392 (1.1342)	grad_norm 2.2781 (2.8110)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:23:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:34:01 lr 0.000014	 wd 0.0500	time 0.7858 (0.8869)	loss 1.3515 (1.1380)	grad_norm 2.4203 (2.9328)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:24:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:31:53 lr 0.000014	 wd 0.0500	time 0.7738 (0.8691)	loss 0.9293 (1.1406)	grad_norm 2.2877 (2.8919)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:25:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:30:10 lr 0.000014	 wd 0.0500	time 0.8241 (0.8613)	loss 1.5417 (1.1424)	grad_norm 2.1419 (2.8245)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:27:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:28:33 lr 0.000014	 wd 0.0500	time 0.7876 (0.8557)	loss 0.9381 (1.1383)	grad_norm 2.5718 (2.8848)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:28:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:27:00 lr 0.000014	 wd 0.0500	time 0.7757 (0.8518)	loss 0.9547 (1.1357)	grad_norm 2.2807 (2.8116)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:29:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:25:30 lr 0.000014	 wd 0.0500	time 0.7767 (0.8491)	loss 1.1869 (1.1374)	grad_norm 3.8535 (2.8392)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:31:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:24:01 lr 0.000014	 wd 0.0500	time 0.7811 (0.8471)	loss 0.7640 (1.1325)	grad_norm 2.6009 (2.8803)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:32:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:22:34 lr 0.000014	 wd 0.0500	time 0.7853 (0.8456)	loss 0.7924 (1.1309)	grad_norm 2.5836 (2.9205)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:34:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:21:08 lr 0.000014	 wd 0.0500	time 0.7899 (0.8446)	loss 0.9550 (1.1290)	grad_norm 2.4307 (2.9208)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:35:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:19:42 lr 0.000014	 wd 0.0500	time 0.7716 (0.8435)	loss 1.3365 (1.1297)	grad_norm 2.2297 (2.9701)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:36:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:18:17 lr 0.000014	 wd 0.0500	time 0.8083 (0.8427)	loss 1.4785 (1.1294)	grad_norm 2.6542 (2.9686)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:38:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:16:51 lr 0.000014	 wd 0.0500	time 0.7671 (0.8419)	loss 1.1635 (1.1327)	grad_norm 3.4339 (2.9654)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:39:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:15:27 lr 0.000014	 wd 0.0500	time 0.7805 (0.8417)	loss 1.2720 (1.1331)	grad_norm 5.3181 (2.9701)	loss_scale 4096.0000 (2112.3198)	mem 23200MB
[2024-08-03 17:41:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:14:02 lr 0.000014	 wd 0.0500	time 0.8083 (0.8412)	loss 1.4166 (1.1317)	grad_norm 2.7004 (2.9486)	loss_scale 4096.0000 (2244.4770)	mem 23200MB
[2024-08-03 17:42:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:12:38 lr 0.000014	 wd 0.0500	time 0.7794 (0.8407)	loss 0.8865 (1.1309)	grad_norm 2.3322 (3.0127)	loss_scale 4096.0000 (2360.1249)	mem 23200MB
[2024-08-03 17:43:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:11:13 lr 0.000014	 wd 0.0500	time 0.7837 (0.8402)	loss 0.8455 (1.1302)	grad_norm 2.0723 (3.0052)	loss_scale 4096.0000 (2462.1752)	mem 23200MB
[2024-08-03 17:45:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:09:49 lr 0.000013	 wd 0.0500	time 0.8282 (0.8398)	loss 1.0173 (1.1313)	grad_norm 2.9329 (2.9922)	loss_scale 4096.0000 (2552.8928)	mem 23200MB
[2024-08-03 17:46:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:08:25 lr 0.000013	 wd 0.0500	time 0.7823 (0.8395)	loss 1.2133 (1.1321)	grad_norm 2.6804 (3.0184)	loss_scale 4096.0000 (2634.0663)	mem 23200MB
[2024-08-03 17:48:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:07:01 lr 0.000013	 wd 0.0500	time 0.7861 (0.8392)	loss 1.3865 (1.1306)	grad_norm 1.9296 (3.0083)	loss_scale 4096.0000 (2707.1264)	mem 23200MB
[2024-08-03 17:49:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:05:37 lr 0.000013	 wd 0.0500	time 0.7441 (0.8388)	loss 1.2400 (1.1298)	grad_norm 2.3272 (3.0175)	loss_scale 4096.0000 (2773.2318)	mem 23200MB
[2024-08-03 17:50:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:04:13 lr 0.000013	 wd 0.0500	time 0.7908 (0.8386)	loss 1.2075 (1.1291)	grad_norm 2.8218 (3.0104)	loss_scale 4096.0000 (2833.3303)	mem 23200MB
[2024-08-03 17:52:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:02:49 lr 0.000013	 wd 0.0500	time 0.7855 (0.8383)	loss 0.8154 (1.1284)	grad_norm 2.0519 (3.0125)	loss_scale 4096.0000 (2888.2051)	mem 23200MB
[2024-08-03 17:53:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:01:25 lr 0.000013	 wd 0.0500	time 0.7837 (0.8382)	loss 0.7859 (1.1280)	grad_norm 2.0791 (2.9967)	loss_scale 4096.0000 (2938.5090)	mem 23200MB
[2024-08-03 17:54:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.7848 (0.8380)	loss 1.2748 (1.1272)	grad_norm 1.9919 (inf)	loss_scale 2048.0000 (2907.8161)	mem 23200MB
[2024-08-03 17:55:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 14 training takes 0:34:59
[2024-08-03 17:55:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.436 (12.436)	Loss 0.4548 (0.4548)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 17:55:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.698 Acc@5 98.068
[2024-08-03 17:55:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-03 17:55:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.70%
[2024-08-03 17:55:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 17:55:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 17:55:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:36:05 lr 0.000013	 wd 0.0500	time 10.9373 (10.9373)	loss 1.2353 (1.2353)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:57:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:37:15 lr 0.000013	 wd 0.0500	time 0.7790 (0.9305)	loss 0.7558 (1.1254)	grad_norm 2.5981 (2.8627)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:58:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:33:45 lr 0.000013	 wd 0.0500	time 0.7775 (0.8801)	loss 1.5037 (1.1145)	grad_norm 2.3794 (2.8576)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 17:59:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:31:41 lr 0.000013	 wd 0.0500	time 0.7726 (0.8636)	loss 0.8710 (1.1129)	grad_norm 2.0500 (2.7889)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:01:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:29:59 lr 0.000013	 wd 0.0500	time 0.8082 (0.8560)	loss 0.7565 (1.1108)	grad_norm 3.7392 (2.7830)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:02:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:28:24 lr 0.000013	 wd 0.0500	time 0.7736 (0.8515)	loss 0.7247 (1.1204)	grad_norm 2.1072 (2.8051)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:04:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:26:53 lr 0.000013	 wd 0.0500	time 0.7907 (0.8484)	loss 1.2449 (1.1254)	grad_norm 2.6249 (2.8452)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:05:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:25:24 lr 0.000013	 wd 0.0500	time 0.8214 (0.8462)	loss 1.3052 (1.1278)	grad_norm 1.9930 (2.8252)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:06:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:23:59 lr 0.000013	 wd 0.0500	time 0.7624 (0.8456)	loss 1.1504 (1.1286)	grad_norm 2.0695 (2.7721)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:08:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:22:32 lr 0.000013	 wd 0.0500	time 0.7716 (0.8441)	loss 1.3261 (1.1297)	grad_norm 2.6328 (2.7997)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:09:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:21:06 lr 0.000013	 wd 0.0500	time 0.7779 (0.8433)	loss 0.7651 (1.1294)	grad_norm 1.9300 (2.8947)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:11:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:19:41 lr 0.000013	 wd 0.0500	time 0.7958 (0.8425)	loss 0.8497 (1.1302)	grad_norm 2.3684 (2.8731)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:12:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:18:15 lr 0.000013	 wd 0.0500	time 0.8119 (0.8415)	loss 0.8532 (1.1306)	grad_norm 2.4998 (2.8520)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:13:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:16:50 lr 0.000013	 wd 0.0500	time 0.7684 (0.8409)	loss 0.9110 (1.1319)	grad_norm 2.1894 (2.8346)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:15:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:15:26 lr 0.000012	 wd 0.0500	time 0.8179 (0.8403)	loss 0.7869 (1.1317)	grad_norm 2.7028 (2.8308)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:16:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:14:01 lr 0.000012	 wd 0.0500	time 0.7744 (0.8398)	loss 1.0597 (1.1278)	grad_norm 2.2670 (2.8527)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:18:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:12:37 lr 0.000012	 wd 0.0500	time 0.7876 (0.8395)	loss 0.9126 (1.1289)	grad_norm 2.6170 (2.8389)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:19:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:11:12 lr 0.000012	 wd 0.0500	time 0.8264 (0.8391)	loss 0.7811 (1.1293)	grad_norm 4.7615 (2.8634)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:20:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:09:48 lr 0.000012	 wd 0.0500	time 0.7836 (0.8390)	loss 1.6149 (1.1279)	grad_norm 2.7496 (2.8478)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:22:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:08:24 lr 0.000012	 wd 0.0500	time 0.7844 (0.8386)	loss 0.8958 (1.1269)	grad_norm 3.3070 (2.8334)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:23:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:07:00 lr 0.000012	 wd 0.0500	time 0.8276 (0.8383)	loss 0.9020 (1.1281)	grad_norm 2.1641 (2.8354)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:24:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:05:36 lr 0.000012	 wd 0.0500	time 0.7866 (0.8381)	loss 0.7052 (1.1267)	grad_norm 3.8595 (2.8404)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:26:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:04:13 lr 0.000012	 wd 0.0500	time 0.7721 (0.8379)	loss 0.8871 (1.1262)	grad_norm 2.6405 (2.8501)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:27:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:02:49 lr 0.000012	 wd 0.0500	time 0.7887 (0.8377)	loss 0.9746 (1.1267)	grad_norm 2.5427 (2.8920)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:29:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:01:25 lr 0.000012	 wd 0.0500	time 0.8325 (0.8375)	loss 1.2946 (1.1270)	grad_norm 2.0195 (2.8911)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:30:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.7767 (0.8373)	loss 1.4301 (1.1279)	grad_norm 2.9884 (2.9331)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:30:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 15 training takes 0:34:57
[2024-08-03 18:30:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_15.pth saving......
[2024-08-03 18:30:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_15.pth saved !!!
[2024-08-03 18:30:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.416 (11.416)	Loss 0.5000 (0.5000)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 18:31:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.702 Acc@5 97.998
[2024-08-03 18:31:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-03 18:31:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.70%
[2024-08-03 18:31:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 18:31:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 18:31:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:54:59 lr 0.000012	 wd 0.0500	time 11.3907 (11.3907)	loss 1.2028 (1.2028)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:32:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:37:35 lr 0.000012	 wd 0.0500	time 0.8118 (0.9390)	loss 1.0794 (1.1370)	grad_norm 2.0414 (2.6486)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:34:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:34:01 lr 0.000012	 wd 0.0500	time 0.8223 (0.8868)	loss 1.1852 (1.1253)	grad_norm 4.4590 (2.7176)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:35:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:31:53 lr 0.000012	 wd 0.0500	time 0.7571 (0.8689)	loss 1.0256 (1.1189)	grad_norm 2.5807 (2.9730)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:36:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:30:08 lr 0.000012	 wd 0.0500	time 0.7881 (0.8602)	loss 0.8308 (1.1085)	grad_norm 3.2320 (2.9581)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:38:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:28:31 lr 0.000012	 wd 0.0500	time 0.8262 (0.8547)	loss 1.3923 (1.1130)	grad_norm 2.5887 (2.8985)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:39:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:26:58 lr 0.000012	 wd 0.0500	time 0.8152 (0.8511)	loss 1.3513 (1.1118)	grad_norm 2.4560 (3.0161)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:41:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:25:28 lr 0.000012	 wd 0.0500	time 0.8174 (0.8485)	loss 1.0065 (1.1141)	grad_norm 3.5649 (2.9668)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:42:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:24:00 lr 0.000012	 wd 0.0500	time 0.7874 (0.8466)	loss 1.3038 (1.1155)	grad_norm 2.4694 (2.9706)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:43:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:22:33 lr 0.000012	 wd 0.0500	time 0.8171 (0.8451)	loss 1.0392 (1.1136)	grad_norm 3.0029 (2.9945)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:45:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:21:07 lr 0.000011	 wd 0.0500	time 0.7821 (0.8440)	loss 1.2785 (1.1148)	grad_norm 1.8186 (3.0288)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:46:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:19:42 lr 0.000011	 wd 0.0500	time 0.7831 (0.8434)	loss 0.8896 (1.1202)	grad_norm 2.3358 (3.0837)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:48:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:18:17 lr 0.000011	 wd 0.0500	time 0.7841 (0.8426)	loss 0.8698 (1.1197)	grad_norm 2.8020 (3.1216)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:49:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:16:51 lr 0.000011	 wd 0.0500	time 0.7814 (0.8418)	loss 1.2501 (1.1227)	grad_norm 2.3144 (3.1213)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:50:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:15:27 lr 0.000011	 wd 0.0500	time 0.7815 (0.8413)	loss 1.1936 (1.1233)	grad_norm 2.9932 (3.0934)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 18:52:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:14:02 lr 0.000011	 wd 0.0500	time 0.7806 (0.8407)	loss 1.1631 (1.1253)	grad_norm 2.1572 (3.1041)	loss_scale 4096.0000 (2181.7135)	mem 23200MB
[2024-08-03 18:53:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:12:37 lr 0.000011	 wd 0.0500	time 0.7655 (0.8402)	loss 1.2303 (1.1256)	grad_norm 3.8292 (3.1000)	loss_scale 4096.0000 (2301.2817)	mem 23200MB
[2024-08-03 18:54:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:11:13 lr 0.000011	 wd 0.0500	time 0.7824 (0.8397)	loss 1.0340 (1.1250)	grad_norm 10.3240 (3.0895)	loss_scale 4096.0000 (2406.7913)	mem 23200MB
[2024-08-03 18:56:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:09:49 lr 0.000011	 wd 0.0500	time 0.7860 (0.8394)	loss 0.8902 (1.1239)	grad_norm 1.8787 (3.0939)	loss_scale 4096.0000 (2500.5841)	mem 23200MB
[2024-08-03 18:57:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:08:25 lr 0.000011	 wd 0.0500	time 0.7457 (0.8392)	loss 0.8438 (1.1246)	grad_norm 2.0766 (3.0910)	loss_scale 4096.0000 (2584.5092)	mem 23200MB
[2024-08-03 18:59:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:07:01 lr 0.000011	 wd 0.0500	time 0.7779 (0.8389)	loss 1.2769 (1.1251)	grad_norm 2.1158 (3.0959)	loss_scale 4096.0000 (2660.0460)	mem 23200MB
[2024-08-03 19:00:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:05:37 lr 0.000011	 wd 0.0500	time 0.7803 (0.8387)	loss 1.2219 (1.1258)	grad_norm 2.2371 (3.0721)	loss_scale 4096.0000 (2728.3922)	mem 23200MB
[2024-08-03 19:01:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:04:13 lr 0.000011	 wd 0.0500	time 0.7953 (0.8385)	loss 0.8854 (1.1256)	grad_norm 2.6118 (3.0758)	loss_scale 4096.0000 (2790.5279)	mem 23200MB
[2024-08-03 19:03:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:02:49 lr 0.000011	 wd 0.0500	time 0.7821 (0.8383)	loss 0.7813 (1.1238)	grad_norm 2.7185 (inf)	loss_scale 2048.0000 (2836.5824)	mem 23200MB
[2024-08-03 19:04:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:01:25 lr 0.000011	 wd 0.0500	time 0.7827 (0.8380)	loss 1.2247 (1.1234)	grad_norm 1.8149 (inf)	loss_scale 2048.0000 (2803.7384)	mem 23200MB
[2024-08-03 19:06:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.7799 (0.8378)	loss 0.7942 (1.1230)	grad_norm 3.1720 (inf)	loss_scale 2048.0000 (2773.5210)	mem 23200MB
[2024-08-03 19:06:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 16 training takes 0:34:58
[2024-08-03 19:06:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.816 (11.816)	Loss 0.4746 (0.4746)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 19:06:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.750 Acc@5 98.016
[2024-08-03 19:06:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 19:06:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.75%
[2024-08-03 19:06:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 19:06:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 19:06:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:58:20 lr 0.000011	 wd 0.0500	time 11.4710 (11.4710)	loss 1.2134 (1.2134)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:08:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:37:34 lr 0.000011	 wd 0.0500	time 0.7864 (0.9386)	loss 1.2160 (1.0997)	grad_norm 2.4528 (2.8248)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:09:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:34:00 lr 0.000011	 wd 0.0500	time 0.8016 (0.8865)	loss 1.3558 (1.1044)	grad_norm 2.8887 (2.8387)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:11:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:31:52 lr 0.000011	 wd 0.0500	time 0.7868 (0.8685)	loss 1.3982 (1.1185)	grad_norm 3.2727 (2.8154)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:12:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:30:07 lr 0.000011	 wd 0.0500	time 0.7872 (0.8600)	loss 1.4972 (1.1141)	grad_norm 2.1481 (2.8000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:13:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:28:34 lr 0.000010	 wd 0.0500	time 0.7791 (0.8562)	loss 1.3219 (1.1206)	grad_norm 2.4162 (2.8214)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:15:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:27:00 lr 0.000010	 wd 0.0500	time 0.7800 (0.8522)	loss 0.8623 (1.1219)	grad_norm 3.5663 (2.8459)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:16:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:25:31 lr 0.000010	 wd 0.0500	time 0.7786 (0.8496)	loss 1.2997 (1.1191)	grad_norm 2.0954 (2.8721)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:18:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:24:02 lr 0.000010	 wd 0.0500	time 0.7827 (0.8473)	loss 1.1865 (1.1186)	grad_norm 2.8720 (2.8856)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:19:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:22:34 lr 0.000010	 wd 0.0500	time 0.7842 (0.8455)	loss 1.2712 (1.1206)	grad_norm 1.9195 (2.9150)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:20:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:21:08 lr 0.000010	 wd 0.0500	time 0.7918 (0.8444)	loss 0.9459 (1.1191)	grad_norm 2.6735 (2.9497)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:22:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:19:42 lr 0.000010	 wd 0.0500	time 0.7853 (0.8432)	loss 0.9563 (1.1237)	grad_norm 2.6055 (2.9279)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:23:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:18:16 lr 0.000010	 wd 0.0500	time 0.7703 (0.8423)	loss 0.8334 (1.1238)	grad_norm 2.8578 (2.8979)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:24:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:16:51 lr 0.000010	 wd 0.0500	time 0.7812 (0.8415)	loss 1.4654 (1.1225)	grad_norm 2.2510 (2.8937)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:26:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:15:26 lr 0.000010	 wd 0.0500	time 0.7735 (0.8409)	loss 1.4000 (1.1226)	grad_norm 2.0519 (2.8994)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:27:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:14:02 lr 0.000010	 wd 0.0500	time 0.7954 (0.8407)	loss 1.2916 (1.1227)	grad_norm 3.1710 (2.8929)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:29:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:12:37 lr 0.000010	 wd 0.0500	time 0.7786 (0.8403)	loss 1.2966 (1.1212)	grad_norm 9.0290 (2.9282)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:30:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:11:13 lr 0.000010	 wd 0.0500	time 0.7660 (0.8398)	loss 1.5186 (1.1230)	grad_norm 2.4786 (2.9188)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:31:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:09:49 lr 0.000010	 wd 0.0500	time 0.8267 (0.8394)	loss 0.9534 (1.1230)	grad_norm 2.2557 (2.9324)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:33:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:08:25 lr 0.000010	 wd 0.0500	time 0.7715 (0.8390)	loss 1.1838 (1.1221)	grad_norm 2.3562 (2.9395)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:34:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:07:01 lr 0.000010	 wd 0.0500	time 0.8115 (0.8388)	loss 0.9720 (1.1230)	grad_norm 2.8822 (2.9353)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:36:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:05:37 lr 0.000010	 wd 0.0500	time 0.7806 (0.8385)	loss 1.4047 (1.1229)	grad_norm 2.6869 (2.9227)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:37:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:04:13 lr 0.000010	 wd 0.0500	time 0.7936 (0.8382)	loss 1.4587 (1.1236)	grad_norm 4.5423 (2.9135)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:38:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:02:49 lr 0.000010	 wd 0.0500	time 0.7842 (0.8379)	loss 0.8262 (1.1232)	grad_norm 2.2049 (2.9122)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:40:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:01:25 lr 0.000010	 wd 0.0500	time 0.7799 (0.8377)	loss 0.8518 (1.1219)	grad_norm 2.8763 (2.8943)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:41:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.7762 (0.8376)	loss 1.4547 (1.1219)	grad_norm 21.2783 (2.9067)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:41:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 17 training takes 0:34:58
[2024-08-03 19:41:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.450 (12.450)	Loss 0.4700 (0.4700)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 19:42:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.758 Acc@5 98.046
[2024-08-03 19:42:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 19:42:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.76%
[2024-08-03 19:42:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 19:42:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 19:42:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:40:18 lr 0.000009	 wd 0.0500	time 11.0387 (11.0387)	loss 1.4563 (1.4563)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:43:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:37:21 lr 0.000009	 wd 0.0500	time 0.7855 (0.9331)	loss 1.4553 (1.1306)	grad_norm 1.9665 (2.8032)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:45:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:33:54 lr 0.000009	 wd 0.0500	time 0.7818 (0.8837)	loss 0.9506 (1.1334)	grad_norm 2.4578 (3.3434)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:46:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:31:48 lr 0.000009	 wd 0.0500	time 0.8194 (0.8666)	loss 1.5643 (1.1352)	grad_norm 4.8616 (3.1469)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:47:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:30:04 lr 0.000009	 wd 0.0500	time 0.7967 (0.8584)	loss 0.7160 (1.1354)	grad_norm 3.1465 (3.1646)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:49:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:28:29 lr 0.000009	 wd 0.0500	time 0.7899 (0.8540)	loss 1.3298 (1.1317)	grad_norm 2.1295 (3.0655)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:50:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:26:58 lr 0.000009	 wd 0.0500	time 0.7806 (0.8508)	loss 1.5267 (1.1259)	grad_norm 3.2047 (3.0371)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:52:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:25:28 lr 0.000009	 wd 0.0500	time 0.7863 (0.8482)	loss 1.3683 (1.1286)	grad_norm 2.3864 (2.9975)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:53:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:24:00 lr 0.000009	 wd 0.0500	time 0.7621 (0.8464)	loss 1.3327 (1.1328)	grad_norm 2.2401 (3.0597)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:54:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:22:35 lr 0.000009	 wd 0.0500	time 0.8111 (0.8458)	loss 0.9423 (1.1308)	grad_norm 2.2125 (3.0581)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:56:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:21:08 lr 0.000009	 wd 0.0500	time 0.7782 (0.8443)	loss 1.3657 (1.1288)	grad_norm 2.3946 (3.0215)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:57:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:19:42 lr 0.000009	 wd 0.0500	time 0.7691 (0.8433)	loss 0.9781 (1.1301)	grad_norm 2.5992 (2.9860)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 19:59:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:18:16 lr 0.000009	 wd 0.0500	time 0.7763 (0.8425)	loss 0.9553 (1.1246)	grad_norm 4.8045 (2.9915)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:00:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:16:51 lr 0.000009	 wd 0.0500	time 0.8051 (0.8418)	loss 1.4486 (1.1244)	grad_norm 2.3143 (2.9837)	loss_scale 4096.0000 (2073.1868)	mem 23200MB
[2024-08-03 20:01:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:15:26 lr 0.000009	 wd 0.0500	time 0.7907 (0.8411)	loss 1.2347 (1.1252)	grad_norm 3.2340 (2.9548)	loss_scale 4096.0000 (2217.5703)	mem 23200MB
[2024-08-03 20:03:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:14:02 lr 0.000009	 wd 0.0500	time 0.7812 (0.8405)	loss 1.3412 (1.1253)	grad_norm 2.2367 (2.9320)	loss_scale 4096.0000 (2342.7155)	mem 23200MB
[2024-08-03 20:04:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:12:37 lr 0.000009	 wd 0.0500	time 0.7830 (0.8400)	loss 1.2881 (1.1268)	grad_norm 3.0036 (2.9334)	loss_scale 4096.0000 (2452.2274)	mem 23200MB
[2024-08-03 20:06:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:11:13 lr 0.000009	 wd 0.0500	time 0.7769 (0.8397)	loss 1.2312 (1.1282)	grad_norm 27.5841 (inf)	loss_scale 2048.0000 (2529.5991)	mem 23200MB
[2024-08-03 20:07:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:09:49 lr 0.000009	 wd 0.0500	time 0.7690 (0.8392)	loss 0.7422 (1.1285)	grad_norm 2.3553 (inf)	loss_scale 2048.0000 (2502.8584)	mem 23200MB
[2024-08-03 20:08:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:08:25 lr 0.000009	 wd 0.0500	time 0.7857 (0.8391)	loss 1.4752 (1.1285)	grad_norm 1.6427 (inf)	loss_scale 2048.0000 (2478.9311)	mem 23200MB
[2024-08-03 20:10:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:07:01 lr 0.000008	 wd 0.0500	time 0.7823 (0.8387)	loss 0.9741 (1.1278)	grad_norm 2.7636 (inf)	loss_scale 2048.0000 (2457.3953)	mem 23200MB
[2024-08-03 20:11:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:05:37 lr 0.000008	 wd 0.0500	time 0.7903 (0.8384)	loss 1.2806 (1.1274)	grad_norm 2.2621 (inf)	loss_scale 2048.0000 (2437.9096)	mem 23200MB
[2024-08-03 20:12:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:04:13 lr 0.000008	 wd 0.0500	time 0.7714 (0.8380)	loss 1.0425 (1.1275)	grad_norm 4.0063 (inf)	loss_scale 2048.0000 (2420.1945)	mem 23200MB
[2024-08-03 20:14:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.7930 (0.8377)	loss 1.3272 (1.1286)	grad_norm 2.6152 (inf)	loss_scale 2048.0000 (2404.0191)	mem 23200MB
[2024-08-03 20:15:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:01:25 lr 0.000008	 wd 0.0500	time 0.7818 (0.8376)	loss 1.3750 (1.1288)	grad_norm 2.5991 (inf)	loss_scale 2048.0000 (2389.1912)	mem 23200MB
[2024-08-03 20:17:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.7846 (0.8374)	loss 1.1261 (1.1290)	grad_norm 1.9225 (inf)	loss_scale 2048.0000 (2375.5490)	mem 23200MB
[2024-08-03 20:17:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 18 training takes 0:34:57
[2024-08-03 20:17:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.280 (12.280)	Loss 0.4814 (0.4814)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 20:17:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.750 Acc@5 98.074
[2024-08-03 20:17:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 20:17:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.76%
[2024-08-03 20:17:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:01:29 lr 0.000008	 wd 0.0500	time 11.5465 (11.5465)	loss 0.8320 (0.8320)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:19:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:37:34 lr 0.000008	 wd 0.0500	time 0.7803 (0.9387)	loss 1.5362 (1.1693)	grad_norm 1.9969 (3.1309)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:20:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:34:01 lr 0.000008	 wd 0.0500	time 0.8844 (0.8867)	loss 0.8705 (1.1389)	grad_norm 2.5240 (3.0127)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:22:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:31:55 lr 0.000008	 wd 0.0500	time 0.7809 (0.8697)	loss 1.3902 (1.1280)	grad_norm 3.0953 (2.9217)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:23:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:30:08 lr 0.000008	 wd 0.0500	time 0.7713 (0.8604)	loss 0.9783 (1.1192)	grad_norm 2.9626 (2.8575)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:24:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:28:31 lr 0.000008	 wd 0.0500	time 0.7800 (0.8550)	loss 1.3625 (1.1271)	grad_norm 6.7607 (2.8671)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:26:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:26:58 lr 0.000008	 wd 0.0500	time 0.7672 (0.8512)	loss 0.8545 (1.1193)	grad_norm 2.9520 (2.8792)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:27:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:25:29 lr 0.000008	 wd 0.0500	time 0.7752 (0.8485)	loss 1.0803 (1.1183)	grad_norm 2.0380 (2.9569)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 20:29:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:24:00 lr 0.000008	 wd 0.0500	time 0.8278 (0.8463)	loss 1.3620 (1.1194)	grad_norm 3.3821 (inf)	loss_scale 1024.0000 (2007.0911)	mem 23200MB
[2024-08-03 20:30:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:22:33 lr 0.000008	 wd 0.0500	time 0.7827 (0.8448)	loss 1.1539 (1.1205)	grad_norm 7.1103 (inf)	loss_scale 1024.0000 (1897.9800)	mem 23200MB
[2024-08-03 20:31:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:21:07 lr 0.000008	 wd 0.0500	time 0.7456 (0.8436)	loss 1.3186 (1.1208)	grad_norm 3.0530 (inf)	loss_scale 1024.0000 (1810.6693)	mem 23200MB
[2024-08-03 20:33:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:19:41 lr 0.000008	 wd 0.0500	time 0.7892 (0.8425)	loss 1.1067 (1.1190)	grad_norm 2.2777 (inf)	loss_scale 1024.0000 (1739.2189)	mem 23200MB
[2024-08-03 20:34:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:18:16 lr 0.000008	 wd 0.0500	time 0.7705 (0.8419)	loss 1.3117 (1.1175)	grad_norm 2.2764 (inf)	loss_scale 1024.0000 (1679.6669)	mem 23200MB
[2024-08-03 20:36:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:16:51 lr 0.000008	 wd 0.0500	time 0.7826 (0.8414)	loss 0.8826 (1.1170)	grad_norm 3.4007 (inf)	loss_scale 1024.0000 (1629.2698)	mem 23200MB
[2024-08-03 20:37:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:15:26 lr 0.000008	 wd 0.0500	time 0.7724 (0.8407)	loss 0.8696 (1.1170)	grad_norm 4.4693 (inf)	loss_scale 1024.0000 (1586.0671)	mem 23200MB
[2024-08-03 20:38:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:14:01 lr 0.000008	 wd 0.0500	time 0.7636 (0.8402)	loss 1.4376 (1.1179)	grad_norm 2.3556 (inf)	loss_scale 1024.0000 (1548.6209)	mem 23200MB
[2024-08-03 20:40:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:12:37 lr 0.000007	 wd 0.0500	time 0.8166 (0.8397)	loss 1.1745 (1.1194)	grad_norm 1.9217 (inf)	loss_scale 1024.0000 (1515.8526)	mem 23200MB
[2024-08-03 20:41:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:11:13 lr 0.000007	 wd 0.0500	time 0.7742 (0.8393)	loss 0.7737 (1.1198)	grad_norm 2.5535 (inf)	loss_scale 1024.0000 (1486.9371)	mem 23200MB
[2024-08-03 20:42:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:09:48 lr 0.000007	 wd 0.0500	time 0.7790 (0.8389)	loss 1.2421 (1.1216)	grad_norm 3.1593 (inf)	loss_scale 1024.0000 (1461.2326)	mem 23200MB
[2024-08-03 20:44:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:08:24 lr 0.000007	 wd 0.0500	time 0.7780 (0.8387)	loss 1.1273 (1.1202)	grad_norm 2.2717 (inf)	loss_scale 1024.0000 (1438.2325)	mem 23200MB
[2024-08-03 20:45:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:07:00 lr 0.000007	 wd 0.0500	time 0.7833 (0.8382)	loss 1.2660 (1.1212)	grad_norm 2.6654 (inf)	loss_scale 1024.0000 (1417.5312)	mem 23200MB
[2024-08-03 20:47:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:05:36 lr 0.000007	 wd 0.0500	time 0.7715 (0.8380)	loss 1.5854 (1.1232)	grad_norm 2.2437 (inf)	loss_scale 1024.0000 (1398.8006)	mem 23200MB
[2024-08-03 20:48:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:04:13 lr 0.000007	 wd 0.0500	time 0.7707 (0.8379)	loss 1.2154 (1.1225)	grad_norm 2.6876 (inf)	loss_scale 1024.0000 (1381.7719)	mem 23200MB
[2024-08-03 20:49:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:02:49 lr 0.000007	 wd 0.0500	time 0.7846 (0.8377)	loss 0.9650 (1.1221)	grad_norm 3.0743 (inf)	loss_scale 1024.0000 (1366.2234)	mem 23200MB
[2024-08-03 20:51:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:01:25 lr 0.000007	 wd 0.0500	time 0.7665 (0.8375)	loss 1.0635 (1.1213)	grad_norm 3.9442 (inf)	loss_scale 1024.0000 (1351.9700)	mem 23200MB
[2024-08-03 20:52:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.7751 (0.8374)	loss 0.7955 (1.1213)	grad_norm 2.6969 (inf)	loss_scale 1024.0000 (1338.8565)	mem 23200MB
[2024-08-03 20:52:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 19 training takes 0:34:57
[2024-08-03 20:52:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.060 (12.060)	Loss 0.4578 (0.4578)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 20:53:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.798 Acc@5 98.084
[2024-08-03 20:53:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 20:53:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.80%
[2024-08-03 20:53:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 20:53:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 20:53:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:48:09 lr 0.000007	 wd 0.0500	time 11.2267 (11.2267)	loss 1.3218 (1.3218)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 20:54:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:37:22 lr 0.000007	 wd 0.0500	time 0.8173 (0.9335)	loss 0.9187 (1.1273)	grad_norm 2.5293 (3.5735)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 20:56:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:33:52 lr 0.000007	 wd 0.0500	time 0.7911 (0.8831)	loss 1.2746 (1.1167)	grad_norm 2.3286 (3.1884)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 20:57:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:31:47 lr 0.000007	 wd 0.0500	time 0.7850 (0.8663)	loss 0.7000 (1.1267)	grad_norm 2.3700 (3.1336)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 20:59:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:30:02 lr 0.000007	 wd 0.0500	time 0.7767 (0.8577)	loss 1.2633 (1.1242)	grad_norm 2.0388 (3.0523)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:00:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:28:27 lr 0.000007	 wd 0.0500	time 0.7731 (0.8528)	loss 1.2532 (1.1235)	grad_norm 2.8557 (3.0594)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:01:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:26:57 lr 0.000007	 wd 0.0500	time 0.7831 (0.8506)	loss 1.3421 (1.1276)	grad_norm 2.5579 (2.9775)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:03:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:25:29 lr 0.000007	 wd 0.0500	time 0.7489 (0.8486)	loss 1.2425 (1.1259)	grad_norm 2.5261 (2.9400)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:04:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:24:00 lr 0.000007	 wd 0.0500	time 0.7779 (0.8466)	loss 1.1540 (1.1277)	grad_norm 1.9607 (2.9549)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:05:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:22:33 lr 0.000007	 wd 0.0500	time 0.7673 (0.8449)	loss 1.3311 (1.1259)	grad_norm 3.6594 (2.9490)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:07:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:21:07 lr 0.000007	 wd 0.0500	time 0.7842 (0.8438)	loss 1.2071 (1.1275)	grad_norm 2.1727 (2.9703)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:08:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:19:41 lr 0.000007	 wd 0.0500	time 0.7467 (0.8426)	loss 0.8977 (1.1254)	grad_norm 3.9654 (2.9600)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:10:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:18:15 lr 0.000006	 wd 0.0500	time 0.7575 (0.8416)	loss 1.2156 (1.1223)	grad_norm 2.1818 (2.9288)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:11:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:16:50 lr 0.000006	 wd 0.0500	time 0.7862 (0.8409)	loss 1.3620 (1.1223)	grad_norm 1.8872 (2.9160)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:12:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:15:26 lr 0.000006	 wd 0.0500	time 0.7840 (0.8403)	loss 1.0523 (1.1242)	grad_norm 3.0199 (2.8982)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:14:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:14:01 lr 0.000006	 wd 0.0500	time 0.7813 (0.8398)	loss 1.3440 (1.1258)	grad_norm 2.6120 (2.9097)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:15:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:12:37 lr 0.000006	 wd 0.0500	time 0.7874 (0.8396)	loss 1.2096 (1.1277)	grad_norm 6.4038 (2.9410)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:17:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:11:13 lr 0.000006	 wd 0.0500	time 0.7839 (0.8393)	loss 1.4876 (1.1283)	grad_norm 3.9760 (2.9283)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:18:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:09:48 lr 0.000006	 wd 0.0500	time 0.7785 (0.8389)	loss 0.7806 (1.1262)	grad_norm 2.2409 (2.9236)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:19:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:08:24 lr 0.000006	 wd 0.0500	time 0.7443 (0.8386)	loss 0.7974 (1.1267)	grad_norm 2.0067 (2.9175)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:21:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:07:00 lr 0.000006	 wd 0.0500	time 0.7828 (0.8383)	loss 1.0798 (1.1262)	grad_norm 2.3930 (2.9619)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:22:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:05:36 lr 0.000006	 wd 0.0500	time 0.7833 (0.8380)	loss 1.2922 (1.1266)	grad_norm 1.8757 (2.9695)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:24:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:04:13 lr 0.000006	 wd 0.0500	time 0.8330 (0.8378)	loss 1.3115 (1.1280)	grad_norm 2.0808 (2.9690)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 21:25:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:02:49 lr 0.000006	 wd 0.0500	time 0.7785 (0.8377)	loss 0.7950 (1.1293)	grad_norm 2.1872 (2.9632)	loss_scale 2048.0000 (1039.1308)	mem 23200MB
[2024-08-03 21:26:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:01:25 lr 0.000006	 wd 0.0500	time 0.7982 (0.8376)	loss 0.7228 (1.1288)	grad_norm 2.5025 (2.9491)	loss_scale 2048.0000 (1081.1495)	mem 23200MB
[2024-08-03 21:28:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.7677 (0.8374)	loss 1.0868 (1.1306)	grad_norm 2.4877 (2.9448)	loss_scale 2048.0000 (1119.8081)	mem 23200MB
[2024-08-03 21:28:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 20 training takes 0:34:58
[2024-08-03 21:28:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.261 (12.261)	Loss 0.4727 (0.4727)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 23200MB
[2024-08-03 21:28:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.740 Acc@5 98.024
[2024-08-03 21:28:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.7%
[2024-08-03 21:28:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.80%
[2024-08-03 21:29:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:46:03 lr 0.000006	 wd 0.0500	time 12.6154 (12.6154)	loss 0.8726 (0.8726)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:30:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:38:11 lr 0.000006	 wd 0.0500	time 0.7859 (0.9539)	loss 1.3061 (1.1316)	grad_norm 3.2187 (3.1005)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:31:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:34:16 lr 0.000006	 wd 0.0500	time 0.7865 (0.8933)	loss 1.2739 (1.1344)	grad_norm 17.5962 (3.0980)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:33:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:32:03 lr 0.000006	 wd 0.0500	time 0.7745 (0.8736)	loss 1.0815 (1.1302)	grad_norm 2.7932 (3.0883)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:34:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:30:15 lr 0.000006	 wd 0.0500	time 0.7807 (0.8636)	loss 1.4142 (1.1207)	grad_norm 2.7082 (2.9979)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:35:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:28:36 lr 0.000006	 wd 0.0500	time 0.7459 (0.8575)	loss 0.8044 (1.1162)	grad_norm 2.3065 (2.9482)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:37:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:27:02 lr 0.000006	 wd 0.0500	time 0.8142 (0.8531)	loss 1.2657 (1.1121)	grad_norm 3.0251 (2.9376)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:38:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:25:31 lr 0.000006	 wd 0.0500	time 0.8305 (0.8501)	loss 1.2474 (1.1144)	grad_norm 2.5286 (2.9759)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:40:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:24:03 lr 0.000006	 wd 0.0500	time 0.7769 (0.8479)	loss 1.2790 (1.1135)	grad_norm 2.9668 (2.9229)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:41:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:22:35 lr 0.000005	 wd 0.0500	time 0.7849 (0.8462)	loss 1.3345 (1.1173)	grad_norm 2.5216 (2.9228)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:42:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:21:09 lr 0.000005	 wd 0.0500	time 0.7831 (0.8454)	loss 1.3247 (1.1207)	grad_norm 7.5878 (2.9379)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:44:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:19:43 lr 0.000005	 wd 0.0500	time 0.7759 (0.8444)	loss 0.8169 (1.1242)	grad_norm 2.3821 (2.9293)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:45:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:18:18 lr 0.000005	 wd 0.0500	time 0.7610 (0.8434)	loss 1.2671 (1.1207)	grad_norm 2.5792 (2.9476)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:47:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:16:52 lr 0.000005	 wd 0.0500	time 0.7681 (0.8425)	loss 1.2660 (1.1208)	grad_norm 2.2596 (2.9161)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:48:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:15:27 lr 0.000005	 wd 0.0500	time 0.7761 (0.8418)	loss 0.7760 (1.1186)	grad_norm 2.8329 (2.9001)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:49:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:14:02 lr 0.000005	 wd 0.0500	time 0.7732 (0.8413)	loss 0.7788 (1.1200)	grad_norm 1.9709 (2.9540)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:51:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:12:38 lr 0.000005	 wd 0.0500	time 0.7885 (0.8409)	loss 1.1017 (1.1168)	grad_norm 2.7832 (3.0749)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:52:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:11:13 lr 0.000005	 wd 0.0500	time 0.7778 (0.8404)	loss 1.3291 (1.1167)	grad_norm 5.5882 (3.0599)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:54:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:09:49 lr 0.000005	 wd 0.0500	time 0.7832 (0.8399)	loss 0.8315 (1.1170)	grad_norm 2.9033 (3.0563)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:55:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:08:25 lr 0.000005	 wd 0.0500	time 0.7863 (0.8395)	loss 0.7169 (1.1184)	grad_norm 2.7423 (3.0312)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:56:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:07:01 lr 0.000005	 wd 0.0500	time 0.7753 (0.8393)	loss 1.1065 (1.1180)	grad_norm 2.5449 (3.0188)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:58:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:05:37 lr 0.000005	 wd 0.0500	time 0.7821 (0.8390)	loss 1.2168 (1.1175)	grad_norm 5.1334 (3.0075)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 21:59:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:04:13 lr 0.000005	 wd 0.0500	time 0.7999 (0.8387)	loss 1.3742 (1.1159)	grad_norm 2.2841 (3.0036)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:00:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:02:49 lr 0.000005	 wd 0.0500	time 0.7880 (0.8385)	loss 1.3686 (1.1180)	grad_norm 4.2717 (2.9967)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:02:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:01:25 lr 0.000005	 wd 0.0500	time 0.7460 (0.8383)	loss 1.0754 (1.1190)	grad_norm 2.5489 (2.9926)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:03:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.7708 (0.8380)	loss 1.4044 (1.1185)	grad_norm 2.3916 (2.9881)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:03:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 21 training takes 0:35:00
[2024-08-03 22:04:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.972 (11.972)	Loss 0.4871 (0.4871)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 22:04:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.760 Acc@5 98.044
[2024-08-03 22:04:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 22:04:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.80%
[2024-08-03 22:04:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:31:50 lr 0.000005	 wd 0.0500	time 12.2744 (12.2744)	loss 0.9979 (0.9979)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:06:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:37:55 lr 0.000005	 wd 0.0500	time 0.7903 (0.9472)	loss 1.2822 (1.1289)	grad_norm 2.0351 (2.8776)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:07:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:34:08 lr 0.000005	 wd 0.0500	time 0.7831 (0.8898)	loss 0.6556 (1.1392)	grad_norm 2.4308 (3.6919)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:08:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:31:57 lr 0.000005	 wd 0.0500	time 0.7909 (0.8708)	loss 1.4887 (1.1410)	grad_norm 2.2857 (3.3813)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:10:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:30:14 lr 0.000005	 wd 0.0500	time 0.7891 (0.8633)	loss 0.7531 (1.1413)	grad_norm 2.3683 (3.2642)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:11:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:28:36 lr 0.000005	 wd 0.0500	time 0.7466 (0.8574)	loss 1.0703 (1.1375)	grad_norm 2.2805 (3.3080)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:12:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:27:02 lr 0.000005	 wd 0.0500	time 0.7801 (0.8533)	loss 0.8558 (1.1368)	grad_norm 1.8496 (3.1887)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:14:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:25:32 lr 0.000005	 wd 0.0500	time 0.7744 (0.8502)	loss 1.3807 (1.1309)	grad_norm 2.4891 (3.1071)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:15:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:24:03 lr 0.000004	 wd 0.0500	time 0.7803 (0.8480)	loss 1.0113 (1.1310)	grad_norm 2.1977 (3.0619)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:17:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:22:35 lr 0.000004	 wd 0.0500	time 0.7923 (0.8462)	loss 1.1146 (1.1310)	grad_norm 2.0550 (3.0918)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:18:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:21:09 lr 0.000004	 wd 0.0500	time 0.7764 (0.8450)	loss 1.3369 (1.1315)	grad_norm 4.1652 (3.0821)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:19:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:19:43 lr 0.000004	 wd 0.0500	time 0.7784 (0.8439)	loss 0.7684 (1.1276)	grad_norm 1.9886 (3.0746)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:21:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:18:17 lr 0.000004	 wd 0.0500	time 0.7859 (0.8428)	loss 1.2773 (1.1283)	grad_norm 2.6606 (3.0595)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-03 22:22:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:16:52 lr 0.000004	 wd 0.0500	time 0.7766 (0.8421)	loss 0.8127 (1.1223)	grad_norm 2.6574 (3.0420)	loss_scale 4096.0000 (2107.8186)	mem 23200MB
[2024-08-03 22:24:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:15:27 lr 0.000004	 wd 0.0500	time 0.8284 (0.8417)	loss 1.0190 (1.1175)	grad_norm 3.1751 (3.0167)	loss_scale 4096.0000 (2249.7302)	mem 23200MB
[2024-08-03 22:25:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:14:02 lr 0.000004	 wd 0.0500	time 0.7727 (0.8411)	loss 1.1007 (1.1170)	grad_norm 2.3829 (3.0329)	loss_scale 4096.0000 (2372.7328)	mem 23200MB
[2024-08-03 22:26:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:12:38 lr 0.000004	 wd 0.0500	time 0.7739 (0.8406)	loss 1.3165 (1.1165)	grad_norm 2.0217 (3.0122)	loss_scale 4096.0000 (2480.3698)	mem 23200MB
[2024-08-03 22:28:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:11:13 lr 0.000004	 wd 0.0500	time 0.8190 (0.8400)	loss 0.9213 (1.1146)	grad_norm 2.0108 (2.9946)	loss_scale 4096.0000 (2575.3510)	mem 23200MB
[2024-08-03 22:29:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:09:49 lr 0.000004	 wd 0.0500	time 0.7687 (0.8396)	loss 1.1689 (1.1132)	grad_norm 3.2317 (3.0180)	loss_scale 4096.0000 (2659.7846)	mem 23200MB
[2024-08-03 22:31:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:08:25 lr 0.000004	 wd 0.0500	time 0.7753 (0.8393)	loss 0.9417 (1.1141)	grad_norm 2.1275 (3.0080)	loss_scale 4096.0000 (2735.3351)	mem 23200MB
[2024-08-03 22:32:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:07:01 lr 0.000004	 wd 0.0500	time 0.8301 (0.8389)	loss 0.8959 (1.1153)	grad_norm 2.0650 (2.9972)	loss_scale 4096.0000 (2803.3343)	mem 23200MB
[2024-08-03 22:33:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:05:37 lr 0.000004	 wd 0.0500	time 0.7788 (0.8387)	loss 1.2344 (1.1164)	grad_norm 2.3626 (3.0209)	loss_scale 4096.0000 (2864.8605)	mem 23200MB
[2024-08-03 22:35:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:04:13 lr 0.000004	 wd 0.0500	time 0.7724 (0.8383)	loss 1.0685 (1.1162)	grad_norm 2.1469 (3.0140)	loss_scale 4096.0000 (2920.7960)	mem 23200MB
[2024-08-03 22:36:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:02:49 lr 0.000004	 wd 0.0500	time 0.8067 (0.8381)	loss 1.1878 (1.1166)	grad_norm 2.1627 (3.0045)	loss_scale 4096.0000 (2971.8696)	mem 23200MB
[2024-08-03 22:37:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:01:25 lr 0.000004	 wd 0.0500	time 0.7845 (0.8380)	loss 1.4026 (1.1182)	grad_norm 2.3718 (3.0016)	loss_scale 4096.0000 (3018.6889)	mem 23200MB
[2024-08-03 22:39:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.7919 (0.8378)	loss 0.8763 (1.1185)	grad_norm 2.6911 (3.0120)	loss_scale 4096.0000 (3061.7641)	mem 23200MB
[2024-08-03 22:39:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 22 training takes 0:35:00
[2024-08-03 22:39:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.617 (11.617)	Loss 0.4705 (0.4705)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 22:40:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.786 Acc@5 98.096
[2024-08-03 22:40:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 22:40:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.80%
[2024-08-03 22:40:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][0/2502]	eta 8:34:06 lr 0.000004	 wd 0.0500	time 12.3288 (12.3288)	loss 0.7686 (0.7686)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 23200MB
[2024-08-03 22:41:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:37:51 lr 0.000004	 wd 0.0500	time 0.7871 (0.9455)	loss 1.3320 (1.1047)	grad_norm 2.8825 (2.6685)	loss_scale 4096.0000 (4096.0000)	mem 23200MB
[2024-08-03 22:43:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:34:05 lr 0.000004	 wd 0.0500	time 0.7958 (0.8884)	loss 0.8992 (1.1137)	grad_norm 5.0733 (3.0256)	loss_scale 4096.0000 (4096.0000)	mem 23200MB
[2024-08-03 22:44:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:31:54 lr 0.000004	 wd 0.0500	time 0.7849 (0.8694)	loss 1.3158 (1.1031)	grad_norm 2.3782 (3.0358)	loss_scale 4096.0000 (4096.0000)	mem 23200MB
[2024-08-03 22:45:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:30:08 lr 0.000004	 wd 0.0500	time 0.8051 (0.8605)	loss 0.8941 (1.1101)	grad_norm 2.3764 (2.9742)	loss_scale 4096.0000 (4096.0000)	mem 23200MB
[2024-08-03 22:47:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:28:31 lr 0.000004	 wd 0.0500	time 0.7840 (0.8548)	loss 0.8752 (1.1101)	grad_norm 3.0075 (2.9139)	loss_scale 4096.0000 (4096.0000)	mem 23200MB
[2024-08-03 22:48:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:26:58 lr 0.000004	 wd 0.0500	time 0.7746 (0.8510)	loss 1.3400 (1.1094)	grad_norm 3.0370 (nan)	loss_scale 2048.0000 (3980.1398)	mem 23200MB
[2024-08-03 22:49:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:25:28 lr 0.000004	 wd 0.0500	time 0.8937 (0.8484)	loss 1.2263 (1.1121)	grad_norm 3.0673 (nan)	loss_scale 2048.0000 (3704.5136)	mem 23200MB
[2024-08-03 22:51:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:24:02 lr 0.000003	 wd 0.0500	time 0.8275 (0.8475)	loss 1.3274 (1.1147)	grad_norm 1.8853 (nan)	loss_scale 2048.0000 (3497.7079)	mem 23200MB
[2024-08-03 22:52:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:22:35 lr 0.000003	 wd 0.0500	time 0.7760 (0.8459)	loss 1.1140 (1.1158)	grad_norm 1.8490 (nan)	loss_scale 2048.0000 (3336.8080)	mem 23200MB
[2024-08-03 22:54:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:21:08 lr 0.000003	 wd 0.0500	time 0.7790 (0.8446)	loss 1.1482 (1.1121)	grad_norm 1.8810 (nan)	loss_scale 2048.0000 (3208.0559)	mem 23200MB
[2024-08-03 22:55:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:19:42 lr 0.000003	 wd 0.0500	time 0.7793 (0.8435)	loss 0.9977 (1.1142)	grad_norm 2.8036 (nan)	loss_scale 2048.0000 (3102.6921)	mem 23200MB
[2024-08-03 22:56:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:18:17 lr 0.000003	 wd 0.0500	time 0.7858 (0.8426)	loss 1.3135 (1.1165)	grad_norm 2.4378 (nan)	loss_scale 2048.0000 (3014.8743)	mem 23200MB
[2024-08-03 22:58:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:16:51 lr 0.000003	 wd 0.0500	time 0.7647 (0.8419)	loss 1.1943 (1.1177)	grad_norm 2.5256 (nan)	loss_scale 1024.0000 (2937.4081)	mem 23200MB
[2024-08-03 22:59:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:15:27 lr 0.000003	 wd 0.0500	time 0.7648 (0.8412)	loss 1.3587 (1.1176)	grad_norm 2.4553 (nan)	loss_scale 1024.0000 (2800.8337)	mem 23200MB
[2024-08-03 23:01:03 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:14:02 lr 0.000003	 wd 0.0500	time 0.7865 (0.8408)	loss 1.3462 (1.1160)	grad_norm 2.6942 (nan)	loss_scale 1024.0000 (2682.4570)	mem 23200MB
[2024-08-03 23:02:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:12:37 lr 0.000003	 wd 0.0500	time 0.7829 (0.8403)	loss 1.1618 (1.1189)	grad_norm 2.3756 (nan)	loss_scale 1024.0000 (2578.8682)	mem 23200MB
[2024-08-03 23:03:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:11:13 lr 0.000003	 wd 0.0500	time 0.7749 (0.8399)	loss 0.6891 (1.1162)	grad_norm 5.9592 (nan)	loss_scale 1024.0000 (2487.4591)	mem 23200MB
[2024-08-03 23:05:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:09:49 lr 0.000003	 wd 0.0500	time 0.7937 (0.8397)	loss 1.3654 (1.1157)	grad_norm 2.3277 (nan)	loss_scale 1024.0000 (2406.2010)	mem 23200MB
[2024-08-03 23:06:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:08:25 lr 0.000003	 wd 0.0500	time 0.7921 (0.8393)	loss 1.3164 (1.1152)	grad_norm 2.6453 (nan)	loss_scale 1024.0000 (2333.4918)	mem 23200MB
[2024-08-03 23:08:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:07:01 lr 0.000003	 wd 0.0500	time 0.7839 (0.8389)	loss 0.8086 (1.1143)	grad_norm 2.3835 (nan)	loss_scale 1024.0000 (2268.0500)	mem 23200MB
[2024-08-03 23:09:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:05:37 lr 0.000003	 wd 0.0500	time 0.7818 (0.8387)	loss 1.2438 (1.1151)	grad_norm 2.8387 (nan)	loss_scale 1024.0000 (2208.8377)	mem 23200MB
[2024-08-03 23:10:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:04:13 lr 0.000003	 wd 0.0500	time 0.7455 (0.8384)	loss 1.2745 (1.1140)	grad_norm 2.8767 (nan)	loss_scale 1024.0000 (2155.0059)	mem 23200MB
[2024-08-03 23:12:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:02:49 lr 0.000003	 wd 0.0500	time 0.7812 (0.8381)	loss 1.4043 (1.1136)	grad_norm 2.1931 (nan)	loss_scale 1024.0000 (2105.8531)	mem 23200MB
[2024-08-03 23:13:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:01:25 lr 0.000003	 wd 0.0500	time 0.7667 (0.8379)	loss 0.9588 (1.1134)	grad_norm 2.2882 (nan)	loss_scale 1024.0000 (2060.7947)	mem 23200MB
[2024-08-03 23:14:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.8057 (0.8376)	loss 0.8435 (1.1123)	grad_norm 2.1727 (nan)	loss_scale 1024.0000 (2019.3395)	mem 23200MB
[2024-08-03 23:15:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 23 training takes 0:34:59
[2024-08-03 23:15:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 13.269 (13.269)	Loss 0.4858 (0.4858)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 23:15:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.786 Acc@5 98.068
[2024-08-03 23:15:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 23:15:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.80%
[2024-08-03 23:15:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:42:05 lr 0.000003	 wd 0.0500	time 12.5203 (12.5203)	loss 1.2400 (1.2400)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:17:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:38:07 lr 0.000003	 wd 0.0500	time 0.7846 (0.9522)	loss 0.9289 (1.1239)	grad_norm 1.9714 (3.2080)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:18:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:34:20 lr 0.000003	 wd 0.0500	time 0.7795 (0.8951)	loss 1.1355 (1.1160)	grad_norm 1.7914 (3.2473)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:20:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:32:07 lr 0.000003	 wd 0.0500	time 0.7856 (0.8752)	loss 1.3130 (1.1280)	grad_norm 2.2267 (3.1671)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:21:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:30:16 lr 0.000003	 wd 0.0500	time 0.7837 (0.8643)	loss 0.6686 (1.1257)	grad_norm 2.4599 (3.0458)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:22:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:28:37 lr 0.000003	 wd 0.0500	time 0.7812 (0.8577)	loss 1.2910 (1.1264)	grad_norm 2.5640 (3.1346)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:24:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:27:03 lr 0.000003	 wd 0.0500	time 0.7870 (0.8535)	loss 0.8383 (1.1227)	grad_norm 2.3759 (3.1629)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:25:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:25:32 lr 0.000003	 wd 0.0500	time 0.7571 (0.8503)	loss 1.2572 (1.1210)	grad_norm 2.7256 (3.1525)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:26:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:24:03 lr 0.000003	 wd 0.0500	time 0.8093 (0.8483)	loss 1.2329 (1.1220)	grad_norm 2.2180 (3.0716)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:28:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:22:36 lr 0.000003	 wd 0.0500	time 0.7876 (0.8467)	loss 0.9021 (1.1266)	grad_norm 4.5356 (3.1326)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:29:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:21:09 lr 0.000003	 wd 0.0500	time 0.8245 (0.8454)	loss 0.8462 (1.1239)	grad_norm 2.2428 (3.1033)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:31:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:19:44 lr 0.000003	 wd 0.0500	time 0.7826 (0.8447)	loss 0.8576 (1.1225)	grad_norm 2.3291 (3.0858)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:32:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:18:18 lr 0.000002	 wd 0.0500	time 0.8033 (0.8440)	loss 0.9249 (1.1199)	grad_norm 3.5812 (3.0654)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:33:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:16:53 lr 0.000002	 wd 0.0500	time 0.7914 (0.8432)	loss 0.8519 (1.1178)	grad_norm 3.1338 (3.0475)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:35:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:15:28 lr 0.000002	 wd 0.0500	time 0.7808 (0.8424)	loss 1.4428 (1.1190)	grad_norm 2.1215 (3.0144)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:36:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:14:03 lr 0.000002	 wd 0.0500	time 0.8172 (0.8422)	loss 1.1985 (1.1156)	grad_norm 2.5287 (3.0186)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:38:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:12:39 lr 0.000002	 wd 0.0500	time 0.8036 (0.8418)	loss 1.2516 (1.1145)	grad_norm 2.6303 (3.0157)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:39:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:11:14 lr 0.000002	 wd 0.0500	time 0.7850 (0.8413)	loss 1.5636 (1.1158)	grad_norm 2.1572 (3.1367)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:40:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:09:50 lr 0.000002	 wd 0.0500	time 0.8175 (0.8409)	loss 1.1447 (1.1162)	grad_norm 2.0644 (3.1376)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:42:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:08:26 lr 0.000002	 wd 0.0500	time 0.7947 (0.8406)	loss 0.8249 (1.1169)	grad_norm 2.6178 (3.1391)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:43:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:07:01 lr 0.000002	 wd 0.0500	time 0.7849 (0.8402)	loss 1.2338 (1.1165)	grad_norm 2.4409 (3.1198)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:45:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:05:37 lr 0.000002	 wd 0.0500	time 0.7468 (0.8401)	loss 0.9783 (1.1162)	grad_norm 2.4781 (3.1182)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:46:27 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:04:13 lr 0.000002	 wd 0.0500	time 0.7850 (0.8397)	loss 0.9046 (1.1179)	grad_norm 5.2554 (3.0994)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:47:50 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:02:49 lr 0.000002	 wd 0.0500	time 0.7474 (0.8394)	loss 1.2705 (1.1189)	grad_norm 3.8314 (3.0829)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:49:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:01:25 lr 0.000002	 wd 0.0500	time 0.7779 (0.8392)	loss 1.3678 (1.1166)	grad_norm 3.2465 (3.0739)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:50:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.7834 (0.8389)	loss 0.7635 (1.1170)	grad_norm 2.1834 (3.0635)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:50:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 24 training takes 0:35:04
[2024-08-03 23:50:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 13.037 (13.037)	Loss 0.4673 (0.4673)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-03 23:51:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.846 Acc@5 98.090
[2024-08-03 23:51:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-03 23:51:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.85%
[2024-08-03 23:51:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-03 23:51:23 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-03 23:51:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:58:00 lr 0.000002	 wd 0.0500	time 11.4630 (11.4630)	loss 1.3005 (1.3005)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:52:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:37:29 lr 0.000002	 wd 0.0500	time 0.7613 (0.9365)	loss 1.1865 (1.1805)	grad_norm 1.9495 (2.9625)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:54:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:33:57 lr 0.000002	 wd 0.0500	time 0.7842 (0.8852)	loss 1.4190 (1.1483)	grad_norm 2.2548 (2.8358)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-03 23:55:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:31:51 lr 0.000002	 wd 0.0500	time 0.7480 (0.8679)	loss 1.1616 (1.1316)	grad_norm 2.1234 (2.8231)	loss_scale 2048.0000 (1051.2159)	mem 23200MB
[2024-08-03 23:57:08 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:30:05 lr 0.000002	 wd 0.0500	time 0.7783 (0.8591)	loss 1.1505 (1.1287)	grad_norm 2.0383 (2.9048)	loss_scale 2048.0000 (1299.7905)	mem 23200MB
[2024-08-03 23:58:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:28:31 lr 0.000002	 wd 0.0500	time 0.8287 (0.8549)	loss 1.2849 (1.1264)	grad_norm 4.2856 (2.8836)	loss_scale 2048.0000 (1449.1337)	mem 23200MB
[2024-08-03 23:59:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:26:59 lr 0.000002	 wd 0.0500	time 0.7822 (0.8516)	loss 0.8910 (1.1176)	grad_norm 2.7600 (2.9163)	loss_scale 2048.0000 (1548.7787)	mem 23200MB
[2024-08-04 00:01:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:25:29 lr 0.000002	 wd 0.0500	time 0.7775 (0.8490)	loss 0.7697 (1.1194)	grad_norm 2.3475 (2.9116)	loss_scale 2048.0000 (1619.9943)	mem 23200MB
[2024-08-04 00:02:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:24:01 lr 0.000002	 wd 0.0500	time 0.7723 (0.8469)	loss 1.3413 (1.1175)	grad_norm 2.4375 (2.9198)	loss_scale 2048.0000 (1673.4282)	mem 23200MB
[2024-08-04 00:04:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:22:34 lr 0.000002	 wd 0.0500	time 0.7821 (0.8455)	loss 1.2324 (1.1174)	grad_norm 2.0645 (2.9295)	loss_scale 2048.0000 (1715.0011)	mem 23200MB
[2024-08-04 00:05:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:21:07 lr 0.000002	 wd 0.0500	time 0.7833 (0.8441)	loss 1.5487 (1.1182)	grad_norm 7.9095 (2.9161)	loss_scale 2048.0000 (1748.2677)	mem 23200MB
[2024-08-04 00:06:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:19:42 lr 0.000002	 wd 0.0500	time 0.7862 (0.8432)	loss 1.4052 (1.1203)	grad_norm 2.5200 (2.9720)	loss_scale 2048.0000 (1775.4914)	mem 23200MB
[2024-08-04 00:08:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:18:16 lr 0.000002	 wd 0.0500	time 0.7774 (0.8424)	loss 0.9538 (1.1169)	grad_norm 2.2819 (2.9347)	loss_scale 2048.0000 (1798.1815)	mem 23200MB
[2024-08-04 00:09:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:16:51 lr 0.000002	 wd 0.0500	time 0.7899 (0.8418)	loss 0.9268 (1.1164)	grad_norm 5.3453 (2.9312)	loss_scale 2048.0000 (1817.3836)	mem 23200MB
[2024-08-04 00:11:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:15:26 lr 0.000002	 wd 0.0500	time 0.7831 (0.8412)	loss 0.9679 (1.1163)	grad_norm 5.0160 (2.9305)	loss_scale 2048.0000 (1833.8444)	mem 23200MB
[2024-08-04 00:12:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:14:02 lr 0.000002	 wd 0.0500	time 0.7829 (0.8410)	loss 0.7778 (1.1158)	grad_norm 2.4911 (2.9225)	loss_scale 2048.0000 (1848.1119)	mem 23200MB
[2024-08-04 00:13:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:12:38 lr 0.000002	 wd 0.0500	time 0.7673 (0.8406)	loss 0.8318 (1.1150)	grad_norm 3.5573 (2.9200)	loss_scale 2048.0000 (1860.5971)	mem 23200MB
[2024-08-04 00:15:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:11:13 lr 0.000002	 wd 0.0500	time 0.7813 (0.8401)	loss 0.7303 (1.1127)	grad_norm 2.0971 (2.9200)	loss_scale 2048.0000 (1871.6143)	mem 23200MB
[2024-08-04 00:16:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:09:49 lr 0.000002	 wd 0.0500	time 0.7792 (0.8398)	loss 0.8109 (1.1144)	grad_norm 2.0357 (2.8951)	loss_scale 2048.0000 (1881.4081)	mem 23200MB
[2024-08-04 00:17:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:08:25 lr 0.000002	 wd 0.0500	time 0.7839 (0.8395)	loss 1.4318 (1.1162)	grad_norm 2.6087 (2.8937)	loss_scale 2048.0000 (1890.1715)	mem 23200MB
[2024-08-04 00:19:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:07:01 lr 0.000002	 wd 0.0500	time 0.7776 (0.8391)	loss 0.9050 (1.1163)	grad_norm 2.0963 (2.8932)	loss_scale 2048.0000 (1898.0590)	mem 23200MB
[2024-08-04 00:20:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:05:37 lr 0.000002	 wd 0.0500	time 0.7822 (0.8389)	loss 1.2512 (1.1172)	grad_norm 2.4299 (2.9182)	loss_scale 2048.0000 (1905.1956)	mem 23200MB
[2024-08-04 00:22:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:04:13 lr 0.000001	 wd 0.0500	time 0.7858 (0.8388)	loss 1.0608 (1.1176)	grad_norm 2.6167 (2.9958)	loss_scale 2048.0000 (1911.6838)	mem 23200MB
[2024-08-04 00:23:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:02:49 lr 0.000001	 wd 0.0500	time 0.7846 (0.8386)	loss 1.3264 (1.1174)	grad_norm 2.0560 (3.0176)	loss_scale 2048.0000 (1917.6080)	mem 23200MB
[2024-08-04 00:24:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:01:25 lr 0.000001	 wd 0.0500	time 0.8276 (0.8385)	loss 1.4649 (1.1170)	grad_norm 3.0582 (3.0198)	loss_scale 2048.0000 (1923.0387)	mem 23200MB
[2024-08-04 00:26:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.7466 (0.8383)	loss 1.0026 (1.1157)	grad_norm 14.2696 (3.0591)	loss_scale 2048.0000 (1928.0352)	mem 23200MB
[2024-08-04 00:26:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 25 training takes 0:35:01
[2024-08-04 00:26:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.993 (12.993)	Loss 0.4905 (0.4905)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-04 00:27:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.792 Acc@5 98.060
[2024-08-04 00:27:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 00:27:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.85%
[2024-08-04 00:27:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:25:29 lr 0.000001	 wd 0.0500	time 12.1221 (12.1221)	loss 1.0523 (1.0523)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-04 00:28:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:37:56 lr 0.000001	 wd 0.0500	time 0.7808 (0.9479)	loss 1.2745 (1.1237)	grad_norm 2.5447 (2.9283)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-04 00:30:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:34:10 lr 0.000001	 wd 0.0500	time 0.7875 (0.8906)	loss 1.0155 (1.1200)	grad_norm 2.7263 (2.7284)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-04 00:31:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:31:59 lr 0.000001	 wd 0.0500	time 0.8076 (0.8719)	loss 0.8999 (1.1081)	grad_norm 2.1920 (2.7654)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-04 00:32:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:30:13 lr 0.000001	 wd 0.0500	time 0.8272 (0.8626)	loss 1.4237 (1.1080)	grad_norm 2.1947 (2.7064)	loss_scale 2048.0000 (2048.0000)	mem 23200MB
[2024-08-04 00:34:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:28:35 lr 0.000001	 wd 0.0500	time 0.7813 (0.8569)	loss 1.3134 (1.1162)	grad_norm 3.9250 (nan)	loss_scale 1024.0000 (1986.6826)	mem 23200MB
[2024-08-04 00:35:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:27:02 lr 0.000001	 wd 0.0500	time 0.7807 (0.8532)	loss 0.7659 (1.1188)	grad_norm 1.9221 (nan)	loss_scale 1024.0000 (1826.5025)	mem 23200MB
[2024-08-04 00:36:58 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:25:32 lr 0.000001	 wd 0.0500	time 0.7817 (0.8504)	loss 1.2064 (1.1148)	grad_norm 1.6777 (nan)	loss_scale 1024.0000 (1712.0228)	mem 23200MB
[2024-08-04 00:38:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:24:03 lr 0.000001	 wd 0.0500	time 0.7792 (0.8484)	loss 1.2888 (1.1192)	grad_norm 2.7556 (nan)	loss_scale 1024.0000 (1626.1273)	mem 23200MB
[2024-08-04 00:39:45 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:22:36 lr 0.000001	 wd 0.0500	time 0.8529 (0.8469)	loss 1.1641 (1.1196)	grad_norm 1.9852 (nan)	loss_scale 1024.0000 (1559.2986)	mem 23200MB
[2024-08-04 00:41:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:21:10 lr 0.000001	 wd 0.0500	time 0.7778 (0.8458)	loss 0.9977 (1.1188)	grad_norm 3.0564 (nan)	loss_scale 1024.0000 (1505.8222)	mem 23200MB
[2024-08-04 00:42:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:19:43 lr 0.000001	 wd 0.0500	time 0.7841 (0.8445)	loss 0.8317 (1.1175)	grad_norm 1.9596 (nan)	loss_scale 512.0000 (1451.8292)	mem 23200MB
[2024-08-04 00:43:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:18:18 lr 0.000001	 wd 0.0500	time 0.8060 (0.8435)	loss 0.9224 (1.1180)	grad_norm 1.9554 (nan)	loss_scale 512.0000 (1373.5754)	mem 23200MB
[2024-08-04 00:45:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:16:52 lr 0.000001	 wd 0.0500	time 0.7846 (0.8427)	loss 1.2314 (1.1200)	grad_norm 2.6317 (nan)	loss_scale 512.0000 (1307.3513)	mem 23200MB
[2024-08-04 00:46:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:15:27 lr 0.000001	 wd 0.0500	time 0.7840 (0.8420)	loss 1.3794 (1.1219)	grad_norm 2.0731 (nan)	loss_scale 512.0000 (1250.5810)	mem 23200MB
[2024-08-04 00:48:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:14:02 lr 0.000001	 wd 0.0500	time 0.7852 (0.8412)	loss 1.3179 (1.1235)	grad_norm 2.0745 (nan)	loss_scale 512.0000 (1201.3751)	mem 23200MB
[2024-08-04 00:49:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:12:38 lr 0.000001	 wd 0.0500	time 0.7781 (0.8406)	loss 1.1336 (1.1222)	grad_norm 3.0810 (nan)	loss_scale 512.0000 (1158.3161)	mem 23200MB
[2024-08-04 00:50:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:11:13 lr 0.000001	 wd 0.0500	time 0.7805 (0.8401)	loss 1.1041 (1.1190)	grad_norm 3.0934 (nan)	loss_scale 512.0000 (1120.3198)	mem 23200MB
[2024-08-04 00:52:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:09:49 lr 0.000001	 wd 0.0500	time 0.7791 (0.8398)	loss 0.7492 (1.1182)	grad_norm 2.4470 (nan)	loss_scale 512.0000 (1086.5430)	mem 23200MB
[2024-08-04 00:53:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:08:25 lr 0.000001	 wd 0.0500	time 0.7814 (0.8395)	loss 1.3000 (1.1198)	grad_norm 2.0684 (nan)	loss_scale 512.0000 (1056.3198)	mem 23200MB
[2024-08-04 00:55:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:07:01 lr 0.000001	 wd 0.0500	time 0.7854 (0.8392)	loss 1.1160 (1.1185)	grad_norm 2.2927 (nan)	loss_scale 512.0000 (1029.1174)	mem 23200MB
[2024-08-04 00:56:25 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:05:37 lr 0.000001	 wd 0.0500	time 0.7803 (0.8389)	loss 1.3123 (1.1180)	grad_norm 2.4648 (nan)	loss_scale 512.0000 (1004.5045)	mem 23200MB
[2024-08-04 00:57:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:04:13 lr 0.000001	 wd 0.0500	time 0.7801 (0.8386)	loss 1.2314 (1.1162)	grad_norm 2.6661 (nan)	loss_scale 512.0000 (982.1281)	mem 23200MB
[2024-08-04 00:59:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:02:49 lr 0.000001	 wd 0.0500	time 0.7830 (0.8384)	loss 1.3463 (1.1165)	grad_norm 2.1740 (nan)	loss_scale 512.0000 (961.6967)	mem 23200MB
[2024-08-04 01:00:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:01:25 lr 0.000001	 wd 0.0500	time 0.7951 (0.8381)	loss 0.7382 (1.1171)	grad_norm 2.2541 (nan)	loss_scale 512.0000 (942.9671)	mem 23200MB
[2024-08-04 01:01:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.7806 (0.8378)	loss 0.8099 (1.1172)	grad_norm 3.0987 (nan)	loss_scale 512.0000 (925.7353)	mem 23200MB
[2024-08-04 01:02:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 26 training takes 0:35:01
[2024-08-04 01:02:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.767 (12.767)	Loss 0.4778 (0.4778)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-04 01:02:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.846 Acc@5 98.080
[2024-08-04 01:02:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 01:02:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.85%
[2024-08-04 01:02:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-04 01:02:43 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-04 01:02:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:02:15 lr 0.000001	 wd 0.0500	time 11.5650 (11.5650)	loss 1.3411 (1.3411)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:04:18 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:37:26 lr 0.000001	 wd 0.0500	time 0.8261 (0.9352)	loss 1.2199 (1.1251)	grad_norm 1.9205 (2.7393)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:05:41 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:33:58 lr 0.000001	 wd 0.0500	time 0.7789 (0.8854)	loss 1.1900 (1.1305)	grad_norm 3.2856 (2.8003)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:07:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:31:53 lr 0.000001	 wd 0.0500	time 0.7786 (0.8691)	loss 1.2618 (1.1271)	grad_norm 2.6101 (2.7304)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:08:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:30:07 lr 0.000001	 wd 0.0500	time 0.7834 (0.8597)	loss 0.9279 (1.1261)	grad_norm 3.7583 (2.7804)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:09:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:28:30 lr 0.000001	 wd 0.0500	time 0.7804 (0.8543)	loss 0.7281 (1.1270)	grad_norm 2.3376 (3.1671)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:11:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:26:57 lr 0.000001	 wd 0.0500	time 0.7798 (0.8506)	loss 1.3400 (1.1264)	grad_norm 1.7788 (3.0983)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:12:38 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:25:28 lr 0.000001	 wd 0.0500	time 0.7822 (0.8483)	loss 0.7748 (1.1265)	grad_norm 2.0280 (3.0963)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:14:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:24:00 lr 0.000001	 wd 0.0500	time 0.7846 (0.8463)	loss 1.0768 (1.1263)	grad_norm 2.0475 (3.0882)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:15:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:22:33 lr 0.000001	 wd 0.0500	time 0.7836 (0.8447)	loss 1.2949 (1.1249)	grad_norm 1.7886 (3.0588)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:16:48 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:21:06 lr 0.000001	 wd 0.0500	time 0.8588 (0.8434)	loss 0.8951 (1.1208)	grad_norm 2.2462 (3.0859)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:18:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:19:41 lr 0.000001	 wd 0.0500	time 0.7824 (0.8424)	loss 1.1281 (1.1222)	grad_norm 2.5965 (3.0670)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:19:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:18:15 lr 0.000001	 wd 0.0500	time 0.7828 (0.8416)	loss 0.8107 (1.1219)	grad_norm 2.3441 (3.0478)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:20:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:16:50 lr 0.000001	 wd 0.0500	time 0.7844 (0.8409)	loss 0.7805 (1.1176)	grad_norm 3.1795 (3.0341)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:22:20 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:15:25 lr 0.000001	 wd 0.0500	time 0.7834 (0.8402)	loss 0.8680 (1.1190)	grad_norm 2.2557 (3.0332)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:23:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:14:01 lr 0.000001	 wd 0.0500	time 0.7796 (0.8397)	loss 1.4215 (1.1204)	grad_norm 2.4557 (3.0294)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:25:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:12:36 lr 0.000001	 wd 0.0500	time 0.8176 (0.8391)	loss 1.4744 (1.1218)	grad_norm 3.9795 (3.0172)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:26:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:11:12 lr 0.000001	 wd 0.0500	time 0.7805 (0.8387)	loss 1.3115 (1.1227)	grad_norm 2.5580 (2.9975)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:27:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:09:48 lr 0.000001	 wd 0.0500	time 0.7965 (0.8383)	loss 1.2974 (1.1218)	grad_norm 2.3652 (3.0196)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:29:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:08:24 lr 0.000001	 wd 0.0500	time 0.8008 (0.8378)	loss 1.3183 (1.1224)	grad_norm 2.2819 (3.0125)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:30:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:07:00 lr 0.000001	 wd 0.0500	time 0.7825 (0.8375)	loss 1.3353 (1.1228)	grad_norm 2.0351 (3.0232)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:32:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:05:36 lr 0.000001	 wd 0.0500	time 0.7659 (0.8373)	loss 1.4863 (1.1226)	grad_norm 4.1754 (3.0509)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:33:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:04:12 lr 0.000001	 wd 0.0500	time 0.7833 (0.8371)	loss 1.1879 (1.1233)	grad_norm 2.5654 (3.0473)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:34:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:02:49 lr 0.000001	 wd 0.0500	time 0.7491 (0.8370)	loss 1.2880 (1.1233)	grad_norm 2.3799 (3.0756)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:36:13 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:01:25 lr 0.000001	 wd 0.0500	time 0.8182 (0.8369)	loss 1.3283 (1.1246)	grad_norm 3.0217 (3.0712)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:37:36 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.7470 (0.8366)	loss 1.1195 (1.1237)	grad_norm 2.1864 (3.0556)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:37:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 27 training takes 0:34:58
[2024-08-04 01:37:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.886 (11.886)	Loss 0.4639 (0.4639)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-04 01:38:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.802 Acc@5 98.084
[2024-08-04 01:38:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 01:38:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.85%
[2024-08-04 01:38:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][0/2502]	eta 9:35:24 lr 0.000001	 wd 0.0500	time 13.7987 (13.7987)	loss 0.7449 (0.7449)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 23200MB
[2024-08-04 01:39:56 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:38:20 lr 0.000000	 wd 0.0500	time 0.7870 (0.9578)	loss 0.7484 (1.1448)	grad_norm 5.9977 (2.8464)	loss_scale 1024.0000 (643.8020)	mem 23200MB
[2024-08-04 01:41:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:34:20 lr 0.000000	 wd 0.0500	time 0.7768 (0.8950)	loss 1.3450 (1.1412)	grad_norm 2.3586 (3.0651)	loss_scale 1024.0000 (832.9552)	mem 23200MB
[2024-08-04 01:42:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:32:05 lr 0.000000	 wd 0.0500	time 0.7820 (0.8744)	loss 1.3666 (1.1329)	grad_norm 3.0206 (2.9645)	loss_scale 1024.0000 (896.4252)	mem 23200MB
[2024-08-04 01:44:06 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:30:16 lr 0.000000	 wd 0.0500	time 0.7795 (0.8642)	loss 0.7300 (1.1245)	grad_norm 2.3461 (3.0861)	loss_scale 1024.0000 (928.2394)	mem 23200MB
[2024-08-04 01:45:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:28:37 lr 0.000000	 wd 0.0500	time 0.8070 (0.8581)	loss 0.7963 (1.1295)	grad_norm 2.1365 (3.1396)	loss_scale 1024.0000 (947.3533)	mem 23200MB
[2024-08-04 01:46:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:27:05 lr 0.000000	 wd 0.0500	time 0.7744 (0.8545)	loss 0.8632 (1.1222)	grad_norm 2.3684 (3.1556)	loss_scale 1024.0000 (960.1065)	mem 23200MB
[2024-08-04 01:48:16 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:25:34 lr 0.000000	 wd 0.0500	time 0.7843 (0.8516)	loss 1.2038 (1.1192)	grad_norm 2.7657 (3.2229)	loss_scale 1024.0000 (969.2211)	mem 23200MB
[2024-08-04 01:49:39 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:24:05 lr 0.000000	 wd 0.0500	time 0.8446 (0.8493)	loss 0.8046 (1.1206)	grad_norm 2.2613 (3.1668)	loss_scale 1024.0000 (976.0599)	mem 23200MB
[2024-08-04 01:51:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:22:37 lr 0.000000	 wd 0.0500	time 0.8189 (0.8471)	loss 0.7565 (1.1173)	grad_norm 2.6896 (3.1520)	loss_scale 1024.0000 (981.3807)	mem 23200MB
[2024-08-04 01:52:26 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:21:10 lr 0.000000	 wd 0.0500	time 0.7829 (0.8456)	loss 1.0029 (1.1183)	grad_norm 2.4996 (3.1478)	loss_scale 1024.0000 (985.6384)	mem 23200MB
[2024-08-04 01:53:49 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:19:43 lr 0.000000	 wd 0.0500	time 0.7829 (0.8444)	loss 1.1468 (1.1194)	grad_norm 1.9279 (3.1141)	loss_scale 1024.0000 (989.1226)	mem 23200MB
[2024-08-04 01:55:12 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:18:18 lr 0.000000	 wd 0.0500	time 0.7813 (0.8436)	loss 1.3317 (1.1202)	grad_norm 4.1045 (3.0868)	loss_scale 1024.0000 (992.0266)	mem 23200MB
[2024-08-04 01:56:35 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:16:52 lr 0.000000	 wd 0.0500	time 0.7857 (0.8426)	loss 1.3379 (1.1220)	grad_norm 2.0490 (3.1564)	loss_scale 1024.0000 (994.4842)	mem 23200MB
[2024-08-04 01:57:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:15:27 lr 0.000000	 wd 0.0500	time 0.8246 (0.8420)	loss 0.8726 (1.1207)	grad_norm 1.5735 (3.1408)	loss_scale 1024.0000 (996.5910)	mem 23200MB
[2024-08-04 01:59:22 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:14:03 lr 0.000000	 wd 0.0500	time 0.7781 (0.8413)	loss 1.2027 (1.1205)	grad_norm 2.0639 (3.1354)	loss_scale 1024.0000 (998.4171)	mem 23200MB
[2024-08-04 02:00:46 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:12:38 lr 0.000000	 wd 0.0500	time 0.8520 (0.8410)	loss 1.3824 (1.1228)	grad_norm 8.0769 (3.1421)	loss_scale 1024.0000 (1000.0150)	mem 23200MB
[2024-08-04 02:02:09 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:11:14 lr 0.000000	 wd 0.0500	time 0.7763 (0.8406)	loss 1.3278 (1.1227)	grad_norm 2.3014 (3.1779)	loss_scale 1024.0000 (1001.4250)	mem 23200MB
[2024-08-04 02:03:32 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:09:49 lr 0.000000	 wd 0.0500	time 0.7463 (0.8402)	loss 0.7244 (1.1223)	grad_norm 2.5845 (3.1732)	loss_scale 1024.0000 (1002.6785)	mem 23200MB
[2024-08-04 02:04:55 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:08:25 lr 0.000000	 wd 0.0500	time 0.7806 (0.8397)	loss 0.9664 (1.1197)	grad_norm 2.4074 (3.1447)	loss_scale 1024.0000 (1003.8001)	mem 23200MB
[2024-08-04 02:06:19 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:07:01 lr 0.000000	 wd 0.0500	time 0.8201 (0.8394)	loss 1.1244 (1.1197)	grad_norm 4.6295 (3.1410)	loss_scale 1024.0000 (1004.8096)	mem 23200MB
[2024-08-04 02:07:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:05:37 lr 0.000000	 wd 0.0500	time 0.7828 (0.8391)	loss 1.0157 (1.1202)	grad_norm 4.8976 (3.1317)	loss_scale 1024.0000 (1005.7230)	mem 23200MB
[2024-08-04 02:09:05 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:04:13 lr 0.000000	 wd 0.0500	time 0.7789 (0.8388)	loss 1.2711 (1.1195)	grad_norm 3.2106 (3.1190)	loss_scale 1024.0000 (1006.5534)	mem 23200MB
[2024-08-04 02:10:29 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:02:49 lr 0.000000	 wd 0.0500	time 0.7715 (0.8386)	loss 1.2680 (1.1194)	grad_norm 2.0850 (3.1040)	loss_scale 1024.0000 (1007.3116)	mem 23200MB
[2024-08-04 02:11:52 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:01:25 lr 0.000000	 wd 0.0500	time 0.7983 (0.8383)	loss 1.5039 (1.1185)	grad_norm 2.7052 (3.1063)	loss_scale 1024.0000 (1008.0067)	mem 23200MB
[2024-08-04 02:13:15 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.7808 (0.8381)	loss 1.0366 (1.1176)	grad_norm 1.8783 (3.0879)	loss_scale 1024.0000 (1008.6461)	mem 23200MB
[2024-08-04 02:13:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 28 training takes 0:35:01
[2024-08-04 02:13:33 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 12.137 (12.137)	Loss 0.4729 (0.4729)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-04 02:13:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.846 Acc@5 98.088
[2024-08-04 02:13:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.8%
[2024-08-04 02:13:59 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.85%
[2024-08-04 02:14:11 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:27:58 lr 0.000000	 wd 0.0500	time 12.1815 (12.1815)	loss 1.3164 (1.3164)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:15:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:37:57 lr 0.000000	 wd 0.0500	time 0.7883 (0.9483)	loss 1.3194 (1.1328)	grad_norm 2.7791 (2.7488)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:16:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:34:09 lr 0.000000	 wd 0.0500	time 0.7879 (0.8904)	loss 0.7946 (1.1339)	grad_norm 2.6144 (2.7026)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:18:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:31:58 lr 0.000000	 wd 0.0500	time 0.7812 (0.8714)	loss 0.6893 (1.1228)	grad_norm 2.3758 (2.7745)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:19:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:30:11 lr 0.000000	 wd 0.0500	time 0.7452 (0.8616)	loss 1.0119 (1.1187)	grad_norm 2.5524 (2.8381)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:21:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:28:33 lr 0.000000	 wd 0.0500	time 0.8360 (0.8558)	loss 1.1661 (1.1141)	grad_norm 2.9634 (2.8860)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:22:31 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:27:00 lr 0.000000	 wd 0.0500	time 0.7813 (0.8522)	loss 0.6953 (1.1162)	grad_norm 2.2569 (3.0619)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:23:54 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:25:30 lr 0.000000	 wd 0.0500	time 0.7804 (0.8494)	loss 1.1612 (1.1159)	grad_norm 4.0050 (3.2143)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:25:17 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:24:01 lr 0.000000	 wd 0.0500	time 0.7801 (0.8472)	loss 0.8387 (1.1172)	grad_norm 2.2402 (3.1611)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:26:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:22:34 lr 0.000000	 wd 0.0500	time 0.7769 (0.8455)	loss 1.3999 (1.1179)	grad_norm 3.2317 (3.1141)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:28:04 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:21:08 lr 0.000000	 wd 0.0500	time 0.7851 (0.8446)	loss 1.2451 (1.1184)	grad_norm 2.6648 (3.0870)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:29:28 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:19:43 lr 0.000000	 wd 0.0500	time 0.7843 (0.8439)	loss 1.2462 (1.1164)	grad_norm 2.2860 (3.0481)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:30:51 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:18:17 lr 0.000000	 wd 0.0500	time 0.7859 (0.8430)	loss 0.9461 (1.1154)	grad_norm 4.3041 (3.0246)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:32:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:16:52 lr 0.000000	 wd 0.0500	time 0.7823 (0.8421)	loss 1.1066 (1.1191)	grad_norm 2.8153 (3.0593)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:33:37 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:15:27 lr 0.000000	 wd 0.0500	time 0.7694 (0.8415)	loss 1.0804 (1.1142)	grad_norm 3.2707 (3.0949)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:35:00 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:14:02 lr 0.000000	 wd 0.0500	time 0.7859 (0.8407)	loss 0.9658 (1.1134)	grad_norm 1.9618 (3.0685)	loss_scale 1024.0000 (1024.0000)	mem 23200MB
[2024-08-04 02:36:24 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:12:37 lr 0.000000	 wd 0.0500	time 0.7718 (0.8402)	loss 0.7733 (1.1162)	grad_norm 7.6037 (3.0582)	loss_scale 2048.0000 (1041.9088)	mem 23200MB
[2024-08-04 02:37:47 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:11:13 lr 0.000000	 wd 0.0500	time 0.7829 (0.8399)	loss 1.1850 (1.1181)	grad_norm 2.2338 (3.0361)	loss_scale 2048.0000 (1101.0558)	mem 23200MB
[2024-08-04 02:39:10 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:09:49 lr 0.000000	 wd 0.0500	time 0.7762 (0.8395)	loss 1.2389 (1.1172)	grad_norm 3.8972 (3.0512)	loss_scale 2048.0000 (1153.6346)	mem 23200MB
[2024-08-04 02:40:34 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:08:25 lr 0.000000	 wd 0.0500	time 0.7819 (0.8392)	loss 0.8566 (1.1174)	grad_norm 3.9985 (3.0576)	loss_scale 2048.0000 (1200.6817)	mem 23200MB
[2024-08-04 02:41:57 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:07:01 lr 0.000000	 wd 0.0500	time 0.7745 (0.8391)	loss 0.8901 (1.1194)	grad_norm 1.8526 (3.0545)	loss_scale 2048.0000 (1243.0265)	mem 23200MB
[2024-08-04 02:43:21 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:05:37 lr 0.000000	 wd 0.0500	time 0.7793 (0.8388)	loss 0.8178 (1.1206)	grad_norm 2.0534 (3.0553)	loss_scale 2048.0000 (1281.3403)	mem 23200MB
[2024-08-04 02:44:44 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:04:13 lr 0.000000	 wd 0.0500	time 0.7866 (0.8385)	loss 0.8334 (1.1211)	grad_norm 2.2529 (3.0409)	loss_scale 2048.0000 (1316.1726)	mem 23200MB
[2024-08-04 02:46:07 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:02:49 lr 0.000000	 wd 0.0500	time 0.7848 (0.8381)	loss 0.7022 (1.1183)	grad_norm 2.4955 (3.0292)	loss_scale 2048.0000 (1347.9774)	mem 23200MB
[2024-08-04 02:47:30 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:01:25 lr 0.000000	 wd 0.0500	time 0.7781 (0.8379)	loss 1.2048 (1.1185)	grad_norm 3.1113 (3.0408)	loss_scale 2048.0000 (1377.1329)	mem 23200MB
[2024-08-04 02:48:53 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.7796 (0.8376)	loss 1.1998 (1.1179)	grad_norm 2.0955 (3.0409)	loss_scale 2048.0000 (1403.9568)	mem 23200MB
[2024-08-04 02:49:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 249): INFO EPOCH 29 training takes 0:35:02
[2024-08-04 02:49:01 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_29.pth saving......
[2024-08-04 02:49:02 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_29.pth saved !!!
[2024-08-04 02:49:14 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 289): INFO Test: [0/98]	Time 11.770 (11.770)	Loss 0.4614 (0.4614)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 23200MB
[2024-08-04 02:49:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 296): INFO  * Acc@1 86.872 Acc@5 98.098
[2024-08-04 02:49:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.9%
[2024-08-04 02:49:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 182): INFO Max accuracy: 86.87%
[2024-08-04 02:49:40 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-04 02:49:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_sequence2/diffusion_ft_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-04 02:49:42 smt_diffusion_finetune_large_224_22kto1k_step_sequence2] (main.py 189): INFO Training time 17:47:08
