[2024-07-31 09:51:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/config.json
[2024-07-31 09:51:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /media/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: smt_diffusion_finetune_large_224_22kto1k_step_stage0
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: stage0
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0
PRINT_FREQ: 100
SAVE_FREQ: 15
SEED: 0
TAG: diffusion_ft_smt_l_step_stage0
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-31 09:51:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/smt/diffusion_ft_smt_large_224_22kto1k_step_stage0.yaml", "opts": null, "batch_size": 64, "data_path": "/media/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_smt_l_step_stage0", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-31 09:51:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 108): INFO Creating model:smt_diffusion_finetune/smt_diffusion_finetune_large_224_22kto1k_step_stage0
[2024-07-31 09:51:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 110): INFO SMT_Diffusion_Finetune(
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-31 09:51:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 113): INFO number of params: 1822312
[2024-07-31 09:51:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0, ignoring auto resume
[2024-07-31 09:51:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth for fine-tuning......
[2024-07-31 09:51:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 112): INFO loading ImageNet-22K weight to ImageNet-1K ......
[2024-07-31 09:51:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-31 09:51:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/smt-l/smt_large_224_22k.pth'
[2024-07-31 09:51:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 13.813 (13.813)	Loss 0.4014 (0.4014)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 2315MB
[2024-07-31 09:51:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 84.492 Acc@5 97.134
[2024-07-31 09:51:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 162): INFO Accuracy of the network on the 50000 test images: 84.5%
[2024-07-31 09:51:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 168): INFO Start training
[2024-07-31 09:52:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][0/2502]	eta 9:06:46 lr 0.000000	 wd 0.0500	time 13.1120 (13.1120)	loss 1.6913 (1.6913)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 19076MB
[2024-07-31 09:52:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:25:14 lr 0.000000	 wd 0.0500	time 0.4993 (0.6305)	loss 1.5046 (1.3251)	grad_norm 2.7879 (nan)	loss_scale 8192.0000 (16870.6535)	mem 19076MB
[2024-07-31 09:53:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:21:49 lr 0.000000	 wd 0.0500	time 0.4987 (0.5689)	loss 1.1561 (1.3206)	grad_norm 1.6295 (nan)	loss_scale 8192.0000 (12552.9154)	mem 19076MB
[2024-07-31 09:54:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:20:07 lr 0.000000	 wd 0.0500	time 0.5015 (0.5484)	loss 0.9220 (1.2758)	grad_norm 2.7560 (nan)	loss_scale 8192.0000 (11104.1063)	mem 19076MB
[2024-07-31 09:55:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:18:51 lr 0.000001	 wd 0.0500	time 0.4988 (0.5383)	loss 1.1810 (1.2852)	grad_norm 2.3480 (nan)	loss_scale 4096.0000 (9540.3092)	mem 19076MB
[2024-07-31 09:56:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:17:45 lr 0.000001	 wd 0.0500	time 0.5037 (0.5324)	loss 1.2996 (1.2874)	grad_norm 2.2495 (nan)	loss_scale 4096.0000 (8453.6208)	mem 19076MB
[2024-07-31 09:57:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:16:45 lr 0.000001	 wd 0.0500	time 0.5030 (0.5286)	loss 1.4597 (1.2849)	grad_norm 3.3990 (nan)	loss_scale 2048.0000 (7428.6855)	mem 19076MB
[2024-07-31 09:58:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:15:47 lr 0.000001	 wd 0.0500	time 0.5010 (0.5260)	loss 1.4329 (1.2880)	grad_norm 8.5030 (nan)	loss_scale 2048.0000 (6661.1127)	mem 19076MB
[2024-07-31 09:58:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:14:51 lr 0.000001	 wd 0.0500	time 0.5009 (0.5240)	loss 1.2042 (1.2857)	grad_norm 1.9972 (nan)	loss_scale 2048.0000 (6085.1935)	mem 19076MB
[2024-07-31 09:59:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:13:56 lr 0.000001	 wd 0.0500	time 0.5033 (0.5225)	loss 1.4442 (1.2858)	grad_norm 2.3886 (nan)	loss_scale 2048.0000 (5637.1143)	mem 19076MB
[2024-07-31 10:00:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:13:02 lr 0.000002	 wd 0.0500	time 0.5029 (0.5213)	loss 1.6135 (1.2833)	grad_norm 2.3127 (nan)	loss_scale 2048.0000 (5278.5614)	mem 19076MB
[2024-07-31 10:01:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:12:09 lr 0.000002	 wd 0.0500	time 0.5001 (0.5203)	loss 1.4708 (1.2852)	grad_norm 2.3043 (nan)	loss_scale 2048.0000 (4985.1408)	mem 19076MB
[2024-07-31 10:02:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:11:16 lr 0.000002	 wd 0.0500	time 0.5044 (0.5196)	loss 1.0539 (1.2846)	grad_norm 2.5033 (nan)	loss_scale 2048.0000 (4740.5828)	mem 19076MB
[2024-07-31 10:03:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:10:23 lr 0.000002	 wd 0.0500	time 0.5003 (0.5189)	loss 1.3017 (1.2860)	grad_norm 2.0867 (nan)	loss_scale 2048.0000 (4533.6203)	mem 19076MB
[2024-07-31 10:04:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:09:31 lr 0.000002	 wd 0.0500	time 0.5025 (0.5183)	loss 1.6088 (1.2874)	grad_norm 2.5680 (nan)	loss_scale 2048.0000 (4356.2027)	mem 19076MB
[2024-07-31 10:04:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:08:38 lr 0.000002	 wd 0.0500	time 0.4985 (0.5178)	loss 1.1504 (1.2905)	grad_norm 1.8534 (nan)	loss_scale 2048.0000 (4202.4250)	mem 19076MB
[2024-07-31 10:05:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:07:46 lr 0.000003	 wd 0.0500	time 0.5022 (0.5173)	loss 1.3110 (1.2877)	grad_norm 2.5689 (nan)	loss_scale 2048.0000 (4067.8576)	mem 19076MB
[2024-07-31 10:06:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:06:54 lr 0.000003	 wd 0.0500	time 0.5046 (0.5170)	loss 1.0830 (1.2870)	grad_norm 11.9243 (nan)	loss_scale 2048.0000 (3949.1123)	mem 19076MB
[2024-07-31 10:07:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:06:02 lr 0.000003	 wd 0.0500	time 0.5010 (0.5166)	loss 1.1824 (1.2866)	grad_norm 2.0447 (nan)	loss_scale 2048.0000 (3843.5536)	mem 19076MB
[2024-07-31 10:08:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:05:10 lr 0.000003	 wd 0.0500	time 0.5043 (0.5163)	loss 1.6635 (1.2858)	grad_norm 1.9607 (nan)	loss_scale 2048.0000 (3749.1005)	mem 19076MB
[2024-07-31 10:09:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:04:19 lr 0.000003	 wd 0.0500	time 0.5029 (0.5160)	loss 0.8935 (1.2845)	grad_norm 2.1076 (nan)	loss_scale 2048.0000 (3664.0880)	mem 19076MB
[2024-07-31 10:09:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:03:27 lr 0.000003	 wd 0.0500	time 0.5058 (0.5158)	loss 1.1054 (1.2829)	grad_norm 2.3027 (nan)	loss_scale 2048.0000 (3587.1680)	mem 19076MB
[2024-07-31 10:10:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:02:35 lr 0.000004	 wd 0.0500	time 0.4894 (0.5155)	loss 1.6667 (1.2817)	grad_norm 2.5274 (nan)	loss_scale 2048.0000 (3517.2376)	mem 19076MB
[2024-07-31 10:11:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:44 lr 0.000004	 wd 0.0500	time 0.5018 (0.5153)	loss 1.5282 (1.2817)	grad_norm 2.6172 (nan)	loss_scale 2048.0000 (3453.3855)	mem 19076MB
[2024-07-31 10:12:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:52 lr 0.000004	 wd 0.0500	time 0.5032 (0.5152)	loss 1.4778 (1.2810)	grad_norm 2.7192 (nan)	loss_scale 2048.0000 (3394.8521)	mem 19076MB
[2024-07-31 10:13:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.5026 (0.5150)	loss 1.0937 (1.2789)	grad_norm 2.2071 (nan)	loss_scale 2048.0000 (3340.9996)	mem 19076MB
[2024-07-31 10:13:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 0 training takes 0:21:30
[2024-07-31 10:13:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_0.pth saving......
[2024-07-31 10:13:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_0.pth saved !!!
[2024-07-31 10:13:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.125 (11.125)	Loss 0.3872 (0.3872)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 10:13:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 84.706 Acc@5 97.216
[2024-07-31 10:13:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.7%
[2024-07-31 10:13:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 84.71%
[2024-07-31 10:13:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 10:13:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 10:14:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:47:31 lr 0.000004	 wd 0.0500	time 11.2116 (11.2116)	loss 1.4460 (1.4460)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 10:15:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:24:33 lr 0.000004	 wd 0.0500	time 0.4969 (0.6133)	loss 1.3596 (1.2581)	grad_norm 2.4718 (3.7871)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 10:15:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:21:29 lr 0.000004	 wd 0.0500	time 0.4919 (0.5602)	loss 0.8888 (1.2603)	grad_norm 2.1039 (nan)	loss_scale 1024.0000 (1671.0050)	mem 19076MB
[2024-07-31 10:16:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:19:55 lr 0.000004	 wd 0.0500	time 0.5010 (0.5429)	loss 1.0094 (1.2663)	grad_norm 2.7774 (nan)	loss_scale 1024.0000 (1456.0532)	mem 19076MB
[2024-07-31 10:17:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:18:43 lr 0.000005	 wd 0.0500	time 0.5030 (0.5344)	loss 1.0020 (1.2830)	grad_norm 1.9676 (nan)	loss_scale 1024.0000 (1348.3092)	mem 19076MB
[2024-07-31 10:18:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:17:39 lr 0.000005	 wd 0.0500	time 0.5030 (0.5294)	loss 1.6852 (1.2860)	grad_norm 5.6680 (nan)	loss_scale 1024.0000 (1283.5768)	mem 19076MB
[2024-07-31 10:19:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:16:40 lr 0.000005	 wd 0.0500	time 0.5030 (0.5262)	loss 1.5863 (1.2846)	grad_norm 4.1130 (nan)	loss_scale 1024.0000 (1240.3860)	mem 19076MB
[2024-07-31 10:20:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:15:44 lr 0.000005	 wd 0.0500	time 0.4956 (0.5240)	loss 0.9386 (1.2797)	grad_norm 3.1950 (nan)	loss_scale 1024.0000 (1209.5178)	mem 19076MB
[2024-07-31 10:20:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:14:48 lr 0.000005	 wd 0.0500	time 0.5046 (0.5223)	loss 1.7348 (1.2790)	grad_norm 5.1459 (nan)	loss_scale 1024.0000 (1186.3571)	mem 19076MB
[2024-07-31 10:21:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:13:54 lr 0.000005	 wd 0.0500	time 0.5036 (0.5210)	loss 1.2762 (1.2766)	grad_norm 2.6564 (nan)	loss_scale 1024.0000 (1168.3374)	mem 19076MB
[2024-07-31 10:22:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:13:01 lr 0.000006	 wd 0.0500	time 0.4981 (0.5200)	loss 1.5790 (1.2728)	grad_norm 2.8436 (nan)	loss_scale 1024.0000 (1153.9181)	mem 19076MB
[2024-07-31 10:23:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:12:07 lr 0.000006	 wd 0.0500	time 0.5014 (0.5192)	loss 1.6591 (1.2709)	grad_norm 4.8198 (nan)	loss_scale 1024.0000 (1142.1181)	mem 19076MB
[2024-07-31 10:24:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:11:15 lr 0.000006	 wd 0.0500	time 0.5027 (0.5185)	loss 1.0337 (1.2686)	grad_norm 2.2053 (nan)	loss_scale 1024.0000 (1132.2831)	mem 19076MB
[2024-07-31 10:25:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:10:22 lr 0.000006	 wd 0.0500	time 0.4979 (0.5179)	loss 1.3742 (1.2698)	grad_norm 2.7036 (nan)	loss_scale 1024.0000 (1123.9600)	mem 19076MB
[2024-07-31 10:26:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:09:30 lr 0.000006	 wd 0.0500	time 0.5011 (0.5174)	loss 0.8700 (1.2694)	grad_norm 2.3786 (nan)	loss_scale 1024.0000 (1116.8251)	mem 19076MB
[2024-07-31 10:26:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:08:37 lr 0.000006	 wd 0.0500	time 0.5033 (0.5170)	loss 1.6442 (1.2691)	grad_norm 2.2779 (nan)	loss_scale 1024.0000 (1110.6409)	mem 19076MB
[2024-07-31 10:27:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:07:45 lr 0.000007	 wd 0.0500	time 0.5050 (0.5166)	loss 1.4517 (1.2695)	grad_norm 2.7248 (nan)	loss_scale 1024.0000 (1105.2292)	mem 19076MB
[2024-07-31 10:28:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:06:54 lr 0.000007	 wd 0.0500	time 0.5005 (0.5163)	loss 0.8948 (1.2698)	grad_norm 2.2053 (nan)	loss_scale 1024.0000 (1100.4539)	mem 19076MB
[2024-07-31 10:29:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:06:02 lr 0.000007	 wd 0.0500	time 0.5076 (0.5160)	loss 1.7876 (1.2688)	grad_norm 2.3542 (nan)	loss_scale 1024.0000 (1096.2088)	mem 19076MB
[2024-07-31 10:30:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:05:10 lr 0.000007	 wd 0.0500	time 0.4941 (0.5157)	loss 1.6083 (1.2702)	grad_norm 2.4429 (nan)	loss_scale 1024.0000 (1092.4103)	mem 19076MB
[2024-07-31 10:31:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:04:18 lr 0.000007	 wd 0.0500	time 0.5018 (0.5155)	loss 1.4435 (1.2704)	grad_norm 3.0947 (nan)	loss_scale 1024.0000 (1088.9915)	mem 19076MB
[2024-07-31 10:32:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:03:27 lr 0.000007	 wd 0.0500	time 0.5060 (0.5153)	loss 1.5459 (1.2725)	grad_norm 3.0813 (nan)	loss_scale 1024.0000 (1085.8981)	mem 19076MB
[2024-07-31 10:32:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:02:35 lr 0.000008	 wd 0.0500	time 0.5047 (0.5151)	loss 1.2473 (1.2727)	grad_norm 3.8863 (nan)	loss_scale 1024.0000 (1083.0859)	mem 19076MB
[2024-07-31 10:33:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:44 lr 0.000008	 wd 0.0500	time 0.5006 (0.5149)	loss 0.8935 (1.2740)	grad_norm 2.2099 (nan)	loss_scale 1024.0000 (1080.5180)	mem 19076MB
[2024-07-31 10:34:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:52 lr 0.000008	 wd 0.0500	time 0.4994 (0.5148)	loss 1.3278 (1.2735)	grad_norm 3.4401 (nan)	loss_scale 1024.0000 (1078.1641)	mem 19076MB
[2024-07-31 10:35:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.5057 (0.5146)	loss 0.9859 (1.2732)	grad_norm 1.9226 (nan)	loss_scale 1024.0000 (1075.9984)	mem 19076MB
[2024-07-31 10:35:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 1 training takes 0:21:29
[2024-07-31 10:35:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.339 (12.339)	Loss 0.3977 (0.3977)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 10:36:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 84.906 Acc@5 97.286
[2024-07-31 10:36:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 84.9%
[2024-07-31 10:36:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 84.91%
[2024-07-31 10:36:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 10:36:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 10:36:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][0/2502]	eta 8:14:04 lr 0.000008	 wd 0.0500	time 11.8482 (11.8482)	loss 1.2290 (1.2290)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:37:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:24:45 lr 0.000008	 wd 0.0500	time 0.5009 (0.6185)	loss 1.6407 (1.2667)	grad_norm 1.6747 (2.8779)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:37:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:21:36 lr 0.000008	 wd 0.0500	time 0.4954 (0.5630)	loss 1.3859 (1.2556)	grad_norm 1.7775 (2.9146)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:38:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:19:59 lr 0.000008	 wd 0.0500	time 0.4998 (0.5447)	loss 1.3436 (1.2427)	grad_norm 3.1857 (3.2477)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:39:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:18:45 lr 0.000009	 wd 0.0500	time 0.5005 (0.5356)	loss 1.4604 (1.2462)	grad_norm 4.7201 (3.0655)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:40:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:17:41 lr 0.000009	 wd 0.0500	time 0.5029 (0.5304)	loss 1.4500 (1.2434)	grad_norm 1.8379 (3.0133)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:41:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:16:42 lr 0.000009	 wd 0.0500	time 0.5003 (0.5270)	loss 1.3282 (1.2423)	grad_norm 2.8893 (2.9941)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:42:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:15:45 lr 0.000009	 wd 0.0500	time 0.5037 (0.5246)	loss 1.4634 (1.2465)	grad_norm 3.3032 (2.9699)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:43:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:14:49 lr 0.000009	 wd 0.0500	time 0.5017 (0.5228)	loss 1.3383 (1.2495)	grad_norm 1.7595 (2.9612)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:43:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:13:55 lr 0.000009	 wd 0.0500	time 0.5069 (0.5214)	loss 0.8687 (1.2537)	grad_norm 4.7831 (2.9119)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:44:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:13:01 lr 0.000010	 wd 0.0500	time 0.5025 (0.5203)	loss 0.9265 (1.2508)	grad_norm 10.4815 (2.9174)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:45:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:12:08 lr 0.000010	 wd 0.0500	time 0.4907 (0.5195)	loss 1.5162 (1.2494)	grad_norm 2.4046 (2.9451)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:46:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:11:15 lr 0.000010	 wd 0.0500	time 0.5028 (0.5188)	loss 1.0783 (1.2497)	grad_norm 2.8810 (2.9349)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:47:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:10:22 lr 0.000010	 wd 0.0500	time 0.5009 (0.5181)	loss 1.5718 (1.2501)	grad_norm 1.6174 (2.9137)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:48:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:09:30 lr 0.000010	 wd 0.0500	time 0.5035 (0.5176)	loss 1.3030 (1.2514)	grad_norm 8.9872 (2.9754)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:48:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:08:38 lr 0.000010	 wd 0.0500	time 0.5037 (0.5171)	loss 1.5206 (1.2519)	grad_norm 1.8231 (3.0019)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:49:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:07:46 lr 0.000011	 wd 0.0500	time 0.5049 (0.5167)	loss 1.0345 (1.2517)	grad_norm 1.8967 (3.0448)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 10:50:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:06:54 lr 0.000011	 wd 0.0500	time 0.4991 (0.5164)	loss 1.3399 (1.2514)	grad_norm 2.6519 (3.0593)	loss_scale 2048.0000 (1069.7519)	mem 19076MB
[2024-07-31 10:51:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:06:02 lr 0.000011	 wd 0.0500	time 0.5086 (0.5161)	loss 1.2504 (1.2522)	grad_norm 3.1664 (3.0374)	loss_scale 2048.0000 (1124.0689)	mem 19076MB
[2024-07-31 10:52:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:05:10 lr 0.000011	 wd 0.0500	time 0.4920 (0.5158)	loss 1.4432 (1.2514)	grad_norm 2.1699 (3.0191)	loss_scale 2048.0000 (1172.6712)	mem 19076MB
[2024-07-31 10:53:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:04:18 lr 0.000011	 wd 0.0500	time 0.5045 (0.5155)	loss 1.0549 (1.2514)	grad_norm 1.4844 (3.0159)	loss_scale 2048.0000 (1216.4158)	mem 19076MB
[2024-07-31 10:54:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:03:27 lr 0.000011	 wd 0.0500	time 0.5041 (0.5153)	loss 1.0559 (1.2502)	grad_norm 2.5563 (3.0316)	loss_scale 2048.0000 (1255.9962)	mem 19076MB
[2024-07-31 10:54:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:02:35 lr 0.000012	 wd 0.0500	time 0.5031 (0.5151)	loss 1.3810 (1.2502)	grad_norm 2.9333 (3.0045)	loss_scale 2048.0000 (1291.9800)	mem 19076MB
[2024-07-31 10:55:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:44 lr 0.000012	 wd 0.0500	time 0.5000 (0.5149)	loss 1.2327 (1.2498)	grad_norm 12.0135 (3.0169)	loss_scale 2048.0000 (1324.8362)	mem 19076MB
[2024-07-31 10:56:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:52 lr 0.000012	 wd 0.0500	time 0.5016 (0.5147)	loss 0.8346 (1.2500)	grad_norm 1.5296 (3.0174)	loss_scale 2048.0000 (1354.9554)	mem 19076MB
[2024-07-31 10:57:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.5017 (0.5146)	loss 1.2768 (1.2495)	grad_norm 2.1200 (3.0017)	loss_scale 2048.0000 (1382.6661)	mem 19076MB
[2024-07-31 10:57:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 2 training takes 0:21:29
[2024-07-31 10:57:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.297 (11.297)	Loss 0.4148 (0.4148)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 10:58:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.062 Acc@5 97.344
[2024-07-31 10:58:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-31 10:58:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.06%
[2024-07-31 10:58:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 10:58:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 10:58:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:54:32 lr 0.000012	 wd 0.0500	time 11.3799 (11.3799)	loss 0.7212 (0.7212)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 10:59:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:24:40 lr 0.000012	 wd 0.0500	time 0.5002 (0.6163)	loss 1.3365 (1.2799)	grad_norm 3.1910 (2.7664)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 10:59:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:21:33 lr 0.000012	 wd 0.0500	time 0.4974 (0.5621)	loss 1.6103 (1.2528)	grad_norm 2.4769 (2.5710)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:00:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:19:57 lr 0.000012	 wd 0.0500	time 0.4920 (0.5440)	loss 1.5620 (1.2525)	grad_norm 2.9591 (3.0486)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:01:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:18:45 lr 0.000013	 wd 0.0500	time 0.5014 (0.5353)	loss 1.5128 (1.2429)	grad_norm 2.9293 (3.0664)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:02:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:17:41 lr 0.000013	 wd 0.0500	time 0.5031 (0.5302)	loss 0.8648 (1.2424)	grad_norm 2.1847 (2.9586)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:03:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:16:41 lr 0.000013	 wd 0.0500	time 0.4998 (0.5268)	loss 1.1077 (1.2336)	grad_norm 2.7540 (2.8992)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:04:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:15:45 lr 0.000013	 wd 0.0500	time 0.5035 (0.5245)	loss 1.5026 (1.2310)	grad_norm 2.6614 (2.9334)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:05:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:14:49 lr 0.000013	 wd 0.0500	time 0.5047 (0.5227)	loss 1.0229 (1.2254)	grad_norm 1.7376 (2.8967)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:05:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:13:55 lr 0.000013	 wd 0.0500	time 0.5011 (0.5214)	loss 1.6751 (1.2282)	grad_norm 2.4867 (2.8633)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:06:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:13:01 lr 0.000014	 wd 0.0500	time 0.5000 (0.5203)	loss 1.3137 (1.2279)	grad_norm 21.2619 (2.8683)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:07:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:12:08 lr 0.000014	 wd 0.0500	time 0.5061 (0.5194)	loss 0.8725 (1.2287)	grad_norm 4.0833 (2.8494)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:08:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:11:15 lr 0.000014	 wd 0.0500	time 0.5059 (0.5187)	loss 1.2905 (1.2265)	grad_norm 2.5669 (2.9105)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:09:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:10:22 lr 0.000014	 wd 0.0500	time 0.5051 (0.5181)	loss 1.3296 (1.2261)	grad_norm 1.7255 (2.8718)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 11:10:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:09:30 lr 0.000014	 wd 0.0500	time 0.5047 (0.5175)	loss 1.2574 (1.2266)	grad_norm 1.4095 (inf)	loss_scale 1024.0000 (2001.2220)	mem 19076MB
[2024-07-31 11:11:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:08:38 lr 0.000014	 wd 0.0500	time 0.5096 (0.5170)	loss 1.5251 (1.2243)	grad_norm 1.9892 (inf)	loss_scale 1024.0000 (1936.1173)	mem 19076MB
[2024-07-31 11:11:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:07:45 lr 0.000015	 wd 0.0500	time 0.5025 (0.5166)	loss 0.7608 (1.2215)	grad_norm 2.5545 (inf)	loss_scale 1024.0000 (1879.1455)	mem 19076MB
[2024-07-31 11:12:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:06:54 lr 0.000015	 wd 0.0500	time 0.5061 (0.5163)	loss 1.0596 (1.2241)	grad_norm 2.8140 (inf)	loss_scale 1024.0000 (1828.8724)	mem 19076MB
[2024-07-31 11:13:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:06:02 lr 0.000015	 wd 0.0500	time 0.5017 (0.5160)	loss 1.3061 (1.2254)	grad_norm 2.3472 (inf)	loss_scale 1024.0000 (1784.1821)	mem 19076MB
[2024-07-31 11:14:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:05:10 lr 0.000015	 wd 0.0500	time 0.5030 (0.5157)	loss 1.4440 (1.2236)	grad_norm 1.7173 (inf)	loss_scale 1024.0000 (1744.1936)	mem 19076MB
[2024-07-31 11:15:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:04:18 lr 0.000015	 wd 0.0500	time 0.5014 (0.5155)	loss 1.3026 (1.2227)	grad_norm 1.6243 (inf)	loss_scale 1024.0000 (1708.2019)	mem 19076MB
[2024-07-31 11:16:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:03:27 lr 0.000015	 wd 0.0500	time 0.5011 (0.5152)	loss 0.8422 (1.2224)	grad_norm 1.6071 (inf)	loss_scale 1024.0000 (1675.6364)	mem 19076MB
[2024-07-31 11:17:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:02:35 lr 0.000016	 wd 0.0500	time 0.5039 (0.5150)	loss 0.8305 (1.2211)	grad_norm 1.8771 (inf)	loss_scale 1024.0000 (1646.0300)	mem 19076MB
[2024-07-31 11:17:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:44 lr 0.000016	 wd 0.0500	time 0.5027 (0.5149)	loss 0.9547 (1.2214)	grad_norm 1.9270 (inf)	loss_scale 1024.0000 (1618.9970)	mem 19076MB
[2024-07-31 11:18:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:52 lr 0.000016	 wd 0.0500	time 0.5009 (0.5147)	loss 1.3810 (1.2225)	grad_norm 9.0648 (inf)	loss_scale 1024.0000 (1594.2157)	mem 19076MB
[2024-07-31 11:19:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5041 (0.5145)	loss 1.2986 (1.2223)	grad_norm 4.2721 (inf)	loss_scale 1024.0000 (1571.4162)	mem 19076MB
[2024-07-31 11:19:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 3 training takes 0:21:29
[2024-07-31 11:19:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.615 (12.615)	Loss 0.4578 (0.4578)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 11:20:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.084 Acc@5 97.382
[2024-07-31 11:20:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.1%
[2024-07-31 11:20:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.08%
[2024-07-31 11:20:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 11:20:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 11:20:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:10:24 lr 0.000016	 wd 0.0500	time 10.3217 (10.3217)	loss 1.3344 (1.3344)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:21:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:24:21 lr 0.000016	 wd 0.0500	time 0.4960 (0.6086)	loss 0.9806 (1.2336)	grad_norm 1.9998 (2.6819)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:22:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:21:25 lr 0.000016	 wd 0.0500	time 0.4980 (0.5583)	loss 1.1246 (1.2234)	grad_norm 2.1448 (2.7942)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:22:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:19:52 lr 0.000016	 wd 0.0500	time 0.5034 (0.5414)	loss 0.7806 (1.2187)	grad_norm 15.5763 (2.7784)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:23:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:18:40 lr 0.000017	 wd 0.0500	time 0.5015 (0.5332)	loss 1.4233 (1.2157)	grad_norm 2.3047 (2.7330)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:24:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:17:38 lr 0.000017	 wd 0.0500	time 0.5013 (0.5285)	loss 1.2733 (1.2152)	grad_norm 2.0862 (2.6985)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:25:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:16:39 lr 0.000017	 wd 0.0500	time 0.5042 (0.5254)	loss 1.0452 (1.2157)	grad_norm 2.1122 (2.7434)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:26:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:15:43 lr 0.000017	 wd 0.0500	time 0.5028 (0.5233)	loss 1.0268 (1.2165)	grad_norm 2.1358 (2.7098)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:27:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:14:47 lr 0.000017	 wd 0.0500	time 0.5064 (0.5217)	loss 0.8470 (1.2184)	grad_norm 2.0001 (2.7321)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:27:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:13:53 lr 0.000017	 wd 0.0500	time 0.5017 (0.5204)	loss 0.9258 (1.2190)	grad_norm 1.7045 (2.6882)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 11:28:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:13:00 lr 0.000018	 wd 0.0500	time 0.4990 (0.5194)	loss 1.5462 (1.2198)	grad_norm 2.0833 (nan)	loss_scale 512.0000 (997.4026)	mem 19076MB
[2024-07-31 11:29:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:12:07 lr 0.000018	 wd 0.0500	time 0.5014 (0.5186)	loss 1.5155 (1.2210)	grad_norm 2.6825 (nan)	loss_scale 512.0000 (953.3152)	mem 19076MB
[2024-07-31 11:30:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:11:14 lr 0.000018	 wd 0.0500	time 0.5011 (0.5179)	loss 1.3221 (1.2187)	grad_norm 1.6879 (nan)	loss_scale 512.0000 (916.5695)	mem 19076MB
[2024-07-31 11:31:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:10:21 lr 0.000018	 wd 0.0500	time 0.5051 (0.5173)	loss 0.9730 (1.2178)	grad_norm 1.5554 (nan)	loss_scale 512.0000 (885.4727)	mem 19076MB
[2024-07-31 11:32:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:09:29 lr 0.000018	 wd 0.0500	time 0.5028 (0.5168)	loss 1.4731 (1.2178)	grad_norm 2.6026 (nan)	loss_scale 512.0000 (858.8151)	mem 19076MB
[2024-07-31 11:33:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:08:37 lr 0.000018	 wd 0.0500	time 0.5036 (0.5164)	loss 1.2495 (1.2177)	grad_norm 2.1617 (nan)	loss_scale 512.0000 (835.7095)	mem 19076MB
[2024-07-31 11:33:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:07:45 lr 0.000019	 wd 0.0500	time 0.5074 (0.5160)	loss 1.4166 (1.2156)	grad_norm 1.3675 (nan)	loss_scale 512.0000 (815.4903)	mem 19076MB
[2024-07-31 11:34:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:06:53 lr 0.000019	 wd 0.0500	time 0.5011 (0.5156)	loss 1.2220 (1.2155)	grad_norm 1.6235 (nan)	loss_scale 512.0000 (797.6484)	mem 19076MB
[2024-07-31 11:35:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:06:01 lr 0.000019	 wd 0.0500	time 0.5025 (0.5153)	loss 1.5680 (1.2168)	grad_norm 1.4812 (nan)	loss_scale 512.0000 (781.7879)	mem 19076MB
[2024-07-31 11:36:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:05:10 lr 0.000019	 wd 0.0500	time 0.4929 (0.5151)	loss 1.4203 (1.2170)	grad_norm 1.8299 (nan)	loss_scale 512.0000 (767.5960)	mem 19076MB
[2024-07-31 11:37:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:04:18 lr 0.000019	 wd 0.0500	time 0.4977 (0.5148)	loss 0.8371 (1.2155)	grad_norm 1.7555 (nan)	loss_scale 512.0000 (754.8226)	mem 19076MB
[2024-07-31 11:38:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:03:26 lr 0.000019	 wd 0.0500	time 0.4973 (0.5146)	loss 0.9463 (1.2139)	grad_norm 1.6276 (nan)	loss_scale 512.0000 (743.2651)	mem 19076MB
[2024-07-31 11:39:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:02:35 lr 0.000020	 wd 0.0500	time 0.4947 (0.5144)	loss 0.9589 (1.2139)	grad_norm 3.1037 (nan)	loss_scale 512.0000 (732.7578)	mem 19076MB
[2024-07-31 11:39:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:43 lr 0.000020	 wd 0.0500	time 0.5069 (0.5142)	loss 0.8428 (1.2139)	grad_norm 6.1122 (nan)	loss_scale 512.0000 (723.1638)	mem 19076MB
[2024-07-31 11:40:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.4969 (0.5141)	loss 0.7754 (1.2130)	grad_norm 1.6571 (nan)	loss_scale 512.0000 (714.3690)	mem 19076MB
[2024-07-31 11:41:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5010 (0.5139)	loss 1.1959 (1.2133)	grad_norm 2.1772 (nan)	loss_scale 512.0000 (706.2775)	mem 19076MB
[2024-07-31 11:41:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 4 training takes 0:21:28
[2024-07-31 11:41:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.055 (12.055)	Loss 0.4836 (0.4836)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 11:42:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.194 Acc@5 97.502
[2024-07-31 11:42:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.2%
[2024-07-31 11:42:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.19%
[2024-07-31 11:42:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 11:42:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 11:42:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:47:42 lr 0.000020	 wd 0.0500	time 11.2160 (11.2160)	loss 1.4202 (1.4202)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:43:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:24:30 lr 0.000020	 wd 0.0500	time 0.4961 (0.6122)	loss 1.0265 (1.2416)	grad_norm 2.2235 (2.3095)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:44:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:21:29 lr 0.000020	 wd 0.0500	time 0.4995 (0.5600)	loss 1.2811 (1.2035)	grad_norm 1.2684 (2.3433)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:44:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:19:54 lr 0.000020	 wd 0.0500	time 0.5017 (0.5427)	loss 0.8537 (1.1972)	grad_norm 1.8991 (2.4360)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:45:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:18:42 lr 0.000020	 wd 0.0500	time 0.5046 (0.5342)	loss 1.5315 (1.2048)	grad_norm 2.2873 (2.5101)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:46:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:17:39 lr 0.000020	 wd 0.0500	time 0.5022 (0.5292)	loss 1.0418 (1.2019)	grad_norm 4.0072 (2.4917)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:47:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:16:40 lr 0.000020	 wd 0.0500	time 0.5047 (0.5260)	loss 0.9468 (1.2027)	grad_norm 3.8017 (2.4666)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:48:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:15:43 lr 0.000020	 wd 0.0500	time 0.4933 (0.5237)	loss 0.9350 (1.2029)	grad_norm 1.7390 (2.4742)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:49:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:14:48 lr 0.000020	 wd 0.0500	time 0.5073 (0.5220)	loss 1.5086 (1.2004)	grad_norm 2.0001 (2.4856)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:50:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:13:54 lr 0.000020	 wd 0.0500	time 0.5065 (0.5208)	loss 1.0460 (1.1988)	grad_norm 2.2962 (2.4935)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:50:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:13:00 lr 0.000020	 wd 0.0500	time 0.4969 (0.5198)	loss 1.1929 (1.1975)	grad_norm 3.4273 (2.4572)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:51:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:12:07 lr 0.000020	 wd 0.0500	time 0.4995 (0.5190)	loss 1.2191 (1.1990)	grad_norm 2.6735 (2.4362)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:52:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:11:14 lr 0.000020	 wd 0.0500	time 0.5044 (0.5183)	loss 1.5338 (1.1979)	grad_norm 2.4778 (2.4259)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 11:53:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:10:22 lr 0.000020	 wd 0.0500	time 0.5040 (0.5177)	loss 0.8401 (1.1987)	grad_norm 2.5837 (nan)	loss_scale 256.0000 (507.6710)	mem 19076MB
[2024-07-31 11:54:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:09:29 lr 0.000020	 wd 0.0500	time 0.5028 (0.5172)	loss 1.3404 (1.2025)	grad_norm 1.8874 (nan)	loss_scale 256.0000 (489.7074)	mem 19076MB
[2024-07-31 11:55:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:08:37 lr 0.000020	 wd 0.0500	time 0.5034 (0.5167)	loss 0.7994 (1.2042)	grad_norm 2.1442 (nan)	loss_scale 256.0000 (474.1372)	mem 19076MB
[2024-07-31 11:55:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:07:45 lr 0.000020	 wd 0.0500	time 0.5019 (0.5163)	loss 1.4220 (1.2074)	grad_norm 2.6242 (nan)	loss_scale 256.0000 (460.5122)	mem 19076MB
[2024-07-31 11:56:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:06:53 lr 0.000020	 wd 0.0500	time 0.5035 (0.5160)	loss 0.8534 (1.2075)	grad_norm 1.7942 (nan)	loss_scale 256.0000 (448.4891)	mem 19076MB
[2024-07-31 11:57:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:06:02 lr 0.000020	 wd 0.0500	time 0.5014 (0.5157)	loss 0.8312 (1.2048)	grad_norm 1.8436 (nan)	loss_scale 256.0000 (437.8012)	mem 19076MB
[2024-07-31 11:58:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:05:10 lr 0.000020	 wd 0.0500	time 0.5022 (0.5154)	loss 1.1693 (1.2042)	grad_norm 2.6792 (nan)	loss_scale 256.0000 (428.2378)	mem 19076MB
[2024-07-31 11:59:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:04:18 lr 0.000020	 wd 0.0500	time 0.5071 (0.5152)	loss 1.6241 (1.2053)	grad_norm 3.0607 (nan)	loss_scale 256.0000 (419.6302)	mem 19076MB
[2024-07-31 12:00:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:03:27 lr 0.000020	 wd 0.0500	time 0.5037 (0.5150)	loss 1.6160 (1.2051)	grad_norm 2.1133 (nan)	loss_scale 256.0000 (411.8420)	mem 19076MB
[2024-07-31 12:01:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:02:35 lr 0.000020	 wd 0.0500	time 0.5058 (0.5148)	loss 1.3328 (1.2055)	grad_norm 1.4630 (nan)	loss_scale 256.0000 (404.7615)	mem 19076MB
[2024-07-31 12:01:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:43 lr 0.000020	 wd 0.0500	time 0.4991 (0.5146)	loss 1.1738 (1.2040)	grad_norm 1.9775 (nan)	loss_scale 256.0000 (398.2964)	mem 19076MB
[2024-07-31 12:02:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.5023 (0.5144)	loss 0.9232 (1.2033)	grad_norm 2.4781 (nan)	loss_scale 256.0000 (392.3698)	mem 19076MB
[2024-07-31 12:03:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.5059 (0.5143)	loss 1.4350 (1.2042)	grad_norm 1.5567 (nan)	loss_scale 256.0000 (386.9172)	mem 19076MB
[2024-07-31 12:03:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 5 training takes 0:21:29
[2024-07-31 12:03:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.289 (12.289)	Loss 0.5054 (0.5054)	Acc@1 93.359 (93.359)	Acc@5 98.633 (98.633)	Mem 19076MB
[2024-07-31 12:04:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.322 Acc@5 97.530
[2024-07-31 12:04:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-31 12:04:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.32%
[2024-07-31 12:04:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 12:04:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 12:04:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:16:33 lr 0.000020	 wd 0.0500	time 11.9080 (11.9080)	loss 1.2373 (1.2373)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:05:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:24:46 lr 0.000020	 wd 0.0500	time 0.4979 (0.6189)	loss 1.0025 (1.2317)	grad_norm 1.3522 (2.0156)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:06:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:21:36 lr 0.000020	 wd 0.0500	time 0.5022 (0.5631)	loss 0.9346 (1.2131)	grad_norm 1.5191 (2.1347)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:06:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:19:59 lr 0.000020	 wd 0.0500	time 0.4980 (0.5446)	loss 0.9885 (1.2091)	grad_norm 1.8437 (2.1085)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:07:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:18:45 lr 0.000020	 wd 0.0500	time 0.5008 (0.5356)	loss 0.7071 (1.2076)	grad_norm 2.0857 (2.1532)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:08:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:17:41 lr 0.000020	 wd 0.0500	time 0.4998 (0.5303)	loss 1.3308 (1.2086)	grad_norm 1.5928 (2.1661)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:09:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:16:42 lr 0.000020	 wd 0.0500	time 0.4975 (0.5269)	loss 0.9221 (1.2000)	grad_norm 1.8126 (2.2372)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:10:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:15:45 lr 0.000020	 wd 0.0500	time 0.4825 (0.5245)	loss 1.4303 (1.2024)	grad_norm 1.6159 (2.2297)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:11:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:14:49 lr 0.000020	 wd 0.0500	time 0.5039 (0.5228)	loss 0.7823 (1.2076)	grad_norm 2.0326 (2.2419)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:12:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:13:55 lr 0.000020	 wd 0.0500	time 0.5029 (0.5214)	loss 1.6868 (1.2033)	grad_norm 1.6849 (2.2482)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:12:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:13:01 lr 0.000020	 wd 0.0500	time 0.5004 (0.5204)	loss 1.0753 (1.2030)	grad_norm 2.3810 (2.2514)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:13:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:12:08 lr 0.000020	 wd 0.0500	time 0.4858 (0.5195)	loss 0.8434 (1.1983)	grad_norm 2.0375 (2.2652)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:14:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:11:15 lr 0.000020	 wd 0.0500	time 0.5026 (0.5188)	loss 1.3333 (1.1952)	grad_norm 1.7245 (2.2988)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:15:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:10:22 lr 0.000020	 wd 0.0500	time 0.5016 (0.5181)	loss 1.1690 (1.1939)	grad_norm 1.9041 (2.3191)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:16:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:09:30 lr 0.000020	 wd 0.0500	time 0.5034 (0.5176)	loss 1.0340 (1.1931)	grad_norm 1.9774 (2.3359)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:17:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:08:38 lr 0.000020	 wd 0.0500	time 0.4965 (0.5171)	loss 0.7202 (1.1929)	grad_norm 6.1837 (2.3370)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:18:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:07:46 lr 0.000020	 wd 0.0500	time 0.5042 (0.5167)	loss 1.4005 (1.1906)	grad_norm 3.3227 (2.3370)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:18:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:06:54 lr 0.000020	 wd 0.0500	time 0.5060 (0.5164)	loss 1.4265 (1.1886)	grad_norm 1.1204 (2.3384)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:19:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:06:02 lr 0.000020	 wd 0.0500	time 0.5043 (0.5160)	loss 1.2765 (1.1888)	grad_norm 1.2964 (2.3329)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:20:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:05:10 lr 0.000020	 wd 0.0500	time 0.5018 (0.5158)	loss 1.4593 (1.1910)	grad_norm 2.1188 (2.3171)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:21:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:04:18 lr 0.000020	 wd 0.0500	time 0.5026 (0.5155)	loss 0.9487 (1.1917)	grad_norm 1.4954 (2.3062)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:22:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:03:27 lr 0.000020	 wd 0.0500	time 0.5044 (0.5152)	loss 1.1156 (1.1919)	grad_norm 1.8624 (2.3208)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:23:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:02:35 lr 0.000020	 wd 0.0500	time 0.5048 (0.5150)	loss 0.8420 (1.1912)	grad_norm 2.6775 (2.3781)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:23:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:43 lr 0.000020	 wd 0.0500	time 0.5038 (0.5148)	loss 1.0022 (1.1933)	grad_norm 4.6605 (2.3913)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:24:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.5026 (0.5147)	loss 1.4210 (1.1921)	grad_norm 2.2112 (2.3907)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:25:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:01 lr 0.000020	 wd 0.0500	time 0.4981 (0.5145)	loss 1.4574 (1.1915)	grad_norm 1.9382 (2.3960)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:25:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 6 training takes 0:21:29
[2024-07-31 12:25:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.708 (12.708)	Loss 0.5308 (0.5308)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 12:26:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.342 Acc@5 97.554
[2024-07-31 12:26:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.3%
[2024-07-31 12:26:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.34%
[2024-07-31 12:26:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 12:26:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 12:26:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:54:51 lr 0.000020	 wd 0.0500	time 11.3873 (11.3873)	loss 0.9741 (0.9741)	grad_norm 0.0000 (0.0000)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:27:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:24:33 lr 0.000020	 wd 0.0500	time 0.4991 (0.6135)	loss 1.2759 (1.1725)	grad_norm 1.7956 (2.4124)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:28:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:21:30 lr 0.000020	 wd 0.0500	time 0.5014 (0.5604)	loss 1.1659 (1.1935)	grad_norm 1.7901 (2.3937)	loss_scale 256.0000 (256.0000)	mem 19076MB
[2024-07-31 12:29:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:19:54 lr 0.000020	 wd 0.0500	time 0.4935 (0.5426)	loss 1.5303 (1.1933)	grad_norm 1.5779 (2.4517)	loss_scale 512.0000 (278.1130)	mem 19076MB
[2024-07-31 12:29:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:18:42 lr 0.000020	 wd 0.0500	time 0.5047 (0.5340)	loss 0.8926 (1.1985)	grad_norm 2.3573 (2.3597)	loss_scale 512.0000 (336.4389)	mem 19076MB
[2024-07-31 12:30:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:17:39 lr 0.000020	 wd 0.0500	time 0.5068 (0.5290)	loss 1.4531 (1.1961)	grad_norm 1.3274 (2.3688)	loss_scale 512.0000 (371.4810)	mem 19076MB
[2024-07-31 12:31:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:16:40 lr 0.000020	 wd 0.0500	time 0.5039 (0.5258)	loss 1.3001 (1.2009)	grad_norm 1.8763 (2.4128)	loss_scale 512.0000 (394.8619)	mem 19076MB
[2024-07-31 12:32:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:15:43 lr 0.000020	 wd 0.0500	time 0.5045 (0.5235)	loss 1.1075 (1.1954)	grad_norm 1.8979 (2.3657)	loss_scale 512.0000 (411.5720)	mem 19076MB
[2024-07-31 12:33:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:14:48 lr 0.000020	 wd 0.0500	time 0.5049 (0.5219)	loss 0.9215 (1.1927)	grad_norm 2.0585 (2.3920)	loss_scale 512.0000 (424.1099)	mem 19076MB
[2024-07-31 12:34:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:13:54 lr 0.000020	 wd 0.0500	time 0.5033 (0.5206)	loss 1.5390 (1.1958)	grad_norm 1.2957 (2.3827)	loss_scale 512.0000 (433.8646)	mem 19076MB
[2024-07-31 12:34:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:13:00 lr 0.000020	 wd 0.0500	time 0.4971 (0.5196)	loss 0.8550 (1.1957)	grad_norm 2.2354 (2.3607)	loss_scale 512.0000 (441.6703)	mem 19076MB
[2024-07-31 12:35:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:12:07 lr 0.000020	 wd 0.0500	time 0.4945 (0.5187)	loss 1.2413 (1.1937)	grad_norm 3.6288 (2.3736)	loss_scale 512.0000 (448.0581)	mem 19076MB
[2024-07-31 12:36:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:11:14 lr 0.000020	 wd 0.0500	time 0.5022 (0.5180)	loss 1.3406 (1.1899)	grad_norm 2.1518 (2.3912)	loss_scale 512.0000 (453.3822)	mem 19076MB
[2024-07-31 12:37:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:10:21 lr 0.000020	 wd 0.0500	time 0.5026 (0.5174)	loss 1.3058 (1.1897)	grad_norm 2.2097 (2.3759)	loss_scale 512.0000 (457.8878)	mem 19076MB
[2024-07-31 12:38:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:09:29 lr 0.000019	 wd 0.0500	time 0.5013 (0.5169)	loss 1.2807 (1.1909)	grad_norm 14.4641 (2.4210)	loss_scale 512.0000 (461.7502)	mem 19076MB
[2024-07-31 12:39:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:08:37 lr 0.000019	 wd 0.0500	time 0.5029 (0.5165)	loss 0.8277 (1.1918)	grad_norm 1.9426 (2.4069)	loss_scale 512.0000 (465.0979)	mem 19076MB
[2024-07-31 12:40:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:07:45 lr 0.000019	 wd 0.0500	time 0.5018 (0.5161)	loss 1.2337 (1.1925)	grad_norm 5.9072 (2.4130)	loss_scale 512.0000 (468.0275)	mem 19076MB
[2024-07-31 12:40:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:06:53 lr 0.000019	 wd 0.0500	time 0.4972 (0.5157)	loss 1.2646 (1.1924)	grad_norm 2.4554 (2.4066)	loss_scale 512.0000 (470.6126)	mem 19076MB
[2024-07-31 12:41:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:06:01 lr 0.000019	 wd 0.0500	time 0.5052 (0.5154)	loss 0.7760 (1.1946)	grad_norm 1.5140 (2.3906)	loss_scale 512.0000 (472.9106)	mem 19076MB
[2024-07-31 12:42:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:05:10 lr 0.000019	 wd 0.0500	time 0.5043 (0.5152)	loss 1.2018 (1.1951)	grad_norm 1.8915 (2.3890)	loss_scale 512.0000 (474.9669)	mem 19076MB
[2024-07-31 12:43:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:04:18 lr 0.000019	 wd 0.0500	time 0.5046 (0.5149)	loss 1.3214 (1.1956)	grad_norm 1.4908 (2.4009)	loss_scale 512.0000 (476.8176)	mem 19076MB
[2024-07-31 12:44:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:03:26 lr 0.000019	 wd 0.0500	time 0.5068 (0.5147)	loss 1.5783 (1.1981)	grad_norm 1.7750 (2.3929)	loss_scale 512.0000 (478.4921)	mem 19076MB
[2024-07-31 12:45:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:02:35 lr 0.000019	 wd 0.0500	time 0.4936 (0.5146)	loss 1.3060 (1.1960)	grad_norm 2.8944 (2.3787)	loss_scale 512.0000 (480.0145)	mem 19076MB
[2024-07-31 12:46:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:43 lr 0.000019	 wd 0.0500	time 0.5033 (0.5144)	loss 1.5245 (1.1954)	grad_norm 1.9508 (2.3694)	loss_scale 512.0000 (481.4046)	mem 19076MB
[2024-07-31 12:46:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:52 lr 0.000019	 wd 0.0500	time 0.4984 (0.5142)	loss 1.3784 (1.1953)	grad_norm 1.5065 (2.3566)	loss_scale 512.0000 (482.6789)	mem 19076MB
[2024-07-31 12:47:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5031 (0.5141)	loss 1.5542 (1.1945)	grad_norm 1.7228 (2.4473)	loss_scale 512.0000 (483.8513)	mem 19076MB
[2024-07-31 12:47:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 7 training takes 0:21:28
[2024-07-31 12:48:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.688 (12.688)	Loss 0.5127 (0.5127)	Acc@1 93.555 (93.555)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 12:48:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.378 Acc@5 97.618
[2024-07-31 12:48:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-31 12:48:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.38%
[2024-07-31 12:48:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 12:48:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 12:48:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:48:52 lr 0.000019	 wd 0.0500	time 11.2442 (11.2442)	loss 1.2733 (1.2733)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:49:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:24:31 lr 0.000019	 wd 0.0500	time 0.4906 (0.6125)	loss 0.9121 (1.2548)	grad_norm 1.8289 (2.3162)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:50:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:21:29 lr 0.000019	 wd 0.0500	time 0.4919 (0.5600)	loss 0.8971 (1.2270)	grad_norm 2.3229 (2.2992)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:51:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:19:54 lr 0.000019	 wd 0.0500	time 0.5021 (0.5426)	loss 1.4931 (1.2112)	grad_norm 1.7518 (2.3231)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:51:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:18:42 lr 0.000019	 wd 0.0500	time 0.5022 (0.5340)	loss 0.9211 (1.2081)	grad_norm 1.4123 (2.2965)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:52:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:17:39 lr 0.000019	 wd 0.0500	time 0.4998 (0.5291)	loss 0.8336 (1.1981)	grad_norm 3.1756 (2.2617)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:53:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:16:40 lr 0.000019	 wd 0.0500	time 0.5007 (0.5259)	loss 0.8811 (1.1979)	grad_norm 1.5086 (2.2709)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:54:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:15:43 lr 0.000019	 wd 0.0500	time 0.5090 (0.5236)	loss 1.3492 (1.1974)	grad_norm 1.9376 (2.2494)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:55:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:14:48 lr 0.000019	 wd 0.0500	time 0.5003 (0.5219)	loss 1.3410 (1.1996)	grad_norm 1.6691 (2.2632)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:56:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:13:54 lr 0.000019	 wd 0.0500	time 0.5006 (0.5206)	loss 0.9599 (1.1960)	grad_norm 1.7443 (2.2538)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:57:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:13:00 lr 0.000019	 wd 0.0500	time 0.5044 (0.5196)	loss 1.3885 (1.1979)	grad_norm 1.9995 (2.2481)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:57:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:12:07 lr 0.000019	 wd 0.0500	time 0.5045 (0.5188)	loss 1.3446 (1.1951)	grad_norm 1.4351 (2.2353)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:58:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:11:14 lr 0.000019	 wd 0.0500	time 0.5021 (0.5182)	loss 1.0971 (1.1957)	grad_norm 1.4907 (2.2178)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 12:59:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:10:22 lr 0.000019	 wd 0.0500	time 0.5042 (0.5176)	loss 0.8482 (1.1955)	grad_norm 2.2304 (2.2430)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 13:00:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:09:29 lr 0.000019	 wd 0.0500	time 0.5037 (0.5170)	loss 1.3060 (1.1967)	grad_norm 1.6272 (2.2632)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 13:01:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:08:37 lr 0.000019	 wd 0.0500	time 0.5040 (0.5166)	loss 1.1403 (1.1973)	grad_norm 1.9914 (2.2705)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 13:02:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:07:45 lr 0.000019	 wd 0.0500	time 0.5039 (0.5162)	loss 1.4287 (1.1992)	grad_norm 2.3679 (2.2554)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 13:02:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:06:53 lr 0.000019	 wd 0.0500	time 0.5017 (0.5158)	loss 0.7969 (1.2017)	grad_norm 3.2932 (2.2708)	loss_scale 512.0000 (512.0000)	mem 19076MB
[2024-07-31 13:03:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:06:01 lr 0.000019	 wd 0.0500	time 0.5018 (0.5155)	loss 0.8261 (1.2000)	grad_norm 2.0332 (2.2627)	loss_scale 1024.0000 (519.9600)	mem 19076MB
[2024-07-31 13:04:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:05:10 lr 0.000019	 wd 0.0500	time 0.5031 (0.5152)	loss 1.2976 (1.2018)	grad_norm 1.9157 (2.2821)	loss_scale 1024.0000 (546.4745)	mem 19076MB
[2024-07-31 13:05:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:04:18 lr 0.000019	 wd 0.0500	time 0.5019 (0.5150)	loss 0.7766 (1.2004)	grad_norm 1.8271 (2.2750)	loss_scale 1024.0000 (570.3388)	mem 19076MB
[2024-07-31 13:06:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:03:26 lr 0.000019	 wd 0.0500	time 0.4949 (0.5148)	loss 0.8997 (1.2012)	grad_norm 2.2149 (2.2868)	loss_scale 1024.0000 (591.9315)	mem 19076MB
[2024-07-31 13:07:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:02:35 lr 0.000019	 wd 0.0500	time 0.4967 (0.5146)	loss 1.2421 (1.1998)	grad_norm 4.1343 (2.2788)	loss_scale 1024.0000 (611.5620)	mem 19076MB
[2024-07-31 13:08:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:43 lr 0.000019	 wd 0.0500	time 0.5020 (0.5144)	loss 1.3044 (1.1980)	grad_norm 1.8495 (2.2635)	loss_scale 1024.0000 (629.4863)	mem 19076MB
[2024-07-31 13:08:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:52 lr 0.000019	 wd 0.0500	time 0.5011 (0.5142)	loss 1.1094 (1.1979)	grad_norm 2.1793 (2.2723)	loss_scale 1024.0000 (645.9175)	mem 19076MB
[2024-07-31 13:09:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:01 lr 0.000019	 wd 0.0500	time 0.5029 (0.5140)	loss 0.9338 (1.1975)	grad_norm 1.7010 (2.2750)	loss_scale 1024.0000 (661.0348)	mem 19076MB
[2024-07-31 13:09:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 8 training takes 0:21:28
[2024-07-31 13:10:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.588 (12.588)	Loss 0.5142 (0.5142)	Acc@1 93.750 (93.750)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 13:10:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.448 Acc@5 97.602
[2024-07-31 13:10:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.4%
[2024-07-31 13:10:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.45%
[2024-07-31 13:10:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 13:10:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 13:10:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:38:36 lr 0.000019	 wd 0.0500	time 10.9977 (10.9977)	loss 1.3998 (1.3998)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:11:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:24:26 lr 0.000019	 wd 0.0500	time 0.4915 (0.6104)	loss 1.3026 (1.1860)	grad_norm 2.0555 (2.0884)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:12:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:21:27 lr 0.000019	 wd 0.0500	time 0.5005 (0.5591)	loss 1.0166 (1.2076)	grad_norm 1.5315 (2.1692)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:13:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:19:53 lr 0.000019	 wd 0.0500	time 0.4996 (0.5420)	loss 1.4359 (1.2127)	grad_norm 2.1990 (2.1857)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:13:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:18:41 lr 0.000019	 wd 0.0500	time 0.5030 (0.5336)	loss 0.9309 (1.1924)	grad_norm 3.1901 (2.1835)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:14:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:17:38 lr 0.000019	 wd 0.0500	time 0.5035 (0.5288)	loss 0.7309 (1.1874)	grad_norm 1.8398 (2.1820)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:15:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:16:39 lr 0.000019	 wd 0.0500	time 0.5027 (0.5256)	loss 0.8123 (1.1901)	grad_norm 1.8660 (2.1649)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:16:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:15:43 lr 0.000019	 wd 0.0500	time 0.5037 (0.5234)	loss 1.4579 (1.1927)	grad_norm 1.6863 (2.2062)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:17:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:14:48 lr 0.000019	 wd 0.0500	time 0.5028 (0.5218)	loss 1.4365 (1.1948)	grad_norm 1.4552 (2.2359)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:18:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:13:53 lr 0.000019	 wd 0.0500	time 0.4899 (0.5206)	loss 1.3922 (1.1947)	grad_norm 1.8389 (2.2610)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:19:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:13:00 lr 0.000019	 wd 0.0500	time 0.4957 (0.5196)	loss 1.2991 (1.1952)	grad_norm 1.4996 (2.2648)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:19:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:12:07 lr 0.000018	 wd 0.0500	time 0.4971 (0.5188)	loss 1.4269 (1.1908)	grad_norm 1.7999 (2.2944)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:20:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:11:14 lr 0.000018	 wd 0.0500	time 0.4992 (0.5181)	loss 0.9896 (1.1910)	grad_norm 1.4845 (2.3019)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:21:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:10:22 lr 0.000018	 wd 0.0500	time 0.4982 (0.5175)	loss 1.3833 (1.1919)	grad_norm 1.9714 (2.2850)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:22:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:09:29 lr 0.000018	 wd 0.0500	time 0.5042 (0.5170)	loss 0.9560 (1.1900)	grad_norm 1.5857 (2.2828)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:23:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:08:37 lr 0.000018	 wd 0.0500	time 0.5046 (0.5166)	loss 0.9423 (1.1926)	grad_norm 21.3357 (2.3205)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:24:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:07:45 lr 0.000018	 wd 0.0500	time 0.5057 (0.5162)	loss 0.9577 (1.1906)	grad_norm 1.6927 (2.3357)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:25:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:06:53 lr 0.000018	 wd 0.0500	time 0.5039 (0.5159)	loss 1.4715 (1.1908)	grad_norm 9.7619 (2.3425)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:25:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:06:01 lr 0.000018	 wd 0.0500	time 0.5047 (0.5156)	loss 1.3256 (1.1901)	grad_norm 2.2272 (2.3191)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:26:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:05:10 lr 0.000018	 wd 0.0500	time 0.5034 (0.5153)	loss 1.4561 (1.1890)	grad_norm 2.0992 (2.3134)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:27:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:04:18 lr 0.000018	 wd 0.0500	time 0.5026 (0.5151)	loss 1.2567 (1.1894)	grad_norm 1.4982 (2.3208)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:28:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:03:26 lr 0.000018	 wd 0.0500	time 0.4991 (0.5149)	loss 1.4357 (1.1904)	grad_norm 5.2655 (2.3413)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:29:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:02:35 lr 0.000018	 wd 0.0500	time 0.5013 (0.5147)	loss 1.2579 (1.1914)	grad_norm 1.6758 (2.3290)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:30:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:43 lr 0.000018	 wd 0.0500	time 0.4987 (0.5145)	loss 0.8430 (1.1913)	grad_norm 2.2576 (2.3352)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:30:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:52 lr 0.000018	 wd 0.0500	time 0.5030 (0.5143)	loss 1.1446 (1.1917)	grad_norm 1.7220 (2.3212)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:31:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:01 lr 0.000018	 wd 0.0500	time 0.5026 (0.5141)	loss 1.4252 (1.1912)	grad_norm 1.8623 (2.3204)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:31:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 9 training takes 0:21:28
[2024-07-31 13:32:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.113 (12.113)	Loss 0.5303 (0.5303)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 13:32:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.494 Acc@5 97.618
[2024-07-31 13:32:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-31 13:32:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.49%
[2024-07-31 13:32:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 13:32:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 13:32:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:43:56 lr 0.000018	 wd 0.0500	time 11.1256 (11.1256)	loss 1.4465 (1.4465)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:33:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:24:29 lr 0.000018	 wd 0.0500	time 0.5008 (0.6117)	loss 0.8292 (1.1968)	grad_norm 2.9800 (3.1659)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:34:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:21:28 lr 0.000018	 wd 0.0500	time 0.5040 (0.5596)	loss 1.4029 (1.1874)	grad_norm 1.9487 (2.7845)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:35:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:19:54 lr 0.000018	 wd 0.0500	time 0.4993 (0.5424)	loss 1.3852 (1.1879)	grad_norm 2.6142 (2.6836)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:36:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:18:42 lr 0.000018	 wd 0.0500	time 0.4915 (0.5340)	loss 0.7985 (1.1961)	grad_norm 2.0414 (2.5546)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:36:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:17:39 lr 0.000018	 wd 0.0500	time 0.4978 (0.5291)	loss 0.7582 (1.1932)	grad_norm 1.5558 (2.5267)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:37:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:16:40 lr 0.000018	 wd 0.0500	time 0.5054 (0.5260)	loss 1.2535 (1.1996)	grad_norm 1.6301 (2.4449)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:38:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:15:43 lr 0.000018	 wd 0.0500	time 0.5070 (0.5238)	loss 1.3156 (1.1967)	grad_norm 1.3295 (2.4084)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 13:39:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:14:48 lr 0.000018	 wd 0.0500	time 0.5019 (0.5221)	loss 1.3125 (1.1993)	grad_norm 1.6442 (2.4009)	loss_scale 2048.0000 (1064.9089)	mem 19076MB
[2024-07-31 13:40:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:13:54 lr 0.000018	 wd 0.0500	time 0.5012 (0.5208)	loss 1.3677 (1.1976)	grad_norm 2.3035 (2.4031)	loss_scale 2048.0000 (1174.0200)	mem 19076MB
[2024-07-31 13:41:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:13:00 lr 0.000018	 wd 0.0500	time 0.4947 (0.5198)	loss 1.3354 (1.1984)	grad_norm 10.8572 (2.3940)	loss_scale 2048.0000 (1261.3307)	mem 19076MB
[2024-07-31 13:41:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:12:07 lr 0.000018	 wd 0.0500	time 0.5048 (0.5190)	loss 1.0828 (1.2059)	grad_norm 1.8562 (2.3704)	loss_scale 2048.0000 (1332.7811)	mem 19076MB
[2024-07-31 13:42:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:11:14 lr 0.000018	 wd 0.0500	time 0.5035 (0.5183)	loss 1.4829 (1.2026)	grad_norm 1.5886 (2.3821)	loss_scale 2048.0000 (1392.3331)	mem 19076MB
[2024-07-31 13:43:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:10:22 lr 0.000018	 wd 0.0500	time 0.5020 (0.5177)	loss 1.2173 (1.2004)	grad_norm 1.5692 (2.3609)	loss_scale 2048.0000 (1442.7302)	mem 19076MB
[2024-07-31 13:44:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:09:29 lr 0.000018	 wd 0.0500	time 0.5062 (0.5172)	loss 1.4725 (1.1987)	grad_norm 1.5290 (2.3415)	loss_scale 2048.0000 (1485.9329)	mem 19076MB
[2024-07-31 13:45:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:08:37 lr 0.000018	 wd 0.0500	time 0.5036 (0.5168)	loss 1.2597 (1.1969)	grad_norm 3.4184 (2.3370)	loss_scale 2048.0000 (1523.3791)	mem 19076MB
[2024-07-31 13:46:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:07:45 lr 0.000018	 wd 0.0500	time 0.5071 (0.5164)	loss 1.1812 (1.1950)	grad_norm 1.9007 (2.3179)	loss_scale 2048.0000 (1556.1474)	mem 19076MB
[2024-07-31 13:47:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:06:53 lr 0.000018	 wd 0.0500	time 0.4999 (0.5161)	loss 1.3220 (1.1970)	grad_norm 2.1082 (2.3141)	loss_scale 2048.0000 (1585.0629)	mem 19076MB
[2024-07-31 13:47:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:06:02 lr 0.000018	 wd 0.0500	time 0.5033 (0.5158)	loss 0.9922 (1.1953)	grad_norm 1.8133 (2.3743)	loss_scale 2048.0000 (1610.7674)	mem 19076MB
[2024-07-31 13:48:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:05:10 lr 0.000018	 wd 0.0500	time 0.5031 (0.5155)	loss 1.0831 (1.1940)	grad_norm 1.7203 (2.3630)	loss_scale 2048.0000 (1633.7675)	mem 19076MB
[2024-07-31 13:49:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:04:18 lr 0.000017	 wd 0.0500	time 0.5078 (0.5153)	loss 1.3477 (1.1971)	grad_norm 1.9489 (2.3478)	loss_scale 2048.0000 (1654.4688)	mem 19076MB
[2024-07-31 13:50:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:03:27 lr 0.000017	 wd 0.0500	time 0.5026 (0.5151)	loss 1.1214 (1.1985)	grad_norm 2.2986 (2.3502)	loss_scale 2048.0000 (1673.1994)	mem 19076MB
[2024-07-31 13:51:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:02:35 lr 0.000017	 wd 0.0500	time 0.5041 (0.5149)	loss 1.3139 (1.1996)	grad_norm 2.3105 (2.3574)	loss_scale 2048.0000 (1690.2281)	mem 19076MB
[2024-07-31 13:52:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:43 lr 0.000017	 wd 0.0500	time 0.5029 (0.5147)	loss 1.3136 (1.1974)	grad_norm 2.0838 (2.3828)	loss_scale 2048.0000 (1705.7766)	mem 19076MB
[2024-07-31 13:53:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:52 lr 0.000017	 wd 0.0500	time 0.4980 (0.5146)	loss 1.4748 (1.1962)	grad_norm 1.7062 (2.3756)	loss_scale 2048.0000 (1720.0300)	mem 19076MB
[2024-07-31 13:53:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:01 lr 0.000017	 wd 0.0500	time 0.4988 (0.5144)	loss 0.8893 (1.1953)	grad_norm 2.5702 (2.3747)	loss_scale 2048.0000 (1733.1435)	mem 19076MB
[2024-07-31 13:53:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 10 training takes 0:21:29
[2024-07-31 13:54:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.468 (11.468)	Loss 0.5190 (0.5190)	Acc@1 93.555 (93.555)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 13:54:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.556 Acc@5 97.628
[2024-07-31 13:54:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-31 13:54:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.56%
[2024-07-31 13:54:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 13:54:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 13:54:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][0/2502]	eta 6:49:47 lr 0.000017	 wd 0.0500	time 9.8273 (9.8273)	loss 0.9721 (0.9721)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 13:55:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:24:23 lr 0.000017	 wd 0.0500	time 0.5020 (0.6091)	loss 1.3819 (1.1840)	grad_norm 1.6824 (2.2075)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 13:56:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:21:25 lr 0.000017	 wd 0.0500	time 0.4920 (0.5584)	loss 1.2906 (1.1939)	grad_norm 1.7219 (2.5260)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 13:57:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:19:52 lr 0.000017	 wd 0.0500	time 0.4923 (0.5415)	loss 0.9312 (1.1861)	grad_norm 1.4874 (2.3900)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 13:58:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:18:41 lr 0.000017	 wd 0.0500	time 0.5038 (0.5334)	loss 1.4499 (1.1886)	grad_norm 2.5288 (2.3513)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 13:58:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:17:38 lr 0.000017	 wd 0.0500	time 0.5040 (0.5287)	loss 0.7825 (1.1875)	grad_norm 1.8527 (2.3090)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 13:59:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:16:39 lr 0.000017	 wd 0.0500	time 0.5019 (0.5255)	loss 1.5678 (1.1893)	grad_norm 2.1881 (2.3362)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:00:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:15:43 lr 0.000017	 wd 0.0500	time 0.4997 (0.5234)	loss 0.9423 (1.1861)	grad_norm 2.0891 (2.3237)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:01:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:14:48 lr 0.000017	 wd 0.0500	time 0.5025 (0.5218)	loss 1.4515 (1.1847)	grad_norm 1.8449 (2.2838)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:02:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:13:53 lr 0.000017	 wd 0.0500	time 0.5056 (0.5206)	loss 0.9169 (1.1870)	grad_norm 1.5202 (2.2927)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:03:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:13:00 lr 0.000017	 wd 0.0500	time 0.5009 (0.5196)	loss 1.2850 (1.1880)	grad_norm 1.7177 (2.2711)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:04:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:12:07 lr 0.000017	 wd 0.0500	time 0.5023 (0.5188)	loss 0.9193 (1.1860)	grad_norm 2.0609 (2.2721)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:04:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:11:14 lr 0.000017	 wd 0.0500	time 0.5028 (0.5181)	loss 1.0259 (1.1820)	grad_norm 1.5967 (2.2490)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:05:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:10:22 lr 0.000017	 wd 0.0500	time 0.4966 (0.5176)	loss 1.2813 (1.1824)	grad_norm 1.8905 (2.2530)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:06:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:09:29 lr 0.000017	 wd 0.0500	time 0.5047 (0.5171)	loss 0.9109 (1.1833)	grad_norm 6.3325 (2.2538)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:07:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:08:37 lr 0.000017	 wd 0.0500	time 0.5034 (0.5167)	loss 1.4248 (1.1828)	grad_norm 1.7190 (2.2351)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:08:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:07:45 lr 0.000017	 wd 0.0500	time 0.5058 (0.5164)	loss 1.3116 (1.1842)	grad_norm 2.6702 (2.2240)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:09:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:06:53 lr 0.000017	 wd 0.0500	time 0.5038 (0.5160)	loss 1.0549 (1.1847)	grad_norm 2.2520 (2.2209)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:09:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:06:02 lr 0.000017	 wd 0.0500	time 0.5001 (0.5157)	loss 0.8934 (1.1838)	grad_norm 1.6451 (2.2246)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:10:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:05:10 lr 0.000017	 wd 0.0500	time 0.4915 (0.5155)	loss 1.0548 (1.1823)	grad_norm 2.1216 (2.2246)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:11:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:04:18 lr 0.000017	 wd 0.0500	time 0.5030 (0.5152)	loss 1.2591 (1.1827)	grad_norm 1.2143 (2.2333)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:12:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:03:27 lr 0.000017	 wd 0.0500	time 0.5042 (0.5150)	loss 1.4325 (1.1823)	grad_norm 1.9358 (2.2574)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:13:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:02:35 lr 0.000017	 wd 0.0500	time 0.5075 (0.5148)	loss 0.8980 (1.1836)	grad_norm 2.1482 (2.2514)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:14:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:43 lr 0.000016	 wd 0.0500	time 0.5044 (0.5146)	loss 1.1906 (1.1820)	grad_norm 2.3563 (2.2464)	loss_scale 4096.0000 (2078.2616)	mem 19076MB
[2024-07-31 14:15:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:52 lr 0.000016	 wd 0.0500	time 0.5023 (0.5144)	loss 1.3282 (1.1833)	grad_norm 2.5127 (2.2451)	loss_scale 4096.0000 (2162.2990)	mem 19076MB
[2024-07-31 14:15:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:01 lr 0.000016	 wd 0.0500	time 0.5021 (0.5143)	loss 0.8673 (1.1812)	grad_norm 1.7947 (2.2428)	loss_scale 4096.0000 (2239.6162)	mem 19076MB
[2024-07-31 14:15:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 11 training takes 0:21:29
[2024-07-31 14:16:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.163 (12.163)	Loss 0.5171 (0.5171)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 14:16:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.532 Acc@5 97.632
[2024-07-31 14:16:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.5%
[2024-07-31 14:16:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.56%
[2024-07-31 14:16:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:24:53 lr 0.000016	 wd 0.0500	time 12.1076 (12.1076)	loss 0.9447 (0.9447)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 14:17:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:24:52 lr 0.000016	 wd 0.0500	time 0.5008 (0.6214)	loss 1.3211 (1.1778)	grad_norm 1.5905 (1.9993)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 14:18:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:21:39 lr 0.000016	 wd 0.0500	time 0.4952 (0.5647)	loss 1.2385 (1.1759)	grad_norm 1.9266 (nan)	loss_scale 2048.0000 (3505.0348)	mem 19076MB
[2024-07-31 14:19:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:20:01 lr 0.000016	 wd 0.0500	time 0.4989 (0.5458)	loss 0.9355 (1.1867)	grad_norm 1.8136 (nan)	loss_scale 2048.0000 (3020.9701)	mem 19076MB
[2024-07-31 14:20:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:18:47 lr 0.000016	 wd 0.0500	time 0.5043 (0.5366)	loss 1.2599 (1.1808)	grad_norm 1.6141 (nan)	loss_scale 2048.0000 (2778.3342)	mem 19076MB
[2024-07-31 14:20:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:17:43 lr 0.000016	 wd 0.0500	time 0.5045 (0.5313)	loss 1.4237 (1.1790)	grad_norm 1.1672 (nan)	loss_scale 2048.0000 (2632.5589)	mem 19076MB
[2024-07-31 14:21:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:16:43 lr 0.000016	 wd 0.0500	time 0.5019 (0.5278)	loss 1.4294 (1.1766)	grad_norm 3.1698 (nan)	loss_scale 2048.0000 (2535.2945)	mem 19076MB
[2024-07-31 14:22:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:15:46 lr 0.000016	 wd 0.0500	time 0.4926 (0.5254)	loss 1.3047 (1.1747)	grad_norm 2.2276 (nan)	loss_scale 2048.0000 (2465.7803)	mem 19076MB
[2024-07-31 14:23:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:14:51 lr 0.000016	 wd 0.0500	time 0.5025 (0.5235)	loss 1.4070 (1.1759)	grad_norm 1.5611 (nan)	loss_scale 2048.0000 (2413.6230)	mem 19076MB
[2024-07-31 14:24:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:13:56 lr 0.000016	 wd 0.0500	time 0.4952 (0.5221)	loss 1.4231 (1.1730)	grad_norm 1.6553 (nan)	loss_scale 2048.0000 (2373.0433)	mem 19076MB
[2024-07-31 14:25:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:13:02 lr 0.000016	 wd 0.0500	time 0.5028 (0.5210)	loss 1.3058 (1.1775)	grad_norm 1.5810 (nan)	loss_scale 2048.0000 (2340.5714)	mem 19076MB
[2024-07-31 14:26:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:12:09 lr 0.000016	 wd 0.0500	time 0.4964 (0.5201)	loss 0.9617 (1.1795)	grad_norm 1.8933 (nan)	loss_scale 2048.0000 (2313.9982)	mem 19076MB
[2024-07-31 14:26:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:11:16 lr 0.000016	 wd 0.0500	time 0.5011 (0.5193)	loss 1.3513 (1.1779)	grad_norm 1.9797 (nan)	loss_scale 2048.0000 (2291.8501)	mem 19076MB
[2024-07-31 14:27:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:10:23 lr 0.000016	 wd 0.0500	time 0.5050 (0.5187)	loss 1.0654 (1.1795)	grad_norm 1.4289 (nan)	loss_scale 2048.0000 (2273.1068)	mem 19076MB
[2024-07-31 14:28:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:09:30 lr 0.000016	 wd 0.0500	time 0.5036 (0.5181)	loss 1.4404 (1.1804)	grad_norm 1.5838 (nan)	loss_scale 2048.0000 (2257.0393)	mem 19076MB
[2024-07-31 14:29:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:08:38 lr 0.000016	 wd 0.0500	time 0.5048 (0.5177)	loss 0.8525 (1.1771)	grad_norm 1.7965 (nan)	loss_scale 2048.0000 (2243.1126)	mem 19076MB
[2024-07-31 14:30:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:07:46 lr 0.000016	 wd 0.0500	time 0.5021 (0.5173)	loss 1.4573 (1.1746)	grad_norm 2.1735 (nan)	loss_scale 2048.0000 (2230.9257)	mem 19076MB
[2024-07-31 14:31:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:06:54 lr 0.000016	 wd 0.0500	time 0.5006 (0.5169)	loss 1.2302 (1.1771)	grad_norm 2.7703 (nan)	loss_scale 2048.0000 (2220.1717)	mem 19076MB
[2024-07-31 14:32:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:06:02 lr 0.000016	 wd 0.0500	time 0.5004 (0.5165)	loss 0.8327 (1.1760)	grad_norm 2.0892 (nan)	loss_scale 2048.0000 (2210.6119)	mem 19076MB
[2024-07-31 14:32:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:05:10 lr 0.000016	 wd 0.0500	time 0.5026 (0.5162)	loss 1.2164 (1.1763)	grad_norm 1.4660 (nan)	loss_scale 2048.0000 (2202.0579)	mem 19076MB
[2024-07-31 14:33:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:04:19 lr 0.000016	 wd 0.0500	time 0.5008 (0.5159)	loss 1.0498 (1.1773)	grad_norm 2.0687 (nan)	loss_scale 2048.0000 (2194.3588)	mem 19076MB
[2024-07-31 14:34:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:03:27 lr 0.000016	 wd 0.0500	time 0.4994 (0.5157)	loss 0.8423 (1.1764)	grad_norm 6.1508 (nan)	loss_scale 2048.0000 (2187.3927)	mem 19076MB
[2024-07-31 14:35:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:02:35 lr 0.000016	 wd 0.0500	time 0.5054 (0.5155)	loss 0.8102 (1.1778)	grad_norm 2.6795 (nan)	loss_scale 2048.0000 (2181.0595)	mem 19076MB
[2024-07-31 14:36:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:44 lr 0.000015	 wd 0.0500	time 0.5015 (0.5153)	loss 1.0847 (1.1783)	grad_norm 3.6286 (nan)	loss_scale 2048.0000 (2175.2768)	mem 19076MB
[2024-07-31 14:37:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:52 lr 0.000015	 wd 0.0500	time 0.5032 (0.5151)	loss 1.3908 (1.1789)	grad_norm 1.4289 (nan)	loss_scale 2048.0000 (2169.9758)	mem 19076MB
[2024-07-31 14:37:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:01 lr 0.000015	 wd 0.0500	time 0.5044 (0.5149)	loss 0.8150 (1.1783)	grad_norm 1.9313 (nan)	loss_scale 2048.0000 (2165.0988)	mem 19076MB
[2024-07-31 14:38:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 12 training takes 0:21:30
[2024-07-31 14:38:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.563 (11.563)	Loss 0.5337 (0.5337)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 14:38:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.654 Acc@5 97.638
[2024-07-31 14:38:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 14:38:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.65%
[2024-07-31 14:38:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 14:38:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 14:38:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:08:40 lr 0.000015	 wd 0.0500	time 10.2799 (10.2799)	loss 1.3375 (1.3375)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:39:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:24:24 lr 0.000015	 wd 0.0500	time 0.4964 (0.6096)	loss 1.4520 (1.2404)	grad_norm 1.0465 (2.0680)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:40:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:21:25 lr 0.000015	 wd 0.0500	time 0.4990 (0.5586)	loss 1.4396 (1.2253)	grad_norm 1.4464 (2.4355)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:41:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:19:53 lr 0.000015	 wd 0.0500	time 0.5033 (0.5418)	loss 1.4478 (1.2250)	grad_norm 2.4085 (2.2558)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:42:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:18:41 lr 0.000015	 wd 0.0500	time 0.5025 (0.5337)	loss 1.4663 (1.2080)	grad_norm 2.4044 (2.2346)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:43:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:17:38 lr 0.000015	 wd 0.0500	time 0.5013 (0.5288)	loss 1.3962 (1.2009)	grad_norm 1.8956 (2.2648)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:43:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:16:39 lr 0.000015	 wd 0.0500	time 0.4986 (0.5257)	loss 1.2850 (1.1944)	grad_norm 2.1228 (2.3098)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:44:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:15:43 lr 0.000015	 wd 0.0500	time 0.5042 (0.5235)	loss 1.0972 (1.1875)	grad_norm 1.7378 (2.2880)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:45:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:14:48 lr 0.000015	 wd 0.0500	time 0.5006 (0.5219)	loss 1.4762 (1.1873)	grad_norm 20.0186 (2.3145)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:46:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:13:54 lr 0.000015	 wd 0.0500	time 0.5031 (0.5207)	loss 1.1815 (1.1870)	grad_norm 1.5080 (2.2787)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:47:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:13:00 lr 0.000015	 wd 0.0500	time 0.5038 (0.5197)	loss 1.3187 (1.1861)	grad_norm 4.7500 (2.2729)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:48:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:12:07 lr 0.000015	 wd 0.0500	time 0.5069 (0.5189)	loss 1.3790 (1.1824)	grad_norm 1.5251 (2.2524)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:48:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:11:14 lr 0.000015	 wd 0.0500	time 0.5030 (0.5182)	loss 1.2989 (1.1865)	grad_norm 2.4377 (2.2353)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:49:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:10:22 lr 0.000015	 wd 0.0500	time 0.5012 (0.5176)	loss 0.8720 (1.1862)	grad_norm 1.9861 (2.2405)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:50:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:09:29 lr 0.000015	 wd 0.0500	time 0.5036 (0.5171)	loss 1.4468 (1.1848)	grad_norm 3.5624 (2.2456)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:51:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:08:37 lr 0.000015	 wd 0.0500	time 0.5036 (0.5167)	loss 1.2242 (1.1850)	grad_norm 2.4268 (2.2670)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:52:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:07:45 lr 0.000015	 wd 0.0500	time 0.5046 (0.5163)	loss 1.0108 (1.1847)	grad_norm 1.4585 (2.2709)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 14:53:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:06:53 lr 0.000015	 wd 0.0500	time 0.5050 (0.5160)	loss 1.1120 (1.1855)	grad_norm 1.7503 (2.2985)	loss_scale 4096.0000 (2120.2399)	mem 19076MB
[2024-07-31 14:54:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:06:02 lr 0.000015	 wd 0.0500	time 0.4992 (0.5157)	loss 1.4406 (1.1858)	grad_norm 9.3125 (2.2967)	loss_scale 4096.0000 (2229.9434)	mem 19076MB
[2024-07-31 14:54:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:05:10 lr 0.000015	 wd 0.0500	time 0.5064 (0.5154)	loss 1.1256 (1.1870)	grad_norm 2.0238 (2.3031)	loss_scale 4096.0000 (2328.1052)	mem 19076MB
[2024-07-31 14:55:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:04:18 lr 0.000015	 wd 0.0500	time 0.5027 (0.5152)	loss 1.3964 (1.1866)	grad_norm 1.7994 (2.3088)	loss_scale 4096.0000 (2416.4558)	mem 19076MB
[2024-07-31 14:56:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:03:27 lr 0.000014	 wd 0.0500	time 0.5035 (0.5150)	loss 1.4795 (1.1870)	grad_norm 1.8602 (2.3017)	loss_scale 4096.0000 (2496.3960)	mem 19076MB
[2024-07-31 14:57:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:02:35 lr 0.000014	 wd 0.0500	time 0.5050 (0.5148)	loss 1.2154 (1.1854)	grad_norm 3.7580 (2.2904)	loss_scale 4096.0000 (2569.0722)	mem 19076MB
[2024-07-31 14:58:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:43 lr 0.000014	 wd 0.0500	time 0.5034 (0.5146)	loss 1.3837 (1.1850)	grad_norm 1.9332 (2.2821)	loss_scale 4096.0000 (2635.4316)	mem 19076MB
[2024-07-31 14:59:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:52 lr 0.000014	 wd 0.0500	time 0.5019 (0.5145)	loss 0.9037 (1.1881)	grad_norm 2.0181 (2.2705)	loss_scale 4096.0000 (2696.2632)	mem 19076MB
[2024-07-31 15:00:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:01 lr 0.000014	 wd 0.0500	time 0.5035 (0.5143)	loss 0.8489 (1.1861)	grad_norm 1.6710 (2.2566)	loss_scale 4096.0000 (2752.2303)	mem 19076MB
[2024-07-31 15:00:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 13 training takes 0:21:29
[2024-07-31 15:00:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.712 (12.712)	Loss 0.5059 (0.5059)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 15:00:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.658 Acc@5 97.652
[2024-07-31 15:00:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 15:00:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.66%
[2024-07-31 15:00:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 15:00:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 15:00:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:55:08 lr 0.000014	 wd 0.0500	time 11.3942 (11.3942)	loss 1.3430 (1.3430)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 15:01:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:24:40 lr 0.000014	 wd 0.0500	time 0.4968 (0.6163)	loss 1.1057 (1.1932)	grad_norm 1.8539 (2.5290)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 15:02:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:21:33 lr 0.000014	 wd 0.0500	time 0.4965 (0.5620)	loss 1.4040 (1.1981)	grad_norm 1.4819 (2.3036)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 15:03:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:19:57 lr 0.000014	 wd 0.0500	time 0.4898 (0.5439)	loss 0.9757 (1.1999)	grad_norm 2.6315 (inf)	loss_scale 2048.0000 (3456.4252)	mem 19076MB
[2024-07-31 15:04:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:18:44 lr 0.000014	 wd 0.0500	time 0.5056 (0.5352)	loss 1.6585 (1.2006)	grad_norm 2.1398 (inf)	loss_scale 2048.0000 (3105.1970)	mem 19076MB
[2024-07-31 15:05:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:17:41 lr 0.000014	 wd 0.0500	time 0.5013 (0.5301)	loss 0.9851 (1.1957)	grad_norm 1.5966 (inf)	loss_scale 2048.0000 (2894.1796)	mem 19076MB
[2024-07-31 15:05:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:16:42 lr 0.000014	 wd 0.0500	time 0.5027 (0.5269)	loss 0.9991 (1.1926)	grad_norm 1.6985 (inf)	loss_scale 2048.0000 (2753.3844)	mem 19076MB
[2024-07-31 15:06:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:15:45 lr 0.000014	 wd 0.0500	time 0.4988 (0.5245)	loss 1.2475 (1.1947)	grad_norm 2.0151 (inf)	loss_scale 2048.0000 (2652.7589)	mem 19076MB
[2024-07-31 15:07:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:14:49 lr 0.000014	 wd 0.0500	time 0.5063 (0.5228)	loss 0.7762 (1.1889)	grad_norm 2.1571 (inf)	loss_scale 2048.0000 (2577.2584)	mem 19076MB
[2024-07-31 15:08:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:13:55 lr 0.000014	 wd 0.0500	time 0.5071 (0.5214)	loss 0.8274 (1.1877)	grad_norm 1.9970 (inf)	loss_scale 2048.0000 (2518.5172)	mem 19076MB
[2024-07-31 15:09:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:13:01 lr 0.000014	 wd 0.0500	time 0.5062 (0.5204)	loss 1.0291 (1.1853)	grad_norm 3.2948 (inf)	loss_scale 2048.0000 (2471.5125)	mem 19076MB
[2024-07-31 15:10:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:12:08 lr 0.000014	 wd 0.0500	time 0.5013 (0.5195)	loss 1.3934 (1.1859)	grad_norm 1.5933 (inf)	loss_scale 2048.0000 (2433.0463)	mem 19076MB
[2024-07-31 15:11:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:11:15 lr 0.000014	 wd 0.0500	time 0.5005 (0.5188)	loss 1.5506 (1.1857)	grad_norm 1.6737 (inf)	loss_scale 2048.0000 (2400.9858)	mem 19076MB
[2024-07-31 15:11:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:10:22 lr 0.000014	 wd 0.0500	time 0.5032 (0.5182)	loss 1.2332 (1.1894)	grad_norm 1.8458 (inf)	loss_scale 2048.0000 (2373.8540)	mem 19076MB
[2024-07-31 15:12:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:09:30 lr 0.000014	 wd 0.0500	time 0.5035 (0.5177)	loss 1.3592 (1.1897)	grad_norm 6.8225 (inf)	loss_scale 2048.0000 (2350.5953)	mem 19076MB
[2024-07-31 15:13:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:08:38 lr 0.000014	 wd 0.0500	time 0.4940 (0.5172)	loss 1.4583 (1.1885)	grad_norm 1.9956 (inf)	loss_scale 2048.0000 (2330.4357)	mem 19076MB
[2024-07-31 15:14:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:07:46 lr 0.000014	 wd 0.0500	time 0.5057 (0.5168)	loss 0.9271 (1.1874)	grad_norm 1.7005 (inf)	loss_scale 2048.0000 (2312.7945)	mem 19076MB
[2024-07-31 15:15:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:06:54 lr 0.000014	 wd 0.0500	time 0.4984 (0.5165)	loss 0.9043 (1.1866)	grad_norm 8.4475 (inf)	loss_scale 2048.0000 (2297.2275)	mem 19076MB
[2024-07-31 15:16:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:06:02 lr 0.000013	 wd 0.0500	time 0.5040 (0.5161)	loss 1.0670 (1.1876)	grad_norm 1.9526 (inf)	loss_scale 2048.0000 (2283.3892)	mem 19076MB
[2024-07-31 15:16:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:05:10 lr 0.000013	 wd 0.0500	time 0.4878 (0.5158)	loss 1.2661 (1.1885)	grad_norm 2.7392 (inf)	loss_scale 2048.0000 (2271.0068)	mem 19076MB
[2024-07-31 15:17:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:04:18 lr 0.000013	 wd 0.0500	time 0.5005 (0.5155)	loss 1.4252 (1.1870)	grad_norm 1.6347 (inf)	loss_scale 2048.0000 (2259.8621)	mem 19076MB
[2024-07-31 15:18:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:03:27 lr 0.000013	 wd 0.0500	time 0.4990 (0.5153)	loss 1.3122 (1.1862)	grad_norm 2.0511 (inf)	loss_scale 2048.0000 (2249.7782)	mem 19076MB
[2024-07-31 15:19:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:02:35 lr 0.000013	 wd 0.0500	time 0.5042 (0.5151)	loss 1.2699 (1.1853)	grad_norm 2.6688 (inf)	loss_scale 2048.0000 (2240.6106)	mem 19076MB
[2024-07-31 15:20:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:44 lr 0.000013	 wd 0.0500	time 0.5061 (0.5149)	loss 0.8497 (1.1847)	grad_norm 1.7482 (inf)	loss_scale 2048.0000 (2232.2399)	mem 19076MB
[2024-07-31 15:21:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:52 lr 0.000013	 wd 0.0500	time 0.5046 (0.5147)	loss 0.8352 (1.1841)	grad_norm 1.8754 (inf)	loss_scale 2048.0000 (2224.5664)	mem 19076MB
[2024-07-31 15:22:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:01 lr 0.000013	 wd 0.0500	time 0.5056 (0.5146)	loss 1.3500 (1.1832)	grad_norm 1.3820 (inf)	loss_scale 2048.0000 (2217.5066)	mem 19076MB
[2024-07-31 15:22:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 14 training takes 0:21:29
[2024-07-31 15:22:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.279 (12.279)	Loss 0.4951 (0.4951)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 15:22:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.676 Acc@5 97.664
[2024-07-31 15:22:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 15:22:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-31 15:22:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 15:22:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 15:22:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][0/2502]	eta 8:16:50 lr 0.000013	 wd 0.0500	time 11.9147 (11.9147)	loss 1.3168 (1.3168)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:23:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:24:47 lr 0.000013	 wd 0.0500	time 0.4871 (0.6192)	loss 0.8105 (1.1803)	grad_norm 1.9246 (2.3010)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:24:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:21:37 lr 0.000013	 wd 0.0500	time 0.5016 (0.5636)	loss 1.5571 (1.1698)	grad_norm 2.1434 (2.2251)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:25:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:20:00 lr 0.000013	 wd 0.0500	time 0.4948 (0.5450)	loss 0.9263 (1.1683)	grad_norm 1.7300 (2.1708)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:26:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:18:46 lr 0.000013	 wd 0.0500	time 0.4989 (0.5359)	loss 0.7898 (1.1670)	grad_norm 1.3935 (2.1262)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:27:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:17:42 lr 0.000013	 wd 0.0500	time 0.5022 (0.5306)	loss 0.7668 (1.1770)	grad_norm 1.4260 (2.1407)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:27:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:16:42 lr 0.000013	 wd 0.0500	time 0.5031 (0.5271)	loss 1.3079 (1.1819)	grad_norm 1.9515 (2.2471)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:28:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:15:45 lr 0.000013	 wd 0.0500	time 0.5038 (0.5247)	loss 1.3728 (1.1839)	grad_norm 1.8359 (2.2743)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:29:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:14:50 lr 0.000013	 wd 0.0500	time 0.5057 (0.5230)	loss 1.2027 (1.1848)	grad_norm 1.8363 (2.2720)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:30:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:13:55 lr 0.000013	 wd 0.0500	time 0.5051 (0.5217)	loss 1.3848 (1.1863)	grad_norm 1.8679 (2.3119)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:31:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:13:01 lr 0.000013	 wd 0.0500	time 0.5053 (0.5206)	loss 0.7902 (1.1861)	grad_norm 2.1749 (2.3182)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:32:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:12:08 lr 0.000013	 wd 0.0500	time 0.4990 (0.5197)	loss 0.8832 (1.1868)	grad_norm 1.8891 (2.2996)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:33:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:11:15 lr 0.000013	 wd 0.0500	time 0.5045 (0.5190)	loss 0.8947 (1.1868)	grad_norm 2.3803 (2.3369)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:33:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:10:23 lr 0.000013	 wd 0.0500	time 0.5048 (0.5185)	loss 0.9453 (1.1879)	grad_norm 1.6301 (2.3292)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:34:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:09:30 lr 0.000012	 wd 0.0500	time 0.5040 (0.5179)	loss 0.8511 (1.1878)	grad_norm 1.5993 (2.3093)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:35:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:08:38 lr 0.000012	 wd 0.0500	time 0.4979 (0.5175)	loss 1.0912 (1.1836)	grad_norm 1.6841 (2.3218)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:36:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:07:46 lr 0.000012	 wd 0.0500	time 0.4997 (0.5171)	loss 0.9417 (1.1845)	grad_norm 2.2429 (2.3086)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:37:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:06:54 lr 0.000012	 wd 0.0500	time 0.5029 (0.5167)	loss 0.7810 (1.1850)	grad_norm 1.6356 (2.2939)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:38:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:06:02 lr 0.000012	 wd 0.0500	time 0.5038 (0.5164)	loss 1.6854 (1.1833)	grad_norm 1.8550 (2.2920)	loss_scale 4096.0000 (2157.1660)	mem 19076MB
[2024-07-31 15:39:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:05:10 lr 0.000012	 wd 0.0500	time 0.4965 (0.5161)	loss 0.9523 (1.1822)	grad_norm 1.6210 (inf)	loss_scale 2048.0000 (2192.3619)	mem 19076MB
[2024-07-31 15:39:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:04:18 lr 0.000012	 wd 0.0500	time 0.5040 (0.5158)	loss 0.9692 (1.1833)	grad_norm 1.6343 (inf)	loss_scale 2048.0000 (2185.1474)	mem 19076MB
[2024-07-31 15:40:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:03:27 lr 0.000012	 wd 0.0500	time 0.4930 (0.5156)	loss 0.7719 (1.1820)	grad_norm 2.1545 (inf)	loss_scale 2048.0000 (2178.6197)	mem 19076MB
[2024-07-31 15:41:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:02:35 lr 0.000012	 wd 0.0500	time 0.5001 (0.5154)	loss 0.9208 (1.1815)	grad_norm 1.6857 (inf)	loss_scale 2048.0000 (2172.6851)	mem 19076MB
[2024-07-31 15:42:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:44 lr 0.000012	 wd 0.0500	time 0.5041 (0.5152)	loss 1.0339 (1.1821)	grad_norm 2.5341 (inf)	loss_scale 2048.0000 (2167.2664)	mem 19076MB
[2024-07-31 15:43:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:52 lr 0.000012	 wd 0.0500	time 0.5076 (0.5150)	loss 1.3806 (1.1823)	grad_norm 1.5685 (inf)	loss_scale 2048.0000 (2162.2990)	mem 19076MB
[2024-07-31 15:44:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:01 lr 0.000012	 wd 0.0500	time 0.4970 (0.5148)	loss 1.5102 (1.1833)	grad_norm 1.7206 (inf)	loss_scale 2048.0000 (2157.7289)	mem 19076MB
[2024-07-31 15:44:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 15 training takes 0:21:30
[2024-07-31 15:44:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_15.pth saving......
[2024-07-31 15:44:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_15.pth saved !!!
[2024-07-31 15:44:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.630 (11.630)	Loss 0.5479 (0.5479)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 15:44:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.614 Acc@5 97.660
[2024-07-31 15:44:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.6%
[2024-07-31 15:44:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-31 15:44:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:35:15 lr 0.000012	 wd 0.0500	time 12.3562 (12.3562)	loss 1.2471 (1.2471)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:45:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:24:57 lr 0.000012	 wd 0.0500	time 0.5000 (0.6236)	loss 1.1620 (1.1926)	grad_norm 1.4897 (2.5800)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:46:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:21:42 lr 0.000012	 wd 0.0500	time 0.5010 (0.5659)	loss 1.2538 (1.1826)	grad_norm 1.7858 (2.4892)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:47:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:20:03 lr 0.000012	 wd 0.0500	time 0.5039 (0.5466)	loss 1.0682 (1.1765)	grad_norm 1.6963 (2.3662)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:48:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:18:49 lr 0.000012	 wd 0.0500	time 0.5046 (0.5372)	loss 0.8611 (1.1657)	grad_norm 1.9008 (2.3771)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:49:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:17:44 lr 0.000012	 wd 0.0500	time 0.5066 (0.5317)	loss 1.4908 (1.1702)	grad_norm 1.7152 (2.4057)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:50:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:16:44 lr 0.000012	 wd 0.0500	time 0.4970 (0.5281)	loss 1.4149 (1.1686)	grad_norm 1.7562 (2.3849)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:50:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:15:46 lr 0.000012	 wd 0.0500	time 0.5062 (0.5255)	loss 1.0338 (1.1706)	grad_norm 1.5266 (2.3506)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 15:51:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:14:51 lr 0.000012	 wd 0.0500	time 0.5054 (0.5236)	loss 1.3665 (1.1725)	grad_norm 2.0496 (nan)	loss_scale 1024.0000 (2001.9775)	mem 19076MB
[2024-07-31 15:52:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:13:56 lr 0.000012	 wd 0.0500	time 0.4966 (0.5221)	loss 1.1070 (1.1706)	grad_norm 1.3099 (nan)	loss_scale 1024.0000 (1893.4340)	mem 19076MB
[2024-07-31 15:53:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:13:02 lr 0.000011	 wd 0.0500	time 0.5028 (0.5209)	loss 1.3510 (1.1716)	grad_norm 1.2920 (nan)	loss_scale 1024.0000 (1806.5774)	mem 19076MB
[2024-07-31 15:54:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:12:09 lr 0.000011	 wd 0.0500	time 0.5005 (0.5200)	loss 0.9146 (1.1770)	grad_norm 1.3873 (nan)	loss_scale 1024.0000 (1735.4986)	mem 19076MB
[2024-07-31 15:55:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:11:16 lr 0.000011	 wd 0.0500	time 0.5080 (0.5192)	loss 0.9234 (1.1765)	grad_norm 2.7547 (nan)	loss_scale 1024.0000 (1676.2565)	mem 19076MB
[2024-07-31 15:56:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:10:23 lr 0.000011	 wd 0.0500	time 0.4919 (0.5186)	loss 1.3282 (1.1799)	grad_norm 1.5682 (nan)	loss_scale 1024.0000 (1626.1214)	mem 19076MB
[2024-07-31 15:56:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:09:30 lr 0.000011	 wd 0.0500	time 0.5015 (0.5180)	loss 1.2574 (1.1806)	grad_norm 1.5629 (nan)	loss_scale 1024.0000 (1583.1435)	mem 19076MB
[2024-07-31 15:57:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:08:38 lr 0.000011	 wd 0.0500	time 0.5043 (0.5175)	loss 1.2071 (1.1826)	grad_norm 5.3528 (nan)	loss_scale 1024.0000 (1545.8921)	mem 19076MB
[2024-07-31 15:58:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:07:46 lr 0.000011	 wd 0.0500	time 0.5025 (0.5170)	loss 1.2735 (1.1828)	grad_norm 1.7390 (nan)	loss_scale 1024.0000 (1513.2942)	mem 19076MB
[2024-07-31 15:59:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:06:54 lr 0.000011	 wd 0.0500	time 0.5037 (0.5166)	loss 1.0998 (1.1822)	grad_norm 1.7970 (nan)	loss_scale 1024.0000 (1484.5291)	mem 19076MB
[2024-07-31 16:00:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:06:02 lr 0.000011	 wd 0.0500	time 0.5015 (0.5163)	loss 0.9263 (1.1811)	grad_norm 1.6940 (nan)	loss_scale 1024.0000 (1458.9584)	mem 19076MB
[2024-07-31 16:01:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:05:10 lr 0.000011	 wd 0.0500	time 0.5026 (0.5160)	loss 0.9162 (1.1817)	grad_norm 2.4162 (nan)	loss_scale 1024.0000 (1436.0779)	mem 19076MB
[2024-07-31 16:01:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:04:18 lr 0.000011	 wd 0.0500	time 0.5012 (0.5157)	loss 1.3411 (1.1823)	grad_norm 2.1844 (nan)	loss_scale 1024.0000 (1415.4843)	mem 19076MB
[2024-07-31 16:02:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:03:27 lr 0.000011	 wd 0.0500	time 0.5040 (0.5154)	loss 1.2678 (1.1829)	grad_norm 1.6230 (nan)	loss_scale 1024.0000 (1396.8510)	mem 19076MB
[2024-07-31 16:03:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:02:35 lr 0.000011	 wd 0.0500	time 0.5032 (0.5152)	loss 0.9401 (1.1825)	grad_norm 5.5511 (nan)	loss_scale 1024.0000 (1379.9109)	mem 19076MB
[2024-07-31 16:04:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:44 lr 0.000011	 wd 0.0500	time 0.5042 (0.5150)	loss 0.8398 (1.1806)	grad_norm 2.8448 (nan)	loss_scale 1024.0000 (1364.4433)	mem 19076MB
[2024-07-31 16:05:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:52 lr 0.000011	 wd 0.0500	time 0.5056 (0.5149)	loss 1.2935 (1.1800)	grad_norm 1.6859 (nan)	loss_scale 1024.0000 (1350.2641)	mem 19076MB
[2024-07-31 16:06:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:01 lr 0.000011	 wd 0.0500	time 0.5042 (0.5147)	loss 0.8475 (1.1795)	grad_norm 1.8650 (nan)	loss_scale 1024.0000 (1337.2187)	mem 19076MB
[2024-07-31 16:06:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 16 training takes 0:21:30
[2024-07-31 16:06:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 13.219 (13.219)	Loss 0.5166 (0.5166)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 16:06:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.670 Acc@5 97.674
[2024-07-31 16:06:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 16:06:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.68%
[2024-07-31 16:07:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:57:09 lr 0.000011	 wd 0.0500	time 11.4427 (11.4427)	loss 1.3174 (1.3174)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:07:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:24:43 lr 0.000011	 wd 0.0500	time 0.4886 (0.6176)	loss 1.3108 (1.1541)	grad_norm 1.6011 (2.0252)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:08:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:21:35 lr 0.000011	 wd 0.0500	time 0.5019 (0.5626)	loss 1.3882 (1.1614)	grad_norm 1.4970 (2.0870)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:09:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:19:58 lr 0.000011	 wd 0.0500	time 0.4980 (0.5443)	loss 1.4279 (1.1757)	grad_norm 3.6614 (2.2614)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:10:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:18:45 lr 0.000011	 wd 0.0500	time 0.5000 (0.5354)	loss 1.5470 (1.1713)	grad_norm 1.8759 (2.5821)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:11:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:17:41 lr 0.000010	 wd 0.0500	time 0.4958 (0.5301)	loss 1.3596 (1.1780)	grad_norm 1.6668 (2.6033)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:12:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:16:41 lr 0.000010	 wd 0.0500	time 0.5087 (0.5267)	loss 0.8958 (1.1794)	grad_norm 2.2935 (2.4975)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:12:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:15:44 lr 0.000010	 wd 0.0500	time 0.5049 (0.5243)	loss 1.3670 (1.1766)	grad_norm 1.3975 (2.5216)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:13:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:14:49 lr 0.000010	 wd 0.0500	time 0.4948 (0.5226)	loss 1.2346 (1.1762)	grad_norm 2.6039 (2.4717)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:14:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:13:55 lr 0.000010	 wd 0.0500	time 0.5046 (0.5213)	loss 1.3482 (1.1783)	grad_norm 1.5275 (2.4613)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:15:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:13:01 lr 0.000010	 wd 0.0500	time 0.5073 (0.5202)	loss 0.9977 (1.1767)	grad_norm 1.9582 (2.4638)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:16:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:12:08 lr 0.000010	 wd 0.0500	time 0.5045 (0.5194)	loss 1.0044 (1.1818)	grad_norm 1.6042 (2.4607)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:17:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:11:15 lr 0.000010	 wd 0.0500	time 0.5036 (0.5187)	loss 0.8776 (1.1815)	grad_norm 2.2318 (2.4451)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:18:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:10:22 lr 0.000010	 wd 0.0500	time 0.5040 (0.5181)	loss 1.5229 (1.1801)	grad_norm 1.5344 (2.4200)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:18:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:09:30 lr 0.000010	 wd 0.0500	time 0.4938 (0.5176)	loss 1.4451 (1.1805)	grad_norm 1.5117 (2.5202)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:19:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:08:38 lr 0.000010	 wd 0.0500	time 0.5032 (0.5171)	loss 1.3956 (1.1806)	grad_norm 1.7000 (2.5050)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:20:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:07:46 lr 0.000010	 wd 0.0500	time 0.5041 (0.5167)	loss 1.3567 (1.1790)	grad_norm 1.5437 (2.4827)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:21:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:06:54 lr 0.000010	 wd 0.0500	time 0.5028 (0.5164)	loss 1.5715 (1.1810)	grad_norm 2.6456 (2.5061)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:22:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:06:02 lr 0.000010	 wd 0.0500	time 0.4988 (0.5161)	loss 1.0000 (1.1809)	grad_norm 1.6128 (2.5379)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:23:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:05:10 lr 0.000010	 wd 0.0500	time 0.5057 (0.5158)	loss 1.2806 (1.1800)	grad_norm 2.2022 (2.5183)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:24:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:04:18 lr 0.000010	 wd 0.0500	time 0.5042 (0.5155)	loss 1.0103 (1.1808)	grad_norm 2.4261 (2.5029)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:24:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:03:27 lr 0.000010	 wd 0.0500	time 0.5078 (0.5153)	loss 1.4901 (1.1808)	grad_norm 3.2763 (2.5154)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:25:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:02:35 lr 0.000010	 wd 0.0500	time 0.5048 (0.5151)	loss 1.5350 (1.1814)	grad_norm 1.3570 (2.4895)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 16:26:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:44 lr 0.000010	 wd 0.0500	time 0.5056 (0.5149)	loss 0.8582 (1.1808)	grad_norm 1.6069 (2.4768)	loss_scale 2048.0000 (1040.9109)	mem 19076MB
[2024-07-31 16:27:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:52 lr 0.000010	 wd 0.0500	time 0.5021 (0.5148)	loss 0.9224 (1.1795)	grad_norm 1.9297 (2.4512)	loss_scale 2048.0000 (1082.8555)	mem 19076MB
[2024-07-31 16:28:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:01 lr 0.000009	 wd 0.0500	time 0.5039 (0.5146)	loss 1.5463 (1.1796)	grad_norm 2.7392 (2.4356)	loss_scale 2048.0000 (1121.4458)	mem 19076MB
[2024-07-31 16:28:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 17 training takes 0:21:29
[2024-07-31 16:28:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.522 (12.522)	Loss 0.5054 (0.5054)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 16:28:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.694 Acc@5 97.666
[2024-07-31 16:28:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 16:28:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.69%
[2024-07-31 16:28:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 16:28:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 16:29:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:04:21 lr 0.000009	 wd 0.0500	time 11.6152 (11.6152)	loss 1.5743 (1.5743)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:29:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:24:40 lr 0.000009	 wd 0.0500	time 0.5017 (0.6162)	loss 1.5337 (1.1897)	grad_norm 1.3063 (2.0367)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:30:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:21:33 lr 0.000009	 wd 0.0500	time 0.5021 (0.5621)	loss 1.0130 (1.1923)	grad_norm 1.9095 (2.0186)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:31:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:19:57 lr 0.000009	 wd 0.0500	time 0.5041 (0.5440)	loss 1.6192 (1.1944)	grad_norm 1.7295 (2.0444)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:32:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:18:45 lr 0.000009	 wd 0.0500	time 0.4992 (0.5352)	loss 0.7517 (1.1939)	grad_norm 2.8859 (2.1570)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:33:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:17:41 lr 0.000009	 wd 0.0500	time 0.5075 (0.5302)	loss 1.4200 (1.1904)	grad_norm 1.3290 (2.1494)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:34:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:16:42 lr 0.000009	 wd 0.0500	time 0.4977 (0.5268)	loss 1.6197 (1.1843)	grad_norm 1.5821 (2.1629)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:35:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:15:44 lr 0.000009	 wd 0.0500	time 0.4916 (0.5244)	loss 1.4202 (1.1870)	grad_norm 3.7801 (2.1855)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:35:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:14:49 lr 0.000009	 wd 0.0500	time 0.5032 (0.5226)	loss 1.3539 (1.1915)	grad_norm 2.1165 (2.2190)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:36:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:13:55 lr 0.000009	 wd 0.0500	time 0.4966 (0.5213)	loss 0.9827 (1.1900)	grad_norm 1.3921 (2.2286)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:37:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:13:01 lr 0.000009	 wd 0.0500	time 0.4955 (0.5201)	loss 1.4208 (1.1874)	grad_norm 1.8659 (2.2220)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:38:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:12:07 lr 0.000009	 wd 0.0500	time 0.5026 (0.5193)	loss 1.0550 (1.1891)	grad_norm 2.6275 (2.2407)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:39:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:11:15 lr 0.000009	 wd 0.0500	time 0.5078 (0.5185)	loss 0.9938 (1.1834)	grad_norm 1.4863 (2.2634)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:40:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:10:22 lr 0.000009	 wd 0.0500	time 0.5009 (0.5180)	loss 1.5135 (1.1832)	grad_norm 2.2537 (2.2609)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:40:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:09:30 lr 0.000009	 wd 0.0500	time 0.5072 (0.5175)	loss 1.2923 (1.1840)	grad_norm 3.6265 (2.2518)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:41:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:08:38 lr 0.000009	 wd 0.0500	time 0.5031 (0.5170)	loss 1.4097 (1.1844)	grad_norm 1.6307 (2.2417)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:42:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:07:45 lr 0.000009	 wd 0.0500	time 0.5001 (0.5166)	loss 1.3707 (1.1858)	grad_norm 1.7634 (2.2482)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:43:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:06:54 lr 0.000009	 wd 0.0500	time 0.5073 (0.5163)	loss 1.3018 (1.1871)	grad_norm 1.6112 (2.2583)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:44:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:06:02 lr 0.000009	 wd 0.0500	time 0.5065 (0.5160)	loss 0.7816 (1.1874)	grad_norm 2.3316 (2.2506)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:45:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:05:10 lr 0.000009	 wd 0.0500	time 0.5027 (0.5157)	loss 1.5577 (1.1873)	grad_norm 1.4727 (2.2492)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:46:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:04:18 lr 0.000008	 wd 0.0500	time 0.5009 (0.5155)	loss 1.0206 (1.1868)	grad_norm 2.9126 (2.2381)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:46:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:03:27 lr 0.000008	 wd 0.0500	time 0.5043 (0.5153)	loss 1.3240 (1.1863)	grad_norm 1.6926 (2.2309)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:47:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:02:35 lr 0.000008	 wd 0.0500	time 0.5085 (0.5151)	loss 1.0806 (1.1864)	grad_norm 1.9097 (2.2179)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:48:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:44 lr 0.000008	 wd 0.0500	time 0.5032 (0.5149)	loss 1.3630 (1.1873)	grad_norm 1.5086 (2.2134)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:49:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:52 lr 0.000008	 wd 0.0500	time 0.5037 (0.5147)	loss 1.4662 (1.1876)	grad_norm 2.2668 (2.2137)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:50:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:01 lr 0.000008	 wd 0.0500	time 0.5055 (0.5146)	loss 1.2132 (1.1878)	grad_norm 1.7354 (2.2188)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:50:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 18 training takes 0:21:29
[2024-07-31 16:50:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.662 (12.662)	Loss 0.5166 (0.5166)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 16:50:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.666 Acc@5 97.684
[2024-07-31 16:50:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 16:50:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.69%
[2024-07-31 16:51:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:39:15 lr 0.000008	 wd 0.0500	time 12.4522 (12.4522)	loss 0.8934 (0.8934)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:51:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:25:00 lr 0.000008	 wd 0.0500	time 0.5020 (0.6247)	loss 1.6013 (1.2275)	grad_norm 1.5115 (2.2116)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:52:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:21:43 lr 0.000008	 wd 0.0500	time 0.5024 (0.5664)	loss 0.9085 (1.1968)	grad_norm 1.7022 (2.3281)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:53:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:20:04 lr 0.000008	 wd 0.0500	time 0.4984 (0.5470)	loss 1.4555 (1.1867)	grad_norm 2.6497 (2.2343)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:54:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:18:49 lr 0.000008	 wd 0.0500	time 0.4962 (0.5375)	loss 0.9976 (1.1776)	grad_norm 1.6939 (2.3044)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:55:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:17:44 lr 0.000008	 wd 0.0500	time 0.5002 (0.5320)	loss 1.4168 (1.1861)	grad_norm 1.8358 (2.3076)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:56:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:16:44 lr 0.000008	 wd 0.0500	time 0.5020 (0.5284)	loss 0.8991 (1.1777)	grad_norm 1.7719 (2.3290)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:57:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:15:47 lr 0.000008	 wd 0.0500	time 0.5065 (0.5258)	loss 1.1616 (1.1764)	grad_norm 1.4670 (2.3238)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:57:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:14:51 lr 0.000008	 wd 0.0500	time 0.4976 (0.5238)	loss 1.4175 (1.1781)	grad_norm 1.7842 (2.3512)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:58:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:13:56 lr 0.000008	 wd 0.0500	time 0.5011 (0.5223)	loss 1.1891 (1.1791)	grad_norm 2.0334 (2.3123)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 16:59:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:13:02 lr 0.000008	 wd 0.0500	time 0.5037 (0.5212)	loss 1.4207 (1.1792)	grad_norm 1.9798 (2.3248)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:00:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:12:09 lr 0.000008	 wd 0.0500	time 0.5030 (0.5203)	loss 1.1746 (1.1776)	grad_norm 1.8021 (2.3240)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:01:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:11:16 lr 0.000008	 wd 0.0500	time 0.4909 (0.5195)	loss 1.3717 (1.1760)	grad_norm 2.1449 (2.3156)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:02:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:10:23 lr 0.000008	 wd 0.0500	time 0.4939 (0.5189)	loss 0.9424 (1.1756)	grad_norm 2.3414 (2.3414)	loss_scale 4096.0000 (2114.1153)	mem 19076MB
[2024-07-31 17:03:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:09:31 lr 0.000008	 wd 0.0500	time 0.5035 (0.5183)	loss 0.9480 (1.1756)	grad_norm 2.0711 (2.3220)	loss_scale 4096.0000 (2255.5774)	mem 19076MB
[2024-07-31 17:03:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:08:38 lr 0.000008	 wd 0.0500	time 0.5016 (0.5177)	loss 1.5136 (1.1764)	grad_norm 2.1112 (2.3386)	loss_scale 4096.0000 (2378.1905)	mem 19076MB
[2024-07-31 17:04:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:07:46 lr 0.000007	 wd 0.0500	time 0.5047 (0.5173)	loss 1.2466 (1.1781)	grad_norm 1.5058 (2.3276)	loss_scale 4096.0000 (2485.4866)	mem 19076MB
[2024-07-31 17:05:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:06:54 lr 0.000007	 wd 0.0500	time 0.4949 (0.5169)	loss 0.8156 (1.1785)	grad_norm 1.4489 (2.3016)	loss_scale 4096.0000 (2580.1670)	mem 19076MB
[2024-07-31 17:06:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:06:02 lr 0.000007	 wd 0.0500	time 0.5056 (0.5166)	loss 1.3214 (1.1805)	grad_norm 2.0638 (2.3007)	loss_scale 4096.0000 (2664.3331)	mem 19076MB
[2024-07-31 17:07:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:05:10 lr 0.000007	 wd 0.0500	time 0.5042 (0.5163)	loss 1.1823 (1.1791)	grad_norm 3.0639 (2.2952)	loss_scale 4096.0000 (2739.6444)	mem 19076MB
[2024-07-31 17:08:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:04:19 lr 0.000007	 wd 0.0500	time 0.5069 (0.5160)	loss 1.3419 (1.1801)	grad_norm 2.0897 (2.2917)	loss_scale 4096.0000 (2807.4283)	mem 19076MB
[2024-07-31 17:09:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:03:27 lr 0.000007	 wd 0.0500	time 0.5021 (0.5157)	loss 1.6426 (1.1821)	grad_norm 1.4581 (2.2949)	loss_scale 4096.0000 (2868.7596)	mem 19076MB
[2024-07-31 17:09:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:02:35 lr 0.000007	 wd 0.0500	time 0.5047 (0.5155)	loss 1.2495 (1.1814)	grad_norm 1.8923 (2.2912)	loss_scale 4096.0000 (2924.5179)	mem 19076MB
[2024-07-31 17:10:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:44 lr 0.000007	 wd 0.0500	time 0.5049 (0.5153)	loss 1.0175 (1.1810)	grad_norm 3.1169 (2.2948)	loss_scale 4096.0000 (2975.4298)	mem 19076MB
[2024-07-31 17:11:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:52 lr 0.000007	 wd 0.0500	time 0.5082 (0.5151)	loss 1.1472 (1.1802)	grad_norm 1.8998 (2.2895)	loss_scale 4096.0000 (3022.1008)	mem 19076MB
[2024-07-31 17:12:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:01 lr 0.000007	 wd 0.0500	time 0.4954 (0.5149)	loss 0.8454 (1.1801)	grad_norm 1.7443 (inf)	loss_scale 2048.0000 (2996.2543)	mem 19076MB
[2024-07-31 17:12:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 19 training takes 0:21:30
[2024-07-31 17:12:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.648 (12.648)	Loss 0.5005 (0.5005)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 17:13:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.670 Acc@5 97.650
[2024-07-31 17:13:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 17:13:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.69%
[2024-07-31 17:13:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:35:23 lr 0.000007	 wd 0.0500	time 10.9208 (10.9208)	loss 1.3954 (1.3954)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:14:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:24:53 lr 0.000007	 wd 0.0500	time 0.4990 (0.6218)	loss 0.9812 (1.1860)	grad_norm 3.1994 (2.2578)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:14:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:21:40 lr 0.000007	 wd 0.0500	time 0.5021 (0.5648)	loss 1.3332 (1.1770)	grad_norm 3.5001 (2.2810)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:15:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:20:02 lr 0.000007	 wd 0.0500	time 0.5014 (0.5459)	loss 0.7419 (1.1868)	grad_norm 1.7359 (2.2048)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:16:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:18:47 lr 0.000007	 wd 0.0500	time 0.4960 (0.5366)	loss 1.3209 (1.1841)	grad_norm 1.7662 (2.1852)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:17:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:17:43 lr 0.000007	 wd 0.0500	time 0.5000 (0.5312)	loss 1.2993 (1.1831)	grad_norm 1.7732 (2.1934)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:18:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:16:43 lr 0.000007	 wd 0.0500	time 0.5045 (0.5277)	loss 1.3808 (1.1878)	grad_norm 2.3136 (2.2126)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:19:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:15:46 lr 0.000007	 wd 0.0500	time 0.5036 (0.5253)	loss 1.3505 (1.1860)	grad_norm 2.9032 (2.1812)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:20:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:14:50 lr 0.000007	 wd 0.0500	time 0.5063 (0.5235)	loss 1.2036 (1.1879)	grad_norm 1.9867 (2.1988)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:20:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:13:56 lr 0.000007	 wd 0.0500	time 0.5005 (0.5221)	loss 1.3909 (1.1866)	grad_norm 2.1844 (2.1666)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:21:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:13:02 lr 0.000007	 wd 0.0500	time 0.4980 (0.5209)	loss 1.2739 (1.1883)	grad_norm 2.1942 (2.2002)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:22:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:12:08 lr 0.000007	 wd 0.0500	time 0.5032 (0.5200)	loss 0.9582 (1.1863)	grad_norm 1.6695 (2.2078)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:23:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:11:16 lr 0.000006	 wd 0.0500	time 0.5012 (0.5192)	loss 1.2771 (1.1831)	grad_norm 1.9414 (2.1874)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:24:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:10:23 lr 0.000006	 wd 0.0500	time 0.4996 (0.5186)	loss 1.3981 (1.1829)	grad_norm 2.1124 (2.2376)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:25:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:09:30 lr 0.000006	 wd 0.0500	time 0.5016 (0.5180)	loss 1.0724 (1.1851)	grad_norm 2.2408 (2.2360)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:25:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:08:38 lr 0.000006	 wd 0.0500	time 0.4982 (0.5175)	loss 1.4088 (1.1866)	grad_norm 1.8796 (2.2297)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:26:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:07:46 lr 0.000006	 wd 0.0500	time 0.5029 (0.5171)	loss 1.2546 (1.1887)	grad_norm 10.1856 (2.2507)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:27:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:06:54 lr 0.000006	 wd 0.0500	time 0.5040 (0.5167)	loss 1.5499 (1.1894)	grad_norm 1.8929 (2.2313)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:28:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:06:02 lr 0.000006	 wd 0.0500	time 0.5029 (0.5164)	loss 0.8061 (1.1871)	grad_norm 2.0132 (2.2230)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:29:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:05:10 lr 0.000006	 wd 0.0500	time 0.5057 (0.5161)	loss 0.8293 (1.1878)	grad_norm 1.5363 (2.2186)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:30:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:04:18 lr 0.000006	 wd 0.0500	time 0.5014 (0.5159)	loss 1.1221 (1.1873)	grad_norm 1.7585 (2.2150)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:31:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:03:27 lr 0.000006	 wd 0.0500	time 0.5038 (0.5156)	loss 1.3762 (1.1876)	grad_norm 1.3920 (2.2186)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:31:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:02:35 lr 0.000006	 wd 0.0500	time 0.5044 (0.5154)	loss 1.3689 (1.1888)	grad_norm 1.8992 (2.2283)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:32:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:44 lr 0.000006	 wd 0.0500	time 0.4959 (0.5152)	loss 0.8405 (1.1901)	grad_norm 6.5041 (2.2312)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:33:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:52 lr 0.000006	 wd 0.0500	time 0.5030 (0.5150)	loss 0.7438 (1.1896)	grad_norm 1.9696 (2.2331)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:34:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:01 lr 0.000006	 wd 0.0500	time 0.4989 (0.5148)	loss 1.1604 (1.1915)	grad_norm 1.5483 (2.2305)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:34:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 20 training takes 0:21:31
[2024-07-31 17:34:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.952 (11.952)	Loss 0.5088 (0.5088)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 17:35:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.724 Acc@5 97.670
[2024-07-31 17:35:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 17:35:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 17:35:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 160): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saving......
[2024-07-31 17:35:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 162): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_best.pth saved !!!
[2024-07-31 17:35:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:19:24 lr 0.000006	 wd 0.0500	time 11.9761 (11.9761)	loss 0.8994 (0.8994)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:36:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:24:48 lr 0.000006	 wd 0.0500	time 0.5055 (0.6198)	loss 1.4078 (1.1933)	grad_norm 2.3514 (2.1586)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:37:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:21:37 lr 0.000006	 wd 0.0500	time 0.4975 (0.5637)	loss 1.3476 (1.1959)	grad_norm 1.9759 (2.1419)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:37:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:20:00 lr 0.000006	 wd 0.0500	time 0.5012 (0.5452)	loss 1.1604 (1.1905)	grad_norm 2.3558 (2.1720)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:38:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:18:46 lr 0.000006	 wd 0.0500	time 0.5009 (0.5361)	loss 1.4937 (1.1811)	grad_norm 3.0193 (2.1167)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:39:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:17:42 lr 0.000006	 wd 0.0500	time 0.5002 (0.5307)	loss 0.8676 (1.1762)	grad_norm 2.2837 (2.0978)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:40:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:16:42 lr 0.000006	 wd 0.0500	time 0.4976 (0.5273)	loss 1.3414 (1.1722)	grad_norm 2.5580 (2.0942)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:41:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:15:45 lr 0.000006	 wd 0.0500	time 0.5047 (0.5250)	loss 1.3088 (1.1742)	grad_norm 1.4811 (2.0893)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:42:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:14:50 lr 0.000006	 wd 0.0500	time 0.5020 (0.5232)	loss 1.3309 (1.1732)	grad_norm 1.6935 (2.0776)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:42:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:13:55 lr 0.000005	 wd 0.0500	time 0.5039 (0.5218)	loss 1.3904 (1.1774)	grad_norm 1.6616 (2.1003)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:43:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:13:02 lr 0.000005	 wd 0.0500	time 0.5035 (0.5206)	loss 1.4176 (1.1813)	grad_norm 1.6622 (2.1236)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:44:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:12:08 lr 0.000005	 wd 0.0500	time 0.5049 (0.5197)	loss 0.8354 (1.1850)	grad_norm 1.4296 (2.1297)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:45:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:11:15 lr 0.000005	 wd 0.0500	time 0.5054 (0.5190)	loss 1.3356 (1.1814)	grad_norm 1.8947 (2.1884)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:46:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:10:23 lr 0.000005	 wd 0.0500	time 0.4920 (0.5184)	loss 1.3375 (1.1814)	grad_norm 1.7277 (2.1871)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:47:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:09:30 lr 0.000005	 wd 0.0500	time 0.5039 (0.5178)	loss 0.8527 (1.1792)	grad_norm 1.6689 (2.1964)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 17:48:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:08:38 lr 0.000005	 wd 0.0500	time 0.5048 (0.5174)	loss 0.8085 (1.1805)	grad_norm 1.7892 (2.1922)	loss_scale 4096.0000 (2168.0693)	mem 19076MB
[2024-07-31 17:48:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:07:46 lr 0.000005	 wd 0.0500	time 0.5051 (0.5170)	loss 1.1260 (1.1772)	grad_norm 3.0012 (2.1837)	loss_scale 4096.0000 (2288.4897)	mem 19076MB
[2024-07-31 17:49:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:06:54 lr 0.000005	 wd 0.0500	time 0.5058 (0.5166)	loss 1.4135 (1.1772)	grad_norm 1.4532 (2.1899)	loss_scale 4096.0000 (2394.7513)	mem 19076MB
[2024-07-31 17:50:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:06:02 lr 0.000005	 wd 0.0500	time 0.4990 (0.5163)	loss 0.8573 (1.1772)	grad_norm 2.4767 (2.2075)	loss_scale 4096.0000 (2489.2127)	mem 19076MB
[2024-07-31 17:51:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:05:10 lr 0.000005	 wd 0.0500	time 0.5041 (0.5160)	loss 0.7493 (1.1788)	grad_norm 1.6377 (2.2071)	loss_scale 4096.0000 (2573.7359)	mem 19076MB
[2024-07-31 17:52:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:04:18 lr 0.000005	 wd 0.0500	time 0.5052 (0.5157)	loss 1.1709 (1.1782)	grad_norm 2.0834 (2.2045)	loss_scale 4096.0000 (2649.8111)	mem 19076MB
[2024-07-31 17:53:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:03:27 lr 0.000005	 wd 0.0500	time 0.5045 (0.5155)	loss 1.3059 (1.1779)	grad_norm 1.8884 (2.2008)	loss_scale 4096.0000 (2718.6445)	mem 19076MB
[2024-07-31 17:54:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:02:35 lr 0.000005	 wd 0.0500	time 0.5065 (0.5153)	loss 1.4544 (1.1762)	grad_norm 3.2014 (2.2071)	loss_scale 4096.0000 (2781.2231)	mem 19076MB
[2024-07-31 17:54:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:44 lr 0.000005	 wd 0.0500	time 0.5038 (0.5151)	loss 1.4594 (1.1785)	grad_norm 2.2324 (2.2241)	loss_scale 4096.0000 (2838.3625)	mem 19076MB
[2024-07-31 17:55:45 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:52 lr 0.000005	 wd 0.0500	time 0.5008 (0.5149)	loss 1.1495 (1.1795)	grad_norm 2.3778 (2.2288)	loss_scale 4096.0000 (2890.7422)	mem 19076MB
[2024-07-31 17:56:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:01 lr 0.000005	 wd 0.0500	time 0.5020 (0.5147)	loss 1.4841 (1.1790)	grad_norm 1.4841 (2.2333)	loss_scale 4096.0000 (2938.9332)	mem 19076MB
[2024-07-31 17:56:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 21 training takes 0:21:31
[2024-07-31 17:56:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.145 (12.145)	Loss 0.5283 (0.5283)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 17:57:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.676 Acc@5 97.662
[2024-07-31 17:57:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 17:57:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 17:57:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:28:12 lr 0.000005	 wd 0.0500	time 12.1873 (12.1873)	loss 1.0356 (1.0356)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 17:58:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:24:58 lr 0.000005	 wd 0.0500	time 0.4969 (0.6241)	loss 1.3672 (1.1905)	grad_norm 1.6804 (2.2158)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 17:59:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:21:42 lr 0.000005	 wd 0.0500	time 0.5021 (0.5660)	loss 0.7412 (1.2023)	grad_norm 2.5426 (2.2690)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:00:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:20:03 lr 0.000005	 wd 0.0500	time 0.4930 (0.5467)	loss 1.5800 (1.2037)	grad_norm 2.2003 (2.5504)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:00:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:18:49 lr 0.000005	 wd 0.0500	time 0.4963 (0.5371)	loss 0.7883 (1.2034)	grad_norm 1.4237 (2.4653)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:01:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:17:44 lr 0.000005	 wd 0.0500	time 0.5009 (0.5317)	loss 1.1373 (1.1995)	grad_norm 2.2317 (2.3562)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:02:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:16:44 lr 0.000005	 wd 0.0500	time 0.5026 (0.5281)	loss 0.8695 (1.1988)	grad_norm 1.6082 (2.3483)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:03:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:15:47 lr 0.000005	 wd 0.0500	time 0.4987 (0.5256)	loss 1.4440 (1.1925)	grad_norm 2.3916 (2.2996)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:04:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:14:51 lr 0.000004	 wd 0.0500	time 0.5070 (0.5238)	loss 1.0854 (1.1927)	grad_norm 1.4544 (2.2644)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:05:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:13:56 lr 0.000004	 wd 0.0500	time 0.5009 (0.5224)	loss 1.1502 (1.1923)	grad_norm 1.8710 (2.2391)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:05:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:13:02 lr 0.000004	 wd 0.0500	time 0.4996 (0.5212)	loss 1.4257 (1.1929)	grad_norm 1.8425 (2.2520)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:06:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:12:09 lr 0.000004	 wd 0.0500	time 0.5016 (0.5202)	loss 0.8031 (1.1888)	grad_norm 1.4417 (2.2234)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:07:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:11:16 lr 0.000004	 wd 0.0500	time 0.5020 (0.5194)	loss 1.3156 (1.1895)	grad_norm 1.8197 (2.2252)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:08:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:10:23 lr 0.000004	 wd 0.0500	time 0.5026 (0.5188)	loss 0.8672 (1.1832)	grad_norm 1.5747 (2.2107)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:09:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:09:31 lr 0.000004	 wd 0.0500	time 0.5009 (0.5182)	loss 1.0551 (1.1782)	grad_norm 2.2097 (2.2023)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:10:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:08:38 lr 0.000004	 wd 0.0500	time 0.5013 (0.5177)	loss 1.1706 (1.1777)	grad_norm 1.6011 (2.1937)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:11:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:07:46 lr 0.000004	 wd 0.0500	time 0.4963 (0.5173)	loss 1.4106 (1.1771)	grad_norm 1.4573 (2.1920)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:11:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:06:54 lr 0.000004	 wd 0.0500	time 0.5020 (0.5169)	loss 0.9704 (1.1749)	grad_norm 1.6222 (2.1799)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:12:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:06:02 lr 0.000004	 wd 0.0500	time 0.4984 (0.5166)	loss 1.2916 (1.1735)	grad_norm 2.0120 (2.1800)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:13:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:05:10 lr 0.000004	 wd 0.0500	time 0.5030 (0.5163)	loss 0.9554 (1.1744)	grad_norm 1.4843 (2.1756)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:14:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:04:19 lr 0.000004	 wd 0.0500	time 0.4972 (0.5160)	loss 0.9176 (1.1756)	grad_norm 1.4207 (2.1681)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:15:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:03:27 lr 0.000004	 wd 0.0500	time 0.5026 (0.5158)	loss 1.3012 (1.1767)	grad_norm 1.3995 (2.1707)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 18:16:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:02:35 lr 0.000004	 wd 0.0500	time 0.5021 (0.5155)	loss 1.1377 (1.1764)	grad_norm 1.8395 (nan)	loss_scale 2048.0000 (4029.0050)	mem 19076MB
[2024-07-31 18:17:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:44 lr 0.000004	 wd 0.0500	time 0.5073 (0.5153)	loss 1.2416 (1.1768)	grad_norm 4.2208 (nan)	loss_scale 2048.0000 (3942.9118)	mem 19076MB
[2024-07-31 18:17:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:52 lr 0.000004	 wd 0.0500	time 0.4971 (0.5151)	loss 1.4888 (1.1785)	grad_norm 1.3969 (nan)	loss_scale 2048.0000 (3863.9900)	mem 19076MB
[2024-07-31 18:18:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:01 lr 0.000004	 wd 0.0500	time 0.4994 (0.5149)	loss 0.9601 (1.1786)	grad_norm 1.6535 (nan)	loss_scale 2048.0000 (3791.3794)	mem 19076MB
[2024-07-31 18:18:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 22 training takes 0:21:33
[2024-07-31 18:19:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.360 (12.360)	Loss 0.5098 (0.5098)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 18:19:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.652 Acc@5 97.678
[2024-07-31 18:19:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 18:19:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 18:19:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:49:23 lr 0.000004	 wd 0.0500	time 11.2563 (11.2563)	loss 0.7841 (0.7841)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:20:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:24:43 lr 0.000004	 wd 0.0500	time 0.4980 (0.6177)	loss 1.4075 (1.1679)	grad_norm 1.8971 (2.1590)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:21:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:21:34 lr 0.000004	 wd 0.0500	time 0.5023 (0.5625)	loss 0.9301 (1.1770)	grad_norm 2.3576 (2.2878)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:22:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:19:58 lr 0.000004	 wd 0.0500	time 0.5003 (0.5444)	loss 1.3747 (1.1665)	grad_norm 1.6988 (2.2755)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:23:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:18:45 lr 0.000004	 wd 0.0500	time 0.4919 (0.5355)	loss 0.9493 (1.1722)	grad_norm 1.5271 (2.2247)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:23:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:17:41 lr 0.000004	 wd 0.0500	time 0.5054 (0.5304)	loss 0.9331 (1.1724)	grad_norm 1.6528 (2.2653)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:24:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:16:42 lr 0.000004	 wd 0.0500	time 0.5048 (0.5270)	loss 1.4172 (1.1713)	grad_norm 2.7341 (2.2934)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:25:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:15:45 lr 0.000004	 wd 0.0500	time 0.5031 (0.5247)	loss 1.2618 (1.1736)	grad_norm 9.2646 (2.2688)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:26:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:14:49 lr 0.000003	 wd 0.0500	time 0.5038 (0.5229)	loss 1.4061 (1.1760)	grad_norm 2.0123 (2.2402)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:27:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:13:55 lr 0.000003	 wd 0.0500	time 0.5005 (0.5215)	loss 1.1538 (1.1769)	grad_norm 1.9984 (2.2333)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:28:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:13:01 lr 0.000003	 wd 0.0500	time 0.5056 (0.5204)	loss 1.2106 (1.1729)	grad_norm 1.5749 (2.2307)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:28:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:12:08 lr 0.000003	 wd 0.0500	time 0.4955 (0.5195)	loss 1.0334 (1.1750)	grad_norm 2.2353 (2.2269)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:29:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:11:15 lr 0.000003	 wd 0.0500	time 0.5033 (0.5188)	loss 1.3579 (1.1777)	grad_norm 1.5050 (2.2218)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:30:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:10:22 lr 0.000003	 wd 0.0500	time 0.5023 (0.5181)	loss 1.2841 (1.1785)	grad_norm 1.9588 (2.3140)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:31:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:09:30 lr 0.000003	 wd 0.0500	time 0.5062 (0.5176)	loss 1.3940 (1.1787)	grad_norm 1.9219 (2.3034)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:32:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:08:38 lr 0.000003	 wd 0.0500	time 0.5058 (0.5172)	loss 1.3941 (1.1772)	grad_norm 1.6486 (2.3100)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:33:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:07:46 lr 0.000003	 wd 0.0500	time 0.5031 (0.5168)	loss 1.2476 (1.1803)	grad_norm 2.8556 (2.2974)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:34:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:06:54 lr 0.000003	 wd 0.0500	time 0.5063 (0.5164)	loss 0.7506 (1.1773)	grad_norm 2.1196 (2.2955)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:34:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:06:02 lr 0.000003	 wd 0.0500	time 0.5065 (0.5161)	loss 1.4344 (1.1767)	grad_norm 1.5869 (2.3173)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:35:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:05:10 lr 0.000003	 wd 0.0500	time 0.5031 (0.5159)	loss 1.3908 (1.1761)	grad_norm 1.9424 (2.3085)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:36:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:04:18 lr 0.000003	 wd 0.0500	time 0.5042 (0.5156)	loss 0.8289 (1.1753)	grad_norm 2.9221 (2.2833)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:37:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:03:27 lr 0.000003	 wd 0.0500	time 0.5022 (0.5154)	loss 1.2723 (1.1760)	grad_norm 2.5675 (2.3127)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:38:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:02:35 lr 0.000003	 wd 0.0500	time 0.4982 (0.5151)	loss 1.3589 (1.1750)	grad_norm 1.7193 (2.3009)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:39:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:44 lr 0.000003	 wd 0.0500	time 0.5062 (0.5150)	loss 1.4560 (1.1745)	grad_norm 10.4842 (2.3013)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:40:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:52 lr 0.000003	 wd 0.0500	time 0.4990 (0.5148)	loss 1.0127 (1.1742)	grad_norm 2.0188 (2.2946)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:40:54 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:01 lr 0.000003	 wd 0.0500	time 0.5048 (0.5146)	loss 0.8826 (1.1730)	grad_norm 2.0213 (2.2861)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:41:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 23 training takes 0:21:33
[2024-07-31 18:41:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.181 (12.181)	Loss 0.5273 (0.5273)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 18:41:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.676 Acc@5 97.672
[2024-07-31 18:41:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 18:41:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 18:41:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:07:12 lr 0.000003	 wd 0.0500	time 11.6837 (11.6837)	loss 1.2852 (1.2852)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:42:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:24:50 lr 0.000003	 wd 0.0500	time 0.5007 (0.6205)	loss 0.9835 (1.1885)	grad_norm 1.4427 (2.3516)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:43:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:21:37 lr 0.000003	 wd 0.0500	time 0.5012 (0.5637)	loss 1.1464 (1.1792)	grad_norm 1.6821 (2.2604)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:44:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:20:00 lr 0.000003	 wd 0.0500	time 0.4966 (0.5451)	loss 1.3887 (1.1916)	grad_norm 1.7077 (2.6821)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:45:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:18:46 lr 0.000003	 wd 0.0500	time 0.5017 (0.5360)	loss 0.7004 (1.1899)	grad_norm 2.4937 (2.6848)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:46:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:17:42 lr 0.000003	 wd 0.0500	time 0.5066 (0.5307)	loss 1.3328 (1.1906)	grad_norm 1.4375 (2.5637)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:46:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:16:42 lr 0.000003	 wd 0.0500	time 0.5033 (0.5273)	loss 0.8961 (1.1856)	grad_norm 2.2065 (2.4926)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:47:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:15:45 lr 0.000003	 wd 0.0500	time 0.5044 (0.5248)	loss 1.3568 (1.1833)	grad_norm 2.6184 (2.4550)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:48:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:14:50 lr 0.000003	 wd 0.0500	time 0.5038 (0.5230)	loss 1.2987 (1.1843)	grad_norm 1.5608 (2.3788)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:49:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:13:55 lr 0.000003	 wd 0.0500	time 0.5073 (0.5216)	loss 0.9429 (1.1887)	grad_norm 2.0248 (2.3554)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:50:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:13:01 lr 0.000003	 wd 0.0500	time 0.4941 (0.5205)	loss 0.8842 (1.1861)	grad_norm 1.6001 (2.3235)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:51:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:12:08 lr 0.000003	 wd 0.0500	time 0.5021 (0.5196)	loss 0.8831 (1.1844)	grad_norm 1.5776 (2.4010)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 18:52:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:11:15 lr 0.000002	 wd 0.0500	time 0.4991 (0.5189)	loss 0.9692 (1.1816)	grad_norm 1.8709 (2.3762)	loss_scale 4096.0000 (2177.5987)	mem 19076MB
[2024-07-31 18:52:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:10:22 lr 0.000002	 wd 0.0500	time 0.5050 (0.5182)	loss 0.8372 (1.1791)	grad_norm 2.3213 (2.3582)	loss_scale 4096.0000 (2325.0546)	mem 19076MB
[2024-07-31 18:53:43 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:09:30 lr 0.000002	 wd 0.0500	time 0.4994 (0.5177)	loss 1.4796 (1.1804)	grad_norm 2.0658 (2.3436)	loss_scale 4096.0000 (2451.4604)	mem 19076MB
[2024-07-31 18:54:34 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:08:38 lr 0.000002	 wd 0.0500	time 0.5038 (0.5172)	loss 1.2696 (1.1767)	grad_norm 1.7057 (inf)	loss_scale 2048.0000 (2525.5483)	mem 19076MB
[2024-07-31 18:55:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:07:46 lr 0.000002	 wd 0.0500	time 0.5019 (0.5167)	loss 1.3151 (1.1754)	grad_norm 2.7899 (inf)	loss_scale 2048.0000 (2495.7202)	mem 19076MB
[2024-07-31 18:56:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:06:54 lr 0.000002	 wd 0.0500	time 0.5026 (0.5163)	loss 1.6095 (1.1767)	grad_norm 1.8326 (inf)	loss_scale 2048.0000 (2469.3992)	mem 19076MB
[2024-07-31 18:57:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:06:02 lr 0.000002	 wd 0.0500	time 0.5040 (0.5160)	loss 1.1867 (1.1772)	grad_norm 2.1524 (inf)	loss_scale 2048.0000 (2446.0011)	mem 19076MB
[2024-07-31 18:57:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:05:10 lr 0.000002	 wd 0.0500	time 0.4943 (0.5157)	loss 0.8819 (1.1779)	grad_norm 1.5549 (inf)	loss_scale 2048.0000 (2425.0647)	mem 19076MB
[2024-07-31 18:58:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:04:18 lr 0.000002	 wd 0.0500	time 0.5026 (0.5155)	loss 1.2991 (1.1776)	grad_norm 1.6654 (inf)	loss_scale 2048.0000 (2406.2209)	mem 19076MB
[2024-07-31 18:59:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:03:27 lr 0.000002	 wd 0.0500	time 0.5034 (0.5152)	loss 1.0530 (1.1773)	grad_norm 1.5539 (inf)	loss_scale 2048.0000 (2389.1709)	mem 19076MB
[2024-07-31 19:00:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:02:35 lr 0.000002	 wd 0.0500	time 0.4937 (0.5150)	loss 0.9647 (1.1789)	grad_norm 3.3557 (inf)	loss_scale 2048.0000 (2373.6701)	mem 19076MB
[2024-07-31 19:01:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:44 lr 0.000002	 wd 0.0500	time 0.5048 (0.5149)	loss 1.3374 (1.1800)	grad_norm 2.5989 (inf)	loss_scale 2048.0000 (2359.5167)	mem 19076MB
[2024-07-31 19:02:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:52 lr 0.000002	 wd 0.0500	time 0.5045 (0.5147)	loss 1.4120 (1.1777)	grad_norm 2.3857 (inf)	loss_scale 2048.0000 (2346.5423)	mem 19076MB
[2024-07-31 19:03:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:01 lr 0.000002	 wd 0.0500	time 0.5017 (0.5145)	loss 0.8004 (1.1780)	grad_norm 1.5551 (inf)	loss_scale 2048.0000 (2334.6054)	mem 19076MB
[2024-07-31 19:03:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 24 training takes 0:21:34
[2024-07-31 19:03:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.342 (11.342)	Loss 0.5107 (0.5107)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 19:03:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.660 Acc@5 97.686
[2024-07-31 19:03:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 19:03:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 19:04:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:55:34 lr 0.000002	 wd 0.0500	time 11.4048 (11.4048)	loss 1.3499 (1.3499)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:04:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:24:38 lr 0.000002	 wd 0.0500	time 0.4980 (0.6154)	loss 1.2458 (1.2461)	grad_norm 4.4123 (2.0867)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:05:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:21:32 lr 0.000002	 wd 0.0500	time 0.5024 (0.5616)	loss 1.4567 (1.2104)	grad_norm 1.8321 (2.1251)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:06:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:19:57 lr 0.000002	 wd 0.0500	time 0.5032 (0.5437)	loss 1.1989 (1.1939)	grad_norm 1.5675 (2.1272)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:07:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:18:44 lr 0.000002	 wd 0.0500	time 0.5030 (0.5349)	loss 1.2302 (1.1916)	grad_norm 1.4919 (2.1408)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:08:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:17:40 lr 0.000002	 wd 0.0500	time 0.5009 (0.5298)	loss 1.3359 (1.1893)	grad_norm 2.0068 (2.1398)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:09:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:16:41 lr 0.000002	 wd 0.0500	time 0.5060 (0.5266)	loss 0.9252 (1.1798)	grad_norm 2.2141 (2.1685)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:09:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:15:44 lr 0.000002	 wd 0.0500	time 0.5044 (0.5242)	loss 0.8166 (1.1814)	grad_norm 1.7217 (2.2513)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:10:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:14:49 lr 0.000002	 wd 0.0500	time 0.5034 (0.5225)	loss 1.4083 (1.1790)	grad_norm 3.4624 (2.2519)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:11:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:13:54 lr 0.000002	 wd 0.0500	time 0.5038 (0.5212)	loss 1.3135 (1.1791)	grad_norm 1.8210 (2.2499)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:12:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:13:01 lr 0.000002	 wd 0.0500	time 0.4990 (0.5201)	loss 1.6334 (1.1799)	grad_norm 3.1893 (2.2478)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:13:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:12:07 lr 0.000002	 wd 0.0500	time 0.5014 (0.5192)	loss 1.4764 (1.1820)	grad_norm 2.1893 (2.2496)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:14:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:11:15 lr 0.000002	 wd 0.0500	time 0.4958 (0.5185)	loss 0.9840 (1.1785)	grad_norm 1.2948 (2.2446)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:15:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:10:22 lr 0.000002	 wd 0.0500	time 0.5054 (0.5179)	loss 0.9551 (1.1778)	grad_norm 1.7586 (2.2226)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:15:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:09:30 lr 0.000002	 wd 0.0500	time 0.5033 (0.5174)	loss 1.0308 (1.1777)	grad_norm 1.8114 (2.2169)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:16:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:08:37 lr 0.000002	 wd 0.0500	time 0.4950 (0.5169)	loss 0.8021 (1.1771)	grad_norm 1.5212 (2.2188)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:17:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:07:45 lr 0.000002	 wd 0.0500	time 0.5017 (0.5165)	loss 0.8601 (1.1760)	grad_norm 2.9565 (2.2349)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:18:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:06:53 lr 0.000002	 wd 0.0500	time 0.5061 (0.5162)	loss 0.7913 (1.1737)	grad_norm 2.1702 (2.2298)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:19:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:06:02 lr 0.000002	 wd 0.0500	time 0.5025 (0.5159)	loss 0.8558 (1.1754)	grad_norm 3.1501 (2.2756)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:20:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:05:10 lr 0.000002	 wd 0.0500	time 0.5043 (0.5157)	loss 1.4973 (1.1773)	grad_norm 1.5358 (2.2751)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:21:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:04:18 lr 0.000002	 wd 0.0500	time 0.5021 (0.5154)	loss 0.9677 (1.1774)	grad_norm 1.9101 (2.2643)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:21:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:03:27 lr 0.000002	 wd 0.0500	time 0.4939 (0.5152)	loss 1.3563 (1.1784)	grad_norm 1.9268 (2.2607)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:22:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:02:35 lr 0.000001	 wd 0.0500	time 0.5018 (0.5150)	loss 1.0781 (1.1788)	grad_norm 1.7622 (2.2522)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:23:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:43 lr 0.000001	 wd 0.0500	time 0.5053 (0.5148)	loss 1.3523 (1.1786)	grad_norm 1.3312 (2.2626)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:24:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:52 lr 0.000001	 wd 0.0500	time 0.4943 (0.5147)	loss 1.5626 (1.1781)	grad_norm 2.0562 (2.2595)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:25:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5041 (0.5145)	loss 1.0290 (1.1768)	grad_norm 1.6401 (2.2622)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:25:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 25 training takes 0:21:33
[2024-07-31 19:25:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.072 (12.072)	Loss 0.5332 (0.5332)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 19:26:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.684 Acc@5 97.670
[2024-07-31 19:26:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 19:26:03 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 19:26:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:25:04 lr 0.000001	 wd 0.0500	time 12.1121 (12.1121)	loss 1.1141 (1.1141)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:27:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:24:54 lr 0.000001	 wd 0.0500	time 0.5011 (0.6221)	loss 1.3712 (1.1873)	grad_norm 2.6064 (2.9587)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:27:56 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:21:40 lr 0.000001	 wd 0.0500	time 0.4998 (0.5650)	loss 1.0982 (1.1827)	grad_norm 1.9719 (2.4917)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:28:47 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:20:02 lr 0.000001	 wd 0.0500	time 0.4972 (0.5460)	loss 0.9328 (1.1692)	grad_norm 1.6549 (2.5979)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:29:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:18:48 lr 0.000001	 wd 0.0500	time 0.4991 (0.5367)	loss 1.4822 (1.1689)	grad_norm 1.4099 (2.5528)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:30:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:17:43 lr 0.000001	 wd 0.0500	time 0.5034 (0.5313)	loss 1.3566 (1.1771)	grad_norm 2.0451 (2.5906)	loss_scale 4096.0000 (2170.6347)	mem 19076MB
[2024-07-31 19:31:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:16:43 lr 0.000001	 wd 0.0500	time 0.5030 (0.5277)	loss 0.8177 (1.1799)	grad_norm 1.5383 (2.6307)	loss_scale 4096.0000 (2490.9950)	mem 19076MB
[2024-07-31 19:32:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:15:46 lr 0.000001	 wd 0.0500	time 0.5039 (0.5252)	loss 1.2688 (1.1757)	grad_norm 1.6736 (2.5683)	loss_scale 4096.0000 (2719.9544)	mem 19076MB
[2024-07-31 19:33:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:14:50 lr 0.000001	 wd 0.0500	time 0.5039 (0.5234)	loss 1.3727 (1.1801)	grad_norm 1.4387 (nan)	loss_scale 2048.0000 (2809.9276)	mem 19076MB
[2024-07-31 19:33:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:13:56 lr 0.000001	 wd 0.0500	time 0.5045 (0.5220)	loss 1.2671 (1.1804)	grad_norm 1.8267 (nan)	loss_scale 2048.0000 (2725.3629)	mem 19076MB
[2024-07-31 19:34:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:13:02 lr 0.000001	 wd 0.0500	time 0.5043 (0.5208)	loss 1.0213 (1.1795)	grad_norm 2.9106 (nan)	loss_scale 2048.0000 (2657.6943)	mem 19076MB
[2024-07-31 19:35:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:12:08 lr 0.000001	 wd 0.0500	time 0.5062 (0.5199)	loss 0.8787 (1.1784)	grad_norm 1.3875 (nan)	loss_scale 2048.0000 (2602.3179)	mem 19076MB
[2024-07-31 19:36:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:11:15 lr 0.000001	 wd 0.0500	time 0.5049 (0.5192)	loss 0.9786 (1.1791)	grad_norm 8.0311 (nan)	loss_scale 2048.0000 (2556.1632)	mem 19076MB
[2024-07-31 19:37:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:10:23 lr 0.000001	 wd 0.0500	time 0.5048 (0.5185)	loss 1.3183 (1.1812)	grad_norm 2.0029 (nan)	loss_scale 2048.0000 (2517.1038)	mem 19076MB
[2024-07-31 19:38:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:09:30 lr 0.000001	 wd 0.0500	time 0.5020 (0.5180)	loss 1.4509 (1.1831)	grad_norm 1.4860 (nan)	loss_scale 2048.0000 (2483.6203)	mem 19076MB
[2024-07-31 19:39:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:08:38 lr 0.000001	 wd 0.0500	time 0.4992 (0.5175)	loss 1.4057 (1.1849)	grad_norm 1.7833 (nan)	loss_scale 2048.0000 (2454.5983)	mem 19076MB
[2024-07-31 19:39:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:07:46 lr 0.000001	 wd 0.0500	time 0.5063 (0.5171)	loss 1.1737 (1.1834)	grad_norm 1.2681 (nan)	loss_scale 2048.0000 (2429.2017)	mem 19076MB
[2024-07-31 19:40:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:06:54 lr 0.000001	 wd 0.0500	time 0.5046 (0.5167)	loss 1.1645 (1.1802)	grad_norm 1.7379 (nan)	loss_scale 2048.0000 (2406.7913)	mem 19076MB
[2024-07-31 19:41:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:06:02 lr 0.000001	 wd 0.0500	time 0.5034 (0.5164)	loss 0.8128 (1.1794)	grad_norm 1.8428 (nan)	loss_scale 2048.0000 (2386.8695)	mem 19076MB
[2024-07-31 19:42:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:05:10 lr 0.000001	 wd 0.0500	time 0.5059 (0.5161)	loss 1.3832 (1.1811)	grad_norm 1.8398 (nan)	loss_scale 2048.0000 (2369.0437)	mem 19076MB
[2024-07-31 19:43:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:04:18 lr 0.000001	 wd 0.0500	time 0.5002 (0.5158)	loss 1.2138 (1.1799)	grad_norm 1.5448 (nan)	loss_scale 2048.0000 (2352.9995)	mem 19076MB
[2024-07-31 19:44:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:03:27 lr 0.000001	 wd 0.0500	time 0.5007 (0.5156)	loss 1.3491 (1.1792)	grad_norm 2.3899 (nan)	loss_scale 2048.0000 (2338.4826)	mem 19076MB
[2024-07-31 19:44:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:02:35 lr 0.000001	 wd 0.0500	time 0.5042 (0.5153)	loss 1.3156 (1.1775)	grad_norm 1.3673 (nan)	loss_scale 2048.0000 (2325.2849)	mem 19076MB
[2024-07-31 19:45:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:44 lr 0.000001	 wd 0.0500	time 0.5029 (0.5151)	loss 1.4044 (1.1778)	grad_norm 2.4300 (nan)	loss_scale 2048.0000 (2313.2342)	mem 19076MB
[2024-07-31 19:46:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:52 lr 0.000001	 wd 0.0500	time 0.5035 (0.5150)	loss 0.7759 (1.1782)	grad_norm 3.5286 (nan)	loss_scale 2048.0000 (2302.1874)	mem 19076MB
[2024-07-31 19:47:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5058 (0.5148)	loss 0.8532 (1.1784)	grad_norm 2.9695 (nan)	loss_scale 2048.0000 (2292.0240)	mem 19076MB
[2024-07-31 19:47:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 26 training takes 0:21:34
[2024-07-31 19:47:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.665 (11.665)	Loss 0.5171 (0.5171)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 19:48:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.686 Acc@5 97.700
[2024-07-31 19:48:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 19:48:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 19:48:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:34:16 lr 0.000001	 wd 0.0500	time 12.3328 (12.3328)	loss 1.4174 (1.4174)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:49:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:24:57 lr 0.000001	 wd 0.0500	time 0.4933 (0.6236)	loss 1.3083 (1.1853)	grad_norm 1.3834 (2.0095)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:50:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:21:42 lr 0.000001	 wd 0.0500	time 0.4992 (0.5657)	loss 1.2532 (1.1924)	grad_norm 1.5463 (2.0599)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:51:01 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:20:03 lr 0.000001	 wd 0.0500	time 0.5018 (0.5465)	loss 1.3341 (1.1889)	grad_norm 1.8694 (2.3480)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:51:52 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:18:48 lr 0.000001	 wd 0.0500	time 0.4923 (0.5371)	loss 1.0045 (1.1883)	grad_norm 2.3270 (2.2955)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:52:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:17:44 lr 0.000001	 wd 0.0500	time 0.5061 (0.5315)	loss 0.7871 (1.1891)	grad_norm 1.8361 (2.2721)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:53:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:16:44 lr 0.000001	 wd 0.0500	time 0.5032 (0.5280)	loss 1.3805 (1.1880)	grad_norm 1.8217 (2.3256)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:54:25 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:15:46 lr 0.000001	 wd 0.0500	time 0.5029 (0.5254)	loss 0.7875 (1.1880)	grad_norm 1.9126 (2.2969)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:55:16 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:14:51 lr 0.000001	 wd 0.0500	time 0.5048 (0.5236)	loss 1.1548 (1.1882)	grad_norm 1.6445 (2.4287)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:56:07 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:13:56 lr 0.000001	 wd 0.0500	time 0.5030 (0.5222)	loss 1.3417 (1.1870)	grad_norm 1.0986 (2.4007)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:56:58 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:13:02 lr 0.000001	 wd 0.0500	time 0.5021 (0.5210)	loss 0.9665 (1.1826)	grad_norm 1.5051 (2.3753)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:57:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:12:09 lr 0.000001	 wd 0.0500	time 0.5021 (0.5200)	loss 1.1854 (1.1842)	grad_norm 1.6937 (2.3540)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:58:40 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:11:16 lr 0.000001	 wd 0.0500	time 0.5015 (0.5192)	loss 0.8702 (1.1837)	grad_norm 8.6745 (2.3873)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 19:59:31 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:10:23 lr 0.000001	 wd 0.0500	time 0.5013 (0.5186)	loss 0.8391 (1.1793)	grad_norm 1.9985 (2.3684)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:00:22 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:09:30 lr 0.000001	 wd 0.0500	time 0.5023 (0.5180)	loss 0.8919 (1.1806)	grad_norm 1.5496 (2.3665)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:01:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:08:38 lr 0.000001	 wd 0.0500	time 0.5049 (0.5175)	loss 1.4828 (1.1818)	grad_norm 5.2146 (2.3633)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:02:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:07:46 lr 0.000001	 wd 0.0500	time 0.5053 (0.5171)	loss 1.5391 (1.1833)	grad_norm 1.9526 (2.3437)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:02:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:06:54 lr 0.000001	 wd 0.0500	time 0.5035 (0.5167)	loss 1.3763 (1.1843)	grad_norm 1.7736 (2.3553)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:03:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:06:02 lr 0.000001	 wd 0.0500	time 0.5010 (0.5164)	loss 1.3866 (1.1835)	grad_norm 3.8816 (2.3322)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:04:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:05:10 lr 0.000001	 wd 0.0500	time 0.5028 (0.5161)	loss 1.4041 (1.1840)	grad_norm 2.3114 (2.3208)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:05:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:04:18 lr 0.000001	 wd 0.0500	time 0.5042 (0.5158)	loss 1.4041 (1.1843)	grad_norm 2.5087 (2.3241)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:06:19 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:03:27 lr 0.000001	 wd 0.0500	time 0.5035 (0.5156)	loss 1.5773 (1.1840)	grad_norm 2.0803 (2.3082)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:07:10 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:02:35 lr 0.000001	 wd 0.0500	time 0.5080 (0.5154)	loss 1.2678 (1.1847)	grad_norm 1.9101 (2.2923)	loss_scale 2048.0000 (2048.0000)	mem 19076MB
[2024-07-31 20:08:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:44 lr 0.000001	 wd 0.0500	time 0.4992 (0.5152)	loss 1.3327 (1.1847)	grad_norm 1.4261 (2.2832)	loss_scale 4096.0000 (2078.2616)	mem 19076MB
[2024-07-31 20:08:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:52 lr 0.000001	 wd 0.0500	time 0.4936 (0.5150)	loss 1.3889 (1.1861)	grad_norm 2.1971 (2.2710)	loss_scale 4096.0000 (2162.2990)	mem 19076MB
[2024-07-31 20:09:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:01 lr 0.000001	 wd 0.0500	time 0.5069 (0.5149)	loss 1.1829 (1.1852)	grad_norm 2.5476 (2.2629)	loss_scale 4096.0000 (2239.6162)	mem 19076MB
[2024-07-31 20:09:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 27 training takes 0:21:32
[2024-07-31 20:10:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.654 (12.654)	Loss 0.5049 (0.5049)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 20:10:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.714 Acc@5 97.706
[2024-07-31 20:10:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 20:10:28 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 20:10:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][0/2502]	eta 8:02:19 lr 0.000001	 wd 0.0500	time 11.5664 (11.5664)	loss 0.7693 (0.7693)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:11:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:24:43 lr 0.000000	 wd 0.0500	time 0.4986 (0.6177)	loss 0.7958 (1.2056)	grad_norm 4.3956 (2.1181)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:12:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:21:35 lr 0.000000	 wd 0.0500	time 0.5009 (0.5627)	loss 1.4058 (1.2001)	grad_norm 2.2696 (2.1135)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:13:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:19:58 lr 0.000000	 wd 0.0500	time 0.4995 (0.5445)	loss 1.4448 (1.1933)	grad_norm 1.7059 (2.1308)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:14:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:18:45 lr 0.000000	 wd 0.0500	time 0.5021 (0.5356)	loss 0.7937 (1.1845)	grad_norm 1.2954 (2.2703)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:14:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:17:41 lr 0.000000	 wd 0.0500	time 0.5005 (0.5304)	loss 0.8290 (1.1900)	grad_norm 1.7399 (2.2335)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:15:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:16:42 lr 0.000000	 wd 0.0500	time 0.5042 (0.5271)	loss 0.9003 (1.1826)	grad_norm 3.2690 (2.2169)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:16:36 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:15:45 lr 0.000000	 wd 0.0500	time 0.5062 (0.5247)	loss 1.2311 (1.1792)	grad_norm 1.7885 (2.2242)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:17:27 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:14:50 lr 0.000000	 wd 0.0500	time 0.4977 (0.5230)	loss 0.8558 (1.1808)	grad_norm 1.5675 (2.2128)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:18:18 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:13:55 lr 0.000000	 wd 0.0500	time 0.5037 (0.5216)	loss 0.8004 (1.1773)	grad_norm 1.9920 (2.3124)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:19:09 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:13:01 lr 0.000000	 wd 0.0500	time 0.5047 (0.5205)	loss 1.0532 (1.1788)	grad_norm 1.9127 (2.2818)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:20:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:12:08 lr 0.000000	 wd 0.0500	time 0.5068 (0.5196)	loss 1.2075 (1.1801)	grad_norm 2.2031 (2.2623)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:20:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:11:15 lr 0.000000	 wd 0.0500	time 0.5008 (0.5188)	loss 1.4029 (1.1809)	grad_norm 3.3543 (2.2568)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:21:42 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:10:22 lr 0.000000	 wd 0.0500	time 0.5038 (0.5182)	loss 1.4580 (1.1830)	grad_norm 1.6301 (2.2552)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:22:33 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:09:30 lr 0.000000	 wd 0.0500	time 0.5023 (0.5177)	loss 0.8812 (1.1816)	grad_norm 1.1729 (2.2364)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:23:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:08:38 lr 0.000000	 wd 0.0500	time 0.5065 (0.5173)	loss 1.1987 (1.1812)	grad_norm 1.3430 (2.2149)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:24:15 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:07:46 lr 0.000000	 wd 0.0500	time 0.5053 (0.5169)	loss 1.4583 (1.1836)	grad_norm 2.0984 (2.2182)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:25:06 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:06:54 lr 0.000000	 wd 0.0500	time 0.4996 (0.5165)	loss 1.4158 (1.1835)	grad_norm 1.9248 (2.2236)	loss_scale 4096.0000 (4096.0000)	mem 19076MB
[2024-07-31 20:25:57 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:06:02 lr 0.000000	 wd 0.0500	time 0.4993 (0.5162)	loss 0.7941 (1.1831)	grad_norm 2.2187 (nan)	loss_scale 2048.0000 (4036.8684)	mem 19076MB
[2024-07-31 20:26:48 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:05:10 lr 0.000000	 wd 0.0500	time 0.5027 (0.5159)	loss 1.0338 (1.1804)	grad_norm 1.9109 (nan)	loss_scale 2048.0000 (3932.2462)	mem 19076MB
[2024-07-31 20:27:39 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:04:18 lr 0.000000	 wd 0.0500	time 0.5018 (0.5156)	loss 1.1720 (1.1804)	grad_norm 1.9928 (nan)	loss_scale 1024.0000 (3813.5172)	mem 19076MB
[2024-07-31 20:28:30 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:03:27 lr 0.000000	 wd 0.0500	time 0.5031 (0.5153)	loss 1.0521 (1.1811)	grad_norm 3.6936 (nan)	loss_scale 1024.0000 (3680.7463)	mem 19076MB
[2024-07-31 20:29:21 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:02:35 lr 0.000000	 wd 0.0500	time 0.5014 (0.5151)	loss 1.3485 (1.1805)	grad_norm 1.8034 (nan)	loss_scale 1024.0000 (3560.0400)	mem 19076MB
[2024-07-31 20:30:13 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:44 lr 0.000000	 wd 0.0500	time 0.5007 (0.5149)	loss 1.3598 (1.1804)	grad_norm 1.8106 (nan)	loss_scale 1024.0000 (3449.8253)	mem 19076MB
[2024-07-31 20:31:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:52 lr 0.000000	 wd 0.0500	time 0.4939 (0.5147)	loss 1.5881 (1.1795)	grad_norm 1.5443 (nan)	loss_scale 1024.0000 (3348.7913)	mem 19076MB
[2024-07-31 20:31:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.5015 (0.5146)	loss 1.0752 (1.1787)	grad_norm 1.7795 (nan)	loss_scale 1024.0000 (3255.8369)	mem 19076MB
[2024-07-31 20:32:00 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 28 training takes 0:21:32
[2024-07-31 20:32:12 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 11.760 (11.760)	Loss 0.5117 (0.5117)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 19076MB
[2024-07-31 20:32:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.676 Acc@5 97.688
[2024-07-31 20:32:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 20:32:38 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 20:32:51 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:56:05 lr 0.000000	 wd 0.0500	time 12.8559 (12.8559)	loss 1.3787 (1.3787)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:33:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:25:09 lr 0.000000	 wd 0.0500	time 0.4904 (0.6284)	loss 1.4299 (1.1947)	grad_norm 1.6983 (2.0802)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:34:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:21:47 lr 0.000000	 wd 0.0500	time 0.4992 (0.5678)	loss 0.8265 (1.1952)	grad_norm 2.4196 (2.1187)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:35:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:20:05 lr 0.000000	 wd 0.0500	time 0.5012 (0.5476)	loss 0.7301 (1.1832)	grad_norm 1.7046 (2.2054)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:36:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:18:50 lr 0.000000	 wd 0.0500	time 0.5063 (0.5378)	loss 1.0635 (1.1797)	grad_norm 2.4209 (2.2879)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:37:04 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:17:45 lr 0.000000	 wd 0.0500	time 0.5028 (0.5322)	loss 1.2430 (1.1755)	grad_norm 2.1630 (2.2864)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:37:55 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:16:45 lr 0.000000	 wd 0.0500	time 0.5073 (0.5285)	loss 0.7243 (1.1775)	grad_norm 1.8131 (2.4167)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:38:46 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:15:47 lr 0.000000	 wd 0.0500	time 0.4931 (0.5258)	loss 1.1891 (1.1776)	grad_norm 2.1474 (2.3460)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:39:37 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:14:51 lr 0.000000	 wd 0.0500	time 0.5008 (0.5239)	loss 0.8765 (1.1791)	grad_norm 1.4041 (2.3242)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:40:29 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:13:56 lr 0.000000	 wd 0.0500	time 0.5016 (0.5224)	loss 1.4542 (1.1804)	grad_norm 1.6880 (2.3324)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:41:20 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:13:02 lr 0.000000	 wd 0.0500	time 0.5049 (0.5212)	loss 1.3249 (1.1805)	grad_norm 2.3598 (2.3306)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:42:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:12:09 lr 0.000000	 wd 0.0500	time 0.5054 (0.5202)	loss 1.3152 (1.1783)	grad_norm 2.3363 (2.3206)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:43:02 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:11:16 lr 0.000000	 wd 0.0500	time 0.5050 (0.5194)	loss 0.9773 (1.1771)	grad_norm 2.0438 (2.2951)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:43:53 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:10:23 lr 0.000000	 wd 0.0500	time 0.5019 (0.5187)	loss 1.2330 (1.1808)	grad_norm 2.2117 (2.2958)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:44:44 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:09:31 lr 0.000000	 wd 0.0500	time 0.5029 (0.5182)	loss 1.1136 (1.1758)	grad_norm 2.2318 (2.3016)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:45:35 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:08:38 lr 0.000000	 wd 0.0500	time 0.5012 (0.5176)	loss 0.9996 (1.1749)	grad_norm 1.9036 (2.2805)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:46:26 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:07:46 lr 0.000000	 wd 0.0500	time 0.5033 (0.5172)	loss 0.8339 (1.1776)	grad_norm 1.5514 (2.3065)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:47:17 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:06:54 lr 0.000000	 wd 0.0500	time 0.5037 (0.5169)	loss 1.2499 (1.1798)	grad_norm 2.2871 (2.3004)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:48:08 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:06:02 lr 0.000000	 wd 0.0500	time 0.5027 (0.5165)	loss 1.3273 (1.1790)	grad_norm 1.6962 (2.2866)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:48:59 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:05:10 lr 0.000000	 wd 0.0500	time 0.5021 (0.5162)	loss 0.8992 (1.1792)	grad_norm 1.8622 (2.2725)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:49:50 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:04:18 lr 0.000000	 wd 0.0500	time 0.5049 (0.5159)	loss 0.9232 (1.1812)	grad_norm 4.5283 (2.2704)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:50:41 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:03:27 lr 0.000000	 wd 0.0500	time 0.5086 (0.5156)	loss 0.8769 (1.1825)	grad_norm 1.9646 (2.2901)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:51:32 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:02:35 lr 0.000000	 wd 0.0500	time 0.5002 (0.5154)	loss 0.8590 (1.1830)	grad_norm 2.3042 (2.2754)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:52:23 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:44 lr 0.000000	 wd 0.0500	time 0.5046 (0.5152)	loss 0.7407 (1.1801)	grad_norm 2.6609 (2.2745)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:53:14 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:52 lr 0.000000	 wd 0.0500	time 0.5029 (0.5150)	loss 1.2942 (1.1803)	grad_norm 2.0539 (2.2626)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:54:05 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:01 lr 0.000000	 wd 0.0500	time 0.4997 (0.5148)	loss 1.2802 (1.1797)	grad_norm 1.6334 (2.2530)	loss_scale 1024.0000 (1024.0000)	mem 19076MB
[2024-07-31 20:54:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 249): INFO EPOCH 29 training takes 0:21:33
[2024-07-31 20:54:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 145): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_29.pth saving......
[2024-07-31 20:54:11 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (utils.py 147): INFO pretrain/diffusion_ft/smt_diffusion_finetune_large_224_22kto1k_step_stage0/diffusion_ft_smt_l_step_stage0/ckpt_epoch_29.pth saved !!!
[2024-07-31 20:54:24 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 289): INFO Test: [0/98]	Time 12.186 (12.186)	Loss 0.5010 (0.5010)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 19076MB
[2024-07-31 20:54:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 296): INFO  * Acc@1 85.704 Acc@5 97.682
[2024-07-31 20:54:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.7%
[2024-07-31 20:54:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 182): INFO Max accuracy: 85.72%
[2024-07-31 20:54:49 smt_diffusion_finetune_large_224_22kto1k_step_stage0] (main.py 189): INFO Training time 11:02:53
