[2024-07-31 15:50:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/config.json
[2024-07-31 15:50:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: stage2
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_step_stage2
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-31 15:50:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_step_stage2.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_step_stage2", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-31 15:50:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2
[2024-07-31 15:50:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-31 15:50:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 113): INFO number of params: 2848360
[2024-07-31 15:50:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2, ignoring auto resume
[2024-07-31 15:50:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth for fine-tuning......
[2024-07-31 15:50:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-31 15:50:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage1/diffusion_ft_adapter_smt_l_step_stage1/ckpt_epoch_best.pth'
[2024-07-31 15:50:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 14.684 (14.684)	Loss 0.4836 (0.4836)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 2333MB
[2024-07-31 15:50:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.774 Acc@5 97.764
[2024-07-31 15:50:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 15:50:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 168): INFO Start training
[2024-07-31 15:51:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:32:57 lr 0.000000	 wd 0.0500	time 12.3013 (12.3013)	loss 1.5226 (1.5226)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 9639MB
[2024-07-31 15:51:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:14:46 lr 0.000000	 wd 0.0500	time 0.2278 (0.3692)	loss 1.2860 (1.2143)	grad_norm 0.3587 (nan)	loss_scale 16384.0000 (32443.5644)	mem 9639MB
[2024-07-31 15:51:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:11:54 lr 0.000000	 wd 0.0500	time 0.2632 (0.3103)	loss 1.0273 (1.1994)	grad_norm 0.3427 (nan)	loss_scale 8192.0000 (21926.8458)	mem 9639MB
[2024-07-31 15:52:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:10:56 lr 0.000000	 wd 0.0500	time 0.2348 (0.2982)	loss 0.9104 (1.1622)	grad_norm 0.3609 (nan)	loss_scale 8192.0000 (17363.7741)	mem 9639MB
[2024-07-31 15:52:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:10:04 lr 0.000001	 wd 0.0500	time 0.2924 (0.2875)	loss 0.9712 (1.1695)	grad_norm 0.3514 (nan)	loss_scale 8192.0000 (15076.5486)	mem 9639MB
[2024-07-31 15:53:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:09:23 lr 0.000001	 wd 0.0500	time 0.2357 (0.2812)	loss 1.1439 (1.1742)	grad_norm 0.3262 (nan)	loss_scale 8192.0000 (13702.3872)	mem 9639MB
[2024-07-31 15:53:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:08:50 lr 0.000001	 wd 0.0500	time 0.2387 (0.2789)	loss 1.3209 (1.1742)	grad_norm 0.3343 (nan)	loss_scale 4096.0000 (12281.1847)	mem 9639MB
[2024-07-31 15:54:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:08:16 lr 0.000001	 wd 0.0500	time 0.2730 (0.2753)	loss 1.2422 (1.1763)	grad_norm 0.3513 (nan)	loss_scale 4096.0000 (11113.5407)	mem 9639MB
[2024-07-31 15:54:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:07:44 lr 0.000001	 wd 0.0500	time 0.2321 (0.2726)	loss 1.0654 (1.1759)	grad_norm 0.3346 (nan)	loss_scale 4096.0000 (10237.4432)	mem 9639MB
[2024-07-31 15:54:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:07:17 lr 0.000001	 wd 0.0500	time 0.2250 (0.2730)	loss 1.3592 (1.1757)	grad_norm 0.3428 (nan)	loss_scale 4096.0000 (9555.8180)	mem 9639MB
[2024-07-31 15:55:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:06:46 lr 0.000002	 wd 0.0500	time 0.2621 (0.2707)	loss 1.4539 (1.1734)	grad_norm 0.3491 (nan)	loss_scale 4096.0000 (9010.3816)	mem 9639MB
[2024-07-31 15:55:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:06:17 lr 0.000002	 wd 0.0500	time 0.2458 (0.2696)	loss 1.2477 (1.1744)	grad_norm 0.3521 (nan)	loss_scale 4096.0000 (8564.0254)	mem 9639MB
[2024-07-31 15:56:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:05:48 lr 0.000002	 wd 0.0500	time 0.2150 (0.2680)	loss 0.9382 (1.1741)	grad_norm 0.3374 (nan)	loss_scale 4096.0000 (8192.0000)	mem 9639MB
[2024-07-31 15:56:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:05:20 lr 0.000002	 wd 0.0500	time 0.2349 (0.2664)	loss 1.2361 (1.1758)	grad_norm 0.3390 (nan)	loss_scale 4096.0000 (7877.1653)	mem 9639MB
[2024-07-31 15:56:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:04:52 lr 0.000002	 wd 0.0500	time 0.2724 (0.2655)	loss 1.4505 (1.1776)	grad_norm 0.3414 (nan)	loss_scale 4096.0000 (7607.2748)	mem 9639MB
[2024-07-31 15:57:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:04:25 lr 0.000002	 wd 0.0500	time 0.2310 (0.2651)	loss 1.1778 (1.1815)	grad_norm 0.3472 (nan)	loss_scale 4096.0000 (7373.3458)	mem 9639MB
[2024-07-31 15:57:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:03:58 lr 0.000003	 wd 0.0500	time 0.2469 (0.2644)	loss 1.2148 (1.1785)	grad_norm 0.3367 (nan)	loss_scale 4096.0000 (7168.6396)	mem 9639MB
[2024-07-31 15:58:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:03:31 lr 0.000003	 wd 0.0500	time 0.2590 (0.2641)	loss 0.8952 (1.1779)	grad_norm 0.3560 (nan)	loss_scale 4096.0000 (6988.0024)	mem 9639MB
[2024-07-31 15:58:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:05 lr 0.000003	 wd 0.0500	time 0.2308 (0.2639)	loss 1.1251 (1.1781)	grad_norm 0.3355 (nan)	loss_scale 4096.0000 (6827.4248)	mem 9639MB
[2024-07-31 15:59:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:02:38 lr 0.000003	 wd 0.0500	time 0.2507 (0.2640)	loss 1.4185 (1.1777)	grad_norm 0.3391 (nan)	loss_scale 4096.0000 (6683.7412)	mem 9639MB
[2024-07-31 15:59:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:12 lr 0.000003	 wd 0.0500	time 0.2375 (0.2636)	loss 0.8539 (1.1767)	grad_norm 0.3455 (nan)	loss_scale 4096.0000 (6554.4188)	mem 9639MB
[2024-07-31 16:00:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:45 lr 0.000003	 wd 0.0500	time 0.2215 (0.2630)	loss 1.0720 (1.1755)	grad_norm 0.3486 (nan)	loss_scale 4096.0000 (6437.4069)	mem 9639MB
[2024-07-31 16:00:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:19 lr 0.000004	 wd 0.0500	time 0.2725 (0.2625)	loss 1.5344 (1.1750)	grad_norm 0.3476 (nan)	loss_scale 4096.0000 (6331.0277)	mem 9639MB
[2024-07-31 16:00:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:52 lr 0.000004	 wd 0.0500	time 0.2595 (0.2622)	loss 1.3450 (1.1752)	grad_norm 0.3588 (nan)	loss_scale 4096.0000 (6233.8948)	mem 9639MB
[2024-07-31 16:01:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:26 lr 0.000004	 wd 0.0500	time 0.2356 (0.2619)	loss 1.3143 (1.1751)	grad_norm 0.3401 (nan)	loss_scale 4096.0000 (6144.8530)	mem 9639MB
[2024-07-31 16:01:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2385 (0.2609)	loss 0.8858 (1.1732)	grad_norm 0.3487 (nan)	loss_scale 2048.0000 (6026.9012)	mem 9639MB
[2024-07-31 16:01:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 0 training takes 0:10:55
[2024-07-31 16:01:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_0.pth saving......
[2024-07-31 16:01:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_0.pth saved !!!
[2024-07-31 16:01:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 10.608 (10.608)	Loss 0.5176 (0.5176)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 9639MB
[2024-07-31 16:02:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.802 Acc@5 97.744
[2024-07-31 16:02:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-07-31 16:02:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.80%
[2024-07-31 16:02:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 16:02:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 16:02:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:25:05 lr 0.000004	 wd 0.0500	time 10.6737 (10.6737)	loss 1.1172 (1.1172)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:02:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:14:19 lr 0.000004	 wd 0.0500	time 0.2318 (0.3578)	loss 1.1878 (1.1549)	grad_norm 0.3463 (0.3457)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:03:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:11:40 lr 0.000004	 wd 0.0500	time 0.2275 (0.3045)	loss 0.7701 (1.1720)	grad_norm 0.3453 (0.3462)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:03:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:10:32 lr 0.000004	 wd 0.0500	time 0.2568 (0.2871)	loss 0.8841 (1.1717)	grad_norm 0.3469 (0.3480)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:04:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:09:46 lr 0.000005	 wd 0.0500	time 0.2629 (0.2788)	loss 0.9809 (1.1868)	grad_norm 0.3490 (0.3493)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:04:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:09:09 lr 0.000005	 wd 0.0500	time 0.2469 (0.2744)	loss 1.4581 (1.1840)	grad_norm 0.3362 (0.3488)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:04:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:08:34 lr 0.000005	 wd 0.0500	time 0.2389 (0.2707)	loss 1.5031 (1.1829)	grad_norm 0.3316 (0.3484)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:05:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:08:03 lr 0.000005	 wd 0.0500	time 0.2278 (0.2686)	loss 0.8795 (1.1796)	grad_norm 0.3325 (0.3476)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:05:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:07:34 lr 0.000005	 wd 0.0500	time 0.2247 (0.2669)	loss 1.4334 (1.1787)	grad_norm 0.3235 (0.3481)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:06:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:07:04 lr 0.000005	 wd 0.0500	time 0.2664 (0.2648)	loss 1.1836 (1.1777)	grad_norm 0.3645 (0.3478)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:06:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:06:35 lr 0.000006	 wd 0.0500	time 0.2546 (0.2634)	loss 1.3734 (1.1763)	grad_norm 0.3510 (0.3491)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:07:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:06:08 lr 0.000006	 wd 0.0500	time 0.2325 (0.2626)	loss 1.4323 (1.1748)	grad_norm 0.3409 (0.3490)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:07:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:05:41 lr 0.000006	 wd 0.0500	time 0.2389 (0.2622)	loss 0.9604 (1.1726)	grad_norm 0.3364 (0.3491)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:07:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:05:14 lr 0.000006	 wd 0.0500	time 0.2325 (0.2618)	loss 1.5431 (1.1736)	grad_norm 0.3786 (0.3496)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:08:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:04:48 lr 0.000006	 wd 0.0500	time 0.2616 (0.2615)	loss 0.9555 (1.1735)	grad_norm 0.3956 (0.3492)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:08:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:04:21 lr 0.000006	 wd 0.0500	time 0.2314 (0.2611)	loss 1.4178 (1.1742)	grad_norm 0.3381 (0.3490)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:09:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:03:56 lr 0.000007	 wd 0.0500	time 0.2288 (0.2623)	loss 1.2998 (1.1759)	grad_norm 0.3426 (0.3488)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:09:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:03:29 lr 0.000007	 wd 0.0500	time 0.2707 (0.2617)	loss 0.9942 (1.1762)	grad_norm 0.3599 (0.3486)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:10:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:03 lr 0.000007	 wd 0.0500	time 0.2886 (0.2614)	loss 1.7068 (1.1755)	grad_norm 0.3436 (0.3486)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:10:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:02:37 lr 0.000007	 wd 0.0500	time 0.2204 (0.2611)	loss 1.4462 (1.1767)	grad_norm 0.3541 (0.3488)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:10:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:10 lr 0.000007	 wd 0.0500	time 0.2266 (0.2609)	loss 1.4116 (1.1771)	grad_norm 0.3531 (0.3484)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:11:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:44 lr 0.000007	 wd 0.0500	time 0.2283 (0.2607)	loss 1.5031 (1.1792)	grad_norm 0.3643 (0.3485)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:11:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:18 lr 0.000008	 wd 0.0500	time 0.3475 (0.2604)	loss 1.2374 (1.1796)	grad_norm 0.3375 (0.3483)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:12:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:52 lr 0.000008	 wd 0.0500	time 0.2274 (0.2609)	loss 0.9658 (1.1819)	grad_norm 0.3394 (0.3481)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:12:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:26 lr 0.000008	 wd 0.0500	time 0.2566 (0.2605)	loss 1.2165 (1.1819)	grad_norm 0.3390 (0.3479)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:13:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2161 (0.2595)	loss 0.9137 (1.1820)	grad_norm 0.3425 (0.3478)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:13:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 1 training takes 0:10:51
[2024-07-31 16:13:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.882 (11.882)	Loss 0.5088 (0.5088)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 9639MB
[2024-07-31 16:13:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.858 Acc@5 97.764
[2024-07-31 16:13:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-31 16:13:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.86%
[2024-07-31 16:13:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 16:13:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 16:13:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:35:45 lr 0.000008	 wd 0.0500	time 10.9293 (10.9293)	loss 1.2484 (1.2484)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:14:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:15:18 lr 0.000008	 wd 0.0500	time 0.2546 (0.3825)	loss 1.4230 (1.1848)	grad_norm 0.3393 (0.3446)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:14:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:12:15 lr 0.000008	 wd 0.0500	time 0.2626 (0.3194)	loss 1.4043 (1.1755)	grad_norm 0.3351 (0.3446)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:15:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:10:54 lr 0.000008	 wd 0.0500	time 0.2364 (0.2974)	loss 1.1874 (1.1661)	grad_norm 0.3446 (0.3503)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:15:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:10:04 lr 0.000009	 wd 0.0500	time 0.2316 (0.2874)	loss 1.4284 (1.1691)	grad_norm 0.3299 (0.3483)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:15:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:09:23 lr 0.000009	 wd 0.0500	time 0.2406 (0.2812)	loss 1.4358 (1.1648)	grad_norm 0.3532 (0.3474)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:16:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:08:46 lr 0.000009	 wd 0.0500	time 0.2375 (0.2767)	loss 1.2820 (1.1659)	grad_norm 0.3340 (0.3473)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:16:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:08:12 lr 0.000009	 wd 0.0500	time 0.2630 (0.2731)	loss 1.3285 (1.1709)	grad_norm 0.3449 (0.3462)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:17:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:07:40 lr 0.000009	 wd 0.0500	time 0.2232 (0.2703)	loss 1.3936 (1.1738)	grad_norm 0.3333 (0.3456)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:17:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:07:10 lr 0.000009	 wd 0.0500	time 0.2493 (0.2684)	loss 0.9435 (1.1788)	grad_norm 0.3522 (0.3455)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:17:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:06:41 lr 0.000010	 wd 0.0500	time 0.2865 (0.2672)	loss 0.8750 (1.1759)	grad_norm 0.3552 (0.3449)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:18:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:06:13 lr 0.000010	 wd 0.0500	time 0.2622 (0.2662)	loss 1.3808 (1.1734)	grad_norm 0.3533 (0.3447)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:18:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:05:45 lr 0.000010	 wd 0.0500	time 0.2350 (0.2654)	loss 0.8991 (1.1738)	grad_norm 0.3609 (0.3452)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:19:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:05:17 lr 0.000010	 wd 0.0500	time 0.2557 (0.2645)	loss 1.4134 (1.1748)	grad_norm 0.3533 (0.3455)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:19:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:04:50 lr 0.000010	 wd 0.0500	time 0.2505 (0.2638)	loss 1.2214 (1.1765)	grad_norm 0.3391 (0.3452)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:20:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:04:23 lr 0.000010	 wd 0.0500	time 0.3046 (0.2630)	loss 1.3346 (1.1770)	grad_norm 0.3387 (0.3459)	loss_scale 4096.0000 (2113.4923)	mem 9639MB
[2024-07-31 16:20:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:03:56 lr 0.000011	 wd 0.0500	time 0.2433 (0.2625)	loss 0.9730 (1.1774)	grad_norm 0.3332 (0.3476)	loss_scale 4096.0000 (2237.3217)	mem 9639MB
[2024-07-31 16:20:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:03:30 lr 0.000011	 wd 0.0500	time 0.2407 (0.2622)	loss 1.2996 (1.1778)	grad_norm 0.3302 (0.3474)	loss_scale 4096.0000 (2346.5914)	mem 9639MB
[2024-07-31 16:21:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:03 lr 0.000011	 wd 0.0500	time 0.2196 (0.2620)	loss 1.0713 (1.1784)	grad_norm 0.3437 (0.3471)	loss_scale 4096.0000 (2443.7268)	mem 9639MB
[2024-07-31 16:21:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:02:37 lr 0.000011	 wd 0.0500	time 0.2172 (0.2616)	loss 1.3608 (1.1782)	grad_norm 0.3672 (0.3471)	loss_scale 4096.0000 (2530.6428)	mem 9639MB
[2024-07-31 16:22:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:11 lr 0.000011	 wd 0.0500	time 0.2892 (0.2615)	loss 1.1376 (1.1783)	grad_norm 0.3403 (0.3467)	loss_scale 4096.0000 (2608.8716)	mem 9639MB
[2024-07-31 16:22:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:45 lr 0.000011	 wd 0.0500	time 0.2291 (0.2612)	loss 1.0178 (1.1767)	grad_norm 0.3320 (0.3479)	loss_scale 4096.0000 (2679.6535)	mem 9639MB
[2024-07-31 16:23:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:18 lr 0.000012	 wd 0.0500	time 0.2130 (0.2608)	loss 1.3964 (1.1765)	grad_norm 0.3552 (0.3476)	loss_scale 4096.0000 (2744.0036)	mem 9639MB
[2024-07-31 16:23:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:52 lr 0.000012	 wd 0.0500	time 0.2283 (0.2604)	loss 1.2226 (1.1766)	grad_norm 0.3361 (0.3475)	loss_scale 4096.0000 (2802.7605)	mem 9639MB
[2024-07-31 16:23:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:26 lr 0.000012	 wd 0.0500	time 0.2235 (0.2599)	loss 0.7895 (1.1773)	grad_norm 0.3438 (0.3476)	loss_scale 4096.0000 (2856.6231)	mem 9639MB
[2024-07-31 16:24:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.6289 (0.2592)	loss 1.2061 (1.1776)	grad_norm 0.3255 (0.3473)	loss_scale 4096.0000 (2906.1783)	mem 9639MB
[2024-07-31 16:24:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 2 training takes 0:10:50
[2024-07-31 16:24:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.068 (12.068)	Loss 0.4907 (0.4907)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 16:24:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.872 Acc@5 97.772
[2024-07-31 16:24:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-31 16:24:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.87%
[2024-07-31 16:24:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 16:24:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 16:25:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:55:22 lr 0.000012	 wd 0.0500	time 11.3998 (11.3998)	loss 0.7518 (0.7518)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:25:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:14:16 lr 0.000012	 wd 0.0500	time 0.2515 (0.3567)	loss 1.2384 (1.2133)	grad_norm 0.3447 (0.3437)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:25:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:11:38 lr 0.000012	 wd 0.0500	time 0.2421 (0.3034)	loss 1.5178 (1.1875)	grad_norm 0.3529 (0.3443)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:26:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:10:32 lr 0.000012	 wd 0.0500	time 0.2443 (0.2871)	loss 1.4363 (1.1843)	grad_norm 0.3310 (0.3435)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:26:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:09:56 lr 0.000013	 wd 0.0500	time 0.2424 (0.2837)	loss 1.4207 (1.1766)	grad_norm 0.3440 (0.3428)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:27:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:09:14 lr 0.000013	 wd 0.0500	time 0.2204 (0.2769)	loss 1.0033 (1.1766)	grad_norm 0.3334 (0.3436)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:27:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:08:38 lr 0.000013	 wd 0.0500	time 0.2531 (0.2726)	loss 1.0567 (1.1713)	grad_norm 0.3552 (0.3431)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:08:07 lr 0.000013	 wd 0.0500	time 0.2486 (0.2703)	loss 1.5131 (1.1689)	grad_norm 0.3401 (0.3428)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:28:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:07:35 lr 0.000013	 wd 0.0500	time 0.2291 (0.2676)	loss 0.7776 (1.1646)	grad_norm 0.3472 (0.3430)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:28:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:07:04 lr 0.000013	 wd 0.0500	time 0.2286 (0.2652)	loss 1.5650 (1.1668)	grad_norm 0.3519 (0.3425)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:29:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:06:36 lr 0.000014	 wd 0.0500	time 0.2462 (0.2640)	loss 1.2981 (1.1675)	grad_norm 0.3188 (0.3430)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:29:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:06:09 lr 0.000014	 wd 0.0500	time 0.2449 (0.2635)	loss 0.9989 (1.1682)	grad_norm 0.3592 (0.3440)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:30:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:05:42 lr 0.000014	 wd 0.0500	time 0.2409 (0.2630)	loss 1.1565 (1.1678)	grad_norm 0.3435 (0.3439)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:30:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:05:15 lr 0.000014	 wd 0.0500	time 0.2510 (0.2621)	loss 1.3258 (1.1672)	grad_norm 0.3320 (0.3439)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:30:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:04:48 lr 0.000014	 wd 0.0500	time 0.2290 (0.2615)	loss 1.1367 (1.1682)	grad_norm 0.3318 (0.3440)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:31:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:04:21 lr 0.000014	 wd 0.0500	time 0.2629 (0.2610)	loss 1.4143 (1.1666)	grad_norm 0.3465 (0.3438)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:31:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:03:54 lr 0.000015	 wd 0.0500	time 0.2351 (0.2603)	loss 0.7844 (1.1650)	grad_norm 0.3679 (0.3440)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:32:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:03:28 lr 0.000015	 wd 0.0500	time 0.2343 (0.2600)	loss 0.9914 (1.1674)	grad_norm 0.3244 (0.3441)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:32:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:02 lr 0.000015	 wd 0.0500	time 0.2545 (0.2598)	loss 1.1505 (1.1685)	grad_norm 0.3546 (0.3453)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:33:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:02:36 lr 0.000015	 wd 0.0500	time 0.2265 (0.2597)	loss 1.2618 (1.1677)	grad_norm 0.3352 (0.3451)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:33:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:10 lr 0.000015	 wd 0.0500	time 0.2420 (0.2593)	loss 1.3285 (1.1682)	grad_norm 0.3380 (0.3449)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:33:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:44 lr 0.000015	 wd 0.0500	time 0.2341 (0.2590)	loss 0.9780 (1.1686)	grad_norm 0.3519 (0.3448)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:34:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:18 lr 0.000016	 wd 0.0500	time 0.2105 (0.2589)	loss 0.8137 (1.1678)	grad_norm 0.3389 (0.3447)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:34:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:00:52 lr 0.000016	 wd 0.0500	time 0.2330 (0.2586)	loss 0.9165 (1.1680)	grad_norm 0.3486 (0.3446)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:35:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:26 lr 0.000016	 wd 0.0500	time 0.2248 (0.2586)	loss 1.4155 (1.1697)	grad_norm 0.3553 (0.3454)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:35:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2161 (0.2578)	loss 1.3011 (1.1695)	grad_norm 0.3488 (0.3454)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:35:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 3 training takes 0:10:47
[2024-07-31 16:35:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.490 (12.490)	Loss 0.5068 (0.5068)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 16:36:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.888 Acc@5 97.796
[2024-07-31 16:36:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-31 16:36:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.89%
[2024-07-31 16:36:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 16:36:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 16:36:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:18:22 lr 0.000016	 wd 0.0500	time 10.5125 (10.5125)	loss 1.2690 (1.2690)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:36:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:14:10 lr 0.000016	 wd 0.0500	time 0.2511 (0.3539)	loss 0.9510 (1.1834)	grad_norm 0.3458 (0.3447)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:37:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:11:31 lr 0.000016	 wd 0.0500	time 0.2255 (0.3003)	loss 0.8647 (1.1717)	grad_norm 0.3469 (0.3433)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:37:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:10:23 lr 0.000016	 wd 0.0500	time 0.2472 (0.2830)	loss 0.8030 (1.1685)	grad_norm 0.3541 (0.3430)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:37:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:09:38 lr 0.000017	 wd 0.0500	time 0.2530 (0.2753)	loss 1.3046 (1.1636)	grad_norm 0.3433 (0.3434)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:38:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:09:03 lr 0.000017	 wd 0.0500	time 0.2637 (0.2716)	loss 1.2562 (1.1642)	grad_norm 0.3389 (0.3436)	loss_scale 8192.0000 (4521.1337)	mem 9639MB
[2024-07-31 16:38:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:08:31 lr 0.000017	 wd 0.0500	time 0.2366 (0.2688)	loss 1.1756 (1.1689)	grad_norm 0.3512 (0.3445)	loss_scale 8192.0000 (5131.9268)	mem 9639MB
[2024-07-31 16:39:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:08:00 lr 0.000017	 wd 0.0500	time 0.2287 (0.2666)	loss 0.8406 (1.1692)	grad_norm 0.3459 (0.3449)	loss_scale 8192.0000 (5568.4565)	mem 9639MB
[2024-07-31 16:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:07:30 lr 0.000017	 wd 0.0500	time 0.2472 (0.2647)	loss 0.7760 (1.1718)	grad_norm 0.3550 (0.3447)	loss_scale 8192.0000 (5895.9900)	mem 9639MB
[2024-07-31 16:40:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:07:01 lr 0.000017	 wd 0.0500	time 0.2218 (0.2633)	loss 0.8297 (1.1729)	grad_norm 0.3374 (0.3453)	loss_scale 8192.0000 (6150.8191)	mem 9639MB
[2024-07-31 16:40:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:06:33 lr 0.000018	 wd 0.0500	time 0.2261 (0.2623)	loss 1.5833 (1.1729)	grad_norm 0.3274 (0.3449)	loss_scale 8192.0000 (6354.7333)	mem 9639MB
[2024-07-31 16:40:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:06:06 lr 0.000018	 wd 0.0500	time 0.2304 (0.2616)	loss 1.4669 (1.1739)	grad_norm 0.3377 (0.3450)	loss_scale 8192.0000 (6521.6058)	mem 9639MB
[2024-07-31 16:41:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:05:39 lr 0.000018	 wd 0.0500	time 0.2367 (0.2610)	loss 1.2835 (1.1727)	grad_norm 0.3466 (0.3449)	loss_scale 8192.0000 (6660.6894)	mem 9639MB
[2024-07-31 16:41:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:05:13 lr 0.000018	 wd 0.0500	time 0.2273 (0.2604)	loss 1.0277 (1.1734)	grad_norm 0.3610 (0.3449)	loss_scale 8192.0000 (6778.3920)	mem 9639MB
[2024-07-31 16:42:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:04:46 lr 0.000018	 wd 0.0500	time 0.2364 (0.2599)	loss 1.4227 (1.1733)	grad_norm 0.3458 (0.3448)	loss_scale 8192.0000 (6879.2919)	mem 9639MB
[2024-07-31 16:42:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:04:19 lr 0.000018	 wd 0.0500	time 0.2198 (0.2593)	loss 1.2126 (1.1737)	grad_norm 0.3375 (0.3452)	loss_scale 8192.0000 (6966.7475)	mem 9639MB
[2024-07-31 16:43:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:03:54 lr 0.000019	 wd 0.0500	time 0.2590 (0.2603)	loss 1.4713 (1.1723)	grad_norm 0.3487 (0.3452)	loss_scale 8192.0000 (7043.2780)	mem 9639MB
[2024-07-31 16:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:03:28 lr 0.000019	 wd 0.0500	time 0.2991 (0.2599)	loss 1.1780 (1.1730)	grad_norm 0.3494 (0.3450)	loss_scale 8192.0000 (7110.8101)	mem 9639MB
[2024-07-31 16:43:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:02 lr 0.000019	 wd 0.0500	time 0.3036 (0.2596)	loss 1.5176 (1.1750)	grad_norm 0.3415 (nan)	loss_scale 4096.0000 (7016.1910)	mem 9639MB
[2024-07-31 16:44:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:02:35 lr 0.000019	 wd 0.0500	time 0.2445 (0.2591)	loss 1.4038 (1.1753)	grad_norm 0.3648 (nan)	loss_scale 4096.0000 (6862.5776)	mem 9639MB
[2024-07-31 16:44:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:10 lr 0.000019	 wd 0.0500	time 0.2379 (0.2592)	loss 0.8613 (1.1731)	grad_norm 0.3500 (nan)	loss_scale 4096.0000 (6724.3178)	mem 9639MB
[2024-07-31 16:45:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:44 lr 0.000019	 wd 0.0500	time 0.2349 (0.2590)	loss 1.0115 (1.1719)	grad_norm 0.3341 (nan)	loss_scale 4096.0000 (6599.2194)	mem 9639MB
[2024-07-31 16:45:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:18 lr 0.000020	 wd 0.0500	time 0.2259 (0.2584)	loss 0.8894 (1.1715)	grad_norm 0.3231 (nan)	loss_scale 4096.0000 (6485.4884)	mem 9639MB
[2024-07-31 16:46:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.2323 (0.2583)	loss 0.7971 (1.1708)	grad_norm 0.3424 (nan)	loss_scale 4096.0000 (6381.6428)	mem 9639MB
[2024-07-31 16:46:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:26 lr 0.000020	 wd 0.0500	time 0.2429 (0.2585)	loss 0.7913 (1.1707)	grad_norm 0.3187 (nan)	loss_scale 4096.0000 (6286.4473)	mem 9639MB
[2024-07-31 16:46:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2107 (0.2578)	loss 1.2438 (1.1714)	grad_norm 0.3561 (nan)	loss_scale 4096.0000 (6198.8645)	mem 9639MB
[2024-07-31 16:46:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 4 training takes 0:10:47
[2024-07-31 16:47:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.436 (12.436)	Loss 0.4961 (0.4961)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 9639MB
[2024-07-31 16:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.884 Acc@5 97.804
[2024-07-31 16:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-31 16:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.89%
[2024-07-31 16:47:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][0/2502]	eta 8:15:29 lr 0.000020	 wd 0.0500	time 11.8822 (11.8822)	loss 1.3911 (1.3911)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:47:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:14:16 lr 0.000020	 wd 0.0500	time 0.2251 (0.3567)	loss 0.9886 (1.2007)	grad_norm 0.3594 (0.3638)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:48:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:11:39 lr 0.000020	 wd 0.0500	time 0.2647 (0.3038)	loss 1.3508 (1.1715)	grad_norm 0.3420 (0.3551)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:48:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:10:30 lr 0.000020	 wd 0.0500	time 0.2236 (0.2862)	loss 1.0169 (1.1605)	grad_norm 0.3385 (0.3504)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:49:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:09:43 lr 0.000020	 wd 0.0500	time 0.2211 (0.2775)	loss 1.2680 (1.1693)	grad_norm 0.3523 (0.3492)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:49:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:09:06 lr 0.000020	 wd 0.0500	time 0.2535 (0.2729)	loss 1.0046 (1.1680)	grad_norm 0.7833 (0.3505)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 16:50:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:08:33 lr 0.000020	 wd 0.0500	time 0.2214 (0.2699)	loss 1.0607 (1.1693)	grad_norm 0.3574 (nan)	loss_scale 2048.0000 (3830.2030)	mem 9639MB
[2024-07-31 16:50:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:08:03 lr 0.000020	 wd 0.0500	time 0.2370 (0.2683)	loss 0.9319 (1.1680)	grad_norm 0.3303 (nan)	loss_scale 2048.0000 (3575.9658)	mem 9639MB
[2024-07-31 16:50:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:07:34 lr 0.000020	 wd 0.0500	time 0.2411 (0.2671)	loss 1.5194 (1.1671)	grad_norm 0.3382 (nan)	loss_scale 2048.0000 (3385.2085)	mem 9639MB
[2024-07-31 16:51:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:07:07 lr 0.000020	 wd 0.0500	time 0.2378 (0.2669)	loss 1.0682 (1.1659)	grad_norm 0.3357 (nan)	loss_scale 2048.0000 (3236.7947)	mem 9639MB
[2024-07-31 16:51:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:06:39 lr 0.000020	 wd 0.0500	time 0.2242 (0.2657)	loss 1.2565 (1.1641)	grad_norm 0.3225 (nan)	loss_scale 2048.0000 (3118.0340)	mem 9639MB
[2024-07-31 16:52:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:06:11 lr 0.000020	 wd 0.0500	time 0.2613 (0.2650)	loss 1.1844 (1.1651)	grad_norm 0.3515 (nan)	loss_scale 2048.0000 (3020.8465)	mem 9639MB
[2024-07-31 16:52:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:05:43 lr 0.000020	 wd 0.0500	time 0.2240 (0.2639)	loss 1.4177 (1.1637)	grad_norm 0.3363 (nan)	loss_scale 2048.0000 (2939.8435)	mem 9639MB
[2024-07-31 16:53:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:05:16 lr 0.000020	 wd 0.0500	time 0.2321 (0.2631)	loss 0.9970 (1.1645)	grad_norm 0.3471 (nan)	loss_scale 2048.0000 (2871.2929)	mem 9639MB
[2024-07-31 16:53:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:04:49 lr 0.000020	 wd 0.0500	time 0.2591 (0.2623)	loss 1.4428 (1.1681)	grad_norm 0.3547 (nan)	loss_scale 2048.0000 (2812.5282)	mem 9639MB
[2024-07-31 16:53:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:04:22 lr 0.000020	 wd 0.0500	time 0.2324 (0.2620)	loss 0.8443 (1.1692)	grad_norm 0.3552 (nan)	loss_scale 2048.0000 (2761.5936)	mem 9639MB
[2024-07-31 16:54:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:03:56 lr 0.000020	 wd 0.0500	time 0.2301 (0.2616)	loss 1.4165 (1.1719)	grad_norm 0.3492 (nan)	loss_scale 2048.0000 (2717.0219)	mem 9639MB
[2024-07-31 16:54:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:03:29 lr 0.000020	 wd 0.0500	time 0.2186 (0.2613)	loss 0.9676 (1.1721)	grad_norm 0.3296 (nan)	loss_scale 2048.0000 (2677.6908)	mem 9639MB
[2024-07-31 16:55:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:03 lr 0.000020	 wd 0.0500	time 0.2472 (0.2611)	loss 0.7691 (1.1700)	grad_norm 0.3401 (nan)	loss_scale 2048.0000 (2642.7274)	mem 9639MB
[2024-07-31 16:55:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:02:37 lr 0.000020	 wd 0.0500	time 0.2300 (0.2611)	loss 1.1389 (1.1695)	grad_norm 0.3374 (nan)	loss_scale 2048.0000 (2611.4424)	mem 9639MB
[2024-07-31 16:56:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:10 lr 0.000020	 wd 0.0500	time 0.2565 (0.2609)	loss 1.5692 (1.1710)	grad_norm 0.3399 (nan)	loss_scale 2048.0000 (2583.2844)	mem 9639MB
[2024-07-31 16:56:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:44 lr 0.000020	 wd 0.0500	time 0.2141 (0.2608)	loss 1.4797 (1.1705)	grad_norm 0.3505 (nan)	loss_scale 2048.0000 (2557.8068)	mem 9639MB
[2024-07-31 16:56:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:18 lr 0.000020	 wd 0.0500	time 0.2449 (0.2606)	loss 1.2244 (1.1717)	grad_norm 0.3370 (nan)	loss_scale 2048.0000 (2534.6443)	mem 9639MB
[2024-07-31 16:57:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.2616 (0.2604)	loss 1.1402 (1.1702)	grad_norm 0.3597 (nan)	loss_scale 2048.0000 (2513.4950)	mem 9639MB
[2024-07-31 16:57:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:26 lr 0.000020	 wd 0.0500	time 0.2517 (0.2602)	loss 0.7868 (1.1697)	grad_norm 0.3580 (nan)	loss_scale 2048.0000 (2494.1075)	mem 9639MB
[2024-07-31 16:58:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2257 (0.2593)	loss 1.3044 (1.1709)	grad_norm 0.3429 (nan)	loss_scale 2048.0000 (2476.2703)	mem 9639MB
[2024-07-31 16:58:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 5 training takes 0:10:51
[2024-07-31 16:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.534 (12.534)	Loss 0.4939 (0.4939)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 16:58:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.942 Acc@5 97.824
[2024-07-31 16:58:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-31 16:58:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.94%
[2024-07-31 16:58:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 16:58:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 16:58:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:00:03 lr 0.000020	 wd 0.0500	time 11.5121 (11.5121)	loss 1.2125 (1.2125)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:59:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:14:40 lr 0.000020	 wd 0.0500	time 0.2731 (0.3665)	loss 0.8982 (1.1931)	grad_norm 0.3575 (0.3498)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 16:59:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:11:51 lr 0.000020	 wd 0.0500	time 0.2185 (0.3091)	loss 0.8669 (1.1830)	grad_norm 0.3148 (0.3467)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:00:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:10:38 lr 0.000020	 wd 0.0500	time 0.2644 (0.2901)	loss 0.9615 (1.1747)	grad_norm 0.4617 (0.3474)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:00:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:10:06 lr 0.000020	 wd 0.0500	time 0.2296 (0.2884)	loss 0.7848 (1.1766)	grad_norm 0.3443 (0.3465)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:01:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:09:23 lr 0.000020	 wd 0.0500	time 0.2285 (0.2816)	loss 1.2062 (1.1769)	grad_norm 0.3384 (0.3463)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:01:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:08:46 lr 0.000020	 wd 0.0500	time 0.2678 (0.2769)	loss 0.8625 (1.1682)	grad_norm 0.3411 (0.3475)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:01:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:08:12 lr 0.000020	 wd 0.0500	time 0.2364 (0.2736)	loss 1.4853 (1.1707)	grad_norm 0.3207 (0.3463)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:02:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:07:41 lr 0.000020	 wd 0.0500	time 0.2310 (0.2712)	loss 0.7127 (1.1750)	grad_norm 0.3375 (0.3460)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:02:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:07:11 lr 0.000020	 wd 0.0500	time 0.2372 (0.2691)	loss 1.4900 (1.1715)	grad_norm 0.3281 (0.3458)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:03:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:06:41 lr 0.000020	 wd 0.0500	time 0.2327 (0.2673)	loss 1.0437 (1.1712)	grad_norm 0.3300 (0.3459)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:03:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:06:13 lr 0.000020	 wd 0.0500	time 0.2097 (0.2661)	loss 0.8560 (1.1679)	grad_norm 0.3453 (0.3464)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:04:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:05:45 lr 0.000020	 wd 0.0500	time 0.2628 (0.2654)	loss 1.2428 (1.1649)	grad_norm 0.3583 (0.3461)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:04:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:05:18 lr 0.000020	 wd 0.0500	time 0.2308 (0.2651)	loss 1.1013 (1.1637)	grad_norm 0.3360 (0.3462)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:04:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:04:51 lr 0.000020	 wd 0.0500	time 0.2723 (0.2644)	loss 0.9445 (1.1638)	grad_norm 0.3432 (0.3466)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:05:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:04:24 lr 0.000020	 wd 0.0500	time 0.2166 (0.2639)	loss 0.7366 (1.1647)	grad_norm 0.3365 (0.3467)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:05:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:03:57 lr 0.000020	 wd 0.0500	time 0.2526 (0.2637)	loss 1.4543 (1.1622)	grad_norm 0.3269 (0.3467)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:06:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:03:30 lr 0.000020	 wd 0.0500	time 0.2271 (0.2629)	loss 1.3813 (1.1615)	grad_norm 0.3373 (0.3468)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:06:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:04 lr 0.000020	 wd 0.0500	time 0.2968 (0.2625)	loss 1.2139 (1.1620)	grad_norm 0.3377 (0.3471)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:07:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:02:37 lr 0.000020	 wd 0.0500	time 0.2737 (0.2623)	loss 1.3467 (1.1631)	grad_norm 0.3388 (0.3469)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:07:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:11 lr 0.000020	 wd 0.0500	time 0.2442 (0.2620)	loss 0.9906 (1.1640)	grad_norm 0.3472 (0.3472)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:07:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:45 lr 0.000020	 wd 0.0500	time 0.2364 (0.2617)	loss 1.0100 (1.1641)	grad_norm 0.3463 (0.3471)	loss_scale 4096.0000 (2125.9819)	mem 9639MB
[2024-07-31 17:08:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:18 lr 0.000020	 wd 0.0500	time 0.2341 (0.2615)	loss 0.8117 (1.1635)	grad_norm 0.3757 (0.3469)	loss_scale 4096.0000 (2215.4875)	mem 9639MB
[2024-07-31 17:08:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.2700 (0.2613)	loss 1.0359 (1.1659)	grad_norm 0.3597 (0.3469)	loss_scale 4096.0000 (2297.2134)	mem 9639MB
[2024-07-31 17:09:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:26 lr 0.000020	 wd 0.0500	time 0.2400 (0.2613)	loss 1.3928 (1.1652)	grad_norm 0.3719 (0.3469)	loss_scale 4096.0000 (2372.1316)	mem 9639MB
[2024-07-31 17:09:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2163 (0.2604)	loss 1.3560 (1.1649)	grad_norm 0.3351 (0.3468)	loss_scale 4096.0000 (2441.0588)	mem 9639MB
[2024-07-31 17:09:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 6 training takes 0:10:53
[2024-07-31 17:09:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.145 (12.145)	Loss 0.5073 (0.5073)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 17:10:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.940 Acc@5 97.820
[2024-07-31 17:10:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-07-31 17:10:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.94%
[2024-07-31 17:10:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:12:18 lr 0.000020	 wd 0.0500	time 11.8059 (11.8059)	loss 0.9192 (0.9192)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:10:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:14:39 lr 0.000020	 wd 0.0500	time 0.2404 (0.3663)	loss 1.2039 (1.1492)	grad_norm 0.3399 (0.3438)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:11:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:11:51 lr 0.000020	 wd 0.0500	time 0.2213 (0.3092)	loss 1.1174 (1.1707)	grad_norm 0.3499 (0.3455)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:11:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:10:37 lr 0.000020	 wd 0.0500	time 0.2450 (0.2894)	loss 1.2995 (1.1655)	grad_norm 0.3443 (0.3460)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:11:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:09:48 lr 0.000020	 wd 0.0500	time 0.2303 (0.2800)	loss 0.8302 (1.1708)	grad_norm 0.3578 (0.3480)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:12:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:09:09 lr 0.000020	 wd 0.0500	time 0.2227 (0.2744)	loss 1.3424 (1.1677)	grad_norm 0.3327 (0.3492)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:12:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:08:35 lr 0.000020	 wd 0.0500	time 0.2497 (0.2710)	loss 1.3009 (1.1719)	grad_norm 0.3443 (0.3484)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:13:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:08:03 lr 0.000020	 wd 0.0500	time 0.2206 (0.2683)	loss 0.9723 (1.1666)	grad_norm 0.3376 (0.3482)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:13:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:07:33 lr 0.000020	 wd 0.0500	time 0.2269 (0.2666)	loss 0.8837 (1.1641)	grad_norm 0.3369 (0.3477)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:14:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:07:05 lr 0.000020	 wd 0.0500	time 0.2270 (0.2656)	loss 1.4408 (1.1657)	grad_norm 0.3343 (0.3478)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:14:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:06:36 lr 0.000020	 wd 0.0500	time 0.2302 (0.2642)	loss 0.8107 (1.1658)	grad_norm 0.3640 (0.3474)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:14:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:06:09 lr 0.000020	 wd 0.0500	time 0.2229 (0.2634)	loss 1.3654 (1.1651)	grad_norm 0.3411 (0.3474)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:15:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:05:41 lr 0.000020	 wd 0.0500	time 0.2242 (0.2624)	loss 1.3965 (1.1619)	grad_norm 0.3462 (0.3470)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:15:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:05:14 lr 0.000020	 wd 0.0500	time 0.2894 (0.2617)	loss 1.3686 (1.1630)	grad_norm 0.3203 (0.3470)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:16:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:04:47 lr 0.000019	 wd 0.0500	time 0.2669 (0.2612)	loss 1.2807 (1.1627)	grad_norm 0.3355 (0.3467)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:16:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:04:21 lr 0.000019	 wd 0.0500	time 0.2758 (0.2612)	loss 0.8328 (1.1632)	grad_norm 0.3336 (0.3465)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:17:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:03:55 lr 0.000019	 wd 0.0500	time 0.2475 (0.2608)	loss 1.1099 (1.1638)	grad_norm 0.3717 (0.3462)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:17:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:03:28 lr 0.000019	 wd 0.0500	time 0.2694 (0.2605)	loss 1.3297 (1.1640)	grad_norm 0.3293 (0.3460)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:17:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:02 lr 0.000019	 wd 0.0500	time 0.2249 (0.2603)	loss 0.7549 (1.1663)	grad_norm 0.3351 (0.3458)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:18:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:02:36 lr 0.000019	 wd 0.0500	time 0.2361 (0.2601)	loss 1.1238 (1.1667)	grad_norm 0.3460 (0.3457)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:18:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:10 lr 0.000019	 wd 0.0500	time 0.2530 (0.2600)	loss 1.1544 (1.1674)	grad_norm 0.3467 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:19:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:44 lr 0.000019	 wd 0.0500	time 0.2223 (0.2598)	loss 1.4477 (1.1700)	grad_norm 0.3515 (0.3481)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:19:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:18 lr 0.000019	 wd 0.0500	time 0.2345 (0.2598)	loss 1.2294 (1.1681)	grad_norm 0.3384 (0.3478)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:20:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:52 lr 0.000019	 wd 0.0500	time 0.2691 (0.2598)	loss 1.3888 (1.1672)	grad_norm 0.3548 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:20:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:26 lr 0.000019	 wd 0.0500	time 0.2596 (0.2598)	loss 1.2456 (1.1676)	grad_norm 0.3381 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:20:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2161 (0.2589)	loss 1.5737 (1.1678)	grad_norm 0.3251 (0.3483)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:20:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 7 training takes 0:10:50
[2024-07-31 17:21:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.043 (12.043)	Loss 0.4851 (0.4851)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 17:21:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.966 Acc@5 97.842
[2024-07-31 17:21:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 17:21:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.97%
[2024-07-31 17:21:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 17:21:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 17:21:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:00:07 lr 0.000019	 wd 0.0500	time 10.0748 (10.0748)	loss 1.2487 (1.2487)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:22:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:14:06 lr 0.000019	 wd 0.0500	time 0.2257 (0.3523)	loss 0.9772 (1.2096)	grad_norm 0.3270 (0.3536)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:22:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:11:33 lr 0.000019	 wd 0.0500	time 0.2203 (0.3013)	loss 0.8161 (1.1949)	grad_norm 0.3429 (0.3491)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:22:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:10:27 lr 0.000019	 wd 0.0500	time 0.2327 (0.2849)	loss 1.4068 (1.1865)	grad_norm 0.3507 (0.3472)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:23:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:09:41 lr 0.000019	 wd 0.0500	time 0.2527 (0.2767)	loss 0.8417 (1.1852)	grad_norm 0.3495 (0.3470)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:23:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:09:05 lr 0.000019	 wd 0.0500	time 0.2661 (0.2723)	loss 0.8013 (1.1742)	grad_norm 0.3354 (0.3469)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:24:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:08:33 lr 0.000019	 wd 0.0500	time 0.2327 (0.2699)	loss 0.9097 (1.1711)	grad_norm 0.3631 (0.3466)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:24:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:08:01 lr 0.000019	 wd 0.0500	time 0.2337 (0.2672)	loss 1.3470 (1.1716)	grad_norm 0.3956 (0.3495)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:24:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:07:32 lr 0.000019	 wd 0.0500	time 0.2183 (0.2656)	loss 1.4540 (1.1733)	grad_norm 0.3508 (0.3487)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:25:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:07:02 lr 0.000019	 wd 0.0500	time 0.2202 (0.2639)	loss 0.9486 (1.1684)	grad_norm 0.3406 (0.3478)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:25:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:06:34 lr 0.000019	 wd 0.0500	time 0.2444 (0.2628)	loss 1.3480 (1.1719)	grad_norm 0.3453 (0.3481)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:26:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:06:07 lr 0.000019	 wd 0.0500	time 0.2481 (0.2620)	loss 1.2027 (1.1701)	grad_norm 0.3336 (0.3479)	loss_scale 8192.0000 (4408.5014)	mem 9639MB
[2024-07-31 17:26:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:05:40 lr 0.000019	 wd 0.0500	time 0.2324 (0.2613)	loss 1.1250 (1.1697)	grad_norm 0.3558 (0.3476)	loss_scale 8192.0000 (4723.5304)	mem 9639MB
[2024-07-31 17:27:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:05:13 lr 0.000019	 wd 0.0500	time 0.2425 (0.2605)	loss 0.9027 (1.1691)	grad_norm 0.3491 (0.3474)	loss_scale 8192.0000 (4990.1307)	mem 9639MB
[2024-07-31 17:27:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:04:46 lr 0.000019	 wd 0.0500	time 0.2210 (0.2597)	loss 1.2686 (1.1700)	grad_norm 0.3492 (0.3474)	loss_scale 8192.0000 (5218.6724)	mem 9639MB
[2024-07-31 17:27:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:04:19 lr 0.000019	 wd 0.0500	time 0.2485 (0.2593)	loss 1.1702 (1.1702)	grad_norm 0.3521 (0.3474)	loss_scale 8192.0000 (5416.7622)	mem 9639MB
[2024-07-31 17:28:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:03:53 lr 0.000019	 wd 0.0500	time 0.2300 (0.2590)	loss 1.3737 (1.1729)	grad_norm 0.3471 (0.3476)	loss_scale 8192.0000 (5590.1062)	mem 9639MB
[2024-07-31 17:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:03:27 lr 0.000019	 wd 0.0500	time 0.2176 (0.2590)	loss 0.7687 (1.1754)	grad_norm 0.3365 (0.3475)	loss_scale 8192.0000 (5743.0688)	mem 9639MB
[2024-07-31 17:29:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:01 lr 0.000019	 wd 0.0500	time 0.2592 (0.2587)	loss 0.7894 (1.1740)	grad_norm 0.3604 (0.3473)	loss_scale 8192.0000 (5879.0450)	mem 9639MB
[2024-07-31 17:29:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:02:35 lr 0.000019	 wd 0.0500	time 0.2658 (0.2587)	loss 1.1794 (1.1758)	grad_norm 0.3342 (0.3473)	loss_scale 8192.0000 (6000.7154)	mem 9639MB
[2024-07-31 17:30:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:09 lr 0.000019	 wd 0.0500	time 0.2366 (0.2586)	loss 0.8097 (1.1746)	grad_norm 0.3458 (0.3472)	loss_scale 8192.0000 (6110.2249)	mem 9639MB
[2024-07-31 17:30:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:43 lr 0.000019	 wd 0.0500	time 0.2370 (0.2586)	loss 0.8707 (1.1751)	grad_norm 0.3551 (0.3471)	loss_scale 8192.0000 (6209.3099)	mem 9639MB
[2024-07-31 17:30:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:18 lr 0.000019	 wd 0.0500	time 0.2296 (0.2585)	loss 1.1166 (1.1734)	grad_norm 0.3678 (0.3470)	loss_scale 8192.0000 (6299.3912)	mem 9639MB
[2024-07-31 17:31:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:52 lr 0.000019	 wd 0.0500	time 0.2322 (0.2583)	loss 1.3423 (1.1721)	grad_norm 0.3299 (0.3468)	loss_scale 8192.0000 (6381.6428)	mem 9639MB
[2024-07-31 17:31:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:26 lr 0.000019	 wd 0.0500	time 0.2470 (0.2581)	loss 1.1640 (1.1720)	grad_norm 0.3577 (nan)	loss_scale 4096.0000 (6436.5714)	mem 9639MB
[2024-07-31 17:32:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2176 (0.2573)	loss 0.9094 (1.1718)	grad_norm 0.3291 (nan)	loss_scale 4096.0000 (6342.9860)	mem 9639MB
[2024-07-31 17:32:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 8 training takes 0:10:46
[2024-07-31 17:32:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.224 (12.224)	Loss 0.4875 (0.4875)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 9639MB
[2024-07-31 17:32:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.952 Acc@5 97.850
[2024-07-31 17:32:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 17:32:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 85.97%
[2024-07-31 17:32:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:44:55 lr 0.000019	 wd 0.0500	time 11.1494 (11.1494)	loss 1.3315 (1.3315)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:33:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:14:38 lr 0.000019	 wd 0.0500	time 0.2380 (0.3656)	loss 1.2501 (1.1485)	grad_norm 0.3519 (0.3576)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:33:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:11:48 lr 0.000019	 wd 0.0500	time 0.2416 (0.3079)	loss 0.9031 (1.1723)	grad_norm 0.3405 (0.3508)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:34:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:10:39 lr 0.000019	 wd 0.0500	time 0.2351 (0.2902)	loss 1.5050 (1.1810)	grad_norm 0.3377 (0.3507)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:34:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:09:47 lr 0.000019	 wd 0.0500	time 0.2550 (0.2794)	loss 0.8447 (1.1644)	grad_norm 0.3405 (0.3489)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:34:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:09:10 lr 0.000019	 wd 0.0500	time 0.2863 (0.2751)	loss 0.7493 (1.1597)	grad_norm 0.3514 (0.3476)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:08:36 lr 0.000019	 wd 0.0500	time 0.2397 (0.2718)	loss 0.8414 (1.1640)	grad_norm 0.3412 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:35:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:08:04 lr 0.000019	 wd 0.0500	time 0.2227 (0.2691)	loss 1.3860 (1.1677)	grad_norm 0.3378 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:36:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:07:34 lr 0.000019	 wd 0.0500	time 0.2215 (0.2672)	loss 1.3836 (1.1689)	grad_norm 0.3312 (0.3473)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:36:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:07:04 lr 0.000019	 wd 0.0500	time 0.2530 (0.2650)	loss 1.3547 (1.1685)	grad_norm 0.3462 (0.3476)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:37:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:06:35 lr 0.000019	 wd 0.0500	time 0.2216 (0.2635)	loss 1.1639 (1.1693)	grad_norm 0.3384 (0.3474)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:37:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:06:08 lr 0.000018	 wd 0.0500	time 0.2532 (0.2630)	loss 1.3339 (1.1653)	grad_norm 0.3442 (0.3483)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:37:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:05:42 lr 0.000018	 wd 0.0500	time 0.2614 (0.2630)	loss 0.8396 (1.1642)	grad_norm 0.3428 (0.3481)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:38:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:05:15 lr 0.000018	 wd 0.0500	time 0.2269 (0.2623)	loss 1.2732 (1.1652)	grad_norm 0.3369 (0.3481)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:38:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:04:48 lr 0.000018	 wd 0.0500	time 0.2457 (0.2615)	loss 1.0707 (1.1643)	grad_norm 0.3393 (0.3479)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:39:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:04:21 lr 0.000018	 wd 0.0500	time 0.2652 (0.2611)	loss 0.8574 (1.1676)	grad_norm 0.3516 (0.3477)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:39:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:03:55 lr 0.000018	 wd 0.0500	time 0.2704 (0.2612)	loss 0.9140 (1.1658)	grad_norm 0.3387 (0.3475)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:40:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:03:29 lr 0.000018	 wd 0.0500	time 0.2379 (0.2617)	loss 1.4397 (1.1663)	grad_norm 0.3500 (0.3477)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:40:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:03 lr 0.000018	 wd 0.0500	time 0.2324 (0.2613)	loss 1.2498 (1.1651)	grad_norm 0.3460 (0.3475)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:40:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:02:36 lr 0.000018	 wd 0.0500	time 0.2370 (0.2608)	loss 1.4005 (1.1645)	grad_norm 0.3474 (0.3473)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:41:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:10 lr 0.000018	 wd 0.0500	time 0.2615 (0.2606)	loss 1.4013 (1.1651)	grad_norm 0.3212 (0.3475)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:41:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:44 lr 0.000018	 wd 0.0500	time 0.2218 (0.2607)	loss 1.4042 (1.1666)	grad_norm 0.3527 (0.3474)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:42:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:18 lr 0.000018	 wd 0.0500	time 0.2354 (0.2602)	loss 1.1601 (1.1677)	grad_norm 0.3489 (0.3473)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:42:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:52 lr 0.000018	 wd 0.0500	time 0.2332 (0.2598)	loss 0.7915 (1.1677)	grad_norm 0.3450 (0.3472)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 17:43:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:26 lr 0.000018	 wd 0.0500	time 0.2875 (0.2599)	loss 1.0830 (1.1680)	grad_norm 0.3480 (nan)	loss_scale 2048.0000 (4063.5868)	mem 9639MB
[2024-07-31 17:43:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2157 (0.2592)	loss 1.2588 (1.1675)	grad_norm 0.3478 (nan)	loss_scale 2048.0000 (3982.9956)	mem 9639MB
[2024-07-31 17:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 9 training takes 0:10:50
[2024-07-31 17:43:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.629 (11.629)	Loss 0.5015 (0.5015)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 17:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.004 Acc@5 97.848
[2024-07-31 17:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 17:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-31 17:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 17:43:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 17:44:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:18:20 lr 0.000018	 wd 0.0500	time 10.5116 (10.5116)	loss 1.6257 (1.6257)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:14:39 lr 0.000018	 wd 0.0500	time 0.2300 (0.3661)	loss 0.8834 (1.1785)	grad_norm 0.3766 (0.3463)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:45:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:11:44 lr 0.000018	 wd 0.0500	time 0.2499 (0.3060)	loss 1.4421 (1.1738)	grad_norm 0.3278 (0.3520)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:45:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:10:30 lr 0.000018	 wd 0.0500	time 0.2325 (0.2864)	loss 1.3217 (1.1687)	grad_norm 0.3560 (0.3491)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:45:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:09:44 lr 0.000018	 wd 0.0500	time 0.2441 (0.2780)	loss 0.8956 (1.1723)	grad_norm 0.3522 (0.3481)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:46:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:09:08 lr 0.000018	 wd 0.0500	time 0.2248 (0.2740)	loss 0.7018 (1.1678)	grad_norm 0.3354 (0.3480)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:46:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:08:32 lr 0.000018	 wd 0.0500	time 0.2619 (0.2697)	loss 1.1720 (1.1756)	grad_norm 0.3451 (0.3476)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:47:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:08:01 lr 0.000018	 wd 0.0500	time 0.2326 (0.2671)	loss 1.2976 (1.1736)	grad_norm 0.3164 (0.3473)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:47:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:07:31 lr 0.000018	 wd 0.0500	time 0.2539 (0.2654)	loss 1.2748 (1.1763)	grad_norm 0.3405 (0.3477)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:47:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:07:06 lr 0.000018	 wd 0.0500	time 0.2188 (0.2660)	loss 1.4565 (1.1739)	grad_norm 0.3368 (0.3472)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:48:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:06:36 lr 0.000018	 wd 0.0500	time 0.2523 (0.2642)	loss 1.2858 (1.1750)	grad_norm 0.3445 (0.3475)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:48:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:06:11 lr 0.000018	 wd 0.0500	time 0.2284 (0.2653)	loss 0.9976 (1.1808)	grad_norm 0.3583 (0.3497)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:49:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:05:43 lr 0.000018	 wd 0.0500	time 0.2256 (0.2637)	loss 1.3955 (1.1783)	grad_norm 0.3418 (0.3502)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:49:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:05:15 lr 0.000018	 wd 0.0500	time 0.2729 (0.2628)	loss 1.3687 (1.1758)	grad_norm 0.3383 (0.3499)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:50:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:04:49 lr 0.000018	 wd 0.0500	time 0.2611 (0.2630)	loss 1.3445 (1.1737)	grad_norm 0.3433 (0.3500)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:50:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:04:22 lr 0.000018	 wd 0.0500	time 0.2556 (0.2622)	loss 1.1689 (1.1712)	grad_norm 0.3570 (0.3499)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:50:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:03:56 lr 0.000018	 wd 0.0500	time 0.2337 (0.2617)	loss 1.1181 (1.1699)	grad_norm 0.3291 (0.3497)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:51:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:03:30 lr 0.000018	 wd 0.0500	time 0.2940 (0.2619)	loss 1.2419 (1.1719)	grad_norm 0.3424 (0.3496)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:51:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:03 lr 0.000018	 wd 0.0500	time 0.2389 (0.2620)	loss 1.1414 (1.1709)	grad_norm 0.3667 (0.3496)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:52:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:02:37 lr 0.000018	 wd 0.0500	time 0.2101 (0.2614)	loss 1.0933 (1.1699)	grad_norm 0.3406 (0.3498)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:52:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:11 lr 0.000017	 wd 0.0500	time 0.2347 (0.2611)	loss 1.3273 (1.1734)	grad_norm 0.3504 (0.3507)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:53:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:44 lr 0.000017	 wd 0.0500	time 0.2454 (0.2608)	loss 1.1960 (1.1747)	grad_norm 0.3265 (0.3513)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:53:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:18 lr 0.000017	 wd 0.0500	time 0.2090 (0.2604)	loss 1.3315 (1.1758)	grad_norm 0.3325 (0.3510)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:53:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:52 lr 0.000017	 wd 0.0500	time 0.2256 (0.2604)	loss 1.3023 (1.1737)	grad_norm 0.3478 (0.3508)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:54:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:26 lr 0.000017	 wd 0.0500	time 0.2426 (0.2602)	loss 1.3691 (1.1727)	grad_norm 0.3299 (0.3508)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:54:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2199 (0.2592)	loss 0.8946 (1.1722)	grad_norm 0.3406 (0.3509)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:54:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 10 training takes 0:10:50
[2024-07-31 17:54:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_10.pth saving......
[2024-07-31 17:54:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_10.pth saved !!!
[2024-07-31 17:55:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.757 (11.757)	Loss 0.4941 (0.4941)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 9639MB
[2024-07-31 17:55:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.000 Acc@5 97.876
[2024-07-31 17:55:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 17:55:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-31 17:55:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:27:37 lr 0.000017	 wd 0.0500	time 10.7344 (10.7344)	loss 0.9083 (0.9083)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:55:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:14:47 lr 0.000017	 wd 0.0500	time 0.2616 (0.3694)	loss 1.3976 (1.1575)	grad_norm 0.3349 (0.3457)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:56:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:11:53 lr 0.000017	 wd 0.0500	time 0.2410 (0.3100)	loss 1.3300 (1.1783)	grad_norm 0.3494 (0.3454)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:56:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:10:35 lr 0.000017	 wd 0.0500	time 0.2688 (0.2886)	loss 0.8526 (1.1683)	grad_norm 0.3369 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 17:57:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:09:47 lr 0.000017	 wd 0.0500	time 0.2530 (0.2797)	loss 1.4043 (1.1666)	grad_norm 0.3548 (nan)	loss_scale 1024.0000 (1869.2469)	mem 9639MB
[2024-07-31 17:57:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:09:08 lr 0.000017	 wd 0.0500	time 0.2419 (0.2740)	loss 0.7332 (1.1661)	grad_norm 0.3296 (nan)	loss_scale 1024.0000 (1700.5349)	mem 9639MB
[2024-07-31 17:58:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:08:34 lr 0.000017	 wd 0.0500	time 0.2599 (0.2707)	loss 1.4569 (1.1667)	grad_norm 0.3404 (nan)	loss_scale 1024.0000 (1587.9667)	mem 9639MB
[2024-07-31 17:58:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:08:03 lr 0.000017	 wd 0.0500	time 0.2663 (0.2681)	loss 1.0082 (1.1620)	grad_norm 0.3634 (nan)	loss_scale 1024.0000 (1507.5150)	mem 9639MB
[2024-07-31 17:58:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:07:33 lr 0.000017	 wd 0.0500	time 0.2313 (0.2664)	loss 1.4825 (1.1601)	grad_norm 0.3346 (nan)	loss_scale 1024.0000 (1447.1511)	mem 9639MB
[2024-07-31 17:59:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:07:04 lr 0.000017	 wd 0.0500	time 0.2447 (0.2649)	loss 1.0060 (1.1629)	grad_norm 0.3605 (nan)	loss_scale 1024.0000 (1400.1865)	mem 9639MB
[2024-07-31 17:59:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:06:36 lr 0.000017	 wd 0.0500	time 0.2458 (0.2637)	loss 1.1696 (1.1630)	grad_norm 0.3502 (nan)	loss_scale 1024.0000 (1362.6054)	mem 9639MB
[2024-07-31 18:00:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:06:08 lr 0.000017	 wd 0.0500	time 0.2353 (0.2626)	loss 0.9080 (1.1615)	grad_norm 0.3537 (nan)	loss_scale 1024.0000 (1331.8510)	mem 9639MB
[2024-07-31 18:00:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:05:40 lr 0.000017	 wd 0.0500	time 0.2272 (0.2617)	loss 0.9343 (1.1584)	grad_norm 0.3391 (nan)	loss_scale 1024.0000 (1306.2182)	mem 9639MB
[2024-07-31 18:00:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:05:14 lr 0.000017	 wd 0.0500	time 0.2239 (0.2613)	loss 1.3899 (1.1588)	grad_norm 0.3642 (nan)	loss_scale 1024.0000 (1284.5257)	mem 9639MB
[2024-07-31 18:01:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:04:47 lr 0.000017	 wd 0.0500	time 0.2288 (0.2605)	loss 0.8068 (1.1591)	grad_norm 0.3470 (nan)	loss_scale 1024.0000 (1265.9300)	mem 9639MB
[2024-07-31 18:01:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:04:20 lr 0.000017	 wd 0.0500	time 0.2329 (0.2600)	loss 1.3954 (1.1599)	grad_norm 0.3489 (nan)	loss_scale 1024.0000 (1249.8121)	mem 9639MB
[2024-07-31 18:02:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:03:55 lr 0.000017	 wd 0.0500	time 0.2466 (0.2610)	loss 1.2800 (1.1611)	grad_norm 0.3446 (nan)	loss_scale 1024.0000 (1235.7077)	mem 9639MB
[2024-07-31 18:02:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:03:29 lr 0.000017	 wd 0.0500	time 0.2621 (0.2606)	loss 1.0825 (1.1602)	grad_norm 0.3391 (nan)	loss_scale 1024.0000 (1223.2616)	mem 9639MB
[2024-07-31 18:03:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:02 lr 0.000017	 wd 0.0500	time 0.2784 (0.2606)	loss 0.8417 (1.1602)	grad_norm 0.3325 (nan)	loss_scale 1024.0000 (1212.1977)	mem 9639MB
[2024-07-31 18:03:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:02:36 lr 0.000017	 wd 0.0500	time 0.2232 (0.2607)	loss 1.1423 (1.1593)	grad_norm 0.3530 (nan)	loss_scale 1024.0000 (1202.2977)	mem 9639MB
[2024-07-31 18:04:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:10 lr 0.000017	 wd 0.0500	time 0.2634 (0.2606)	loss 1.0630 (1.1595)	grad_norm 0.3210 (nan)	loss_scale 1024.0000 (1193.3873)	mem 9639MB
[2024-07-31 18:04:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:44 lr 0.000017	 wd 0.0500	time 0.2601 (0.2606)	loss 1.3594 (1.1587)	grad_norm 0.3611 (nan)	loss_scale 1024.0000 (1185.3251)	mem 9639MB
[2024-07-31 18:04:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:18 lr 0.000017	 wd 0.0500	time 0.2557 (0.2601)	loss 0.9899 (1.1606)	grad_norm 0.3652 (nan)	loss_scale 1024.0000 (1177.9955)	mem 9639MB
[2024-07-31 18:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:52 lr 0.000016	 wd 0.0500	time 0.2392 (0.2598)	loss 1.1635 (1.1588)	grad_norm 0.3554 (nan)	loss_scale 1024.0000 (1171.3029)	mem 9639MB
[2024-07-31 18:05:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:26 lr 0.000016	 wd 0.0500	time 0.2371 (0.2597)	loss 1.3941 (1.1600)	grad_norm 0.3382 (nan)	loss_scale 1024.0000 (1165.1678)	mem 9639MB
[2024-07-31 18:06:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2156 (0.2590)	loss 0.8485 (1.1583)	grad_norm 0.3211 (nan)	loss_scale 1024.0000 (1159.5234)	mem 9639MB
[2024-07-31 18:06:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 11 training takes 0:10:50
[2024-07-31 18:06:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.611 (11.611)	Loss 0.4927 (0.4927)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 9639MB
[2024-07-31 18:06:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 85.982 Acc@5 97.858
[2024-07-31 18:06:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 18:06:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.00%
[2024-07-31 18:06:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:58:08 lr 0.000016	 wd 0.0500	time 11.4662 (11.4662)	loss 0.9520 (0.9520)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:07:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:14:49 lr 0.000016	 wd 0.0500	time 0.2336 (0.3703)	loss 1.3060 (1.1610)	grad_norm 0.3376 (0.3462)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:07:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:11:56 lr 0.000016	 wd 0.0500	time 0.2495 (0.3114)	loss 1.2970 (1.1589)	grad_norm 0.3382 (0.3456)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:08:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:10:45 lr 0.000016	 wd 0.0500	time 0.2234 (0.2930)	loss 0.7482 (1.1715)	grad_norm 0.4632 (0.3460)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:08:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:09:53 lr 0.000016	 wd 0.0500	time 0.2267 (0.2823)	loss 1.2290 (1.1633)	grad_norm 0.3386 (0.3463)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:08:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:09:15 lr 0.000016	 wd 0.0500	time 0.2407 (0.2774)	loss 1.4355 (1.1633)	grad_norm 0.3392 (0.3457)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:09:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:08:39 lr 0.000016	 wd 0.0500	time 0.2528 (0.2730)	loss 1.4139 (1.1617)	grad_norm 0.3553 (0.3460)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:09:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:08:07 lr 0.000016	 wd 0.0500	time 0.2344 (0.2704)	loss 1.2142 (1.1598)	grad_norm 0.3451 (0.3459)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:10:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:07:37 lr 0.000016	 wd 0.0500	time 0.2242 (0.2689)	loss 1.3683 (1.1607)	grad_norm 0.3401 (0.3465)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:10:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:07:08 lr 0.000016	 wd 0.0500	time 0.2292 (0.2673)	loss 1.5326 (1.1582)	grad_norm 0.3431 (0.3463)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:11:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:06:39 lr 0.000016	 wd 0.0500	time 0.2264 (0.2659)	loss 1.3860 (1.1603)	grad_norm 0.3560 (0.3467)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:11:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:06:10 lr 0.000016	 wd 0.0500	time 0.2208 (0.2646)	loss 0.8064 (1.1618)	grad_norm 0.3409 (0.3472)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:11:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:05:43 lr 0.000016	 wd 0.0500	time 0.2583 (0.2635)	loss 1.4373 (1.1600)	grad_norm 0.3493 (0.3472)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:12:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:05:16 lr 0.000016	 wd 0.0500	time 0.2400 (0.2630)	loss 1.0462 (1.1609)	grad_norm 0.3300 (0.3472)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:12:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:04:49 lr 0.000016	 wd 0.0500	time 0.2341 (0.2623)	loss 1.3248 (1.1606)	grad_norm 0.3317 (0.3470)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:13:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:04:22 lr 0.000016	 wd 0.0500	time 0.2519 (0.2616)	loss 0.8130 (1.1579)	grad_norm 0.3588 (0.3478)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:13:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:03:55 lr 0.000016	 wd 0.0500	time 0.2734 (0.2611)	loss 1.3638 (1.1547)	grad_norm 0.3266 (0.3476)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:14:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:03:29 lr 0.000016	 wd 0.0500	time 0.2424 (0.2607)	loss 1.0668 (1.1569)	grad_norm 0.3520 (0.3481)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:14:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:02 lr 0.000016	 wd 0.0500	time 0.2365 (0.2605)	loss 0.8236 (1.1554)	grad_norm 0.3452 (0.3486)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:14:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:02:36 lr 0.000016	 wd 0.0500	time 0.2932 (0.2604)	loss 1.3082 (1.1561)	grad_norm 0.3475 (0.3486)	loss_scale 2048.0000 (1062.7838)	mem 9639MB
[2024-07-31 18:15:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:10 lr 0.000016	 wd 0.0500	time 0.2869 (0.2604)	loss 1.0529 (1.1570)	grad_norm 0.3429 (0.3486)	loss_scale 2048.0000 (1112.0200)	mem 9639MB
[2024-07-31 18:15:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:44 lr 0.000016	 wd 0.0500	time 0.2332 (0.2602)	loss 0.8074 (1.1563)	grad_norm 0.3149 (0.3484)	loss_scale 2048.0000 (1156.5693)	mem 9639MB
[2024-07-31 18:16:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:18 lr 0.000016	 wd 0.0500	time 0.2297 (0.2600)	loss 0.8158 (1.1577)	grad_norm 0.3493 (0.3489)	loss_scale 2048.0000 (1197.0704)	mem 9639MB
[2024-07-31 18:16:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:52 lr 0.000015	 wd 0.0500	time 0.2447 (0.2598)	loss 1.1965 (1.1582)	grad_norm 0.3410 (nan)	loss_scale 1024.0000 (1220.7006)	mem 9639MB
[2024-07-31 18:17:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:26 lr 0.000015	 wd 0.0500	time 0.2228 (0.2595)	loss 1.3660 (1.1585)	grad_norm 0.3278 (nan)	loss_scale 1024.0000 (1212.5081)	mem 9639MB
[2024-07-31 18:17:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.2165 (0.2587)	loss 0.9433 (1.1578)	grad_norm 0.3449 (nan)	loss_scale 1024.0000 (1204.9708)	mem 9639MB
[2024-07-31 18:17:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 12 training takes 0:10:49
[2024-07-31 18:17:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.456 (12.456)	Loss 0.5054 (0.5054)	Acc@1 92.969 (92.969)	Acc@5 98.242 (98.242)	Mem 9639MB
[2024-07-31 18:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.052 Acc@5 97.852
[2024-07-31 18:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 18:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.05%
[2024-07-31 18:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 18:17:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 18:18:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:17:10 lr 0.000015	 wd 0.0500	time 10.4838 (10.4838)	loss 1.3304 (1.3304)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:18:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:14:14 lr 0.000015	 wd 0.0500	time 0.2371 (0.3559)	loss 1.3428 (1.2225)	grad_norm 0.3485 (0.3522)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:18:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:11:40 lr 0.000015	 wd 0.0500	time 0.2303 (0.3044)	loss 1.3900 (1.2103)	grad_norm 0.3379 (0.3492)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:19:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:10:33 lr 0.000015	 wd 0.0500	time 0.2480 (0.2877)	loss 1.3968 (1.2104)	grad_norm 0.3411 (0.3508)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:19:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:09:46 lr 0.000015	 wd 0.0500	time 0.2455 (0.2792)	loss 1.4621 (1.1909)	grad_norm 0.3501 (0.3492)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:20:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:09:07 lr 0.000015	 wd 0.0500	time 0.2340 (0.2737)	loss 1.2854 (1.1843)	grad_norm 0.3682 (0.3492)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:20:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:08:34 lr 0.000015	 wd 0.0500	time 0.2363 (0.2704)	loss 1.1877 (1.1781)	grad_norm 0.3392 (0.3493)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:21:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:08:03 lr 0.000015	 wd 0.0500	time 0.2369 (0.2684)	loss 1.1852 (1.1691)	grad_norm 0.3569 (0.3494)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:21:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:07:33 lr 0.000015	 wd 0.0500	time 0.2323 (0.2664)	loss 1.2391 (1.1683)	grad_norm 0.3427 (0.3491)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:21:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:07:04 lr 0.000015	 wd 0.0500	time 0.2297 (0.2651)	loss 1.3289 (1.1689)	grad_norm 0.3543 (0.3528)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:22:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:06:36 lr 0.000015	 wd 0.0500	time 0.2518 (0.2640)	loss 1.3862 (1.1678)	grad_norm 0.3300 (0.3538)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:22:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:06:08 lr 0.000015	 wd 0.0500	time 0.2356 (0.2628)	loss 1.4006 (1.1638)	grad_norm 0.3647 (0.3533)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:23:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:05:40 lr 0.000015	 wd 0.0500	time 0.2541 (0.2615)	loss 1.1820 (1.1678)	grad_norm 0.3492 (0.3526)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:23:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:05:13 lr 0.000015	 wd 0.0500	time 0.2602 (0.2610)	loss 0.7288 (1.1662)	grad_norm 0.3453 (0.3529)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:24:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:04:47 lr 0.000015	 wd 0.0500	time 0.2271 (0.2604)	loss 1.3641 (1.1649)	grad_norm 0.3442 (0.3523)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:24:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:04:20 lr 0.000015	 wd 0.0500	time 0.2574 (0.2598)	loss 1.2138 (1.1654)	grad_norm 0.3528 (0.3522)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:24:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:03:53 lr 0.000015	 wd 0.0500	time 0.2643 (0.2594)	loss 0.9311 (1.1643)	grad_norm 0.3468 (0.3520)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:25:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:03:27 lr 0.000015	 wd 0.0500	time 0.2349 (0.2592)	loss 1.0334 (1.1655)	grad_norm 0.3448 (0.3516)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:25:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:01 lr 0.000015	 wd 0.0500	time 0.2180 (0.2589)	loss 1.2330 (1.1659)	grad_norm 0.3402 (0.3525)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:26:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:02:35 lr 0.000015	 wd 0.0500	time 0.2331 (0.2586)	loss 1.0926 (1.1660)	grad_norm 0.3661 (0.3523)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:26:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:09 lr 0.000015	 wd 0.0500	time 0.2568 (0.2582)	loss 1.3951 (1.1659)	grad_norm 0.3269 (0.3521)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:26:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:43 lr 0.000014	 wd 0.0500	time 0.2433 (0.2580)	loss 1.3560 (1.1659)	grad_norm 0.3496 (0.3543)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:27:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:17 lr 0.000014	 wd 0.0500	time 0.2629 (0.2577)	loss 1.1997 (1.1645)	grad_norm 0.3454 (0.3541)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:27:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:51 lr 0.000014	 wd 0.0500	time 0.2490 (0.2574)	loss 1.3431 (1.1635)	grad_norm 0.3523 (0.3538)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:28:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:26 lr 0.000014	 wd 0.0500	time 0.2297 (0.2572)	loss 0.9346 (1.1662)	grad_norm 0.3257 (0.3535)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:28:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.2156 (0.2563)	loss 0.7925 (1.1649)	grad_norm 0.3475 (0.3536)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:28:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 13 training takes 0:10:43
[2024-07-31 18:28:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.206 (11.206)	Loss 0.4800 (0.4800)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 18:29:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.012 Acc@5 97.876
[2024-07-31 18:29:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 18:29:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.05%
[2024-07-31 18:29:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:11:54 lr 0.000014	 wd 0.0500	time 10.3574 (10.3574)	loss 1.4213 (1.4213)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:29:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:14:13 lr 0.000014	 wd 0.0500	time 0.2350 (0.3554)	loss 1.1858 (1.1812)	grad_norm 0.3425 (nan)	loss_scale 512.0000 (729.9802)	mem 9639MB
[2024-07-31 18:30:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:11:32 lr 0.000014	 wd 0.0500	time 0.2217 (0.3008)	loss 1.4572 (1.1802)	grad_norm 0.3496 (nan)	loss_scale 512.0000 (621.5323)	mem 9639MB
[2024-07-31 18:30:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:10:21 lr 0.000014	 wd 0.0500	time 0.2334 (0.2824)	loss 0.9291 (1.1832)	grad_norm 0.3581 (nan)	loss_scale 512.0000 (585.1429)	mem 9639MB
[2024-07-31 18:30:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:09:36 lr 0.000014	 wd 0.0500	time 0.2281 (0.2741)	loss 1.5920 (1.1828)	grad_norm 0.3365 (nan)	loss_scale 512.0000 (566.9027)	mem 9639MB
[2024-07-31 18:31:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:08:59 lr 0.000014	 wd 0.0500	time 0.2742 (0.2696)	loss 0.9707 (1.1784)	grad_norm 0.3380 (nan)	loss_scale 512.0000 (555.9441)	mem 9639MB
[2024-07-31 18:31:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:08:26 lr 0.000014	 wd 0.0500	time 0.2447 (0.2662)	loss 0.9746 (1.1774)	grad_norm 0.3584 (nan)	loss_scale 512.0000 (548.6323)	mem 9639MB
[2024-07-31 18:32:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:07:55 lr 0.000014	 wd 0.0500	time 0.2845 (0.2638)	loss 1.1344 (1.1780)	grad_norm 0.3513 (nan)	loss_scale 512.0000 (543.4066)	mem 9639MB
[2024-07-31 18:32:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:07:25 lr 0.000014	 wd 0.0500	time 0.2290 (0.2619)	loss 0.7998 (1.1720)	grad_norm 0.3294 (nan)	loss_scale 512.0000 (539.4856)	mem 9639MB
[2024-07-31 18:33:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:06:57 lr 0.000014	 wd 0.0500	time 0.2306 (0.2606)	loss 0.8581 (1.1709)	grad_norm 0.3555 (nan)	loss_scale 512.0000 (536.4351)	mem 9639MB
[2024-07-31 18:33:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:06:33 lr 0.000014	 wd 0.0500	time 0.2283 (0.2621)	loss 0.9985 (1.1682)	grad_norm 0.3511 (nan)	loss_scale 512.0000 (533.9940)	mem 9639MB
[2024-07-31 18:33:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:06:07 lr 0.000014	 wd 0.0500	time 0.2291 (0.2621)	loss 1.3814 (1.1677)	grad_norm 0.3610 (nan)	loss_scale 512.0000 (531.9964)	mem 9639MB
[2024-07-31 18:34:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:05:40 lr 0.000014	 wd 0.0500	time 0.2287 (0.2615)	loss 1.4738 (1.1673)	grad_norm 0.3563 (nan)	loss_scale 512.0000 (530.3314)	mem 9639MB
[2024-07-31 18:34:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:05:13 lr 0.000014	 wd 0.0500	time 0.2649 (0.2611)	loss 1.2398 (1.1706)	grad_norm 0.3476 (nan)	loss_scale 512.0000 (528.9224)	mem 9639MB
[2024-07-31 18:35:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:04:47 lr 0.000014	 wd 0.0500	time 0.2282 (0.2608)	loss 1.2403 (1.1702)	grad_norm 0.3501 (nan)	loss_scale 512.0000 (527.7145)	mem 9639MB
[2024-07-31 18:35:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:04:21 lr 0.000014	 wd 0.0500	time 0.2718 (0.2606)	loss 1.3827 (1.1692)	grad_norm 0.3391 (nan)	loss_scale 512.0000 (526.6676)	mem 9639MB
[2024-07-31 18:36:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:03:54 lr 0.000014	 wd 0.0500	time 0.2225 (0.2604)	loss 0.7678 (1.1680)	grad_norm 0.3431 (nan)	loss_scale 512.0000 (525.7514)	mem 9639MB
[2024-07-31 18:36:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:03:28 lr 0.000014	 wd 0.0500	time 0.2509 (0.2601)	loss 0.9349 (1.1670)	grad_norm 0.3350 (nan)	loss_scale 512.0000 (524.9430)	mem 9639MB
[2024-07-31 18:36:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:02 lr 0.000013	 wd 0.0500	time 0.2582 (0.2597)	loss 0.9082 (1.1676)	grad_norm 0.3562 (nan)	loss_scale 512.0000 (524.2243)	mem 9639MB
[2024-07-31 18:37:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:02:36 lr 0.000013	 wd 0.0500	time 0.2562 (0.2597)	loss 1.1617 (1.1679)	grad_norm 0.3633 (nan)	loss_scale 512.0000 (523.5813)	mem 9639MB
[2024-07-31 18:37:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:10 lr 0.000013	 wd 0.0500	time 0.2621 (0.2596)	loss 1.2198 (1.1673)	grad_norm 0.4328 (nan)	loss_scale 512.0000 (523.0025)	mem 9639MB
[2024-07-31 18:38:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:44 lr 0.000013	 wd 0.0500	time 0.2517 (0.2594)	loss 1.3478 (1.1666)	grad_norm 0.3442 (nan)	loss_scale 512.0000 (522.4788)	mem 9639MB
[2024-07-31 18:38:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:18 lr 0.000013	 wd 0.0500	time 0.2611 (0.2594)	loss 1.3209 (1.1657)	grad_norm 0.3452 (nan)	loss_scale 512.0000 (522.0027)	mem 9639MB
[2024-07-31 18:39:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:52 lr 0.000013	 wd 0.0500	time 0.2957 (0.2591)	loss 0.8572 (1.1646)	grad_norm 0.3443 (nan)	loss_scale 512.0000 (521.5680)	mem 9639MB
[2024-07-31 18:39:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:26 lr 0.000013	 wd 0.0500	time 0.2402 (0.2590)	loss 0.8618 (1.1638)	grad_norm 0.3637 (nan)	loss_scale 512.0000 (521.1695)	mem 9639MB
[2024-07-31 18:39:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2162 (0.2582)	loss 1.2527 (1.1626)	grad_norm 0.3443 (nan)	loss_scale 512.0000 (520.8029)	mem 9639MB
[2024-07-31 18:39:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 14 training takes 0:10:48
[2024-07-31 18:40:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.386 (12.386)	Loss 0.4705 (0.4705)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 9639MB
[2024-07-31 18:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.076 Acc@5 97.896
[2024-07-31 18:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 18:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-31 18:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 18:40:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 18:40:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:17:01 lr 0.000013	 wd 0.0500	time 10.4801 (10.4801)	loss 1.2719 (1.2719)	grad_norm 0.0000 (0.0000)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:41:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:14:14 lr 0.000013	 wd 0.0500	time 0.2355 (0.3558)	loss 0.7776 (1.1697)	grad_norm 0.3498 (0.3472)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:41:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:11:37 lr 0.000013	 wd 0.0500	time 0.2199 (0.3031)	loss 1.4494 (1.1553)	grad_norm 0.3549 (0.3485)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:41:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:10:30 lr 0.000013	 wd 0.0500	time 0.2308 (0.2863)	loss 0.9286 (1.1548)	grad_norm 0.3518 (0.3478)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:42:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:09:45 lr 0.000013	 wd 0.0500	time 0.2460 (0.2786)	loss 0.8019 (1.1534)	grad_norm 0.3403 (0.3472)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:42:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:09:07 lr 0.000013	 wd 0.0500	time 0.2208 (0.2732)	loss 0.9491 (1.1645)	grad_norm 0.3581 (0.3507)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:43:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:08:32 lr 0.000013	 wd 0.0500	time 0.2697 (0.2696)	loss 1.3934 (1.1672)	grad_norm 0.3493 (0.3504)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:43:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:08:01 lr 0.000013	 wd 0.0500	time 0.2515 (0.2674)	loss 1.2455 (1.1672)	grad_norm 0.3473 (0.3497)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:07:31 lr 0.000013	 wd 0.0500	time 0.2222 (0.2654)	loss 1.4034 (1.1689)	grad_norm 0.3318 (0.3492)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:44:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:07:02 lr 0.000013	 wd 0.0500	time 0.2262 (0.2639)	loss 1.2645 (1.1685)	grad_norm 0.3525 (0.3491)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:44:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:06:35 lr 0.000013	 wd 0.0500	time 0.2457 (0.2632)	loss 0.8528 (1.1684)	grad_norm 0.3508 (0.3494)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:45:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:06:08 lr 0.000013	 wd 0.0500	time 0.2713 (0.2628)	loss 0.8705 (1.1692)	grad_norm 0.3393 (0.3492)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:45:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:05:42 lr 0.000013	 wd 0.0500	time 0.2395 (0.2631)	loss 0.8355 (1.1692)	grad_norm 0.3883 (0.3491)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:46:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:05:15 lr 0.000013	 wd 0.0500	time 0.2426 (0.2622)	loss 0.8658 (1.1703)	grad_norm 0.3487 (0.3495)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:46:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:04:48 lr 0.000012	 wd 0.0500	time 0.2424 (0.2615)	loss 0.9508 (1.1703)	grad_norm 0.3437 (0.3494)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:46:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:04:21 lr 0.000012	 wd 0.0500	time 0.2570 (0.2612)	loss 1.0192 (1.1665)	grad_norm 0.3460 (0.3494)	loss_scale 512.0000 (512.0000)	mem 9639MB
[2024-07-31 18:47:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:03:54 lr 0.000012	 wd 0.0500	time 0.2861 (0.2605)	loss 1.0206 (1.1673)	grad_norm 0.3502 (0.3494)	loss_scale 1024.0000 (531.1880)	mem 9639MB
[2024-07-31 18:47:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:03:28 lr 0.000012	 wd 0.0500	time 0.2351 (0.2603)	loss 0.8211 (1.1674)	grad_norm 0.3525 (0.3493)	loss_scale 1024.0000 (560.1599)	mem 9639MB
[2024-07-31 18:48:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:02 lr 0.000012	 wd 0.0500	time 0.2681 (0.2602)	loss 1.6782 (1.1654)	grad_norm 0.3799 (0.3505)	loss_scale 1024.0000 (585.9145)	mem 9639MB
[2024-07-31 18:48:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:02:36 lr 0.000012	 wd 0.0500	time 0.2539 (0.2597)	loss 0.9719 (1.1636)	grad_norm 0.3663 (0.3505)	loss_scale 1024.0000 (608.9595)	mem 9639MB
[2024-07-31 18:49:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:10 lr 0.000012	 wd 0.0500	time 0.2292 (0.2591)	loss 0.8444 (1.1649)	grad_norm 0.3500 (0.3503)	loss_scale 1024.0000 (629.7011)	mem 9639MB
[2024-07-31 18:49:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:43 lr 0.000012	 wd 0.0500	time 0.2230 (0.2587)	loss 0.7878 (1.1637)	grad_norm 0.3459 (0.3504)	loss_scale 1024.0000 (648.4683)	mem 9639MB
[2024-07-31 18:49:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:18 lr 0.000012	 wd 0.0500	time 0.2794 (0.2584)	loss 1.0801 (1.1633)	grad_norm 0.3339 (0.3510)	loss_scale 1024.0000 (665.5302)	mem 9639MB
[2024-07-31 18:50:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:52 lr 0.000012	 wd 0.0500	time 0.2619 (0.2587)	loss 1.0191 (1.1636)	grad_norm 0.3582 (0.3509)	loss_scale 1024.0000 (681.1091)	mem 9639MB
[2024-07-31 18:50:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:26 lr 0.000012	 wd 0.0500	time 0.2655 (0.2585)	loss 1.4716 (1.1640)	grad_norm 0.3511 (0.3507)	loss_scale 1024.0000 (695.3903)	mem 9639MB
[2024-07-31 18:51:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2259 (0.2575)	loss 1.5166 (1.1645)	grad_norm 0.3534 (0.3506)	loss_scale 1024.0000 (708.5294)	mem 9639MB
[2024-07-31 18:51:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 15 training takes 0:10:46
[2024-07-31 18:51:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.168 (12.168)	Loss 0.5244 (0.5244)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 18:51:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.006 Acc@5 97.840
[2024-07-31 18:51:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 18:51:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-31 18:51:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:23:31 lr 0.000012	 wd 0.0500	time 12.0749 (12.0749)	loss 1.2075 (1.2075)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:52:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:14:32 lr 0.000012	 wd 0.0500	time 0.2667 (0.3633)	loss 1.0471 (1.1757)	grad_norm 0.3542 (0.3469)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:52:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:11:52 lr 0.000012	 wd 0.0500	time 0.2414 (0.3093)	loss 1.2571 (1.1650)	grad_norm 0.3485 (0.3481)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:53:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:10:36 lr 0.000012	 wd 0.0500	time 0.2481 (0.2891)	loss 1.0193 (1.1580)	grad_norm 0.3398 (0.3530)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:53:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:09:47 lr 0.000012	 wd 0.0500	time 0.2215 (0.2793)	loss 0.9333 (1.1466)	grad_norm 0.3465 (0.3538)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:53:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:09:08 lr 0.000012	 wd 0.0500	time 0.2658 (0.2740)	loss 1.5335 (1.1494)	grad_norm 0.3610 (0.3559)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:54:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:08:34 lr 0.000012	 wd 0.0500	time 0.2630 (0.2706)	loss 1.4364 (1.1500)	grad_norm 0.3257 (0.3556)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:54:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:08:04 lr 0.000012	 wd 0.0500	time 0.2267 (0.2691)	loss 1.0659 (1.1526)	grad_norm 0.3487 (0.3583)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:55:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:07:34 lr 0.000012	 wd 0.0500	time 0.2620 (0.2671)	loss 1.3225 (1.1540)	grad_norm 0.3561 (0.3580)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:55:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:07:05 lr 0.000012	 wd 0.0500	time 0.2249 (0.2655)	loss 1.1403 (1.1551)	grad_norm 0.3461 (0.3572)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:56:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:06:36 lr 0.000011	 wd 0.0500	time 0.2333 (0.2643)	loss 1.3545 (1.1550)	grad_norm 0.3405 (0.3567)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:56:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:06:09 lr 0.000011	 wd 0.0500	time 0.2324 (0.2635)	loss 0.8453 (1.1591)	grad_norm 0.3559 (0.3568)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:56:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:05:42 lr 0.000011	 wd 0.0500	time 0.2217 (0.2628)	loss 0.8715 (1.1568)	grad_norm 0.3432 (0.3572)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:57:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:05:15 lr 0.000011	 wd 0.0500	time 0.2533 (0.2623)	loss 1.2987 (1.1597)	grad_norm 0.3450 (0.3564)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:57:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:04:48 lr 0.000011	 wd 0.0500	time 0.2527 (0.2618)	loss 1.2398 (1.1597)	grad_norm 0.3546 (0.3559)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:58:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:04:22 lr 0.000011	 wd 0.0500	time 0.2555 (0.2615)	loss 1.2941 (1.1620)	grad_norm 0.3376 (0.3555)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:58:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:03:55 lr 0.000011	 wd 0.0500	time 0.2547 (0.2609)	loss 1.1213 (1.1625)	grad_norm 0.3621 (0.3554)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:59:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:03:29 lr 0.000011	 wd 0.0500	time 0.2411 (0.2607)	loss 1.0823 (1.1616)	grad_norm 0.3555 (0.3551)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:59:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:02 lr 0.000011	 wd 0.0500	time 0.2614 (0.2606)	loss 0.8644 (1.1605)	grad_norm 0.3247 (0.3548)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 18:59:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:02:36 lr 0.000011	 wd 0.0500	time 0.2495 (0.2606)	loss 1.0347 (1.1613)	grad_norm 0.3443 (0.3544)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:00:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:10 lr 0.000011	 wd 0.0500	time 0.2362 (0.2604)	loss 1.3552 (1.1620)	grad_norm 0.3576 (0.3542)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:00:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:44 lr 0.000011	 wd 0.0500	time 0.2612 (0.2602)	loss 1.3610 (1.1628)	grad_norm 0.3298 (0.3538)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:01:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:18 lr 0.000011	 wd 0.0500	time 0.2354 (0.2602)	loss 1.0182 (1.1623)	grad_norm 0.3548 (0.3537)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:01:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:52 lr 0.000011	 wd 0.0500	time 0.2353 (0.2600)	loss 0.8941 (1.1605)	grad_norm 0.3626 (0.3535)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:02:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:26 lr 0.000011	 wd 0.0500	time 0.2705 (0.2598)	loss 1.2240 (1.1596)	grad_norm 0.3377 (0.3533)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:02:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2482 (0.2588)	loss 0.8451 (1.1599)	grad_norm 0.3636 (0.3532)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:02:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 16 training takes 0:10:49
[2024-07-31 19:02:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.155 (12.155)	Loss 0.4927 (0.4927)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 19:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.018 Acc@5 97.868
[2024-07-31 19:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-07-31 19:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-31 19:03:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:18:36 lr 0.000011	 wd 0.0500	time 10.5180 (10.5180)	loss 1.3989 (1.3989)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:03:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:14:29 lr 0.000011	 wd 0.0500	time 0.2608 (0.3621)	loss 1.2346 (1.1432)	grad_norm 0.3352 (0.3610)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:03:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:11:42 lr 0.000011	 wd 0.0500	time 0.2227 (0.3052)	loss 1.2801 (1.1426)	grad_norm 0.3442 (0.3539)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:04:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:10:29 lr 0.000011	 wd 0.0500	time 0.2344 (0.2859)	loss 1.2393 (1.1583)	grad_norm 0.3458 (0.3523)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:04:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:09:41 lr 0.000011	 wd 0.0500	time 0.2214 (0.2766)	loss 1.4156 (1.1563)	grad_norm 0.3493 (0.3517)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:05:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:09:02 lr 0.000010	 wd 0.0500	time 0.2616 (0.2712)	loss 1.3716 (1.1607)	grad_norm 0.3656 (0.3515)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 19:05:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:08:33 lr 0.000010	 wd 0.0500	time 0.2588 (0.2699)	loss 0.8139 (1.1619)	grad_norm 0.3458 (0.3508)	loss_scale 2048.0000 (1133.0449)	mem 9639MB
[2024-07-31 19:06:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:08:00 lr 0.000010	 wd 0.0500	time 0.2406 (0.2668)	loss 1.2873 (1.1597)	grad_norm 0.3569 (0.3507)	loss_scale 2048.0000 (1263.5663)	mem 9639MB
[2024-07-31 19:06:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:07:30 lr 0.000010	 wd 0.0500	time 0.2678 (0.2649)	loss 1.1956 (1.1579)	grad_norm 0.3537 (0.3511)	loss_scale 2048.0000 (1361.4981)	mem 9639MB
[2024-07-31 19:06:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:07:02 lr 0.000010	 wd 0.0500	time 0.2548 (0.2635)	loss 1.2973 (1.1596)	grad_norm 0.3512 (0.3512)	loss_scale 2048.0000 (1437.6915)	mem 9639MB
[2024-07-31 19:07:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:06:33 lr 0.000010	 wd 0.0500	time 0.2353 (0.2623)	loss 1.0394 (1.1591)	grad_norm 0.3596 (0.3510)	loss_scale 2048.0000 (1498.6613)	mem 9639MB
[2024-07-31 19:07:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:06:06 lr 0.000010	 wd 0.0500	time 0.2182 (0.2614)	loss 1.0273 (1.1635)	grad_norm 0.3388 (0.3509)	loss_scale 2048.0000 (1548.5559)	mem 9639MB
[2024-07-31 19:08:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:05:39 lr 0.000010	 wd 0.0500	time 0.2218 (0.2608)	loss 0.7642 (1.1635)	grad_norm 0.3387 (0.3509)	loss_scale 2048.0000 (1590.1415)	mem 9639MB
[2024-07-31 19:08:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:05:13 lr 0.000010	 wd 0.0500	time 0.2460 (0.2604)	loss 1.4162 (1.1606)	grad_norm 0.3612 (0.3510)	loss_scale 2048.0000 (1625.3344)	mem 9639MB
[2024-07-31 19:09:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:04:46 lr 0.000010	 wd 0.0500	time 0.2394 (0.2597)	loss 1.3306 (1.1608)	grad_norm 0.3271 (0.3509)	loss_scale 2048.0000 (1655.5032)	mem 9639MB
[2024-07-31 19:09:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:04:19 lr 0.000010	 wd 0.0500	time 0.2478 (0.2594)	loss 1.4612 (1.1613)	grad_norm 0.3406 (0.3509)	loss_scale 2048.0000 (1681.6522)	mem 9639MB
[2024-07-31 19:09:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:03:53 lr 0.000010	 wd 0.0500	time 0.2355 (0.2589)	loss 1.3926 (1.1612)	grad_norm 0.3388 (0.3511)	loss_scale 2048.0000 (1704.5347)	mem 9639MB
[2024-07-31 19:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:03:27 lr 0.000010	 wd 0.0500	time 0.2106 (0.2590)	loss 1.5257 (1.1630)	grad_norm 0.3505 (0.3511)	loss_scale 2048.0000 (1724.7266)	mem 9639MB
[2024-07-31 19:10:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:01 lr 0.000010	 wd 0.0500	time 0.2221 (0.2588)	loss 0.9751 (1.1624)	grad_norm 0.3630 (0.3509)	loss_scale 2048.0000 (1742.6763)	mem 9639MB
[2024-07-31 19:11:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:02:35 lr 0.000010	 wd 0.0500	time 0.2565 (0.2587)	loss 1.2503 (1.1614)	grad_norm 0.3431 (0.3506)	loss_scale 2048.0000 (1758.7375)	mem 9639MB
[2024-07-31 19:11:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:09 lr 0.000010	 wd 0.0500	time 0.2299 (0.2587)	loss 1.2163 (1.1617)	grad_norm 0.3472 (0.3511)	loss_scale 2048.0000 (1773.1934)	mem 9639MB
[2024-07-31 19:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:43 lr 0.000010	 wd 0.0500	time 0.2889 (0.2586)	loss 1.4726 (1.1616)	grad_norm 0.3768 (0.3510)	loss_scale 2048.0000 (1786.2732)	mem 9639MB
[2024-07-31 19:12:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:18 lr 0.000010	 wd 0.0500	time 0.2169 (0.2583)	loss 1.3727 (1.1614)	grad_norm 0.3416 (0.3511)	loss_scale 2048.0000 (1798.1645)	mem 9639MB
[2024-07-31 19:12:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:52 lr 0.000010	 wd 0.0500	time 0.2492 (0.2582)	loss 0.8021 (1.1609)	grad_norm 0.3530 (0.3511)	loss_scale 2048.0000 (1809.0222)	mem 9639MB
[2024-07-31 19:13:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:26 lr 0.000010	 wd 0.0500	time 0.2537 (0.2582)	loss 0.9369 (1.1596)	grad_norm 0.3586 (0.3511)	loss_scale 2048.0000 (1818.9754)	mem 9639MB
[2024-07-31 19:13:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2157 (0.2575)	loss 1.4564 (1.1592)	grad_norm 0.5146 (0.3513)	loss_scale 2048.0000 (1828.1327)	mem 9639MB
[2024-07-31 19:13:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 17 training takes 0:10:46
[2024-07-31 19:13:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.194 (12.194)	Loss 0.4856 (0.4856)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 19:14:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.052 Acc@5 97.868
[2024-07-31 19:14:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 19:14:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-31 19:14:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:49:47 lr 0.000009	 wd 0.0500	time 11.2660 (11.2660)	loss 1.4463 (1.4463)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:14:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:14:31 lr 0.000009	 wd 0.0500	time 0.2641 (0.3627)	loss 1.4890 (1.1940)	grad_norm 0.3504 (0.3509)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:15:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:11:48 lr 0.000009	 wd 0.0500	time 0.2512 (0.3079)	loss 1.0292 (1.1854)	grad_norm 0.3834 (0.3497)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:15:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:10:33 lr 0.000009	 wd 0.0500	time 0.2194 (0.2875)	loss 1.5100 (1.1844)	grad_norm 0.3369 (0.3500)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:16:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:09:46 lr 0.000009	 wd 0.0500	time 0.2202 (0.2789)	loss 0.7515 (1.1832)	grad_norm 0.3513 (0.3497)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:16:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:09:06 lr 0.000009	 wd 0.0500	time 0.2690 (0.2729)	loss 1.2894 (1.1746)	grad_norm 0.3287 (0.3509)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:16:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:08:33 lr 0.000009	 wd 0.0500	time 0.2504 (0.2700)	loss 1.4969 (1.1671)	grad_norm 0.3507 (0.3509)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:17:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:08:01 lr 0.000009	 wd 0.0500	time 0.2641 (0.2674)	loss 1.4660 (1.1721)	grad_norm 0.3415 (0.3529)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:17:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:07:32 lr 0.000009	 wd 0.0500	time 0.2394 (0.2656)	loss 1.3242 (1.1751)	grad_norm 0.3355 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:18:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:07:04 lr 0.000009	 wd 0.0500	time 0.2377 (0.2652)	loss 0.9228 (1.1750)	grad_norm 0.3554 (0.3549)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:18:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:06:38 lr 0.000009	 wd 0.0500	time 0.2789 (0.2652)	loss 1.4288 (1.1706)	grad_norm 0.3416 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:19:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:06:11 lr 0.000009	 wd 0.0500	time 0.2616 (0.2652)	loss 1.0022 (1.1725)	grad_norm 0.3372 (0.3536)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:19:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:05:45 lr 0.000009	 wd 0.0500	time 0.2599 (0.2654)	loss 1.0549 (1.1657)	grad_norm 0.3490 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:19:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:05:18 lr 0.000009	 wd 0.0500	time 0.2445 (0.2651)	loss 1.4777 (1.1660)	grad_norm 0.3548 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:20:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:04:52 lr 0.000009	 wd 0.0500	time 0.2521 (0.2651)	loss 1.1599 (1.1662)	grad_norm 0.3295 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:20:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:04:25 lr 0.000009	 wd 0.0500	time 0.2740 (0.2647)	loss 1.3174 (1.1658)	grad_norm 0.3367 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:21:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:03:59 lr 0.000009	 wd 0.0500	time 0.2618 (0.2652)	loss 1.4590 (1.1677)	grad_norm 0.3619 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:21:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:03:32 lr 0.000009	 wd 0.0500	time 0.2633 (0.2653)	loss 1.3285 (1.1695)	grad_norm 0.3523 (0.3551)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:22:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:06 lr 0.000009	 wd 0.0500	time 0.2539 (0.2655)	loss 0.7226 (1.1698)	grad_norm 0.3488 (0.3563)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:22:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:02:39 lr 0.000009	 wd 0.0500	time 0.3364 (0.2657)	loss 1.4256 (1.1695)	grad_norm 0.3419 (0.3561)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:23:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:13 lr 0.000008	 wd 0.0500	time 0.2705 (0.2657)	loss 1.0706 (1.1691)	grad_norm 0.3413 (0.3558)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:23:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:46 lr 0.000008	 wd 0.0500	time 0.2969 (0.2658)	loss 1.3409 (1.1680)	grad_norm 0.3437 (0.3559)	loss_scale 4096.0000 (2112.3351)	mem 9639MB
[2024-07-31 19:23:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:20 lr 0.000008	 wd 0.0500	time 0.2387 (0.2657)	loss 1.0266 (1.1682)	grad_norm 0.3264 (0.3554)	loss_scale 4096.0000 (2202.4607)	mem 9639MB
[2024-07-31 19:24:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.2240 (0.2659)	loss 1.3170 (1.1689)	grad_norm 0.3359 (0.3556)	loss_scale 4096.0000 (2284.7527)	mem 9639MB
[2024-07-31 19:24:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:27 lr 0.000008	 wd 0.0500	time 0.2705 (0.2659)	loss 1.4311 (1.1695)	grad_norm 0.3656 (0.3553)	loss_scale 4096.0000 (2360.1899)	mem 9639MB
[2024-07-31 19:25:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2117 (0.2649)	loss 1.2554 (1.1698)	grad_norm 0.3411 (nan)	loss_scale 2048.0000 (2360.8093)	mem 9639MB
[2024-07-31 19:25:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 18 training takes 0:11:05
[2024-07-31 19:25:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.602 (12.602)	Loss 0.4971 (0.4971)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 19:25:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.074 Acc@5 97.844
[2024-07-31 19:25:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 19:25:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-31 19:25:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:37:13 lr 0.000008	 wd 0.0500	time 10.9644 (10.9644)	loss 0.8032 (0.8032)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:26:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:14:50 lr 0.000008	 wd 0.0500	time 0.2425 (0.3708)	loss 1.5976 (1.2158)	grad_norm 0.3356 (0.3479)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:26:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:12:03 lr 0.000008	 wd 0.0500	time 0.2523 (0.3142)	loss 1.1751 (1.1855)	grad_norm 0.3566 (0.3488)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:27:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:10:51 lr 0.000008	 wd 0.0500	time 0.2609 (0.2959)	loss 1.4119 (1.1717)	grad_norm 0.3525 (0.3492)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:27:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:10:01 lr 0.000008	 wd 0.0500	time 0.2573 (0.2862)	loss 0.9648 (1.1652)	grad_norm 0.3379 (0.3487)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:28:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:09:22 lr 0.000008	 wd 0.0500	time 0.2188 (0.2811)	loss 1.4581 (1.1696)	grad_norm 0.3489 (0.3493)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:28:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:08:49 lr 0.000008	 wd 0.0500	time 0.2692 (0.2785)	loss 0.8474 (1.1591)	grad_norm 0.3540 (0.3495)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:29:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:08:17 lr 0.000008	 wd 0.0500	time 0.2621 (0.2759)	loss 1.2427 (1.1572)	grad_norm 0.3388 (0.3517)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:29:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:07:47 lr 0.000008	 wd 0.0500	time 0.2542 (0.2746)	loss 1.5193 (1.1589)	grad_norm 0.3558 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:29:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:07:18 lr 0.000008	 wd 0.0500	time 0.2533 (0.2736)	loss 1.2781 (1.1594)	grad_norm 0.3772 (0.3530)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:30:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:06:50 lr 0.000008	 wd 0.0500	time 0.2766 (0.2732)	loss 1.4505 (1.1609)	grad_norm 0.3353 (0.3529)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:30:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:06:21 lr 0.000008	 wd 0.0500	time 0.2307 (0.2723)	loss 1.1786 (1.1599)	grad_norm 0.3483 (0.3526)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:31:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:05:53 lr 0.000008	 wd 0.0500	time 0.2707 (0.2718)	loss 1.3235 (1.1595)	grad_norm 0.3541 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:31:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:05:25 lr 0.000008	 wd 0.0500	time 0.2309 (0.2711)	loss 0.7816 (1.1585)	grad_norm 0.3593 (0.3545)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:32:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:04:58 lr 0.000008	 wd 0.0500	time 0.2432 (0.2712)	loss 0.8131 (1.1577)	grad_norm 0.3475 (0.3543)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:32:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:04:31 lr 0.000008	 wd 0.0500	time 0.2815 (0.2707)	loss 1.3691 (1.1589)	grad_norm 0.3361 (0.3541)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:33:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:04 lr 0.000007	 wd 0.0500	time 0.2473 (0.2707)	loss 1.4186 (1.1607)	grad_norm 0.3472 (0.3537)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:33:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:03:36 lr 0.000007	 wd 0.0500	time 0.2503 (0.2703)	loss 0.8959 (1.1613)	grad_norm 0.3595 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:33:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:09 lr 0.000007	 wd 0.0500	time 0.2600 (0.2701)	loss 1.3228 (1.1637)	grad_norm 0.3428 (0.3537)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:34:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:02:42 lr 0.000007	 wd 0.0500	time 0.2726 (0.2695)	loss 1.1079 (1.1627)	grad_norm 0.3577 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:34:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:15 lr 0.000007	 wd 0.0500	time 0.2397 (0.2691)	loss 1.4805 (1.1638)	grad_norm 0.3530 (0.3543)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:35:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:48 lr 0.000007	 wd 0.0500	time 0.2797 (0.2689)	loss 1.5778 (1.1652)	grad_norm 0.3398 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:35:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:21 lr 0.000007	 wd 0.0500	time 0.2712 (0.2683)	loss 1.1452 (1.1643)	grad_norm 0.5755 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:36:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:54 lr 0.000007	 wd 0.0500	time 0.2301 (0.2678)	loss 1.0231 (1.1645)	grad_norm 0.3396 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:36:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:27 lr 0.000007	 wd 0.0500	time 0.2591 (0.2674)	loss 1.1121 (1.1636)	grad_norm 0.3526 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:36:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2148 (0.2662)	loss 0.8089 (1.1639)	grad_norm 0.3502 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:36:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 19 training takes 0:11:08
[2024-07-31 19:37:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.521 (11.521)	Loss 0.4800 (0.4800)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 19:37:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.078 Acc@5 97.888
[2024-07-31 19:37:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 19:37:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.08%
[2024-07-31 19:37:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 19:37:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 19:37:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:03:32 lr 0.000007	 wd 0.0500	time 10.1567 (10.1567)	loss 1.5305 (1.5305)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:38:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:14:44 lr 0.000007	 wd 0.0500	time 0.2348 (0.3681)	loss 1.0757 (1.1737)	grad_norm 0.3587 (0.3513)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:38:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:11:49 lr 0.000007	 wd 0.0500	time 0.2670 (0.3083)	loss 1.3095 (1.1569)	grad_norm 0.3705 (0.3493)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:38:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:10:44 lr 0.000007	 wd 0.0500	time 0.2697 (0.2926)	loss 0.7030 (1.1692)	grad_norm 0.3644 (0.3486)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:39:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:09:55 lr 0.000007	 wd 0.0500	time 0.2771 (0.2831)	loss 1.3131 (1.1686)	grad_norm 0.3477 (0.3493)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:39:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:09:15 lr 0.000007	 wd 0.0500	time 0.2457 (0.2773)	loss 1.2805 (1.1666)	grad_norm 0.3438 (0.3495)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:40:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:08:41 lr 0.000007	 wd 0.0500	time 0.2493 (0.2742)	loss 1.2681 (1.1716)	grad_norm 0.3734 (0.3494)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:40:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:08:09 lr 0.000007	 wd 0.0500	time 0.2553 (0.2715)	loss 1.2623 (1.1697)	grad_norm 0.3509 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:41:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:07:39 lr 0.000007	 wd 0.0500	time 0.2338 (0.2701)	loss 1.1654 (1.1701)	grad_norm 0.3420 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:41:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:07:09 lr 0.000007	 wd 0.0500	time 0.2608 (0.2683)	loss 1.2558 (1.1694)	grad_norm 0.3557 (0.3543)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:41:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:06:41 lr 0.000007	 wd 0.0500	time 0.2396 (0.2673)	loss 1.3933 (1.1714)	grad_norm 0.3387 (0.3541)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:42:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:06:13 lr 0.000007	 wd 0.0500	time 0.2480 (0.2667)	loss 1.0738 (1.1692)	grad_norm 0.3610 (0.3539)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:42:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:05:46 lr 0.000006	 wd 0.0500	time 0.2523 (0.2661)	loss 1.2283 (1.1653)	grad_norm 0.3373 (0.3537)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:43:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:05:18 lr 0.000006	 wd 0.0500	time 0.2462 (0.2650)	loss 1.3495 (1.1648)	grad_norm 0.3486 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:43:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:04:51 lr 0.000006	 wd 0.0500	time 0.2702 (0.2649)	loss 1.0590 (1.1663)	grad_norm 0.3642 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 19:44:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:04:25 lr 0.000006	 wd 0.0500	time 0.2187 (0.2647)	loss 1.3315 (1.1690)	grad_norm 0.3364 (0.3554)	loss_scale 4096.0000 (2168.0693)	mem 9639MB
[2024-07-31 19:44:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:03:58 lr 0.000006	 wd 0.0500	time 0.2168 (0.2642)	loss 1.2962 (1.1711)	grad_norm 0.3450 (0.3554)	loss_scale 4096.0000 (2288.4897)	mem 9639MB
[2024-07-31 19:44:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:03:31 lr 0.000006	 wd 0.0500	time 0.2415 (0.2640)	loss 1.6166 (1.1724)	grad_norm 0.3350 (0.3558)	loss_scale 4096.0000 (2394.7513)	mem 9639MB
[2024-07-31 19:45:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:05 lr 0.000006	 wd 0.0500	time 0.2150 (0.2637)	loss 0.7674 (1.1700)	grad_norm 0.3438 (0.3555)	loss_scale 4096.0000 (2489.2127)	mem 9639MB
[2024-07-31 19:45:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:02:38 lr 0.000006	 wd 0.0500	time 0.2508 (0.2636)	loss 0.8439 (1.1713)	grad_norm 0.3490 (0.3552)	loss_scale 4096.0000 (2573.7359)	mem 9639MB
[2024-07-31 19:46:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:12 lr 0.000006	 wd 0.0500	time 0.2408 (0.2634)	loss 1.1018 (1.1705)	grad_norm 0.3708 (0.3551)	loss_scale 4096.0000 (2649.8111)	mem 9639MB
[2024-07-31 19:46:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:45 lr 0.000006	 wd 0.0500	time 0.2789 (0.2635)	loss 1.2445 (1.1704)	grad_norm 0.3348 (0.3549)	loss_scale 4096.0000 (2718.6445)	mem 9639MB
[2024-07-31 19:47:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:19 lr 0.000006	 wd 0.0500	time 0.2850 (0.2635)	loss 1.3966 (1.1716)	grad_norm 0.3518 (0.3554)	loss_scale 4096.0000 (2781.2231)	mem 9639MB
[2024-07-31 19:47:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:53 lr 0.000006	 wd 0.0500	time 0.2266 (0.2632)	loss 0.8724 (1.1727)	grad_norm 0.3520 (0.3551)	loss_scale 4096.0000 (2838.3625)	mem 9639MB
[2024-07-31 19:47:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:26 lr 0.000006	 wd 0.0500	time 0.2482 (0.2631)	loss 0.7816 (1.1721)	grad_norm 0.3591 (0.3548)	loss_scale 4096.0000 (2890.7422)	mem 9639MB
[2024-07-31 19:48:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.2313 (0.2622)	loss 1.0574 (1.1740)	grad_norm 0.3376 (0.3553)	loss_scale 4096.0000 (2938.9332)	mem 9639MB
[2024-07-31 19:48:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 20 training takes 0:10:58
[2024-07-31 19:48:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_20.pth saving......
[2024-07-31 19:48:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_20.pth saved !!!
[2024-07-31 19:48:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.023 (11.023)	Loss 0.4900 (0.4900)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 9639MB
[2024-07-31 19:48:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.086 Acc@5 97.874
[2024-07-31 19:48:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 19:48:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.09%
[2024-07-31 19:48:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 19:48:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 19:49:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:37:30 lr 0.000006	 wd 0.0500	time 10.9715 (10.9715)	loss 0.9979 (0.9979)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:49:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:14:29 lr 0.000006	 wd 0.0500	time 0.2253 (0.3621)	loss 1.3765 (1.1950)	grad_norm 0.3628 (0.3470)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:49:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:11:45 lr 0.000006	 wd 0.0500	time 0.2321 (0.3065)	loss 1.3286 (1.1892)	grad_norm 0.3528 (0.3490)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:50:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:10:34 lr 0.000006	 wd 0.0500	time 0.2682 (0.2884)	loss 1.1928 (1.1786)	grad_norm 0.3675 (0.3503)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:50:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:09:48 lr 0.000006	 wd 0.0500	time 0.2233 (0.2800)	loss 1.3651 (1.1707)	grad_norm 0.3428 (0.3513)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:51:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:09:11 lr 0.000006	 wd 0.0500	time 0.2431 (0.2755)	loss 0.9031 (1.1655)	grad_norm 0.3515 (0.3512)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:51:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:08:38 lr 0.000006	 wd 0.0500	time 0.2795 (0.2725)	loss 1.3046 (1.1608)	grad_norm 0.3385 (0.3514)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:52:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:08:05 lr 0.000006	 wd 0.0500	time 0.2338 (0.2696)	loss 1.2324 (1.1583)	grad_norm 0.3428 (0.3512)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:52:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:07:36 lr 0.000006	 wd 0.0500	time 0.2241 (0.2681)	loss 1.2147 (1.1568)	grad_norm 0.3503 (0.3509)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:52:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:07:07 lr 0.000005	 wd 0.0500	time 0.2293 (0.2671)	loss 1.4387 (1.1600)	grad_norm 0.3551 (0.3507)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:53:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:06:39 lr 0.000005	 wd 0.0500	time 0.2511 (0.2661)	loss 1.4981 (1.1634)	grad_norm 0.3457 (0.3507)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:53:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:06:12 lr 0.000005	 wd 0.0500	time 0.2599 (0.2655)	loss 0.8050 (1.1667)	grad_norm 0.3533 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:54:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:05:44 lr 0.000005	 wd 0.0500	time 0.2906 (0.2647)	loss 1.4026 (1.1636)	grad_norm 0.3533 (0.3510)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:54:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:05:17 lr 0.000005	 wd 0.0500	time 0.2399 (0.2639)	loss 1.1994 (1.1639)	grad_norm 0.3438 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:55:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:04:50 lr 0.000005	 wd 0.0500	time 0.2360 (0.2633)	loss 0.8332 (1.1613)	grad_norm 0.3629 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:55:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:04:23 lr 0.000005	 wd 0.0500	time 0.2663 (0.2628)	loss 0.8453 (1.1629)	grad_norm 0.3518 (0.3512)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:55:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:03:56 lr 0.000005	 wd 0.0500	time 0.2274 (0.2624)	loss 1.0501 (1.1595)	grad_norm 0.3694 (0.3511)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:56:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:03:30 lr 0.000005	 wd 0.0500	time 0.2294 (0.2622)	loss 1.3628 (1.1602)	grad_norm 0.3334 (0.3510)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:56:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:03 lr 0.000005	 wd 0.0500	time 0.2679 (0.2619)	loss 0.7820 (1.1608)	grad_norm 0.3626 (0.3510)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:57:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:02:37 lr 0.000005	 wd 0.0500	time 0.2252 (0.2614)	loss 0.8513 (1.1615)	grad_norm 0.3447 (0.3509)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:57:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:11 lr 0.000005	 wd 0.0500	time 0.2773 (0.2613)	loss 1.0694 (1.1605)	grad_norm 0.3752 (0.3510)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:58:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:45 lr 0.000005	 wd 0.0500	time 0.2707 (0.2613)	loss 1.1429 (1.1602)	grad_norm 0.3492 (0.3509)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:18 lr 0.000005	 wd 0.0500	time 0.2699 (0.2613)	loss 1.4455 (1.1588)	grad_norm 0.3522 (0.3509)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:58:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:52 lr 0.000005	 wd 0.0500	time 0.2237 (0.2610)	loss 1.5458 (1.1606)	grad_norm 0.3709 (0.3510)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:59:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:26 lr 0.000005	 wd 0.0500	time 0.2610 (0.2609)	loss 1.2299 (1.1623)	grad_norm 0.3550 (0.3512)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:59:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2301 (0.2599)	loss 1.4889 (1.1628)	grad_norm 0.3681 (0.3512)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 19:59:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 21 training takes 0:10:52
[2024-07-31 19:59:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.441 (12.441)	Loss 0.5107 (0.5107)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 20:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.056 Acc@5 97.860
[2024-07-31 20:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 20:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.09%
[2024-07-31 20:00:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:36:36 lr 0.000005	 wd 0.0500	time 10.9497 (10.9497)	loss 0.9922 (0.9922)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 20:00:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:14:35 lr 0.000005	 wd 0.0500	time 0.2534 (0.3643)	loss 1.4750 (1.1786)	grad_norm 0.3719 (0.3516)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 20:01:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:11:48 lr 0.000005	 wd 0.0500	time 0.2362 (0.3079)	loss 0.7530 (1.1713)	grad_norm 0.3362 (0.3530)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 20:01:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:10:38 lr 0.000005	 wd 0.0500	time 0.2439 (0.2900)	loss 1.3816 (1.1771)	grad_norm 0.3509 (0.3537)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 20:02:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:09:49 lr 0.000005	 wd 0.0500	time 0.2335 (0.2805)	loss 0.7555 (1.1833)	grad_norm 0.3433 (0.3595)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 20:02:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:09:11 lr 0.000005	 wd 0.0500	time 0.2558 (0.2753)	loss 1.1960 (1.1809)	grad_norm 0.3365 (0.3578)	loss_scale 8192.0000 (4848.1597)	mem 9639MB
[2024-07-31 20:02:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:08:36 lr 0.000005	 wd 0.0500	time 0.2369 (0.2718)	loss 0.8858 (1.1825)	grad_norm 0.3387 (nan)	loss_scale 4096.0000 (5050.1431)	mem 9639MB
[2024-07-31 20:03:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:08:07 lr 0.000005	 wd 0.0500	time 0.2270 (0.2706)	loss 1.3504 (1.1770)	grad_norm 0.3618 (nan)	loss_scale 4096.0000 (4914.0314)	mem 9639MB
[2024-07-31 20:03:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:07:36 lr 0.000004	 wd 0.0500	time 0.2435 (0.2684)	loss 0.9388 (1.1756)	grad_norm 0.3359 (nan)	loss_scale 4096.0000 (4811.9051)	mem 9639MB
[2024-07-31 20:04:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:07:08 lr 0.000004	 wd 0.0500	time 0.2625 (0.2676)	loss 1.1098 (1.1745)	grad_norm 0.3611 (nan)	loss_scale 4096.0000 (4732.4484)	mem 9639MB
[2024-07-31 20:04:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:06:40 lr 0.000004	 wd 0.0500	time 0.2859 (0.2668)	loss 1.2784 (1.1748)	grad_norm 0.3437 (nan)	loss_scale 4096.0000 (4668.8671)	mem 9639MB
[2024-07-31 20:05:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:06:12 lr 0.000004	 wd 0.0500	time 0.2487 (0.2657)	loss 0.8168 (1.1718)	grad_norm 0.3482 (nan)	loss_scale 4096.0000 (4616.8356)	mem 9639MB
[2024-07-31 20:05:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:05:44 lr 0.000004	 wd 0.0500	time 0.2297 (0.2645)	loss 1.3686 (1.1728)	grad_norm 0.3462 (nan)	loss_scale 4096.0000 (4573.4688)	mem 9639MB
[2024-07-31 20:05:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:05:17 lr 0.000004	 wd 0.0500	time 0.2656 (0.2641)	loss 0.8550 (1.1665)	grad_norm 0.3458 (nan)	loss_scale 4096.0000 (4536.7686)	mem 9639MB
[2024-07-31 20:06:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:04:50 lr 0.000004	 wd 0.0500	time 0.2425 (0.2636)	loss 1.0035 (1.1610)	grad_norm 0.3499 (nan)	loss_scale 4096.0000 (4505.3076)	mem 9639MB
[2024-07-31 20:06:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:04:23 lr 0.000004	 wd 0.0500	time 0.2384 (0.2630)	loss 1.2111 (1.1607)	grad_norm 0.3877 (nan)	loss_scale 4096.0000 (4478.0386)	mem 9639MB
[2024-07-31 20:07:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:03:57 lr 0.000004	 wd 0.0500	time 0.2248 (0.2629)	loss 1.5245 (1.1608)	grad_norm 0.3442 (nan)	loss_scale 4096.0000 (4454.1761)	mem 9639MB
[2024-07-31 20:07:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:03:30 lr 0.000004	 wd 0.0500	time 0.2289 (0.2626)	loss 0.8972 (1.1590)	grad_norm 0.3594 (nan)	loss_scale 4096.0000 (4433.1193)	mem 9639MB
[2024-07-31 20:08:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:04 lr 0.000004	 wd 0.0500	time 0.2505 (0.2627)	loss 1.4024 (1.1576)	grad_norm 0.3285 (nan)	loss_scale 4096.0000 (4414.4009)	mem 9639MB
[2024-07-31 20:08:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:02:38 lr 0.000004	 wd 0.0500	time 0.2458 (0.2628)	loss 0.9412 (1.1585)	grad_norm 0.3367 (nan)	loss_scale 4096.0000 (4397.6518)	mem 9639MB
[2024-07-31 20:09:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:11 lr 0.000004	 wd 0.0500	time 0.2563 (0.2628)	loss 0.9265 (1.1595)	grad_norm 0.3383 (nan)	loss_scale 4096.0000 (4382.5767)	mem 9639MB
[2024-07-31 20:09:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:45 lr 0.000004	 wd 0.0500	time 0.2680 (0.2627)	loss 1.3074 (1.1610)	grad_norm 0.3433 (nan)	loss_scale 4096.0000 (4368.9367)	mem 9639MB
[2024-07-31 20:09:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:19 lr 0.000004	 wd 0.0500	time 0.2740 (0.2626)	loss 1.0949 (1.1608)	grad_norm 0.3932 (nan)	loss_scale 4096.0000 (4356.5361)	mem 9639MB
[2024-07-31 20:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:53 lr 0.000004	 wd 0.0500	time 0.2277 (0.2626)	loss 1.1429 (1.1609)	grad_norm 0.3360 (nan)	loss_scale 2048.0000 (4286.4702)	mem 9639MB
[2024-07-31 20:10:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:26 lr 0.000004	 wd 0.0500	time 0.2582 (0.2626)	loss 1.3486 (1.1619)	grad_norm 0.3654 (nan)	loss_scale 2048.0000 (4193.2395)	mem 9639MB
[2024-07-31 20:11:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2225 (0.2615)	loss 1.1130 (1.1616)	grad_norm 0.3391 (nan)	loss_scale 2048.0000 (4107.4642)	mem 9639MB
[2024-07-31 20:11:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 22 training takes 0:10:56
[2024-07-31 20:11:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.726 (11.726)	Loss 0.4932 (0.4932)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 9639MB
[2024-07-31 20:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.104 Acc@5 97.876
[2024-07-31 20:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 20:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-07-31 20:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 20:11:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 20:11:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:21:17 lr 0.000004	 wd 0.0500	time 10.5825 (10.5825)	loss 0.7683 (0.7683)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:12:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:14:03 lr 0.000004	 wd 0.0500	time 0.2151 (0.3510)	loss 1.4849 (1.1513)	grad_norm 0.3575 (nan)	loss_scale 1024.0000 (1642.4554)	mem 9639MB
[2024-07-31 20:12:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:11:32 lr 0.000004	 wd 0.0500	time 0.2424 (0.3007)	loss 0.8535 (1.1564)	grad_norm 0.3513 (nan)	loss_scale 1024.0000 (1334.7662)	mem 9639MB
[2024-07-31 20:13:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:10:25 lr 0.000004	 wd 0.0500	time 0.2345 (0.2842)	loss 1.4305 (1.1471)	grad_norm 0.3575 (nan)	loss_scale 1024.0000 (1231.5216)	mem 9639MB
[2024-07-31 20:13:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:09:40 lr 0.000004	 wd 0.0500	time 0.2295 (0.2763)	loss 0.9043 (1.1526)	grad_norm 0.3531 (nan)	loss_scale 1024.0000 (1179.7706)	mem 9639MB
[2024-07-31 20:13:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:09:03 lr 0.000004	 wd 0.0500	time 0.2684 (0.2714)	loss 0.8207 (1.1557)	grad_norm 0.3397 (nan)	loss_scale 1024.0000 (1148.6786)	mem 9639MB
[2024-07-31 20:14:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:08:30 lr 0.000004	 wd 0.0500	time 0.2526 (0.2683)	loss 1.4399 (1.1549)	grad_norm 0.3629 (nan)	loss_scale 1024.0000 (1127.9334)	mem 9639MB
[2024-07-31 20:14:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:08:01 lr 0.000004	 wd 0.0500	time 0.2198 (0.2672)	loss 1.1414 (1.1583)	grad_norm 0.3588 (nan)	loss_scale 1024.0000 (1113.1070)	mem 9639MB
[2024-07-31 20:15:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:07:31 lr 0.000003	 wd 0.0500	time 0.2572 (0.2652)	loss 1.4525 (1.1600)	grad_norm 0.3373 (nan)	loss_scale 1024.0000 (1101.9825)	mem 9639MB
[2024-07-31 20:15:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:07:01 lr 0.000003	 wd 0.0500	time 0.2504 (0.2633)	loss 1.1451 (1.1606)	grad_norm 0.3437 (nan)	loss_scale 1024.0000 (1093.3274)	mem 9639MB
[2024-07-31 20:16:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:06:34 lr 0.000003	 wd 0.0500	time 0.2652 (0.2623)	loss 1.1345 (1.1562)	grad_norm 0.3177 (nan)	loss_scale 1024.0000 (1086.4016)	mem 9639MB
[2024-07-31 20:16:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:06:06 lr 0.000003	 wd 0.0500	time 0.2458 (0.2615)	loss 1.0374 (1.1590)	grad_norm 0.3587 (nan)	loss_scale 1024.0000 (1080.7339)	mem 9639MB
[2024-07-31 20:16:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:05:39 lr 0.000003	 wd 0.0500	time 0.2136 (0.2609)	loss 1.4081 (1.1620)	grad_norm 0.3471 (nan)	loss_scale 1024.0000 (1076.0100)	mem 9639MB
[2024-07-31 20:17:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:05:12 lr 0.000003	 wd 0.0500	time 0.2594 (0.2604)	loss 1.2651 (1.1627)	grad_norm 0.3331 (nan)	loss_scale 1024.0000 (1072.0123)	mem 9639MB
[2024-07-31 20:17:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:04:46 lr 0.000003	 wd 0.0500	time 0.2414 (0.2602)	loss 1.2722 (1.1628)	grad_norm 0.3679 (nan)	loss_scale 1024.0000 (1068.5853)	mem 9639MB
[2024-07-31 20:18:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:04:20 lr 0.000003	 wd 0.0500	time 0.2424 (0.2600)	loss 1.2929 (1.1612)	grad_norm 0.3527 (nan)	loss_scale 1024.0000 (1065.6149)	mem 9639MB
[2024-07-31 20:18:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:03:54 lr 0.000003	 wd 0.0500	time 0.2437 (0.2599)	loss 1.1449 (1.1646)	grad_norm 0.3321 (nan)	loss_scale 1024.0000 (1063.0156)	mem 9639MB
[2024-07-31 20:19:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:03:28 lr 0.000003	 wd 0.0500	time 0.2442 (0.2601)	loss 0.7176 (1.1620)	grad_norm 0.3562 (nan)	loss_scale 1024.0000 (1060.7219)	mem 9639MB
[2024-07-31 20:19:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:02 lr 0.000003	 wd 0.0500	time 0.2530 (0.2599)	loss 1.3140 (1.1611)	grad_norm 0.3704 (nan)	loss_scale 1024.0000 (1058.6830)	mem 9639MB
[2024-07-31 20:19:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:02:36 lr 0.000003	 wd 0.0500	time 0.2456 (0.2597)	loss 1.3033 (1.1605)	grad_norm 0.3397 (nan)	loss_scale 1024.0000 (1056.8585)	mem 9639MB
[2024-07-31 20:20:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:10 lr 0.000003	 wd 0.0500	time 0.2324 (0.2596)	loss 0.9383 (1.1602)	grad_norm 0.3574 (nan)	loss_scale 1024.0000 (1055.2164)	mem 9639MB
[2024-07-31 20:20:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:44 lr 0.000003	 wd 0.0500	time 0.2419 (0.2597)	loss 1.3899 (1.1606)	grad_norm 0.3499 (nan)	loss_scale 1024.0000 (1053.7306)	mem 9639MB
[2024-07-31 20:21:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:18 lr 0.000003	 wd 0.0500	time 0.2990 (0.2597)	loss 1.3305 (1.1591)	grad_norm 0.3674 (nan)	loss_scale 1024.0000 (1052.3798)	mem 9639MB
[2024-07-31 20:21:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:52 lr 0.000003	 wd 0.0500	time 0.2559 (0.2598)	loss 1.3736 (1.1592)	grad_norm 0.3357 (nan)	loss_scale 1024.0000 (1051.1465)	mem 9639MB
[2024-07-31 20:22:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:26 lr 0.000003	 wd 0.0500	time 0.2260 (0.2597)	loss 0.8197 (1.1590)	grad_norm 0.3458 (nan)	loss_scale 1024.0000 (1050.0158)	mem 9639MB
[2024-07-31 20:22:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2156 (0.2588)	loss 0.8768 (1.1582)	grad_norm 0.3434 (nan)	loss_scale 1024.0000 (1048.9756)	mem 9639MB
[2024-07-31 20:22:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 23 training takes 0:10:51
[2024-07-31 20:22:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.953 (11.953)	Loss 0.5107 (0.5107)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 9639MB
[2024-07-31 20:23:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.074 Acc@5 97.868
[2024-07-31 20:23:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 20:23:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-07-31 20:23:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:21:43 lr 0.000003	 wd 0.0500	time 10.5930 (10.5930)	loss 1.2264 (1.2264)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:23:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:14:24 lr 0.000003	 wd 0.0500	time 0.2395 (0.3601)	loss 0.9433 (1.1648)	grad_norm 0.3352 (0.3508)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:24:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:11:43 lr 0.000003	 wd 0.0500	time 0.2252 (0.3055)	loss 1.2306 (1.1544)	grad_norm 0.3336 (0.3507)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:10:33 lr 0.000003	 wd 0.0500	time 0.2596 (0.2877)	loss 1.3722 (1.1687)	grad_norm 0.3782 (0.3514)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:24:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:09:46 lr 0.000003	 wd 0.0500	time 0.2560 (0.2792)	loss 0.7246 (1.1684)	grad_norm 0.3575 (0.3529)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:25:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:09:06 lr 0.000003	 wd 0.0500	time 0.2660 (0.2731)	loss 1.4447 (1.1686)	grad_norm 0.3680 (0.3528)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:25:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:08:32 lr 0.000003	 wd 0.0500	time 0.2346 (0.2693)	loss 0.6877 (1.1637)	grad_norm 0.3418 (0.3532)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:26:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:08:02 lr 0.000003	 wd 0.0500	time 0.2408 (0.2677)	loss 1.2014 (1.1622)	grad_norm 0.3330 (0.3526)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:07:32 lr 0.000003	 wd 0.0500	time 0.2631 (0.2657)	loss 1.2261 (1.1643)	grad_norm 0.3553 (0.3522)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:27:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:07:04 lr 0.000003	 wd 0.0500	time 0.2218 (0.2647)	loss 0.9402 (1.1681)	grad_norm 0.3372 (0.3519)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:27:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:06:35 lr 0.000003	 wd 0.0500	time 0.2525 (0.2636)	loss 0.8011 (1.1651)	grad_norm 0.3542 (0.3528)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:27:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:06:09 lr 0.000003	 wd 0.0500	time 0.2644 (0.2638)	loss 0.8674 (1.1645)	grad_norm 0.3617 (0.3543)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:28:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:05:42 lr 0.000002	 wd 0.0500	time 0.2158 (0.2632)	loss 0.8940 (1.1625)	grad_norm 0.3502 (0.3541)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:05:15 lr 0.000002	 wd 0.0500	time 0.2244 (0.2626)	loss 0.7662 (1.1595)	grad_norm 0.3644 (0.3547)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:29:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:04:48 lr 0.000002	 wd 0.0500	time 0.2390 (0.2620)	loss 1.4653 (1.1606)	grad_norm 0.6014 (0.3558)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:29:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:04:21 lr 0.000002	 wd 0.0500	time 0.2305 (0.2613)	loss 1.2479 (1.1568)	grad_norm 0.3978 (0.3559)	loss_scale 1024.0000 (1024.0000)	mem 9639MB
[2024-07-31 20:30:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:03:55 lr 0.000002	 wd 0.0500	time 0.2461 (0.2612)	loss 1.2989 (1.1558)	grad_norm 0.3578 (0.3558)	loss_scale 2048.0000 (1050.8632)	mem 9639MB
[2024-07-31 20:30:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:03:29 lr 0.000002	 wd 0.0500	time 0.2656 (0.2611)	loss 1.5085 (1.1573)	grad_norm 0.3575 (0.3554)	loss_scale 2048.0000 (1109.4838)	mem 9639MB
[2024-07-31 20:30:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:03 lr 0.000002	 wd 0.0500	time 0.2518 (0.2607)	loss 1.0960 (1.1572)	grad_norm 0.3547 (0.3550)	loss_scale 2048.0000 (1161.5947)	mem 9639MB
[2024-07-31 20:31:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:02:36 lr 0.000002	 wd 0.0500	time 0.2628 (0.2607)	loss 0.9225 (1.1584)	grad_norm 0.3357 (0.3547)	loss_scale 2048.0000 (1208.2230)	mem 9639MB
[2024-07-31 20:31:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:10 lr 0.000002	 wd 0.0500	time 0.2280 (0.2604)	loss 1.2876 (1.1584)	grad_norm 0.4541 (0.3545)	loss_scale 2048.0000 (1250.1909)	mem 9639MB
[2024-07-31 20:32:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:44 lr 0.000002	 wd 0.0500	time 0.2557 (0.2604)	loss 1.0141 (1.1586)	grad_norm 0.3562 (0.3544)	loss_scale 2048.0000 (1288.1637)	mem 9639MB
[2024-07-31 20:32:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:18 lr 0.000002	 wd 0.0500	time 0.2599 (0.2602)	loss 1.0183 (1.1603)	grad_norm 0.3647 (0.3542)	loss_scale 2048.0000 (1322.6861)	mem 9639MB
[2024-07-31 20:33:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:52 lr 0.000002	 wd 0.0500	time 0.2244 (0.2602)	loss 1.3876 (1.1627)	grad_norm 0.3773 (0.3542)	loss_scale 2048.0000 (1354.2077)	mem 9639MB
[2024-07-31 20:33:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:26 lr 0.000002	 wd 0.0500	time 0.2229 (0.2601)	loss 1.2268 (1.1605)	grad_norm 0.3367 (0.3542)	loss_scale 2048.0000 (1383.1037)	mem 9639MB
[2024-07-31 20:33:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2159 (0.2593)	loss 0.9361 (1.1611)	grad_norm 0.3314 (0.3543)	loss_scale 2048.0000 (1409.6889)	mem 9639MB
[2024-07-31 20:33:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 24 training takes 0:10:54
[2024-07-31 20:34:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.017 (12.017)	Loss 0.4939 (0.4939)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 9639MB
[2024-07-31 20:34:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.066 Acc@5 97.872
[2024-07-31 20:34:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 20:34:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-07-31 20:34:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:16:58 lr 0.000002	 wd 0.0500	time 11.9177 (11.9177)	loss 1.2799 (1.2799)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:35:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:14:50 lr 0.000002	 wd 0.0500	time 0.2481 (0.3709)	loss 1.1978 (1.2185)	grad_norm 0.3449 (0.3526)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:35:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:11:55 lr 0.000002	 wd 0.0500	time 0.2434 (0.3109)	loss 1.4355 (1.1880)	grad_norm 0.3494 (0.3534)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:35:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:10:43 lr 0.000002	 wd 0.0500	time 0.2311 (0.2923)	loss 1.1361 (1.1715)	grad_norm 0.3516 (0.3521)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:36:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:09:52 lr 0.000002	 wd 0.0500	time 0.2429 (0.2819)	loss 1.3068 (1.1724)	grad_norm 0.3361 (0.3544)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:36:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:09:15 lr 0.000002	 wd 0.0500	time 0.2499 (0.2773)	loss 1.4289 (1.1707)	grad_norm 0.3562 (0.3533)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:37:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:08:39 lr 0.000002	 wd 0.0500	time 0.2387 (0.2733)	loss 0.8250 (1.1606)	grad_norm 0.3506 (0.3523)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:37:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:08:06 lr 0.000002	 wd 0.0500	time 0.2483 (0.2700)	loss 0.8035 (1.1624)	grad_norm 0.3639 (0.3522)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:38:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:07:36 lr 0.000002	 wd 0.0500	time 0.2308 (0.2679)	loss 1.3593 (1.1615)	grad_norm 0.3732 (0.3521)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:38:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:07:06 lr 0.000002	 wd 0.0500	time 0.2349 (0.2663)	loss 1.4401 (1.1613)	grad_norm 0.3453 (0.3517)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:38:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:06:38 lr 0.000002	 wd 0.0500	time 0.2348 (0.2651)	loss 1.5142 (1.1627)	grad_norm 0.3601 (0.3515)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:39:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:06:11 lr 0.000002	 wd 0.0500	time 0.2336 (0.2646)	loss 1.5512 (1.1636)	grad_norm 0.3560 (0.3513)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:39:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:05:43 lr 0.000002	 wd 0.0500	time 0.2347 (0.2639)	loss 0.9707 (1.1601)	grad_norm 0.3385 (0.3512)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:40:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:05:16 lr 0.000002	 wd 0.0500	time 0.2271 (0.2632)	loss 0.8338 (1.1590)	grad_norm 0.3673 (0.3511)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:40:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:04:49 lr 0.000002	 wd 0.0500	time 0.2362 (0.2625)	loss 0.8117 (1.1586)	grad_norm 0.3614 (0.3509)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:41:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:04:22 lr 0.000002	 wd 0.0500	time 0.2593 (0.2622)	loss 0.8644 (1.1587)	grad_norm 0.3606 (0.3509)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:41:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:03:56 lr 0.000002	 wd 0.0500	time 0.2407 (0.2620)	loss 0.8071 (1.1579)	grad_norm 0.3326 (0.3512)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:41:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:03:29 lr 0.000002	 wd 0.0500	time 0.2817 (0.2618)	loss 0.7457 (1.1556)	grad_norm 0.3536 (0.3511)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:42:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:03 lr 0.000002	 wd 0.0500	time 0.2382 (0.2616)	loss 0.8854 (1.1575)	grad_norm 0.3502 (0.3512)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:42:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:02:37 lr 0.000002	 wd 0.0500	time 0.2307 (0.2615)	loss 1.6558 (1.1598)	grad_norm 0.3478 (0.3521)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:43:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:11 lr 0.000002	 wd 0.0500	time 0.2604 (0.2617)	loss 0.9888 (1.1603)	grad_norm 0.3426 (0.3520)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:43:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:45 lr 0.000002	 wd 0.0500	time 0.2470 (0.2614)	loss 1.2914 (1.1615)	grad_norm 0.3407 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:44:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:18 lr 0.000001	 wd 0.0500	time 0.2348 (0.2615)	loss 1.1207 (1.1618)	grad_norm 0.3463 (0.3523)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:44:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:52 lr 0.000001	 wd 0.0500	time 0.2703 (0.2616)	loss 1.3442 (1.1613)	grad_norm 0.3428 (0.3524)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:44:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:26 lr 0.000001	 wd 0.0500	time 0.2196 (0.2615)	loss 1.3600 (1.1603)	grad_norm 0.3172 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:45:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2627 (0.2605)	loss 0.7475 (1.1592)	grad_norm 0.3802 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:45:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 25 training takes 0:10:56
[2024-07-31 20:45:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 12.304 (12.304)	Loss 0.5171 (0.5171)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 20:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.068 Acc@5 97.842
[2024-07-31 20:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 20:45:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-07-31 20:46:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:41:42 lr 0.000001	 wd 0.0500	time 11.0722 (11.0722)	loss 1.1666 (1.1666)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:46:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:14:36 lr 0.000001	 wd 0.0500	time 0.2902 (0.3650)	loss 1.3147 (1.1722)	grad_norm 0.3355 (0.3547)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:46:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:11:51 lr 0.000001	 wd 0.0500	time 0.2599 (0.3090)	loss 1.0880 (1.1715)	grad_norm 0.3589 (0.3523)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:47:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:10:40 lr 0.000001	 wd 0.0500	time 0.2343 (0.2907)	loss 0.9154 (1.1534)	grad_norm 0.3418 (0.3562)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:47:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:09:51 lr 0.000001	 wd 0.0500	time 0.2344 (0.2813)	loss 1.3509 (1.1518)	grad_norm 0.3644 (0.3609)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:48:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:09:11 lr 0.000001	 wd 0.0500	time 0.2354 (0.2756)	loss 1.3213 (1.1608)	grad_norm 0.3459 (0.3595)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:48:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:08:38 lr 0.000001	 wd 0.0500	time 0.2352 (0.2724)	loss 0.7914 (1.1647)	grad_norm 0.3535 (0.3582)	loss_scale 4096.0000 (2204.7521)	mem 9639MB
[2024-07-31 20:49:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:08:05 lr 0.000001	 wd 0.0500	time 0.2388 (0.2694)	loss 1.4154 (1.1607)	grad_norm 0.3284 (0.3581)	loss_scale 4096.0000 (2474.5449)	mem 9639MB
[2024-07-31 20:49:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:07:34 lr 0.000001	 wd 0.0500	time 0.2321 (0.2670)	loss 1.2455 (1.1647)	grad_norm 0.3630 (0.3582)	loss_scale 4096.0000 (2676.9738)	mem 9639MB
[2024-07-31 20:49:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:07:05 lr 0.000001	 wd 0.0500	time 0.2294 (0.2655)	loss 1.2849 (1.1639)	grad_norm 0.3404 (0.3573)	loss_scale 4096.0000 (2834.4684)	mem 9639MB
[2024-07-31 20:50:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:06:37 lr 0.000001	 wd 0.0500	time 0.3095 (0.2648)	loss 1.1299 (1.1629)	grad_norm 0.3411 (0.3565)	loss_scale 4096.0000 (2960.4955)	mem 9639MB
[2024-07-31 20:50:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:06:09 lr 0.000001	 wd 0.0500	time 0.2583 (0.2636)	loss 0.9419 (1.1629)	grad_norm 0.3419 (0.3557)	loss_scale 4096.0000 (3063.6294)	mem 9639MB
[2024-07-31 20:51:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:05:42 lr 0.000001	 wd 0.0500	time 0.2614 (0.2628)	loss 1.0844 (1.1645)	grad_norm 0.3497 (0.3555)	loss_scale 4096.0000 (3149.5887)	mem 9639MB
[2024-07-31 20:51:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:05:14 lr 0.000001	 wd 0.0500	time 0.2385 (0.2620)	loss 1.2860 (1.1660)	grad_norm 0.3647 (0.3552)	loss_scale 4096.0000 (3222.3336)	mem 9639MB
[2024-07-31 20:52:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:04:47 lr 0.000001	 wd 0.0500	time 0.2824 (0.2611)	loss 1.3845 (1.1669)	grad_norm 0.3492 (0.3550)	loss_scale 4096.0000 (3284.6938)	mem 9639MB
[2024-07-31 20:52:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:04:20 lr 0.000001	 wd 0.0500	time 0.2230 (0.2603)	loss 1.4084 (1.1685)	grad_norm 0.3411 (nan)	loss_scale 2048.0000 (3215.9467)	mem 9639MB
[2024-07-31 20:52:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:03:54 lr 0.000001	 wd 0.0500	time 0.2188 (0.2599)	loss 1.2105 (1.1660)	grad_norm 0.3433 (nan)	loss_scale 2048.0000 (3142.9956)	mem 9639MB
[2024-07-31 20:53:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:03:28 lr 0.000001	 wd 0.0500	time 0.2608 (0.2596)	loss 1.2971 (1.1641)	grad_norm 0.3433 (nan)	loss_scale 2048.0000 (3078.6220)	mem 9639MB
[2024-07-31 20:53:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:02 lr 0.000001	 wd 0.0500	time 0.2452 (0.2594)	loss 0.8779 (1.1640)	grad_norm 0.3638 (nan)	loss_scale 2048.0000 (3021.3970)	mem 9639MB
[2024-07-31 20:54:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:02:36 lr 0.000001	 wd 0.0500	time 0.2650 (0.2592)	loss 1.2967 (1.1651)	grad_norm 0.3585 (nan)	loss_scale 2048.0000 (2970.1925)	mem 9639MB
[2024-07-31 20:54:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:09 lr 0.000001	 wd 0.0500	time 0.2277 (0.2588)	loss 1.1888 (1.1636)	grad_norm 0.3520 (nan)	loss_scale 2048.0000 (2924.1059)	mem 9639MB
[2024-07-31 20:55:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:43 lr 0.000001	 wd 0.0500	time 0.2613 (0.2586)	loss 1.3685 (1.1625)	grad_norm 0.3665 (nan)	loss_scale 2048.0000 (2882.4065)	mem 9639MB
[2024-07-31 20:55:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:18 lr 0.000001	 wd 0.0500	time 0.2216 (0.2586)	loss 1.2882 (1.1610)	grad_norm 0.3511 (nan)	loss_scale 2048.0000 (2844.4961)	mem 9639MB
[2024-07-31 20:55:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:52 lr 0.000001	 wd 0.0500	time 0.2705 (0.2587)	loss 1.2968 (1.1612)	grad_norm 0.3291 (nan)	loss_scale 2048.0000 (2809.8809)	mem 9639MB
[2024-07-31 20:56:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:26 lr 0.000001	 wd 0.0500	time 0.2518 (0.2585)	loss 0.7218 (1.1616)	grad_norm 0.3398 (nan)	loss_scale 2048.0000 (2778.1491)	mem 9639MB
[2024-07-31 20:56:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2165 (0.2580)	loss 0.9383 (1.1621)	grad_norm 0.3604 (nan)	loss_scale 2048.0000 (2748.9548)	mem 9639MB
[2024-07-31 20:56:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 26 training takes 0:10:50
[2024-07-31 20:56:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.736 (11.736)	Loss 0.4998 (0.4998)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 20:57:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.078 Acc@5 97.830
[2024-07-31 20:57:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 20:57:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-07-31 20:57:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:01:06 lr 0.000001	 wd 0.0500	time 11.5373 (11.5373)	loss 1.4042 (1.4042)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:57:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:14:36 lr 0.000001	 wd 0.0500	time 0.2239 (0.3648)	loss 1.2858 (1.1528)	grad_norm 0.3243 (0.3514)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:58:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:11:51 lr 0.000001	 wd 0.0500	time 0.2215 (0.3093)	loss 1.3797 (1.1651)	grad_norm 0.3533 (0.3632)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:58:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:10:40 lr 0.000001	 wd 0.0500	time 0.2336 (0.2907)	loss 1.3490 (1.1646)	grad_norm 0.3421 (0.3589)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:59:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:09:51 lr 0.000001	 wd 0.0500	time 0.2304 (0.2813)	loss 1.0020 (1.1610)	grad_norm 0.3558 (0.3563)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 20:59:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:09:09 lr 0.000001	 wd 0.0500	time 0.2213 (0.2747)	loss 0.6801 (1.1620)	grad_norm 0.3521 (0.3549)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:00:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:08:34 lr 0.000001	 wd 0.0500	time 0.2467 (0.2708)	loss 1.3249 (1.1619)	grad_norm 0.3583 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:00:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:08:03 lr 0.000001	 wd 0.0500	time 0.2614 (0.2685)	loss 0.9835 (1.1637)	grad_norm 0.3559 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:00:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:07:33 lr 0.000001	 wd 0.0500	time 0.2462 (0.2666)	loss 0.9307 (1.1653)	grad_norm 0.3607 (0.3538)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:01:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:07:05 lr 0.000001	 wd 0.0500	time 0.2475 (0.2655)	loss 1.2420 (1.1633)	grad_norm 0.3413 (0.3536)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:01:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:06:37 lr 0.000001	 wd 0.0500	time 0.2591 (0.2645)	loss 1.1371 (1.1600)	grad_norm 0.3155 (0.3566)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:02:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:06:09 lr 0.000001	 wd 0.0500	time 0.2258 (0.2634)	loss 1.1417 (1.1621)	grad_norm 0.3418 (0.3566)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:02:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:05:41 lr 0.000001	 wd 0.0500	time 0.2438 (0.2626)	loss 0.7207 (1.1628)	grad_norm 0.3407 (0.3561)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:03:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:05:14 lr 0.000001	 wd 0.0500	time 0.2519 (0.2618)	loss 0.8051 (1.1596)	grad_norm 0.3515 (0.3557)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:03:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:04:47 lr 0.000001	 wd 0.0500	time 0.2494 (0.2612)	loss 0.8302 (1.1608)	grad_norm 0.3442 (0.3562)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:03:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:04:21 lr 0.000001	 wd 0.0500	time 0.2166 (0.2607)	loss 1.4588 (1.1618)	grad_norm 0.3321 (0.3559)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:04:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:03:54 lr 0.000001	 wd 0.0500	time 0.2289 (0.2602)	loss 1.3182 (1.1635)	grad_norm 0.3501 (0.3557)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:04:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:03:28 lr 0.000001	 wd 0.0500	time 0.2248 (0.2601)	loss 1.3574 (1.1652)	grad_norm 0.3557 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:05:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:02 lr 0.000001	 wd 0.0500	time 0.2261 (0.2603)	loss 1.4088 (1.1647)	grad_norm 0.3437 (0.3553)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:05:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:02:36 lr 0.000001	 wd 0.0500	time 0.2346 (0.2600)	loss 1.3819 (1.1655)	grad_norm 0.3484 (0.3550)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:05:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:10 lr 0.000001	 wd 0.0500	time 0.2880 (0.2598)	loss 1.4038 (1.1663)	grad_norm 0.3365 (0.3551)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:06:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:44 lr 0.000001	 wd 0.0500	time 0.2665 (0.2596)	loss 1.5079 (1.1653)	grad_norm 0.3556 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:06:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:18 lr 0.000001	 wd 0.0500	time 0.2576 (0.2595)	loss 1.1907 (1.1666)	grad_norm 0.3606 (0.3561)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:07:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:52 lr 0.000001	 wd 0.0500	time 0.2837 (0.2595)	loss 1.3011 (1.1664)	grad_norm 0.3402 (0.3558)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:07:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:26 lr 0.000001	 wd 0.0500	time 0.2376 (0.2594)	loss 1.3543 (1.1680)	grad_norm 0.3736 (0.3556)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:08:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2153 (0.2584)	loss 1.2618 (1.1672)	grad_norm 0.3645 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:08:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 27 training takes 0:10:51
[2024-07-31 21:08:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.097 (11.097)	Loss 0.4871 (0.4871)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 21:08:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.126 Acc@5 97.864
[2024-07-31 21:08:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 21:08:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.13%
[2024-07-31 21:08:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saving......
[2024-07-31 21:08:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth saved !!!
[2024-07-31 21:08:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:16:29 lr 0.000001	 wd 0.0500	time 10.4673 (10.4673)	loss 0.9666 (0.9666)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:09:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:14:34 lr 0.000000	 wd 0.0500	time 0.2238 (0.3641)	loss 0.9062 (1.2025)	grad_norm 0.3443 (0.3502)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:09:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:11:49 lr 0.000000	 wd 0.0500	time 0.2628 (0.3083)	loss 1.5432 (1.1890)	grad_norm 0.3425 (0.3498)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:10:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:10:54 lr 0.000000	 wd 0.0500	time 0.2620 (0.2972)	loss 1.4357 (1.1829)	grad_norm 0.3636 (0.3521)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:10:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:10:00 lr 0.000000	 wd 0.0500	time 0.2612 (0.2855)	loss 0.7403 (1.1743)	grad_norm 0.3370 (0.3512)	loss_scale 2048.0000 (2048.0000)	mem 9639MB
[2024-07-31 21:11:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:09:20 lr 0.000000	 wd 0.0500	time 0.2487 (0.2800)	loss 0.7952 (1.1792)	grad_norm 0.3399 (0.3530)	loss_scale 4096.0000 (2432.2555)	mem 9639MB
[2024-07-31 21:11:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:08:44 lr 0.000000	 wd 0.0500	time 0.2473 (0.2758)	loss 0.8887 (1.1718)	grad_norm 0.3696 (0.3538)	loss_scale 4096.0000 (2709.0849)	mem 9639MB
[2024-07-31 21:11:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:08:10 lr 0.000000	 wd 0.0500	time 0.2735 (0.2722)	loss 1.2837 (1.1688)	grad_norm 0.3622 (0.3536)	loss_scale 4096.0000 (2906.9330)	mem 9639MB
[2024-07-31 21:12:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:07:39 lr 0.000000	 wd 0.0500	time 0.2271 (0.2701)	loss 0.8414 (1.1692)	grad_norm 0.3431 (0.3535)	loss_scale 4096.0000 (3055.3808)	mem 9639MB
[2024-07-31 21:12:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:07:09 lr 0.000000	 wd 0.0500	time 0.2335 (0.2680)	loss 0.8552 (1.1656)	grad_norm 0.3570 (0.3531)	loss_scale 4096.0000 (3170.8768)	mem 9639MB
[2024-07-31 21:13:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:06:39 lr 0.000000	 wd 0.0500	time 0.2254 (0.2660)	loss 1.0083 (1.1672)	grad_norm 0.3442 (0.3550)	loss_scale 4096.0000 (3263.2967)	mem 9639MB
[2024-07-31 21:13:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:06:12 lr 0.000000	 wd 0.0500	time 0.2350 (0.2658)	loss 1.2667 (1.1674)	grad_norm 0.3538 (0.3547)	loss_scale 4096.0000 (3338.9282)	mem 9639MB
[2024-07-31 21:14:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:05:44 lr 0.000000	 wd 0.0500	time 0.2154 (0.2649)	loss 1.4352 (1.1674)	grad_norm 0.3963 (0.3544)	loss_scale 4096.0000 (3401.9650)	mem 9639MB
[2024-07-31 21:14:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:05:17 lr 0.000000	 wd 0.0500	time 0.2528 (0.2638)	loss 1.5115 (1.1687)	grad_norm 0.3551 (0.3541)	loss_scale 4096.0000 (3455.3113)	mem 9639MB
[2024-07-31 21:14:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:04:49 lr 0.000000	 wd 0.0500	time 0.2651 (0.2630)	loss 0.9531 (1.1681)	grad_norm 0.3438 (0.3542)	loss_scale 4096.0000 (3501.0421)	mem 9639MB
[2024-07-31 21:15:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:04:23 lr 0.000000	 wd 0.0500	time 0.2092 (0.2627)	loss 1.1882 (1.1666)	grad_norm 0.3467 (0.3537)	loss_scale 4096.0000 (3540.6795)	mem 9639MB
[2024-07-31 21:15:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:03:56 lr 0.000000	 wd 0.0500	time 0.2168 (0.2620)	loss 1.3925 (1.1694)	grad_norm 0.3715 (0.3535)	loss_scale 4096.0000 (3575.3654)	mem 9639MB
[2024-07-31 21:16:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:03:29 lr 0.000000	 wd 0.0500	time 0.2302 (0.2615)	loss 1.2458 (1.1688)	grad_norm 0.3588 (0.3536)	loss_scale 4096.0000 (3605.9730)	mem 9639MB
[2024-07-31 21:16:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:03 lr 0.000000	 wd 0.0500	time 0.2638 (0.2615)	loss 0.7984 (1.1684)	grad_norm 0.3484 (0.3537)	loss_scale 4096.0000 (3633.1816)	mem 9639MB
[2024-07-31 21:17:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:02:37 lr 0.000000	 wd 0.0500	time 0.2674 (0.2613)	loss 1.1034 (1.1660)	grad_norm 0.3453 (0.3534)	loss_scale 4096.0000 (3657.5276)	mem 9639MB
[2024-07-31 21:17:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:11 lr 0.000000	 wd 0.0500	time 0.2470 (0.2618)	loss 1.1156 (1.1661)	grad_norm 0.4698 (0.3534)	loss_scale 4096.0000 (3679.4403)	mem 9639MB
[2024-07-31 21:17:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:45 lr 0.000000	 wd 0.0500	time 0.2641 (0.2631)	loss 0.9768 (1.1660)	grad_norm 0.3577 (0.3540)	loss_scale 4096.0000 (3699.2670)	mem 9639MB
[2024-07-31 21:18:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:19 lr 0.000000	 wd 0.0500	time 0.2334 (0.2629)	loss 1.2770 (1.1651)	grad_norm 0.3557 (0.3537)	loss_scale 4096.0000 (3717.2921)	mem 9639MB
[2024-07-31 21:18:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:53 lr 0.000000	 wd 0.0500	time 0.2603 (0.2628)	loss 1.2395 (1.1650)	grad_norm 0.4326 (0.3537)	loss_scale 4096.0000 (3733.7505)	mem 9639MB
[2024-07-31 21:19:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:26 lr 0.000000	 wd 0.0500	time 0.2665 (0.2626)	loss 1.5129 (1.1637)	grad_norm 0.3311 (0.3539)	loss_scale 4096.0000 (3748.8380)	mem 9639MB
[2024-07-31 21:19:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2160 (0.2617)	loss 1.0790 (1.1629)	grad_norm 0.3323 (0.3537)	loss_scale 4096.0000 (3762.7189)	mem 9639MB
[2024-07-31 21:19:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 28 training takes 0:10:59
[2024-07-31 21:19:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 10.333 (10.333)	Loss 0.4946 (0.4946)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 9639MB
[2024-07-31 21:20:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.070 Acc@5 97.868
[2024-07-31 21:20:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 21:20:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.13%
[2024-07-31 21:20:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:13:29 lr 0.000000	 wd 0.0500	time 11.8344 (11.8344)	loss 1.2454 (1.2454)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:20:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:16:33 lr 0.000000	 wd 0.0500	time 0.2529 (0.4137)	loss 1.4142 (1.1748)	grad_norm 0.3525 (0.3520)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:21:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:12:46 lr 0.000000	 wd 0.0500	time 0.2362 (0.3331)	loss 0.8206 (1.1726)	grad_norm 0.3723 (0.3565)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:21:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:11:13 lr 0.000000	 wd 0.0500	time 0.2393 (0.3061)	loss 0.8808 (1.1692)	grad_norm 0.3580 (0.3545)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:22:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:10:14 lr 0.000000	 wd 0.0500	time 0.2275 (0.2921)	loss 1.1284 (1.1629)	grad_norm 0.3514 (0.3539)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:22:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:09:30 lr 0.000000	 wd 0.0500	time 0.2215 (0.2848)	loss 1.3051 (1.1583)	grad_norm 0.3602 (0.3536)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:23:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:08:53 lr 0.000000	 wd 0.0500	time 0.2186 (0.2805)	loss 0.7343 (1.1615)	grad_norm 0.3490 (0.3618)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:23:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:08:17 lr 0.000000	 wd 0.0500	time 0.2230 (0.2759)	loss 1.1290 (1.1622)	grad_norm 0.3559 (0.3602)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:23:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:07:44 lr 0.000000	 wd 0.0500	time 0.2245 (0.2726)	loss 0.7677 (1.1631)	grad_norm 0.3378 (0.3595)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:24:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:07:12 lr 0.000000	 wd 0.0500	time 0.2447 (0.2701)	loss 1.4899 (1.1645)	grad_norm 0.3594 (0.3586)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:24:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:06:42 lr 0.000000	 wd 0.0500	time 0.2293 (0.2683)	loss 1.4490 (1.1652)	grad_norm 0.3556 (0.3589)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:25:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:06:14 lr 0.000000	 wd 0.0500	time 0.2350 (0.2668)	loss 1.2387 (1.1651)	grad_norm 0.3383 (0.3581)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:25:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:05:46 lr 0.000000	 wd 0.0500	time 0.2442 (0.2658)	loss 1.0229 (1.1628)	grad_norm 0.3304 (0.3591)	loss_scale 4096.0000 (4096.0000)	mem 9639MB
[2024-07-31 21:26:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:05:19 lr 0.000000	 wd 0.0500	time 0.2450 (0.2660)	loss 1.1405 (1.1662)	grad_norm 0.3741 (nan)	loss_scale 2048.0000 (4061.3682)	mem 9639MB
[2024-07-31 21:26:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:04:52 lr 0.000000	 wd 0.0500	time 0.2687 (0.2654)	loss 1.1187 (1.1613)	grad_norm 0.3683 (nan)	loss_scale 2048.0000 (3917.6588)	mem 9639MB
[2024-07-31 21:26:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:04:25 lr 0.000000	 wd 0.0500	time 0.2678 (0.2648)	loss 1.0287 (1.1615)	grad_norm 0.3463 (nan)	loss_scale 2048.0000 (3793.0979)	mem 9639MB
[2024-07-31 21:27:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:03:58 lr 0.000000	 wd 0.0500	time 0.2346 (0.2644)	loss 0.8196 (1.1636)	grad_norm 0.3403 (nan)	loss_scale 2048.0000 (3684.0974)	mem 9639MB
[2024-07-31 21:27:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:03:31 lr 0.000000	 wd 0.0500	time 0.2460 (0.2641)	loss 1.3592 (1.1662)	grad_norm 0.3509 (nan)	loss_scale 2048.0000 (3587.9130)	mem 9639MB
[2024-07-31 21:28:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:04 lr 0.000000	 wd 0.0500	time 0.2558 (0.2635)	loss 1.1802 (1.1646)	grad_norm 0.3266 (nan)	loss_scale 2048.0000 (3502.4098)	mem 9639MB
[2024-07-31 21:28:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:02:38 lr 0.000000	 wd 0.0500	time 0.2439 (0.2640)	loss 0.7610 (1.1649)	grad_norm 0.3602 (nan)	loss_scale 2048.0000 (3425.9022)	mem 9639MB
[2024-07-31 21:29:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:12 lr 0.000000	 wd 0.0500	time 0.2545 (0.2641)	loss 0.9072 (1.1667)	grad_norm 0.3561 (nan)	loss_scale 2048.0000 (3357.0415)	mem 9639MB
[2024-07-31 21:29:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:46 lr 0.000000	 wd 0.0500	time 0.2501 (0.2637)	loss 0.7614 (1.1676)	grad_norm 0.3196 (nan)	loss_scale 2048.0000 (3294.7358)	mem 9639MB
[2024-07-31 21:29:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:19 lr 0.000000	 wd 0.0500	time 0.2500 (0.2636)	loss 0.8366 (1.1682)	grad_norm 0.3583 (nan)	loss_scale 2048.0000 (3238.0918)	mem 9639MB
[2024-07-31 21:30:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:53 lr 0.000000	 wd 0.0500	time 0.2505 (0.2634)	loss 0.8562 (1.1653)	grad_norm 0.3579 (nan)	loss_scale 2048.0000 (3186.3711)	mem 9639MB
[2024-07-31 21:30:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:26 lr 0.000000	 wd 0.0500	time 0.2198 (0.2631)	loss 1.2641 (1.1653)	grad_norm 0.3551 (nan)	loss_scale 2048.0000 (3138.9588)	mem 9639MB
[2024-07-31 21:31:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2143 (0.2620)	loss 1.3244 (1.1646)	grad_norm 0.3374 (nan)	loss_scale 2048.0000 (3095.3379)	mem 9639MB
[2024-07-31 21:31:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 249): INFO EPOCH 29 training takes 0:10:59
[2024-07-31 21:31:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_29.pth saving......
[2024-07-31 21:31:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_29.pth saved !!!
[2024-07-31 21:31:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 289): INFO Test: [0/98]	Time 11.886 (11.886)	Loss 0.4846 (0.4846)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 9639MB
[2024-07-31 21:31:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 296): INFO  * Acc@1 86.112 Acc@5 97.892
[2024-07-31 21:31:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 21:31:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 182): INFO Max accuracy: 86.13%
[2024-07-31 21:31:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2] (main.py 189): INFO Training time 5:41:01
