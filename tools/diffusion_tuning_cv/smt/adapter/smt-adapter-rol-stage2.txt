[2024-08-01 16:50:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/config.json
[2024-08-01 16:50:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: sequence_stage2
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_sequence_stage2
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-01 16:50:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_sequence_stage2.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_sequence_stage2", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-01 16:50:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2
[2024-08-01 16:50:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-01 16:50:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 113): INFO number of params: 2979304
[2024-08-01 16:50:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2, ignoring auto resume
[2024-08-01 16:50:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth for fine-tuning......
[2024-08-01 16:50:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-01 16:50:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage1/diffusion_ft_adapter_smt_l_sequence_stage1/ckpt_epoch_best.pth'
[2024-08-01 16:50:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 15.506 (15.506)	Loss 0.4829 (0.4829)	Acc@1 93.359 (93.359)	Acc@5 98.438 (98.438)	Mem 2334MB
[2024-08-01 16:51:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.790 Acc@5 97.758
[2024-08-01 16:51:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 16:51:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 168): INFO Start training
[2024-08-01 16:51:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:34:11 lr 0.000000	 wd 0.0500	time 12.3307 (12.3307)	loss 1.5285 (1.5285)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 17011MB
[2024-08-01 16:51:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:17:36 lr 0.000000	 wd 0.0500	time 0.3227 (0.4400)	loss 1.2871 (1.2143)	grad_norm 0.3649 (nan)	loss_scale 16384.0000 (28550.3366)	mem 17011MB
[2024-08-01 16:52:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:40 lr 0.000000	 wd 0.0500	time 0.3292 (0.3826)	loss 1.0258 (1.1993)	grad_norm 0.3595 (nan)	loss_scale 16384.0000 (22497.4328)	mem 17011MB
[2024-08-01 16:53:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:13:15 lr 0.000000	 wd 0.0500	time 0.3347 (0.3612)	loss 0.9094 (1.1622)	grad_norm 0.3763 (nan)	loss_scale 16384.0000 (20466.3920)	mem 17011MB
[2024-08-01 16:53:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:12:20 lr 0.000001	 wd 0.0500	time 0.3039 (0.3525)	loss 0.9717 (1.1694)	grad_norm 0.3714 (nan)	loss_scale 16384.0000 (19448.3392)	mem 17011MB
[2024-08-01 16:54:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:34 lr 0.000001	 wd 0.0500	time 0.3006 (0.3471)	loss 1.1443 (1.1742)	grad_norm 0.3335 (nan)	loss_scale 16384.0000 (18836.6946)	mem 17011MB
[2024-08-01 16:54:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:10:54 lr 0.000001	 wd 0.0500	time 0.3149 (0.3441)	loss 1.3212 (1.1741)	grad_norm 0.3495 (nan)	loss_scale 16384.0000 (18428.5923)	mem 17011MB
[2024-08-01 16:55:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:10:15 lr 0.000001	 wd 0.0500	time 0.3124 (0.3416)	loss 1.2413 (1.1763)	grad_norm 0.3749 (nan)	loss_scale 8192.0000 (17482.4993)	mem 17011MB
[2024-08-01 16:55:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:37 lr 0.000001	 wd 0.0500	time 0.3385 (0.3394)	loss 1.0650 (1.1758)	grad_norm 0.3408 (nan)	loss_scale 4096.0000 (15841.9576)	mem 17011MB
[2024-08-01 16:56:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:09:01 lr 0.000001	 wd 0.0500	time 0.3198 (0.3378)	loss 1.3592 (1.1756)	grad_norm 0.3567 (nan)	loss_scale 4096.0000 (14538.2997)	mem 17011MB
[2024-08-01 16:56:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:08:25 lr 0.000002	 wd 0.0500	time 0.3252 (0.3368)	loss 1.4542 (1.1733)	grad_norm 0.3624 (nan)	loss_scale 4096.0000 (13495.1129)	mem 17011MB
[2024-08-01 16:57:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:07:50 lr 0.000002	 wd 0.0500	time 0.2985 (0.3353)	loss 1.2479 (1.1743)	grad_norm 0.4191 (nan)	loss_scale 4096.0000 (12641.4242)	mem 17011MB
[2024-08-01 16:57:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:07:16 lr 0.000002	 wd 0.0500	time 0.3157 (0.3349)	loss 0.9376 (1.1740)	grad_norm 0.3454 (nan)	loss_scale 4096.0000 (11929.8984)	mem 17011MB
[2024-08-01 16:58:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:42 lr 0.000002	 wd 0.0500	time 0.3279 (0.3346)	loss 1.2348 (1.1757)	grad_norm 0.3574 (nan)	loss_scale 2048.0000 (11182.9301)	mem 17011MB
[2024-08-01 16:59:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:06:08 lr 0.000002	 wd 0.0500	time 0.3743 (0.3345)	loss 1.4503 (1.1775)	grad_norm 0.3522 (nan)	loss_scale 2048.0000 (10530.9008)	mem 17011MB
[2024-08-01 16:59:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:34 lr 0.000002	 wd 0.0500	time 0.3202 (0.3337)	loss 1.1777 (1.1814)	grad_norm 0.3585 (nan)	loss_scale 2048.0000 (9965.7508)	mem 17011MB
[2024-08-01 17:00:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:05:00 lr 0.000003	 wd 0.0500	time 0.2915 (0.3334)	loss 1.2150 (1.1784)	grad_norm 0.3435 (nan)	loss_scale 2048.0000 (9471.2005)	mem 17011MB
[2024-08-01 17:00:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:27 lr 0.000003	 wd 0.0500	time 0.3425 (0.3334)	loss 0.8961 (1.1778)	grad_norm 0.3694 (nan)	loss_scale 2048.0000 (9034.7984)	mem 17011MB
[2024-08-01 17:01:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:53 lr 0.000003	 wd 0.0500	time 0.2985 (0.3330)	loss 1.1250 (1.1780)	grad_norm 0.3475 (nan)	loss_scale 2048.0000 (8646.8584)	mem 17011MB
[2024-08-01 17:01:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:20 lr 0.000003	 wd 0.0500	time 0.2984 (0.3325)	loss 1.4183 (1.1776)	grad_norm 0.3550 (nan)	loss_scale 2048.0000 (8299.7328)	mem 17011MB
[2024-08-01 17:02:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:46 lr 0.000003	 wd 0.0500	time 0.3363 (0.3320)	loss 0.8529 (1.1766)	grad_norm 0.3827 (nan)	loss_scale 2048.0000 (7987.3023)	mem 17011MB
[2024-08-01 17:02:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:13 lr 0.000003	 wd 0.0500	time 0.3284 (0.3320)	loss 1.0719 (1.1754)	grad_norm 0.4141 (nan)	loss_scale 2048.0000 (7704.6130)	mem 17011MB
[2024-08-01 17:03:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:40 lr 0.000004	 wd 0.0500	time 0.3202 (0.3324)	loss 1.5364 (1.1749)	grad_norm 0.3575 (nan)	loss_scale 2048.0000 (7447.6111)	mem 17011MB
[2024-08-01 17:03:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:07 lr 0.000004	 wd 0.0500	time 0.3168 (0.3322)	loss 1.3447 (1.1751)	grad_norm 0.3678 (nan)	loss_scale 2048.0000 (7212.9474)	mem 17011MB
[2024-08-01 17:04:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:33 lr 0.000004	 wd 0.0500	time 0.3099 (0.3319)	loss 1.3145 (1.1750)	grad_norm 0.3533 (nan)	loss_scale 2048.0000 (6997.8309)	mem 17011MB
[2024-08-01 17:05:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2883 (0.3309)	loss 0.8853 (1.1731)	grad_norm 0.3661 (nan)	loss_scale 2048.0000 (6799.9168)	mem 17011MB
[2024-08-01 17:05:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 0 training takes 0:13:50
[2024-08-01 17:05:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_0.pth saving......
[2024-08-01 17:05:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_0.pth saved !!!
[2024-08-01 17:05:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 10.847 (10.847)	Loss 0.5171 (0.5171)	Acc@1 93.359 (93.359)	Acc@5 98.242 (98.242)	Mem 17011MB
[2024-08-01 17:05:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.800 Acc@5 97.750
[2024-08-01 17:05:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 17:05:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.80%
[2024-08-01 17:05:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 17:05:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 17:05:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:44:14 lr 0.000004	 wd 0.0500	time 11.1328 (11.1328)	loss 1.1169 (1.1169)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:06:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:17:05 lr 0.000004	 wd 0.0500	time 0.2952 (0.4269)	loss 1.1883 (1.1547)	grad_norm 0.3642 (0.3612)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:06:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:14:19 lr 0.000004	 wd 0.0500	time 0.3010 (0.3734)	loss 0.7706 (1.1719)	grad_norm 0.3711 (0.3608)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:07:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:13:06 lr 0.000004	 wd 0.0500	time 0.3093 (0.3572)	loss 0.8832 (1.1716)	grad_norm 0.3560 (0.3604)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:07:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:12:12 lr 0.000005	 wd 0.0500	time 0.2960 (0.3483)	loss 0.9808 (1.1867)	grad_norm 0.3584 (0.3629)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:08:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:11:27 lr 0.000005	 wd 0.0500	time 0.2958 (0.3435)	loss 1.4590 (1.1839)	grad_norm 0.3479 (0.3628)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:08:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:10:47 lr 0.000005	 wd 0.0500	time 0.2959 (0.3402)	loss 1.5017 (1.1829)	grad_norm 0.3445 (0.3628)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:09:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:10:08 lr 0.000005	 wd 0.0500	time 0.3024 (0.3378)	loss 0.8782 (1.1796)	grad_norm 0.3429 (0.3642)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:10:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:31 lr 0.000005	 wd 0.0500	time 0.3366 (0.3359)	loss 1.4330 (1.1787)	grad_norm 0.4778 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:10:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:08:55 lr 0.000005	 wd 0.0500	time 0.3099 (0.3344)	loss 1.1841 (1.1776)	grad_norm 0.3761 (0.3645)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:11:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:08:20 lr 0.000006	 wd 0.0500	time 0.2950 (0.3329)	loss 1.3738 (1.1763)	grad_norm 0.3651 (0.3644)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:07:46 lr 0.000006	 wd 0.0500	time 0.3295 (0.3327)	loss 1.4323 (1.1747)	grad_norm 0.3570 (0.3708)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:12:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:07:11 lr 0.000006	 wd 0.0500	time 0.3215 (0.3317)	loss 0.9608 (1.1725)	grad_norm 0.3764 (0.3749)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:12:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:37 lr 0.000006	 wd 0.0500	time 0.2940 (0.3310)	loss 1.5425 (1.1735)	grad_norm 0.3712 (0.3770)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:13:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:06:04 lr 0.000006	 wd 0.0500	time 0.3058 (0.3308)	loss 0.9555 (1.1735)	grad_norm 0.3918 (0.3756)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:13:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:31 lr 0.000006	 wd 0.0500	time 0.3552 (0.3305)	loss 1.4189 (1.1742)	grad_norm 0.3428 (0.3749)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:14:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:58 lr 0.000007	 wd 0.0500	time 0.3430 (0.3313)	loss 1.2977 (1.1758)	grad_norm 0.3622 (0.3744)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:14:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:25 lr 0.000007	 wd 0.0500	time 0.3031 (0.3315)	loss 0.9939 (1.1761)	grad_norm 0.3664 (0.3740)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:15:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:52 lr 0.000007	 wd 0.0500	time 0.3047 (0.3319)	loss 1.7074 (1.1754)	grad_norm 0.3561 (0.3735)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:16:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:19 lr 0.000007	 wd 0.0500	time 0.2933 (0.3316)	loss 1.4468 (1.1766)	grad_norm 0.3616 (0.3728)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:16:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:46 lr 0.000007	 wd 0.0500	time 0.3127 (0.3312)	loss 1.4119 (1.1771)	grad_norm 0.3611 (0.3724)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:17:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:12 lr 0.000007	 wd 0.0500	time 0.2979 (0.3308)	loss 1.5038 (1.1791)	grad_norm 0.3743 (0.3720)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:17:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:39 lr 0.000008	 wd 0.0500	time 0.2969 (0.3305)	loss 1.2377 (1.1796)	grad_norm 0.3422 (0.3715)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:18:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:06 lr 0.000008	 wd 0.0500	time 0.3060 (0.3305)	loss 0.9645 (1.1818)	grad_norm 0.3493 (0.3710)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:18:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:33 lr 0.000008	 wd 0.0500	time 0.3321 (0.3309)	loss 1.2148 (1.1818)	grad_norm 0.3642 (0.3704)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:19:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2865 (0.3301)	loss 0.9132 (1.1819)	grad_norm 0.3565 (0.3704)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:19:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 1 training takes 0:13:48
[2024-08-01 17:19:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.109 (12.109)	Loss 0.5083 (0.5083)	Acc@1 93.164 (93.164)	Acc@5 98.242 (98.242)	Mem 17011MB
[2024-08-01 17:19:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.842 Acc@5 97.766
[2024-08-01 17:19:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.8%
[2024-08-01 17:19:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.84%
[2024-08-01 17:19:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 17:19:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 17:20:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:54:53 lr 0.000008	 wd 0.0500	time 11.3883 (11.3883)	loss 1.2499 (1.2499)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:20:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:17:13 lr 0.000008	 wd 0.0500	time 0.3439 (0.4301)	loss 1.4230 (1.1847)	grad_norm 0.3495 (0.3921)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:21:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:14:26 lr 0.000008	 wd 0.0500	time 0.2933 (0.3765)	loss 1.4041 (1.1755)	grad_norm 0.3490 (0.3765)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:21:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:13:17 lr 0.000008	 wd 0.0500	time 0.3486 (0.3624)	loss 1.1864 (1.1661)	grad_norm 0.3581 (0.4070)	loss_scale 4096.0000 (2701.1827)	mem 17011MB
[2024-08-01 17:22:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:12:31 lr 0.000009	 wd 0.0500	time 0.3040 (0.3575)	loss 1.4282 (1.1691)	grad_norm 0.3416 (0.3936)	loss_scale 4096.0000 (3049.0175)	mem 17011MB
[2024-08-01 17:22:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:11:42 lr 0.000009	 wd 0.0500	time 0.2952 (0.3508)	loss 1.4355 (1.1648)	grad_norm 0.3647 (0.3872)	loss_scale 4096.0000 (3257.9960)	mem 17011MB
[2024-08-01 17:23:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:10:59 lr 0.000009	 wd 0.0500	time 0.2923 (0.3465)	loss 1.2816 (1.1659)	grad_norm 0.3459 (0.3856)	loss_scale 4096.0000 (3397.4309)	mem 17011MB
[2024-08-01 17:23:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:10:19 lr 0.000009	 wd 0.0500	time 0.3028 (0.3438)	loss 1.3286 (1.1708)	grad_norm 0.3921 (0.3818)	loss_scale 4096.0000 (3497.0842)	mem 17011MB
[2024-08-01 17:24:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:40 lr 0.000009	 wd 0.0500	time 0.3430 (0.3413)	loss 1.3929 (1.1737)	grad_norm 0.3466 (0.3816)	loss_scale 4096.0000 (3571.8552)	mem 17011MB
[2024-08-01 17:24:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:09:04 lr 0.000009	 wd 0.0500	time 0.3182 (0.3400)	loss 0.9429 (1.1787)	grad_norm 0.3603 (0.3812)	loss_scale 4096.0000 (3630.0289)	mem 17011MB
[2024-08-01 17:25:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:08:30 lr 0.000010	 wd 0.0500	time 0.3483 (0.3396)	loss 0.8749 (1.1759)	grad_norm 0.3688 (0.3797)	loss_scale 4096.0000 (3676.5794)	mem 17011MB
[2024-08-01 17:26:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:54 lr 0.000010	 wd 0.0500	time 0.2975 (0.3384)	loss 1.3808 (1.1733)	grad_norm 0.3636 (0.3783)	loss_scale 4096.0000 (3714.6739)	mem 17011MB
[2024-08-01 17:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:07:18 lr 0.000010	 wd 0.0500	time 0.3297 (0.3370)	loss 0.8994 (1.1737)	grad_norm 0.5120 (0.3777)	loss_scale 4096.0000 (3746.4246)	mem 17011MB
[2024-08-01 17:27:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:43 lr 0.000010	 wd 0.0500	time 0.3101 (0.3360)	loss 1.4130 (1.1748)	grad_norm 0.3736 (0.3766)	loss_scale 4096.0000 (3773.2944)	mem 17011MB
[2024-08-01 17:27:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:06:09 lr 0.000010	 wd 0.0500	time 0.2976 (0.3350)	loss 1.2226 (1.1764)	grad_norm 0.3546 (0.3752)	loss_scale 4096.0000 (3796.3283)	mem 17011MB
[2024-08-01 17:28:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:35 lr 0.000010	 wd 0.0500	time 0.3416 (0.3344)	loss 1.3346 (1.1769)	grad_norm 0.3526 (0.3748)	loss_scale 4096.0000 (3816.2931)	mem 17011MB
[2024-08-01 17:28:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:05:01 lr 0.000011	 wd 0.0500	time 0.2974 (0.3347)	loss 0.9727 (1.1773)	grad_norm 0.3422 (nan)	loss_scale 2048.0000 (3762.1287)	mem 17011MB
[2024-08-01 17:29:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:28 lr 0.000011	 wd 0.0500	time 0.3030 (0.3345)	loss 1.3017 (1.1777)	grad_norm 0.3405 (nan)	loss_scale 2048.0000 (3661.3568)	mem 17011MB
[2024-08-01 17:29:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:54 lr 0.000011	 wd 0.0500	time 0.3137 (0.3341)	loss 1.0712 (1.1783)	grad_norm 0.3560 (nan)	loss_scale 2048.0000 (3571.7757)	mem 17011MB
[2024-08-01 17:30:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:20 lr 0.000011	 wd 0.0500	time 0.3387 (0.3339)	loss 1.3622 (1.1781)	grad_norm 0.3783 (nan)	loss_scale 2048.0000 (3491.6191)	mem 17011MB
[2024-08-01 17:30:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:47 lr 0.000011	 wd 0.0500	time 0.3393 (0.3335)	loss 1.1378 (1.1782)	grad_norm 0.3456 (nan)	loss_scale 2048.0000 (3419.4743)	mem 17011MB
[2024-08-01 17:31:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:13 lr 0.000011	 wd 0.0500	time 0.3133 (0.3331)	loss 1.0180 (1.1766)	grad_norm 0.3649 (nan)	loss_scale 2048.0000 (3354.1970)	mem 17011MB
[2024-08-01 17:32:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:40 lr 0.000012	 wd 0.0500	time 0.3298 (0.3329)	loss 1.3964 (1.1763)	grad_norm 0.3661 (nan)	loss_scale 2048.0000 (3294.8514)	mem 17011MB
[2024-08-01 17:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:07 lr 0.000012	 wd 0.0500	time 0.3214 (0.3326)	loss 1.2214 (1.1765)	grad_norm 0.3449 (nan)	loss_scale 2048.0000 (3240.6641)	mem 17011MB
[2024-08-01 17:33:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:33 lr 0.000012	 wd 0.0500	time 0.2994 (0.3326)	loss 0.7905 (1.1772)	grad_norm 0.3561 (nan)	loss_scale 2048.0000 (3190.9904)	mem 17011MB
[2024-08-01 17:33:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.3165 (0.3318)	loss 1.2062 (1.1775)	grad_norm 0.3394 (nan)	loss_scale 2048.0000 (3145.2891)	mem 17011MB
[2024-08-01 17:33:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 2 training takes 0:13:52
[2024-08-01 17:33:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.652 (11.652)	Loss 0.4900 (0.4900)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 17:34:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.890 Acc@5 97.784
[2024-08-01 17:34:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 17:34:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.89%
[2024-08-01 17:34:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 17:34:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 17:34:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:11:08 lr 0.000012	 wd 0.0500	time 10.3393 (10.3393)	loss 0.7518 (0.7518)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:34:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:17:36 lr 0.000012	 wd 0.0500	time 0.2981 (0.4399)	loss 1.2380 (1.2130)	grad_norm 0.3524 (0.3570)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:35:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:14:49 lr 0.000012	 wd 0.0500	time 0.3150 (0.3866)	loss 1.5159 (1.1874)	grad_norm 0.3628 (0.3645)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:36:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:13:35 lr 0.000012	 wd 0.0500	time 0.3078 (0.3701)	loss 1.4358 (1.1842)	grad_norm 0.3535 (0.3615)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:36:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:12:33 lr 0.000013	 wd 0.0500	time 0.3112 (0.3585)	loss 1.4203 (1.1764)	grad_norm 0.3576 (0.3623)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:37:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:11:43 lr 0.000013	 wd 0.0500	time 0.2955 (0.3512)	loss 1.0033 (1.1765)	grad_norm 0.3458 (0.3626)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:37:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:10:59 lr 0.000013	 wd 0.0500	time 0.3058 (0.3469)	loss 1.0576 (1.1712)	grad_norm 0.3625 (0.3621)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:38:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:10:20 lr 0.000013	 wd 0.0500	time 0.3308 (0.3446)	loss 1.5141 (1.1688)	grad_norm 0.3508 (0.3617)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:38:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:41 lr 0.000013	 wd 0.0500	time 0.2991 (0.3419)	loss 0.7764 (1.1645)	grad_norm 0.3602 (0.3613)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:39:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:09:05 lr 0.000013	 wd 0.0500	time 0.3093 (0.3405)	loss 1.5653 (1.1667)	grad_norm 0.3826 (0.3604)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:39:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:08:29 lr 0.000014	 wd 0.0500	time 0.3280 (0.3390)	loss 1.2975 (1.1674)	grad_norm 0.3236 (0.3633)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:53 lr 0.000014	 wd 0.0500	time 0.3143 (0.3375)	loss 0.9992 (1.1681)	grad_norm 0.3715 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:40:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:07:17 lr 0.000014	 wd 0.0500	time 0.2961 (0.3363)	loss 1.1571 (1.1677)	grad_norm 0.3560 (0.3639)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:41:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:42 lr 0.000014	 wd 0.0500	time 0.3024 (0.3348)	loss 1.3257 (1.1670)	grad_norm 0.3392 (0.3637)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:42:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:06:07 lr 0.000014	 wd 0.0500	time 0.3177 (0.3339)	loss 1.1352 (1.1681)	grad_norm 0.3376 (0.3632)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:42:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:33 lr 0.000014	 wd 0.0500	time 0.2931 (0.3331)	loss 1.4151 (1.1664)	grad_norm 0.3543 (0.3625)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:43:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:04:59 lr 0.000015	 wd 0.0500	time 0.3016 (0.3324)	loss 0.7852 (1.1649)	grad_norm 0.3529 (0.3627)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:43:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:26 lr 0.000015	 wd 0.0500	time 0.3335 (0.3324)	loss 0.9910 (1.1673)	grad_norm 0.3311 (0.3624)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:44:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:53 lr 0.000015	 wd 0.0500	time 0.2979 (0.3328)	loss 1.1480 (1.1684)	grad_norm 0.3698 (0.3631)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:44:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:20 lr 0.000015	 wd 0.0500	time 0.3190 (0.3332)	loss 1.2621 (1.1676)	grad_norm 0.3438 (0.3629)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:45:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:47 lr 0.000015	 wd 0.0500	time 0.3344 (0.3333)	loss 1.3288 (1.1681)	grad_norm 0.3426 (0.3627)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:45:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:13 lr 0.000015	 wd 0.0500	time 0.2932 (0.3332)	loss 0.9767 (1.1685)	grad_norm 0.3887 (0.3635)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:46:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:40 lr 0.000016	 wd 0.0500	time 0.3396 (0.3328)	loss 0.8134 (1.1677)	grad_norm 0.3464 (0.3640)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:46:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:07 lr 0.000016	 wd 0.0500	time 0.2963 (0.3324)	loss 0.9158 (1.1679)	grad_norm 0.3645 (0.3636)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:47:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:33 lr 0.000016	 wd 0.0500	time 0.3254 (0.3322)	loss 1.4131 (1.1696)	grad_norm 0.3679 (0.3637)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:48:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2890 (0.3314)	loss 1.2994 (1.1693)	grad_norm 0.3559 (0.3635)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:48:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 3 training takes 0:13:51
[2024-08-01 17:48:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.118 (12.118)	Loss 0.5073 (0.5073)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 17:48:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.918 Acc@5 97.800
[2024-08-01 17:48:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 17:48:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.92%
[2024-08-01 17:48:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 17:48:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 17:48:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:37:58 lr 0.000016	 wd 0.0500	time 10.9826 (10.9826)	loss 1.2681 (1.2681)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:49:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:17:33 lr 0.000016	 wd 0.0500	time 0.3302 (0.4387)	loss 0.9512 (1.1833)	grad_norm 0.4140 (0.3606)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:49:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:14:49 lr 0.000016	 wd 0.0500	time 0.3066 (0.3862)	loss 0.8650 (1.1716)	grad_norm 0.3585 (0.3654)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:50:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:13:26 lr 0.000016	 wd 0.0500	time 0.3469 (0.3660)	loss 0.8046 (1.1684)	grad_norm 1.1794 (0.3688)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:50:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:12:27 lr 0.000017	 wd 0.0500	time 0.3416 (0.3554)	loss 1.3045 (1.1634)	grad_norm 0.3522 (0.3672)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:51:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:11:38 lr 0.000017	 wd 0.0500	time 0.3334 (0.3487)	loss 1.2540 (1.1641)	grad_norm 0.3477 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 17:52:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:10:54 lr 0.000017	 wd 0.0500	time 0.3295 (0.3441)	loss 1.1762 (1.1688)	grad_norm 0.3713 (0.3679)	loss_scale 4096.0000 (2252.4592)	mem 17011MB
[2024-08-01 17:52:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:10:14 lr 0.000017	 wd 0.0500	time 0.2914 (0.3407)	loss 0.8405 (1.1691)	grad_norm 0.3509 (0.3669)	loss_scale 4096.0000 (2515.4465)	mem 17011MB
[2024-08-01 17:53:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:09:35 lr 0.000017	 wd 0.0500	time 0.2909 (0.3383)	loss 0.7762 (1.1716)	grad_norm 0.3741 (0.3658)	loss_scale 4096.0000 (2712.7690)	mem 17011MB
[2024-08-01 17:53:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:09:01 lr 0.000017	 wd 0.0500	time 0.3173 (0.3378)	loss 0.8308 (1.1728)	grad_norm 0.3463 (0.3657)	loss_scale 4096.0000 (2866.2908)	mem 17011MB
[2024-08-01 17:54:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:08:27 lr 0.000018	 wd 0.0500	time 0.2978 (0.3377)	loss 1.5832 (1.1728)	grad_norm 0.3419 (0.3659)	loss_scale 4096.0000 (2989.1389)	mem 17011MB
[2024-08-01 17:54:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:07:52 lr 0.000018	 wd 0.0500	time 0.3651 (0.3368)	loss 1.4665 (1.1738)	grad_norm 0.4248 (0.3669)	loss_scale 4096.0000 (3089.6712)	mem 17011MB
[2024-08-01 17:55:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:07:17 lr 0.000018	 wd 0.0500	time 0.3225 (0.3357)	loss 1.2825 (1.1726)	grad_norm 0.3607 (0.3663)	loss_scale 4096.0000 (3173.4621)	mem 17011MB
[2024-08-01 17:55:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:42 lr 0.000018	 wd 0.0500	time 0.3372 (0.3347)	loss 1.0282 (1.1733)	grad_norm 0.3682 (0.3656)	loss_scale 4096.0000 (3244.3720)	mem 17011MB
[2024-08-01 17:56:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:06:08 lr 0.000018	 wd 0.0500	time 0.3074 (0.3340)	loss 1.4231 (1.1732)	grad_norm 0.3617 (0.3650)	loss_scale 4096.0000 (3305.1592)	mem 17011MB
[2024-08-01 17:56:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:33 lr 0.000018	 wd 0.0500	time 0.3137 (0.3332)	loss 1.2141 (1.1736)	grad_norm 0.3477 (0.3669)	loss_scale 4096.0000 (3357.8468)	mem 17011MB
[2024-08-01 17:57:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:05:00 lr 0.000019	 wd 0.0500	time 0.3248 (0.3327)	loss 1.4720 (1.1722)	grad_norm 0.3611 (0.3672)	loss_scale 4096.0000 (3403.9525)	mem 17011MB
[2024-08-01 17:58:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:26 lr 0.000019	 wd 0.0500	time 0.3192 (0.3329)	loss 1.1778 (1.1728)	grad_norm 0.3595 (0.3670)	loss_scale 4096.0000 (3444.6373)	mem 17011MB
[2024-08-01 17:58:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:53 lr 0.000019	 wd 0.0500	time 0.3096 (0.3332)	loss 1.5174 (1.1749)	grad_norm 0.3543 (0.3674)	loss_scale 4096.0000 (3480.8040)	mem 17011MB
[2024-08-01 17:59:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:20 lr 0.000019	 wd 0.0500	time 0.3362 (0.3334)	loss 1.4018 (1.1752)	grad_norm 0.3577 (0.3669)	loss_scale 4096.0000 (3513.1657)	mem 17011MB
[2024-08-01 17:59:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:47 lr 0.000019	 wd 0.0500	time 0.2965 (0.3332)	loss 0.8617 (1.1730)	grad_norm 0.3690 (0.3665)	loss_scale 4096.0000 (3542.2929)	mem 17011MB
[2024-08-01 18:00:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:13 lr 0.000019	 wd 0.0500	time 0.3071 (0.3330)	loss 1.0116 (1.1718)	grad_norm 0.3810 (0.3664)	loss_scale 4096.0000 (3568.6473)	mem 17011MB
[2024-08-01 18:00:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:40 lr 0.000020	 wd 0.0500	time 0.2957 (0.3326)	loss 0.8883 (1.1714)	grad_norm 0.3331 (0.3662)	loss_scale 4096.0000 (3592.6070)	mem 17011MB
[2024-08-01 18:01:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:07 lr 0.000020	 wd 0.0500	time 0.3043 (0.3324)	loss 0.7968 (1.1707)	grad_norm 0.3866 (0.3663)	loss_scale 4096.0000 (3614.4841)	mem 17011MB
[2024-08-01 18:01:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:33 lr 0.000020	 wd 0.0500	time 0.3009 (0.3327)	loss 0.7909 (1.1706)	grad_norm 0.3414 (0.3662)	loss_scale 4096.0000 (3634.5389)	mem 17011MB
[2024-08-01 18:02:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2883 (0.3319)	loss 1.2423 (1.1712)	grad_norm 0.3898 (nan)	loss_scale 2048.0000 (3626.7861)	mem 17011MB
[2024-08-01 18:02:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 4 training takes 0:13:52
[2024-08-01 18:02:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.190 (12.190)	Loss 0.4954 (0.4954)	Acc@1 93.164 (93.164)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 18:02:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.916 Acc@5 97.802
[2024-08-01 18:02:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 18:02:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.92%
[2024-08-01 18:03:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:39:20 lr 0.000020	 wd 0.0500	time 11.0154 (11.0154)	loss 1.3905 (1.3905)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:03:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:17:17 lr 0.000020	 wd 0.0500	time 0.2853 (0.4319)	loss 0.9888 (1.2004)	grad_norm 0.4105 (0.3585)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:04:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:14:34 lr 0.000020	 wd 0.0500	time 0.3235 (0.3797)	loss 1.3503 (1.1713)	grad_norm 0.3574 (0.3599)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:04:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:13:13 lr 0.000020	 wd 0.0500	time 0.3015 (0.3605)	loss 1.0163 (1.1603)	grad_norm 0.3518 (0.3594)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:12:19 lr 0.000020	 wd 0.0500	time 0.3086 (0.3518)	loss 1.2680 (1.1691)	grad_norm 0.3624 (0.3602)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:05:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:11:33 lr 0.000020	 wd 0.0500	time 0.3016 (0.3464)	loss 1.0047 (1.1678)	grad_norm 0.3576 (0.3597)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:06:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:10:51 lr 0.000020	 wd 0.0500	time 0.2955 (0.3425)	loss 1.0595 (1.1692)	grad_norm 0.3648 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:06:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:10:12 lr 0.000020	 wd 0.0500	time 0.3208 (0.3397)	loss 0.9305 (1.1678)	grad_norm 0.3397 (0.3634)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:07:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:35 lr 0.000020	 wd 0.0500	time 0.2934 (0.3379)	loss 1.5199 (1.1669)	grad_norm 0.3540 (0.3626)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:07:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:08:59 lr 0.000020	 wd 0.0500	time 0.2926 (0.3366)	loss 1.0693 (1.1657)	grad_norm 0.3439 (0.3625)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:08:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:08:23 lr 0.000020	 wd 0.0500	time 0.3145 (0.3354)	loss 1.2569 (1.1640)	grad_norm 0.3326 (0.3633)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:09:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:07:49 lr 0.000020	 wd 0.0500	time 0.3369 (0.3346)	loss 1.1859 (1.1649)	grad_norm 0.3693 (0.3644)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:09:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:07:14 lr 0.000020	 wd 0.0500	time 0.3262 (0.3340)	loss 1.4188 (1.1635)	grad_norm 0.3527 (0.3645)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:10:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:40 lr 0.000020	 wd 0.0500	time 0.3228 (0.3336)	loss 0.9967 (1.1643)	grad_norm 0.3562 (0.3641)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:10:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:06:07 lr 0.000020	 wd 0.0500	time 0.3428 (0.3331)	loss 1.4431 (1.1679)	grad_norm 0.3663 (0.3646)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:11:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:33 lr 0.000020	 wd 0.0500	time 0.3547 (0.3330)	loss 0.8447 (1.1690)	grad_norm 0.3733 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:11:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:04:59 lr 0.000020	 wd 0.0500	time 0.3195 (0.3326)	loss 1.4152 (1.1717)	grad_norm 0.3560 (0.3638)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:12:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:26 lr 0.000020	 wd 0.0500	time 0.3128 (0.3324)	loss 0.9673 (1.1719)	grad_norm 0.3413 (0.3633)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:12:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:53 lr 0.000020	 wd 0.0500	time 0.3331 (0.3323)	loss 0.7690 (1.1698)	grad_norm 0.3707 (0.3632)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:13:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:20 lr 0.000020	 wd 0.0500	time 0.3146 (0.3322)	loss 1.1386 (1.1693)	grad_norm 0.3511 (0.3633)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:13:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:46 lr 0.000020	 wd 0.0500	time 0.3041 (0.3320)	loss 1.5685 (1.1708)	grad_norm 0.3525 (0.3643)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:14:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:13 lr 0.000020	 wd 0.0500	time 0.3109 (0.3321)	loss 1.4793 (1.1703)	grad_norm 0.3666 (0.3650)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:15:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:40 lr 0.000020	 wd 0.0500	time 0.3216 (0.3320)	loss 1.2230 (1.1715)	grad_norm 0.3423 (0.3648)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:15:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:07 lr 0.000020	 wd 0.0500	time 0.3164 (0.3319)	loss 1.1407 (1.1700)	grad_norm 0.3797 (0.3649)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:16:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:33 lr 0.000020	 wd 0.0500	time 0.3114 (0.3318)	loss 0.7876 (1.1695)	grad_norm 0.3655 (0.3656)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:16:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2878 (0.3308)	loss 1.3039 (1.1707)	grad_norm 0.3545 (0.3653)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:16:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 5 training takes 0:13:50
[2024-08-01 18:16:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.075 (12.075)	Loss 0.4944 (0.4944)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 18:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.938 Acc@5 97.840
[2024-08-01 18:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-01 18:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.94%
[2024-08-01 18:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 18:17:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 18:17:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:05:20 lr 0.000020	 wd 0.0500	time 11.6388 (11.6388)	loss 1.2126 (1.2126)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:17:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:17:40 lr 0.000020	 wd 0.0500	time 0.3030 (0.4415)	loss 0.8991 (1.1930)	grad_norm 0.3685 (0.3548)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:18:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:14:39 lr 0.000020	 wd 0.0500	time 0.3286 (0.3819)	loss 0.8665 (1.1830)	grad_norm 0.3301 (0.3628)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:19:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:13:21 lr 0.000020	 wd 0.0500	time 0.3351 (0.3642)	loss 0.9593 (1.1747)	grad_norm 0.3706 (0.3662)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:19:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:12:25 lr 0.000020	 wd 0.0500	time 0.3493 (0.3549)	loss 0.7844 (1.1766)	grad_norm 0.3611 (0.3703)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:20:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:11:37 lr 0.000020	 wd 0.0500	time 0.3343 (0.3486)	loss 1.2064 (1.1768)	grad_norm 0.3441 (0.3673)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:20:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:10:57 lr 0.000020	 wd 0.0500	time 0.3277 (0.3456)	loss 0.8638 (1.1682)	grad_norm 0.3602 (0.3682)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:21:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:10:17 lr 0.000020	 wd 0.0500	time 0.2953 (0.3429)	loss 1.4843 (1.1706)	grad_norm 0.3258 (0.3660)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:21:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:41 lr 0.000020	 wd 0.0500	time 0.3629 (0.3416)	loss 0.7152 (1.1750)	grad_norm 0.3405 (0.3689)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:22:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:09:09 lr 0.000020	 wd 0.0500	time 0.3382 (0.3432)	loss 1.4904 (1.1714)	grad_norm 0.3419 (0.3796)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:22:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:08:32 lr 0.000020	 wd 0.0500	time 0.3323 (0.3412)	loss 1.0431 (1.1712)	grad_norm 0.3566 (0.3779)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:23:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:56 lr 0.000020	 wd 0.0500	time 0.3612 (0.3400)	loss 0.8560 (1.1678)	grad_norm 0.3628 (0.3770)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:24:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:07:21 lr 0.000020	 wd 0.0500	time 0.3080 (0.3389)	loss 1.2414 (1.1648)	grad_norm 0.3654 (0.3816)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:24:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:46 lr 0.000020	 wd 0.0500	time 0.2900 (0.3383)	loss 1.1013 (1.1635)	grad_norm 0.3496 (0.3813)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:25:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:06:11 lr 0.000020	 wd 0.0500	time 0.3321 (0.3375)	loss 0.9442 (1.1637)	grad_norm 0.3535 (0.3807)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:25:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:37 lr 0.000020	 wd 0.0500	time 0.3093 (0.3370)	loss 0.7349 (1.1646)	grad_norm 0.3444 (0.3791)	loss_scale 4096.0000 (2097.1193)	mem 17011MB
[2024-08-01 18:26:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:05:03 lr 0.000020	 wd 0.0500	time 0.3182 (0.3363)	loss 1.4554 (1.1620)	grad_norm 0.3358 (0.3786)	loss_scale 4096.0000 (2221.9713)	mem 17011MB
[2024-08-01 18:26:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:29 lr 0.000020	 wd 0.0500	time 0.3526 (0.3359)	loss 1.3811 (1.1613)	grad_norm 0.3430 (0.3778)	loss_scale 4096.0000 (2332.1434)	mem 17011MB
[2024-08-01 18:27:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:55 lr 0.000020	 wd 0.0500	time 0.3338 (0.3353)	loss 1.2132 (1.1618)	grad_norm 0.3513 (0.3773)	loss_scale 4096.0000 (2430.0811)	mem 17011MB
[2024-08-01 18:27:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:21 lr 0.000020	 wd 0.0500	time 0.3090 (0.3350)	loss 1.3479 (1.1630)	grad_norm 0.3470 (0.3767)	loss_scale 4096.0000 (2517.7149)	mem 17011MB
[2024-08-01 18:28:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:47 lr 0.000020	 wd 0.0500	time 0.3204 (0.3344)	loss 0.9888 (1.1638)	grad_norm 0.3548 (0.3761)	loss_scale 4096.0000 (2596.5897)	mem 17011MB
[2024-08-01 18:28:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:14 lr 0.000020	 wd 0.0500	time 0.3284 (0.3342)	loss 1.0086 (1.1639)	grad_norm 0.3594 (0.3757)	loss_scale 4096.0000 (2667.9562)	mem 17011MB
[2024-08-01 18:29:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:40 lr 0.000020	 wd 0.0500	time 0.2902 (0.3340)	loss 0.8113 (1.1633)	grad_norm 0.3852 (0.3769)	loss_scale 4096.0000 (2732.8378)	mem 17011MB
[2024-08-01 18:30:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:07 lr 0.000020	 wd 0.0500	time 0.3100 (0.3338)	loss 1.0366 (1.1658)	grad_norm 0.3726 (0.3765)	loss_scale 4096.0000 (2792.0800)	mem 17011MB
[2024-08-01 18:30:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:34 lr 0.000020	 wd 0.0500	time 0.2955 (0.3338)	loss 1.3940 (1.1650)	grad_norm 0.3655 (0.3759)	loss_scale 4096.0000 (2846.3873)	mem 17011MB
[2024-08-01 18:31:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2888 (0.3327)	loss 1.3556 (1.1647)	grad_norm 0.3475 (0.3759)	loss_scale 4096.0000 (2896.3519)	mem 17011MB
[2024-08-01 18:31:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 6 training takes 0:13:54
[2024-08-01 18:31:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.774 (11.774)	Loss 0.5073 (0.5073)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 18:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.964 Acc@5 97.830
[2024-08-01 18:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 18:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.96%
[2024-08-01 18:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 18:31:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 18:31:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:16:09 lr 0.000020	 wd 0.0500	time 11.8983 (11.8983)	loss 0.9193 (0.9193)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:32:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:17:27 lr 0.000020	 wd 0.0500	time 0.3638 (0.4362)	loss 1.2035 (1.1491)	grad_norm 0.3501 (0.3595)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:32:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:38 lr 0.000020	 wd 0.0500	time 0.2923 (0.3816)	loss 1.1163 (1.1705)	grad_norm 0.3597 (0.3593)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:33:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:13:17 lr 0.000020	 wd 0.0500	time 0.3235 (0.3620)	loss 1.2984 (1.1654)	grad_norm 0.3518 (0.3595)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:33:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:12:24 lr 0.000020	 wd 0.0500	time 0.3019 (0.3540)	loss 0.8274 (1.1706)	grad_norm 0.3801 (0.3613)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:34:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:36 lr 0.000020	 wd 0.0500	time 0.3144 (0.3479)	loss 1.3421 (1.1675)	grad_norm 0.3408 (0.3617)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:35:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:10:55 lr 0.000020	 wd 0.0500	time 0.3127 (0.3448)	loss 1.3001 (1.1717)	grad_norm 0.3643 (0.3632)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:35:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:10:16 lr 0.000020	 wd 0.0500	time 0.3047 (0.3421)	loss 0.9713 (1.1664)	grad_norm 0.3460 (0.3630)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:36:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:38 lr 0.000020	 wd 0.0500	time 0.3301 (0.3398)	loss 0.8844 (1.1639)	grad_norm 0.3909 (0.3622)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:36:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:09:02 lr 0.000020	 wd 0.0500	time 0.3562 (0.3383)	loss 1.4382 (1.1656)	grad_norm 0.3516 (0.3622)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:37:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:26 lr 0.000020	 wd 0.0500	time 0.3252 (0.3370)	loss 0.8113 (1.1657)	grad_norm 0.3745 (0.3634)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:37:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:51 lr 0.000020	 wd 0.0500	time 0.3358 (0.3360)	loss 1.3668 (1.1649)	grad_norm 0.3542 (0.3629)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:38:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:07:18 lr 0.000020	 wd 0.0500	time 0.3660 (0.3371)	loss 1.3944 (1.1617)	grad_norm 0.3570 (0.3642)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:38:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:44 lr 0.000020	 wd 0.0500	time 0.3253 (0.3362)	loss 1.3722 (1.1628)	grad_norm 0.3329 (0.3646)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:39:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:06:10 lr 0.000019	 wd 0.0500	time 0.3210 (0.3358)	loss 1.2818 (1.1626)	grad_norm 0.3641 (0.3642)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:40:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:35 lr 0.000019	 wd 0.0500	time 0.3245 (0.3352)	loss 0.8311 (1.1631)	grad_norm 0.3508 (0.3651)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:40:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:05:01 lr 0.000019	 wd 0.0500	time 0.3035 (0.3346)	loss 1.1093 (1.1636)	grad_norm 0.4028 (0.3647)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:41:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:28 lr 0.000019	 wd 0.0500	time 0.2972 (0.3344)	loss 1.3308 (1.1639)	grad_norm 0.3348 (0.3645)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:41:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:54 lr 0.000019	 wd 0.0500	time 0.3420 (0.3343)	loss 0.7549 (1.1662)	grad_norm 0.3449 (0.3646)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:42:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:21 lr 0.000019	 wd 0.0500	time 0.3642 (0.3341)	loss 1.1218 (1.1665)	grad_norm 0.3649 (0.3652)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:42:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:47 lr 0.000019	 wd 0.0500	time 0.3377 (0.3340)	loss 1.1554 (1.1672)	grad_norm 0.3525 (0.3654)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:43:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:14 lr 0.000019	 wd 0.0500	time 0.2961 (0.3339)	loss 1.4467 (1.1698)	grad_norm 0.3635 (0.3655)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:43:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:40 lr 0.000019	 wd 0.0500	time 0.3265 (0.3337)	loss 1.2288 (1.1680)	grad_norm 0.3501 (0.3654)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 18:44:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:07 lr 0.000019	 wd 0.0500	time 0.3152 (0.3337)	loss 1.3903 (1.1671)	grad_norm 0.3681 (nan)	loss_scale 2048.0000 (4051.4976)	mem 17011MB
[2024-08-01 18:45:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3294 (0.3342)	loss 1.2449 (1.1675)	grad_norm 0.3485 (nan)	loss_scale 2048.0000 (3968.0533)	mem 17011MB
[2024-08-01 18:45:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2880 (0.3334)	loss 1.5752 (1.1676)	grad_norm 0.3293 (nan)	loss_scale 2048.0000 (3891.2819)	mem 17011MB
[2024-08-01 18:45:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 7 training takes 0:13:56
[2024-08-01 18:45:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.273 (12.273)	Loss 0.4846 (0.4846)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 18:46:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.954 Acc@5 97.850
[2024-08-01 18:46:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 18:46:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.96%
[2024-08-01 18:46:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][0/2502]	eta 7:50:42 lr 0.000019	 wd 0.0500	time 11.2878 (11.2878)	loss 1.2434 (1.2434)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:46:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:17:14 lr 0.000019	 wd 0.0500	time 0.3211 (0.4308)	loss 0.9740 (1.2093)	grad_norm 0.3352 (0.3742)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:47:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:14:26 lr 0.000019	 wd 0.0500	time 0.3234 (0.3765)	loss 0.8168 (1.1947)	grad_norm 0.3507 (0.3718)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:47:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:13:09 lr 0.000019	 wd 0.0500	time 0.3417 (0.3586)	loss 1.4053 (1.1864)	grad_norm 0.3582 (0.3695)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:48:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:12:14 lr 0.000019	 wd 0.0500	time 0.3315 (0.3494)	loss 0.8407 (1.1851)	grad_norm 0.3560 (0.3732)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:48:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:11:31 lr 0.000019	 wd 0.0500	time 0.2973 (0.3456)	loss 0.8014 (1.1741)	grad_norm 0.3485 (0.3769)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:49:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:10:52 lr 0.000019	 wd 0.0500	time 0.3458 (0.3429)	loss 0.9092 (1.1710)	grad_norm 0.3786 (0.3745)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:50:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:10:13 lr 0.000019	 wd 0.0500	time 0.2936 (0.3404)	loss 1.3451 (1.1715)	grad_norm 0.3642 (0.3725)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:50:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:38 lr 0.000019	 wd 0.0500	time 0.3415 (0.3399)	loss 1.4533 (1.1732)	grad_norm 0.3580 (0.3719)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:51:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:09:02 lr 0.000019	 wd 0.0500	time 0.3401 (0.3387)	loss 0.9477 (1.1683)	grad_norm 0.3980 (0.3763)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:51:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:08:27 lr 0.000019	 wd 0.0500	time 0.3041 (0.3376)	loss 1.3480 (1.1718)	grad_norm 0.3761 (0.3762)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:52:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:52 lr 0.000019	 wd 0.0500	time 0.2962 (0.3372)	loss 1.2029 (1.1699)	grad_norm 0.3405 (0.3745)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:52:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:07:18 lr 0.000019	 wd 0.0500	time 0.3212 (0.3368)	loss 1.1249 (1.1695)	grad_norm 0.4371 (0.3733)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:53:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:44 lr 0.000019	 wd 0.0500	time 0.3025 (0.3361)	loss 0.9018 (1.1690)	grad_norm 0.3629 (0.3721)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:53:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:06:10 lr 0.000019	 wd 0.0500	time 0.3144 (0.3359)	loss 1.2686 (1.1698)	grad_norm 0.3588 (0.3715)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:54:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:36 lr 0.000019	 wd 0.0500	time 0.2935 (0.3354)	loss 1.1694 (1.1700)	grad_norm 0.3621 (0.3713)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:54:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:05:02 lr 0.000019	 wd 0.0500	time 0.2849 (0.3349)	loss 1.3741 (1.1728)	grad_norm 0.3572 (0.3707)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:55:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:28 lr 0.000019	 wd 0.0500	time 0.2918 (0.3349)	loss 0.7689 (1.1752)	grad_norm 0.3479 (0.3700)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:56:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:54 lr 0.000019	 wd 0.0500	time 0.3044 (0.3347)	loss 0.7881 (1.1739)	grad_norm 0.3857 (0.3694)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:56:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:21 lr 0.000019	 wd 0.0500	time 0.3017 (0.3348)	loss 1.1778 (1.1757)	grad_norm 1.1152 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:57:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:47 lr 0.000019	 wd 0.0500	time 0.3055 (0.3347)	loss 0.8107 (1.1745)	grad_norm 0.3541 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:57:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:14 lr 0.000019	 wd 0.0500	time 0.2914 (0.3350)	loss 0.8706 (1.1749)	grad_norm 0.3776 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:58:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:41 lr 0.000019	 wd 0.0500	time 0.3248 (0.3348)	loss 1.1155 (1.1733)	grad_norm 0.3803 (0.3693)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:58:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:07 lr 0.000019	 wd 0.0500	time 0.3328 (0.3353)	loss 1.3424 (1.1719)	grad_norm 0.3574 (0.3691)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:59:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:34 lr 0.000019	 wd 0.0500	time 0.3486 (0.3352)	loss 1.1634 (1.1718)	grad_norm 0.3716 (0.3688)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 18:59:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2880 (0.3346)	loss 0.9105 (1.1717)	grad_norm 0.3415 (0.3694)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:00:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 8 training takes 0:13:59
[2024-08-01 19:00:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.605 (11.605)	Loss 0.4875 (0.4875)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 19:00:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.970 Acc@5 97.836
[2024-08-01 19:00:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 19:00:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 85.97%
[2024-08-01 19:00:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 19:00:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 19:00:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:41:27 lr 0.000019	 wd 0.0500	time 11.0662 (11.0662)	loss 1.3337 (1.3337)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:01:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:17:12 lr 0.000019	 wd 0.0500	time 0.3290 (0.4297)	loss 1.2506 (1.1482)	grad_norm 0.4321 (0.3868)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:01:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:14:23 lr 0.000019	 wd 0.0500	time 0.3035 (0.3753)	loss 0.9012 (1.1720)	grad_norm 0.3502 (0.3729)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:02:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:13:07 lr 0.000019	 wd 0.0500	time 0.3250 (0.3575)	loss 1.5049 (1.1807)	grad_norm 0.3452 (0.3678)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:02:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:12:12 lr 0.000019	 wd 0.0500	time 0.3406 (0.3486)	loss 0.8466 (1.1642)	grad_norm 0.3535 (0.3653)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:03:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:11:27 lr 0.000019	 wd 0.0500	time 0.3215 (0.3432)	loss 0.7500 (1.1594)	grad_norm 0.3652 (0.3654)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:03:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:10:47 lr 0.000019	 wd 0.0500	time 0.3274 (0.3406)	loss 0.8426 (1.1638)	grad_norm 1.3162 (0.3688)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:04:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:10:10 lr 0.000019	 wd 0.0500	time 0.3494 (0.3388)	loss 1.3854 (1.1675)	grad_norm 0.3439 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:05:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:35 lr 0.000019	 wd 0.0500	time 0.3279 (0.3380)	loss 1.3844 (1.1686)	grad_norm 0.3379 (0.3712)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:09:00 lr 0.000019	 wd 0.0500	time 0.3005 (0.3372)	loss 1.3561 (1.1683)	grad_norm 0.3432 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:06:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:08:24 lr 0.000019	 wd 0.0500	time 0.3444 (0.3362)	loss 1.1625 (1.1691)	grad_norm 0.3475 (0.3708)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:06:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:50 lr 0.000018	 wd 0.0500	time 0.2913 (0.3355)	loss 1.3323 (1.1650)	grad_norm 0.3588 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:07:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:07:16 lr 0.000018	 wd 0.0500	time 0.2972 (0.3352)	loss 0.8391 (1.1639)	grad_norm 0.3545 (0.3695)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:07:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:41 lr 0.000018	 wd 0.0500	time 0.2913 (0.3343)	loss 1.2736 (1.1649)	grad_norm 0.3482 (0.3696)	loss_scale 4096.0000 (2133.0054)	mem 17011MB
[2024-08-01 19:08:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:06:07 lr 0.000018	 wd 0.0500	time 0.3142 (0.3339)	loss 1.0723 (1.1641)	grad_norm 0.3899 (0.3691)	loss_scale 4096.0000 (2273.1192)	mem 17011MB
[2024-08-01 19:08:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:34 lr 0.000018	 wd 0.0500	time 0.3341 (0.3337)	loss 0.8572 (1.1673)	grad_norm 0.3589 (0.3687)	loss_scale 4096.0000 (2394.5636)	mem 17011MB
[2024-08-01 19:09:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:05:00 lr 0.000018	 wd 0.0500	time 0.3508 (0.3333)	loss 0.9121 (1.1656)	grad_norm 0.3510 (0.3695)	loss_scale 4096.0000 (2500.8370)	mem 17011MB
[2024-08-01 19:09:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:27 lr 0.000018	 wd 0.0500	time 0.3592 (0.3333)	loss 1.4390 (1.1661)	grad_norm 0.3577 (0.3701)	loss_scale 4096.0000 (2594.6149)	mem 17011MB
[2024-08-01 19:10:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:53 lr 0.000018	 wd 0.0500	time 0.3106 (0.3331)	loss 1.2504 (1.1649)	grad_norm 0.3621 (0.3707)	loss_scale 4096.0000 (2677.9789)	mem 17011MB
[2024-08-01 19:11:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:20 lr 0.000018	 wd 0.0500	time 0.2959 (0.3328)	loss 1.4003 (1.1643)	grad_norm 0.3734 (0.3708)	loss_scale 4096.0000 (2752.5723)	mem 17011MB
[2024-08-01 19:11:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:46 lr 0.000018	 wd 0.0500	time 0.2949 (0.3326)	loss 1.4032 (1.1649)	grad_norm 0.3504 (0.3704)	loss_scale 4096.0000 (2819.7101)	mem 17011MB
[2024-08-01 19:12:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:13 lr 0.000018	 wd 0.0500	time 0.2888 (0.3327)	loss 1.4051 (1.1664)	grad_norm 0.4556 (0.3699)	loss_scale 4096.0000 (2880.4569)	mem 17011MB
[2024-08-01 19:12:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:40 lr 0.000018	 wd 0.0500	time 0.2948 (0.3325)	loss 1.1603 (1.1675)	grad_norm 0.3876 (0.3696)	loss_scale 4096.0000 (2935.6838)	mem 17011MB
[2024-08-01 19:13:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:07 lr 0.000018	 wd 0.0500	time 0.3127 (0.3323)	loss 0.7919 (1.1675)	grad_norm 0.3546 (0.3693)	loss_scale 4096.0000 (2986.1104)	mem 17011MB
[2024-08-01 19:13:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:33 lr 0.000018	 wd 0.0500	time 0.3288 (0.3322)	loss 1.0833 (1.1678)	grad_norm 0.3610 (0.3689)	loss_scale 4096.0000 (3032.3365)	mem 17011MB
[2024-08-01 19:14:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2882 (0.3314)	loss 1.2601 (1.1673)	grad_norm 0.3901 (0.3694)	loss_scale 4096.0000 (3074.8661)	mem 17011MB
[2024-08-01 19:14:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 9 training takes 0:13:51
[2024-08-01 19:14:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.635 (11.635)	Loss 0.5024 (0.5024)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 19:14:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.004 Acc@5 97.842
[2024-08-01 19:14:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 19:14:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.00%
[2024-08-01 19:14:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 19:14:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 19:15:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:35:06 lr 0.000018	 wd 0.0500	time 10.9140 (10.9140)	loss 1.6264 (1.6264)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:15:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:17:13 lr 0.000018	 wd 0.0500	time 0.3057 (0.4303)	loss 0.8859 (1.1785)	grad_norm 0.3877 (inf)	loss_scale 2048.0000 (2717.1485)	mem 17011MB
[2024-08-01 19:16:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:14:27 lr 0.000018	 wd 0.0500	time 0.3106 (0.3766)	loss 1.4428 (1.1736)	grad_norm 0.3325 (inf)	loss_scale 2048.0000 (2384.2388)	mem 17011MB
[2024-08-01 19:16:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:13:10 lr 0.000018	 wd 0.0500	time 0.3414 (0.3592)	loss 1.3215 (1.1685)	grad_norm 0.5372 (inf)	loss_scale 2048.0000 (2272.5316)	mem 17011MB
[2024-08-01 19:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:12:16 lr 0.000018	 wd 0.0500	time 0.3112 (0.3503)	loss 0.8949 (1.1721)	grad_norm 0.7913 (inf)	loss_scale 2048.0000 (2216.5387)	mem 17011MB
[2024-08-01 19:17:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:11:29 lr 0.000018	 wd 0.0500	time 0.2987 (0.3445)	loss 0.7022 (1.1676)	grad_norm 0.3438 (inf)	loss_scale 2048.0000 (2182.8982)	mem 17011MB
[2024-08-01 19:18:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:10:50 lr 0.000018	 wd 0.0500	time 0.2905 (0.3418)	loss 1.1716 (1.1753)	grad_norm 0.3511 (inf)	loss_scale 2048.0000 (2160.4526)	mem 17011MB
[2024-08-01 19:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:10:11 lr 0.000018	 wd 0.0500	time 0.2947 (0.3395)	loss 1.2994 (1.1734)	grad_norm 0.3226 (inf)	loss_scale 2048.0000 (2144.4108)	mem 17011MB
[2024-08-01 19:19:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:35 lr 0.000018	 wd 0.0500	time 0.3201 (0.3379)	loss 1.2737 (1.1761)	grad_norm 0.3515 (inf)	loss_scale 2048.0000 (2132.3745)	mem 17011MB
[2024-08-01 19:19:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:08:59 lr 0.000018	 wd 0.0500	time 0.3122 (0.3366)	loss 1.4575 (1.1737)	grad_norm 0.3439 (inf)	loss_scale 2048.0000 (2123.0100)	mem 17011MB
[2024-08-01 19:20:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:08:23 lr 0.000018	 wd 0.0500	time 0.3333 (0.3355)	loss 1.2852 (1.1747)	grad_norm 0.3588 (inf)	loss_scale 2048.0000 (2115.5165)	mem 17011MB
[2024-08-01 19:21:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:07:50 lr 0.000018	 wd 0.0500	time 0.3009 (0.3359)	loss 0.9965 (1.1806)	grad_norm 0.3714 (inf)	loss_scale 2048.0000 (2109.3842)	mem 17011MB
[2024-08-01 19:21:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:07:16 lr 0.000018	 wd 0.0500	time 0.3202 (0.3354)	loss 1.3948 (1.1781)	grad_norm 0.3562 (inf)	loss_scale 2048.0000 (2104.2731)	mem 17011MB
[2024-08-01 19:22:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:42 lr 0.000018	 wd 0.0500	time 0.3135 (0.3345)	loss 1.3682 (1.1756)	grad_norm 0.3503 (inf)	loss_scale 2048.0000 (2099.9477)	mem 17011MB
[2024-08-01 19:22:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:06:08 lr 0.000018	 wd 0.0500	time 0.3089 (0.3342)	loss 1.3440 (1.1735)	grad_norm 0.3490 (inf)	loss_scale 2048.0000 (2096.2398)	mem 17011MB
[2024-08-01 19:23:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:34 lr 0.000018	 wd 0.0500	time 0.3134 (0.3334)	loss 1.1691 (1.1710)	grad_norm 0.3773 (inf)	loss_scale 2048.0000 (2093.0260)	mem 17011MB
[2024-08-01 19:23:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:05:00 lr 0.000018	 wd 0.0500	time 0.3348 (0.3334)	loss 1.1173 (1.1697)	grad_norm 0.3447 (inf)	loss_scale 2048.0000 (2090.2136)	mem 17011MB
[2024-08-01 19:24:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:27 lr 0.000018	 wd 0.0500	time 0.3274 (0.3333)	loss 1.2396 (1.1717)	grad_norm 0.3562 (inf)	loss_scale 2048.0000 (2087.7319)	mem 17011MB
[2024-08-01 19:24:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:53 lr 0.000018	 wd 0.0500	time 0.3507 (0.3331)	loss 1.1404 (1.1707)	grad_norm 0.3782 (inf)	loss_scale 2048.0000 (2085.5258)	mem 17011MB
[2024-08-01 19:25:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:20 lr 0.000018	 wd 0.0500	time 0.3360 (0.3332)	loss 1.0932 (1.1697)	grad_norm 0.3491 (inf)	loss_scale 2048.0000 (2083.5518)	mem 17011MB
[2024-08-01 19:25:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:47 lr 0.000017	 wd 0.0500	time 0.3046 (0.3330)	loss 1.3272 (1.1732)	grad_norm 0.3680 (inf)	loss_scale 2048.0000 (2081.7751)	mem 17011MB
[2024-08-01 19:26:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:13 lr 0.000017	 wd 0.0500	time 0.3436 (0.3326)	loss 1.1997 (1.1745)	grad_norm 0.3364 (inf)	loss_scale 2048.0000 (2080.1675)	mem 17011MB
[2024-08-01 19:27:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:40 lr 0.000017	 wd 0.0500	time 0.3292 (0.3323)	loss 1.3316 (1.1756)	grad_norm 0.3494 (inf)	loss_scale 2048.0000 (2078.7060)	mem 17011MB
[2024-08-01 19:27:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:07 lr 0.000017	 wd 0.0500	time 0.3251 (0.3324)	loss 1.3036 (1.1735)	grad_norm 0.3578 (inf)	loss_scale 2048.0000 (2077.3716)	mem 17011MB
[2024-08-01 19:28:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:33 lr 0.000017	 wd 0.0500	time 0.3199 (0.3325)	loss 1.3673 (1.1725)	grad_norm 0.3424 (inf)	loss_scale 2048.0000 (2076.1483)	mem 17011MB
[2024-08-01 19:28:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2883 (0.3316)	loss 0.8940 (1.1721)	grad_norm 0.3456 (inf)	loss_scale 2048.0000 (2075.0228)	mem 17011MB
[2024-08-01 19:28:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 10 training takes 0:13:52
[2024-08-01 19:28:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_10.pth saving......
[2024-08-01 19:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_10.pth saved !!!
[2024-08-01 19:28:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.900 (11.900)	Loss 0.4941 (0.4941)	Acc@1 93.164 (93.164)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 19:29:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.044 Acc@5 97.888
[2024-08-01 19:29:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 19:29:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.04%
[2024-08-01 19:29:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 19:29:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 19:29:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:58:00 lr 0.000017	 wd 0.0500	time 11.4631 (11.4631)	loss 0.9069 (0.9069)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:29:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:17:10 lr 0.000017	 wd 0.0500	time 0.2973 (0.4292)	loss 1.3962 (1.1573)	grad_norm 0.3512 (0.3582)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:30:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:14:20 lr 0.000017	 wd 0.0500	time 0.2920 (0.3737)	loss 1.3316 (1.1781)	grad_norm 0.3617 (0.3627)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:30:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:13:02 lr 0.000017	 wd 0.0500	time 0.2986 (0.3555)	loss 0.8563 (1.1681)	grad_norm 0.3502 (0.3628)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:31:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:12:10 lr 0.000017	 wd 0.0500	time 0.2897 (0.3474)	loss 1.4052 (1.1664)	grad_norm 0.3720 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:32:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:11:23 lr 0.000017	 wd 0.0500	time 0.2992 (0.3416)	loss 0.7321 (1.1658)	grad_norm 0.4275 (0.3669)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:32:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:10:42 lr 0.000017	 wd 0.0500	time 0.3330 (0.3380)	loss 1.4581 (1.1665)	grad_norm 0.3509 (0.3687)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:33:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:10:04 lr 0.000017	 wd 0.0500	time 0.3506 (0.3356)	loss 1.0078 (1.1619)	grad_norm 0.4198 (0.3683)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:33:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:34 lr 0.000017	 wd 0.0500	time 0.3170 (0.3378)	loss 1.4837 (1.1599)	grad_norm 0.3467 (0.3672)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:34:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:08:59 lr 0.000017	 wd 0.0500	time 0.2911 (0.3365)	loss 1.0071 (1.1627)	grad_norm 0.4001 (0.3672)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:34:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:08:23 lr 0.000017	 wd 0.0500	time 0.3255 (0.3353)	loss 1.1740 (1.1628)	grad_norm 0.3766 (0.3665)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:35:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:49 lr 0.000017	 wd 0.0500	time 0.3172 (0.3346)	loss 0.9100 (1.1614)	grad_norm 0.3748 (0.3660)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:35:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:07:14 lr 0.000017	 wd 0.0500	time 0.3148 (0.3336)	loss 0.9356 (1.1582)	grad_norm 0.3592 (0.3668)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:36:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:40 lr 0.000017	 wd 0.0500	time 0.3309 (0.3335)	loss 1.3894 (1.1586)	grad_norm 0.3897 (0.3662)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:37:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:06:08 lr 0.000017	 wd 0.0500	time 0.3030 (0.3345)	loss 0.8078 (1.1590)	grad_norm 0.3602 (0.3658)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:37:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:35 lr 0.000017	 wd 0.0500	time 0.3294 (0.3344)	loss 1.3973 (1.1597)	grad_norm 0.3680 (0.3656)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:38:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:05:00 lr 0.000017	 wd 0.0500	time 0.3011 (0.3335)	loss 1.2810 (1.1609)	grad_norm 0.3540 (0.3654)	loss_scale 4096.0000 (2137.5440)	mem 17011MB
[2024-08-01 19:38:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:27 lr 0.000017	 wd 0.0500	time 0.2937 (0.3330)	loss 1.0831 (1.1601)	grad_norm 0.3491 (0.3652)	loss_scale 4096.0000 (2252.6796)	mem 17011MB
[2024-08-01 19:39:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:53 lr 0.000017	 wd 0.0500	time 0.3490 (0.3327)	loss 0.8410 (1.1600)	grad_norm 0.3420 (0.3650)	loss_scale 4096.0000 (2355.0294)	mem 17011MB
[2024-08-01 19:39:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:20 lr 0.000017	 wd 0.0500	time 0.3269 (0.3324)	loss 1.1387 (1.1592)	grad_norm 0.3649 (0.3647)	loss_scale 4096.0000 (2446.6113)	mem 17011MB
[2024-08-01 19:40:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:46 lr 0.000017	 wd 0.0500	time 0.3484 (0.3325)	loss 1.0646 (1.1593)	grad_norm 0.3268 (0.3647)	loss_scale 4096.0000 (2529.0395)	mem 17011MB
[2024-08-01 19:40:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:13 lr 0.000017	 wd 0.0500	time 0.3309 (0.3331)	loss 1.3588 (1.1586)	grad_norm 0.3660 (0.3644)	loss_scale 4096.0000 (2603.6211)	mem 17011MB
[2024-08-01 19:41:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:40 lr 0.000017	 wd 0.0500	time 0.3268 (0.3331)	loss 0.9900 (1.1605)	grad_norm 0.3646 (0.3646)	loss_scale 4096.0000 (2671.4257)	mem 17011MB
[2024-08-01 19:41:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:07 lr 0.000016	 wd 0.0500	time 0.2960 (0.3328)	loss 1.1639 (1.1586)	grad_norm 0.3640 (0.3645)	loss_scale 4096.0000 (2733.3368)	mem 17011MB
[2024-08-01 19:42:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:33 lr 0.000016	 wd 0.0500	time 0.2858 (0.3325)	loss 1.3930 (1.1599)	grad_norm 0.3469 (0.3647)	loss_scale 4096.0000 (2790.0908)	mem 17011MB
[2024-08-01 19:43:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.3060 (0.3317)	loss 0.8471 (1.1582)	grad_norm 0.3278 (0.3646)	loss_scale 4096.0000 (2842.3063)	mem 17011MB
[2024-08-01 19:43:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 11 training takes 0:13:52
[2024-08-01 19:43:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.131 (12.131)	Loss 0.4937 (0.4937)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 19:43:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 85.972 Acc@5 97.864
[2024-08-01 19:43:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 19:43:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.04%
[2024-08-01 19:43:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][0/2502]	eta 8:22:12 lr 0.000016	 wd 0.0500	time 12.0432 (12.0432)	loss 0.9523 (0.9523)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:44:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:17:34 lr 0.000016	 wd 0.0500	time 0.3143 (0.4390)	loss 1.3070 (1.1608)	grad_norm 0.3474 (0.3601)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:44:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:14:37 lr 0.000016	 wd 0.0500	time 0.3209 (0.3811)	loss 1.2967 (1.1587)	grad_norm 0.3525 (0.3602)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:13:16 lr 0.000016	 wd 0.0500	time 0.3500 (0.3615)	loss 0.7500 (1.1712)	grad_norm 0.3697 (0.3627)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:45:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:12:21 lr 0.000016	 wd 0.0500	time 0.3161 (0.3526)	loss 1.2299 (1.1630)	grad_norm 0.3420 (0.3636)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:46:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:11:36 lr 0.000016	 wd 0.0500	time 0.3165 (0.3478)	loss 1.4365 (1.1630)	grad_norm 0.3467 (0.3627)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:47:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:10:55 lr 0.000016	 wd 0.0500	time 0.2973 (0.3444)	loss 1.4148 (1.1615)	grad_norm 0.3689 (0.3631)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:47:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:10:16 lr 0.000016	 wd 0.0500	time 0.3337 (0.3423)	loss 1.2133 (1.1596)	grad_norm 0.3609 (0.3641)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:48:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:40 lr 0.000016	 wd 0.0500	time 0.3447 (0.3412)	loss 1.3681 (1.1606)	grad_norm 0.3580 (0.3639)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:48:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:09:03 lr 0.000016	 wd 0.0500	time 0.3027 (0.3395)	loss 1.5325 (1.1581)	grad_norm 0.3491 (0.3641)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:49:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:08:29 lr 0.000016	 wd 0.0500	time 0.3511 (0.3389)	loss 1.3865 (1.1601)	grad_norm 0.3662 (0.3637)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:49:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:54 lr 0.000016	 wd 0.0500	time 0.2915 (0.3381)	loss 0.8062 (1.1617)	grad_norm 0.8374 (0.3728)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:50:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:07:19 lr 0.000016	 wd 0.0500	time 0.3160 (0.3373)	loss 1.4361 (1.1599)	grad_norm 0.3587 (0.3722)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:50:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:44 lr 0.000016	 wd 0.0500	time 0.3311 (0.3366)	loss 1.0476 (1.1608)	grad_norm 0.3374 (0.3711)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:51:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:06:10 lr 0.000016	 wd 0.0500	time 0.3124 (0.3361)	loss 1.3246 (1.1605)	grad_norm 0.3412 (0.3719)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:51:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:36 lr 0.000016	 wd 0.0500	time 0.3146 (0.3355)	loss 0.8137 (1.1578)	grad_norm 0.3702 (0.3711)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:52:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:05:02 lr 0.000016	 wd 0.0500	time 0.3852 (0.3353)	loss 1.3631 (1.1545)	grad_norm 0.3379 (0.3708)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:53:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:28 lr 0.000016	 wd 0.0500	time 0.3358 (0.3348)	loss 1.0662 (1.1567)	grad_norm 0.3662 (0.3708)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 19:53:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:54 lr 0.000016	 wd 0.0500	time 0.3186 (0.3346)	loss 0.8235 (1.1552)	grad_norm 0.3545 (nan)	loss_scale 2048.0000 (4055.0627)	mem 17011MB
[2024-08-01 19:54:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:21 lr 0.000016	 wd 0.0500	time 0.3133 (0.3344)	loss 1.3086 (1.1560)	grad_norm 0.3541 (nan)	loss_scale 2048.0000 (3949.4834)	mem 17011MB
[2024-08-01 19:54:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:47 lr 0.000016	 wd 0.0500	time 0.3256 (0.3342)	loss 1.0546 (1.1569)	grad_norm 0.3568 (nan)	loss_scale 2048.0000 (3854.4568)	mem 17011MB
[2024-08-01 19:55:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:14 lr 0.000016	 wd 0.0500	time 0.3010 (0.3341)	loss 0.8046 (1.1562)	grad_norm 2.4726 (nan)	loss_scale 2048.0000 (3768.4760)	mem 17011MB
[2024-08-01 19:55:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:40 lr 0.000016	 wd 0.0500	time 0.2962 (0.3340)	loss 0.8158 (1.1576)	grad_norm 0.4340 (nan)	loss_scale 2048.0000 (3690.3080)	mem 17011MB
[2024-08-01 19:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:07 lr 0.000015	 wd 0.0500	time 0.3123 (0.3341)	loss 1.1930 (1.1581)	grad_norm 0.3466 (nan)	loss_scale 2048.0000 (3618.9344)	mem 17011MB
[2024-08-01 19:56:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:34 lr 0.000015	 wd 0.0500	time 0.3494 (0.3342)	loss 1.3646 (1.1584)	grad_norm 0.3391 (nan)	loss_scale 2048.0000 (3553.5060)	mem 17011MB
[2024-08-01 19:57:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.2885 (0.3333)	loss 0.9422 (1.1577)	grad_norm 0.3564 (nan)	loss_scale 2048.0000 (3493.3099)	mem 17011MB
[2024-08-01 19:57:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 12 training takes 0:13:56
[2024-08-01 19:57:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.813 (11.813)	Loss 0.5054 (0.5054)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 19:57:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.056 Acc@5 97.852
[2024-08-01 19:57:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 19:57:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.06%
[2024-08-01 19:57:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 19:57:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 19:58:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:34:51 lr 0.000015	 wd 0.0500	time 10.9078 (10.9078)	loss 1.3294 (1.3294)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:58:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:17:26 lr 0.000015	 wd 0.0500	time 0.3053 (0.4357)	loss 1.3423 (1.2225)	grad_norm 0.3560 (0.3631)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:59:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:14:33 lr 0.000015	 wd 0.0500	time 0.3284 (0.3796)	loss 1.3896 (1.2102)	grad_norm 0.3452 (0.3648)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 19:59:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:13:27 lr 0.000015	 wd 0.0500	time 0.3371 (0.3667)	loss 1.3932 (1.2103)	grad_norm 0.3517 (0.3647)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:00:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:12:31 lr 0.000015	 wd 0.0500	time 0.3090 (0.3576)	loss 1.4635 (1.1908)	grad_norm 0.3625 (nan)	loss_scale 1024.0000 (1996.9277)	mem 17011MB
[2024-08-01 20:00:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:11:44 lr 0.000015	 wd 0.0500	time 0.3141 (0.3517)	loss 1.2846 (1.1842)	grad_norm 1.4416 (nan)	loss_scale 1024.0000 (1802.7305)	mem 17011MB
[2024-08-01 20:01:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:11:05 lr 0.000015	 wd 0.0500	time 0.3539 (0.3497)	loss 1.1840 (1.1781)	grad_norm 0.3443 (nan)	loss_scale 1024.0000 (1673.1581)	mem 17011MB
[2024-08-01 20:02:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:10:24 lr 0.000015	 wd 0.0500	time 0.3538 (0.3466)	loss 1.1830 (1.1690)	grad_norm 0.3689 (nan)	loss_scale 1024.0000 (1580.5535)	mem 17011MB
[2024-08-01 20:02:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:47 lr 0.000015	 wd 0.0500	time 0.3152 (0.3451)	loss 1.2393 (1.1682)	grad_norm 0.3622 (nan)	loss_scale 1024.0000 (1511.0712)	mem 17011MB
[2024-08-01 20:03:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:09:10 lr 0.000015	 wd 0.0500	time 0.3178 (0.3434)	loss 1.3292 (1.1688)	grad_norm 0.3661 (nan)	loss_scale 1024.0000 (1457.0122)	mem 17011MB
[2024-08-01 20:03:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:08:33 lr 0.000015	 wd 0.0500	time 0.3229 (0.3419)	loss 1.3900 (1.1677)	grad_norm 0.3442 (nan)	loss_scale 1024.0000 (1413.7542)	mem 17011MB
[2024-08-01 20:04:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:58 lr 0.000015	 wd 0.0500	time 0.3512 (0.3411)	loss 1.4023 (1.1637)	grad_norm 0.3617 (nan)	loss_scale 1024.0000 (1378.3542)	mem 17011MB
[2024-08-01 20:04:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:07:23 lr 0.000015	 wd 0.0500	time 0.2991 (0.3403)	loss 1.1813 (1.1676)	grad_norm 0.3686 (nan)	loss_scale 1024.0000 (1348.8493)	mem 17011MB
[2024-08-01 20:05:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:47 lr 0.000015	 wd 0.0500	time 0.3005 (0.3394)	loss 0.7290 (1.1661)	grad_norm 0.3571 (nan)	loss_scale 1024.0000 (1323.8801)	mem 17011MB
[2024-08-01 20:05:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:06:13 lr 0.000015	 wd 0.0500	time 0.3176 (0.3388)	loss 1.3622 (1.1647)	grad_norm 0.3564 (nan)	loss_scale 1024.0000 (1302.4754)	mem 17011MB
[2024-08-01 20:06:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:39 lr 0.000015	 wd 0.0500	time 0.3196 (0.3392)	loss 1.2119 (1.1653)	grad_norm 0.3713 (nan)	loss_scale 1024.0000 (1283.9227)	mem 17011MB
[2024-08-01 20:07:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:05:05 lr 0.000015	 wd 0.0500	time 0.2987 (0.3386)	loss 0.9260 (1.1642)	grad_norm 0.3566 (nan)	loss_scale 1024.0000 (1267.6877)	mem 17011MB
[2024-08-01 20:07:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:31 lr 0.000015	 wd 0.0500	time 0.3304 (0.3380)	loss 1.0331 (1.1653)	grad_norm 0.3563 (nan)	loss_scale 1024.0000 (1253.3616)	mem 17011MB
[2024-08-01 20:08:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:57 lr 0.000015	 wd 0.0500	time 0.3289 (0.3378)	loss 1.2352 (1.1658)	grad_norm 0.3747 (nan)	loss_scale 1024.0000 (1240.6263)	mem 17011MB
[2024-08-01 20:08:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:23 lr 0.000015	 wd 0.0500	time 0.3376 (0.3378)	loss 1.0912 (1.1659)	grad_norm 0.3562 (nan)	loss_scale 1024.0000 (1229.2309)	mem 17011MB
[2024-08-01 20:09:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:49 lr 0.000015	 wd 0.0500	time 0.2996 (0.3375)	loss 1.3961 (1.1658)	grad_norm 0.3378 (nan)	loss_scale 1024.0000 (1218.9745)	mem 17011MB
[2024-08-01 20:09:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:15 lr 0.000014	 wd 0.0500	time 0.3302 (0.3370)	loss 1.3553 (1.1658)	grad_norm 0.3656 (nan)	loss_scale 1024.0000 (1209.6944)	mem 17011MB
[2024-08-01 20:10:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:41 lr 0.000014	 wd 0.0500	time 0.3432 (0.3367)	loss 1.1992 (1.1644)	grad_norm 0.3635 (nan)	loss_scale 1024.0000 (1201.2576)	mem 17011MB
[2024-08-01 20:10:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:08 lr 0.000014	 wd 0.0500	time 0.3279 (0.3372)	loss 1.3424 (1.1633)	grad_norm 0.3664 (nan)	loss_scale 1024.0000 (1193.5541)	mem 17011MB
[2024-08-01 20:11:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:34 lr 0.000014	 wd 0.0500	time 0.3060 (0.3370)	loss 0.9344 (1.1661)	grad_norm 0.3450 (nan)	loss_scale 1024.0000 (1186.4923)	mem 17011MB
[2024-08-01 20:11:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.2868 (0.3360)	loss 0.7926 (1.1648)	grad_norm 0.3520 (nan)	loss_scale 1024.0000 (1179.9952)	mem 17011MB
[2024-08-01 20:12:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 13 training takes 0:14:03
[2024-08-01 20:12:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.392 (12.392)	Loss 0.4800 (0.4800)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 20:12:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.020 Acc@5 97.870
[2024-08-01 20:12:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 20:12:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.06%
[2024-08-01 20:12:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:15:17 lr 0.000014	 wd 0.0500	time 10.4388 (10.4388)	loss 1.4213 (1.4213)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:13:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:17:26 lr 0.000014	 wd 0.0500	time 0.3100 (0.4358)	loss 1.1845 (1.1810)	grad_norm 0.3523 (0.3866)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:13:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:14:41 lr 0.000014	 wd 0.0500	time 0.3111 (0.3827)	loss 1.4568 (1.1800)	grad_norm 0.3423 (0.3777)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:14:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:13:19 lr 0.000014	 wd 0.0500	time 0.2875 (0.3630)	loss 0.9284 (1.1830)	grad_norm 0.3724 (0.3726)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:14:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:12:19 lr 0.000014	 wd 0.0500	time 0.3083 (0.3520)	loss 1.5914 (1.1826)	grad_norm 0.3458 (0.3703)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:15:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:11:32 lr 0.000014	 wd 0.0500	time 0.3224 (0.3461)	loss 0.9715 (1.1782)	grad_norm 0.3482 (0.3688)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:15:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:10:51 lr 0.000014	 wd 0.0500	time 0.3772 (0.3427)	loss 0.9777 (1.1772)	grad_norm 0.3686 (0.3673)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:16:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:10:16 lr 0.000014	 wd 0.0500	time 0.3167 (0.3421)	loss 1.1347 (1.1778)	grad_norm 0.3641 (0.3676)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:17:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:39 lr 0.000014	 wd 0.0500	time 0.3302 (0.3407)	loss 0.7989 (1.1719)	grad_norm 0.3450 (0.3664)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:17:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:09:02 lr 0.000014	 wd 0.0500	time 0.2899 (0.3386)	loss 0.8558 (1.1708)	grad_norm 0.3740 (0.3669)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:18:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:08:27 lr 0.000014	 wd 0.0500	time 0.2981 (0.3376)	loss 0.9969 (1.1681)	grad_norm 0.3790 (0.3662)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:18:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:51 lr 0.000014	 wd 0.0500	time 0.3127 (0.3364)	loss 1.3810 (1.1675)	grad_norm 0.6858 (0.3682)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:19:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:07:17 lr 0.000014	 wd 0.0500	time 0.3160 (0.3360)	loss 1.4738 (1.1672)	grad_norm 0.3673 (0.3680)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:19:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:43 lr 0.000014	 wd 0.0500	time 0.3243 (0.3356)	loss 1.2402 (1.1704)	grad_norm 0.3608 (0.3682)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:20:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:06:09 lr 0.000014	 wd 0.0500	time 0.3225 (0.3354)	loss 1.2409 (1.1700)	grad_norm 0.3638 (0.3689)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:20:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:35 lr 0.000014	 wd 0.0500	time 0.3402 (0.3351)	loss 1.3830 (1.1690)	grad_norm 0.3504 (0.3681)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:21:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:05:02 lr 0.000014	 wd 0.0500	time 0.2949 (0.3352)	loss 0.7679 (1.1679)	grad_norm 0.3606 (0.3684)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:22:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:28 lr 0.000014	 wd 0.0500	time 0.3219 (0.3350)	loss 0.9350 (1.1668)	grad_norm 0.3459 (0.3726)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:22:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:55 lr 0.000013	 wd 0.0500	time 0.2930 (0.3350)	loss 0.9058 (1.1674)	grad_norm 0.4327 (0.3728)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:23:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:21 lr 0.000013	 wd 0.0500	time 0.3341 (0.3348)	loss 1.1596 (1.1677)	grad_norm 0.3731 (0.3724)	loss_scale 2048.0000 (1035.8506)	mem 17011MB
[2024-08-01 20:23:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:48 lr 0.000013	 wd 0.0500	time 0.3390 (0.3347)	loss 1.2211 (1.1671)	grad_norm 0.3493 (0.3718)	loss_scale 2048.0000 (1086.4328)	mem 17011MB
[2024-08-01 20:24:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:14 lr 0.000013	 wd 0.0500	time 0.2913 (0.3344)	loss 1.3472 (1.1664)	grad_norm 0.3551 (0.3712)	loss_scale 2048.0000 (1132.1999)	mem 17011MB
[2024-08-01 20:24:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:40 lr 0.000013	 wd 0.0500	time 0.2866 (0.3343)	loss 1.3206 (1.1656)	grad_norm 0.3592 (0.3708)	loss_scale 2048.0000 (1173.8083)	mem 17011MB
[2024-08-01 20:25:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:07 lr 0.000013	 wd 0.0500	time 0.3036 (0.3345)	loss 0.8578 (1.1645)	grad_norm 0.3575 (0.3702)	loss_scale 2048.0000 (1211.8001)	mem 17011MB
[2024-08-01 20:25:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:34 lr 0.000013	 wd 0.0500	time 0.2967 (0.3344)	loss 0.8600 (1.1636)	grad_norm 0.3905 (0.3697)	loss_scale 2048.0000 (1246.6272)	mem 17011MB
[2024-08-01 20:26:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2891 (0.3335)	loss 1.2500 (1.1625)	grad_norm 0.3680 (0.3696)	loss_scale 2048.0000 (1278.6693)	mem 17011MB
[2024-08-01 20:26:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 14 training takes 0:13:56
[2024-08-01 20:26:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 10.983 (10.983)	Loss 0.4707 (0.4707)	Acc@1 92.969 (92.969)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 20:26:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.096 Acc@5 97.886
[2024-08-01 20:26:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 20:26:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 20:26:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 20:26:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 20:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:32:22 lr 0.000013	 wd 0.0500	time 10.8483 (10.8483)	loss 1.2680 (1.2680)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:27:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:17:10 lr 0.000013	 wd 0.0500	time 0.3279 (0.4292)	loss 0.7785 (1.1694)	grad_norm 0.3573 (0.3628)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:28:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:14:27 lr 0.000013	 wd 0.0500	time 0.3241 (0.3767)	loss 1.4507 (1.1551)	grad_norm 0.3605 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:13:11 lr 0.000013	 wd 0.0500	time 0.3052 (0.3597)	loss 0.9289 (1.1545)	grad_norm 0.3749 (0.3676)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:29:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:12:17 lr 0.000013	 wd 0.0500	time 0.2955 (0.3508)	loss 0.8015 (1.1532)	grad_norm 0.3503 (0.3662)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:29:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:11:32 lr 0.000013	 wd 0.0500	time 0.3317 (0.3458)	loss 0.9502 (1.1644)	grad_norm 0.3661 (0.3690)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:30:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:10:51 lr 0.000013	 wd 0.0500	time 0.3928 (0.3425)	loss 1.3910 (1.1671)	grad_norm 0.3633 (0.3701)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:30:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:10:12 lr 0.000013	 wd 0.0500	time 0.3076 (0.3396)	loss 1.2464 (1.1670)	grad_norm 0.3592 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:31:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:35 lr 0.000013	 wd 0.0500	time 0.3162 (0.3379)	loss 1.4039 (1.1687)	grad_norm 0.3476 (0.3688)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:31:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:08:59 lr 0.000013	 wd 0.0500	time 0.2953 (0.3367)	loss 1.2645 (1.1683)	grad_norm 0.3648 (0.3688)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:32:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:08:24 lr 0.000013	 wd 0.0500	time 0.3187 (0.3358)	loss 0.8536 (1.1683)	grad_norm 0.3583 (0.3687)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:33:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:07:49 lr 0.000013	 wd 0.0500	time 0.3188 (0.3352)	loss 0.8707 (1.1690)	grad_norm 0.3487 (0.3700)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:33:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:07:15 lr 0.000013	 wd 0.0500	time 0.3048 (0.3342)	loss 0.8351 (1.1690)	grad_norm 0.3747 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:34:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:41 lr 0.000013	 wd 0.0500	time 0.3115 (0.3338)	loss 0.8644 (1.1701)	grad_norm 0.3531 (0.3694)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:34:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:06:07 lr 0.000012	 wd 0.0500	time 0.3126 (0.3335)	loss 0.9508 (1.1702)	grad_norm 0.3575 (0.3687)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:35:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:34 lr 0.000012	 wd 0.0500	time 0.3043 (0.3334)	loss 1.0164 (1.1664)	grad_norm 0.3598 (0.3681)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:35:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:05:00 lr 0.000012	 wd 0.0500	time 0.3087 (0.3333)	loss 1.0196 (1.1672)	grad_norm 0.3726 (0.3680)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:36:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:27 lr 0.000012	 wd 0.0500	time 0.3037 (0.3330)	loss 0.8211 (1.1672)	grad_norm 0.3612 (0.3706)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:36:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:53 lr 0.000012	 wd 0.0500	time 0.3353 (0.3329)	loss 1.6779 (1.1652)	grad_norm 0.3859 (0.3702)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:37:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:20 lr 0.000012	 wd 0.0500	time 0.3400 (0.3326)	loss 0.9715 (1.1634)	grad_norm 0.3786 (0.3702)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:38:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:46 lr 0.000012	 wd 0.0500	time 0.3365 (0.3325)	loss 0.8440 (1.1647)	grad_norm 0.3594 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:38:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:13 lr 0.000012	 wd 0.0500	time 0.3451 (0.3325)	loss 0.7871 (1.1635)	grad_norm 0.3562 (0.3695)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:39:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:40 lr 0.000012	 wd 0.0500	time 0.2928 (0.3322)	loss 1.0791 (1.1631)	grad_norm 0.3473 (0.3691)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:07 lr 0.000012	 wd 0.0500	time 0.3101 (0.3321)	loss 1.0184 (1.1634)	grad_norm 0.3684 (0.3696)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:40:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:33 lr 0.000012	 wd 0.0500	time 0.3319 (0.3320)	loss 1.4728 (1.1638)	grad_norm 0.3380 (0.3692)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:40:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2885 (0.3313)	loss 1.5164 (1.1643)	grad_norm 0.3652 (0.3690)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:40:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 15 training takes 0:13:51
[2024-08-01 20:40:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.935 (11.935)	Loss 0.5244 (0.5244)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 17011MB
[2024-08-01 20:41:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.004 Acc@5 97.838
[2024-08-01 20:41:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 20:41:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 20:41:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:09:01 lr 0.000012	 wd 0.0500	time 11.7272 (11.7272)	loss 1.2066 (1.2066)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:41:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:17:24 lr 0.000012	 wd 0.0500	time 0.2969 (0.4347)	loss 1.0454 (1.1756)	grad_norm 0.3641 (0.3779)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:42:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:14:31 lr 0.000012	 wd 0.0500	time 0.3195 (0.3787)	loss 1.2566 (1.1650)	grad_norm 0.3544 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:43:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:13:10 lr 0.000012	 wd 0.0500	time 0.3144 (0.3590)	loss 1.0188 (1.1580)	grad_norm 0.3505 (0.3718)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:43:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:12:15 lr 0.000012	 wd 0.0500	time 0.2957 (0.3498)	loss 0.9330 (1.1465)	grad_norm 0.3534 (0.3696)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 20:44:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:11:29 lr 0.000012	 wd 0.0500	time 0.3336 (0.3446)	loss 1.5324 (1.1492)	grad_norm 0.3704 (nan)	loss_scale 1024.0000 (1998.9461)	mem 17011MB
[2024-08-01 20:44:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:10:48 lr 0.000012	 wd 0.0500	time 0.3246 (0.3412)	loss 1.4365 (1.1499)	grad_norm 0.3343 (nan)	loss_scale 1024.0000 (1836.7255)	mem 17011MB
[2024-08-01 20:45:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:10:10 lr 0.000012	 wd 0.0500	time 0.3322 (0.3387)	loss 1.0667 (1.1524)	grad_norm 0.3564 (nan)	loss_scale 1024.0000 (1720.7874)	mem 17011MB
[2024-08-01 20:45:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:34 lr 0.000012	 wd 0.0500	time 0.2954 (0.3373)	loss 1.3212 (1.1539)	grad_norm 0.3657 (nan)	loss_scale 1024.0000 (1633.7978)	mem 17011MB
[2024-08-01 20:46:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:08:57 lr 0.000012	 wd 0.0500	time 0.3007 (0.3357)	loss 1.1409 (1.1550)	grad_norm 0.3921 (nan)	loss_scale 1024.0000 (1566.1176)	mem 17011MB
[2024-08-01 20:46:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:08:22 lr 0.000011	 wd 0.0500	time 0.3129 (0.3344)	loss 1.3537 (1.1548)	grad_norm 0.3499 (nan)	loss_scale 1024.0000 (1511.9600)	mem 17011MB
[2024-08-01 20:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:48 lr 0.000011	 wd 0.0500	time 0.3249 (0.3343)	loss 0.8448 (1.1589)	grad_norm 0.3627 (nan)	loss_scale 1024.0000 (1467.6403)	mem 17011MB
[2024-08-01 20:47:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:07:14 lr 0.000011	 wd 0.0500	time 0.3338 (0.3334)	loss 0.8711 (1.1567)	grad_norm 0.3715 (nan)	loss_scale 1024.0000 (1430.7011)	mem 17011MB
[2024-08-01 20:48:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:41 lr 0.000011	 wd 0.0500	time 0.3192 (0.3336)	loss 1.2981 (1.1596)	grad_norm 0.3607 (nan)	loss_scale 1024.0000 (1399.4404)	mem 17011MB
[2024-08-01 20:49:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:06:07 lr 0.000011	 wd 0.0500	time 0.2972 (0.3333)	loss 1.2389 (1.1595)	grad_norm 0.3675 (nan)	loss_scale 1024.0000 (1372.6424)	mem 17011MB
[2024-08-01 20:49:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:33 lr 0.000011	 wd 0.0500	time 0.3228 (0.3333)	loss 1.2939 (1.1618)	grad_norm 0.3472 (nan)	loss_scale 1024.0000 (1349.4151)	mem 17011MB
[2024-08-01 20:50:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:05:00 lr 0.000011	 wd 0.0500	time 0.3354 (0.3331)	loss 1.1219 (1.1624)	grad_norm 0.3781 (nan)	loss_scale 1024.0000 (1329.0893)	mem 17011MB
[2024-08-01 20:50:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:27 lr 0.000011	 wd 0.0500	time 0.2955 (0.3330)	loss 1.0817 (1.1615)	grad_norm 0.3863 (nan)	loss_scale 1024.0000 (1311.1534)	mem 17011MB
[2024-08-01 20:51:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:53 lr 0.000011	 wd 0.0500	time 0.3159 (0.3329)	loss 0.8641 (1.1604)	grad_norm 0.3315 (nan)	loss_scale 1024.0000 (1295.2093)	mem 17011MB
[2024-08-01 20:51:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:20 lr 0.000011	 wd 0.0500	time 0.3394 (0.3327)	loss 1.0317 (1.1611)	grad_norm 0.3431 (nan)	loss_scale 1024.0000 (1280.9427)	mem 17011MB
[2024-08-01 20:52:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:46 lr 0.000011	 wd 0.0500	time 0.3294 (0.3327)	loss 1.3561 (1.1618)	grad_norm 0.3337 (nan)	loss_scale 1024.0000 (1268.1019)	mem 17011MB
[2024-08-01 20:52:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:13 lr 0.000011	 wd 0.0500	time 0.3620 (0.3325)	loss 1.3598 (1.1626)	grad_norm 0.3424 (nan)	loss_scale 1024.0000 (1256.4836)	mem 17011MB
[2024-08-01 20:53:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:40 lr 0.000011	 wd 0.0500	time 0.3106 (0.3323)	loss 1.0176 (1.1622)	grad_norm 0.3644 (nan)	loss_scale 1024.0000 (1245.9209)	mem 17011MB
[2024-08-01 20:54:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:07 lr 0.000011	 wd 0.0500	time 0.3331 (0.3324)	loss 0.8955 (1.1604)	grad_norm 0.3941 (nan)	loss_scale 1024.0000 (1236.2764)	mem 17011MB
[2024-08-01 20:54:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:33 lr 0.000011	 wd 0.0500	time 0.3171 (0.3323)	loss 1.2234 (1.1594)	grad_norm 0.3472 (nan)	loss_scale 1024.0000 (1227.4352)	mem 17011MB
[2024-08-01 20:55:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2885 (0.3314)	loss 0.8441 (1.1597)	grad_norm 0.3619 (nan)	loss_scale 1024.0000 (1219.3011)	mem 17011MB
[2024-08-01 20:55:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 16 training takes 0:13:51
[2024-08-01 20:55:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.608 (11.608)	Loss 0.4944 (0.4944)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 20:55:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.048 Acc@5 97.850
[2024-08-01 20:55:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-01 20:55:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 20:55:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][0/2502]	eta 8:02:57 lr 0.000011	 wd 0.0500	time 11.5817 (11.5817)	loss 1.4000 (1.4000)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:56:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:17:26 lr 0.000011	 wd 0.0500	time 0.2960 (0.4357)	loss 1.2336 (1.1432)	grad_norm 0.3413 (0.3703)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:56:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:14:37 lr 0.000011	 wd 0.0500	time 0.3207 (0.3812)	loss 1.2805 (1.1426)	grad_norm 0.3529 (0.3799)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:57:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:13:19 lr 0.000011	 wd 0.0500	time 0.2993 (0.3633)	loss 1.2390 (1.1584)	grad_norm 0.4181 (0.3768)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:57:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:12:24 lr 0.000011	 wd 0.0500	time 0.3349 (0.3542)	loss 1.4153 (1.1564)	grad_norm 0.3595 (0.3746)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:58:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:11:38 lr 0.000010	 wd 0.0500	time 0.3537 (0.3491)	loss 1.3719 (1.1607)	grad_norm 0.3730 (0.3762)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:59:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:10:58 lr 0.000010	 wd 0.0500	time 0.3347 (0.3460)	loss 0.8134 (1.1618)	grad_norm 0.3569 (0.3743)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 20:59:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:10:18 lr 0.000010	 wd 0.0500	time 0.3489 (0.3433)	loss 1.2888 (1.1596)	grad_norm 0.3663 (0.3740)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:00:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:41 lr 0.000010	 wd 0.0500	time 0.3590 (0.3414)	loss 1.1931 (1.1578)	grad_norm 0.3720 (0.3734)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:00:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:09:04 lr 0.000010	 wd 0.0500	time 0.3260 (0.3400)	loss 1.2980 (1.1595)	grad_norm 0.3604 (0.3754)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:01:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:29 lr 0.000010	 wd 0.0500	time 0.3034 (0.3395)	loss 1.0394 (1.1589)	grad_norm 0.3681 (0.3765)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:01:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:53 lr 0.000010	 wd 0.0500	time 0.3203 (0.3380)	loss 1.0279 (1.1634)	grad_norm 0.3482 (0.3757)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:02:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:07:20 lr 0.000010	 wd 0.0500	time 0.2937 (0.3385)	loss 0.7638 (1.1634)	grad_norm 0.3488 (0.3764)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:02:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:45 lr 0.000010	 wd 0.0500	time 0.3185 (0.3375)	loss 1.4154 (1.1605)	grad_norm 0.3702 (0.3761)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:03:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:06:11 lr 0.000010	 wd 0.0500	time 0.2923 (0.3367)	loss 1.3314 (1.1606)	grad_norm 0.3361 (0.3751)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:03:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:36 lr 0.000010	 wd 0.0500	time 0.3026 (0.3361)	loss 1.4615 (1.1612)	grad_norm 0.3633 (0.3751)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:04:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:05:02 lr 0.000010	 wd 0.0500	time 0.3267 (0.3358)	loss 1.3884 (1.1610)	grad_norm 0.3449 (0.3757)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:05:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:29 lr 0.000010	 wd 0.0500	time 0.3444 (0.3355)	loss 1.5221 (1.1628)	grad_norm 0.3484 (0.3752)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:05:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:55 lr 0.000010	 wd 0.0500	time 0.3203 (0.3354)	loss 0.9744 (1.1622)	grad_norm 0.3750 (0.3750)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:06:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:21 lr 0.000010	 wd 0.0500	time 0.3346 (0.3352)	loss 1.2495 (1.1613)	grad_norm 0.3507 (0.3754)	loss_scale 1024.0000 (1024.0000)	mem 17011MB
[2024-08-01 21:06:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:48 lr 0.000010	 wd 0.0500	time 0.3053 (0.3349)	loss 1.2167 (1.1615)	grad_norm 0.3589 (0.3766)	loss_scale 2048.0000 (1037.3053)	mem 17011MB
[2024-08-01 21:07:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:14 lr 0.000010	 wd 0.0500	time 0.3398 (0.3350)	loss 1.4720 (1.1614)	grad_norm 0.3866 (0.3760)	loss_scale 2048.0000 (1085.4108)	mem 17011MB
[2024-08-01 21:07:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:41 lr 0.000010	 wd 0.0500	time 0.3333 (0.3346)	loss 1.3721 (1.1612)	grad_norm 0.3533 (0.3757)	loss_scale 2048.0000 (1129.1449)	mem 17011MB
[2024-08-01 21:08:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:07 lr 0.000010	 wd 0.0500	time 0.3030 (0.3345)	loss 0.8034 (1.1607)	grad_norm 0.3625 (0.3757)	loss_scale 2048.0000 (1169.0778)	mem 17011MB
[2024-08-01 21:08:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:34 lr 0.000010	 wd 0.0500	time 0.3328 (0.3347)	loss 0.9409 (1.1594)	grad_norm 0.3703 (0.3751)	loss_scale 2048.0000 (1205.6843)	mem 17011MB
[2024-08-01 21:09:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2888 (0.3337)	loss 1.4559 (1.1591)	grad_norm 0.3465 (0.3749)	loss_scale 2048.0000 (1239.3635)	mem 17011MB
[2024-08-01 21:09:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 17 training takes 0:13:57
[2024-08-01 21:09:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.038 (11.038)	Loss 0.4871 (0.4871)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 21:10:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.082 Acc@5 97.860
[2024-08-01 21:10:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 21:10:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 21:10:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:16:21 lr 0.000009	 wd 0.0500	time 11.9029 (11.9029)	loss 1.4462 (1.4462)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:10:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:17:27 lr 0.000009	 wd 0.0500	time 0.3145 (0.4363)	loss 1.4887 (1.1936)	grad_norm 0.3600 (0.3708)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:11:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:14:37 lr 0.000009	 wd 0.0500	time 0.3379 (0.3810)	loss 1.0323 (1.1851)	grad_norm 0.4793 (0.3708)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:11:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:13:20 lr 0.000009	 wd 0.0500	time 0.3284 (0.3636)	loss 1.5088 (1.1842)	grad_norm 0.3567 (0.3734)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:12:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:12:30 lr 0.000009	 wd 0.0500	time 0.2924 (0.3571)	loss 0.7529 (1.1829)	grad_norm 0.3683 (0.3731)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:12:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:11:46 lr 0.000009	 wd 0.0500	time 0.3244 (0.3528)	loss 1.2905 (1.1744)	grad_norm 0.3393 (0.3711)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:13:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:11:05 lr 0.000009	 wd 0.0500	time 0.2958 (0.3498)	loss 1.4935 (1.1669)	grad_norm 0.3683 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:14:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:10:24 lr 0.000009	 wd 0.0500	time 0.3149 (0.3465)	loss 1.4681 (1.1718)	grad_norm 0.4172 (0.3697)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:14:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:45 lr 0.000009	 wd 0.0500	time 0.2975 (0.3438)	loss 1.3235 (1.1749)	grad_norm 0.3460 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:15:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:09:10 lr 0.000009	 wd 0.0500	time 0.2834 (0.3434)	loss 0.9228 (1.1747)	grad_norm 0.3622 (0.3734)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:15:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:08:32 lr 0.000009	 wd 0.0500	time 0.3311 (0.3412)	loss 1.4298 (1.1704)	grad_norm 0.3516 (0.3741)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:16:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:57 lr 0.000009	 wd 0.0500	time 0.3321 (0.3404)	loss 1.0021 (1.1723)	grad_norm 0.3914 (0.3730)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:16:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:07:21 lr 0.000009	 wd 0.0500	time 0.3292 (0.3394)	loss 1.0546 (1.1655)	grad_norm 0.3549 (0.3720)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:17:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:48 lr 0.000009	 wd 0.0500	time 0.3450 (0.3401)	loss 1.4775 (1.1658)	grad_norm 0.3735 (0.3716)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:17:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:06:13 lr 0.000009	 wd 0.0500	time 0.2933 (0.3387)	loss 1.1597 (1.1660)	grad_norm 0.3411 (0.3712)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:18:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:39 lr 0.000009	 wd 0.0500	time 0.2935 (0.3383)	loss 1.3177 (1.1656)	grad_norm 0.3453 (0.3705)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:19:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:05:04 lr 0.000009	 wd 0.0500	time 0.2964 (0.3377)	loss 1.4572 (1.1675)	grad_norm 0.3967 (0.3703)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:19:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:30 lr 0.000009	 wd 0.0500	time 0.3184 (0.3370)	loss 1.3303 (1.1693)	grad_norm 0.3670 (0.3713)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:20:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:56 lr 0.000009	 wd 0.0500	time 0.3232 (0.3370)	loss 0.7229 (1.1697)	grad_norm 0.3637 (0.3719)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:20:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:22 lr 0.000009	 wd 0.0500	time 0.2930 (0.3371)	loss 1.4249 (1.1693)	grad_norm 0.3504 (0.3728)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:21:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:49 lr 0.000008	 wd 0.0500	time 0.3145 (0.3370)	loss 1.0717 (1.1689)	grad_norm 0.3510 (0.3740)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:21:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:15 lr 0.000008	 wd 0.0500	time 0.3065 (0.3366)	loss 1.3409 (1.1678)	grad_norm 0.3544 (0.3742)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:22:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:41 lr 0.000008	 wd 0.0500	time 0.2919 (0.3363)	loss 1.0265 (1.1680)	grad_norm 0.3435 (0.3737)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:22:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:07 lr 0.000008	 wd 0.0500	time 0.3426 (0.3360)	loss 1.3162 (1.1687)	grad_norm 0.3460 (0.3732)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:23:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:34 lr 0.000008	 wd 0.0500	time 0.3044 (0.3356)	loss 1.4304 (1.1693)	grad_norm 0.3756 (0.3728)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:23:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2875 (0.3348)	loss 1.2547 (1.1696)	grad_norm 0.3511 (0.3731)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:24:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 18 training takes 0:14:00
[2024-08-01 21:24:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.226 (11.226)	Loss 0.4985 (0.4985)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 21:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.104 Acc@5 97.820
[2024-08-01 21:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 21:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 21:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 21:24:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 21:24:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][0/2502]	eta 8:19:30 lr 0.000008	 wd 0.0500	time 11.9785 (11.9785)	loss 0.8034 (0.8034)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:25:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:15 lr 0.000008	 wd 0.0500	time 0.3250 (0.4310)	loss 1.5987 (1.2156)	grad_norm 0.3414 (0.3659)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:25:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:14:22 lr 0.000008	 wd 0.0500	time 0.3077 (0.3745)	loss 1.1755 (1.1854)	grad_norm 0.3803 (0.3681)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:26:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:13:07 lr 0.000008	 wd 0.0500	time 0.3112 (0.3576)	loss 1.4105 (1.1717)	grad_norm 0.3702 (0.3671)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:26:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:12:11 lr 0.000008	 wd 0.0500	time 0.3507 (0.3482)	loss 0.9666 (1.1651)	grad_norm 0.3521 (0.3686)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:27:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:26 lr 0.000008	 wd 0.0500	time 0.3069 (0.3427)	loss 1.4560 (1.1695)	grad_norm 0.3726 (0.3686)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:27:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:10:43 lr 0.000008	 wd 0.0500	time 0.3483 (0.3385)	loss 0.8455 (1.1590)	grad_norm 0.3724 (0.3714)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:28:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:10:06 lr 0.000008	 wd 0.0500	time 0.3257 (0.3365)	loss 1.2452 (1.1570)	grad_norm 0.3419 (0.3885)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:28:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:30 lr 0.000008	 wd 0.0500	time 0.3161 (0.3350)	loss 1.5208 (1.1587)	grad_norm 0.3646 (0.3863)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:29:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:08:55 lr 0.000008	 wd 0.0500	time 0.2952 (0.3341)	loss 1.2772 (1.1592)	grad_norm 0.6903 (0.3861)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:30:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:20 lr 0.000008	 wd 0.0500	time 0.2991 (0.3334)	loss 1.4516 (1.1607)	grad_norm 0.4021 (0.3844)	loss_scale 4096.0000 (2109.3786)	mem 17011MB
[2024-08-01 21:30:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:46 lr 0.000008	 wd 0.0500	time 0.2918 (0.3327)	loss 1.1764 (1.1597)	grad_norm 0.3750 (0.3830)	loss_scale 4096.0000 (2289.8165)	mem 17011MB
[2024-08-01 21:31:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:07:11 lr 0.000008	 wd 0.0500	time 0.3041 (0.3317)	loss 1.3225 (1.1593)	grad_norm 0.3679 (0.3814)	loss_scale 4096.0000 (2440.2065)	mem 17011MB
[2024-08-01 21:31:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:38 lr 0.000008	 wd 0.0500	time 0.3281 (0.3313)	loss 0.7831 (1.1583)	grad_norm 0.3736 (0.3815)	loss_scale 4096.0000 (2567.4773)	mem 17011MB
[2024-08-01 21:32:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:06:04 lr 0.000008	 wd 0.0500	time 0.3059 (0.3309)	loss 0.8130 (1.1575)	grad_norm 0.3989 (0.3807)	loss_scale 4096.0000 (2676.5796)	mem 17011MB
[2024-08-01 21:32:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:33 lr 0.000008	 wd 0.0500	time 0.3302 (0.3331)	loss 1.3691 (1.1587)	grad_norm 0.3503 (0.3804)	loss_scale 4096.0000 (2771.1446)	mem 17011MB
[2024-08-01 21:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:05:00 lr 0.000007	 wd 0.0500	time 0.3104 (0.3329)	loss 1.4195 (1.1605)	grad_norm 0.3556 (0.3799)	loss_scale 4096.0000 (2853.8963)	mem 17011MB
[2024-08-01 21:33:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:26 lr 0.000007	 wd 0.0500	time 0.3324 (0.3327)	loss 0.8974 (1.1611)	grad_norm 0.3917 (0.3791)	loss_scale 4096.0000 (2926.9183)	mem 17011MB
[2024-08-01 21:34:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:53 lr 0.000007	 wd 0.0500	time 0.3159 (0.3326)	loss 1.3243 (1.1635)	grad_norm 0.3754 (0.3782)	loss_scale 4096.0000 (2991.8312)	mem 17011MB
[2024-08-01 21:35:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:20 lr 0.000007	 wd 0.0500	time 0.3232 (0.3324)	loss 1.1078 (1.1625)	grad_norm 0.3734 (0.3774)	loss_scale 4096.0000 (3049.9148)	mem 17011MB
[2024-08-01 21:35:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:46 lr 0.000007	 wd 0.0500	time 0.2832 (0.3326)	loss 1.4804 (1.1636)	grad_norm 0.3612 (0.3776)	loss_scale 4096.0000 (3102.1929)	mem 17011MB
[2024-08-01 21:36:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:13 lr 0.000007	 wd 0.0500	time 0.3150 (0.3326)	loss 1.5787 (1.1650)	grad_norm 0.3471 (0.3768)	loss_scale 4096.0000 (3149.4945)	mem 17011MB
[2024-08-01 21:36:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:40 lr 0.000007	 wd 0.0500	time 0.3671 (0.3328)	loss 1.1435 (1.1641)	grad_norm 0.4075 (nan)	loss_scale 2048.0000 (3185.0541)	mem 17011MB
[2024-08-01 21:37:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:07 lr 0.000007	 wd 0.0500	time 0.3254 (0.3325)	loss 1.0213 (1.1643)	grad_norm 0.3636 (nan)	loss_scale 2048.0000 (3135.6384)	mem 17011MB
[2024-08-01 21:37:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:33 lr 0.000007	 wd 0.0500	time 0.2931 (0.3323)	loss 1.1127 (1.1634)	grad_norm 0.3811 (nan)	loss_scale 2048.0000 (3090.3390)	mem 17011MB
[2024-08-01 21:38:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2888 (0.3315)	loss 0.8082 (1.1637)	grad_norm 0.3966 (nan)	loss_scale 2048.0000 (3048.6621)	mem 17011MB
[2024-08-01 21:38:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 19 training takes 0:13:51
[2024-08-01 21:38:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.709 (12.709)	Loss 0.4802 (0.4802)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 21:38:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.084 Acc@5 97.892
[2024-08-01 21:38:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 21:38:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 21:39:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:14:36 lr 0.000007	 wd 0.0500	time 10.4222 (10.4222)	loss 1.5305 (1.5305)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:39:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:17:27 lr 0.000007	 wd 0.0500	time 0.3012 (0.4359)	loss 1.0760 (1.1735)	grad_norm 0.3703 (0.4247)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:40:06 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:14:34 lr 0.000007	 wd 0.0500	time 0.3370 (0.3801)	loss 1.3103 (1.1567)	grad_norm 0.3850 (0.4012)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:40:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:13:20 lr 0.000007	 wd 0.0500	time 0.3072 (0.3636)	loss 0.7029 (1.1690)	grad_norm 0.3793 (0.3901)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:41:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:12:30 lr 0.000007	 wd 0.0500	time 0.3069 (0.3571)	loss 1.3136 (1.1684)	grad_norm 0.3591 (0.3830)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:41:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:11:41 lr 0.000007	 wd 0.0500	time 0.3276 (0.3503)	loss 1.2810 (1.1663)	grad_norm 0.3681 (0.3806)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:42:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:11:04 lr 0.000007	 wd 0.0500	time 0.3084 (0.3492)	loss 1.2677 (1.1713)	grad_norm 0.3916 (0.3806)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:42:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:10:24 lr 0.000007	 wd 0.0500	time 0.3222 (0.3466)	loss 1.2609 (1.1695)	grad_norm 0.3555 (0.3794)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:43:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:45 lr 0.000007	 wd 0.0500	time 0.2885 (0.3441)	loss 1.1672 (1.1699)	grad_norm 0.3497 (0.3782)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:09:07 lr 0.000007	 wd 0.0500	time 0.3111 (0.3420)	loss 1.2540 (1.1692)	grad_norm 0.3645 (0.3764)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:44:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:08:31 lr 0.000007	 wd 0.0500	time 0.3272 (0.3408)	loss 1.3919 (1.1712)	grad_norm 0.3488 (0.3752)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:45:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:07:55 lr 0.000007	 wd 0.0500	time 0.3286 (0.3393)	loss 1.0734 (1.1690)	grad_norm 0.3682 (0.3740)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:45:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:07:19 lr 0.000006	 wd 0.0500	time 0.3288 (0.3379)	loss 1.2267 (1.1651)	grad_norm 0.3426 (0.3737)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:46:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:45 lr 0.000006	 wd 0.0500	time 0.2959 (0.3372)	loss 1.3500 (1.1646)	grad_norm 0.3595 (0.3734)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:46:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:06:11 lr 0.000006	 wd 0.0500	time 0.3033 (0.3367)	loss 1.0587 (1.1661)	grad_norm 0.5373 (0.3737)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:47:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:36 lr 0.000006	 wd 0.0500	time 0.3618 (0.3361)	loss 1.3322 (1.1688)	grad_norm 0.3634 (0.3753)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:47:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:05:02 lr 0.000006	 wd 0.0500	time 0.3183 (0.3353)	loss 1.2960 (1.1709)	grad_norm 0.3515 (0.3747)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:48:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:28 lr 0.000006	 wd 0.0500	time 0.3175 (0.3350)	loss 1.6157 (1.1722)	grad_norm 0.3516 (0.3799)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:48:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:55 lr 0.000006	 wd 0.0500	time 0.3655 (0.3349)	loss 0.7671 (1.1698)	grad_norm 0.3512 (0.3789)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:49:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:21 lr 0.000006	 wd 0.0500	time 0.3066 (0.3347)	loss 0.8428 (1.1711)	grad_norm 0.3557 (0.3783)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:49:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:47 lr 0.000006	 wd 0.0500	time 0.3077 (0.3343)	loss 1.1022 (1.1703)	grad_norm 0.4596 (0.3782)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:50:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:14 lr 0.000006	 wd 0.0500	time 0.2996 (0.3338)	loss 1.2453 (1.1702)	grad_norm 0.3479 (0.3791)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:51:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:40 lr 0.000006	 wd 0.0500	time 0.3175 (0.3336)	loss 1.3965 (1.1714)	grad_norm 0.3642 (0.3785)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:51:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:07 lr 0.000006	 wd 0.0500	time 0.3119 (0.3333)	loss 0.8713 (1.1725)	grad_norm 0.3610 (0.3779)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:52:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:33 lr 0.000006	 wd 0.0500	time 0.2915 (0.3331)	loss 0.7822 (1.1719)	grad_norm 0.3705 (0.3775)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:52:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.3225 (0.3322)	loss 1.0566 (1.1738)	grad_norm 0.3481 (0.3770)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:52:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 20 training takes 0:13:53
[2024-08-01 21:52:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_20.pth saving......
[2024-08-01 21:52:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_20.pth saved !!!
[2024-08-01 21:52:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.964 (11.964)	Loss 0.4905 (0.4905)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 17011MB
[2024-08-01 21:53:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.086 Acc@5 97.864
[2024-08-01 21:53:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 21:53:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 21:53:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][0/2502]	eta 8:21:33 lr 0.000006	 wd 0.0500	time 12.0278 (12.0278)	loss 0.9967 (0.9967)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:53:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:17:28 lr 0.000006	 wd 0.0500	time 0.2957 (0.4363)	loss 1.3759 (1.1945)	grad_norm 0.3928 (0.3699)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:54:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:14:32 lr 0.000006	 wd 0.0500	time 0.2926 (0.3792)	loss 1.3288 (1.1888)	grad_norm 0.4010 (0.3701)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:55:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:13:14 lr 0.000006	 wd 0.0500	time 0.3046 (0.3607)	loss 1.1937 (1.1783)	grad_norm 0.3860 (0.3700)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:55:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:12:17 lr 0.000006	 wd 0.0500	time 0.3171 (0.3509)	loss 1.3666 (1.1704)	grad_norm 0.3631 (0.3700)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:56:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:11:32 lr 0.000006	 wd 0.0500	time 0.3298 (0.3457)	loss 0.9046 (1.1653)	grad_norm 0.3578 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:56:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:50 lr 0.000006	 wd 0.0500	time 0.3115 (0.3422)	loss 1.3024 (1.1606)	grad_norm 0.3476 (0.3711)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:57:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:10:12 lr 0.000006	 wd 0.0500	time 0.3026 (0.3398)	loss 1.2328 (1.1581)	grad_norm 0.3698 (0.3702)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:57:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:35 lr 0.000006	 wd 0.0500	time 0.3637 (0.3382)	loss 1.2136 (1.1566)	grad_norm 0.3602 (0.3689)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:58:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:09:01 lr 0.000005	 wd 0.0500	time 0.2916 (0.3380)	loss 1.4374 (1.1598)	grad_norm 0.4334 (0.3692)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:58:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:08:26 lr 0.000005	 wd 0.0500	time 0.3128 (0.3371)	loss 1.4998 (1.1633)	grad_norm 0.3517 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:59:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:51 lr 0.000005	 wd 0.0500	time 0.3340 (0.3364)	loss 0.8051 (1.1665)	grad_norm 0.3507 (0.3698)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 21:59:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:07:17 lr 0.000005	 wd 0.0500	time 0.3267 (0.3357)	loss 1.4022 (1.1634)	grad_norm 0.3637 (0.3706)	loss_scale 4096.0000 (2068.4629)	mem 17011MB
[2024-08-01 22:00:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:43 lr 0.000005	 wd 0.0500	time 0.3185 (0.3354)	loss 1.1994 (1.1637)	grad_norm 0.3514 (0.3703)	loss_scale 4096.0000 (2224.3075)	mem 17011MB
[2024-08-01 22:01:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:06:09 lr 0.000005	 wd 0.0500	time 0.2949 (0.3353)	loss 0.8333 (1.1611)	grad_norm 0.3775 (0.3702)	loss_scale 4096.0000 (2357.9044)	mem 17011MB
[2024-08-01 22:01:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:35 lr 0.000005	 wd 0.0500	time 0.3064 (0.3350)	loss 0.8457 (1.1628)	grad_norm 0.3678 (0.3699)	loss_scale 4096.0000 (2473.7002)	mem 17011MB
[2024-08-01 22:02:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:05:01 lr 0.000005	 wd 0.0500	time 0.3458 (0.3345)	loss 1.0504 (1.1593)	grad_norm 0.3795 (0.3694)	loss_scale 4096.0000 (2575.0306)	mem 17011MB
[2024-08-01 22:02:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:28 lr 0.000005	 wd 0.0500	time 0.3348 (0.3344)	loss 1.3624 (1.1601)	grad_norm 0.3409 (0.3695)	loss_scale 4096.0000 (2664.4468)	mem 17011MB
[2024-08-01 22:03:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:54 lr 0.000005	 wd 0.0500	time 0.3372 (0.3341)	loss 0.7820 (1.1606)	grad_norm 0.3726 (0.3697)	loss_scale 4096.0000 (2743.9334)	mem 17011MB
[2024-08-01 22:03:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:21 lr 0.000005	 wd 0.0500	time 0.3073 (0.3340)	loss 0.8514 (1.1613)	grad_norm 0.3521 (0.3697)	loss_scale 4096.0000 (2815.0573)	mem 17011MB
[2024-08-01 22:04:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:47 lr 0.000005	 wd 0.0500	time 0.3411 (0.3344)	loss 1.0695 (1.1604)	grad_norm 0.3930 (0.3702)	loss_scale 4096.0000 (2879.0725)	mem 17011MB
[2024-08-01 22:04:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:14 lr 0.000005	 wd 0.0500	time 0.3125 (0.3342)	loss 1.1404 (1.1601)	grad_norm 0.3605 (0.3705)	loss_scale 4096.0000 (2936.9938)	mem 17011MB
[2024-08-01 22:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:40 lr 0.000005	 wd 0.0500	time 0.3279 (0.3340)	loss 1.4456 (1.1587)	grad_norm 0.3841 (0.3703)	loss_scale 4096.0000 (2989.6520)	mem 17011MB
[2024-08-01 22:06:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:07 lr 0.000005	 wd 0.0500	time 0.3081 (0.3340)	loss 1.5465 (1.1604)	grad_norm 0.3894 (0.3702)	loss_scale 4096.0000 (3037.7332)	mem 17011MB
[2024-08-01 22:06:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:34 lr 0.000005	 wd 0.0500	time 0.2931 (0.3337)	loss 1.2301 (1.1622)	grad_norm 0.3624 (0.3701)	loss_scale 4096.0000 (3081.8092)	mem 17011MB
[2024-08-01 22:07:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2875 (0.3328)	loss 1.4893 (1.1626)	grad_norm 0.3648 (0.3703)	loss_scale 4096.0000 (3122.3607)	mem 17011MB
[2024-08-01 22:07:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 21 training takes 0:13:55
[2024-08-01 22:07:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.165 (12.165)	Loss 0.5117 (0.5117)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 22:07:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.056 Acc@5 97.864
[2024-08-01 22:07:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 22:07:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 22:07:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][0/2502]	eta 8:24:54 lr 0.000005	 wd 0.0500	time 12.1080 (12.1080)	loss 0.9927 (0.9927)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:08:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:17:47 lr 0.000005	 wd 0.0500	time 0.3279 (0.4444)	loss 1.4739 (1.1786)	grad_norm 0.3896 (0.3962)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:08:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:14:49 lr 0.000005	 wd 0.0500	time 0.3070 (0.3865)	loss 0.7529 (1.1711)	grad_norm 0.3589 (0.3808)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:09:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:13:24 lr 0.000005	 wd 0.0500	time 0.3037 (0.3654)	loss 1.3820 (1.1769)	grad_norm 0.3673 (0.3981)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:09:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:12:29 lr 0.000005	 wd 0.0500	time 0.3217 (0.3566)	loss 0.7541 (1.1830)	grad_norm 0.3548 (0.3902)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:10:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:11:40 lr 0.000005	 wd 0.0500	time 0.3127 (0.3501)	loss 1.1951 (1.1806)	grad_norm 0.3511 (0.3845)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:11:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:10:57 lr 0.000005	 wd 0.0500	time 0.3185 (0.3455)	loss 0.8863 (1.1822)	grad_norm 0.3478 (0.3811)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:11:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:10:19 lr 0.000005	 wd 0.0500	time 0.3058 (0.3437)	loss 1.3515 (1.1767)	grad_norm 0.3685 (0.3787)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:12:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:41 lr 0.000004	 wd 0.0500	time 0.3021 (0.3416)	loss 0.9379 (1.1754)	grad_norm 0.3413 (0.3764)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:12:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:09:04 lr 0.000004	 wd 0.0500	time 0.2999 (0.3396)	loss 1.1112 (1.1742)	grad_norm 0.3760 (0.3754)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:13:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:08:28 lr 0.000004	 wd 0.0500	time 0.3112 (0.3385)	loss 1.2810 (1.1746)	grad_norm 0.3580 (0.3742)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:13:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:07:53 lr 0.000004	 wd 0.0500	time 0.2962 (0.3378)	loss 0.8153 (1.1717)	grad_norm 0.4443 (0.3740)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:14:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:07:19 lr 0.000004	 wd 0.0500	time 0.2963 (0.3372)	loss 1.3772 (1.1726)	grad_norm 0.3493 (0.3728)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:14:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:44 lr 0.000004	 wd 0.0500	time 0.3318 (0.3362)	loss 0.8552 (1.1663)	grad_norm 0.3406 (0.3719)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:15:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:06:09 lr 0.000004	 wd 0.0500	time 0.3024 (0.3354)	loss 1.0047 (1.1609)	grad_norm 0.3632 (0.3717)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:15:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:35 lr 0.000004	 wd 0.0500	time 0.2980 (0.3347)	loss 1.2107 (1.1605)	grad_norm 0.3375 (0.3731)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:16:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:05:01 lr 0.000004	 wd 0.0500	time 0.3077 (0.3341)	loss 1.5271 (1.1607)	grad_norm 0.3509 (0.3725)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:17:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:27 lr 0.000004	 wd 0.0500	time 0.3031 (0.3337)	loss 0.8994 (1.1589)	grad_norm 0.3610 (0.3723)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:17:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:54 lr 0.000004	 wd 0.0500	time 0.2933 (0.3336)	loss 1.4055 (1.1575)	grad_norm 0.4561 (0.3723)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:18:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:20 lr 0.000004	 wd 0.0500	time 0.3283 (0.3334)	loss 0.9402 (1.1583)	grad_norm 0.3472 (0.3719)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:18:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:47 lr 0.000004	 wd 0.0500	time 0.3045 (0.3333)	loss 0.9268 (1.1593)	grad_norm 0.3496 (0.3716)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:19:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:13 lr 0.000004	 wd 0.0500	time 0.3006 (0.3330)	loss 1.3054 (1.1608)	grad_norm 0.3544 (0.3718)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:19:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:40 lr 0.000004	 wd 0.0500	time 0.2980 (0.3326)	loss 1.0952 (1.1606)	grad_norm 0.3720 (0.3718)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:20:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:07 lr 0.000004	 wd 0.0500	time 0.3003 (0.3323)	loss 1.1432 (1.1607)	grad_norm 0.3429 (0.3716)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:20:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:33 lr 0.000004	 wd 0.0500	time 0.3128 (0.3320)	loss 1.3483 (1.1617)	grad_norm 0.3738 (0.3714)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:21:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2885 (0.3311)	loss 1.1151 (1.1614)	grad_norm 0.3511 (0.3713)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 22 training takes 0:13:51
[2024-08-01 22:21:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.529 (12.529)	Loss 0.4932 (0.4932)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 22:21:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.082 Acc@5 97.886
[2024-08-01 22:21:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 22:21:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 22:22:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:43:27 lr 0.000004	 wd 0.0500	time 11.1141 (11.1141)	loss 0.7676 (0.7676)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:22:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:17:12 lr 0.000004	 wd 0.0500	time 0.3134 (0.4298)	loss 1.4871 (1.1513)	grad_norm 0.3697 (0.3696)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:23:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:14:30 lr 0.000004	 wd 0.0500	time 0.3128 (0.3781)	loss 0.8532 (1.1562)	grad_norm 0.4788 (0.3674)	loss_scale 8192.0000 (4422.0498)	mem 17011MB
[2024-08-01 22:23:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:13:21 lr 0.000004	 wd 0.0500	time 0.3285 (0.3640)	loss 1.4309 (1.1470)	grad_norm 0.3664 (0.3671)	loss_scale 8192.0000 (5674.5249)	mem 17011MB
[2024-08-01 22:24:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:12:26 lr 0.000004	 wd 0.0500	time 0.3228 (0.3553)	loss 0.9039 (1.1525)	grad_norm 0.3634 (0.3669)	loss_scale 8192.0000 (6302.3242)	mem 17011MB
[2024-08-01 22:24:51 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:11:38 lr 0.000004	 wd 0.0500	time 0.3008 (0.3487)	loss 0.8185 (1.1556)	grad_norm 0.3377 (0.3680)	loss_scale 8192.0000 (6679.5050)	mem 17011MB
[2024-08-01 22:25:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:10:54 lr 0.000004	 wd 0.0500	time 0.2963 (0.3441)	loss 1.4407 (1.1548)	grad_norm 0.3790 (0.3678)	loss_scale 8192.0000 (6931.1681)	mem 17011MB
[2024-08-01 22:25:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:10:15 lr 0.000004	 wd 0.0500	time 0.2921 (0.3415)	loss 1.1406 (1.1581)	grad_norm 0.3987 (0.3683)	loss_scale 8192.0000 (7111.0300)	mem 17011MB
[2024-08-01 22:26:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:38 lr 0.000003	 wd 0.0500	time 0.3002 (0.3399)	loss 1.4517 (1.1598)	grad_norm 0.3447 (nan)	loss_scale 4096.0000 (6898.2572)	mem 17011MB
[2024-08-01 22:27:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:09:03 lr 0.000003	 wd 0.0500	time 0.3028 (0.3393)	loss 1.1457 (1.1604)	grad_norm 0.3497 (nan)	loss_scale 4096.0000 (6587.2408)	mem 17011MB
[2024-08-01 22:27:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:08:27 lr 0.000003	 wd 0.0500	time 0.3096 (0.3376)	loss 1.1332 (1.1560)	grad_norm 0.3265 (nan)	loss_scale 4096.0000 (6338.3656)	mem 17011MB
[2024-08-01 22:28:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:51 lr 0.000003	 wd 0.0500	time 0.3144 (0.3365)	loss 1.0370 (1.1588)	grad_norm 0.4190 (nan)	loss_scale 4096.0000 (6134.6994)	mem 17011MB
[2024-08-01 22:28:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:07:16 lr 0.000003	 wd 0.0500	time 0.3408 (0.3355)	loss 1.4065 (1.1618)	grad_norm 0.3880 (nan)	loss_scale 4096.0000 (5964.9492)	mem 17011MB
[2024-08-01 22:29:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:42 lr 0.000003	 wd 0.0500	time 0.3044 (0.3351)	loss 1.2609 (1.1625)	grad_norm 0.3550 (nan)	loss_scale 4096.0000 (5821.2944)	mem 17011MB
[2024-08-01 22:29:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:06:09 lr 0.000003	 wd 0.0500	time 0.3059 (0.3349)	loss 1.2736 (1.1627)	grad_norm 0.3801 (nan)	loss_scale 4096.0000 (5698.1470)	mem 17011MB
[2024-08-01 22:30:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:35 lr 0.000003	 wd 0.0500	time 0.3086 (0.3345)	loss 1.2921 (1.1611)	grad_norm 0.3620 (nan)	loss_scale 4096.0000 (5591.4084)	mem 17011MB
[2024-08-01 22:30:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:05:01 lr 0.000003	 wd 0.0500	time 0.3269 (0.3342)	loss 1.1470 (1.1644)	grad_norm 0.3415 (nan)	loss_scale 4096.0000 (5498.0037)	mem 17011MB
[2024-08-01 22:31:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:27 lr 0.000003	 wd 0.0500	time 0.3506 (0.3340)	loss 0.7175 (1.1618)	grad_norm 0.3642 (nan)	loss_scale 4096.0000 (5415.5814)	mem 17011MB
[2024-08-01 22:31:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:54 lr 0.000003	 wd 0.0500	time 0.3360 (0.3343)	loss 1.3144 (1.1610)	grad_norm 0.3797 (nan)	loss_scale 4096.0000 (5342.3120)	mem 17011MB
[2024-08-01 22:32:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:21 lr 0.000003	 wd 0.0500	time 0.2857 (0.3341)	loss 1.3032 (1.1604)	grad_norm 0.3476 (nan)	loss_scale 4096.0000 (5276.7512)	mem 17011MB
[2024-08-01 22:33:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:47 lr 0.000003	 wd 0.0500	time 0.3243 (0.3339)	loss 0.9406 (1.1600)	grad_norm 0.3663 (nan)	loss_scale 4096.0000 (5217.7431)	mem 17011MB
[2024-08-01 22:33:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:14 lr 0.000003	 wd 0.0500	time 0.3752 (0.3338)	loss 1.3918 (1.1604)	grad_norm 0.3761 (nan)	loss_scale 4096.0000 (5164.3522)	mem 17011MB
[2024-08-01 22:34:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:40 lr 0.000003	 wd 0.0500	time 0.3389 (0.3342)	loss 1.3259 (1.1589)	grad_norm 0.3743 (nan)	loss_scale 4096.0000 (5115.8128)	mem 17011MB
[2024-08-01 22:34:45 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:07 lr 0.000003	 wd 0.0500	time 0.3535 (0.3340)	loss 1.3749 (1.1590)	grad_norm 0.3552 (nan)	loss_scale 4096.0000 (5071.4924)	mem 17011MB
[2024-08-01 22:35:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:34 lr 0.000003	 wd 0.0500	time 0.3340 (0.3338)	loss 0.8194 (1.1588)	grad_norm 0.3557 (nan)	loss_scale 4096.0000 (5030.8638)	mem 17011MB
[2024-08-01 22:35:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2882 (0.3329)	loss 0.8786 (1.1580)	grad_norm 0.3524 (nan)	loss_scale 4096.0000 (4993.4842)	mem 17011MB
[2024-08-01 22:35:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 23 training takes 0:13:56
[2024-08-01 22:36:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.097 (12.097)	Loss 0.5107 (0.5107)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 17011MB
[2024-08-01 22:36:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.070 Acc@5 97.884
[2024-08-01 22:36:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 22:36:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 22:36:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:18:46 lr 0.000003	 wd 0.0500	time 11.9611 (11.9611)	loss 1.2268 (1.2268)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:37:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:17:28 lr 0.000003	 wd 0.0500	time 0.2837 (0.4364)	loss 0.9418 (1.1646)	grad_norm 0.3501 (0.3668)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:37:40 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:14:36 lr 0.000003	 wd 0.0500	time 0.3085 (0.3808)	loss 1.2294 (1.1542)	grad_norm 0.3400 (0.3694)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:38:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:13:14 lr 0.000003	 wd 0.0500	time 0.3000 (0.3610)	loss 1.3726 (1.1685)	grad_norm 0.3643 (0.3689)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:38:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:12:30 lr 0.000003	 wd 0.0500	time 0.3622 (0.3569)	loss 0.7248 (1.1681)	grad_norm 0.5069 (0.3739)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:39:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:11:53 lr 0.000003	 wd 0.0500	time 0.3180 (0.3564)	loss 1.4459 (1.1683)	grad_norm 0.3811 (0.3724)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:39:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:11:08 lr 0.000003	 wd 0.0500	time 0.3383 (0.3516)	loss 0.6863 (1.1635)	grad_norm 0.3930 (0.3713)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:40:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:10:27 lr 0.000003	 wd 0.0500	time 0.3145 (0.3483)	loss 1.2017 (1.1620)	grad_norm 0.3422 (0.3702)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:41:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:47 lr 0.000003	 wd 0.0500	time 0.3019 (0.3451)	loss 1.2260 (1.1641)	grad_norm 0.3630 (0.3698)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:41:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:09:09 lr 0.000003	 wd 0.0500	time 0.3118 (0.3427)	loss 0.9382 (1.1679)	grad_norm 0.3491 (0.3732)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:42:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:08:32 lr 0.000003	 wd 0.0500	time 0.3369 (0.3410)	loss 0.7991 (1.1649)	grad_norm 0.3657 (0.3730)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:42:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:07:56 lr 0.000003	 wd 0.0500	time 0.3203 (0.3401)	loss 0.8673 (1.1643)	grad_norm 0.3686 (0.3723)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:43:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:07:21 lr 0.000002	 wd 0.0500	time 0.3340 (0.3391)	loss 0.8956 (1.1623)	grad_norm 0.3943 (0.3733)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:43:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:46 lr 0.000002	 wd 0.0500	time 0.3165 (0.3384)	loss 0.7673 (1.1593)	grad_norm 0.3810 (0.3724)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:44:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:06:12 lr 0.000002	 wd 0.0500	time 0.3075 (0.3376)	loss 1.4648 (1.1604)	grad_norm 0.3565 (0.3720)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:44:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:37 lr 0.000002	 wd 0.0500	time 0.3015 (0.3367)	loss 1.2491 (1.1566)	grad_norm 0.3730 (0.3719)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:05:03 lr 0.000002	 wd 0.0500	time 0.3131 (0.3362)	loss 1.2966 (1.1556)	grad_norm 0.3648 (0.3714)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:45:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:29 lr 0.000002	 wd 0.0500	time 0.3061 (0.3358)	loss 1.5087 (1.1570)	grad_norm 0.3670 (0.3720)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:46:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:55 lr 0.000002	 wd 0.0500	time 0.3067 (0.3355)	loss 1.0957 (1.1570)	grad_norm 0.3925 (0.3722)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:47:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:22 lr 0.000002	 wd 0.0500	time 0.3251 (0.3358)	loss 0.9224 (1.1582)	grad_norm 0.3440 (0.3719)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:47:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:48 lr 0.000002	 wd 0.0500	time 0.2911 (0.3356)	loss 1.2857 (1.1582)	grad_norm 0.3876 (0.3719)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 22:48:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:14 lr 0.000002	 wd 0.0500	time 0.3411 (0.3352)	loss 1.0133 (1.1584)	grad_norm 0.3781 (nan)	loss_scale 2048.0000 (4004.3713)	mem 17011MB
[2024-08-01 22:48:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:41 lr 0.000002	 wd 0.0500	time 0.2990 (0.3350)	loss 1.0195 (1.1601)	grad_norm 0.3788 (nan)	loss_scale 2048.0000 (3915.4857)	mem 17011MB
[2024-08-01 22:49:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:07 lr 0.000002	 wd 0.0500	time 0.2945 (0.3347)	loss 1.3886 (1.1625)	grad_norm 0.3900 (nan)	loss_scale 2048.0000 (3834.3259)	mem 17011MB
[2024-08-01 22:49:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:34 lr 0.000002	 wd 0.0500	time 0.3087 (0.3345)	loss 1.2263 (1.1604)	grad_norm 0.3432 (nan)	loss_scale 2048.0000 (3759.9267)	mem 17011MB
[2024-08-01 22:50:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2887 (0.3337)	loss 0.9376 (1.1609)	grad_norm 0.3490 (nan)	loss_scale 2048.0000 (3691.4770)	mem 17011MB
[2024-08-01 22:50:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 24 training takes 0:13:58
[2024-08-01 22:50:34 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.520 (11.520)	Loss 0.4937 (0.4937)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 17011MB
[2024-08-01 22:50:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.064 Acc@5 97.852
[2024-08-01 22:50:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 22:50:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-01 22:51:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:03:36 lr 0.000002	 wd 0.0500	time 11.5972 (11.5972)	loss 1.2805 (1.2805)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:51:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:17:37 lr 0.000002	 wd 0.0500	time 0.3063 (0.4404)	loss 1.1978 (1.2185)	grad_norm 0.3567 (0.3700)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:52:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:14:39 lr 0.000002	 wd 0.0500	time 0.3187 (0.3820)	loss 1.4386 (1.1880)	grad_norm 0.3581 (0.3692)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:52:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:13:16 lr 0.000002	 wd 0.0500	time 0.2938 (0.3616)	loss 1.1364 (1.1715)	grad_norm 0.3742 (0.3734)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:53:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:12:19 lr 0.000002	 wd 0.0500	time 0.2966 (0.3518)	loss 1.3068 (1.1724)	grad_norm 0.3487 (0.3731)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:53:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:11:32 lr 0.000002	 wd 0.0500	time 0.3198 (0.3460)	loss 1.4284 (1.1706)	grad_norm 0.3712 (0.3752)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:54:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:10:57 lr 0.000002	 wd 0.0500	time 0.2920 (0.3459)	loss 0.8245 (1.1605)	grad_norm 0.3677 (0.3723)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:54:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:10:18 lr 0.000002	 wd 0.0500	time 0.3298 (0.3430)	loss 0.8033 (1.1623)	grad_norm 0.5631 (0.3772)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:55:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:40 lr 0.000002	 wd 0.0500	time 0.3334 (0.3411)	loss 1.3574 (1.1614)	grad_norm 0.3926 (0.3770)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:55:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:09:04 lr 0.000002	 wd 0.0500	time 0.3117 (0.3400)	loss 1.4403 (1.1612)	grad_norm 0.3520 (0.3763)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:56:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:08:29 lr 0.000002	 wd 0.0500	time 0.3429 (0.3389)	loss 1.5134 (1.1626)	grad_norm 0.3674 (0.3748)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:57:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:07:53 lr 0.000002	 wd 0.0500	time 0.3344 (0.3378)	loss 1.5511 (1.1635)	grad_norm 0.3740 (0.3736)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:57:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:07:18 lr 0.000002	 wd 0.0500	time 0.3304 (0.3369)	loss 0.9688 (1.1600)	grad_norm 0.3445 (0.3729)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:58:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:44 lr 0.000002	 wd 0.0500	time 0.3155 (0.3365)	loss 0.8327 (1.1589)	grad_norm 0.3772 (0.3726)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:58:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:06:10 lr 0.000002	 wd 0.0500	time 0.3106 (0.3359)	loss 0.8095 (1.1584)	grad_norm 0.3773 (0.3721)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:59:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:36 lr 0.000002	 wd 0.0500	time 0.3216 (0.3354)	loss 0.8642 (1.1586)	grad_norm 0.3718 (0.3714)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 22:59:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:05:02 lr 0.000002	 wd 0.0500	time 0.3416 (0.3352)	loss 0.8069 (1.1578)	grad_norm 0.3650 (0.3714)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:00:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:28 lr 0.000002	 wd 0.0500	time 0.3201 (0.3348)	loss 0.7459 (1.1554)	grad_norm 0.3607 (0.3712)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:00:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:54 lr 0.000002	 wd 0.0500	time 0.3183 (0.3346)	loss 0.8860 (1.1574)	grad_norm 0.3580 (0.3707)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:01:29 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:21 lr 0.000002	 wd 0.0500	time 0.3526 (0.3344)	loss 1.6562 (1.1597)	grad_norm 0.3593 (0.3707)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:02:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:47 lr 0.000002	 wd 0.0500	time 0.3194 (0.3341)	loss 0.9886 (1.1601)	grad_norm 0.3563 (0.3712)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:02:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:14 lr 0.000002	 wd 0.0500	time 0.3021 (0.3340)	loss 1.2901 (1.1613)	grad_norm 0.3504 (0.3712)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:03:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:40 lr 0.000001	 wd 0.0500	time 0.3300 (0.3341)	loss 1.1217 (1.1617)	grad_norm 0.3519 (0.3711)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:03:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:07 lr 0.000001	 wd 0.0500	time 0.3011 (0.3339)	loss 1.3448 (1.1611)	grad_norm 0.3951 (0.3708)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:04:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3217 (0.3339)	loss 1.3585 (1.1602)	grad_norm 0.3268 (0.3764)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:04:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2883 (0.3332)	loss 0.7462 (1.1590)	grad_norm 0.3892 (0.3766)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:04:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 25 training takes 0:13:57
[2024-08-01 23:05:02 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.113 (12.113)	Loss 0.5176 (0.5176)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 23:05:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.112 Acc@5 97.862
[2024-08-01 23:05:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 23:05:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.11%
[2024-08-01 23:05:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 23:05:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 23:05:35 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][0/2502]	eta 8:03:53 lr 0.000001	 wd 0.0500	time 11.6043 (11.6043)	loss 1.1734 (1.1734)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:06:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:17:07 lr 0.000001	 wd 0.0500	time 0.3296 (0.4279)	loss 1.3138 (1.1720)	grad_norm 0.3429 (0.3685)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:06:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:14:24 lr 0.000001	 wd 0.0500	time 0.3139 (0.3755)	loss 1.0891 (1.1714)	grad_norm 0.3875 (0.3671)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:07:12 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:13:11 lr 0.000001	 wd 0.0500	time 0.2909 (0.3596)	loss 0.9156 (1.1532)	grad_norm 0.3490 (0.3644)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:07:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:12:19 lr 0.000001	 wd 0.0500	time 0.2961 (0.3516)	loss 1.3537 (1.1516)	grad_norm 0.3715 (0.3652)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:08:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:11:33 lr 0.000001	 wd 0.0500	time 0.3310 (0.3465)	loss 1.3197 (1.1606)	grad_norm 0.3508 (0.3681)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:08:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:10:59 lr 0.000001	 wd 0.0500	time 0.3091 (0.3469)	loss 0.7917 (1.1646)	grad_norm 0.3669 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:09:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:10:20 lr 0.000001	 wd 0.0500	time 0.3277 (0.3441)	loss 1.4117 (1.1606)	grad_norm 0.3415 (0.3674)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:09:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:09:44 lr 0.000001	 wd 0.0500	time 0.2985 (0.3437)	loss 1.2440 (1.1646)	grad_norm 0.3727 (0.3675)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:10:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:09:07 lr 0.000001	 wd 0.0500	time 0.3181 (0.3418)	loss 1.2862 (1.1638)	grad_norm 0.3499 (0.3675)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:11:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:08:32 lr 0.000001	 wd 0.0500	time 0.3058 (0.3409)	loss 1.1303 (1.1627)	grad_norm 0.3609 (0.3673)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:11:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:07:57 lr 0.000001	 wd 0.0500	time 0.3031 (0.3405)	loss 0.9424 (1.1627)	grad_norm 0.3519 (0.3682)	loss_scale 4096.0000 (2230.2925)	mem 17011MB
[2024-08-01 23:12:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:07:21 lr 0.000001	 wd 0.0500	time 0.3130 (0.3394)	loss 1.0828 (1.1643)	grad_norm 0.3627 (0.3686)	loss_scale 4096.0000 (2385.6386)	mem 17011MB
[2024-08-01 23:12:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:46 lr 0.000001	 wd 0.0500	time 0.3368 (0.3383)	loss 1.2872 (1.1658)	grad_norm 0.3745 (0.3690)	loss_scale 4096.0000 (2517.1038)	mem 17011MB
[2024-08-01 23:13:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:06:12 lr 0.000001	 wd 0.0500	time 0.3212 (0.3379)	loss 1.3840 (1.1667)	grad_norm 0.3612 (0.3688)	loss_scale 4096.0000 (2629.8016)	mem 17011MB
[2024-08-01 23:13:50 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:38 lr 0.000001	 wd 0.0500	time 0.3530 (0.3374)	loss 1.4084 (1.1684)	grad_norm 0.3500 (0.3698)	loss_scale 4096.0000 (2727.4830)	mem 17011MB
[2024-08-01 23:14:23 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:05:03 lr 0.000001	 wd 0.0500	time 0.3007 (0.3370)	loss 1.2108 (1.1658)	grad_norm 0.3548 (0.3695)	loss_scale 4096.0000 (2812.9619)	mem 17011MB
[2024-08-01 23:14:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:31 lr 0.000001	 wd 0.0500	time 2.6819 (0.3381)	loss 1.2960 (1.1639)	grad_norm 0.3508 (0.3698)	loss_scale 4096.0000 (2888.3904)	mem 17011MB
[2024-08-01 23:15:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:56 lr 0.000001	 wd 0.0500	time 0.3040 (0.3376)	loss 0.8785 (1.1639)	grad_norm 0.3804 (0.3714)	loss_scale 4096.0000 (2955.4425)	mem 17011MB
[2024-08-01 23:16:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:22 lr 0.000001	 wd 0.0500	time 0.3285 (0.3370)	loss 1.2973 (1.1649)	grad_norm 0.3660 (0.3711)	loss_scale 4096.0000 (3015.4403)	mem 17011MB
[2024-08-01 23:16:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:49 lr 0.000001	 wd 0.0500	time 0.3332 (0.3367)	loss 1.1860 (1.1634)	grad_norm 0.3746 (0.3709)	loss_scale 4096.0000 (3069.4413)	mem 17011MB
[2024-08-01 23:17:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:15 lr 0.000001	 wd 0.0500	time 0.3283 (0.3365)	loss 1.3691 (1.1624)	grad_norm 0.3937 (0.3706)	loss_scale 4096.0000 (3118.3018)	mem 17011MB
[2024-08-01 23:17:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:41 lr 0.000001	 wd 0.0500	time 0.3239 (0.3361)	loss 1.2880 (1.1608)	grad_norm 0.3595 (0.3709)	loss_scale 4096.0000 (3162.7224)	mem 17011MB
[2024-08-01 23:18:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:07 lr 0.000001	 wd 0.0500	time 0.3078 (0.3358)	loss 1.2967 (1.1610)	grad_norm 0.3370 (0.3710)	loss_scale 4096.0000 (3203.2821)	mem 17011MB
[2024-08-01 23:18:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3099 (0.3355)	loss 0.7220 (1.1614)	grad_norm 0.3523 (0.3710)	loss_scale 4096.0000 (3240.4631)	mem 17011MB
[2024-08-01 23:19:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2887 (0.3346)	loss 0.9386 (1.1619)	grad_norm 0.3700 (0.3711)	loss_scale 4096.0000 (3274.6709)	mem 17011MB
[2024-08-01 23:19:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 26 training takes 0:14:02
[2024-08-01 23:19:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.553 (11.553)	Loss 0.5005 (0.5005)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 23:19:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.070 Acc@5 97.836
[2024-08-01 23:19:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 23:19:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.11%
[2024-08-01 23:20:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:07:06 lr 0.000001	 wd 0.0500	time 11.6812 (11.6812)	loss 1.4040 (1.4040)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:20:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:17:30 lr 0.000001	 wd 0.0500	time 0.2989 (0.4372)	loss 1.2850 (1.1524)	grad_norm 0.3322 (0.3642)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:21:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:14:43 lr 0.000001	 wd 0.0500	time 0.3363 (0.3838)	loss 1.3824 (1.1649)	grad_norm 0.3630 (0.3661)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:21:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:13:21 lr 0.000001	 wd 0.0500	time 0.3378 (0.3641)	loss 1.3488 (1.1645)	grad_norm 0.3526 (0.3714)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:22:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:12:26 lr 0.000001	 wd 0.0500	time 0.3902 (0.3550)	loss 1.0014 (1.1608)	grad_norm 0.3613 (0.3871)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:22:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:11:39 lr 0.000001	 wd 0.0500	time 0.2969 (0.3495)	loss 0.6810 (1.1619)	grad_norm 0.3803 (nan)	loss_scale 2048.0000 (3842.5549)	mem 17011MB
[2024-08-01 23:23:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:10:58 lr 0.000001	 wd 0.0500	time 0.3150 (0.3462)	loss 1.3233 (1.1618)	grad_norm 0.3676 (nan)	loss_scale 2048.0000 (3543.9601)	mem 17011MB
[2024-08-01 23:23:57 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:10:19 lr 0.000001	 wd 0.0500	time 0.2911 (0.3435)	loss 0.9860 (1.1635)	grad_norm 0.3818 (nan)	loss_scale 2048.0000 (3330.5563)	mem 17011MB
[2024-08-01 23:24:30 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:41 lr 0.000001	 wd 0.0500	time 0.3142 (0.3416)	loss 0.9299 (1.1651)	grad_norm 0.3812 (nan)	loss_scale 2048.0000 (3170.4370)	mem 17011MB
[2024-08-01 23:25:03 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:09:04 lr 0.000001	 wd 0.0500	time 0.3111 (0.3399)	loss 1.2430 (1.1631)	grad_norm 0.3504 (nan)	loss_scale 2048.0000 (3045.8602)	mem 17011MB
[2024-08-01 23:25:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:08:28 lr 0.000001	 wd 0.0500	time 0.3073 (0.3386)	loss 1.1350 (1.1598)	grad_norm 0.3327 (nan)	loss_scale 2048.0000 (2946.1738)	mem 17011MB
[2024-08-01 23:26:08 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:07:53 lr 0.000001	 wd 0.0500	time 0.3239 (0.3377)	loss 1.1422 (1.1620)	grad_norm 0.3569 (nan)	loss_scale 2048.0000 (2864.5958)	mem 17011MB
[2024-08-01 23:26:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:07:18 lr 0.000001	 wd 0.0500	time 0.2925 (0.3366)	loss 0.7223 (1.1626)	grad_norm 0.3518 (nan)	loss_scale 2048.0000 (2796.6028)	mem 17011MB
[2024-08-01 23:27:14 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:43 lr 0.000001	 wd 0.0500	time 0.3006 (0.3358)	loss 0.8060 (1.1594)	grad_norm 0.4523 (nan)	loss_scale 2048.0000 (2739.0623)	mem 17011MB
[2024-08-01 23:27:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:06:09 lr 0.000001	 wd 0.0500	time 0.2970 (0.3350)	loss 0.8295 (1.1607)	grad_norm 0.3464 (nan)	loss_scale 2048.0000 (2689.7359)	mem 17011MB
[2024-08-01 23:28:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:34 lr 0.000001	 wd 0.0500	time 0.2854 (0.3343)	loss 1.4584 (1.1616)	grad_norm 0.3499 (nan)	loss_scale 2048.0000 (2646.9820)	mem 17011MB
[2024-08-01 23:28:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:05:02 lr 0.000001	 wd 0.0500	time 0.3105 (0.3350)	loss 1.3190 (1.1633)	grad_norm 0.3548 (nan)	loss_scale 2048.0000 (2609.5690)	mem 17011MB
[2024-08-01 23:29:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:28 lr 0.000001	 wd 0.0500	time 0.3144 (0.3348)	loss 1.3570 (1.1650)	grad_norm 0.3642 (nan)	loss_scale 2048.0000 (2576.5550)	mem 17011MB
[2024-08-01 23:29:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:54 lr 0.000001	 wd 0.0500	time 0.3388 (0.3345)	loss 1.4083 (1.1646)	grad_norm 0.3511 (nan)	loss_scale 2048.0000 (2547.2071)	mem 17011MB
[2024-08-01 23:30:32 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:21 lr 0.000001	 wd 0.0500	time 0.3090 (0.3343)	loss 1.3840 (1.1654)	grad_norm 0.3558 (nan)	loss_scale 2048.0000 (2520.9469)	mem 17011MB
[2024-08-01 23:31:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:47 lr 0.000001	 wd 0.0500	time 0.3278 (0.3341)	loss 1.4038 (1.1661)	grad_norm 0.3475 (nan)	loss_scale 2048.0000 (2497.3113)	mem 17011MB
[2024-08-01 23:31:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:14 lr 0.000001	 wd 0.0500	time 0.3224 (0.3339)	loss 1.5069 (1.1652)	grad_norm 0.3669 (nan)	loss_scale 2048.0000 (2475.9257)	mem 17011MB
[2024-08-01 23:32:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:40 lr 0.000001	 wd 0.0500	time 0.3333 (0.3338)	loss 1.1906 (1.1665)	grad_norm 0.3611 (nan)	loss_scale 2048.0000 (2456.4834)	mem 17011MB
[2024-08-01 23:32:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:07 lr 0.000001	 wd 0.0500	time 0.3234 (0.3336)	loss 1.3032 (1.1662)	grad_norm 0.3486 (nan)	loss_scale 2048.0000 (2438.7310)	mem 17011MB
[2024-08-01 23:33:18 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:34 lr 0.000001	 wd 0.0500	time 0.3640 (0.3336)	loss 1.3581 (1.1679)	grad_norm 0.4152 (nan)	loss_scale 2048.0000 (2422.4573)	mem 17011MB
[2024-08-01 23:33:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2877 (0.3326)	loss 1.2591 (1.1670)	grad_norm 0.3882 (nan)	loss_scale 2048.0000 (2407.4850)	mem 17011MB
[2024-08-01 23:33:54 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 27 training takes 0:13:56
[2024-08-01 23:34:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 11.507 (11.507)	Loss 0.4871 (0.4871)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 17011MB
[2024-08-01 23:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.122 Acc@5 97.864
[2024-08-01 23:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 23:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.12%
[2024-08-01 23:34:26 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saving......
[2024-08-01 23:34:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_best.pth saved !!!
[2024-08-01 23:34:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:11:16 lr 0.000001	 wd 0.0500	time 10.3422 (10.3422)	loss 0.9670 (0.9670)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:35:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:16:55 lr 0.000000	 wd 0.0500	time 0.3124 (0.4228)	loss 0.9065 (1.2026)	grad_norm 0.4062 (0.3765)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:35:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:14:17 lr 0.000000	 wd 0.0500	time 0.3126 (0.3726)	loss 1.5418 (1.1891)	grad_norm 0.3603 (0.3840)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:36:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:13:07 lr 0.000000	 wd 0.0500	time 0.3538 (0.3575)	loss 1.4351 (1.1830)	grad_norm 0.3759 (0.3826)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:36:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:12:13 lr 0.000000	 wd 0.0500	time 0.2949 (0.3489)	loss 0.7398 (1.1743)	grad_norm 0.3461 (0.3809)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:37:20 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:11:30 lr 0.000000	 wd 0.0500	time 0.3370 (0.3451)	loss 0.7958 (1.1792)	grad_norm 0.3492 (0.3785)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:37:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:10:50 lr 0.000000	 wd 0.0500	time 0.3204 (0.3418)	loss 0.8894 (1.1718)	grad_norm 0.3828 (0.3757)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:38:27 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:10:16 lr 0.000000	 wd 0.0500	time 0.3424 (0.3420)	loss 1.2822 (1.1687)	grad_norm 0.3744 (0.3741)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:39:00 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:39 lr 0.000000	 wd 0.0500	time 0.3731 (0.3407)	loss 0.8420 (1.1691)	grad_norm 0.3568 (0.3728)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:39:33 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:09:03 lr 0.000000	 wd 0.0500	time 0.3525 (0.3393)	loss 0.8551 (1.1655)	grad_norm 0.3688 (0.3721)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:40:05 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:08:27 lr 0.000000	 wd 0.0500	time 0.3136 (0.3379)	loss 1.0085 (1.1671)	grad_norm 0.4057 (0.3739)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:40:38 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:07:52 lr 0.000000	 wd 0.0500	time 0.3293 (0.3370)	loss 1.2645 (1.1673)	grad_norm 0.3594 (0.3734)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:41:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:07:17 lr 0.000000	 wd 0.0500	time 0.3313 (0.3362)	loss 1.4360 (1.1673)	grad_norm 0.4927 (0.3729)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:41:44 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:43 lr 0.000000	 wd 0.0500	time 0.3240 (0.3355)	loss 1.5128 (1.1686)	grad_norm 0.3643 (0.3722)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:42:17 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:06:09 lr 0.000000	 wd 0.0500	time 0.3381 (0.3351)	loss 0.9541 (1.1680)	grad_norm 0.3537 (0.3724)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:42:49 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:35 lr 0.000000	 wd 0.0500	time 0.2903 (0.3345)	loss 1.1849 (1.1664)	grad_norm 0.3539 (0.3717)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:43:22 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:05:01 lr 0.000000	 wd 0.0500	time 0.3634 (0.3341)	loss 1.3911 (1.1692)	grad_norm 0.3892 (0.3732)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:43:55 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:27 lr 0.000000	 wd 0.0500	time 0.3844 (0.3338)	loss 1.2452 (1.1687)	grad_norm 0.3837 (0.3727)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:44:28 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:54 lr 0.000000	 wd 0.0500	time 0.3040 (0.3337)	loss 0.7994 (1.1683)	grad_norm 0.3554 (0.3740)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:45:01 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:20 lr 0.000000	 wd 0.0500	time 0.3206 (0.3335)	loss 1.1030 (1.1659)	grad_norm 0.3547 (0.3738)	loss_scale 2048.0000 (2048.0000)	mem 17011MB
[2024-08-01 23:45:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:47 lr 0.000000	 wd 0.0500	time 0.3089 (0.3340)	loss 1.1153 (1.1659)	grad_norm 0.3762 (0.3736)	loss_scale 4096.0000 (2113.5032)	mem 17011MB
[2024-08-01 23:46:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:14 lr 0.000000	 wd 0.0500	time 0.3016 (0.3340)	loss 0.9785 (1.1659)	grad_norm 5.0198 (0.3782)	loss_scale 4096.0000 (2207.8629)	mem 17011MB
[2024-08-01 23:46:42 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:40 lr 0.000000	 wd 0.0500	time 0.3035 (0.3339)	loss 1.2760 (1.1649)	grad_norm 0.3887 (0.3779)	loss_scale 4096.0000 (2293.6483)	mem 17011MB
[2024-08-01 23:47:15 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:07 lr 0.000000	 wd 0.0500	time 0.3376 (0.3337)	loss 1.2412 (1.1649)	grad_norm 0.3681 (0.3781)	loss_scale 4096.0000 (2371.9774)	mem 17011MB
[2024-08-01 23:47:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:34 lr 0.000000	 wd 0.0500	time 0.3319 (0.3335)	loss 1.5131 (1.1635)	grad_norm 0.3377 (0.3779)	loss_scale 4096.0000 (2443.7818)	mem 17011MB
[2024-08-01 23:48:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2886 (0.3327)	loss 1.0792 (1.1628)	grad_norm 0.3385 (0.3774)	loss_scale 4096.0000 (2509.8441)	mem 17011MB
[2024-08-01 23:48:24 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 28 training takes 0:13:56
[2024-08-01 23:48:36 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.464 (12.464)	Loss 0.4951 (0.4951)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 17011MB
[2024-08-01 23:48:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.084 Acc@5 97.870
[2024-08-01 23:48:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 23:48:56 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.12%
[2024-08-01 23:49:07 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:29:40 lr 0.000000	 wd 0.0500	time 10.7835 (10.7835)	loss 1.2453 (1.2453)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:49:39 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:17:07 lr 0.000000	 wd 0.0500	time 0.2945 (0.4276)	loss 1.4188 (1.1748)	grad_norm 0.4269 (0.3853)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:50:11 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:14:22 lr 0.000000	 wd 0.0500	time 0.3160 (0.3747)	loss 0.8197 (1.1725)	grad_norm 0.3814 (0.3755)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:50:43 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:13:07 lr 0.000000	 wd 0.0500	time 0.3044 (0.3577)	loss 0.8808 (1.1691)	grad_norm 0.3695 (0.3735)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:51:16 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:12:13 lr 0.000000	 wd 0.0500	time 0.3323 (0.3490)	loss 1.1282 (1.1628)	grad_norm 0.3664 (0.3760)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:51:48 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:11:28 lr 0.000000	 wd 0.0500	time 0.2911 (0.3438)	loss 1.3051 (1.1582)	grad_norm 0.3701 (0.3908)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:52:21 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:10:48 lr 0.000000	 wd 0.0500	time 0.3129 (0.3409)	loss 0.7346 (1.1614)	grad_norm 0.3566 (0.3867)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:52:53 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:10:09 lr 0.000000	 wd 0.0500	time 0.3125 (0.3384)	loss 1.1302 (1.1621)	grad_norm 0.3662 (0.3849)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:53:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:32 lr 0.000000	 wd 0.0500	time 0.3056 (0.3366)	loss 0.7683 (1.1629)	grad_norm 0.3465 (0.3847)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:08:58 lr 0.000000	 wd 0.0500	time 0.3104 (0.3360)	loss 1.4914 (1.1643)	grad_norm 0.3697 (0.3836)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:54:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:08:23 lr 0.000000	 wd 0.0500	time 0.3108 (0.3351)	loss 1.4481 (1.1651)	grad_norm 0.4456 (0.3834)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:55:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:49 lr 0.000000	 wd 0.0500	time 0.3346 (0.3346)	loss 1.2369 (1.1649)	grad_norm 0.3457 (0.3891)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:55:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:07:14 lr 0.000000	 wd 0.0500	time 0.3066 (0.3337)	loss 1.0237 (1.1626)	grad_norm 0.3495 (0.3869)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:56:09 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:39 lr 0.000000	 wd 0.0500	time 0.2957 (0.3327)	loss 1.1411 (1.1660)	grad_norm 0.3895 (0.3859)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:56:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:06:05 lr 0.000000	 wd 0.0500	time 0.3239 (0.3320)	loss 1.1181 (1.1611)	grad_norm 0.3729 (0.3845)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:57:13 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:32 lr 0.000000	 wd 0.0500	time 0.3089 (0.3315)	loss 1.0303 (1.1613)	grad_norm 0.3573 (0.3839)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:57:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:04:58 lr 0.000000	 wd 0.0500	time 0.2919 (0.3311)	loss 0.8189 (1.1634)	grad_norm 0.3595 (0.3829)	loss_scale 4096.0000 (4096.0000)	mem 17011MB
[2024-08-01 23:58:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:25 lr 0.000000	 wd 0.0500	time 0.3297 (0.3311)	loss 1.3606 (1.1660)	grad_norm 0.3632 (nan)	loss_scale 2048.0000 (3994.8642)	mem 17011MB
[2024-08-01 23:58:52 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:52 lr 0.000000	 wd 0.0500	time 0.2977 (0.3311)	loss 1.1813 (1.1644)	grad_norm 1.0802 (nan)	loss_scale 2048.0000 (3886.7651)	mem 17011MB
[2024-08-01 23:59:25 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:19 lr 0.000000	 wd 0.0500	time 0.3481 (0.3310)	loss 0.7601 (1.1647)	grad_norm 0.3694 (nan)	loss_scale 2048.0000 (3790.0389)	mem 17011MB
[2024-08-01 23:59:58 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:46 lr 0.000000	 wd 0.0500	time 0.3147 (0.3309)	loss 0.9071 (1.1665)	grad_norm 0.3621 (nan)	loss_scale 2048.0000 (3702.9805)	mem 17011MB
[2024-08-02 00:00:31 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:13 lr 0.000000	 wd 0.0500	time 0.3416 (0.3309)	loss 0.7601 (1.1674)	grad_norm 0.4515 (nan)	loss_scale 2048.0000 (3624.2094)	mem 17011MB
[2024-08-02 00:01:04 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:39 lr 0.000000	 wd 0.0500	time 0.3339 (0.3309)	loss 0.8388 (1.1680)	grad_norm 0.4122 (nan)	loss_scale 2048.0000 (3552.5961)	mem 17011MB
[2024-08-02 00:01:37 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:06 lr 0.000000	 wd 0.0500	time 0.3107 (0.3309)	loss 0.8567 (1.1651)	grad_norm 0.3726 (nan)	loss_scale 2048.0000 (3487.2073)	mem 17011MB
[2024-08-02 00:02:10 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:33 lr 0.000000	 wd 0.0500	time 0.3245 (0.3307)	loss 1.2653 (1.1651)	grad_norm 0.3842 (nan)	loss_scale 2048.0000 (3427.2653)	mem 17011MB
[2024-08-02 00:02:41 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.3189 (0.3299)	loss 1.3242 (1.1644)	grad_norm 0.3445 (nan)	loss_scale 2048.0000 (3372.1168)	mem 17011MB
[2024-08-02 00:02:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 249): INFO EPOCH 29 training takes 0:13:50
[2024-08-02 00:02:46 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_29.pth saving......
[2024-08-02 00:02:47 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2/diffusion_ft_adapter_smt_l_sequence_stage2/ckpt_epoch_29.pth saved !!!
[2024-08-02 00:02:59 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 289): INFO Test: [0/98]	Time 12.164 (12.164)	Loss 0.4849 (0.4849)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 17011MB
[2024-08-02 00:03:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 296): INFO  * Acc@1 86.084 Acc@5 97.884
[2024-08-02 00:03:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 00:03:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 182): INFO Max accuracy: 86.12%
[2024-08-02 00:03:19 adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_stage2] (main.py 189): INFO Training time 7:12:04
