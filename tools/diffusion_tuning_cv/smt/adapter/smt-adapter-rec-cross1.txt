[2024-08-02 15:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/config.json
[2024-08-02 15:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: part1
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_step_cross1
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-08-02 15:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_step_cross1.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_step_cross1", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-08-02 15:33:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1
[2024-08-02 15:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-08-02 15:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 113): INFO number of params: 2466664
[2024-08-02 15:33:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1, ignoring auto resume
[2024-08-02 15:33:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth for fine-tuning......
[2024-08-02 15:33:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 127): WARNING <All keys matched successfully>
[2024-08-02 15:33:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_sequence_cross0/diffusion_ft_adapter_smt_l_sequence_cross0/ckpt_epoch_best.pth'
[2024-08-02 15:33:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 15.497 (15.497)	Loss 0.5083 (0.5083)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2331MB
[2024-08-02 15:34:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.902 Acc@5 97.810
[2024-08-02 15:34:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 162): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 15:34:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 168): INFO Start training
[2024-08-02 15:34:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][0/2502]	eta 8:41:40 lr 0.000000	 wd 0.0500	time 12.5104 (12.5104)	loss 1.5298 (1.5298)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 15606MB
[2024-08-02 15:34:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:17:21 lr 0.000000	 wd 0.0500	time 0.2888 (0.4336)	loss 1.2843 (1.2079)	grad_norm 0.3269 (nan)	loss_scale 16384.0000 (28550.3366)	mem 15606MB
[2024-08-02 15:35:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:14:25 lr 0.000000	 wd 0.0500	time 0.2795 (0.3759)	loss 1.0248 (1.1924)	grad_norm 0.3208 (nan)	loss_scale 8192.0000 (22171.3831)	mem 15606MB
[2024-08-02 15:35:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:12:59 lr 0.000000	 wd 0.0500	time 0.3224 (0.3541)	loss 0.9031 (1.1559)	grad_norm 0.3409 (nan)	loss_scale 8192.0000 (17527.0698)	mem 15606MB
[2024-08-02 15:36:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:11:59 lr 0.000001	 wd 0.0500	time 0.2842 (0.3425)	loss 0.9746 (1.1634)	grad_norm 0.3289 (nan)	loss_scale 8192.0000 (15199.1222)	mem 15606MB
[2024-08-02 15:36:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:11:13 lr 0.000001	 wd 0.0500	time 0.3207 (0.3364)	loss 1.1489 (1.1682)	grad_norm 0.3053 (nan)	loss_scale 4096.0000 (13751.4411)	mem 15606MB
[2024-08-02 15:37:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:10:31 lr 0.000001	 wd 0.0500	time 0.3122 (0.3318)	loss 1.3134 (1.1680)	grad_norm 0.3212 (nan)	loss_scale 4096.0000 (12144.8785)	mem 15606MB
[2024-08-02 15:37:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:09:51 lr 0.000001	 wd 0.0500	time 0.2882 (0.3281)	loss 1.2335 (1.1700)	grad_norm 0.3316 (nan)	loss_scale 4096.0000 (10996.6790)	mem 15606MB
[2024-08-02 15:38:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:09:13 lr 0.000001	 wd 0.0500	time 0.2783 (0.3254)	loss 1.0592 (1.1692)	grad_norm 0.3191 (nan)	loss_scale 4096.0000 (10135.1710)	mem 15606MB
[2024-08-02 15:38:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:08:38 lr 0.000001	 wd 0.0500	time 0.3414 (0.3235)	loss 1.3676 (1.1689)	grad_norm 0.3228 (nan)	loss_scale 4096.0000 (9464.8968)	mem 15606MB
[2024-08-02 15:39:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:08:03 lr 0.000002	 wd 0.0500	time 0.3114 (0.3220)	loss 1.4366 (1.1666)	grad_norm 0.3287 (nan)	loss_scale 4096.0000 (8928.5435)	mem 15606MB
[2024-08-02 15:39:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:07:29 lr 0.000002	 wd 0.0500	time 0.3077 (0.3204)	loss 1.2339 (1.1677)	grad_norm 0.3540 (nan)	loss_scale 4096.0000 (8489.6203)	mem 15606MB
[2024-08-02 15:40:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:06:55 lr 0.000002	 wd 0.0500	time 0.3133 (0.3193)	loss 0.9220 (1.1674)	grad_norm 0.3223 (nan)	loss_scale 4096.0000 (8123.7902)	mem 15606MB
[2024-08-02 15:41:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:06:22 lr 0.000002	 wd 0.0500	time 0.3094 (0.3184)	loss 1.2313 (1.1691)	grad_norm 0.3189 (nan)	loss_scale 4096.0000 (7814.1983)	mem 15606MB
[2024-08-02 15:41:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:05:49 lr 0.000002	 wd 0.0500	time 0.2923 (0.3172)	loss 1.4352 (1.1709)	grad_norm 0.3207 (nan)	loss_scale 4096.0000 (7548.8023)	mem 15606MB
[2024-08-02 15:42:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:05:17 lr 0.000002	 wd 0.0500	time 0.3200 (0.3164)	loss 1.1848 (1.1749)	grad_norm 0.3247 (nan)	loss_scale 4096.0000 (7318.7688)	mem 15606MB
[2024-08-02 15:42:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:04:44 lr 0.000003	 wd 0.0500	time 0.3242 (0.3158)	loss 1.2171 (1.1721)	grad_norm 0.3171 (nan)	loss_scale 4096.0000 (7117.4716)	mem 15606MB
[2024-08-02 15:43:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:04:12 lr 0.000003	 wd 0.0500	time 0.2893 (0.3153)	loss 0.8872 (1.1715)	grad_norm 0.3479 (nan)	loss_scale 4096.0000 (6939.8424)	mem 15606MB
[2024-08-02 15:43:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:03:41 lr 0.000003	 wd 0.0500	time 0.2950 (0.3152)	loss 1.1223 (1.1716)	grad_norm 0.3174 (nan)	loss_scale 4096.0000 (6781.9389)	mem 15606MB
[2024-08-02 15:44:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:03:09 lr 0.000003	 wd 0.0500	time 0.2858 (0.3149)	loss 1.4163 (1.1711)	grad_norm 0.3211 (nan)	loss_scale 4096.0000 (6640.6481)	mem 15606MB
[2024-08-02 15:44:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:02:37 lr 0.000003	 wd 0.0500	time 0.3268 (0.3147)	loss 0.8451 (1.1701)	grad_norm 0.3248 (nan)	loss_scale 4096.0000 (6513.4793)	mem 15606MB
[2024-08-02 15:45:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:02:06 lr 0.000003	 wd 0.0500	time 0.3219 (0.3144)	loss 1.0780 (1.1690)	grad_norm 0.3366 (nan)	loss_scale 4096.0000 (6398.4160)	mem 15606MB
[2024-08-02 15:45:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:01:34 lr 0.000004	 wd 0.0500	time 0.3218 (0.3143)	loss 1.5233 (1.1685)	grad_norm 0.3309 (nan)	loss_scale 4096.0000 (6293.8083)	mem 15606MB
[2024-08-02 15:46:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:01:03 lr 0.000004	 wd 0.0500	time 0.3223 (0.3140)	loss 1.3319 (1.1687)	grad_norm 0.3346 (nan)	loss_scale 4096.0000 (6198.2929)	mem 15606MB
[2024-08-02 15:46:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:32 lr 0.000004	 wd 0.0500	time 0.3007 (0.3138)	loss 1.3102 (1.1686)	grad_norm 0.3179 (nan)	loss_scale 4096.0000 (6110.7339)	mem 15606MB
[2024-08-02 15:47:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2735 (0.3129)	loss 0.8822 (1.1668)	grad_norm 0.3278 (nan)	loss_scale 4096.0000 (6030.1767)	mem 15606MB
[2024-08-02 15:47:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 0 training takes 0:13:05
[2024-08-02 15:47:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_0.pth saving......
[2024-08-02 15:47:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_0.pth saved !!!
[2024-08-02 15:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.153 (11.153)	Loss 0.5278 (0.5278)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 15:47:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.904 Acc@5 97.814
[2024-08-02 15:47:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 15:47:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 85.90%
[2024-08-02 15:47:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 15:47:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 15:47:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][0/2502]	eta 8:18:29 lr 0.000004	 wd 0.0500	time 11.9543 (11.9543)	loss 1.1110 (1.1110)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:48:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:16:47 lr 0.000004	 wd 0.0500	time 0.3047 (0.4194)	loss 1.1743 (1.1493)	grad_norm 0.3210 (0.3288)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:48:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:13:54 lr 0.000004	 wd 0.0500	time 0.2859 (0.3625)	loss 0.7691 (1.1662)	grad_norm 0.3233 (0.3293)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:49:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:12:36 lr 0.000004	 wd 0.0500	time 0.3031 (0.3437)	loss 0.8932 (1.1658)	grad_norm 0.3258 (0.3278)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:49:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:11:43 lr 0.000005	 wd 0.0500	time 0.2868 (0.3346)	loss 0.9746 (1.1805)	grad_norm 0.3464 (0.3310)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:50:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:10:58 lr 0.000005	 wd 0.0500	time 0.3051 (0.3287)	loss 1.4550 (1.1778)	grad_norm 0.3197 (0.3326)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:50:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:10:19 lr 0.000005	 wd 0.0500	time 0.2939 (0.3255)	loss 1.4872 (1.1766)	grad_norm 0.3240 (0.3321)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:51:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:09:41 lr 0.000005	 wd 0.0500	time 0.2794 (0.3227)	loss 0.8718 (1.1735)	grad_norm 0.3119 (0.3332)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:51:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:09:06 lr 0.000005	 wd 0.0500	time 0.3065 (0.3210)	loss 1.4244 (1.1727)	grad_norm 0.3078 (0.3328)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:52:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:08:32 lr 0.000005	 wd 0.0500	time 0.2814 (0.3197)	loss 1.1673 (1.1717)	grad_norm 0.3521 (0.3326)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:52:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:07:57 lr 0.000006	 wd 0.0500	time 0.3012 (0.3182)	loss 1.3617 (1.1705)	grad_norm 0.3748 (0.3332)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:53:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:07:24 lr 0.000006	 wd 0.0500	time 0.3363 (0.3173)	loss 1.4339 (1.1690)	grad_norm 0.3222 (0.3331)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 15:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:06:51 lr 0.000006	 wd 0.0500	time 0.2792 (0.3163)	loss 0.9723 (1.1668)	grad_norm 0.3185 (nan)	loss_scale 2048.0000 (3935.7069)	mem 15606MB
[2024-08-02 15:54:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:06:19 lr 0.000006	 wd 0.0500	time 0.2854 (0.3158)	loss 1.5244 (1.1680)	grad_norm 0.3607 (nan)	loss_scale 2048.0000 (3790.6103)	mem 15606MB
[2024-08-02 15:55:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:05:47 lr 0.000006	 wd 0.0500	time 0.3206 (0.3151)	loss 0.9386 (1.1678)	grad_norm 0.3316 (nan)	loss_scale 2048.0000 (3666.2270)	mem 15606MB
[2024-08-02 15:55:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:05:15 lr 0.000006	 wd 0.0500	time 0.3181 (0.3148)	loss 1.4095 (1.1685)	grad_norm 0.3167 (nan)	loss_scale 2048.0000 (3558.4171)	mem 15606MB
[2024-08-02 15:56:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:04:43 lr 0.000007	 wd 0.0500	time 0.3071 (0.3146)	loss 1.2917 (1.1702)	grad_norm 0.3239 (nan)	loss_scale 2048.0000 (3464.0750)	mem 15606MB
[2024-08-02 15:56:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:04:12 lr 0.000007	 wd 0.0500	time 0.2802 (0.3143)	loss 0.9959 (1.1704)	grad_norm 0.3419 (nan)	loss_scale 2048.0000 (3380.8254)	mem 15606MB
[2024-08-02 15:57:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:03:40 lr 0.000007	 wd 0.0500	time 0.2980 (0.3139)	loss 1.6954 (1.1698)	grad_norm 0.3272 (nan)	loss_scale 2048.0000 (3306.8207)	mem 15606MB
[2024-08-02 15:57:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:03:08 lr 0.000007	 wd 0.0500	time 0.3153 (0.3136)	loss 1.4442 (1.1711)	grad_norm 0.3360 (nan)	loss_scale 2048.0000 (3240.6018)	mem 15606MB
[2024-08-02 15:58:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:02:37 lr 0.000007	 wd 0.0500	time 0.3256 (0.3134)	loss 1.3994 (1.1716)	grad_norm 0.3407 (nan)	loss_scale 2048.0000 (3181.0015)	mem 15606MB
[2024-08-02 15:58:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:02:06 lr 0.000007	 wd 0.0500	time 0.2969 (0.3137)	loss 1.4991 (1.1737)	grad_norm 0.3425 (nan)	loss_scale 2048.0000 (3127.0747)	mem 15606MB
[2024-08-02 15:59:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.3119 (0.3137)	loss 1.2304 (1.1742)	grad_norm 0.3206 (nan)	loss_scale 2048.0000 (3078.0482)	mem 15606MB
[2024-08-02 15:59:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:01:03 lr 0.000008	 wd 0.0500	time 0.2779 (0.3136)	loss 0.9878 (1.1765)	grad_norm 0.3235 (nan)	loss_scale 2048.0000 (3033.2829)	mem 15606MB
[2024-08-02 16:00:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:31 lr 0.000008	 wd 0.0500	time 0.2908 (0.3134)	loss 1.2115 (1.1766)	grad_norm 0.3160 (nan)	loss_scale 2048.0000 (2992.2466)	mem 15606MB
[2024-08-02 16:00:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2751 (0.3125)	loss 0.8964 (1.1767)	grad_norm 0.3243 (nan)	loss_scale 2048.0000 (2954.4918)	mem 15606MB
[2024-08-02 16:00:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 1 training takes 0:13:04
[2024-08-02 16:00:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.111 (12.111)	Loss 0.5234 (0.5234)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 15606MB
[2024-08-02 16:01:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.938 Acc@5 97.790
[2024-08-02 16:01:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 16:01:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 85.94%
[2024-08-02 16:01:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 16:01:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 16:01:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:19:28 lr 0.000008	 wd 0.0500	time 10.5391 (10.5391)	loss 1.2459 (1.2459)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:01:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:16:15 lr 0.000008	 wd 0.0500	time 0.2848 (0.4062)	loss 1.4253 (1.1793)	grad_norm 0.3222 (0.3320)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:02:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:13:35 lr 0.000008	 wd 0.0500	time 0.3065 (0.3542)	loss 1.4134 (1.1713)	grad_norm 0.3183 (0.3288)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:02:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:12:22 lr 0.000008	 wd 0.0500	time 0.3090 (0.3371)	loss 1.1785 (1.1620)	grad_norm 0.3282 (0.3275)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:03:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:11:32 lr 0.000009	 wd 0.0500	time 0.3166 (0.3297)	loss 1.4236 (1.1653)	grad_norm 0.3106 (0.3273)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:03:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:10:50 lr 0.000009	 wd 0.0500	time 0.3049 (0.3247)	loss 1.4288 (1.1605)	grad_norm 0.3352 (0.3269)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:04:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:10:11 lr 0.000009	 wd 0.0500	time 0.2808 (0.3216)	loss 1.2856 (1.1619)	grad_norm 0.3132 (0.3272)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:04:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:09:36 lr 0.000009	 wd 0.0500	time 0.2970 (0.3197)	loss 1.3358 (1.1669)	grad_norm 0.3305 (0.3267)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:09:01 lr 0.000009	 wd 0.0500	time 0.2989 (0.3182)	loss 1.3904 (1.1699)	grad_norm 0.3191 (0.3273)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:05:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:08:27 lr 0.000009	 wd 0.0500	time 0.2832 (0.3168)	loss 0.9392 (1.1749)	grad_norm 0.3416 (0.3272)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:06:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:07:54 lr 0.000010	 wd 0.0500	time 0.3103 (0.3159)	loss 0.8776 (1.1720)	grad_norm 0.3373 (0.3274)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:07:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:07:22 lr 0.000010	 wd 0.0500	time 0.3321 (0.3156)	loss 1.3773 (1.1695)	grad_norm 0.3349 (0.3275)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:07:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:06:50 lr 0.000010	 wd 0.0500	time 0.2752 (0.3151)	loss 0.9101 (1.1698)	grad_norm 0.3458 (0.3287)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:08:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:06:18 lr 0.000010	 wd 0.0500	time 0.2891 (0.3147)	loss 1.4047 (1.1709)	grad_norm 0.3316 (0.3293)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:08:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:05:46 lr 0.000010	 wd 0.0500	time 0.3311 (0.3140)	loss 1.2230 (1.1725)	grad_norm 0.3199 (0.3307)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:09:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:05:14 lr 0.000010	 wd 0.0500	time 0.3031 (0.3136)	loss 1.3403 (1.1730)	grad_norm 0.3184 (0.3305)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:09:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:04:42 lr 0.000011	 wd 0.0500	time 0.2961 (0.3135)	loss 0.9816 (1.1736)	grad_norm 0.3191 (0.3302)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:10:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:04:11 lr 0.000011	 wd 0.0500	time 0.2954 (0.3133)	loss 1.2870 (1.1739)	grad_norm 0.3131 (0.3300)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:10:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:03:39 lr 0.000011	 wd 0.0500	time 0.3247 (0.3133)	loss 1.0768 (1.1745)	grad_norm 0.3271 (0.3299)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:11:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:03:08 lr 0.000011	 wd 0.0500	time 0.3247 (0.3134)	loss 1.3447 (1.1743)	grad_norm 0.3492 (0.3300)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:11:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:02:37 lr 0.000011	 wd 0.0500	time 0.2872 (0.3134)	loss 1.1299 (1.1743)	grad_norm 0.3190 (0.3295)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:12:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:02:06 lr 0.000011	 wd 0.0500	time 0.2910 (0.3134)	loss 1.0059 (1.1728)	grad_norm 0.3197 (0.3302)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:12:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:01:34 lr 0.000012	 wd 0.0500	time 0.3210 (0.3133)	loss 1.3982 (1.1726)	grad_norm 0.3372 (0.3307)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:13:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:01:03 lr 0.000012	 wd 0.0500	time 0.2930 (0.3131)	loss 1.2103 (1.1727)	grad_norm 0.3184 (0.3307)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:13:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:31 lr 0.000012	 wd 0.0500	time 0.3120 (0.3130)	loss 0.7824 (1.1734)	grad_norm 0.3269 (0.3310)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:14:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2852 (0.3123)	loss 1.2080 (1.1738)	grad_norm 0.3094 (0.3316)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:14:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 2 training takes 0:13:03
[2024-08-02 16:14:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.340 (12.340)	Loss 0.5034 (0.5034)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 16:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.954 Acc@5 97.796
[2024-08-02 16:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 16:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 85.95%
[2024-08-02 16:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 16:14:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 16:14:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:40:35 lr 0.000012	 wd 0.0500	time 11.0452 (11.0452)	loss 0.7495 (0.7495)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:15:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:16:23 lr 0.000012	 wd 0.0500	time 0.2988 (0.4093)	loss 1.2444 (1.2092)	grad_norm 0.3268 (0.3291)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:15:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:13:39 lr 0.000012	 wd 0.0500	time 0.2958 (0.3559)	loss 1.5228 (1.1834)	grad_norm 0.3345 (0.3365)	loss_scale 4096.0000 (3046.5274)	mem 15606MB
[2024-08-02 16:16:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:12:28 lr 0.000012	 wd 0.0500	time 0.2833 (0.3397)	loss 1.4429 (1.1802)	grad_norm 0.3206 (0.3358)	loss_scale 4096.0000 (3395.1894)	mem 15606MB
[2024-08-02 16:16:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:11:36 lr 0.000013	 wd 0.0500	time 0.3047 (0.3312)	loss 1.4140 (1.1727)	grad_norm 0.3241 (0.3349)	loss_scale 4096.0000 (3569.9551)	mem 15606MB
[2024-08-02 16:17:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:10:52 lr 0.000013	 wd 0.0500	time 0.3218 (0.3259)	loss 0.9984 (1.1730)	grad_norm 0.3156 (0.3328)	loss_scale 4096.0000 (3674.9541)	mem 15606MB
[2024-08-02 16:17:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:10:13 lr 0.000013	 wd 0.0500	time 0.2761 (0.3224)	loss 1.0618 (1.1679)	grad_norm 0.3380 (0.3318)	loss_scale 4096.0000 (3745.0116)	mem 15606MB
[2024-08-02 16:18:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:09:37 lr 0.000013	 wd 0.0500	time 0.2809 (0.3204)	loss 1.5162 (1.1654)	grad_norm 0.3236 (0.3317)	loss_scale 4096.0000 (3795.0813)	mem 15606MB
[2024-08-02 16:19:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:09:02 lr 0.000013	 wd 0.0500	time 0.3210 (0.3188)	loss 0.7812 (1.1613)	grad_norm 0.3362 (0.3312)	loss_scale 4096.0000 (3832.6492)	mem 15606MB
[2024-08-02 16:19:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:08:28 lr 0.000013	 wd 0.0500	time 0.3032 (0.3175)	loss 1.5732 (1.1634)	grad_norm 0.3377 (0.3304)	loss_scale 4096.0000 (3861.8779)	mem 15606MB
[2024-08-02 16:20:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:07:55 lr 0.000014	 wd 0.0500	time 0.2816 (0.3164)	loss 1.2931 (1.1640)	grad_norm 0.3000 (0.3298)	loss_scale 4096.0000 (3885.2667)	mem 15606MB
[2024-08-02 16:20:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:07:22 lr 0.000014	 wd 0.0500	time 0.2888 (0.3156)	loss 0.9920 (1.1647)	grad_norm 0.3416 (0.3297)	loss_scale 4096.0000 (3904.4069)	mem 15606MB
[2024-08-02 16:21:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:06:49 lr 0.000014	 wd 0.0500	time 0.2855 (0.3147)	loss 1.1500 (1.1644)	grad_norm 0.3224 (0.3303)	loss_scale 4096.0000 (3920.3597)	mem 15606MB
[2024-08-02 16:21:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:06:17 lr 0.000014	 wd 0.0500	time 0.3197 (0.3144)	loss 1.3149 (1.1638)	grad_norm 0.3116 (0.3302)	loss_scale 4096.0000 (3933.8601)	mem 15606MB
[2024-08-02 16:22:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:05:45 lr 0.000014	 wd 0.0500	time 0.3300 (0.3140)	loss 1.1254 (1.1648)	grad_norm 0.3142 (0.3300)	loss_scale 4096.0000 (3945.4333)	mem 15606MB
[2024-08-02 16:22:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:05:14 lr 0.000014	 wd 0.0500	time 0.2861 (0.3134)	loss 1.3985 (1.1633)	grad_norm 0.3261 (0.3305)	loss_scale 4096.0000 (3955.4644)	mem 15606MB
[2024-08-02 16:23:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:04:42 lr 0.000015	 wd 0.0500	time 0.2835 (0.3130)	loss 0.7856 (1.1618)	grad_norm 0.3240 (nan)	loss_scale 2048.0000 (3951.4503)	mem 15606MB
[2024-08-02 16:23:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:04:10 lr 0.000015	 wd 0.0500	time 0.2865 (0.3129)	loss 0.9907 (1.1642)	grad_norm 0.3120 (nan)	loss_scale 2048.0000 (3839.5485)	mem 15606MB
[2024-08-02 16:24:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:03:39 lr 0.000015	 wd 0.0500	time 0.3031 (0.3130)	loss 1.1584 (1.1654)	grad_norm 0.3336 (nan)	loss_scale 2048.0000 (3740.0733)	mem 15606MB
[2024-08-02 16:24:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:03:08 lr 0.000015	 wd 0.0500	time 0.2916 (0.3130)	loss 1.2568 (1.1645)	grad_norm 0.3205 (nan)	loss_scale 2048.0000 (3651.0637)	mem 15606MB
[2024-08-02 16:25:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:02:36 lr 0.000015	 wd 0.0500	time 0.2806 (0.3126)	loss 1.3144 (1.1650)	grad_norm 0.3152 (nan)	loss_scale 2048.0000 (3570.9505)	mem 15606MB
[2024-08-02 16:25:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:02:05 lr 0.000015	 wd 0.0500	time 0.3032 (0.3126)	loss 0.9775 (1.1654)	grad_norm 0.3326 (nan)	loss_scale 2048.0000 (3498.4636)	mem 15606MB
[2024-08-02 16:26:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:01:34 lr 0.000016	 wd 0.0500	time 0.3090 (0.3128)	loss 0.8097 (1.1646)	grad_norm 0.3230 (nan)	loss_scale 2048.0000 (3432.5634)	mem 15606MB
[2024-08-02 16:26:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:01:03 lr 0.000016	 wd 0.0500	time 0.2995 (0.3127)	loss 0.9105 (1.1649)	grad_norm 0.3318 (nan)	loss_scale 2048.0000 (3372.3911)	mem 15606MB
[2024-08-02 16:27:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:31 lr 0.000016	 wd 0.0500	time 0.3047 (0.3127)	loss 1.4124 (1.1665)	grad_norm 0.3439 (nan)	loss_scale 2048.0000 (3317.2312)	mem 15606MB
[2024-08-02 16:27:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.3005 (0.3124)	loss 1.2973 (1.1663)	grad_norm 0.3318 (nan)	loss_scale 2048.0000 (3266.4822)	mem 15606MB
[2024-08-02 16:27:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 3 training takes 0:13:04
[2024-08-02 16:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.960 (11.960)	Loss 0.5215 (0.5215)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 16:28:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.938 Acc@5 97.814
[2024-08-02 16:28:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 85.9%
[2024-08-02 16:28:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 85.95%
[2024-08-02 16:28:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:11:21 lr 0.000016	 wd 0.0500	time 10.3443 (10.3443)	loss 1.2685 (1.2685)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:28:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:16:21 lr 0.000016	 wd 0.0500	time 0.2874 (0.4087)	loss 0.9533 (1.1809)	grad_norm 0.3278 (0.3277)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:29:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:13:35 lr 0.000016	 wd 0.0500	time 0.2903 (0.3542)	loss 0.8680 (1.1691)	grad_norm 0.3273 (0.3377)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:29:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:12:22 lr 0.000016	 wd 0.0500	time 0.3130 (0.3374)	loss 0.7956 (1.1657)	grad_norm 0.3233 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:30:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:11:30 lr 0.000017	 wd 0.0500	time 0.2862 (0.3285)	loss 1.2972 (1.1608)	grad_norm 0.3326 (0.3402)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:30:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:10:48 lr 0.000017	 wd 0.0500	time 0.3660 (0.3238)	loss 1.2540 (1.1614)	grad_norm 0.5997 (nan)	loss_scale 1024.0000 (1986.6826)	mem 15606MB
[2024-08-02 16:31:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:10:10 lr 0.000017	 wd 0.0500	time 0.2896 (0.3210)	loss 1.1931 (1.1662)	grad_norm 0.4778 (nan)	loss_scale 1024.0000 (1826.5025)	mem 15606MB
[2024-08-02 16:32:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:09:33 lr 0.000017	 wd 0.0500	time 0.2958 (0.3184)	loss 0.8374 (1.1665)	grad_norm 0.3293 (nan)	loss_scale 1024.0000 (1712.0228)	mem 15606MB
[2024-08-02 16:32:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:08:58 lr 0.000017	 wd 0.0500	time 0.3220 (0.3166)	loss 0.7787 (1.1691)	grad_norm 0.3380 (nan)	loss_scale 1024.0000 (1626.1273)	mem 15606MB
[2024-08-02 16:33:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:08:25 lr 0.000017	 wd 0.0500	time 0.3438 (0.3154)	loss 0.8331 (1.1702)	grad_norm 0.3381 (nan)	loss_scale 1024.0000 (1559.2986)	mem 15606MB
[2024-08-02 16:33:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:07:51 lr 0.000018	 wd 0.0500	time 0.3029 (0.3142)	loss 1.5698 (1.1702)	grad_norm 0.3111 (nan)	loss_scale 1024.0000 (1505.8222)	mem 15606MB
[2024-08-02 16:34:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:07:19 lr 0.000018	 wd 0.0500	time 0.2861 (0.3136)	loss 1.4446 (1.1712)	grad_norm 0.3198 (nan)	loss_scale 1024.0000 (1462.0599)	mem 15606MB
[2024-08-02 16:34:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:06:47 lr 0.000018	 wd 0.0500	time 0.2986 (0.3133)	loss 1.2874 (1.1700)	grad_norm 0.3298 (nan)	loss_scale 1024.0000 (1425.5853)	mem 15606MB
[2024-08-02 16:35:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:06:16 lr 0.000018	 wd 0.0500	time 0.3211 (0.3130)	loss 1.0288 (1.1708)	grad_norm 0.3406 (nan)	loss_scale 1024.0000 (1394.7179)	mem 15606MB
[2024-08-02 16:35:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:05:44 lr 0.000018	 wd 0.0500	time 0.3004 (0.3125)	loss 1.4144 (1.1707)	grad_norm 0.3288 (nan)	loss_scale 1024.0000 (1368.2570)	mem 15606MB
[2024-08-02 16:36:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:05:14 lr 0.000018	 wd 0.0500	time 0.3563 (0.3137)	loss 1.2103 (1.1712)	grad_norm 0.3212 (nan)	loss_scale 1024.0000 (1345.3218)	mem 15606MB
[2024-08-02 16:36:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:04:42 lr 0.000019	 wd 0.0500	time 0.2877 (0.3134)	loss 1.4581 (1.1697)	grad_norm 0.3424 (nan)	loss_scale 1024.0000 (1325.2517)	mem 15606MB
[2024-08-02 16:37:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:04:11 lr 0.000019	 wd 0.0500	time 0.2983 (0.3132)	loss 1.1798 (1.1703)	grad_norm 0.3293 (nan)	loss_scale 1024.0000 (1307.5414)	mem 15606MB
[2024-08-02 16:37:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:03:39 lr 0.000019	 wd 0.0500	time 0.2851 (0.3130)	loss 1.5254 (1.1724)	grad_norm 0.3276 (nan)	loss_scale 1024.0000 (1291.7979)	mem 15606MB
[2024-08-02 16:38:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:03:08 lr 0.000019	 wd 0.0500	time 0.2768 (0.3129)	loss 1.3935 (1.1728)	grad_norm 0.3263 (nan)	loss_scale 1024.0000 (1277.7107)	mem 15606MB
[2024-08-02 16:38:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:02:37 lr 0.000019	 wd 0.0500	time 0.3225 (0.3129)	loss 0.8670 (1.1706)	grad_norm 0.3342 (nan)	loss_scale 1024.0000 (1265.0315)	mem 15606MB
[2024-08-02 16:39:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:02:05 lr 0.000019	 wd 0.0500	time 0.3349 (0.3129)	loss 1.0016 (1.1693)	grad_norm 0.3480 (nan)	loss_scale 1024.0000 (1253.5593)	mem 15606MB
[2024-08-02 16:39:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:01:34 lr 0.000020	 wd 0.0500	time 0.3260 (0.3129)	loss 0.8771 (1.1689)	grad_norm 0.3097 (nan)	loss_scale 1024.0000 (1243.1295)	mem 15606MB
[2024-08-02 16:40:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:01:03 lr 0.000020	 wd 0.0500	time 0.3195 (0.3126)	loss 0.8018 (1.1683)	grad_norm 0.3264 (nan)	loss_scale 1024.0000 (1233.6063)	mem 15606MB
[2024-08-02 16:40:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:31 lr 0.000020	 wd 0.0500	time 0.2866 (0.3125)	loss 0.7860 (1.1682)	grad_norm 0.3003 (nan)	loss_scale 1024.0000 (1224.8763)	mem 15606MB
[2024-08-02 16:41:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2756 (0.3117)	loss 1.2378 (1.1688)	grad_norm 0.3412 (nan)	loss_scale 1024.0000 (1216.8445)	mem 15606MB
[2024-08-02 16:41:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 4 training takes 0:13:02
[2024-08-02 16:41:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.938 (11.938)	Loss 0.5088 (0.5088)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 15606MB
[2024-08-02 16:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.992 Acc@5 97.828
[2024-08-02 16:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 16:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 85.99%
[2024-08-02 16:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 16:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 16:41:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:24:52 lr 0.000020	 wd 0.0500	time 10.6685 (10.6685)	loss 1.3896 (1.3896)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:42:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:16:39 lr 0.000020	 wd 0.0500	time 0.3372 (0.4159)	loss 0.9878 (1.1979)	grad_norm 0.3397 (0.3300)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:43:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:13:42 lr 0.000020	 wd 0.0500	time 0.2995 (0.3574)	loss 1.3530 (1.1689)	grad_norm 0.3261 (0.3307)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:12:25 lr 0.000020	 wd 0.0500	time 0.3126 (0.3388)	loss 1.0222 (1.1583)	grad_norm 0.3190 (0.3286)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:44:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:11:33 lr 0.000020	 wd 0.0500	time 0.2793 (0.3301)	loss 1.2641 (1.1672)	grad_norm 0.3325 (0.3300)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:44:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:10:51 lr 0.000020	 wd 0.0500	time 0.3206 (0.3253)	loss 1.0075 (1.1659)	grad_norm 0.3295 (0.3326)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:45:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:10:12 lr 0.000020	 wd 0.0500	time 0.3226 (0.3220)	loss 1.0601 (1.1673)	grad_norm 0.3366 (0.3323)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:45:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:09:36 lr 0.000020	 wd 0.0500	time 0.2933 (0.3199)	loss 0.9176 (1.1658)	grad_norm 0.3117 (0.3314)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:46:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:09:01 lr 0.000020	 wd 0.0500	time 0.2865 (0.3184)	loss 1.5231 (1.1649)	grad_norm 0.3216 (0.3317)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:46:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:08:28 lr 0.000020	 wd 0.0500	time 0.2874 (0.3175)	loss 1.0698 (1.1637)	grad_norm 0.3179 (0.3311)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:47:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:07:55 lr 0.000020	 wd 0.0500	time 0.2777 (0.3168)	loss 1.2467 (1.1619)	grad_norm 0.3062 (0.3307)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:47:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:07:23 lr 0.000020	 wd 0.0500	time 0.2979 (0.3161)	loss 1.1745 (1.1629)	grad_norm 0.3397 (0.3312)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:48:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:06:50 lr 0.000020	 wd 0.0500	time 0.2924 (0.3155)	loss 1.4226 (1.1615)	grad_norm 0.3190 (0.3316)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:48:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:06:18 lr 0.000020	 wd 0.0500	time 0.2769 (0.3149)	loss 0.9979 (1.1622)	grad_norm 0.3216 (0.3312)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:49:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:05:46 lr 0.000020	 wd 0.0500	time 0.2927 (0.3145)	loss 1.4230 (1.1658)	grad_norm 0.3493 (0.3311)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:49:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:05:14 lr 0.000020	 wd 0.0500	time 0.3297 (0.3141)	loss 0.8445 (1.1669)	grad_norm 0.3382 (0.3309)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:50:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:04:44 lr 0.000020	 wd 0.0500	time 0.3016 (0.3154)	loss 1.4146 (1.1695)	grad_norm 0.3320 (0.3308)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:50:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:04:12 lr 0.000020	 wd 0.0500	time 0.3201 (0.3150)	loss 0.9597 (1.1697)	grad_norm 0.3221 (0.3308)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:51:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:03:40 lr 0.000020	 wd 0.0500	time 0.3040 (0.3148)	loss 0.7712 (1.1676)	grad_norm 0.3275 (0.3307)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:51:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:03:09 lr 0.000020	 wd 0.0500	time 0.3439 (0.3145)	loss 1.1286 (1.1671)	grad_norm 0.3247 (0.3307)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 16:52:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:02:37 lr 0.000020	 wd 0.0500	time 0.3040 (0.3143)	loss 1.5624 (1.1686)	grad_norm 0.3270 (0.3315)	loss_scale 2048.0000 (1040.3758)	mem 15606MB
[2024-08-02 16:52:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:02:06 lr 0.000020	 wd 0.0500	time 0.3108 (0.3143)	loss 1.4782 (1.1681)	grad_norm 0.3330 (0.3318)	loss_scale 2048.0000 (1088.3351)	mem 15606MB
[2024-08-02 16:53:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:01:35 lr 0.000020	 wd 0.0500	time 0.2933 (0.3151)	loss 1.2123 (1.1693)	grad_norm 0.3308 (0.3316)	loss_scale 2048.0000 (1131.9364)	mem 15606MB
[2024-08-02 16:53:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:01:03 lr 0.000020	 wd 0.0500	time 0.3017 (0.3147)	loss 1.1243 (1.1678)	grad_norm 0.3535 (0.3314)	loss_scale 2048.0000 (1171.7479)	mem 15606MB
[2024-08-02 16:54:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:32 lr 0.000020	 wd 0.0500	time 0.3233 (0.3144)	loss 0.7867 (1.1673)	grad_norm 0.3393 (0.3314)	loss_scale 2048.0000 (1208.2432)	mem 15606MB
[2024-08-02 16:54:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2751 (0.3137)	loss 1.2947 (1.1685)	grad_norm 0.3252 (0.3312)	loss_scale 2048.0000 (1241.8201)	mem 15606MB
[2024-08-02 16:54:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 5 training takes 0:13:07
[2024-08-02 16:55:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.661 (11.661)	Loss 0.5039 (0.5039)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 16:55:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.984 Acc@5 97.830
[2024-08-02 16:55:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 16:55:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 85.99%
[2024-08-02 16:55:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][0/2502]	eta 8:13:07 lr 0.000020	 wd 0.0500	time 11.8257 (11.8257)	loss 1.2244 (1.2244)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:56:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:16:48 lr 0.000020	 wd 0.0500	time 0.3188 (0.4199)	loss 0.8941 (1.1907)	grad_norm 0.3372 (0.3272)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:56:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:13:54 lr 0.000020	 wd 0.0500	time 0.3092 (0.3625)	loss 0.8662 (1.1804)	grad_norm 0.2999 (0.3264)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:57:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:12:36 lr 0.000020	 wd 0.0500	time 0.3325 (0.3437)	loss 0.9583 (1.1721)	grad_norm 0.3293 (0.3270)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:57:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:11:42 lr 0.000020	 wd 0.0500	time 0.2883 (0.3341)	loss 0.7897 (1.1741)	grad_norm 0.4311 (0.3282)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:58:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:10:57 lr 0.000020	 wd 0.0500	time 0.3230 (0.3286)	loss 1.2096 (1.1745)	grad_norm 0.3206 (0.3279)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:58:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:10:17 lr 0.000020	 wd 0.0500	time 0.2870 (0.3244)	loss 0.8762 (1.1659)	grad_norm 0.3329 (0.3278)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:59:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:09:39 lr 0.000020	 wd 0.0500	time 0.2780 (0.3213)	loss 1.4699 (1.1685)	grad_norm 0.3024 (0.3306)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 16:59:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:09:02 lr 0.000020	 wd 0.0500	time 0.3245 (0.3189)	loss 0.7064 (1.1728)	grad_norm 0.3104 (0.3310)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:00:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:08:28 lr 0.000020	 wd 0.0500	time 0.2832 (0.3176)	loss 1.4917 (1.1692)	grad_norm 0.3096 (0.3312)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:00:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:07:54 lr 0.000020	 wd 0.0500	time 0.2972 (0.3162)	loss 1.0380 (1.1689)	grad_norm 0.3119 (0.3317)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:01:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:07:21 lr 0.000020	 wd 0.0500	time 0.2799 (0.3152)	loss 0.8584 (1.1656)	grad_norm 0.3291 (0.3314)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:01:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:06:49 lr 0.000020	 wd 0.0500	time 0.3557 (0.3144)	loss 1.2401 (1.1626)	grad_norm 0.3372 (0.3313)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:02:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:06:17 lr 0.000020	 wd 0.0500	time 0.3303 (0.3138)	loss 1.0974 (1.1614)	grad_norm 0.3145 (0.3313)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:02:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:05:45 lr 0.000020	 wd 0.0500	time 0.2806 (0.3135)	loss 0.9515 (1.1616)	grad_norm 0.3255 (0.3311)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:03:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:05:13 lr 0.000020	 wd 0.0500	time 0.3238 (0.3130)	loss 0.7338 (1.1626)	grad_norm 0.3193 (0.3307)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:03:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:04:41 lr 0.000020	 wd 0.0500	time 0.2855 (0.3125)	loss 1.4550 (1.1601)	grad_norm 0.3389 (0.3305)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:04:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:04:10 lr 0.000020	 wd 0.0500	time 0.2797 (0.3122)	loss 1.3746 (1.1594)	grad_norm 0.3194 (0.3312)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:04:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:03:38 lr 0.000020	 wd 0.0500	time 0.3192 (0.3118)	loss 1.2047 (1.1599)	grad_norm 0.3191 (0.3311)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:03:07 lr 0.000020	 wd 0.0500	time 0.2909 (0.3116)	loss 1.3486 (1.1611)	grad_norm 0.3220 (0.3310)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:05:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:02:36 lr 0.000020	 wd 0.0500	time 0.2809 (0.3120)	loss 0.9781 (1.1619)	grad_norm 0.3300 (0.3311)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:06:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:02:05 lr 0.000020	 wd 0.0500	time 0.2791 (0.3122)	loss 1.0082 (1.1621)	grad_norm 0.3311 (0.3314)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:06:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:01:34 lr 0.000020	 wd 0.0500	time 0.3155 (0.3118)	loss 0.8139 (1.1614)	grad_norm 0.3500 (0.3319)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:07:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:01:02 lr 0.000020	 wd 0.0500	time 0.3057 (0.3115)	loss 1.0287 (1.1639)	grad_norm 0.3401 (0.3322)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:07:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:31 lr 0.000020	 wd 0.0500	time 0.3198 (0.3112)	loss 1.3832 (1.1631)	grad_norm 0.3323 (0.3325)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:08:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.2751 (0.3105)	loss 1.3435 (1.1628)	grad_norm 0.3171 (0.3323)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:08:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 6 training takes 0:12:59
[2024-08-02 17:08:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.628 (11.628)	Loss 0.5181 (0.5181)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 17:08:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.014 Acc@5 97.826
[2024-08-02 17:08:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 17:08:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.01%
[2024-08-02 17:08:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 17:08:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 17:09:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][0/2502]	eta 7:57:26 lr 0.000020	 wd 0.0500	time 11.4493 (11.4493)	loss 0.9301 (0.9301)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:09:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:16:53 lr 0.000020	 wd 0.0500	time 0.3060 (0.4220)	loss 1.2039 (1.1489)	grad_norm 0.3237 (0.3415)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:10:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:14:02 lr 0.000020	 wd 0.0500	time 0.3340 (0.3661)	loss 1.1169 (1.1695)	grad_norm 0.3301 (0.3369)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:10:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:12:50 lr 0.000020	 wd 0.0500	time 0.2775 (0.3499)	loss 1.2909 (1.1645)	grad_norm 0.3302 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:11:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:11:57 lr 0.000020	 wd 0.0500	time 0.3223 (0.3414)	loss 0.8264 (1.1696)	grad_norm 0.3368 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:11:10 lr 0.000020	 wd 0.0500	time 0.3057 (0.3350)	loss 1.3394 (1.1663)	grad_norm 0.3195 (0.3340)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:12:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:10:27 lr 0.000020	 wd 0.0500	time 0.2805 (0.3298)	loss 1.2988 (1.1706)	grad_norm 0.3396 (0.3329)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:12:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:09:48 lr 0.000020	 wd 0.0500	time 0.3376 (0.3266)	loss 0.9683 (1.1652)	grad_norm 0.3165 (0.3334)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:13:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:09:11 lr 0.000020	 wd 0.0500	time 0.3135 (0.3239)	loss 0.8905 (1.1626)	grad_norm 0.3246 (0.3331)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:13:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:08:35 lr 0.000020	 wd 0.0500	time 0.3221 (0.3217)	loss 1.4299 (1.1643)	grad_norm 0.3170 (0.3329)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:14:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:08:01 lr 0.000020	 wd 0.0500	time 0.3055 (0.3208)	loss 0.8067 (1.1642)	grad_norm 0.3414 (0.3322)	loss_scale 4096.0000 (2121.6543)	mem 15606MB
[2024-08-02 17:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:07:27 lr 0.000020	 wd 0.0500	time 0.2977 (0.3192)	loss 1.3637 (1.1634)	grad_norm 0.3225 (0.3318)	loss_scale 4096.0000 (2300.9773)	mem 15606MB
[2024-08-02 17:15:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:06:53 lr 0.000020	 wd 0.0500	time 0.2851 (0.3179)	loss 1.4002 (1.1602)	grad_norm 0.3312 (0.3316)	loss_scale 4096.0000 (2450.4380)	mem 15606MB
[2024-08-02 17:15:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:06:21 lr 0.000020	 wd 0.0500	time 0.3612 (0.3175)	loss 1.3681 (1.1612)	grad_norm 0.4364 (0.3317)	loss_scale 4096.0000 (2576.9224)	mem 15606MB
[2024-08-02 17:16:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:05:49 lr 0.000019	 wd 0.0500	time 0.3017 (0.3169)	loss 1.2758 (1.1610)	grad_norm 0.3157 (0.3320)	loss_scale 4096.0000 (2685.3505)	mem 15606MB
[2024-08-02 17:16:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:05:16 lr 0.000019	 wd 0.0500	time 0.2855 (0.3162)	loss 0.8324 (1.1616)	grad_norm 0.3149 (0.3328)	loss_scale 4096.0000 (2779.3311)	mem 15606MB
[2024-08-02 17:17:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:04:44 lr 0.000019	 wd 0.0500	time 0.2828 (0.3157)	loss 1.1127 (1.1621)	grad_norm 0.3311 (0.3325)	loss_scale 4096.0000 (2861.5715)	mem 15606MB
[2024-08-02 17:17:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:04:12 lr 0.000019	 wd 0.0500	time 0.3102 (0.3153)	loss 1.3354 (1.1623)	grad_norm 0.3124 (0.3323)	loss_scale 4096.0000 (2934.1423)	mem 15606MB
[2024-08-02 17:18:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:03:41 lr 0.000019	 wd 0.0500	time 0.3477 (0.3149)	loss 0.7565 (1.1647)	grad_norm 0.3236 (0.3320)	loss_scale 4096.0000 (2998.6541)	mem 15606MB
[2024-08-02 17:18:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:03:09 lr 0.000019	 wd 0.0500	time 0.3245 (0.3151)	loss 1.1021 (1.1650)	grad_norm 0.3278 (0.3317)	loss_scale 4096.0000 (3056.3787)	mem 15606MB
[2024-08-02 17:19:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:02:38 lr 0.000019	 wd 0.0500	time 0.2805 (0.3150)	loss 1.1401 (1.1657)	grad_norm 0.3270 (0.3319)	loss_scale 4096.0000 (3108.3338)	mem 15606MB
[2024-08-02 17:19:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:02:06 lr 0.000019	 wd 0.0500	time 0.2863 (0.3150)	loss 1.4504 (1.1682)	grad_norm 0.3356 (0.3321)	loss_scale 4096.0000 (3155.3432)	mem 15606MB
[2024-08-02 17:20:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:01:35 lr 0.000019	 wd 0.0500	time 0.2908 (0.3148)	loss 1.2270 (1.1664)	grad_norm 0.3306 (0.3317)	loss_scale 4096.0000 (3198.0809)	mem 15606MB
[2024-08-02 17:20:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:01:03 lr 0.000019	 wd 0.0500	time 0.3012 (0.3147)	loss 1.3801 (1.1655)	grad_norm 0.3378 (0.3316)	loss_scale 4096.0000 (3237.1039)	mem 15606MB
[2024-08-02 17:21:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:32 lr 0.000019	 wd 0.0500	time 0.3235 (0.3147)	loss 1.2500 (1.1659)	grad_norm 0.3202 (nan)	loss_scale 2048.0000 (3233.6393)	mem 15606MB
[2024-08-02 17:21:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2750 (0.3138)	loss 1.5765 (1.1661)	grad_norm 0.3140 (nan)	loss_scale 2048.0000 (3186.2327)	mem 15606MB
[2024-08-02 17:22:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 7 training takes 0:13:07
[2024-08-02 17:22:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.610 (11.610)	Loss 0.4946 (0.4946)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 17:22:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.982 Acc@5 97.860
[2024-08-02 17:22:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 17:22:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.01%
[2024-08-02 17:22:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][0/2502]	eta 8:00:16 lr 0.000019	 wd 0.0500	time 11.5172 (11.5172)	loss 1.2462 (1.2462)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:23:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:16:37 lr 0.000019	 wd 0.0500	time 0.3048 (0.4151)	loss 0.9816 (1.2082)	grad_norm 0.3109 (0.3420)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:23:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:13:44 lr 0.000019	 wd 0.0500	time 0.2830 (0.3580)	loss 0.8190 (1.1935)	grad_norm 0.3208 (0.3399)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:24:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:12:30 lr 0.000019	 wd 0.0500	time 0.3143 (0.3409)	loss 1.3927 (1.1848)	grad_norm 0.3309 (0.3369)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:24:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:11:37 lr 0.000019	 wd 0.0500	time 0.2924 (0.3319)	loss 0.8331 (1.1836)	grad_norm 0.3343 (0.3355)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:10:55 lr 0.000019	 wd 0.0500	time 0.2818 (0.3276)	loss 0.8045 (1.1727)	grad_norm 0.3286 (0.3360)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:25:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:10:15 lr 0.000019	 wd 0.0500	time 0.2739 (0.3235)	loss 0.9154 (1.1697)	grad_norm 0.3448 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:26:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:09:38 lr 0.000019	 wd 0.0500	time 0.2826 (0.3212)	loss 1.3435 (1.1703)	grad_norm 0.3388 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:26:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:09:02 lr 0.000019	 wd 0.0500	time 0.2809 (0.3188)	loss 1.4664 (1.1719)	grad_norm 0.3308 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:27:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:08:28 lr 0.000019	 wd 0.0500	time 0.3134 (0.3176)	loss 0.9487 (1.1670)	grad_norm 0.3213 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:27:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:07:54 lr 0.000019	 wd 0.0500	time 0.2937 (0.3160)	loss 1.3396 (1.1704)	grad_norm 0.3265 (0.3352)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:28:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:07:25 lr 0.000019	 wd 0.0500	time 0.2817 (0.3181)	loss 1.1995 (1.1685)	grad_norm 0.3162 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:28:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:06:53 lr 0.000019	 wd 0.0500	time 0.3066 (0.3174)	loss 1.1254 (1.1681)	grad_norm 0.3362 (0.3341)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:29:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:06:20 lr 0.000019	 wd 0.0500	time 0.3072 (0.3165)	loss 0.9041 (1.1674)	grad_norm 0.3330 (0.3336)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 17:29:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:05:47 lr 0.000019	 wd 0.0500	time 0.3097 (0.3157)	loss 1.2684 (1.1682)	grad_norm 0.3327 (nan)	loss_scale 1024.0000 (1989.5275)	mem 15606MB
[2024-08-02 17:30:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:05:15 lr 0.000019	 wd 0.0500	time 0.3209 (0.3152)	loss 1.1598 (1.1685)	grad_norm 0.3352 (nan)	loss_scale 1024.0000 (1925.2019)	mem 15606MB
[2024-08-02 17:30:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:04:44 lr 0.000019	 wd 0.0500	time 0.2860 (0.3149)	loss 1.3747 (1.1712)	grad_norm 0.3312 (nan)	loss_scale 1024.0000 (1868.9119)	mem 15606MB
[2024-08-02 17:31:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:04:12 lr 0.000019	 wd 0.0500	time 0.2835 (0.3149)	loss 0.7664 (1.1736)	grad_norm 0.3203 (nan)	loss_scale 1024.0000 (1819.2404)	mem 15606MB
[2024-08-02 17:31:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:03:40 lr 0.000019	 wd 0.0500	time 0.2975 (0.3145)	loss 0.7916 (1.1722)	grad_norm 0.3301 (nan)	loss_scale 1024.0000 (1775.0850)	mem 15606MB
[2024-08-02 17:32:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:03:09 lr 0.000019	 wd 0.0500	time 0.3262 (0.3149)	loss 1.1729 (1.1740)	grad_norm 0.3177 (nan)	loss_scale 1024.0000 (1735.5750)	mem 15606MB
[2024-08-02 17:32:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:02:37 lr 0.000019	 wd 0.0500	time 0.3020 (0.3147)	loss 0.8167 (1.1728)	grad_norm 0.3293 (nan)	loss_scale 1024.0000 (1700.0140)	mem 15606MB
[2024-08-02 17:33:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:02:06 lr 0.000019	 wd 0.0500	time 0.2914 (0.3145)	loss 0.8698 (1.1732)	grad_norm 0.5101 (nan)	loss_scale 1024.0000 (1667.8382)	mem 15606MB
[2024-08-02 17:34:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:01:34 lr 0.000019	 wd 0.0500	time 0.3091 (0.3144)	loss 1.1149 (1.1716)	grad_norm 0.3486 (nan)	loss_scale 1024.0000 (1638.5861)	mem 15606MB
[2024-08-02 17:34:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:01:03 lr 0.000019	 wd 0.0500	time 0.3299 (0.3141)	loss 1.3409 (1.1703)	grad_norm 0.3136 (nan)	loss_scale 1024.0000 (1611.8766)	mem 15606MB
[2024-08-02 17:35:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:32 lr 0.000019	 wd 0.0500	time 0.2894 (0.3138)	loss 1.1758 (1.1701)	grad_norm 0.3647 (nan)	loss_scale 1024.0000 (1587.3919)	mem 15606MB
[2024-08-02 17:35:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.2753 (0.3130)	loss 0.9024 (1.1699)	grad_norm 0.3116 (nan)	loss_scale 1024.0000 (1564.8653)	mem 15606MB
[2024-08-02 17:35:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 8 training takes 0:13:05
[2024-08-02 17:35:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.196 (12.196)	Loss 0.4968 (0.4968)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 15606MB
[2024-08-02 17:36:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.060 Acc@5 97.846
[2024-08-02 17:36:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 17:36:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.06%
[2024-08-02 17:36:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 17:36:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 17:36:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:54:52 lr 0.000019	 wd 0.0500	time 11.3877 (11.3877)	loss 1.3288 (1.3288)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:36:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:16:33 lr 0.000019	 wd 0.0500	time 0.3168 (0.4136)	loss 1.2622 (1.1475)	grad_norm 0.3389 (0.3351)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:37:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:13:44 lr 0.000019	 wd 0.0500	time 0.2834 (0.3582)	loss 0.8946 (1.1710)	grad_norm 0.3252 (0.3505)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:37:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:12:30 lr 0.000019	 wd 0.0500	time 0.3065 (0.3408)	loss 1.5014 (1.1798)	grad_norm 0.3201 (0.3534)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:38:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:11:36 lr 0.000019	 wd 0.0500	time 0.3043 (0.3315)	loss 0.8406 (1.1631)	grad_norm 0.3233 (0.3492)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:38:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:10:51 lr 0.000019	 wd 0.0500	time 0.2854 (0.3257)	loss 0.7502 (1.1582)	grad_norm 0.3374 (0.3448)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:39:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:10:12 lr 0.000019	 wd 0.0500	time 0.2934 (0.3218)	loss 0.8407 (1.1625)	grad_norm 0.3195 (0.3451)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:39:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:09:35 lr 0.000019	 wd 0.0500	time 0.2731 (0.3193)	loss 1.3727 (1.1660)	grad_norm 0.3236 (0.3429)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:40:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:09:00 lr 0.000019	 wd 0.0500	time 0.2903 (0.3176)	loss 1.3751 (1.1672)	grad_norm 0.3130 (0.3412)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:40:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:08:26 lr 0.000019	 wd 0.0500	time 0.2830 (0.3165)	loss 1.3465 (1.1669)	grad_norm 0.3131 (0.3396)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:41:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:07:53 lr 0.000019	 wd 0.0500	time 0.2976 (0.3156)	loss 1.1769 (1.1676)	grad_norm 0.3160 (0.3389)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:41:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:07:21 lr 0.000018	 wd 0.0500	time 0.3248 (0.3146)	loss 1.3245 (1.1636)	grad_norm 0.3260 (0.3402)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:42:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:06:49 lr 0.000018	 wd 0.0500	time 0.2884 (0.3142)	loss 0.8386 (1.1625)	grad_norm 0.3281 (0.3393)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:42:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:06:16 lr 0.000018	 wd 0.0500	time 0.2822 (0.3136)	loss 1.2741 (1.1634)	grad_norm 0.3227 (0.3386)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:43:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:05:44 lr 0.000018	 wd 0.0500	time 0.3022 (0.3130)	loss 1.0721 (1.1626)	grad_norm 0.3210 (0.3386)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:43:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:05:13 lr 0.000018	 wd 0.0500	time 0.2931 (0.3126)	loss 0.8609 (1.1659)	grad_norm 0.3473 (0.3383)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:44:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:04:42 lr 0.000018	 wd 0.0500	time 0.3231 (0.3127)	loss 0.9131 (1.1641)	grad_norm 0.3197 (0.3379)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:44:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:04:10 lr 0.000018	 wd 0.0500	time 0.2919 (0.3126)	loss 1.4335 (1.1647)	grad_norm 0.3339 (0.3376)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:45:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:03:39 lr 0.000018	 wd 0.0500	time 0.3508 (0.3127)	loss 1.2498 (1.1634)	grad_norm 0.3375 (0.3371)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:45:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:03:08 lr 0.000018	 wd 0.0500	time 0.3234 (0.3126)	loss 1.4032 (1.1628)	grad_norm 0.3345 (0.3385)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:46:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:02:36 lr 0.000018	 wd 0.0500	time 0.3127 (0.3127)	loss 1.3915 (1.1634)	grad_norm 0.3088 (0.3385)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:47:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:02:05 lr 0.000018	 wd 0.0500	time 0.2865 (0.3131)	loss 1.4065 (1.1649)	grad_norm 0.3350 (0.3386)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:47:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:01:34 lr 0.000018	 wd 0.0500	time 0.3222 (0.3131)	loss 1.1508 (1.1660)	grad_norm 0.3390 (0.3382)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:48:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:01:03 lr 0.000018	 wd 0.0500	time 0.2981 (0.3130)	loss 0.7886 (1.1660)	grad_norm 0.3257 (0.3379)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:48:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:31 lr 0.000018	 wd 0.0500	time 0.2911 (0.3129)	loss 1.0819 (1.1663)	grad_norm 0.3489 (0.3379)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:49:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.2963 (0.3121)	loss 1.2625 (1.1658)	grad_norm 0.3325 (0.3384)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:49:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 9 training takes 0:13:03
[2024-08-02 17:49:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.666 (11.666)	Loss 0.5098 (0.5098)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 17:49:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 85.988 Acc@5 97.858
[2024-08-02 17:49:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 17:49:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.06%
[2024-08-02 17:49:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][0/2502]	eta 8:24:18 lr 0.000018	 wd 0.0500	time 12.0936 (12.0936)	loss 1.6269 (1.6269)	grad_norm 0.0000 (0.0000)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:50:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:16:37 lr 0.000018	 wd 0.0500	time 0.2949 (0.4153)	loss 0.8897 (1.1769)	grad_norm 0.3604 (0.3369)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:50:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:13:45 lr 0.000018	 wd 0.0500	time 0.3338 (0.3588)	loss 1.4308 (1.1720)	grad_norm 0.3114 (0.3334)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:51:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:12:28 lr 0.000018	 wd 0.0500	time 0.2816 (0.3398)	loss 1.3206 (1.1667)	grad_norm 0.3604 (0.3318)	loss_scale 1024.0000 (1024.0000)	mem 15606MB
[2024-08-02 17:51:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:11:34 lr 0.000018	 wd 0.0500	time 0.2992 (0.3302)	loss 0.8916 (1.1702)	grad_norm 0.3302 (0.3317)	loss_scale 2048.0000 (1238.5037)	mem 15606MB
[2024-08-02 17:52:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:10:50 lr 0.000018	 wd 0.0500	time 0.2871 (0.3248)	loss 0.7054 (1.1657)	grad_norm 0.3163 (0.3335)	loss_scale 2048.0000 (1400.0798)	mem 15606MB
[2024-08-02 17:52:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:10:11 lr 0.000018	 wd 0.0500	time 0.3320 (0.3217)	loss 1.1583 (1.1737)	grad_norm 0.3181 (0.3322)	loss_scale 2048.0000 (1507.8869)	mem 15606MB
[2024-08-02 17:53:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:09:35 lr 0.000018	 wd 0.0500	time 0.2930 (0.3192)	loss 1.3000 (1.1719)	grad_norm 0.3021 (0.3333)	loss_scale 2048.0000 (1584.9358)	mem 15606MB
[2024-08-02 17:53:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:09:00 lr 0.000018	 wd 0.0500	time 0.3147 (0.3174)	loss 1.2723 (1.1745)	grad_norm 0.3241 (0.3334)	loss_scale 2048.0000 (1642.7466)	mem 15606MB
[2024-08-02 17:54:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:08:27 lr 0.000018	 wd 0.0500	time 0.3029 (0.3168)	loss 1.4484 (1.1720)	grad_norm 0.3187 (0.3328)	loss_scale 2048.0000 (1687.7248)	mem 15606MB
[2024-08-02 17:54:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:07:54 lr 0.000018	 wd 0.0500	time 0.3195 (0.3159)	loss 1.2768 (1.1731)	grad_norm 0.3233 (0.3326)	loss_scale 2048.0000 (1723.7163)	mem 15606MB
[2024-08-02 17:55:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:07:22 lr 0.000018	 wd 0.0500	time 0.2961 (0.3154)	loss 1.0002 (1.1789)	grad_norm 0.3427 (0.3325)	loss_scale 2048.0000 (1753.1698)	mem 15606MB
[2024-08-02 17:55:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:06:50 lr 0.000018	 wd 0.0500	time 0.3258 (0.3150)	loss 1.3944 (1.1764)	grad_norm 0.5484 (0.3327)	loss_scale 2048.0000 (1777.7186)	mem 15606MB
[2024-08-02 17:56:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:06:18 lr 0.000018	 wd 0.0500	time 0.2849 (0.3145)	loss 1.3698 (1.1739)	grad_norm 0.3786 (0.3326)	loss_scale 2048.0000 (1798.4935)	mem 15606MB
[2024-08-02 17:56:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:05:46 lr 0.000018	 wd 0.0500	time 0.3382 (0.3140)	loss 1.3458 (1.1718)	grad_norm 0.3221 (0.3328)	loss_scale 2048.0000 (1816.3026)	mem 15606MB
[2024-08-02 17:57:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:05:14 lr 0.000018	 wd 0.0500	time 0.2880 (0.3137)	loss 1.1672 (1.1693)	grad_norm 0.3403 (0.3324)	loss_scale 2048.0000 (1831.7388)	mem 15606MB
[2024-08-02 17:57:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:04:42 lr 0.000018	 wd 0.0500	time 0.3146 (0.3133)	loss 1.1046 (1.1680)	grad_norm 0.3088 (0.3328)	loss_scale 2048.0000 (1845.2467)	mem 15606MB
[2024-08-02 17:58:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:04:12 lr 0.000018	 wd 0.0500	time 0.3267 (0.3143)	loss 1.2373 (1.1700)	grad_norm 0.3270 (0.3330)	loss_scale 2048.0000 (1857.1664)	mem 15606MB
[2024-08-02 17:59:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:03:40 lr 0.000018	 wd 0.0500	time 0.2846 (0.3141)	loss 1.1349 (1.1690)	grad_norm 0.3469 (0.3332)	loss_scale 2048.0000 (1867.7624)	mem 15606MB
[2024-08-02 17:59:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:03:08 lr 0.000018	 wd 0.0500	time 0.2883 (0.3139)	loss 1.0839 (1.1680)	grad_norm 0.3220 (0.3352)	loss_scale 2048.0000 (1877.2436)	mem 15606MB
[2024-08-02 18:00:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:02:37 lr 0.000017	 wd 0.0500	time 0.3149 (0.3138)	loss 1.3327 (1.1715)	grad_norm 0.3414 (0.3355)	loss_scale 2048.0000 (1885.7771)	mem 15606MB
[2024-08-02 18:00:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:02:06 lr 0.000017	 wd 0.0500	time 0.3174 (0.3138)	loss 1.2054 (1.1729)	grad_norm 0.3080 (0.3355)	loss_scale 2048.0000 (1893.4983)	mem 15606MB
[2024-08-02 18:01:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:01:34 lr 0.000017	 wd 0.0500	time 0.3269 (0.3136)	loss 1.3221 (1.1739)	grad_norm 0.3137 (0.3355)	loss_scale 2048.0000 (1900.5179)	mem 15606MB
[2024-08-02 18:01:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:01:03 lr 0.000017	 wd 0.0500	time 0.2885 (0.3133)	loss 1.2855 (1.1718)	grad_norm 0.3279 (0.3354)	loss_scale 2048.0000 (1906.9274)	mem 15606MB
[2024-08-02 18:02:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:31 lr 0.000017	 wd 0.0500	time 0.2976 (0.3132)	loss 1.3623 (1.1708)	grad_norm 0.3121 (0.3355)	loss_scale 2048.0000 (1912.8030)	mem 15606MB
[2024-08-02 18:02:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.2790 (0.3124)	loss 0.9012 (1.1703)	grad_norm 0.3309 (0.3355)	loss_scale 2048.0000 (1918.2087)	mem 15606MB
[2024-08-02 18:02:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 10 training takes 0:13:04
[2024-08-02 18:02:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_10.pth saving......
[2024-08-02 18:02:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_10.pth saved !!!
[2024-08-02 18:02:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.177 (11.177)	Loss 0.5010 (0.5010)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 15606MB
[2024-08-02 18:03:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.014 Acc@5 97.854
[2024-08-02 18:03:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 18:03:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.06%
[2024-08-02 18:03:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][0/2502]	eta 8:12:01 lr 0.000017	 wd 0.0500	time 11.7991 (11.7991)	loss 0.9179 (0.9179)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:03:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:16:53 lr 0.000017	 wd 0.0500	time 0.2974 (0.4220)	loss 1.3896 (1.1548)	grad_norm 0.3191 (0.3309)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:04:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:13:57 lr 0.000017	 wd 0.0500	time 0.2802 (0.3639)	loss 1.3229 (1.1759)	grad_norm 0.3723 (0.3369)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:04:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:12:37 lr 0.000017	 wd 0.0500	time 0.2922 (0.3440)	loss 0.8606 (1.1663)	grad_norm 0.3354 (0.3361)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:05:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:11:43 lr 0.000017	 wd 0.0500	time 0.2881 (0.3348)	loss 1.4012 (1.1647)	grad_norm 0.3342 (0.3346)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:05:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:10:57 lr 0.000017	 wd 0.0500	time 0.3006 (0.3285)	loss 0.7262 (1.1639)	grad_norm 0.3121 (0.3334)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:06:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:10:16 lr 0.000017	 wd 0.0500	time 0.3240 (0.3244)	loss 1.4595 (1.1646)	grad_norm 0.3225 (0.3340)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:06:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:09:39 lr 0.000017	 wd 0.0500	time 0.2855 (0.3215)	loss 1.0115 (1.1601)	grad_norm 0.3620 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:07:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:09:04 lr 0.000017	 wd 0.0500	time 0.2820 (0.3197)	loss 1.4781 (1.1583)	grad_norm 0.3198 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:07:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:08:29 lr 0.000017	 wd 0.0500	time 0.3236 (0.3181)	loss 1.0035 (1.1611)	grad_norm 0.3404 (0.3341)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:08:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:07:56 lr 0.000017	 wd 0.0500	time 0.3472 (0.3172)	loss 1.1566 (1.1612)	grad_norm 0.3500 (0.3382)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:08:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:07:23 lr 0.000017	 wd 0.0500	time 0.3521 (0.3165)	loss 0.9098 (1.1597)	grad_norm 0.3322 (0.3380)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:09:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:06:51 lr 0.000017	 wd 0.0500	time 0.3106 (0.3159)	loss 0.9248 (1.1566)	grad_norm 0.3219 (0.3371)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:09:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:06:19 lr 0.000017	 wd 0.0500	time 0.3101 (0.3153)	loss 1.3954 (1.1571)	grad_norm 0.3372 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:10:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:05:47 lr 0.000017	 wd 0.0500	time 0.3194 (0.3151)	loss 0.8205 (1.1575)	grad_norm 0.3264 (0.3367)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:10:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:05:15 lr 0.000017	 wd 0.0500	time 0.3141 (0.3146)	loss 1.3961 (1.1583)	grad_norm 0.3329 (0.3363)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:11:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:04:43 lr 0.000017	 wd 0.0500	time 0.2868 (0.3145)	loss 1.2705 (1.1595)	grad_norm 0.3255 (0.3362)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:12:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:04:12 lr 0.000017	 wd 0.0500	time 0.3050 (0.3147)	loss 1.0850 (1.1586)	grad_norm 0.3258 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:12:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:03:40 lr 0.000017	 wd 0.0500	time 0.2807 (0.3145)	loss 0.8430 (1.1586)	grad_norm 0.3161 (0.3356)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:13:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:03:09 lr 0.000017	 wd 0.0500	time 0.3025 (0.3144)	loss 1.1395 (1.1577)	grad_norm 0.3357 (0.3355)	loss_scale 4096.0000 (2140.6502)	mem 15606MB
[2024-08-02 18:13:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:02:37 lr 0.000017	 wd 0.0500	time 0.3093 (0.3143)	loss 1.0572 (1.1579)	grad_norm 0.3025 (0.3353)	loss_scale 4096.0000 (2238.3688)	mem 15606MB
[2024-08-02 18:14:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:02:06 lr 0.000017	 wd 0.0500	time 0.3104 (0.3142)	loss 1.3602 (1.1571)	grad_norm 0.3403 (0.3349)	loss_scale 4096.0000 (2326.7853)	mem 15606MB
[2024-08-02 18:14:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:01:34 lr 0.000017	 wd 0.0500	time 0.3096 (0.3139)	loss 0.9727 (1.1590)	grad_norm 0.3396 (0.3351)	loss_scale 4096.0000 (2407.1677)	mem 15606MB
[2024-08-02 18:15:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:01:03 lr 0.000016	 wd 0.0500	time 0.3118 (0.3135)	loss 1.1642 (1.1572)	grad_norm 0.3331 (0.3348)	loss_scale 4096.0000 (2480.5632)	mem 15606MB
[2024-08-02 18:15:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:32 lr 0.000016	 wd 0.0500	time 0.2882 (0.3144)	loss 1.3856 (1.1584)	grad_norm 0.3244 (0.3348)	loss_scale 4096.0000 (2547.8451)	mem 15606MB
[2024-08-02 18:16:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.2988 (0.3135)	loss 0.8560 (1.1566)	grad_norm 0.3041 (0.3348)	loss_scale 4096.0000 (2609.7465)	mem 15606MB
[2024-08-02 18:16:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 11 training takes 0:13:06
[2024-08-02 18:16:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.685 (11.685)	Loss 0.5020 (0.5020)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 18:16:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.054 Acc@5 97.850
[2024-08-02 18:16:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 18:16:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.06%
[2024-08-02 18:16:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:36:40 lr 0.000016	 wd 0.0500	time 10.9514 (10.9514)	loss 0.9555 (0.9555)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:17:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:16:33 lr 0.000016	 wd 0.0500	time 0.2945 (0.4138)	loss 1.3040 (1.1602)	grad_norm 0.3204 (0.3442)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:17:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:13:43 lr 0.000016	 wd 0.0500	time 0.3045 (0.3578)	loss 1.3003 (1.1575)	grad_norm 0.3264 (0.3377)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:18:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:12:27 lr 0.000016	 wd 0.0500	time 0.2999 (0.3397)	loss 0.7495 (1.1697)	grad_norm 0.3413 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:18:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:11:35 lr 0.000016	 wd 0.0500	time 0.2706 (0.3307)	loss 1.2285 (1.1616)	grad_norm 0.3176 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:19:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:10:51 lr 0.000016	 wd 0.0500	time 0.2824 (0.3255)	loss 1.4326 (1.1616)	grad_norm 0.3239 (0.3406)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:19:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:10:12 lr 0.000016	 wd 0.0500	time 0.2857 (0.3222)	loss 1.4190 (1.1601)	grad_norm 0.3607 (0.3392)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:20:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:09:36 lr 0.000016	 wd 0.0500	time 0.3169 (0.3197)	loss 1.2163 (1.1583)	grad_norm 0.3312 (0.3380)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:20:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:09:00 lr 0.000016	 wd 0.0500	time 0.2903 (0.3178)	loss 1.3663 (1.1593)	grad_norm 0.3599 (0.3381)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:08:27 lr 0.000016	 wd 0.0500	time 0.3176 (0.3165)	loss 1.5329 (1.1568)	grad_norm 0.3270 (0.3381)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:21:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:07:53 lr 0.000016	 wd 0.0500	time 0.2983 (0.3153)	loss 1.3934 (1.1588)	grad_norm 0.3379 (0.3373)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 18:22:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:07:20 lr 0.000016	 wd 0.0500	time 0.3030 (0.3143)	loss 0.8052 (1.1603)	grad_norm 0.3210 (nan)	loss_scale 2048.0000 (3984.3924)	mem 15606MB
[2024-08-02 18:22:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:06:48 lr 0.000016	 wd 0.0500	time 0.2825 (0.3138)	loss 1.4356 (1.1585)	grad_norm 0.3294 (nan)	loss_scale 2048.0000 (3823.1607)	mem 15606MB
[2024-08-02 18:23:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:06:16 lr 0.000016	 wd 0.0500	time 0.3138 (0.3131)	loss 1.0505 (1.1594)	grad_norm 0.3113 (nan)	loss_scale 2048.0000 (3686.7148)	mem 15606MB
[2024-08-02 18:24:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:05:44 lr 0.000016	 wd 0.0500	time 0.3158 (0.3125)	loss 1.3191 (1.1590)	grad_norm 0.3149 (nan)	loss_scale 2048.0000 (3569.7473)	mem 15606MB
[2024-08-02 18:24:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:05:12 lr 0.000016	 wd 0.0500	time 0.3582 (0.3123)	loss 0.8148 (1.1563)	grad_norm 0.3407 (nan)	loss_scale 2048.0000 (3468.3651)	mem 15606MB
[2024-08-02 18:25:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:04:41 lr 0.000016	 wd 0.0500	time 0.3077 (0.3120)	loss 1.3645 (1.1530)	grad_norm 0.3094 (nan)	loss_scale 2048.0000 (3379.6477)	mem 15606MB
[2024-08-02 18:25:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:04:10 lr 0.000016	 wd 0.0500	time 0.2959 (0.3122)	loss 1.0649 (1.1552)	grad_norm 0.3420 (nan)	loss_scale 2048.0000 (3301.3616)	mem 15606MB
[2024-08-02 18:26:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:03:39 lr 0.000016	 wd 0.0500	time 0.2984 (0.3120)	loss 0.8175 (1.1537)	grad_norm 0.3292 (nan)	loss_scale 2048.0000 (3231.7690)	mem 15606MB
[2024-08-02 18:26:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:03:07 lr 0.000016	 wd 0.0500	time 0.3130 (0.3119)	loss 1.3138 (1.1545)	grad_norm 0.3293 (nan)	loss_scale 2048.0000 (3169.4982)	mem 15606MB
[2024-08-02 18:27:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:02:36 lr 0.000016	 wd 0.0500	time 0.2945 (0.3118)	loss 1.0410 (1.1553)	grad_norm 0.3240 (nan)	loss_scale 2048.0000 (3113.4513)	mem 15606MB
[2024-08-02 18:27:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:02:05 lr 0.000016	 wd 0.0500	time 0.3017 (0.3117)	loss 0.8148 (1.1546)	grad_norm 0.3033 (nan)	loss_scale 2048.0000 (3062.7396)	mem 15606MB
[2024-08-02 18:28:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:01:34 lr 0.000016	 wd 0.0500	time 0.3074 (0.3115)	loss 0.8082 (1.1560)	grad_norm 0.3346 (nan)	loss_scale 2048.0000 (3016.6361)	mem 15606MB
[2024-08-02 18:28:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:01:02 lr 0.000015	 wd 0.0500	time 0.3006 (0.3115)	loss 1.1882 (1.1565)	grad_norm 0.3203 (nan)	loss_scale 2048.0000 (2974.5398)	mem 15606MB
[2024-08-02 18:29:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:31 lr 0.000015	 wd 0.0500	time 0.3395 (0.3112)	loss 1.3529 (1.1568)	grad_norm 0.3150 (nan)	loss_scale 2048.0000 (2935.9500)	mem 15606MB
[2024-08-02 18:29:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.2749 (0.3103)	loss 0.9393 (1.1561)	grad_norm 0.3295 (nan)	loss_scale 2048.0000 (2900.4462)	mem 15606MB
[2024-08-02 18:29:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 12 training takes 0:12:58
[2024-08-02 18:29:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.073 (12.073)	Loss 0.5137 (0.5137)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 18:30:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.000 Acc@5 97.852
[2024-08-02 18:30:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 18:30:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.06%
[2024-08-02 18:30:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][0/2502]	eta 8:12:00 lr 0.000015	 wd 0.0500	time 11.7987 (11.7987)	loss 1.3254 (1.3254)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:30:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:16:33 lr 0.000015	 wd 0.0500	time 0.2835 (0.4137)	loss 1.3443 (1.2208)	grad_norm 0.3276 (0.3289)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:31:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:13:42 lr 0.000015	 wd 0.0500	time 0.2824 (0.3572)	loss 1.3829 (1.2085)	grad_norm 0.3178 (0.3310)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:31:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:12:26 lr 0.000015	 wd 0.0500	time 0.2906 (0.3389)	loss 1.3903 (1.2086)	grad_norm 0.3238 (0.3328)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:32:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:11:37 lr 0.000015	 wd 0.0500	time 0.3178 (0.3318)	loss 1.4555 (1.1894)	grad_norm 0.3372 (0.3331)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:32:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:10:53 lr 0.000015	 wd 0.0500	time 0.2858 (0.3264)	loss 1.2857 (1.1825)	grad_norm 0.3469 (0.3325)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:33:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:10:13 lr 0.000015	 wd 0.0500	time 0.3162 (0.3226)	loss 1.1995 (1.1764)	grad_norm 0.3246 (0.3328)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:33:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:09:38 lr 0.000015	 wd 0.0500	time 0.3719 (0.3209)	loss 1.1742 (1.1674)	grad_norm 0.3391 (0.3350)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:34:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:09:02 lr 0.000015	 wd 0.0500	time 0.3029 (0.3188)	loss 1.2350 (1.1667)	grad_norm 0.3159 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:34:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:08:28 lr 0.000015	 wd 0.0500	time 0.3236 (0.3174)	loss 1.3296 (1.1673)	grad_norm 0.3372 (0.3384)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:35:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:07:55 lr 0.000015	 wd 0.0500	time 0.3228 (0.3167)	loss 1.3795 (1.1660)	grad_norm 0.3953 (0.3377)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:35:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:07:22 lr 0.000015	 wd 0.0500	time 0.3312 (0.3156)	loss 1.3986 (1.1619)	grad_norm 0.3308 (0.3399)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:36:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:06:50 lr 0.000015	 wd 0.0500	time 0.3245 (0.3151)	loss 1.1868 (1.1660)	grad_norm 0.3302 (0.3391)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:36:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:06:17 lr 0.000015	 wd 0.0500	time 0.2894 (0.3142)	loss 0.7265 (1.1643)	grad_norm 0.3276 (0.3388)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:37:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:05:45 lr 0.000015	 wd 0.0500	time 0.2910 (0.3137)	loss 1.3491 (1.1630)	grad_norm 0.3315 (0.3385)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:37:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:05:13 lr 0.000015	 wd 0.0500	time 0.2961 (0.3133)	loss 1.2151 (1.1635)	grad_norm 0.3335 (0.3385)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:38:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:04:42 lr 0.000015	 wd 0.0500	time 0.3364 (0.3131)	loss 0.9126 (1.1624)	grad_norm 0.3287 (0.3383)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:39:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:04:11 lr 0.000015	 wd 0.0500	time 0.3219 (0.3131)	loss 1.0339 (1.1637)	grad_norm 0.4366 (0.3380)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:39:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:03:39 lr 0.000015	 wd 0.0500	time 0.2956 (0.3130)	loss 1.2272 (1.1641)	grad_norm 0.3407 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:40:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:03:08 lr 0.000015	 wd 0.0500	time 0.3062 (0.3129)	loss 1.0776 (1.1642)	grad_norm 0.3309 (0.3374)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:40:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:02:36 lr 0.000015	 wd 0.0500	time 0.2942 (0.3126)	loss 1.3862 (1.1640)	grad_norm 0.3097 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:41:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:02:05 lr 0.000014	 wd 0.0500	time 0.2802 (0.3125)	loss 1.3603 (1.1640)	grad_norm 0.3294 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:41:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:01:34 lr 0.000014	 wd 0.0500	time 0.3062 (0.3124)	loss 1.1991 (1.1626)	grad_norm 0.3240 (0.3382)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:42:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:01:03 lr 0.000014	 wd 0.0500	time 0.3006 (0.3122)	loss 1.3399 (1.1616)	grad_norm 0.3403 (0.3387)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:42:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:31 lr 0.000014	 wd 0.0500	time 0.2944 (0.3123)	loss 0.9467 (1.1643)	grad_norm 0.3088 (0.3383)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:43:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.3095 (0.3116)	loss 0.7892 (1.1630)	grad_norm 0.3278 (0.3383)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:43:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 13 training takes 0:13:02
[2024-08-02 18:43:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.678 (11.678)	Loss 0.4868 (0.4868)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 18:43:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.082 Acc@5 97.862
[2024-08-02 18:43:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 18:43:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.08%
[2024-08-02 18:43:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 18:43:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 18:43:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:36:26 lr 0.000014	 wd 0.0500	time 10.9460 (10.9460)	loss 1.4217 (1.4217)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:44:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:16:41 lr 0.000014	 wd 0.0500	time 0.2968 (0.4169)	loss 1.1835 (1.1802)	grad_norm 0.3260 (0.4284)	loss_scale 4096.0000 (3345.7426)	mem 15606MB
[2024-08-02 18:44:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:13:52 lr 0.000014	 wd 0.0500	time 0.3266 (0.3617)	loss 1.4482 (1.1783)	grad_norm 0.3188 (0.3798)	loss_scale 4096.0000 (3719.0050)	mem 15606MB
[2024-08-02 18:45:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:12:35 lr 0.000014	 wd 0.0500	time 0.2977 (0.3430)	loss 0.9336 (1.1813)	grad_norm 0.3385 (0.3649)	loss_scale 4096.0000 (3844.2525)	mem 15606MB
[2024-08-02 18:45:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:11:39 lr 0.000014	 wd 0.0500	time 0.2959 (0.3328)	loss 1.5978 (1.1807)	grad_norm 0.3205 (0.3608)	loss_scale 4096.0000 (3907.0324)	mem 15606MB
[2024-08-02 18:46:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:10:59 lr 0.000014	 wd 0.0500	time 0.2949 (0.3292)	loss 0.9747 (1.1763)	grad_norm 0.3394 (0.3551)	loss_scale 4096.0000 (3944.7505)	mem 15606MB
[2024-08-02 18:46:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:10:18 lr 0.000014	 wd 0.0500	time 0.3083 (0.3252)	loss 0.9646 (1.1753)	grad_norm 0.3398 (0.3514)	loss_scale 4096.0000 (3969.9168)	mem 15606MB
[2024-08-02 18:47:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:09:40 lr 0.000014	 wd 0.0500	time 0.2863 (0.3224)	loss 1.1322 (1.1759)	grad_norm 0.3371 (0.3491)	loss_scale 4096.0000 (3987.9030)	mem 15606MB
[2024-08-02 18:47:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:09:06 lr 0.000014	 wd 0.0500	time 0.3175 (0.3208)	loss 0.8052 (1.1701)	grad_norm 0.3107 (0.3469)	loss_scale 4096.0000 (4001.3983)	mem 15606MB
[2024-08-02 18:48:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:08:33 lr 0.000014	 wd 0.0500	time 0.3135 (0.3203)	loss 0.8601 (1.1691)	grad_norm 0.3352 (0.3458)	loss_scale 4096.0000 (4011.8979)	mem 15606MB
[2024-08-02 18:48:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:07:58 lr 0.000014	 wd 0.0500	time 0.2848 (0.3188)	loss 0.9996 (1.1664)	grad_norm 0.3332 (nan)	loss_scale 2048.0000 (3950.7373)	mem 15606MB
[2024-08-02 18:49:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:07:25 lr 0.000014	 wd 0.0500	time 0.2778 (0.3176)	loss 1.3865 (1.1657)	grad_norm 0.3496 (nan)	loss_scale 2048.0000 (3777.9183)	mem 15606MB
[2024-08-02 18:50:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:06:52 lr 0.000014	 wd 0.0500	time 0.3431 (0.3166)	loss 1.4720 (1.1654)	grad_norm 0.3407 (nan)	loss_scale 2048.0000 (3633.8784)	mem 15606MB
[2024-08-02 18:50:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:06:19 lr 0.000014	 wd 0.0500	time 0.2949 (0.3161)	loss 1.2424 (1.1686)	grad_norm 0.3314 (nan)	loss_scale 2048.0000 (3511.9816)	mem 15606MB
[2024-08-02 18:51:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:05:47 lr 0.000014	 wd 0.0500	time 0.3154 (0.3153)	loss 1.2376 (1.1682)	grad_norm 0.3304 (nan)	loss_scale 2048.0000 (3407.4861)	mem 15606MB
[2024-08-02 18:51:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:05:15 lr 0.000014	 wd 0.0500	time 0.3006 (0.3147)	loss 1.3889 (1.1672)	grad_norm 0.3386 (nan)	loss_scale 2048.0000 (3316.9141)	mem 15606MB
[2024-08-02 18:52:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:04:43 lr 0.000014	 wd 0.0500	time 0.2842 (0.3140)	loss 0.7721 (1.1661)	grad_norm 0.3251 (nan)	loss_scale 2048.0000 (3237.6565)	mem 15606MB
[2024-08-02 18:52:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:04:11 lr 0.000014	 wd 0.0500	time 0.2832 (0.3138)	loss 0.9414 (1.1651)	grad_norm 0.3307 (nan)	loss_scale 2048.0000 (3167.7178)	mem 15606MB
[2024-08-02 18:53:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:03:40 lr 0.000013	 wd 0.0500	time 0.3066 (0.3143)	loss 0.8988 (1.1656)	grad_norm 0.3373 (nan)	loss_scale 2048.0000 (3105.5458)	mem 15606MB
[2024-08-02 18:53:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:03:09 lr 0.000013	 wd 0.0500	time 0.2900 (0.3141)	loss 1.1618 (1.1660)	grad_norm 0.3462 (nan)	loss_scale 2048.0000 (3049.9148)	mem 15606MB
[2024-08-02 18:54:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:02:37 lr 0.000013	 wd 0.0500	time 0.2854 (0.3137)	loss 1.2156 (1.1653)	grad_norm 0.3333 (nan)	loss_scale 2048.0000 (2999.8441)	mem 15606MB
[2024-08-02 18:54:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:02:05 lr 0.000013	 wd 0.0500	time 0.2859 (0.3132)	loss 1.3476 (1.1647)	grad_norm 0.3369 (nan)	loss_scale 2048.0000 (2954.5397)	mem 15606MB
[2024-08-02 18:55:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:01:34 lr 0.000013	 wd 0.0500	time 0.3007 (0.3131)	loss 1.3112 (1.1639)	grad_norm 0.3298 (nan)	loss_scale 2048.0000 (2913.3521)	mem 15606MB
[2024-08-02 18:55:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:01:03 lr 0.000013	 wd 0.0500	time 0.3175 (0.3131)	loss 0.8530 (1.1627)	grad_norm 0.3269 (nan)	loss_scale 2048.0000 (2875.7445)	mem 15606MB
[2024-08-02 18:56:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:31 lr 0.000013	 wd 0.0500	time 0.3293 (0.3129)	loss 0.8653 (1.1619)	grad_norm 0.3497 (nan)	loss_scale 2048.0000 (2841.2695)	mem 15606MB
[2024-08-02 18:56:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.2757 (0.3121)	loss 1.2496 (1.1608)	grad_norm 0.3274 (nan)	loss_scale 2048.0000 (2809.5514)	mem 15606MB
[2024-08-02 18:56:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 14 training takes 0:13:03
[2024-08-02 18:56:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.976 (11.976)	Loss 0.4780 (0.4780)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 18:57:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.102 Acc@5 97.886
[2024-08-02 18:57:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 18:57:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 18:57:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 18:57:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 18:57:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:47:43 lr 0.000013	 wd 0.0500	time 11.2165 (11.2165)	loss 1.2575 (1.2575)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:57:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:16:29 lr 0.000013	 wd 0.0500	time 0.2811 (0.4119)	loss 0.7751 (1.1670)	grad_norm 0.3354 (0.3384)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:58:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:13:37 lr 0.000013	 wd 0.0500	time 0.2792 (0.3551)	loss 1.4464 (1.1531)	grad_norm 0.3379 (0.3413)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:58:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:12:25 lr 0.000013	 wd 0.0500	time 0.2922 (0.3383)	loss 0.9307 (1.1526)	grad_norm 0.3357 (0.3383)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:59:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:11:33 lr 0.000013	 wd 0.0500	time 0.2921 (0.3298)	loss 0.8057 (1.1512)	grad_norm 0.3221 (0.3366)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 18:59:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:10:48 lr 0.000013	 wd 0.0500	time 0.2963 (0.3241)	loss 0.9455 (1.1625)	grad_norm 0.3397 (0.3368)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:00:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:10:10 lr 0.000013	 wd 0.0500	time 0.3106 (0.3211)	loss 1.3834 (1.1652)	grad_norm 0.3311 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:00:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:09:36 lr 0.000013	 wd 0.0500	time 0.3207 (0.3201)	loss 1.2439 (1.1652)	grad_norm 0.3325 (0.3354)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:01:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:09:02 lr 0.000013	 wd 0.0500	time 0.2777 (0.3186)	loss 1.4000 (1.1669)	grad_norm 0.3136 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:01:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:08:28 lr 0.000013	 wd 0.0500	time 0.2799 (0.3175)	loss 1.2620 (1.1665)	grad_norm 0.3375 (0.3347)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:02:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:07:54 lr 0.000013	 wd 0.0500	time 0.2834 (0.3161)	loss 0.8562 (1.1664)	grad_norm 0.3258 (0.3348)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:02:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:07:22 lr 0.000013	 wd 0.0500	time 0.3161 (0.3155)	loss 0.8624 (1.1673)	grad_norm 0.3245 (0.3344)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:03:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:06:50 lr 0.000013	 wd 0.0500	time 0.2943 (0.3154)	loss 0.8385 (1.1673)	grad_norm 0.3505 (0.3360)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:04:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:06:18 lr 0.000013	 wd 0.0500	time 0.3002 (0.3147)	loss 0.8733 (1.1684)	grad_norm 0.3292 (0.3378)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:04:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:05:46 lr 0.000012	 wd 0.0500	time 0.2891 (0.3147)	loss 0.9483 (1.1685)	grad_norm 0.3255 (0.3377)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:05:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:05:14 lr 0.000012	 wd 0.0500	time 0.3245 (0.3142)	loss 1.0257 (1.1647)	grad_norm 0.3411 (0.3426)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:05:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:04:43 lr 0.000012	 wd 0.0500	time 0.2865 (0.3140)	loss 1.0217 (1.1654)	grad_norm 0.3396 (0.3423)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:06:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:04:11 lr 0.000012	 wd 0.0500	time 0.2936 (0.3134)	loss 0.8257 (1.1655)	grad_norm 0.3374 (0.3432)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:06:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:03:40 lr 0.000012	 wd 0.0500	time 0.2913 (0.3135)	loss 1.6703 (1.1635)	grad_norm 0.3642 (0.3431)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:07:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:03:08 lr 0.000012	 wd 0.0500	time 0.3031 (0.3130)	loss 0.9816 (1.1618)	grad_norm 0.3553 (0.3433)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:07:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:02:37 lr 0.000012	 wd 0.0500	time 0.2922 (0.3129)	loss 0.8452 (1.1631)	grad_norm 0.3324 (0.3429)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:08:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:02:05 lr 0.000012	 wd 0.0500	time 0.3168 (0.3126)	loss 0.7902 (1.1618)	grad_norm 0.3284 (0.3428)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:08:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:01:34 lr 0.000012	 wd 0.0500	time 0.3321 (0.3125)	loss 1.0842 (1.1614)	grad_norm 0.3156 (0.3425)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:09:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:01:03 lr 0.000012	 wd 0.0500	time 0.2987 (0.3124)	loss 1.0236 (1.1617)	grad_norm 0.3422 (0.3421)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:09:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:31 lr 0.000012	 wd 0.0500	time 0.3181 (0.3123)	loss 1.4669 (1.1621)	grad_norm 0.3251 (0.3420)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:10:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.2753 (0.3118)	loss 1.5151 (1.1626)	grad_norm 0.3371 (0.3416)	loss_scale 4096.0000 (2077.4794)	mem 15606MB
[2024-08-02 19:10:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 15 training takes 0:13:02
[2024-08-02 19:10:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.087 (12.087)	Loss 0.5342 (0.5342)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 19:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.008 Acc@5 97.834
[2024-08-02 19:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.0%
[2024-08-02 19:10:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 19:10:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][0/2502]	eta 8:04:29 lr 0.000012	 wd 0.0500	time 11.6187 (11.6187)	loss 1.2036 (1.2036)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:11:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:16:39 lr 0.000012	 wd 0.0500	time 0.3145 (0.4163)	loss 1.0438 (1.1730)	grad_norm 0.3362 (0.3387)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:11:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:13:55 lr 0.000012	 wd 0.0500	time 0.3235 (0.3629)	loss 1.2534 (1.1627)	grad_norm 0.3299 (0.3401)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:12:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:12:36 lr 0.000012	 wd 0.0500	time 0.2835 (0.3433)	loss 1.0104 (1.1558)	grad_norm 0.3231 (0.3478)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:12:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:11:41 lr 0.000012	 wd 0.0500	time 0.3184 (0.3339)	loss 0.9297 (1.1444)	grad_norm 0.3272 (0.3446)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:13:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:10:55 lr 0.000012	 wd 0.0500	time 0.2832 (0.3274)	loss 1.5235 (1.1473)	grad_norm 0.3422 (0.3440)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:13:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:10:15 lr 0.000012	 wd 0.0500	time 0.2966 (0.3238)	loss 1.4463 (1.1480)	grad_norm 0.3087 (0.3433)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:14:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:09:39 lr 0.000012	 wd 0.0500	time 0.2783 (0.3213)	loss 1.0682 (1.1505)	grad_norm 0.3327 (0.3435)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:14:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:09:04 lr 0.000012	 wd 0.0500	time 0.3116 (0.3198)	loss 1.3184 (1.1520)	grad_norm 0.3406 (nan)	loss_scale 2048.0000 (3886.3421)	mem 15606MB
[2024-08-02 19:15:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:08:29 lr 0.000012	 wd 0.0500	time 0.3124 (0.3182)	loss 1.1379 (1.1531)	grad_norm 0.3714 (nan)	loss_scale 2048.0000 (3682.3085)	mem 15606MB
[2024-08-02 19:15:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:07:56 lr 0.000011	 wd 0.0500	time 0.2823 (0.3169)	loss 1.3528 (1.1529)	grad_norm 0.3261 (nan)	loss_scale 2048.0000 (3519.0410)	mem 15606MB
[2024-08-02 19:16:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:07:22 lr 0.000011	 wd 0.0500	time 0.3102 (0.3159)	loss 0.8449 (1.1570)	grad_norm 0.3389 (nan)	loss_scale 2048.0000 (3385.4314)	mem 15606MB
[2024-08-02 19:17:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:06:50 lr 0.000011	 wd 0.0500	time 0.2788 (0.3151)	loss 0.8748 (1.1548)	grad_norm 0.3252 (nan)	loss_scale 2048.0000 (3274.0716)	mem 15606MB
[2024-08-02 19:17:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:06:17 lr 0.000011	 wd 0.0500	time 0.2832 (0.3145)	loss 1.3029 (1.1578)	grad_norm 0.3303 (nan)	loss_scale 2048.0000 (3179.8309)	mem 15606MB
[2024-08-02 19:18:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:05:45 lr 0.000011	 wd 0.0500	time 0.3323 (0.3139)	loss 1.2325 (1.1578)	grad_norm 0.3395 (nan)	loss_scale 2048.0000 (3099.0435)	mem 15606MB
[2024-08-02 19:18:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:05:14 lr 0.000011	 wd 0.0500	time 0.2980 (0.3137)	loss 1.2897 (1.1601)	grad_norm 0.3218 (nan)	loss_scale 2048.0000 (3029.0207)	mem 15606MB
[2024-08-02 19:19:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:04:42 lr 0.000011	 wd 0.0500	time 0.3031 (0.3135)	loss 1.1250 (1.1606)	grad_norm 0.3467 (nan)	loss_scale 2048.0000 (2967.7452)	mem 15606MB
[2024-08-02 19:19:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:04:11 lr 0.000011	 wd 0.0500	time 0.2907 (0.3133)	loss 1.0771 (1.1597)	grad_norm 0.3382 (nan)	loss_scale 2048.0000 (2913.6743)	mem 15606MB
[2024-08-02 19:20:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:03:39 lr 0.000011	 wd 0.0500	time 0.3140 (0.3131)	loss 0.8738 (1.1586)	grad_norm 0.3095 (nan)	loss_scale 2048.0000 (2865.6080)	mem 15606MB
[2024-08-02 19:20:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:03:08 lr 0.000011	 wd 0.0500	time 0.3115 (0.3129)	loss 1.0327 (1.1594)	grad_norm 0.3188 (nan)	loss_scale 2048.0000 (2822.5986)	mem 15606MB
[2024-08-02 19:21:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:02:36 lr 0.000011	 wd 0.0500	time 0.2834 (0.3127)	loss 1.3576 (1.1601)	grad_norm 0.3146 (nan)	loss_scale 2048.0000 (2783.8881)	mem 15606MB
[2024-08-02 19:21:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:02:05 lr 0.000011	 wd 0.0500	time 0.3202 (0.3127)	loss 1.3623 (1.1609)	grad_norm 0.3159 (nan)	loss_scale 2048.0000 (2748.8624)	mem 15606MB
[2024-08-02 19:22:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:01:34 lr 0.000011	 wd 0.0500	time 0.2814 (0.3138)	loss 1.0123 (1.1605)	grad_norm 0.3359 (nan)	loss_scale 2048.0000 (2717.0195)	mem 15606MB
[2024-08-02 19:22:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:01:03 lr 0.000011	 wd 0.0500	time 0.2834 (0.3136)	loss 0.8924 (1.1587)	grad_norm 0.3545 (nan)	loss_scale 2048.0000 (2687.9444)	mem 15606MB
[2024-08-02 19:23:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:31 lr 0.000011	 wd 0.0500	time 0.3231 (0.3134)	loss 1.2187 (1.1577)	grad_norm 0.3220 (nan)	loss_scale 2048.0000 (2661.2911)	mem 15606MB
[2024-08-02 19:23:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.2754 (0.3127)	loss 0.8464 (1.1580)	grad_norm 0.3331 (nan)	loss_scale 2048.0000 (2636.7693)	mem 15606MB
[2024-08-02 19:23:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 16 training takes 0:13:04
[2024-08-02 19:23:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.283 (12.283)	Loss 0.5020 (0.5020)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 15606MB
[2024-08-02 19:24:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.064 Acc@5 97.860
[2024-08-02 19:24:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 19:24:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 19:24:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:15:24 lr 0.000011	 wd 0.0500	time 10.4416 (10.4416)	loss 1.4051 (1.4051)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:24:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:16:33 lr 0.000011	 wd 0.0500	time 0.3429 (0.4134)	loss 1.2352 (1.1414)	grad_norm 0.3213 (0.3338)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:25:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:13:45 lr 0.000011	 wd 0.0500	time 0.2819 (0.3587)	loss 1.2804 (1.1413)	grad_norm 0.3281 (0.3322)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:25:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:12:43 lr 0.000011	 wd 0.0500	time 0.2721 (0.3468)	loss 1.2382 (1.1568)	grad_norm 0.3316 (0.3343)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:26:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:11:45 lr 0.000011	 wd 0.0500	time 0.3043 (0.3356)	loss 1.4180 (1.1549)	grad_norm 0.3324 (0.3338)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:27:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:10:59 lr 0.000010	 wd 0.0500	time 0.2906 (0.3294)	loss 1.3652 (1.1592)	grad_norm 0.3414 (0.3345)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:27:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:10:18 lr 0.000010	 wd 0.0500	time 0.3048 (0.3252)	loss 0.8075 (1.1603)	grad_norm 0.3308 (0.3342)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:28:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:09:41 lr 0.000010	 wd 0.0500	time 0.2931 (0.3225)	loss 1.2843 (1.1581)	grad_norm 0.3393 (0.3349)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:28:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:09:05 lr 0.000010	 wd 0.0500	time 0.2815 (0.3203)	loss 1.1908 (1.1562)	grad_norm 0.3443 (0.3377)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:29:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:08:33 lr 0.000010	 wd 0.0500	time 0.3059 (0.3208)	loss 1.2941 (1.1579)	grad_norm 0.3340 (0.3376)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:29:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:08:00 lr 0.000010	 wd 0.0500	time 0.3188 (0.3196)	loss 1.0319 (1.1572)	grad_norm 0.3427 (0.3373)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:30:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:07:26 lr 0.000010	 wd 0.0500	time 0.2915 (0.3185)	loss 1.0302 (1.1618)	grad_norm 0.3218 (0.3377)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:30:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:06:53 lr 0.000010	 wd 0.0500	time 0.2908 (0.3178)	loss 0.7639 (1.1616)	grad_norm 0.3278 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:31:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:06:21 lr 0.000010	 wd 0.0500	time 0.3135 (0.3175)	loss 1.4198 (1.1587)	grad_norm 0.3455 (0.3374)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:31:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:05:49 lr 0.000010	 wd 0.0500	time 0.3496 (0.3170)	loss 1.3226 (1.1588)	grad_norm 0.3136 (0.3378)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:32:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:05:16 lr 0.000010	 wd 0.0500	time 0.3322 (0.3164)	loss 1.4595 (1.1594)	grad_norm 0.3281 (0.3381)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:32:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:04:44 lr 0.000010	 wd 0.0500	time 0.3089 (0.3160)	loss 1.3881 (1.1593)	grad_norm 0.3279 (0.3378)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:33:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:04:13 lr 0.000010	 wd 0.0500	time 0.3075 (0.3157)	loss 1.5284 (1.1610)	grad_norm 0.3215 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:33:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:03:41 lr 0.000010	 wd 0.0500	time 0.2913 (0.3152)	loss 0.9698 (1.1604)	grad_norm 0.4334 (0.3376)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:34:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:03:09 lr 0.000010	 wd 0.0500	time 0.2862 (0.3150)	loss 1.2442 (1.1594)	grad_norm 0.3236 (0.3375)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:34:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:02:38 lr 0.000010	 wd 0.0500	time 0.3011 (0.3148)	loss 1.2019 (1.1597)	grad_norm 0.3309 (0.3376)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:35:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:02:06 lr 0.000010	 wd 0.0500	time 0.3304 (0.3144)	loss 1.4714 (1.1596)	grad_norm 0.3840 (0.3374)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:35:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:01:34 lr 0.000010	 wd 0.0500	time 0.2876 (0.3143)	loss 1.3678 (1.1594)	grad_norm 0.3286 (0.3374)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 19:36:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:01:03 lr 0.000010	 wd 0.0500	time 0.3173 (0.3140)	loss 0.8000 (1.1589)	grad_norm 0.3354 (0.3373)	loss_scale 4096.0000 (2122.7640)	mem 15606MB
[2024-08-02 19:36:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:32 lr 0.000010	 wd 0.0500	time 0.3697 (0.3140)	loss 0.9405 (1.1576)	grad_norm 0.3402 (0.3371)	loss_scale 4096.0000 (2204.9479)	mem 15606MB
[2024-08-02 19:37:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.2752 (0.3132)	loss 1.4551 (1.1572)	grad_norm 0.3260 (0.3370)	loss_scale 4096.0000 (2280.5598)	mem 15606MB
[2024-08-02 19:37:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 17 training takes 0:13:06
[2024-08-02 19:37:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.972 (11.972)	Loss 0.4924 (0.4924)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 19:37:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.064 Acc@5 97.868
[2024-08-02 19:37:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 19:37:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 19:38:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][0/2502]	eta 7:35:35 lr 0.000009	 wd 0.0500	time 10.9255 (10.9255)	loss 1.4411 (1.4411)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:38:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:16:37 lr 0.000009	 wd 0.0500	time 0.3102 (0.4152)	loss 1.4873 (1.1916)	grad_norm 0.3362 (0.3388)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:39:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:13:47 lr 0.000009	 wd 0.0500	time 0.2899 (0.3595)	loss 1.0326 (1.1829)	grad_norm 0.3902 (0.3371)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:39:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:12:31 lr 0.000009	 wd 0.0500	time 0.3255 (0.3413)	loss 1.5088 (1.1819)	grad_norm 0.3217 (0.3359)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:40:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:11:38 lr 0.000009	 wd 0.0500	time 0.2903 (0.3322)	loss 0.7547 (1.1807)	grad_norm 0.4246 (0.3364)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:40:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:10:55 lr 0.000009	 wd 0.0500	time 0.2794 (0.3272)	loss 1.2874 (1.1724)	grad_norm 0.3136 (0.3361)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:41:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:10:15 lr 0.000009	 wd 0.0500	time 0.2946 (0.3236)	loss 1.4934 (1.1649)	grad_norm 0.3351 (0.3352)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:41:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:09:39 lr 0.000009	 wd 0.0500	time 0.2977 (0.3214)	loss 1.4525 (1.1698)	grad_norm 0.3324 (0.3354)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:42:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:09:03 lr 0.000009	 wd 0.0500	time 0.2882 (0.3195)	loss 1.3245 (1.1728)	grad_norm 0.3220 (0.3357)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:42:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:08:30 lr 0.000009	 wd 0.0500	time 0.3098 (0.3185)	loss 0.9195 (1.1726)	grad_norm 0.3449 (0.3396)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:43:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:07:56 lr 0.000009	 wd 0.0500	time 0.2884 (0.3175)	loss 1.4318 (1.1683)	grad_norm 0.3326 (0.3389)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:43:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:07:23 lr 0.000009	 wd 0.0500	time 0.3458 (0.3166)	loss 0.9962 (1.1703)	grad_norm 0.3225 (0.3406)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:44:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:06:51 lr 0.000009	 wd 0.0500	time 0.2878 (0.3161)	loss 1.0571 (1.1634)	grad_norm 0.3311 (0.3401)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:44:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:06:19 lr 0.000009	 wd 0.0500	time 0.2940 (0.3153)	loss 1.4702 (1.1637)	grad_norm 0.3371 (0.3407)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:45:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:05:47 lr 0.000009	 wd 0.0500	time 0.3004 (0.3150)	loss 1.1510 (1.1640)	grad_norm 0.3134 (0.3403)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:45:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:05:15 lr 0.000009	 wd 0.0500	time 0.2789 (0.3144)	loss 1.3157 (1.1636)	grad_norm 0.3187 (0.3398)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:46:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:04:43 lr 0.000009	 wd 0.0500	time 0.2839 (0.3145)	loss 1.4488 (1.1656)	grad_norm 0.3535 (0.3399)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:46:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:04:11 lr 0.000009	 wd 0.0500	time 0.2838 (0.3142)	loss 1.3315 (1.1675)	grad_norm 0.3370 (0.3397)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:47:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:03:40 lr 0.000009	 wd 0.0500	time 0.3129 (0.3140)	loss 0.7220 (1.1678)	grad_norm 0.3287 (0.3394)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:47:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:03:08 lr 0.000009	 wd 0.0500	time 0.3439 (0.3138)	loss 1.4317 (1.1674)	grad_norm 0.3282 (0.3405)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:48:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:02:37 lr 0.000008	 wd 0.0500	time 0.3005 (0.3135)	loss 1.0662 (1.1671)	grad_norm 0.3270 (0.3409)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:48:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:02:06 lr 0.000008	 wd 0.0500	time 0.2947 (0.3142)	loss 1.3332 (1.1660)	grad_norm 0.3331 (0.3408)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:49:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:01:34 lr 0.000008	 wd 0.0500	time 0.2920 (0.3141)	loss 1.0210 (1.1662)	grad_norm 0.3134 (0.3406)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:49:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:01:03 lr 0.000008	 wd 0.0500	time 0.2908 (0.3140)	loss 1.3163 (1.1668)	grad_norm 0.3190 (0.3405)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:50:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:32 lr 0.000008	 wd 0.0500	time 0.2843 (0.3140)	loss 1.4292 (1.1674)	grad_norm 0.3481 (0.3403)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:50:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.2754 (0.3133)	loss 1.2600 (1.1677)	grad_norm 0.3280 (0.3400)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:50:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 18 training takes 0:13:06
[2024-08-02 19:51:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.362 (12.362)	Loss 0.5039 (0.5039)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 19:51:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.096 Acc@5 97.836
[2024-08-02 19:51:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 19:51:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 19:51:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:38:09 lr 0.000008	 wd 0.0500	time 10.9869 (10.9869)	loss 0.8011 (0.8011)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:52:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:17:42 lr 0.000008	 wd 0.0500	time 0.3152 (0.4422)	loss 1.5992 (1.2142)	grad_norm 0.3175 (0.3401)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 19:52:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:14:17 lr 0.000008	 wd 0.0500	time 0.2787 (0.3726)	loss 1.1784 (1.1836)	grad_norm 0.3473 (nan)	loss_scale 2048.0000 (3382.7662)	mem 15606MB
[2024-08-02 19:53:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:12:59 lr 0.000008	 wd 0.0500	time 0.2912 (0.3539)	loss 1.4039 (1.1701)	grad_norm 0.3372 (nan)	loss_scale 2048.0000 (2939.3223)	mem 15606MB
[2024-08-02 19:53:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:11:58 lr 0.000008	 wd 0.0500	time 0.2893 (0.3420)	loss 0.9680 (1.1634)	grad_norm 0.3187 (nan)	loss_scale 2048.0000 (2717.0474)	mem 15606MB
[2024-08-02 19:54:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:11:11 lr 0.000008	 wd 0.0500	time 0.3132 (0.3352)	loss 1.4532 (1.1677)	grad_norm 0.5034 (nan)	loss_scale 2048.0000 (2583.5050)	mem 15606MB
[2024-08-02 19:54:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:10:27 lr 0.000008	 wd 0.0500	time 0.3166 (0.3301)	loss 0.8490 (1.1572)	grad_norm 0.3402 (nan)	loss_scale 2048.0000 (2494.4027)	mem 15606MB
[2024-08-02 19:55:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:09:50 lr 0.000008	 wd 0.0500	time 0.2974 (0.3278)	loss 1.2423 (1.1553)	grad_norm 0.3194 (nan)	loss_scale 2048.0000 (2430.7218)	mem 15606MB
[2024-08-02 19:55:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:09:13 lr 0.000008	 wd 0.0500	time 0.3039 (0.3253)	loss 1.5239 (1.1570)	grad_norm 0.3405 (nan)	loss_scale 2048.0000 (2382.9413)	mem 15606MB
[2024-08-02 19:56:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:08:37 lr 0.000008	 wd 0.0500	time 0.3117 (0.3232)	loss 1.2810 (1.1576)	grad_norm 0.3428 (nan)	loss_scale 2048.0000 (2345.7669)	mem 15606MB
[2024-08-02 19:56:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:08:04 lr 0.000008	 wd 0.0500	time 0.2880 (0.3222)	loss 1.4553 (1.1591)	grad_norm 0.3228 (nan)	loss_scale 2048.0000 (2316.0200)	mem 15606MB
[2024-08-02 19:57:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:07:30 lr 0.000008	 wd 0.0500	time 0.3559 (0.3211)	loss 1.1774 (1.1580)	grad_norm 0.3362 (nan)	loss_scale 2048.0000 (2291.6767)	mem 15606MB
[2024-08-02 19:57:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:06:56 lr 0.000008	 wd 0.0500	time 0.3244 (0.3202)	loss 1.3256 (1.1576)	grad_norm 0.3396 (nan)	loss_scale 2048.0000 (2271.3872)	mem 15606MB
[2024-08-02 19:58:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:06:24 lr 0.000008	 wd 0.0500	time 0.2993 (0.3195)	loss 0.7807 (1.1566)	grad_norm 0.3435 (nan)	loss_scale 2048.0000 (2254.2168)	mem 15606MB
[2024-08-02 19:58:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:05:51 lr 0.000008	 wd 0.0500	time 0.3019 (0.3189)	loss 0.8058 (1.1558)	grad_norm 0.3321 (nan)	loss_scale 2048.0000 (2239.4975)	mem 15606MB
[2024-08-02 19:59:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:05:19 lr 0.000008	 wd 0.0500	time 0.2977 (0.3184)	loss 1.3652 (1.1570)	grad_norm 0.3202 (nan)	loss_scale 2048.0000 (2226.7395)	mem 15606MB
[2024-08-02 19:59:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:04:46 lr 0.000007	 wd 0.0500	time 0.3061 (0.3178)	loss 1.4143 (1.1588)	grad_norm 0.3298 (nan)	loss_scale 2048.0000 (2215.5753)	mem 15606MB
[2024-08-02 20:00:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:04:14 lr 0.000007	 wd 0.0500	time 0.3273 (0.3178)	loss 0.8931 (1.1593)	grad_norm 0.3461 (nan)	loss_scale 2048.0000 (2205.7237)	mem 15606MB
[2024-08-02 20:00:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:03:42 lr 0.000007	 wd 0.0500	time 0.3157 (0.3176)	loss 1.3261 (1.1617)	grad_norm 0.3321 (nan)	loss_scale 2048.0000 (2196.9661)	mem 15606MB
[2024-08-02 20:01:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:03:11 lr 0.000007	 wd 0.0500	time 0.3072 (0.3173)	loss 1.1001 (1.1607)	grad_norm 0.3450 (nan)	loss_scale 2048.0000 (2189.1299)	mem 15606MB
[2024-08-02 20:01:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:02:39 lr 0.000007	 wd 0.0500	time 0.3163 (0.3170)	loss 1.4743 (1.1618)	grad_norm 0.3364 (nan)	loss_scale 2048.0000 (2182.0770)	mem 15606MB
[2024-08-02 20:02:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:02:07 lr 0.000007	 wd 0.0500	time 0.2857 (0.3169)	loss 1.5794 (1.1632)	grad_norm 0.3239 (nan)	loss_scale 2048.0000 (2175.6954)	mem 15606MB
[2024-08-02 20:03:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:01:35 lr 0.000007	 wd 0.0500	time 0.3091 (0.3164)	loss 1.1436 (1.1624)	grad_norm 0.3503 (nan)	loss_scale 2048.0000 (2169.8937)	mem 15606MB
[2024-08-02 20:03:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:01:03 lr 0.000007	 wd 0.0500	time 0.3280 (0.3161)	loss 1.0114 (1.1626)	grad_norm 0.3239 (nan)	loss_scale 2048.0000 (2164.5963)	mem 15606MB
[2024-08-02 20:04:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:32 lr 0.000007	 wd 0.0500	time 0.3073 (0.3156)	loss 1.1064 (1.1617)	grad_norm 0.3406 (nan)	loss_scale 2048.0000 (2159.7401)	mem 15606MB
[2024-08-02 20:04:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.2754 (0.3147)	loss 0.8091 (1.1619)	grad_norm 0.3366 (nan)	loss_scale 2048.0000 (2155.2723)	mem 15606MB
[2024-08-02 20:04:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 19 training takes 0:13:09
[2024-08-02 20:04:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.312 (12.312)	Loss 0.4871 (0.4871)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 20:05:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.086 Acc@5 97.864
[2024-08-02 20:05:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 20:05:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 20:05:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][0/2502]	eta 6:51:10 lr 0.000007	 wd 0.0500	time 9.8604 (9.8604)	loss 1.5289 (1.5289)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:05:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:16:37 lr 0.000007	 wd 0.0500	time 0.2855 (0.4155)	loss 1.0715 (1.1714)	grad_norm 0.3422 (0.3358)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:06:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:13:47 lr 0.000007	 wd 0.0500	time 0.2890 (0.3597)	loss 1.3180 (1.1550)	grad_norm 0.3904 (0.3353)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:06:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:12:31 lr 0.000007	 wd 0.0500	time 0.2843 (0.3412)	loss 0.7031 (1.1673)	grad_norm 0.3487 (0.3378)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:07:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:11:38 lr 0.000007	 wd 0.0500	time 0.2905 (0.3322)	loss 1.3124 (1.1666)	grad_norm 0.3310 (0.3384)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:07:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:10:54 lr 0.000007	 wd 0.0500	time 0.2798 (0.3269)	loss 1.2792 (1.1645)	grad_norm 0.3291 (0.3395)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:08:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:10:16 lr 0.000007	 wd 0.0500	time 0.2921 (0.3241)	loss 1.2535 (1.1695)	grad_norm 0.3688 (0.3384)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:08:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:09:40 lr 0.000007	 wd 0.0500	time 0.3405 (0.3219)	loss 1.2664 (1.1678)	grad_norm 0.3291 (0.3395)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:09:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:09:05 lr 0.000007	 wd 0.0500	time 0.2845 (0.3205)	loss 1.1688 (1.1682)	grad_norm 0.3235 (0.3394)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:09:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:08:31 lr 0.000007	 wd 0.0500	time 0.2950 (0.3195)	loss 1.2594 (1.1675)	grad_norm 0.3456 (0.3593)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:10:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:07:58 lr 0.000007	 wd 0.0500	time 0.3288 (0.3183)	loss 1.3916 (1.1694)	grad_norm 0.3358 (0.3568)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:10:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:07:25 lr 0.000007	 wd 0.0500	time 0.3258 (0.3177)	loss 1.0746 (1.1672)	grad_norm 0.3429 (0.3574)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:11:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:06:53 lr 0.000006	 wd 0.0500	time 0.3032 (0.3174)	loss 1.2223 (1.1633)	grad_norm 0.3222 (0.3555)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:11:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:06:20 lr 0.000006	 wd 0.0500	time 0.3231 (0.3167)	loss 1.3458 (1.1628)	grad_norm 0.3342 (0.3540)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:12:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:05:48 lr 0.000006	 wd 0.0500	time 0.2957 (0.3161)	loss 1.0704 (1.1642)	grad_norm 0.3444 (0.3527)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:12:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:05:16 lr 0.000006	 wd 0.0500	time 0.3030 (0.3155)	loss 1.3204 (1.1669)	grad_norm 0.3856 (0.3519)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:13:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:04:44 lr 0.000006	 wd 0.0500	time 0.3010 (0.3151)	loss 1.2967 (1.1690)	grad_norm 0.3325 (0.3510)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:13:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:04:12 lr 0.000006	 wd 0.0500	time 0.3187 (0.3151)	loss 1.6117 (1.1703)	grad_norm 0.3180 (0.3505)	loss_scale 4096.0000 (2134.6878)	mem 15606MB
[2024-08-02 20:14:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:03:41 lr 0.000006	 wd 0.0500	time 0.3225 (0.3154)	loss 0.7627 (1.1679)	grad_norm 0.3358 (0.3500)	loss_scale 4096.0000 (2243.5891)	mem 15606MB
[2024-08-02 20:15:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:03:09 lr 0.000006	 wd 0.0500	time 0.3267 (0.3151)	loss 0.8375 (1.1692)	grad_norm 0.3335 (0.3493)	loss_scale 4096.0000 (2341.0331)	mem 15606MB
[2024-08-02 20:15:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:02:37 lr 0.000006	 wd 0.0500	time 0.3145 (0.3147)	loss 1.1007 (1.1683)	grad_norm 0.3505 (0.3489)	loss_scale 4096.0000 (2428.7376)	mem 15606MB
[2024-08-02 20:16:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:02:06 lr 0.000006	 wd 0.0500	time 0.3035 (0.3144)	loss 1.2419 (1.1683)	grad_norm 0.3176 (0.3482)	loss_scale 4096.0000 (2508.0933)	mem 15606MB
[2024-08-02 20:16:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:01:34 lr 0.000006	 wd 0.0500	time 0.3040 (0.3142)	loss 1.3920 (1.1694)	grad_norm 0.3338 (0.3478)	loss_scale 4096.0000 (2580.2381)	mem 15606MB
[2024-08-02 20:17:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:01:03 lr 0.000006	 wd 0.0500	time 0.3631 (0.3141)	loss 0.8682 (1.1706)	grad_norm 0.3327 (0.3473)	loss_scale 4096.0000 (2646.1121)	mem 15606MB
[2024-08-02 20:17:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:32 lr 0.000006	 wd 0.0500	time 0.3027 (0.3150)	loss 0.7913 (1.1699)	grad_norm 0.3414 (0.3468)	loss_scale 4096.0000 (2706.4990)	mem 15606MB
[2024-08-02 20:18:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.2739 (0.3144)	loss 1.0534 (1.1718)	grad_norm 0.3238 (0.3466)	loss_scale 4096.0000 (2762.0568)	mem 15606MB
[2024-08-02 20:18:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 20 training takes 0:13:09
[2024-08-02 20:18:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_20.pth saving......
[2024-08-02 20:18:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_20.pth saved !!!
[2024-08-02 20:18:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.042 (11.042)	Loss 0.4980 (0.4980)	Acc@1 92.578 (92.578)	Acc@5 98.047 (98.047)	Mem 15606MB
[2024-08-02 20:18:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.082 Acc@5 97.846
[2024-08-02 20:18:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 20:18:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 20:18:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:57:18 lr 0.000006	 wd 0.0500	time 11.4461 (11.4461)	loss 1.0005 (1.0005)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:19:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:16:42 lr 0.000006	 wd 0.0500	time 0.2901 (0.4173)	loss 1.3616 (1.1917)	grad_norm 0.3612 (0.3340)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:19:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:13:52 lr 0.000006	 wd 0.0500	time 0.3189 (0.3617)	loss 1.3210 (1.1865)	grad_norm 0.3363 (0.3444)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:20:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:12:37 lr 0.000006	 wd 0.0500	time 0.3007 (0.3439)	loss 1.1845 (1.1762)	grad_norm 0.3864 (0.3475)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:20:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:11:44 lr 0.000006	 wd 0.0500	time 0.3180 (0.3350)	loss 1.3642 (1.1681)	grad_norm 0.3281 (0.3466)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:21:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:10:59 lr 0.000006	 wd 0.0500	time 0.2758 (0.3297)	loss 0.9075 (1.1631)	grad_norm 0.3306 (0.3456)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:21:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:10:19 lr 0.000006	 wd 0.0500	time 0.3061 (0.3258)	loss 1.3039 (1.1583)	grad_norm 0.3254 (0.3450)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:22:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:09:42 lr 0.000006	 wd 0.0500	time 0.3233 (0.3230)	loss 1.2381 (1.1560)	grad_norm 0.3286 (0.3443)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:22:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:09:06 lr 0.000006	 wd 0.0500	time 0.2854 (0.3210)	loss 1.2152 (1.1545)	grad_norm 0.3404 (0.3429)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:23:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:08:32 lr 0.000005	 wd 0.0500	time 0.2787 (0.3200)	loss 1.4413 (1.1576)	grad_norm 0.3485 (0.3421)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:23:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:07:58 lr 0.000005	 wd 0.0500	time 0.2860 (0.3184)	loss 1.5007 (1.1609)	grad_norm 0.3290 (0.3421)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:24:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:07:24 lr 0.000005	 wd 0.0500	time 0.2867 (0.3171)	loss 0.8089 (1.1641)	grad_norm 0.3271 (0.3418)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:24:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:06:51 lr 0.000005	 wd 0.0500	time 0.2849 (0.3162)	loss 1.3977 (1.1611)	grad_norm 0.3365 (0.3415)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:25:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:06:19 lr 0.000005	 wd 0.0500	time 0.2943 (0.3153)	loss 1.1993 (1.1615)	grad_norm 0.3263 (0.3412)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:26:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:05:46 lr 0.000005	 wd 0.0500	time 0.3058 (0.3147)	loss 0.8318 (1.1589)	grad_norm 0.3493 (0.3411)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:26:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:05:15 lr 0.000005	 wd 0.0500	time 0.2897 (0.3149)	loss 0.8454 (1.1605)	grad_norm 0.3342 (0.3411)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:27:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:04:43 lr 0.000005	 wd 0.0500	time 0.3112 (0.3143)	loss 1.0511 (1.1571)	grad_norm 0.3534 (0.3409)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:27:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:04:12 lr 0.000005	 wd 0.0500	time 0.2880 (0.3143)	loss 1.3529 (1.1579)	grad_norm 0.3204 (0.3412)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:28:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:03:40 lr 0.000005	 wd 0.0500	time 0.2948 (0.3144)	loss 0.7742 (1.1583)	grad_norm 0.3493 (0.3408)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:28:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:03:09 lr 0.000005	 wd 0.0500	time 0.3189 (0.3144)	loss 0.8479 (1.1591)	grad_norm 0.3289 (0.3406)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:29:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:02:37 lr 0.000005	 wd 0.0500	time 0.3076 (0.3144)	loss 1.0683 (1.1581)	grad_norm 0.3561 (0.3410)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:29:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:02:06 lr 0.000005	 wd 0.0500	time 0.2836 (0.3142)	loss 1.1387 (1.1579)	grad_norm 0.3318 (0.3415)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:30:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:01:34 lr 0.000005	 wd 0.0500	time 0.2976 (0.3140)	loss 1.4501 (1.1564)	grad_norm 0.3374 (0.3412)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:30:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:01:03 lr 0.000005	 wd 0.0500	time 0.2832 (0.3139)	loss 1.5450 (1.1581)	grad_norm 0.3536 (0.3436)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:31:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:32 lr 0.000005	 wd 0.0500	time 0.2878 (0.3139)	loss 1.2232 (1.1599)	grad_norm 0.3422 (0.3434)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:31:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.2753 (0.3132)	loss 1.4883 (1.1604)	grad_norm 0.3334 (0.3432)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:31:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 21 training takes 0:13:06
[2024-08-02 20:31:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 10.778 (10.778)	Loss 0.5181 (0.5181)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 20:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.082 Acc@5 97.866
[2024-08-02 20:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 20:32:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 20:32:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:45:37 lr 0.000005	 wd 0.0500	time 11.1659 (11.1659)	loss 0.9919 (0.9919)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:32:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:16:32 lr 0.000005	 wd 0.0500	time 0.2815 (0.4133)	loss 1.4788 (1.1770)	grad_norm 0.3599 (0.3362)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:33:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:13:45 lr 0.000005	 wd 0.0500	time 0.3046 (0.3587)	loss 0.7478 (1.1696)	grad_norm 0.3216 (0.3395)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 20:33:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:12:27 lr 0.000005	 wd 0.0500	time 0.3168 (0.3396)	loss 1.3727 (1.1751)	grad_norm 0.3318 (nan)	loss_scale 2048.0000 (3660.5449)	mem 15606MB
[2024-08-02 20:34:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:11:37 lr 0.000005	 wd 0.0500	time 0.3279 (0.3316)	loss 0.7584 (1.1810)	grad_norm 0.3284 (nan)	loss_scale 2048.0000 (3258.4140)	mem 15606MB
[2024-08-02 20:34:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:10:54 lr 0.000005	 wd 0.0500	time 0.3168 (0.3268)	loss 1.2001 (1.1784)	grad_norm 0.3204 (nan)	loss_scale 2048.0000 (3016.8144)	mem 15606MB
[2024-08-02 20:35:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:10:15 lr 0.000005	 wd 0.0500	time 0.2801 (0.3237)	loss 0.8806 (1.1800)	grad_norm 0.3229 (nan)	loss_scale 2048.0000 (2855.6140)	mem 15606MB
[2024-08-02 20:35:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:09:38 lr 0.000005	 wd 0.0500	time 0.3170 (0.3211)	loss 1.3505 (1.1746)	grad_norm 0.3428 (nan)	loss_scale 2048.0000 (2740.4051)	mem 15606MB
[2024-08-02 20:36:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:09:03 lr 0.000004	 wd 0.0500	time 0.3114 (0.3195)	loss 0.9311 (1.1734)	grad_norm 0.3195 (nan)	loss_scale 2048.0000 (2653.9625)	mem 15606MB
[2024-08-02 20:37:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:08:30 lr 0.000004	 wd 0.0500	time 0.3393 (0.3184)	loss 1.1053 (1.1722)	grad_norm 0.3463 (nan)	loss_scale 2048.0000 (2586.7081)	mem 15606MB
[2024-08-02 20:37:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:07:56 lr 0.000004	 wd 0.0500	time 0.2770 (0.3171)	loss 1.2755 (1.1725)	grad_norm 0.3267 (nan)	loss_scale 2048.0000 (2532.8911)	mem 15606MB
[2024-08-02 20:38:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:07:23 lr 0.000004	 wd 0.0500	time 0.2750 (0.3166)	loss 0.8096 (1.1697)	grad_norm 0.3398 (nan)	loss_scale 2048.0000 (2488.8501)	mem 15606MB
[2024-08-02 20:38:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:06:52 lr 0.000004	 wd 0.0500	time 0.2775 (0.3172)	loss 1.3804 (1.1706)	grad_norm 0.3274 (nan)	loss_scale 2048.0000 (2452.1432)	mem 15606MB
[2024-08-02 20:39:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:06:19 lr 0.000004	 wd 0.0500	time 0.2769 (0.3161)	loss 0.8561 (1.1643)	grad_norm 0.3143 (nan)	loss_scale 2048.0000 (2421.0792)	mem 15606MB
[2024-08-02 20:39:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:05:49 lr 0.000004	 wd 0.0500	time 0.3250 (0.3167)	loss 1.0084 (1.1589)	grad_norm 0.3403 (nan)	loss_scale 2048.0000 (2394.4497)	mem 15606MB
[2024-08-02 20:40:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:05:16 lr 0.000004	 wd 0.0500	time 0.3235 (0.3161)	loss 1.2017 (1.1585)	grad_norm 0.3118 (nan)	loss_scale 2048.0000 (2371.3684)	mem 15606MB
[2024-08-02 20:40:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:04:44 lr 0.000004	 wd 0.0500	time 0.2947 (0.3158)	loss 1.5201 (1.1586)	grad_norm 0.3286 (nan)	loss_scale 2048.0000 (2351.1705)	mem 15606MB
[2024-08-02 20:41:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:04:12 lr 0.000004	 wd 0.0500	time 0.2846 (0.3152)	loss 0.9030 (1.1568)	grad_norm 0.3423 (nan)	loss_scale 2048.0000 (2333.3474)	mem 15606MB
[2024-08-02 20:41:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:03:41 lr 0.000004	 wd 0.0500	time 0.3248 (0.3153)	loss 1.4186 (1.1554)	grad_norm 0.3137 (nan)	loss_scale 2048.0000 (2317.5036)	mem 15606MB
[2024-08-02 20:42:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:03:09 lr 0.000004	 wd 0.0500	time 0.3125 (0.3156)	loss 0.9400 (1.1563)	grad_norm 0.3234 (nan)	loss_scale 2048.0000 (2303.3267)	mem 15606MB
[2024-08-02 20:42:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:02:38 lr 0.000004	 wd 0.0500	time 0.3221 (0.3159)	loss 0.9252 (1.1573)	grad_norm 0.3240 (nan)	loss_scale 2048.0000 (2290.5667)	mem 15606MB
[2024-08-02 20:43:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:02:07 lr 0.000004	 wd 0.0500	time 0.3452 (0.3166)	loss 1.3002 (1.1588)	grad_norm 0.3223 (nan)	loss_scale 2048.0000 (2279.0214)	mem 15606MB
[2024-08-02 20:43:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:01:35 lr 0.000004	 wd 0.0500	time 0.3135 (0.3163)	loss 1.0895 (1.1587)	grad_norm 0.3442 (nan)	loss_scale 2048.0000 (2268.5252)	mem 15606MB
[2024-08-02 20:44:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:01:03 lr 0.000004	 wd 0.0500	time 0.2986 (0.3160)	loss 1.1427 (1.1588)	grad_norm 0.3194 (nan)	loss_scale 2048.0000 (2258.9413)	mem 15606MB
[2024-08-02 20:44:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:32 lr 0.000004	 wd 0.0500	time 0.2810 (0.3158)	loss 1.3457 (1.1598)	grad_norm 0.3505 (nan)	loss_scale 2048.0000 (2250.1558)	mem 15606MB
[2024-08-02 20:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.2750 (0.3150)	loss 1.1137 (1.1594)	grad_norm 0.3436 (nan)	loss_scale 2048.0000 (2242.0728)	mem 15606MB
[2024-08-02 20:45:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 22 training takes 0:13:10
[2024-08-02 20:45:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.629 (12.629)	Loss 0.5015 (0.5015)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 20:45:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.092 Acc@5 97.860
[2024-08-02 20:45:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 20:45:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 20:46:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][0/2502]	eta 7:41:57 lr 0.000004	 wd 0.0500	time 11.0782 (11.0782)	loss 0.7682 (0.7682)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:46:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:16:29 lr 0.000004	 wd 0.0500	time 0.2807 (0.4119)	loss 1.4747 (1.1487)	grad_norm 0.3401 (0.3407)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:47:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:13:48 lr 0.000004	 wd 0.0500	time 0.2850 (0.3598)	loss 0.8511 (1.1537)	grad_norm 0.3379 (0.3406)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:47:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:12:30 lr 0.000004	 wd 0.0500	time 0.2880 (0.3408)	loss 1.4216 (1.1447)	grad_norm 3.4125 (0.3593)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:48:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:11:37 lr 0.000004	 wd 0.0500	time 0.3274 (0.3318)	loss 0.9053 (1.1503)	grad_norm 0.3468 (0.3535)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:48:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:10:54 lr 0.000004	 wd 0.0500	time 0.2821 (0.3268)	loss 0.8148 (1.1535)	grad_norm 0.3149 (0.3503)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:49:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:10:15 lr 0.000004	 wd 0.0500	time 0.2925 (0.3235)	loss 1.4377 (1.1527)	grad_norm 0.3536 (0.3510)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:49:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:09:40 lr 0.000004	 wd 0.0500	time 0.2852 (0.3219)	loss 1.1384 (1.1561)	grad_norm 0.3476 (0.3492)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:50:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:09:04 lr 0.000003	 wd 0.0500	time 0.2993 (0.3200)	loss 1.4394 (1.1577)	grad_norm 0.3253 (0.3480)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:50:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:08:29 lr 0.000003	 wd 0.0500	time 0.3135 (0.3182)	loss 1.1421 (1.1584)	grad_norm 0.3280 (0.3467)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:51:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:07:56 lr 0.000003	 wd 0.0500	time 0.3184 (0.3172)	loss 1.1338 (1.1540)	grad_norm 0.3025 (0.3460)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:51:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:07:23 lr 0.000003	 wd 0.0500	time 0.3208 (0.3162)	loss 1.0420 (1.1568)	grad_norm 0.3406 (0.3457)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:52:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:06:51 lr 0.000003	 wd 0.0500	time 0.3098 (0.3157)	loss 1.4110 (1.1598)	grad_norm 0.3277 (0.3449)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:52:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:06:18 lr 0.000003	 wd 0.0500	time 0.2864 (0.3150)	loss 1.2543 (1.1605)	grad_norm 0.3193 (0.3457)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:53:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:05:46 lr 0.000003	 wd 0.0500	time 0.3240 (0.3144)	loss 1.2759 (1.1607)	grad_norm 0.3491 (0.3448)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:53:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:05:14 lr 0.000003	 wd 0.0500	time 0.2848 (0.3142)	loss 1.2953 (1.1591)	grad_norm 0.3337 (0.3445)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:54:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:04:42 lr 0.000003	 wd 0.0500	time 0.3121 (0.3136)	loss 1.1411 (1.1623)	grad_norm 0.3174 (0.3446)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:54:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:04:11 lr 0.000003	 wd 0.0500	time 0.3066 (0.3133)	loss 0.7191 (1.1597)	grad_norm 0.3373 (0.3445)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 20:55:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:03:40 lr 0.000003	 wd 0.0500	time 0.2961 (0.3135)	loss 1.3110 (1.1589)	grad_norm 0.3727 (0.3469)	loss_scale 4096.0000 (2123.0516)	mem 15606MB
[2024-08-02 20:55:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:03:08 lr 0.000003	 wd 0.0500	time 0.2821 (0.3134)	loss 1.2981 (1.1583)	grad_norm 0.3244 (0.3463)	loss_scale 4096.0000 (2226.8364)	mem 15606MB
[2024-08-02 20:56:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:02:37 lr 0.000003	 wd 0.0500	time 0.3090 (0.3133)	loss 0.9330 (1.1580)	grad_norm 0.3429 (0.3457)	loss_scale 4096.0000 (2320.2479)	mem 15606MB
[2024-08-02 20:56:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:02:05 lr 0.000003	 wd 0.0500	time 0.3140 (0.3132)	loss 1.3892 (1.1583)	grad_norm 0.3567 (0.3454)	loss_scale 4096.0000 (2404.7673)	mem 15606MB
[2024-08-02 20:57:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:01:34 lr 0.000003	 wd 0.0500	time 0.3358 (0.3130)	loss 1.3268 (1.1568)	grad_norm 0.3508 (0.3449)	loss_scale 4096.0000 (2481.6065)	mem 15606MB
[2024-08-02 20:57:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:01:03 lr 0.000003	 wd 0.0500	time 0.3103 (0.3131)	loss 1.3665 (1.1569)	grad_norm 0.3415 (0.3445)	loss_scale 4096.0000 (2551.7671)	mem 15606MB
[2024-08-02 20:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:31 lr 0.000003	 wd 0.0500	time 0.2933 (0.3131)	loss 0.8190 (1.1568)	grad_norm 0.3289 (0.3457)	loss_scale 4096.0000 (2616.0833)	mem 15606MB
[2024-08-02 20:58:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.2756 (0.3123)	loss 0.8676 (1.1560)	grad_norm 0.3286 (0.3455)	loss_scale 4096.0000 (2675.2563)	mem 15606MB
[2024-08-02 20:59:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 23 training takes 0:13:04
[2024-08-02 20:59:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.451 (12.451)	Loss 0.5200 (0.5200)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 15606MB
[2024-08-02 20:59:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.092 Acc@5 97.854
[2024-08-02 20:59:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 20:59:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 20:59:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][0/2502]	eta 7:57:02 lr 0.000003	 wd 0.0500	time 11.4397 (11.4397)	loss 1.2261 (1.2261)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:00:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:16:39 lr 0.000003	 wd 0.0500	time 0.2838 (0.4161)	loss 0.9486 (1.1629)	grad_norm 0.3200 (0.3358)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:00:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:13:44 lr 0.000003	 wd 0.0500	time 0.3106 (0.3582)	loss 1.2304 (1.1523)	grad_norm 0.3162 (0.3386)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:01:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:12:28 lr 0.000003	 wd 0.0500	time 0.3119 (0.3398)	loss 1.3746 (1.1666)	grad_norm 0.3273 (0.3508)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:01:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:11:36 lr 0.000003	 wd 0.0500	time 0.2909 (0.3312)	loss 0.7268 (1.1663)	grad_norm 0.3424 (0.3476)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:02:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:10:51 lr 0.000003	 wd 0.0500	time 0.2915 (0.3256)	loss 1.4342 (1.1666)	grad_norm 0.3530 (0.3517)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:02:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:10:13 lr 0.000003	 wd 0.0500	time 0.2777 (0.3226)	loss 0.6886 (1.1616)	grad_norm 0.3270 (0.3496)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:03:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:09:36 lr 0.000003	 wd 0.0500	time 0.2996 (0.3200)	loss 1.1985 (1.1602)	grad_norm 0.3199 (0.3480)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:03:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:09:00 lr 0.000003	 wd 0.0500	time 0.2870 (0.3178)	loss 1.2033 (1.1622)	grad_norm 0.3387 (0.3467)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:04:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:08:27 lr 0.000003	 wd 0.0500	time 0.2834 (0.3167)	loss 0.9540 (1.1660)	grad_norm 0.3219 (0.3453)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:04:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:07:54 lr 0.000003	 wd 0.0500	time 0.2853 (0.3157)	loss 0.8070 (1.1630)	grad_norm 0.3381 (0.3463)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:05:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:07:21 lr 0.000003	 wd 0.0500	time 0.2965 (0.3152)	loss 0.8601 (1.1623)	grad_norm 0.3445 (0.3464)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:05:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:06:49 lr 0.000002	 wd 0.0500	time 0.3083 (0.3145)	loss 0.9010 (1.1604)	grad_norm 0.3399 (0.3462)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:06:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:06:17 lr 0.000002	 wd 0.0500	time 0.2885 (0.3140)	loss 0.7722 (1.1574)	grad_norm 0.3463 (0.3455)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:06:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:05:45 lr 0.000002	 wd 0.0500	time 0.2953 (0.3137)	loss 1.4479 (1.1584)	grad_norm 0.3287 (0.3454)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:07:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:05:14 lr 0.000002	 wd 0.0500	time 0.2990 (0.3140)	loss 1.2462 (1.1546)	grad_norm 0.3412 (0.3450)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:07:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:04:43 lr 0.000002	 wd 0.0500	time 0.3220 (0.3139)	loss 1.2947 (1.1536)	grad_norm 0.3419 (0.3444)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:08:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:04:11 lr 0.000002	 wd 0.0500	time 0.3070 (0.3134)	loss 1.5071 (1.1550)	grad_norm 0.3401 (0.3448)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:08:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:03:39 lr 0.000002	 wd 0.0500	time 0.2982 (0.3132)	loss 1.0901 (1.1550)	grad_norm 0.3394 (0.3443)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:09:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:03:08 lr 0.000002	 wd 0.0500	time 0.3223 (0.3133)	loss 0.9146 (1.1562)	grad_norm 0.3310 (0.3439)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:09:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:02:37 lr 0.000002	 wd 0.0500	time 0.2856 (0.3132)	loss 1.2829 (1.1562)	grad_norm 0.3443 (0.3435)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:10:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:02:05 lr 0.000002	 wd 0.0500	time 0.3099 (0.3132)	loss 1.0000 (1.1564)	grad_norm 0.3402 (0.3433)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:11:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:01:34 lr 0.000002	 wd 0.0500	time 0.3329 (0.3131)	loss 1.0132 (1.1580)	grad_norm 0.3499 (0.3435)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:11:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:01:03 lr 0.000002	 wd 0.0500	time 0.2965 (0.3130)	loss 1.3793 (1.1604)	grad_norm 0.3601 (0.3433)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:12:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:31 lr 0.000002	 wd 0.0500	time 0.2866 (0.3129)	loss 1.2249 (1.1582)	grad_norm 0.3230 (0.3433)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:12:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.2757 (0.3121)	loss 0.9430 (1.1588)	grad_norm 0.3152 (0.3430)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:12:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 24 training takes 0:13:05
[2024-08-02 21:12:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.170 (11.170)	Loss 0.4990 (0.4990)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 21:13:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.094 Acc@5 97.848
[2024-08-02 21:13:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 21:13:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 21:13:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][0/2502]	eta 8:17:28 lr 0.000002	 wd 0.0500	time 11.9300 (11.9300)	loss 1.2757 (1.2757)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:13:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:16:58 lr 0.000002	 wd 0.0500	time 0.2831 (0.4239)	loss 1.1979 (1.2170)	grad_norm 0.3304 (0.3474)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:14:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:13:55 lr 0.000002	 wd 0.0500	time 0.2909 (0.3631)	loss 1.4364 (1.1860)	grad_norm 0.3359 (0.3567)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:14:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:12:35 lr 0.000002	 wd 0.0500	time 0.3091 (0.3432)	loss 1.1381 (1.1695)	grad_norm 0.3332 (0.3505)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:15:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:11:41 lr 0.000002	 wd 0.0500	time 0.2802 (0.3336)	loss 1.3044 (1.1705)	grad_norm 0.3190 (0.3497)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:15:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:10:56 lr 0.000002	 wd 0.0500	time 0.3075 (0.3281)	loss 1.4146 (1.1687)	grad_norm 0.3421 (0.3517)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:16:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:10:17 lr 0.000002	 wd 0.0500	time 0.2968 (0.3245)	loss 0.8231 (1.1587)	grad_norm 0.3345 (0.3482)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:16:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:09:40 lr 0.000002	 wd 0.0500	time 0.3082 (0.3224)	loss 0.8031 (1.1604)	grad_norm 0.3466 (0.3480)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:17:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:09:05 lr 0.000002	 wd 0.0500	time 0.2851 (0.3203)	loss 1.3623 (1.1594)	grad_norm 0.3600 (0.3471)	loss_scale 8192.0000 (4453.9526)	mem 15606MB
[2024-08-02 21:17:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:08:30 lr 0.000002	 wd 0.0500	time 0.2868 (0.3186)	loss 1.4423 (1.1592)	grad_norm 0.3376 (0.3456)	loss_scale 8192.0000 (4868.8302)	mem 15606MB
[2024-08-02 21:18:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:07:57 lr 0.000002	 wd 0.0500	time 0.3055 (0.3178)	loss 1.5144 (1.1606)	grad_norm 0.3379 (0.3449)	loss_scale 8192.0000 (5200.8152)	mem 15606MB
[2024-08-02 21:18:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:07:24 lr 0.000002	 wd 0.0500	time 0.3109 (0.3167)	loss 1.5484 (1.1614)	grad_norm 0.3738 (0.3441)	loss_scale 8192.0000 (5472.4941)	mem 15606MB
[2024-08-02 21:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:06:51 lr 0.000002	 wd 0.0500	time 0.3208 (0.3158)	loss 0.9775 (1.1579)	grad_norm 0.3237 (0.3440)	loss_scale 8192.0000 (5698.9309)	mem 15606MB
[2024-08-02 21:19:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:06:19 lr 0.000002	 wd 0.0500	time 0.3153 (0.3156)	loss 0.8270 (1.1568)	grad_norm 0.3505 (0.3436)	loss_scale 8192.0000 (5890.5580)	mem 15606MB
[2024-08-02 21:20:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:05:47 lr 0.000002	 wd 0.0500	time 0.2953 (0.3151)	loss 0.8059 (1.1563)	grad_norm 0.3638 (0.3435)	loss_scale 8192.0000 (6054.8294)	mem 15606MB
[2024-08-02 21:20:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:05:15 lr 0.000002	 wd 0.0500	time 0.3278 (0.3146)	loss 0.8633 (1.1565)	grad_norm 0.3457 (0.3435)	loss_scale 8192.0000 (6197.2125)	mem 15606MB
[2024-08-02 21:21:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:04:43 lr 0.000002	 wd 0.0500	time 0.3179 (0.3142)	loss 0.8093 (1.1557)	grad_norm 0.3220 (0.3432)	loss_scale 8192.0000 (6321.8089)	mem 15606MB
[2024-08-02 21:22:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:04:11 lr 0.000002	 wd 0.0500	time 0.3338 (0.3140)	loss 0.7582 (1.1534)	grad_norm 0.3371 (0.3428)	loss_scale 8192.0000 (6431.7554)	mem 15606MB
[2024-08-02 21:22:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:03:40 lr 0.000002	 wd 0.0500	time 0.3014 (0.3144)	loss 0.8811 (1.1553)	grad_norm 0.3351 (0.3424)	loss_scale 8192.0000 (6529.4925)	mem 15606MB
[2024-08-02 21:23:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:03:09 lr 0.000002	 wd 0.0500	time 0.2836 (0.3147)	loss 1.6556 (1.1577)	grad_norm 0.3320 (0.3425)	loss_scale 8192.0000 (6616.9469)	mem 15606MB
[2024-08-02 21:23:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:02:37 lr 0.000002	 wd 0.0500	time 0.3048 (0.3143)	loss 0.9935 (1.1582)	grad_norm 0.3277 (0.3423)	loss_scale 8192.0000 (6695.6602)	mem 15606MB
[2024-08-02 21:24:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:02:06 lr 0.000002	 wd 0.0500	time 0.3098 (0.3139)	loss 1.2799 (1.1594)	grad_norm 0.3245 (0.3428)	loss_scale 8192.0000 (6766.8805)	mem 15606MB
[2024-08-02 21:24:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:01:34 lr 0.000001	 wd 0.0500	time 0.2994 (0.3141)	loss 1.1145 (1.1597)	grad_norm 0.3346 (0.3424)	loss_scale 8192.0000 (6831.6293)	mem 15606MB
[2024-08-02 21:25:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:01:03 lr 0.000001	 wd 0.0500	time 0.2982 (0.3138)	loss 1.3415 (1.1591)	grad_norm 0.3260 (0.3424)	loss_scale 8192.0000 (6890.7501)	mem 15606MB
[2024-08-02 21:25:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:31 lr 0.000001	 wd 0.0500	time 0.3370 (0.3134)	loss 1.3567 (1.1582)	grad_norm 0.3059 (0.3422)	loss_scale 8192.0000 (6944.9463)	mem 15606MB
[2024-08-02 21:26:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2757 (0.3124)	loss 0.7467 (1.1570)	grad_norm 0.3627 (0.3422)	loss_scale 8192.0000 (6994.8085)	mem 15606MB
[2024-08-02 21:26:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 25 training takes 0:13:07
[2024-08-02 21:26:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.269 (12.269)	Loss 0.5249 (0.5249)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 15606MB
[2024-08-02 21:26:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.070 Acc@5 97.842
[2024-08-02 21:26:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 21:26:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 21:26:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:18:33 lr 0.000001	 wd 0.0500	time 10.5168 (10.5168)	loss 1.1717 (1.1717)	grad_norm 0.0000 (0.0000)	loss_scale 8192.0000 (8192.0000)	mem 15606MB
[2024-08-02 21:27:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:16:29 lr 0.000001	 wd 0.0500	time 0.2843 (0.4118)	loss 1.3110 (1.1701)	grad_norm 0.3227 (0.3379)	loss_scale 8192.0000 (8192.0000)	mem 15606MB
[2024-08-02 21:27:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:13:41 lr 0.000001	 wd 0.0500	time 0.2802 (0.3567)	loss 1.0878 (1.1693)	grad_norm 0.3433 (0.3373)	loss_scale 8192.0000 (8192.0000)	mem 15606MB
[2024-08-02 21:28:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:12:24 lr 0.000001	 wd 0.0500	time 0.3100 (0.3380)	loss 0.9140 (1.1514)	grad_norm 0.3238 (0.3382)	loss_scale 8192.0000 (8192.0000)	mem 15606MB
[2024-08-02 21:28:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:11:33 lr 0.000001	 wd 0.0500	time 0.3536 (0.3299)	loss 1.3514 (1.1499)	grad_norm 0.3449 (nan)	loss_scale 4096.0000 (7640.4190)	mem 15606MB
[2024-08-02 21:29:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:10:49 lr 0.000001	 wd 0.0500	time 0.3037 (0.3246)	loss 1.3203 (1.1587)	grad_norm 0.3722 (nan)	loss_scale 4096.0000 (6932.9501)	mem 15606MB
[2024-08-02 21:29:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:10:10 lr 0.000001	 wd 0.0500	time 0.2926 (0.3207)	loss 0.7889 (1.1626)	grad_norm 0.3389 (nan)	loss_scale 4096.0000 (6460.9118)	mem 15606MB
[2024-08-02 21:30:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:09:33 lr 0.000001	 wd 0.0500	time 0.2771 (0.3182)	loss 1.4009 (1.1586)	grad_norm 0.3132 (nan)	loss_scale 4096.0000 (6123.5492)	mem 15606MB
[2024-08-02 21:31:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:08:58 lr 0.000001	 wd 0.0500	time 0.3128 (0.3163)	loss 1.2455 (1.1626)	grad_norm 0.3463 (nan)	loss_scale 4096.0000 (5870.4220)	mem 15606MB
[2024-08-02 21:31:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:08:24 lr 0.000001	 wd 0.0500	time 0.3100 (0.3151)	loss 1.2736 (1.1617)	grad_norm 0.3256 (nan)	loss_scale 4096.0000 (5673.4828)	mem 15606MB
[2024-08-02 21:32:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:07:51 lr 0.000001	 wd 0.0500	time 0.2962 (0.3142)	loss 1.1182 (1.1606)	grad_norm 0.3306 (nan)	loss_scale 4096.0000 (5515.8921)	mem 15606MB
[2024-08-02 21:32:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:07:19 lr 0.000001	 wd 0.0500	time 0.3055 (0.3135)	loss 0.9370 (1.1606)	grad_norm 0.3308 (nan)	loss_scale 4096.0000 (5386.9282)	mem 15606MB
[2024-08-02 21:33:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:06:48 lr 0.000001	 wd 0.0500	time 0.2888 (0.3137)	loss 1.0776 (1.1623)	grad_norm 0.3324 (nan)	loss_scale 4096.0000 (5279.4405)	mem 15606MB
[2024-08-02 21:33:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:06:16 lr 0.000001	 wd 0.0500	time 0.3327 (0.3135)	loss 1.2808 (1.1638)	grad_norm 0.3442 (nan)	loss_scale 4096.0000 (5188.4766)	mem 15606MB
[2024-08-02 21:34:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:05:44 lr 0.000001	 wd 0.0500	time 0.2905 (0.3130)	loss 1.3803 (1.1647)	grad_norm 0.3334 (nan)	loss_scale 4096.0000 (5110.4982)	mem 15606MB
[2024-08-02 21:34:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:05:13 lr 0.000001	 wd 0.0500	time 0.3114 (0.3126)	loss 1.4105 (1.1664)	grad_norm 0.3278 (nan)	loss_scale 4096.0000 (5042.9101)	mem 15606MB
[2024-08-02 21:35:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:04:41 lr 0.000001	 wd 0.0500	time 0.2959 (0.3121)	loss 1.2072 (1.1637)	grad_norm 0.3309 (nan)	loss_scale 4096.0000 (4983.7651)	mem 15606MB
[2024-08-02 21:35:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:04:10 lr 0.000001	 wd 0.0500	time 0.3020 (0.3120)	loss 1.2980 (1.1619)	grad_norm 0.3262 (nan)	loss_scale 4096.0000 (4931.5744)	mem 15606MB
[2024-08-02 21:36:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:03:39 lr 0.000001	 wd 0.0500	time 0.2997 (0.3123)	loss 0.8848 (1.1619)	grad_norm 0.3501 (nan)	loss_scale 4096.0000 (4885.1793)	mem 15606MB
[2024-08-02 21:36:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:03:07 lr 0.000001	 wd 0.0500	time 0.3172 (0.3121)	loss 1.2990 (1.1629)	grad_norm 0.3479 (nan)	loss_scale 4096.0000 (4843.6654)	mem 15606MB
[2024-08-02 21:37:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:02:36 lr 0.000001	 wd 0.0500	time 0.3190 (0.3119)	loss 1.1868 (1.1614)	grad_norm 0.3390 (nan)	loss_scale 4096.0000 (4806.3008)	mem 15606MB
[2024-08-02 21:37:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:02:05 lr 0.000001	 wd 0.0500	time 0.3134 (0.3118)	loss 1.3725 (1.1603)	grad_norm 0.3463 (nan)	loss_scale 4096.0000 (4772.4931)	mem 15606MB
[2024-08-02 21:38:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:01:34 lr 0.000001	 wd 0.0500	time 0.2997 (0.3123)	loss 1.2812 (1.1588)	grad_norm 0.3322 (nan)	loss_scale 4096.0000 (4741.7574)	mem 15606MB
[2024-08-02 21:38:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:01:03 lr 0.000001	 wd 0.0500	time 0.3047 (0.3122)	loss 1.2904 (1.1590)	grad_norm 0.3141 (nan)	loss_scale 4096.0000 (4713.6932)	mem 15606MB
[2024-08-02 21:39:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:31 lr 0.000001	 wd 0.0500	time 0.2795 (0.3119)	loss 0.7169 (1.1594)	grad_norm 0.3293 (nan)	loss_scale 4096.0000 (4687.9667)	mem 15606MB
[2024-08-02 21:39:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2752 (0.3110)	loss 0.9396 (1.1599)	grad_norm 0.3472 (nan)	loss_scale 4096.0000 (4664.2975)	mem 15606MB
[2024-08-02 21:39:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 26 training takes 0:13:02
[2024-08-02 21:40:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 10.644 (10.644)	Loss 0.5068 (0.5068)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 21:40:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.100 Acc@5 97.854
[2024-08-02 21:40:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 21:40:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.10%
[2024-08-02 21:40:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][0/2502]	eta 7:36:50 lr 0.000001	 wd 0.0500	time 10.9555 (10.9555)	loss 1.4026 (1.4026)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:41:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:16:39 lr 0.000001	 wd 0.0500	time 0.2864 (0.4160)	loss 1.2784 (1.1504)	grad_norm 0.3064 (0.3333)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:41:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:13:46 lr 0.000001	 wd 0.0500	time 0.3010 (0.3592)	loss 1.3855 (1.1627)	grad_norm 0.3347 (0.3380)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:42:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:12:36 lr 0.000001	 wd 0.0500	time 0.3203 (0.3434)	loss 1.3506 (1.1622)	grad_norm 0.3385 (0.3411)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:42:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:11:39 lr 0.000001	 wd 0.0500	time 0.2767 (0.3327)	loss 0.9999 (1.1587)	grad_norm 0.3362 (0.3404)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:43:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:10:53 lr 0.000001	 wd 0.0500	time 0.2977 (0.3265)	loss 0.6756 (1.1598)	grad_norm 0.3389 (0.3401)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:43:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:10:16 lr 0.000001	 wd 0.0500	time 0.3245 (0.3243)	loss 1.3148 (1.1596)	grad_norm 0.3446 (0.3402)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:44:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:09:39 lr 0.000001	 wd 0.0500	time 0.3115 (0.3218)	loss 0.9820 (1.1614)	grad_norm 0.3456 (0.3399)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:44:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:09:03 lr 0.000001	 wd 0.0500	time 0.2909 (0.3192)	loss 0.9282 (1.1631)	grad_norm 0.3441 (0.3397)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:45:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:08:28 lr 0.000001	 wd 0.0500	time 0.3202 (0.3177)	loss 1.2410 (1.1611)	grad_norm 0.3234 (0.3395)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:45:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:07:56 lr 0.000001	 wd 0.0500	time 0.3104 (0.3170)	loss 1.1233 (1.1577)	grad_norm 0.3186 (0.3396)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:46:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:07:23 lr 0.000001	 wd 0.0500	time 0.2826 (0.3164)	loss 1.1397 (1.1598)	grad_norm 0.3292 (0.3418)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:46:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:06:51 lr 0.000001	 wd 0.0500	time 0.3129 (0.3160)	loss 0.7203 (1.1605)	grad_norm 0.3251 (0.3419)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:47:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:06:19 lr 0.000001	 wd 0.0500	time 0.2866 (0.3155)	loss 0.8064 (1.1573)	grad_norm 0.3452 (0.3421)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:47:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:05:47 lr 0.000001	 wd 0.0500	time 0.2946 (0.3152)	loss 0.8287 (1.1585)	grad_norm 0.3201 (0.3430)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:48:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:05:15 lr 0.000001	 wd 0.0500	time 0.2936 (0.3151)	loss 1.4466 (1.1595)	grad_norm 0.8981 (0.3435)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:48:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:04:44 lr 0.000001	 wd 0.0500	time 0.3076 (0.3150)	loss 1.3156 (1.1612)	grad_norm 0.3334 (0.3455)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:49:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:04:12 lr 0.000001	 wd 0.0500	time 0.3099 (0.3148)	loss 1.3514 (1.1629)	grad_norm 0.3394 (0.3509)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:49:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:03:40 lr 0.000001	 wd 0.0500	time 0.3075 (0.3145)	loss 1.4074 (1.1625)	grad_norm 0.3292 (0.3503)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:50:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:03:09 lr 0.000001	 wd 0.0500	time 0.2870 (0.3140)	loss 1.3762 (1.1633)	grad_norm 0.3291 (0.3497)	loss_scale 8192.0000 (4216.6607)	mem 15606MB
[2024-08-02 21:50:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:02:37 lr 0.000001	 wd 0.0500	time 0.3089 (0.3136)	loss 1.4117 (1.1641)	grad_norm 0.3200 (0.3491)	loss_scale 8192.0000 (4415.3283)	mem 15606MB
[2024-08-02 21:51:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:02:05 lr 0.000001	 wd 0.0500	time 0.3166 (0.3133)	loss 1.5016 (1.1632)	grad_norm 0.3442 (0.3486)	loss_scale 8192.0000 (4595.0842)	mem 15606MB
[2024-08-02 21:51:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:01:34 lr 0.000001	 wd 0.0500	time 0.2866 (0.3132)	loss 1.1859 (1.1644)	grad_norm 0.3408 (0.3482)	loss_scale 8192.0000 (4758.5061)	mem 15606MB
[2024-08-02 21:52:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:01:03 lr 0.000001	 wd 0.0500	time 0.2793 (0.3130)	loss 1.2980 (1.1642)	grad_norm 0.3239 (nan)	loss_scale 4096.0000 (4825.8392)	mem 15606MB
[2024-08-02 21:52:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:31 lr 0.000001	 wd 0.0500	time 0.2918 (0.3128)	loss 1.3441 (1.1658)	grad_norm 0.3719 (nan)	loss_scale 4096.0000 (4795.4419)	mem 15606MB
[2024-08-02 21:53:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.2758 (0.3121)	loss 1.2660 (1.1649)	grad_norm 0.3497 (nan)	loss_scale 4096.0000 (4767.4754)	mem 15606MB
[2024-08-02 21:53:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 27 training takes 0:13:06
[2024-08-02 21:53:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.137 (12.137)	Loss 0.4941 (0.4941)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 21:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.108 Acc@5 97.862
[2024-08-02 21:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 21:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.11%
[2024-08-02 21:53:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saving......
[2024-08-02 21:54:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_best.pth saved !!!
[2024-08-02 21:54:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:20:20 lr 0.000001	 wd 0.0500	time 10.5597 (10.5597)	loss 0.9609 (0.9609)	grad_norm 0.0000 (0.0000)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:54:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:16:27 lr 0.000000	 wd 0.0500	time 0.2873 (0.4113)	loss 0.9081 (1.2002)	grad_norm 0.3288 (0.3414)	loss_scale 4096.0000 (4096.0000)	mem 15606MB
[2024-08-02 21:55:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:13:39 lr 0.000000	 wd 0.0500	time 0.2921 (0.3562)	loss 1.5475 (1.1868)	grad_norm 0.3250 (nan)	loss_scale 2048.0000 (3831.0846)	mem 15606MB
[2024-08-02 21:55:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:12:24 lr 0.000000	 wd 0.0500	time 0.2836 (0.3382)	loss 1.4372 (1.1805)	grad_norm 0.3486 (nan)	loss_scale 2048.0000 (3238.6977)	mem 15606MB
[2024-08-02 21:56:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:11:32 lr 0.000000	 wd 0.0500	time 0.3069 (0.3293)	loss 0.7351 (1.1720)	grad_norm 0.3215 (nan)	loss_scale 2048.0000 (2941.7656)	mem 15606MB
[2024-08-02 21:56:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:10:49 lr 0.000000	 wd 0.0500	time 0.3119 (0.3243)	loss 0.7982 (1.1771)	grad_norm 0.3253 (nan)	loss_scale 2048.0000 (2763.3693)	mem 15606MB
[2024-08-02 21:57:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:10:10 lr 0.000000	 wd 0.0500	time 0.2832 (0.3209)	loss 0.8814 (1.1697)	grad_norm 0.3539 (nan)	loss_scale 2048.0000 (2644.3394)	mem 15606MB
[2024-08-02 21:57:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:09:34 lr 0.000000	 wd 0.0500	time 0.2951 (0.3191)	loss 1.2860 (1.1666)	grad_norm 0.3464 (nan)	loss_scale 2048.0000 (2559.2696)	mem 15606MB
[2024-08-02 21:58:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:09:00 lr 0.000000	 wd 0.0500	time 0.3030 (0.3175)	loss 0.8406 (1.1671)	grad_norm 0.3298 (nan)	loss_scale 2048.0000 (2495.4407)	mem 15606MB
[2024-08-02 21:58:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:08:26 lr 0.000000	 wd 0.0500	time 0.3594 (0.3163)	loss 0.8541 (1.1636)	grad_norm 0.3421 (nan)	loss_scale 2048.0000 (2445.7802)	mem 15606MB
[2024-08-02 21:59:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:07:53 lr 0.000000	 wd 0.0500	time 0.3167 (0.3154)	loss 1.0062 (1.1652)	grad_norm 0.3314 (nan)	loss_scale 2048.0000 (2406.0420)	mem 15606MB
[2024-08-02 21:59:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:07:20 lr 0.000000	 wd 0.0500	time 0.2822 (0.3145)	loss 1.2645 (1.1654)	grad_norm 0.3381 (nan)	loss_scale 2048.0000 (2373.5223)	mem 15606MB
[2024-08-02 22:00:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:06:48 lr 0.000000	 wd 0.0500	time 0.2891 (0.3134)	loss 1.4326 (1.1654)	grad_norm 0.3627 (nan)	loss_scale 2048.0000 (2346.4180)	mem 15606MB
[2024-08-02 22:00:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:06:16 lr 0.000000	 wd 0.0500	time 0.3124 (0.3129)	loss 1.5089 (1.1667)	grad_norm 0.3375 (nan)	loss_scale 2048.0000 (2323.4804)	mem 15606MB
[2024-08-02 22:01:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:05:44 lr 0.000000	 wd 0.0500	time 0.2884 (0.3126)	loss 0.9547 (1.1661)	grad_norm 0.3302 (nan)	loss_scale 2048.0000 (2303.8173)	mem 15606MB
[2024-08-02 22:01:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:05:12 lr 0.000000	 wd 0.0500	time 0.2865 (0.3122)	loss 1.1835 (1.1645)	grad_norm 0.3297 (nan)	loss_scale 2048.0000 (2286.7742)	mem 15606MB
[2024-08-02 22:02:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:04:41 lr 0.000000	 wd 0.0500	time 0.2826 (0.3124)	loss 1.3934 (1.1672)	grad_norm 0.3608 (nan)	loss_scale 2048.0000 (2271.8601)	mem 15606MB
[2024-08-02 22:02:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:04:10 lr 0.000000	 wd 0.0500	time 0.2875 (0.3120)	loss 1.2393 (1.1667)	grad_norm 0.3425 (nan)	loss_scale 2048.0000 (2258.6996)	mem 15606MB
[2024-08-02 22:03:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:03:38 lr 0.000000	 wd 0.0500	time 0.2918 (0.3119)	loss 0.7922 (1.1662)	grad_norm 0.3349 (nan)	loss_scale 2048.0000 (2247.0006)	mem 15606MB
[2024-08-02 22:03:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:03:07 lr 0.000000	 wd 0.0500	time 0.2877 (0.3117)	loss 1.0986 (1.1639)	grad_norm 0.3302 (nan)	loss_scale 2048.0000 (2236.5324)	mem 15606MB
[2024-08-02 22:04:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:02:36 lr 0.000000	 wd 0.0500	time 0.3147 (0.3123)	loss 1.1112 (1.1639)	grad_norm 0.4249 (nan)	loss_scale 2048.0000 (2227.1104)	mem 15606MB
[2024-08-02 22:04:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:02:05 lr 0.000000	 wd 0.0500	time 0.3123 (0.3122)	loss 0.9737 (1.1638)	grad_norm 0.3454 (nan)	loss_scale 2048.0000 (2218.5854)	mem 15606MB
[2024-08-02 22:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:01:34 lr 0.000000	 wd 0.0500	time 0.2809 (0.3122)	loss 1.2730 (1.1629)	grad_norm 0.3392 (nan)	loss_scale 2048.0000 (2210.8351)	mem 15606MB
[2024-08-02 22:05:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:01:03 lr 0.000000	 wd 0.0500	time 0.3007 (0.3120)	loss 1.2420 (1.1629)	grad_norm 0.3429 (nan)	loss_scale 2048.0000 (2203.7584)	mem 15606MB
[2024-08-02 22:06:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:31 lr 0.000000	 wd 0.0500	time 0.3232 (0.3124)	loss 1.5055 (1.1615)	grad_norm 0.3149 (nan)	loss_scale 2048.0000 (2197.2711)	mem 15606MB
[2024-08-02 22:07:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.3015 (0.3117)	loss 1.0801 (1.1608)	grad_norm 0.3178 (nan)	loss_scale 2048.0000 (2191.3027)	mem 15606MB
[2024-08-02 22:07:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 28 training takes 0:13:04
[2024-08-02 22:07:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 12.434 (12.434)	Loss 0.5015 (0.5015)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 22:07:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.090 Acc@5 97.878
[2024-08-02 22:07:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 22:07:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.11%
[2024-08-02 22:07:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][0/2502]	eta 8:30:00 lr 0.000000	 wd 0.0500	time 12.2304 (12.2304)	loss 1.2394 (1.2394)	grad_norm 0.0000 (0.0000)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:08:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:16:51 lr 0.000000	 wd 0.0500	time 0.2836 (0.4209)	loss 1.4118 (1.1719)	grad_norm 0.3394 (0.3407)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:08:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:13:52 lr 0.000000	 wd 0.0500	time 0.2784 (0.3618)	loss 0.8179 (1.1704)	grad_norm 0.3562 (0.3394)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:09:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:12:34 lr 0.000000	 wd 0.0500	time 0.2938 (0.3428)	loss 0.8722 (1.1670)	grad_norm 0.3412 (0.3394)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:09:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:11:41 lr 0.000000	 wd 0.0500	time 0.2814 (0.3337)	loss 1.1210 (1.1607)	grad_norm 0.3396 (0.3383)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:10:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:10:56 lr 0.000000	 wd 0.0500	time 0.2858 (0.3278)	loss 1.3054 (1.1564)	grad_norm 0.3424 (0.3382)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:10:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:10:16 lr 0.000000	 wd 0.0500	time 0.2911 (0.3243)	loss 0.7256 (1.1596)	grad_norm 0.3347 (0.3382)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:11:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:09:44 lr 0.000000	 wd 0.0500	time 0.3064 (0.3245)	loss 1.1193 (1.1602)	grad_norm 0.3385 (0.3383)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:11:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:09:08 lr 0.000000	 wd 0.0500	time 0.2961 (0.3223)	loss 0.7688 (1.1610)	grad_norm 0.3221 (0.3413)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:12:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:08:33 lr 0.000000	 wd 0.0500	time 0.3165 (0.3204)	loss 1.4937 (1.1624)	grad_norm 0.3415 (0.3483)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:12:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:07:58 lr 0.000000	 wd 0.0500	time 0.3132 (0.3189)	loss 1.4441 (1.1632)	grad_norm 0.3416 (0.3480)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:13:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:07:25 lr 0.000000	 wd 0.0500	time 0.3215 (0.3177)	loss 1.2459 (1.1630)	grad_norm 0.3232 (0.3468)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:13:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:06:52 lr 0.000000	 wd 0.0500	time 0.3125 (0.3171)	loss 1.0214 (1.1608)	grad_norm 0.3151 (0.3458)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:14:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:06:21 lr 0.000000	 wd 0.0500	time 0.3154 (0.3173)	loss 1.1349 (1.1643)	grad_norm 0.3552 (0.3456)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:15:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:05:48 lr 0.000000	 wd 0.0500	time 0.2818 (0.3164)	loss 1.1232 (1.1592)	grad_norm 0.3552 (0.3453)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:15:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:05:16 lr 0.000000	 wd 0.0500	time 0.3028 (0.3158)	loss 1.0244 (1.1595)	grad_norm 0.3315 (0.3446)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:16:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:04:45 lr 0.000000	 wd 0.0500	time 0.3137 (0.3167)	loss 0.8145 (1.1616)	grad_norm 0.3267 (0.3450)	loss_scale 2048.0000 (2048.0000)	mem 15606MB
[2024-08-02 22:16:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:04:13 lr 0.000000	 wd 0.0500	time 0.3352 (0.3164)	loss 1.3606 (1.1642)	grad_norm 0.3389 (0.3450)	loss_scale 4096.0000 (2081.7119)	mem 15606MB
[2024-08-02 22:17:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:03:41 lr 0.000000	 wd 0.0500	time 0.2885 (0.3161)	loss 1.1727 (1.1626)	grad_norm 0.3179 (0.3451)	loss_scale 4096.0000 (2193.5547)	mem 15606MB
[2024-08-02 22:17:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:03:10 lr 0.000000	 wd 0.0500	time 0.2829 (0.3157)	loss 0.7582 (1.1629)	grad_norm 0.3472 (0.3448)	loss_scale 4096.0000 (2293.6307)	mem 15606MB
[2024-08-02 22:18:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:02:38 lr 0.000000	 wd 0.0500	time 0.3219 (0.3154)	loss 0.9187 (1.1647)	grad_norm 0.3365 (0.3451)	loss_scale 4096.0000 (2383.7041)	mem 15606MB
[2024-08-02 22:18:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:02:06 lr 0.000000	 wd 0.0500	time 0.2979 (0.3151)	loss 0.7534 (1.1656)	grad_norm 0.3038 (0.3447)	loss_scale 4096.0000 (2465.2032)	mem 15606MB
[2024-08-02 22:19:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:01:35 lr 0.000000	 wd 0.0500	time 0.2826 (0.3155)	loss 0.8349 (1.1661)	grad_norm 0.3417 (0.3449)	loss_scale 4096.0000 (2539.2967)	mem 15606MB
[2024-08-02 22:19:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:01:03 lr 0.000000	 wd 0.0500	time 0.2987 (0.3153)	loss 0.8553 (1.1632)	grad_norm 0.3515 (0.3447)	loss_scale 4096.0000 (2606.9500)	mem 15606MB
[2024-08-02 22:20:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:32 lr 0.000000	 wd 0.0500	time 0.3214 (0.3151)	loss 1.2628 (1.1633)	grad_norm 0.3411 (0.3448)	loss_scale 4096.0000 (2668.9679)	mem 15606MB
[2024-08-02 22:20:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.2915 (0.3143)	loss 1.3176 (1.1626)	grad_norm 0.3210 (0.3444)	loss_scale 4096.0000 (2726.0264)	mem 15606MB
[2024-08-02 22:20:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 249): INFO EPOCH 29 training takes 0:13:10
[2024-08-02 22:20:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_29.pth saving......
[2024-08-02 22:20:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1/diffusion_ft_adapter_smt_l_step_cross1/ckpt_epoch_29.pth saved !!!
[2024-08-02 22:21:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 289): INFO Test: [0/98]	Time 11.576 (11.576)	Loss 0.4915 (0.4915)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 15606MB
[2024-08-02 22:21:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 296): INFO  * Acc@1 86.092 Acc@5 97.854
[2024-08-02 22:21:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-02 22:21:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 182): INFO Max accuracy: 86.11%
[2024-08-02 22:21:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_cross1] (main.py 189): INFO Training time 6:47:14
