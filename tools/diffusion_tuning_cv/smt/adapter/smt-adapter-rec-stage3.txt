[2024-07-31 21:56:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 366): INFO Full config saved to pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/config.json
[2024-07-31 21:56:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 369): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.2
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /mnt/data/ImageNet1k/
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 32
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3
  NUM_CLASSES: 1000
  PRETRAINED: /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth
  RESUME: ''
  SWIN:
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  TYPE: adapter_smt_diffusion_finetune
  VCNU_CONVNEXT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    DEPTHS:
    - 3
    - 3
    - 9
    - 3
    DIMS:
    - 96
    - 192
    - 384
    - 768
    FILTER_STRATEGY1: 18
    FILTER_STRATEGY2: 6
    FINETUNE_MODE: stage0
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MODEL_STYLE: trans
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SMT:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    CA_ATTENTIONS:
    - 1
    - 1
    - 1
    - 0
    CA_NUM_HEADS:
    - 4
    - 4
    - 4
    - -1
    DEPTHS:
    - 4
    - 6
    - 28
    - 4
    EMBED_DIMS:
    - 96
    - 192
    - 384
    - 768
    EXPAND_RATIO: 2
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: stage3
    HEAD_CONV: 7
    IN_CHANS: 3
    LAYERSCALE_VALUE: 0.0001
    MLP_RATIOS:
    - 8
    - 6
    - 4
    - 2
    MODEL_STYLE: trans
    NUM_SCALE: 4
    NUM_STAGES: 4
    OUT_DIM: null
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    SA_NUM_HEADS:
    - -1
    - -1
    - 8
    - 16
    USE_LAYERSCALE: true
    USE_MEMORY_EMBEDDING: false
  VCNU_SWIN:
    AB_NORM_ATTN: true
    AB_NORM_LTM: false
    ADD_EXTRA_ADAPTER: true
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 84
    FILTER_STRATEGY1: 23
    FILTER_STRATEGY2: 7
    FINETUNE_MODE: full
    IN_CHANS: 3
    LAYER_SCALE_INIT_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_SCALE: 4
    OUT_DIM: null
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    QKV_BIAS: true
    QK_SCALE: null
    TRAINING_MODE: tfs
    USE_LAYERSCALE: false
    USE_MEMORY_EMBEDDING: false
    WINDOW_SIZE: 7
  generalVCNUS:
    ABLATION_STRATEGY: UMA
    AB_AGGREGATION_ATTN: cat
    AB_AGGREGATION_LTM: add
    AB_DOWNSAMPLING_STRATEGY: max
    AB_MEMORY_CREATION_STRATEGY: UMA
    AB_NORM_ATTN: true
    AB_NORM_ATTN_NAME: BN
    AB_NORM_LTM: true
    AB_NORM_LTM_NAME: BN
    AB_PATCH_NORM_NAME: BN
    AB_STRATEGY: statistic
    AB_USE_SEQUENCEFUNC: UMA
    AB_WM: l
    APE: false
    DEPTHS:
    - 3
    - 3
    - 12
    - 3
    EMBED_CONV: 7
    EMBED_DIM: 64
    FILTER_STRATEGY1: 12
    FILTER_STRATEGY2: 4
    IN_CHANS: 3
    KERNAL_SIZE: 11
    LAYERSCALE_VALUE: 1.0e-06
    MLP_RATIO: 4.0
    MODEL_STYLE: trans
    NUM_SCALE: 4
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAIN_IMAGE_SIZE: 224
    RECE_FIELD: 7
    SAVE_FREQ: 30
    USE_BIAS: true
    USE_FIBONACCI: true
    USE_LAYERSCALE: false
    USE_SEQUENCEFUNC: statistic
OUTPUT: pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3
PRINT_FREQ: 100
SAVE_FREQ: 10
SEED: 0
TAG: diffusion_ft_adapter_smt_l_step_stage3
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 2
  AUTO_RESUME: true
  BASE_LR: 2.0e-05
  CLIP_GRAD: 5.0
  EFFICIENT_FINETUNE: true
  EPOCHS: 30
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 2.0e-07
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 2.0e-08
  WEIGHT_DECAY: 0.05

[2024-07-31 21:56:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 370): INFO {"cfg": "/mnt/data/vcnu_expansibility_v2/configs/diffusion_finetune/smt/adapter_smt/diffusion_ft_adapter_smt_large_224_22kto1k_step_stage3.yaml", "opts": null, "batch_size": 64, "data_path": "/mnt/data/ImageNet1k/", "zip": false, "cache_mode": "part", "pretrained": "/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth", "resume": null, "accumulation_steps": 2, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "pretrain/diffusion_ft", "tag": "diffusion_ft_adapter_smt_l_step_stage3", "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-07-31 21:56:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 108): INFO Creating model:adapter_smt_diffusion_finetune/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3
[2024-07-31 21:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 110): INFO Adapter_SMT_Diffusion_Finetune(
  (uma): UMA(filter_strategy1=23, filter_strategy2=7,ablation_strategy=UMA, use_sequencefunc=linear,fftscale=4,)
  (patch_embed1): Head(
    (conv): Sequential(
      (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Conv2d(96, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (block1): ModuleList(
    (0): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=96, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=96, out_features=96, bias=True)
        (s): Linear(in_features=96, out_features=96, bias=True)
        (local_conv_1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (local_conv_2): Conv2d(24, 24, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=24)
        (local_conv_3): Conv2d(24, 24, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=24)
        (local_conv_4): Conv2d(24, 24, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=24)
        (proj0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), groups=24)
        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=96, emb_dim=24, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=96, out_features=24, bias=True)
        (up): Linear(in_features=24, out_features=96, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=96, out_features=768, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
        )
        (act): GELU()
        (fc2): Linear(in_features=768, out_features=96, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
  (patch_embed2): OverlapPatchEmbed(
    (proj): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  )
  (block2): ModuleList(
    (0): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=192, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=192, out_features=192, bias=True)
        (s): Linear(in_features=192, out_features=192, bias=True)
        (local_conv_1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (local_conv_2): Conv2d(48, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=48)
        (local_conv_3): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (local_conv_4): Conv2d(48, 48, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=48)
        (proj0): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1), groups=48)
        (bn): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=192, emb_dim=48, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=192, out_features=48, bias=True)
        (up): Linear(in_features=48, out_features=192, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=192, out_features=1152, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152)
        )
        (act): GELU()
        (fc2): Linear(in_features=1152, out_features=192, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
  (patch_embed3): OverlapPatchEmbed(
    (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  )
  (block3): ModuleList(
    (0): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (12): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (13): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (14): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (15): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (16): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (17): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (18): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (19): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (20): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (21): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (22): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (23): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (24): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (25): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (26): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (v): Linear(in_features=384, out_features=384, bias=True)
        (s): Linear(in_features=384, out_features=384, bias=True)
        (local_conv_1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (local_conv_2): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96)
        (local_conv_3): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (local_conv_4): Conv2d(96, 96, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=96)
        (proj0): Conv2d(384, 768, kernel_size=(1, 1), stride=(1, 1), groups=96)
        (bn): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (proj1): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (27): Block(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=384, out_features=384, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=384, out_features=768, bias=True)
        (local_conv): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
      )
      (adapter): Adapter(
        dim=384, emb_dim=96, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=384, out_features=96, bias=True)
        (up): Linear(in_features=96, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm3): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (patch_embed4): OverlapPatchEmbed(
    (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (block4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (act): GELU()
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (q): Linear(in_features=768, out_features=768, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (kv): Linear(in_features=768, out_features=1536, bias=True)
        (local_conv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)
      )
      (adapter): Adapter(
        dim=768, emb_dim=192, model_style=trans, 
        (activation): GELU()
        (down): Linear(in_features=768, out_features=192, bias=True)
        (up): Linear(in_features=192, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): DropPath()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=1536, bias=True)
        (dwconv): DWConv(
          (dwconv): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536)
        )
        (act): GELU()
        (fc2): Linear(in_features=1536, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm4): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
[2024-07-31 21:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 113): INFO number of params: 1954024
[2024-07-31 21:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 150): INFO no checkpoint found in pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3, ignoring auto resume
[2024-07-31 21:56:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 46): INFO ==============> Loading weight /mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth for fine-tuning......
[2024-07-31 21:56:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 127): WARNING <All keys matched successfully>
[2024-07-31 21:56:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 129): INFO => loaded successfully '/mnt/data/vcnu_expansibility_v2/pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage2/diffusion_ft_adapter_smt_l_step_stage2/ckpt_epoch_best.pth'
[2024-07-31 21:57:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 15.132 (15.132)	Loss 0.5000 (0.5000)	Acc@1 92.773 (92.773)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 21:57:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.080 Acc@5 97.876
[2024-07-31 21:57:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 162): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 21:57:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 168): INFO Start training
[2024-07-31 21:57:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][0/2502]	eta 7:29:36 lr 0.000000	 wd 0.0500	time 10.7818 (10.7818)	loss 1.5298 (1.5298)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:57:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][100/2502]	eta 0:11:22 lr 0.000000	 wd 0.0500	time 0.1565 (0.2843)	loss 1.2670 (1.2017)	grad_norm 0.3349 (0.3309)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:58:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][200/2502]	eta 0:08:43 lr 0.000000	 wd 0.0500	time 0.1671 (0.2273)	loss 1.0153 (1.1862)	grad_norm 0.3257 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:58:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][300/2502]	eta 0:07:40 lr 0.000000	 wd 0.0500	time 0.1873 (0.2091)	loss 0.8986 (1.1498)	grad_norm 0.3375 (0.3300)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:58:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][400/2502]	eta 0:06:58 lr 0.000001	 wd 0.0500	time 0.1698 (0.1992)	loss 0.9676 (1.1573)	grad_norm 0.3362 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:59:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][500/2502]	eta 0:06:25 lr 0.000001	 wd 0.0500	time 0.1830 (0.1927)	loss 1.1347 (1.1621)	grad_norm 0.3125 (0.3303)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:59:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][600/2502]	eta 0:06:00 lr 0.000001	 wd 0.0500	time 0.1757 (0.1897)	loss 1.3222 (1.1620)	grad_norm 0.3230 (0.3299)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:59:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][700/2502]	eta 0:05:37 lr 0.000001	 wd 0.0500	time 0.1609 (0.1874)	loss 1.2268 (1.1640)	grad_norm 0.3295 (0.3303)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 21:59:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][800/2502]	eta 0:05:15 lr 0.000001	 wd 0.0500	time 0.2213 (0.1853)	loss 1.0503 (1.1633)	grad_norm 0.3194 (0.3303)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:00:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][900/2502]	eta 0:04:54 lr 0.000001	 wd 0.0500	time 0.1615 (0.1841)	loss 1.3509 (1.1631)	grad_norm 0.3279 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:00:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1000/2502]	eta 0:04:34 lr 0.000002	 wd 0.0500	time 0.1623 (0.1828)	loss 1.4284 (1.1608)	grad_norm 0.3345 (0.3302)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:00:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1100/2502]	eta 0:04:14 lr 0.000002	 wd 0.0500	time 0.1616 (0.1817)	loss 1.2221 (1.1617)	grad_norm 0.3375 (0.3302)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:01:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1200/2502]	eta 0:03:55 lr 0.000002	 wd 0.0500	time 0.1629 (0.1810)	loss 0.9250 (1.1613)	grad_norm 0.3245 (0.3302)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:01:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1300/2502]	eta 0:03:37 lr 0.000002	 wd 0.0500	time 0.1812 (0.1806)	loss 1.2334 (1.1631)	grad_norm 0.3252 (0.3303)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:01:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1400/2502]	eta 0:03:18 lr 0.000002	 wd 0.0500	time 0.1575 (0.1800)	loss 1.4309 (1.1649)	grad_norm 0.3278 (0.3302)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:01:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1500/2502]	eta 0:02:59 lr 0.000002	 wd 0.0500	time 0.1738 (0.1795)	loss 1.1722 (1.1688)	grad_norm 0.3339 (0.3303)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:02:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1600/2502]	eta 0:02:41 lr 0.000003	 wd 0.0500	time 0.1762 (0.1790)	loss 1.2125 (1.1660)	grad_norm 0.3248 (0.3302)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:02:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1700/2502]	eta 0:02:23 lr 0.000003	 wd 0.0500	time 0.1694 (0.1786)	loss 0.8823 (1.1654)	grad_norm 0.3385 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:02:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1800/2502]	eta 0:02:05 lr 0.000003	 wd 0.0500	time 0.1590 (0.1783)	loss 1.1172 (1.1654)	grad_norm 0.3208 (0.3300)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:03:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][1900/2502]	eta 0:01:47 lr 0.000003	 wd 0.0500	time 0.1922 (0.1779)	loss 1.4069 (1.1650)	grad_norm 0.3206 (0.3300)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:03:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2000/2502]	eta 0:01:29 lr 0.000003	 wd 0.0500	time 0.1685 (0.1776)	loss 0.8416 (1.1640)	grad_norm 0.3299 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:03:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2100/2502]	eta 0:01:11 lr 0.000003	 wd 0.0500	time 0.1604 (0.1774)	loss 1.0643 (1.1629)	grad_norm 0.3331 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:03:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2200/2502]	eta 0:00:53 lr 0.000004	 wd 0.0500	time 0.1752 (0.1773)	loss 1.5332 (1.1624)	grad_norm 0.3313 (0.3302)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:04:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2300/2502]	eta 0:00:35 lr 0.000004	 wd 0.0500	time 0.1964 (0.1770)	loss 1.3256 (1.1625)	grad_norm 0.3427 (0.3302)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:04:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2400/2502]	eta 0:00:18 lr 0.000004	 wd 0.0500	time 0.1612 (0.1768)	loss 1.3037 (1.1624)	grad_norm 0.3275 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:04:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [0/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.1541 (0.1762)	loss 0.8740 (1.1606)	grad_norm 0.3327 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:04:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 0 training takes 0:07:23
[2024-07-31 22:04:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_0.pth saving......
[2024-07-31 22:04:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_0.pth saved !!!
[2024-07-31 22:04:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 10.081 (10.081)	Loss 0.5171 (0.5171)	Acc@1 92.773 (92.773)	Acc@5 98.633 (98.633)	Mem 2327MB
[2024-07-31 22:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.086 Acc@5 97.852
[2024-07-31 22:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.09%
[2024-07-31 22:05:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-07-31 22:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-07-31 22:05:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][0/2502]	eta 7:24:18 lr 0.000004	 wd 0.0500	time 10.6548 (10.6548)	loss 1.1022 (1.1022)	grad_norm 0.0000 (0.0000)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:05:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][100/2502]	eta 0:11:17 lr 0.000004	 wd 0.0500	time 0.1702 (0.2819)	loss 1.1638 (1.1421)	grad_norm 0.3305 (0.3279)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:06:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][200/2502]	eta 0:08:41 lr 0.000004	 wd 0.0500	time 0.1618 (0.2265)	loss 0.7634 (1.1596)	grad_norm 0.3291 (0.3286)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][300/2502]	eta 0:07:37 lr 0.000004	 wd 0.0500	time 0.1542 (0.2077)	loss 0.8749 (1.1590)	grad_norm 0.3342 (0.3290)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:06:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][400/2502]	eta 0:06:56 lr 0.000005	 wd 0.0500	time 0.1599 (0.1981)	loss 0.9695 (1.1742)	grad_norm 0.3363 (0.3294)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:06:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][500/2502]	eta 0:06:25 lr 0.000005	 wd 0.0500	time 0.1743 (0.1926)	loss 1.4433 (1.1714)	grad_norm 0.3230 (0.3297)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:07:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][600/2502]	eta 0:06:00 lr 0.000005	 wd 0.0500	time 0.1584 (0.1894)	loss 1.4774 (1.1702)	grad_norm 0.3172 (0.3300)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:07:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][700/2502]	eta 0:05:36 lr 0.000005	 wd 0.0500	time 0.1662 (0.1865)	loss 0.8663 (1.1672)	grad_norm 0.3166 (0.3299)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:07:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][800/2502]	eta 0:05:14 lr 0.000005	 wd 0.0500	time 0.1533 (0.1847)	loss 1.4068 (1.1663)	grad_norm 0.3109 (0.3300)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:08:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][900/2502]	eta 0:04:53 lr 0.000005	 wd 0.0500	time 0.1630 (0.1832)	loss 1.1681 (1.1652)	grad_norm 0.3503 (0.3299)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:08:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1000/2502]	eta 0:04:33 lr 0.000006	 wd 0.0500	time 0.1582 (0.1820)	loss 1.3569 (1.1639)	grad_norm 0.3351 (0.3301)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:08:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1100/2502]	eta 0:04:13 lr 0.000006	 wd 0.0500	time 0.1909 (0.1809)	loss 1.4273 (1.1625)	grad_norm 0.3286 (0.3305)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:08:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1200/2502]	eta 0:03:54 lr 0.000006	 wd 0.0500	time 0.1921 (0.1801)	loss 0.9575 (1.1603)	grad_norm 0.3235 (0.3304)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:09:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1300/2502]	eta 0:03:35 lr 0.000006	 wd 0.0500	time 0.1640 (0.1794)	loss 1.5331 (1.1615)	grad_norm 0.3400 (0.3305)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:09:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1400/2502]	eta 0:03:17 lr 0.000006	 wd 0.0500	time 0.1662 (0.1788)	loss 0.9355 (1.1615)	grad_norm 0.3381 (0.3304)	loss_scale 65536.0000 (65536.0000)	mem 2327MB
[2024-07-31 22:09:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1500/2502]	eta 0:02:58 lr 0.000006	 wd 0.0500	time 0.1598 (0.1784)	loss 1.3905 (1.1622)	grad_norm 0.3259 (0.3304)	loss_scale 131072.0000 (65710.6462)	mem 2327MB
[2024-07-31 22:10:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1600/2502]	eta 0:02:40 lr 0.000007	 wd 0.0500	time 0.1666 (0.1779)	loss 1.2824 (1.1639)	grad_norm 0.3266 (0.3304)	loss_scale 131072.0000 (69793.1793)	mem 2327MB
[2024-07-31 22:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1700/2502]	eta 0:02:22 lr 0.000007	 wd 0.0500	time 0.1653 (0.1776)	loss 0.9865 (1.1641)	grad_norm 0.3455 (0.3305)	loss_scale 131072.0000 (73395.6966)	mem 2327MB
[2024-07-31 22:10:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1800/2502]	eta 0:02:04 lr 0.000007	 wd 0.0500	time 0.1652 (0.1773)	loss 1.6922 (1.1634)	grad_norm 0.3319 (0.3306)	loss_scale 131072.0000 (76598.1566)	mem 2327MB
[2024-07-31 22:10:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][1900/2502]	eta 0:01:46 lr 0.000007	 wd 0.0500	time 0.1806 (0.1772)	loss 1.4409 (1.1647)	grad_norm 0.3434 (0.3306)	loss_scale 131072.0000 (79463.6928)	mem 2327MB
[2024-07-31 22:11:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2000/2502]	eta 0:01:28 lr 0.000007	 wd 0.0500	time 0.2257 (0.1771)	loss 1.4028 (1.1652)	grad_norm 0.3346 (0.3304)	loss_scale 131072.0000 (82042.8186)	mem 2327MB
[2024-07-31 22:11:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2100/2502]	eta 0:01:11 lr 0.000007	 wd 0.0500	time 0.1723 (0.1769)	loss 1.4792 (1.1672)	grad_norm 0.3484 (0.3306)	loss_scale 131072.0000 (84376.4303)	mem 2327MB
[2024-07-31 22:11:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2200/2502]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.1879 (0.1769)	loss 1.2233 (1.1678)	grad_norm 0.3274 (0.3306)	loss_scale 131072.0000 (86497.9918)	mem 2327MB
[2024-07-31 22:12:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2300/2502]	eta 0:00:35 lr 0.000008	 wd 0.0500	time 0.1588 (0.1768)	loss 0.9622 (1.1700)	grad_norm 0.3271 (0.3306)	loss_scale 131072.0000 (88435.1499)	mem 2327MB
[2024-07-31 22:12:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2400/2502]	eta 0:00:18 lr 0.000008	 wd 0.0500	time 0.1579 (0.1766)	loss 1.2046 (1.1701)	grad_norm 0.3267 (0.3305)	loss_scale 131072.0000 (90210.9454)	mem 2327MB
[2024-07-31 22:12:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [1/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.1533 (0.1760)	loss 0.9044 (1.1702)	grad_norm 0.3316 (0.3305)	loss_scale 131072.0000 (91844.7341)	mem 2327MB
[2024-07-31 22:12:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 1 training takes 0:07:22
[2024-07-31 22:12:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.362 (11.362)	Loss 0.5107 (0.5107)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 2327MB
[2024-07-31 22:13:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.110 Acc@5 97.840
[2024-07-31 22:13:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:13:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.11%
[2024-07-31 22:13:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-07-31 22:13:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-07-31 22:13:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][0/2502]	eta 7:25:34 lr 0.000008	 wd 0.0500	time 10.6852 (10.6852)	loss 1.2345 (1.2345)	grad_norm 0.0000 (0.0000)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:13:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][100/2502]	eta 0:11:05 lr 0.000008	 wd 0.0500	time 0.1652 (0.2771)	loss 1.4155 (1.1725)	grad_norm 0.3313 (0.3313)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:13:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][200/2502]	eta 0:08:36 lr 0.000008	 wd 0.0500	time 0.1680 (0.2245)	loss 1.3887 (1.1640)	grad_norm 0.3244 (0.3301)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:14:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][300/2502]	eta 0:07:34 lr 0.000008	 wd 0.0500	time 0.1601 (0.2062)	loss 1.1795 (1.1548)	grad_norm 0.3349 (0.3299)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:14:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][400/2502]	eta 0:06:57 lr 0.000009	 wd 0.0500	time 0.1581 (0.1984)	loss 1.4211 (1.1581)	grad_norm 0.3218 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:14:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][500/2502]	eta 0:06:25 lr 0.000009	 wd 0.0500	time 0.1630 (0.1925)	loss 1.4287 (1.1538)	grad_norm 0.3411 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:15:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][600/2502]	eta 0:05:58 lr 0.000009	 wd 0.0500	time 0.1581 (0.1883)	loss 1.2727 (1.1550)	grad_norm 0.3205 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:15:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][700/2502]	eta 0:05:34 lr 0.000009	 wd 0.0500	time 0.1796 (0.1858)	loss 1.3273 (1.1599)	grad_norm 0.3339 (0.3297)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:15:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][800/2502]	eta 0:05:11 lr 0.000009	 wd 0.0500	time 0.1633 (0.1830)	loss 1.3787 (1.1628)	grad_norm 0.3209 (0.3297)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:15:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][900/2502]	eta 0:04:50 lr 0.000009	 wd 0.0500	time 0.1606 (0.1816)	loss 0.9380 (1.1679)	grad_norm 0.3433 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:16:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1000/2502]	eta 0:04:31 lr 0.000010	 wd 0.0500	time 0.1738 (0.1807)	loss 0.8641 (1.1650)	grad_norm 0.3419 (0.3297)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:16:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1100/2502]	eta 0:04:11 lr 0.000010	 wd 0.0500	time 0.1620 (0.1797)	loss 1.3762 (1.1626)	grad_norm 0.3408 (0.3298)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:16:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1200/2502]	eta 0:03:53 lr 0.000010	 wd 0.0500	time 0.1543 (0.1791)	loss 0.8988 (1.1629)	grad_norm 0.3467 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:16:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1300/2502]	eta 0:03:34 lr 0.000010	 wd 0.0500	time 0.1578 (0.1787)	loss 1.3998 (1.1640)	grad_norm 0.3408 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:17:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1400/2502]	eta 0:03:16 lr 0.000010	 wd 0.0500	time 0.1758 (0.1780)	loss 1.2184 (1.1656)	grad_norm 0.3298 (0.3299)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1500/2502]	eta 0:02:57 lr 0.000010	 wd 0.0500	time 0.1535 (0.1776)	loss 1.3200 (1.1661)	grad_norm 0.3242 (0.3299)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:17:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1600/2502]	eta 0:02:39 lr 0.000011	 wd 0.0500	time 0.1640 (0.1773)	loss 0.9709 (1.1666)	grad_norm 0.3258 (0.3299)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:18:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1700/2502]	eta 0:02:21 lr 0.000011	 wd 0.0500	time 0.1665 (0.1769)	loss 1.2830 (1.1670)	grad_norm 0.3211 (0.3299)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:18:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1800/2502]	eta 0:02:04 lr 0.000011	 wd 0.0500	time 0.1539 (0.1767)	loss 1.0676 (1.1675)	grad_norm 0.3335 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:18:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][1900/2502]	eta 0:01:46 lr 0.000011	 wd 0.0500	time 0.1609 (0.1770)	loss 1.3496 (1.1673)	grad_norm 0.3524 (0.3301)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:19:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2000/2502]	eta 0:01:29 lr 0.000011	 wd 0.0500	time 0.1669 (0.1773)	loss 1.1338 (1.1674)	grad_norm 0.3283 (0.3299)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:19:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2100/2502]	eta 0:01:11 lr 0.000011	 wd 0.0500	time 0.1940 (0.1770)	loss 1.0030 (1.1658)	grad_norm 0.3191 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:19:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2200/2502]	eta 0:00:53 lr 0.000012	 wd 0.0500	time 0.1592 (0.1768)	loss 1.3860 (1.1656)	grad_norm 0.3467 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:19:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2300/2502]	eta 0:00:35 lr 0.000012	 wd 0.0500	time 0.1793 (0.1766)	loss 1.2061 (1.1658)	grad_norm 0.3263 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:20:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2400/2502]	eta 0:00:18 lr 0.000012	 wd 0.0500	time 0.1620 (0.1765)	loss 0.7779 (1.1665)	grad_norm 0.3363 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:20:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [2/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.1540 (0.1759)	loss 1.1904 (1.1669)	grad_norm 0.3161 (0.3300)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:20:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 2 training takes 0:07:22
[2024-07-31 22:20:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.454 (12.454)	Loss 0.4919 (0.4919)	Acc@1 92.969 (92.969)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 22:20:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.088 Acc@5 97.864
[2024-07-31 22:20:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:20:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.11%
[2024-07-31 22:21:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][0/2502]	eta 7:54:41 lr 0.000012	 wd 0.0500	time 11.3836 (11.3836)	loss 0.7518 (0.7518)	grad_norm 0.0000 (0.0000)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:21:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][100/2502]	eta 0:11:26 lr 0.000012	 wd 0.0500	time 0.1834 (0.2859)	loss 1.2342 (1.2032)	grad_norm 0.3350 (0.3299)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:21:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][200/2502]	eta 0:08:42 lr 0.000012	 wd 0.0500	time 0.1558 (0.2271)	loss 1.5103 (1.1770)	grad_norm 0.3402 (0.3308)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:22:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][300/2502]	eta 0:07:37 lr 0.000012	 wd 0.0500	time 0.1810 (0.2076)	loss 1.4349 (1.1741)	grad_norm 0.3228 (0.3309)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:22:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][400/2502]	eta 0:06:55 lr 0.000013	 wd 0.0500	time 0.1636 (0.1977)	loss 1.4103 (1.1662)	grad_norm 0.3339 (0.3304)	loss_scale 131072.0000 (131072.0000)	mem 2327MB
[2024-07-31 22:22:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][500/2502]	eta 0:06:25 lr 0.000013	 wd 0.0500	time 0.1532 (0.1925)	loss 1.0001 (1.1663)	grad_norm 0.3250 (0.3302)	loss_scale 262144.0000 (133164.9661)	mem 2327MB
[2024-07-31 22:22:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][600/2502]	eta 0:05:59 lr 0.000013	 wd 0.0500	time 0.1629 (0.1891)	loss 1.0598 (1.1612)	grad_norm 0.3437 (0.3302)	loss_scale 262144.0000 (154625.7038)	mem 2327MB
[2024-07-31 22:23:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][700/2502]	eta 0:05:35 lr 0.000013	 wd 0.0500	time 0.1658 (0.1863)	loss 1.5037 (1.1589)	grad_norm 0.3298 (0.3303)	loss_scale 262144.0000 (169963.5492)	mem 2327MB
[2024-07-31 22:23:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][800/2502]	eta 0:05:13 lr 0.000013	 wd 0.0500	time 0.1777 (0.1843)	loss 0.7767 (1.1547)	grad_norm 0.3371 (0.3305)	loss_scale 262144.0000 (181471.7203)	mem 2327MB
[2024-07-31 22:23:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][900/2502]	eta 0:04:52 lr 0.000013	 wd 0.0500	time 0.1629 (0.1827)	loss 1.5570 (1.1569)	grad_norm 0.3394 (0.3302)	loss_scale 262144.0000 (190425.3585)	mem 2327MB
[2024-07-31 22:23:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1000/2502]	eta 0:04:32 lr 0.000014	 wd 0.0500	time 0.1649 (0.1811)	loss 1.2912 (1.1575)	grad_norm 0.3111 (0.3302)	loss_scale 262144.0000 (197590.0579)	mem 2327MB
[2024-07-31 22:24:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1100/2502]	eta 0:04:12 lr 0.000014	 wd 0.0500	time 0.1585 (0.1802)	loss 0.9951 (1.1582)	grad_norm 0.3464 (0.3306)	loss_scale 262144.0000 (203453.2679)	mem 2327MB
[2024-07-31 22:24:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1200/2502]	eta 0:03:53 lr 0.000014	 wd 0.0500	time 0.1685 (0.1794)	loss 1.1497 (1.1579)	grad_norm 0.3306 (0.3306)	loss_scale 262144.0000 (208340.0899)	mem 2327MB
[2024-07-31 22:24:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1300/2502]	eta 0:03:34 lr 0.000014	 wd 0.0500	time 0.2031 (0.1786)	loss 1.3129 (1.1572)	grad_norm 0.3231 (0.3307)	loss_scale 262144.0000 (212475.6710)	mem 2327MB
[2024-07-31 22:25:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1400/2502]	eta 0:03:16 lr 0.000014	 wd 0.0500	time 0.1591 (0.1779)	loss 1.1277 (1.1582)	grad_norm 0.3220 (0.3308)	loss_scale 262144.0000 (216020.8765)	mem 2327MB
[2024-07-31 22:25:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1500/2502]	eta 0:02:57 lr 0.000014	 wd 0.0500	time 0.1577 (0.1774)	loss 1.3910 (1.1567)	grad_norm 0.3367 (0.3306)	loss_scale 262144.0000 (219093.7029)	mem 2327MB
[2024-07-31 22:25:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1600/2502]	eta 0:02:39 lr 0.000015	 wd 0.0500	time 0.1572 (0.1770)	loss 0.7629 (1.1552)	grad_norm 0.3329 (0.3307)	loss_scale 262144.0000 (221782.6658)	mem 2327MB
[2024-07-31 22:25:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1700/2502]	eta 0:02:21 lr 0.000015	 wd 0.0500	time 0.1719 (0.1767)	loss 0.9840 (1.1575)	grad_norm 0.3155 (0.3307)	loss_scale 262144.0000 (224155.4662)	mem 2327MB
[2024-07-31 22:26:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1800/2502]	eta 0:02:03 lr 0.000015	 wd 0.0500	time 0.1641 (0.1764)	loss 1.1441 (1.1587)	grad_norm 0.3411 (0.3307)	loss_scale 262144.0000 (226264.7685)	mem 2327MB
[2024-07-31 22:26:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][1900/2502]	eta 0:01:46 lr 0.000015	 wd 0.0500	time 0.1728 (0.1762)	loss 1.2383 (1.1578)	grad_norm 0.3247 (0.3307)	loss_scale 262144.0000 (228152.1557)	mem 2327MB
[2024-07-31 22:26:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2000/2502]	eta 0:01:28 lr 0.000015	 wd 0.0500	time 0.1797 (0.1761)	loss 1.3134 (1.1583)	grad_norm 0.3264 (0.3307)	loss_scale 262144.0000 (229850.8986)	mem 2327MB
[2024-07-31 22:27:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2100/2502]	eta 0:01:10 lr 0.000015	 wd 0.0500	time 0.1681 (0.1759)	loss 0.9689 (1.1587)	grad_norm 0.3399 (0.3307)	loss_scale 262144.0000 (231387.9334)	mem 2327MB
[2024-07-31 22:27:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2200/2502]	eta 0:00:53 lr 0.000016	 wd 0.0500	time 0.1679 (0.1757)	loss 0.8040 (1.1579)	grad_norm 0.3303 (0.3307)	loss_scale 262144.0000 (232785.3012)	mem 2327MB
[2024-07-31 22:27:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2300/2502]	eta 0:00:35 lr 0.000016	 wd 0.0500	time 0.1698 (0.1755)	loss 0.9007 (1.1581)	grad_norm 0.3378 (0.3307)	loss_scale 262144.0000 (234061.2116)	mem 2327MB
[2024-07-31 22:27:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2400/2502]	eta 0:00:17 lr 0.000016	 wd 0.0500	time 0.1668 (0.1754)	loss 1.3993 (1.1598)	grad_norm 0.3402 (0.3307)	loss_scale 262144.0000 (235230.8405)	mem 2327MB
[2024-07-31 22:28:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [3/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.1536 (0.1748)	loss 1.2974 (1.1595)	grad_norm 0.3407 (0.3308)	loss_scale 262144.0000 (236306.9364)	mem 2327MB
[2024-07-31 22:28:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 3 training takes 0:07:19
[2024-07-31 22:28:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.975 (11.975)	Loss 0.5098 (0.5098)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 22:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.130 Acc@5 97.852
[2024-07-31 22:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.13%
[2024-07-31 22:28:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-07-31 22:28:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-07-31 22:28:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][0/2502]	eta 7:25:30 lr 0.000016	 wd 0.0500	time 10.6835 (10.6835)	loss 1.2661 (1.2661)	grad_norm 0.0000 (0.0000)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:29:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][100/2502]	eta 0:10:53 lr 0.000016	 wd 0.0500	time 0.1526 (0.2722)	loss 0.9403 (1.1745)	grad_norm 0.3344 (0.3314)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:29:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][200/2502]	eta 0:08:28 lr 0.000016	 wd 0.0500	time 0.1540 (0.2207)	loss 0.8638 (1.1622)	grad_norm 0.3341 (0.3307)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:29:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][300/2502]	eta 0:07:27 lr 0.000016	 wd 0.0500	time 0.1641 (0.2031)	loss 0.7928 (1.1593)	grad_norm 0.3267 (0.3302)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:30:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][400/2502]	eta 0:06:48 lr 0.000017	 wd 0.0500	time 0.1522 (0.1945)	loss 1.2968 (1.1541)	grad_norm 0.3322 (0.3309)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:30:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][500/2502]	eta 0:06:18 lr 0.000017	 wd 0.0500	time 0.1576 (0.1888)	loss 1.2524 (1.1548)	grad_norm 0.3257 (0.3312)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:30:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][600/2502]	eta 0:05:52 lr 0.000017	 wd 0.0500	time 0.1790 (0.1855)	loss 1.1718 (1.1595)	grad_norm 0.3400 (0.3314)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:30:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][700/2502]	eta 0:05:29 lr 0.000017	 wd 0.0500	time 0.1874 (0.1831)	loss 0.8339 (1.1598)	grad_norm 0.3358 (0.3311)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:31:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][800/2502]	eta 0:05:08 lr 0.000017	 wd 0.0500	time 0.1672 (0.1815)	loss 0.7720 (1.1623)	grad_norm 0.3444 (0.3311)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:31:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][900/2502]	eta 0:04:48 lr 0.000017	 wd 0.0500	time 0.1834 (0.1802)	loss 0.8272 (1.1634)	grad_norm 0.3273 (0.3311)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:31:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1000/2502]	eta 0:04:29 lr 0.000018	 wd 0.0500	time 0.1602 (0.1791)	loss 1.5573 (1.1634)	grad_norm 0.3206 (0.3311)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:32:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1100/2502]	eta 0:04:10 lr 0.000018	 wd 0.0500	time 0.1798 (0.1785)	loss 1.4482 (1.1643)	grad_norm 0.3289 (0.3310)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:32:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1200/2502]	eta 0:03:51 lr 0.000018	 wd 0.0500	time 0.1899 (0.1779)	loss 1.2757 (1.1631)	grad_norm 0.3398 (0.3309)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:32:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1300/2502]	eta 0:03:32 lr 0.000018	 wd 0.0500	time 0.1592 (0.1770)	loss 1.0225 (1.1638)	grad_norm 0.3463 (0.3308)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:32:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1400/2502]	eta 0:03:14 lr 0.000018	 wd 0.0500	time 0.1822 (0.1766)	loss 1.4010 (1.1638)	grad_norm 0.3354 (0.3307)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:33:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1500/2502]	eta 0:02:56 lr 0.000018	 wd 0.0500	time 0.1619 (0.1762)	loss 1.2132 (1.1642)	grad_norm 0.3281 (0.3308)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:33:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1600/2502]	eta 0:02:38 lr 0.000019	 wd 0.0500	time 0.1899 (0.1759)	loss 1.4511 (1.1628)	grad_norm 0.3388 (0.3309)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:33:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1700/2502]	eta 0:02:20 lr 0.000019	 wd 0.0500	time 0.1748 (0.1757)	loss 1.1705 (1.1634)	grad_norm 0.3384 (0.3309)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:34:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1800/2502]	eta 0:02:03 lr 0.000019	 wd 0.0500	time 0.1576 (0.1755)	loss 1.5071 (1.1654)	grad_norm 0.3306 (0.3308)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:34:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][1900/2502]	eta 0:01:45 lr 0.000019	 wd 0.0500	time 0.1759 (0.1753)	loss 1.3785 (1.1658)	grad_norm 0.3346 (0.3308)	loss_scale 262144.0000 (262144.0000)	mem 2327MB
[2024-07-31 22:34:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2000/2502]	eta 0:01:27 lr 0.000019	 wd 0.0500	time 0.1962 (0.1752)	loss 0.8641 (1.1636)	grad_norm 0.3421 (0.3308)	loss_scale 524288.0000 (263454.0650)	mem 2327MB
[2024-07-31 22:34:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2100/2502]	eta 0:01:10 lr 0.000019	 wd 0.0500	time 0.1872 (0.1747)	loss 1.0018 (1.1623)	grad_norm 0.3256 (0.3308)	loss_scale 524288.0000 (275868.8168)	mem 2327MB
[2024-07-31 22:35:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2200/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.1566 (0.1746)	loss 0.8838 (1.1619)	grad_norm 0.3121 (0.3307)	loss_scale 524288.0000 (287155.4675)	mem 2327MB
[2024-07-31 22:35:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2300/2502]	eta 0:00:35 lr 0.000020	 wd 0.0500	time 0.1742 (0.1747)	loss 0.7908 (1.1613)	grad_norm 0.3337 (0.3307)	loss_scale 524288.0000 (297461.0969)	mem 2327MB
[2024-07-31 22:35:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2400/2502]	eta 0:00:17 lr 0.000020	 wd 0.0500	time 0.1586 (0.1745)	loss 0.7765 (1.1612)	grad_norm 0.3106 (0.3306)	loss_scale 524288.0000 (306908.2815)	mem 2327MB
[2024-07-31 22:36:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [4/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.1769 (0.1742)	loss 1.2345 (1.1618)	grad_norm 0.3483 (0.3307)	loss_scale 524288.0000 (315599.9936)	mem 2327MB
[2024-07-31 22:36:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 4 training takes 0:07:18
[2024-07-31 22:36:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.231 (12.231)	Loss 0.4971 (0.4971)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 22:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.148 Acc@5 97.870
[2024-07-31 22:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-31 22:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-07-31 22:36:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-07-31 22:36:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][0/2502]	eta 7:13:36 lr 0.000020	 wd 0.0500	time 10.3984 (10.3984)	loss 1.3829 (1.3829)	grad_norm 0.0000 (0.0000)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:37:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][100/2502]	eta 0:11:02 lr 0.000020	 wd 0.0500	time 0.1791 (0.2760)	loss 0.9850 (1.1909)	grad_norm 0.3469 (0.3317)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:37:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][200/2502]	eta 0:08:31 lr 0.000020	 wd 0.0500	time 0.1595 (0.2224)	loss 1.3331 (1.1622)	grad_norm 0.3340 (0.3333)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:37:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][300/2502]	eta 0:07:32 lr 0.000020	 wd 0.0500	time 0.1724 (0.2053)	loss 1.0126 (1.1513)	grad_norm 0.3253 (0.3321)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:37:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][400/2502]	eta 0:06:54 lr 0.000020	 wd 0.0500	time 0.1735 (0.1970)	loss 1.2564 (1.1602)	grad_norm 0.3399 (0.3327)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:38:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][500/2502]	eta 0:06:23 lr 0.000020	 wd 0.0500	time 0.1605 (0.1917)	loss 0.9982 (1.1589)	grad_norm 0.3290 (0.3324)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:38:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][600/2502]	eta 0:05:57 lr 0.000020	 wd 0.0500	time 0.1658 (0.1882)	loss 1.0620 (1.1601)	grad_norm 0.3501 (0.3323)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:38:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][700/2502]	eta 0:05:34 lr 0.000020	 wd 0.0500	time 0.1599 (0.1856)	loss 0.9218 (1.1588)	grad_norm 0.3231 (0.3322)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:39:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][800/2502]	eta 0:05:12 lr 0.000020	 wd 0.0500	time 0.1555 (0.1834)	loss 1.5107 (1.1579)	grad_norm 0.3276 (0.3318)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:39:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][900/2502]	eta 0:04:51 lr 0.000020	 wd 0.0500	time 0.1572 (0.1818)	loss 1.0599 (1.1567)	grad_norm 0.3267 (0.3318)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:39:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1000/2502]	eta 0:04:30 lr 0.000020	 wd 0.0500	time 0.1613 (0.1798)	loss 1.2519 (1.1550)	grad_norm 0.3124 (0.3315)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:39:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1100/2502]	eta 0:04:10 lr 0.000020	 wd 0.0500	time 0.1697 (0.1788)	loss 1.1821 (1.1561)	grad_norm 0.3376 (0.3315)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:40:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1200/2502]	eta 0:03:51 lr 0.000020	 wd 0.0500	time 0.1570 (0.1781)	loss 1.4065 (1.1547)	grad_norm 0.3259 (0.3314)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1300/2502]	eta 0:03:33 lr 0.000020	 wd 0.0500	time 0.1777 (0.1776)	loss 0.9975 (1.1554)	grad_norm 0.3332 (0.3313)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:40:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1400/2502]	eta 0:03:15 lr 0.000020	 wd 0.0500	time 0.1605 (0.1771)	loss 1.4260 (1.1590)	grad_norm 0.3430 (0.3314)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:40:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1500/2502]	eta 0:02:56 lr 0.000020	 wd 0.0500	time 0.1832 (0.1766)	loss 0.8347 (1.1601)	grad_norm 0.3439 (0.3313)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:41:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1600/2502]	eta 0:02:38 lr 0.000020	 wd 0.0500	time 0.1559 (0.1762)	loss 1.4028 (1.1628)	grad_norm 0.3413 (0.3313)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:41:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1700/2502]	eta 0:02:21 lr 0.000020	 wd 0.0500	time 0.1965 (0.1760)	loss 0.9564 (1.1630)	grad_norm 0.3203 (0.3313)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:41:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1800/2502]	eta 0:02:03 lr 0.000020	 wd 0.0500	time 0.1623 (0.1759)	loss 0.7680 (1.1609)	grad_norm 0.3324 (0.3313)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:42:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][1900/2502]	eta 0:01:45 lr 0.000020	 wd 0.0500	time 0.1687 (0.1757)	loss 1.1322 (1.1604)	grad_norm 0.3303 (0.3314)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:42:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2000/2502]	eta 0:01:28 lr 0.000020	 wd 0.0500	time 0.1579 (0.1755)	loss 1.5633 (1.1619)	grad_norm 0.3207 (0.3314)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:42:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2100/2502]	eta 0:01:10 lr 0.000020	 wd 0.0500	time 0.1660 (0.1753)	loss 1.4712 (1.1614)	grad_norm 0.3388 (0.3315)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:42:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2200/2502]	eta 0:00:52 lr 0.000020	 wd 0.0500	time 0.1730 (0.1751)	loss 1.2137 (1.1626)	grad_norm 0.3293 (0.3316)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:43:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2300/2502]	eta 0:00:35 lr 0.000020	 wd 0.0500	time 0.1548 (0.1750)	loss 1.1294 (1.1611)	grad_norm 0.3432 (0.3316)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:43:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2400/2502]	eta 0:00:17 lr 0.000020	 wd 0.0500	time 0.1527 (0.1748)	loss 0.7849 (1.1606)	grad_norm 0.3485 (0.3316)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:43:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [5/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.1557 (0.1744)	loss 1.2942 (1.1618)	grad_norm 0.3320 (0.3316)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:43:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 5 training takes 0:07:18
[2024-07-31 22:44:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.698 (11.698)	Loss 0.4941 (0.4941)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 2327MB
[2024-07-31 22:44:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.106 Acc@5 97.886
[2024-07-31 22:44:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:44:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-31 22:44:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][0/2502]	eta 7:20:50 lr 0.000020	 wd 0.0500	time 10.5718 (10.5718)	loss 1.2194 (1.2194)	grad_norm 0.0000 (0.0000)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:44:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][100/2502]	eta 0:11:26 lr 0.000020	 wd 0.0500	time 0.1491 (0.2858)	loss 0.8946 (1.1840)	grad_norm 0.3511 (0.3313)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:45:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][200/2502]	eta 0:08:42 lr 0.000020	 wd 0.0500	time 0.1724 (0.2269)	loss 0.8562 (1.1738)	grad_norm 0.3047 (0.3318)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:45:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][300/2502]	eta 0:07:37 lr 0.000020	 wd 0.0500	time 0.1604 (0.2076)	loss 0.9520 (1.1655)	grad_norm 0.3362 (0.3322)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:45:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][400/2502]	eta 0:06:56 lr 0.000020	 wd 0.0500	time 0.1861 (0.1982)	loss 0.7787 (1.1673)	grad_norm 0.3326 (0.3323)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:45:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][500/2502]	eta 0:06:28 lr 0.000020	 wd 0.0500	time 0.1574 (0.1942)	loss 1.1917 (1.1676)	grad_norm 0.3314 (0.3319)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:46:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][600/2502]	eta 0:06:01 lr 0.000020	 wd 0.0500	time 0.1578 (0.1900)	loss 0.8538 (1.1590)	grad_norm 0.3313 (0.3319)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:46:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][700/2502]	eta 0:05:37 lr 0.000020	 wd 0.0500	time 0.1641 (0.1874)	loss 1.4749 (1.1616)	grad_norm 0.3151 (0.3315)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:46:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][800/2502]	eta 0:05:15 lr 0.000020	 wd 0.0500	time 0.2028 (0.1855)	loss 0.7041 (1.1660)	grad_norm 0.3178 (0.3317)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:47:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][900/2502]	eta 0:04:54 lr 0.000020	 wd 0.0500	time 0.1607 (0.1841)	loss 1.4806 (1.1624)	grad_norm 0.3215 (0.3319)	loss_scale 524288.0000 (524288.0000)	mem 2327MB
[2024-07-31 22:47:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1000/2502]	eta 0:04:34 lr 0.000020	 wd 0.0500	time 0.1647 (0.1827)	loss 1.0347 (1.1622)	grad_norm 0.3222 (0.3321)	loss_scale 1048576.0000 (531620.6993)	mem 2327MB
[2024-07-31 22:47:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1100/2502]	eta 0:04:14 lr 0.000020	 wd 0.0500	time 0.1999 (0.1816)	loss 0.8537 (1.1588)	grad_norm 0.3346 (0.3321)	loss_scale 1048576.0000 (578573.9510)	mem 2327MB
[2024-07-31 22:47:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1200/2502]	eta 0:03:55 lr 0.000020	 wd 0.0500	time 0.1737 (0.1806)	loss 1.2304 (1.1559)	grad_norm 0.3428 (0.3321)	loss_scale 1048576.0000 (617708.1765)	mem 2327MB
[2024-07-31 22:48:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1300/2502]	eta 0:03:36 lr 0.000020	 wd 0.0500	time 0.1590 (0.1797)	loss 1.0971 (1.1548)	grad_norm 0.3241 (0.3321)	loss_scale 1048576.0000 (650826.3797)	mem 2327MB
[2024-07-31 22:48:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1400/2502]	eta 0:03:17 lr 0.000020	 wd 0.0500	time 0.1600 (0.1792)	loss 0.9403 (1.1549)	grad_norm 0.3344 (0.3321)	loss_scale 1048576.0000 (679216.7880)	mem 2327MB
[2024-07-31 22:48:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1500/2502]	eta 0:02:59 lr 0.000020	 wd 0.0500	time 0.1766 (0.1786)	loss 0.7354 (1.1559)	grad_norm 0.3260 (0.3321)	loss_scale 1048576.0000 (703824.3304)	mem 2327MB
[2024-07-31 22:49:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1600/2502]	eta 0:02:40 lr 0.000020	 wd 0.0500	time 0.1593 (0.1783)	loss 1.4506 (1.1534)	grad_norm 0.3172 (0.3321)	loss_scale 1048576.0000 (725357.8513)	mem 2327MB
[2024-07-31 22:49:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1700/2502]	eta 0:02:22 lr 0.000020	 wd 0.0500	time 0.1570 (0.1780)	loss 1.3770 (1.1528)	grad_norm 0.3304 (0.3322)	loss_scale 1048576.0000 (744359.5062)	mem 2327MB
[2024-07-31 22:49:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1800/2502]	eta 0:02:04 lr 0.000020	 wd 0.0500	time 0.1591 (0.1776)	loss 1.2024 (1.1533)	grad_norm 0.3304 (0.3323)	loss_scale 1048576.0000 (761251.0383)	mem 2327MB
[2024-07-31 22:49:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][1900/2502]	eta 0:01:46 lr 0.000020	 wd 0.0500	time 0.1655 (0.1773)	loss 1.3346 (1.1544)	grad_norm 0.3302 (0.3323)	loss_scale 1048576.0000 (776365.4498)	mem 2327MB
[2024-07-31 22:50:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2000/2502]	eta 0:01:28 lr 0.000020	 wd 0.0500	time 0.1710 (0.1772)	loss 0.9820 (1.1553)	grad_norm 0.3408 (0.3325)	loss_scale 1048576.0000 (789969.1754)	mem 2327MB
[2024-07-31 22:50:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2100/2502]	eta 0:01:11 lr 0.000020	 wd 0.0500	time 0.1631 (0.1770)	loss 1.0046 (1.1554)	grad_norm 0.3360 (0.3326)	loss_scale 1048576.0000 (802277.9248)	mem 2327MB
[2024-07-31 22:50:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2200/2502]	eta 0:00:53 lr 0.000020	 wd 0.0500	time 0.1495 (0.1768)	loss 0.8107 (1.1548)	grad_norm 0.3569 (0.3326)	loss_scale 1048576.0000 (813468.2054)	mem 2327MB
[2024-07-31 22:51:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2300/2502]	eta 0:00:35 lr 0.000020	 wd 0.0500	time 0.1736 (0.1765)	loss 1.0353 (1.1572)	grad_norm 0.3514 (0.3326)	loss_scale 1048576.0000 (823685.8409)	mem 2327MB
[2024-07-31 22:51:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2400/2502]	eta 0:00:17 lr 0.000020	 wd 0.0500	time 0.1612 (0.1762)	loss 1.3844 (1.1565)	grad_norm 0.3362 (0.3326)	loss_scale 1048576.0000 (833052.3615)	mem 2327MB
[2024-07-31 22:51:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [6/30][2500/2502]	eta 0:00:00 lr 0.000020	 wd 0.0500	time 0.1541 (0.1757)	loss 1.3465 (1.1561)	grad_norm 0.3245 (0.3327)	loss_scale 1048576.0000 (841669.8601)	mem 2327MB
[2024-07-31 22:51:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 6 training takes 0:07:21
[2024-07-31 22:51:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.787 (11.787)	Loss 0.5059 (0.5059)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 22:52:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.142 Acc@5 97.860
[2024-07-31 22:52:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:52:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-31 22:52:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][0/2502]	eta 8:06:06 lr 0.000020	 wd 0.0500	time 11.6574 (11.6574)	loss 0.9155 (0.9155)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:52:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][100/2502]	eta 0:11:22 lr 0.000020	 wd 0.0500	time 0.1676 (0.2840)	loss 1.1941 (1.1416)	grad_norm 0.3341 (0.3324)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:52:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][200/2502]	eta 0:08:41 lr 0.000020	 wd 0.0500	time 0.1538 (0.2264)	loss 1.1095 (1.1629)	grad_norm 0.3382 (0.3340)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:53:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][300/2502]	eta 0:07:35 lr 0.000020	 wd 0.0500	time 0.1504 (0.2068)	loss 1.2868 (1.1577)	grad_norm 0.3353 (0.3332)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:53:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][400/2502]	eta 0:06:55 lr 0.000020	 wd 0.0500	time 0.1494 (0.1975)	loss 0.8213 (1.1628)	grad_norm 0.3488 (0.3333)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:53:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][500/2502]	eta 0:06:24 lr 0.000020	 wd 0.0500	time 0.1578 (0.1919)	loss 1.3325 (1.1597)	grad_norm 0.3244 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:54:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][600/2502]	eta 0:05:58 lr 0.000020	 wd 0.0500	time 0.1727 (0.1884)	loss 1.2942 (1.1638)	grad_norm 0.3312 (0.3333)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:54:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][700/2502]	eta 0:05:34 lr 0.000020	 wd 0.0500	time 0.1617 (0.1858)	loss 0.9646 (1.1584)	grad_norm 0.3278 (0.3334)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:54:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][800/2502]	eta 0:05:13 lr 0.000020	 wd 0.0500	time 0.1596 (0.1840)	loss 0.8872 (1.1560)	grad_norm 0.3299 (0.3332)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:54:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][900/2502]	eta 0:04:52 lr 0.000020	 wd 0.0500	time 0.1753 (0.1824)	loss 1.4276 (1.1575)	grad_norm 0.3262 (0.3332)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:55:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1000/2502]	eta 0:04:32 lr 0.000020	 wd 0.0500	time 0.2310 (0.1812)	loss 0.8064 (1.1576)	grad_norm 0.3546 (0.3330)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:55:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1100/2502]	eta 0:04:12 lr 0.000020	 wd 0.0500	time 0.1829 (0.1801)	loss 1.3572 (1.1569)	grad_norm 0.3290 (0.3329)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:55:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1200/2502]	eta 0:03:53 lr 0.000020	 wd 0.0500	time 0.1674 (0.1793)	loss 1.3820 (1.1537)	grad_norm 0.3369 (0.3330)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:56:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1300/2502]	eta 0:03:34 lr 0.000020	 wd 0.0500	time 0.1631 (0.1785)	loss 1.3614 (1.1548)	grad_norm 0.3106 (0.3329)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:56:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1400/2502]	eta 0:03:16 lr 0.000019	 wd 0.0500	time 0.1578 (0.1780)	loss 1.2687 (1.1546)	grad_norm 0.3242 (0.3329)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:56:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1500/2502]	eta 0:02:57 lr 0.000019	 wd 0.0500	time 0.1647 (0.1775)	loss 0.8294 (1.1551)	grad_norm 0.3246 (0.3328)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:56:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1600/2502]	eta 0:02:39 lr 0.000019	 wd 0.0500	time 0.1599 (0.1771)	loss 1.1021 (1.1557)	grad_norm 0.3397 (0.3328)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:57:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1700/2502]	eta 0:02:21 lr 0.000019	 wd 0.0500	time 0.1623 (0.1767)	loss 1.3179 (1.1559)	grad_norm 0.3215 (0.3327)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:57:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1800/2502]	eta 0:02:03 lr 0.000019	 wd 0.0500	time 0.1599 (0.1764)	loss 0.7514 (1.1582)	grad_norm 0.3269 (0.3326)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:57:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][1900/2502]	eta 0:01:46 lr 0.000019	 wd 0.0500	time 0.1584 (0.1763)	loss 1.1134 (1.1586)	grad_norm 0.3366 (0.3327)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:58:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2000/2502]	eta 0:01:28 lr 0.000019	 wd 0.0500	time 0.1828 (0.1762)	loss 1.1433 (1.1593)	grad_norm 0.3345 (0.3328)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:58:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2100/2502]	eta 0:01:10 lr 0.000019	 wd 0.0500	time 0.1914 (0.1759)	loss 1.4404 (1.1619)	grad_norm 0.3401 (0.3329)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:58:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2200/2502]	eta 0:00:53 lr 0.000019	 wd 0.0500	time 0.1940 (0.1757)	loss 1.2213 (1.1600)	grad_norm 0.3263 (0.3327)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:58:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2300/2502]	eta 0:00:35 lr 0.000019	 wd 0.0500	time 0.1946 (0.1756)	loss 1.3739 (1.1591)	grad_norm 0.3441 (0.3329)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:59:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2400/2502]	eta 0:00:17 lr 0.000019	 wd 0.0500	time 0.1697 (0.1755)	loss 1.2338 (1.1595)	grad_norm 0.3288 (0.3330)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 22:59:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [7/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.1615 (0.1750)	loss 1.5651 (1.1596)	grad_norm inf (inf)	loss_scale 1048576.0000 (1054445.6777)	mem 2327MB
[2024-07-31 22:59:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 7 training takes 0:07:20
[2024-07-31 22:59:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.301 (12.301)	Loss 0.4824 (0.4824)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 22:59:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.146 Acc@5 97.884
[2024-07-31 22:59:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 22:59:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-31 23:00:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][0/2502]	eta 6:50:54 lr 0.000019	 wd 0.0500	time 9.8539 (9.8539)	loss 1.2333 (1.2333)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:00:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][100/2502]	eta 0:11:17 lr 0.000019	 wd 0.0500	time 0.1618 (0.2822)	loss 0.9722 (1.2012)	grad_norm 0.3177 (0.3338)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:00:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][200/2502]	eta 0:08:39 lr 0.000019	 wd 0.0500	time 0.1833 (0.2256)	loss 0.8093 (1.1864)	grad_norm 0.3273 (0.3332)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:01:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][300/2502]	eta 0:07:34 lr 0.000019	 wd 0.0500	time 0.1972 (0.2062)	loss 1.3892 (1.1782)	grad_norm 0.3387 (0.3331)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:01:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][400/2502]	eta 0:06:54 lr 0.000019	 wd 0.0500	time 0.1696 (0.1974)	loss 0.8347 (1.1770)	grad_norm 0.3348 (0.3333)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:01:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][500/2502]	eta 0:06:24 lr 0.000019	 wd 0.0500	time 0.2041 (0.1920)	loss 0.7954 (1.1661)	grad_norm 0.3274 (0.3339)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:01:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][600/2502]	eta 0:05:57 lr 0.000019	 wd 0.0500	time 0.1630 (0.1881)	loss 0.9116 (1.1630)	grad_norm 0.3554 (0.3338)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:02:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][700/2502]	eta 0:05:34 lr 0.000019	 wd 0.0500	time 0.1760 (0.1855)	loss 1.3385 (1.1636)	grad_norm 0.3462 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:02:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][800/2502]	eta 0:05:11 lr 0.000019	 wd 0.0500	time 0.1543 (0.1831)	loss 1.4536 (1.1653)	grad_norm 0.3428 (0.3334)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:02:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][900/2502]	eta 0:04:50 lr 0.000019	 wd 0.0500	time 0.1677 (0.1815)	loss 0.9464 (1.1605)	grad_norm 0.3284 (0.3331)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:02:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1000/2502]	eta 0:04:31 lr 0.000019	 wd 0.0500	time 0.1781 (0.1804)	loss 1.3407 (1.1639)	grad_norm 0.3333 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:03:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1100/2502]	eta 0:04:11 lr 0.000019	 wd 0.0500	time 0.1649 (0.1795)	loss 1.1924 (1.1621)	grad_norm 0.3226 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:03:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1200/2502]	eta 0:03:53 lr 0.000019	 wd 0.0500	time 0.1639 (0.1790)	loss 1.1165 (1.1617)	grad_norm 0.3471 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:03:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1300/2502]	eta 0:03:34 lr 0.000019	 wd 0.0500	time 0.1603 (0.1784)	loss 0.8992 (1.1612)	grad_norm 0.3381 (0.3334)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:04:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1400/2502]	eta 0:03:15 lr 0.000019	 wd 0.0500	time 0.1854 (0.1777)	loss 1.2589 (1.1620)	grad_norm 0.3397 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:04:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1500/2502]	eta 0:02:57 lr 0.000019	 wd 0.0500	time 0.1614 (0.1774)	loss 1.1611 (1.1622)	grad_norm 0.3422 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:04:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1600/2502]	eta 0:02:39 lr 0.000019	 wd 0.0500	time 0.1628 (0.1770)	loss 1.3666 (1.1649)	grad_norm 0.3374 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:04:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1700/2502]	eta 0:02:21 lr 0.000019	 wd 0.0500	time 0.1817 (0.1769)	loss 0.7620 (1.1673)	grad_norm 0.3274 (0.3337)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1800/2502]	eta 0:02:04 lr 0.000019	 wd 0.0500	time 0.1844 (0.1767)	loss 0.7839 (1.1660)	grad_norm 0.3426 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:05:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][1900/2502]	eta 0:01:46 lr 0.000019	 wd 0.0500	time 0.1579 (0.1766)	loss 1.1733 (1.1678)	grad_norm 0.3249 (0.3337)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:05:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2000/2502]	eta 0:01:28 lr 0.000019	 wd 0.0500	time 0.1751 (0.1765)	loss 0.8064 (1.1666)	grad_norm 0.3404 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:06:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2100/2502]	eta 0:01:10 lr 0.000019	 wd 0.0500	time 0.1598 (0.1764)	loss 0.8697 (1.1670)	grad_norm 0.3431 (0.3337)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:06:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2200/2502]	eta 0:00:53 lr 0.000019	 wd 0.0500	time 0.1867 (0.1762)	loss 1.1103 (1.1653)	grad_norm 0.3569 (0.3337)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:06:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2300/2502]	eta 0:00:35 lr 0.000019	 wd 0.0500	time 0.1613 (0.1760)	loss 1.3359 (1.1640)	grad_norm 0.3206 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:07:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2400/2502]	eta 0:00:17 lr 0.000019	 wd 0.0500	time 0.1616 (0.1760)	loss 1.1586 (1.1639)	grad_norm 0.3437 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:07:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [8/30][2500/2502]	eta 0:00:00 lr 0.000019	 wd 0.0500	time 0.1541 (0.1754)	loss 0.9004 (1.1637)	grad_norm 0.3220 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:07:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 8 training takes 0:07:21
[2024-07-31 23:07:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.788 (11.788)	Loss 0.4839 (0.4839)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 2327MB
[2024-07-31 23:07:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.138 Acc@5 97.900
[2024-07-31 23:07:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 23:07:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.15%
[2024-07-31 23:07:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][0/2502]	eta 7:37:56 lr 0.000019	 wd 0.0500	time 10.9820 (10.9820)	loss 1.3246 (1.3246)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:08:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][100/2502]	eta 0:11:26 lr 0.000019	 wd 0.0500	time 0.1566 (0.2859)	loss 1.2388 (1.1404)	grad_norm 0.3387 (0.3352)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:08:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][200/2502]	eta 0:08:44 lr 0.000019	 wd 0.0500	time 0.1636 (0.2280)	loss 0.8932 (1.1642)	grad_norm 0.3332 (0.3345)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:08:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][300/2502]	eta 0:07:37 lr 0.000019	 wd 0.0500	time 0.1651 (0.2078)	loss 1.4972 (1.1730)	grad_norm 0.3283 (0.3344)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:09:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][400/2502]	eta 0:06:57 lr 0.000019	 wd 0.0500	time 0.1812 (0.1985)	loss 0.8385 (1.1565)	grad_norm 0.3329 (0.3338)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:09:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][500/2502]	eta 0:06:26 lr 0.000019	 wd 0.0500	time 0.1924 (0.1928)	loss 0.7436 (1.1517)	grad_norm 0.3429 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:09:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][600/2502]	eta 0:05:59 lr 0.000019	 wd 0.0500	time 0.1694 (0.1892)	loss 0.8371 (1.1562)	grad_norm 0.3302 (0.3337)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:09:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][700/2502]	eta 0:05:35 lr 0.000019	 wd 0.0500	time 0.1586 (0.1863)	loss 1.3782 (1.1599)	grad_norm 0.3324 (0.3338)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:10:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][800/2502]	eta 0:05:13 lr 0.000019	 wd 0.0500	time 0.1649 (0.1841)	loss 1.3842 (1.1611)	grad_norm 0.3245 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:10:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][900/2502]	eta 0:04:52 lr 0.000019	 wd 0.0500	time 0.1628 (0.1829)	loss 1.3392 (1.1607)	grad_norm 0.3259 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:10:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1000/2502]	eta 0:04:32 lr 0.000019	 wd 0.0500	time 0.1718 (0.1817)	loss 1.1546 (1.1614)	grad_norm 0.3278 (0.3335)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:11:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1100/2502]	eta 0:04:13 lr 0.000018	 wd 0.0500	time 0.1592 (0.1806)	loss 1.3211 (1.1574)	grad_norm 0.3334 (0.3334)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:11:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1200/2502]	eta 0:03:54 lr 0.000018	 wd 0.0500	time 0.1824 (0.1799)	loss 0.8350 (1.1564)	grad_norm 0.3341 (0.3336)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:11:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1300/2502]	eta 0:03:35 lr 0.000018	 wd 0.0500	time 0.2148 (0.1792)	loss 1.2660 (1.1574)	grad_norm 0.3283 (0.3338)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:11:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1400/2502]	eta 0:03:16 lr 0.000018	 wd 0.0500	time 0.1714 (0.1787)	loss 1.0676 (1.1565)	grad_norm 0.3284 (0.3339)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:12:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1500/2502]	eta 0:02:58 lr 0.000018	 wd 0.0500	time 0.1581 (0.1781)	loss 0.8495 (1.1598)	grad_norm 0.3429 (0.3340)	loss_scale 2097152.0000 (1052767.5097)	mem 2327MB
[2024-07-31 23:12:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1600/2502]	eta 0:02:40 lr 0.000018	 wd 0.0500	time 0.1720 (0.1776)	loss 0.9075 (1.1581)	grad_norm 0.3299 (inf)	loss_scale 1048576.0000 (1064294.8157)	mem 2327MB
[2024-07-31 23:12:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1700/2502]	eta 0:02:22 lr 0.000018	 wd 0.0500	time 0.1585 (0.1772)	loss 1.4264 (1.1586)	grad_norm 0.3388 (inf)	loss_scale 1048576.0000 (1063370.7231)	mem 2327MB
[2024-07-31 23:13:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1800/2502]	eta 0:02:04 lr 0.000018	 wd 0.0500	time 0.1629 (0.1768)	loss 1.2447 (1.1573)	grad_norm 0.3384 (inf)	loss_scale 1048576.0000 (1062549.2504)	mem 2327MB
[2024-07-31 23:13:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][1900/2502]	eta 0:01:46 lr 0.000018	 wd 0.0500	time 0.1802 (0.1766)	loss 1.3883 (1.1567)	grad_norm 0.3364 (inf)	loss_scale 1048576.0000 (1061814.2031)	mem 2327MB
[2024-07-31 23:13:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2000/2502]	eta 0:01:28 lr 0.000018	 wd 0.0500	time 0.1652 (0.1764)	loss 1.3850 (1.1573)	grad_norm 0.3133 (inf)	loss_scale 1048576.0000 (1061152.6237)	mem 2327MB
[2024-07-31 23:13:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2100/2502]	eta 0:01:10 lr 0.000018	 wd 0.0500	time 0.1565 (0.1762)	loss 1.4001 (1.1589)	grad_norm 0.3426 (inf)	loss_scale 1048576.0000 (1060554.0219)	mem 2327MB
[2024-07-31 23:14:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2200/2502]	eta 0:00:53 lr 0.000018	 wd 0.0500	time 0.2174 (0.1760)	loss 1.1576 (1.1600)	grad_norm 0.3414 (inf)	loss_scale 1048576.0000 (1060009.8137)	mem 2327MB
[2024-07-31 23:14:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2300/2502]	eta 0:00:35 lr 0.000018	 wd 0.0500	time 0.1685 (0.1758)	loss 0.7865 (1.1600)	grad_norm 0.3376 (inf)	loss_scale 1048576.0000 (1059512.9074)	mem 2327MB
[2024-07-31 23:14:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2400/2502]	eta 0:00:17 lr 0.000018	 wd 0.0500	time 0.1714 (0.1756)	loss 1.0740 (1.1603)	grad_norm 0.3356 (inf)	loss_scale 1048576.0000 (1059057.3928)	mem 2327MB
[2024-07-31 23:15:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [9/30][2500/2502]	eta 0:00:00 lr 0.000018	 wd 0.0500	time 0.1544 (0.1751)	loss 1.2572 (1.1598)	grad_norm 0.3369 (inf)	loss_scale 1048576.0000 (1058638.3047)	mem 2327MB
[2024-07-31 23:15:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 9 training takes 0:07:20
[2024-07-31 23:15:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.761 (11.761)	Loss 0.4971 (0.4971)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 23:15:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.174 Acc@5 97.896
[2024-07-31 23:15:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-31 23:15:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-31 23:15:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-07-31 23:15:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-07-31 23:15:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][0/2502]	eta 7:18:15 lr 0.000018	 wd 0.0500	time 10.5100 (10.5100)	loss 1.6185 (1.6185)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:16:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][100/2502]	eta 0:10:59 lr 0.000018	 wd 0.0500	time 0.1736 (0.2744)	loss 0.8812 (1.1703)	grad_norm 0.3643 (0.3339)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:16:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][200/2502]	eta 0:08:30 lr 0.000018	 wd 0.0500	time 0.1581 (0.2218)	loss 1.4287 (1.1657)	grad_norm 0.3198 (0.3344)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:16:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][300/2502]	eta 0:07:29 lr 0.000018	 wd 0.0500	time 0.1600 (0.2040)	loss 1.3169 (1.1607)	grad_norm 0.3461 (0.3339)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:16:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][400/2502]	eta 0:06:51 lr 0.000018	 wd 0.0500	time 0.1495 (0.1957)	loss 0.8800 (1.1643)	grad_norm 0.3399 (0.3338)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:17:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][500/2502]	eta 0:06:20 lr 0.000018	 wd 0.0500	time 0.1682 (0.1903)	loss 0.6986 (1.1598)	grad_norm 0.3293 (0.3344)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:17:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][600/2502]	eta 0:05:55 lr 0.000018	 wd 0.0500	time 0.1642 (0.1867)	loss 1.1635 (1.1675)	grad_norm 0.3243 (0.3340)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:17:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][700/2502]	eta 0:05:31 lr 0.000018	 wd 0.0500	time 0.1584 (0.1841)	loss 1.2906 (1.1658)	grad_norm 0.3105 (0.3341)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:18:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][800/2502]	eta 0:05:09 lr 0.000018	 wd 0.0500	time 0.1630 (0.1821)	loss 1.2669 (1.1685)	grad_norm 0.3310 (0.3344)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:18:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][900/2502]	eta 0:04:49 lr 0.000018	 wd 0.0500	time 0.1618 (0.1805)	loss 1.4486 (1.1661)	grad_norm 0.3295 (0.3343)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:18:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1000/2502]	eta 0:04:29 lr 0.000018	 wd 0.0500	time 0.1801 (0.1796)	loss 1.2810 (1.1672)	grad_norm 0.3361 (0.3345)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:18:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1100/2502]	eta 0:04:10 lr 0.000018	 wd 0.0500	time 0.1582 (0.1786)	loss 0.9908 (1.1729)	grad_norm 0.3465 (0.3346)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:19:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1200/2502]	eta 0:03:51 lr 0.000018	 wd 0.0500	time 0.1669 (0.1778)	loss 1.3923 (1.1705)	grad_norm 0.3340 (0.3348)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1300/2502]	eta 0:03:33 lr 0.000018	 wd 0.0500	time 0.1631 (0.1772)	loss 1.3576 (1.1680)	grad_norm 0.3308 (0.3349)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:19:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1400/2502]	eta 0:03:14 lr 0.000018	 wd 0.0500	time 0.1626 (0.1768)	loss 1.3441 (1.1660)	grad_norm 0.3346 (0.3350)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:20:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1500/2502]	eta 0:02:56 lr 0.000018	 wd 0.0500	time 0.1601 (0.1765)	loss 1.1565 (1.1634)	grad_norm 0.3470 (0.3349)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:20:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1600/2502]	eta 0:02:38 lr 0.000018	 wd 0.0500	time 0.1645 (0.1762)	loss 1.1107 (1.1622)	grad_norm 0.3172 (0.3350)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:20:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1700/2502]	eta 0:02:21 lr 0.000018	 wd 0.0500	time 0.1955 (0.1760)	loss 1.2363 (1.1642)	grad_norm 0.3327 (0.3352)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:20:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1800/2502]	eta 0:02:03 lr 0.000018	 wd 0.0500	time 0.1600 (0.1756)	loss 1.1342 (1.1633)	grad_norm 0.3559 (0.3352)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:21:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][1900/2502]	eta 0:01:45 lr 0.000018	 wd 0.0500	time 0.1597 (0.1755)	loss 1.0825 (1.1623)	grad_norm 0.3313 (0.3350)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2000/2502]	eta 0:01:27 lr 0.000017	 wd 0.0500	time 0.1622 (0.1753)	loss 1.3173 (1.1658)	grad_norm 0.3389 (0.3351)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:21:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2100/2502]	eta 0:01:10 lr 0.000017	 wd 0.0500	time 0.1679 (0.1750)	loss 1.1905 (1.1671)	grad_norm 0.3144 (0.3351)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:22:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2200/2502]	eta 0:00:52 lr 0.000017	 wd 0.0500	time 0.1619 (0.1748)	loss 1.3210 (1.1682)	grad_norm 0.3220 (0.3351)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:22:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2300/2502]	eta 0:00:35 lr 0.000017	 wd 0.0500	time 0.1611 (0.1747)	loss 1.2887 (1.1661)	grad_norm 0.3395 (0.3352)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:22:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2400/2502]	eta 0:00:17 lr 0.000017	 wd 0.0500	time 0.1569 (0.1746)	loss 1.3593 (1.1651)	grad_norm 0.3208 (0.3353)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:22:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [10/30][2500/2502]	eta 0:00:00 lr 0.000017	 wd 0.0500	time 0.1855 (0.1741)	loss 0.8906 (1.1646)	grad_norm 0.3339 (0.3354)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:22:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 10 training takes 0:07:17
[2024-07-31 23:22:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_10.pth saving......
[2024-07-31 23:22:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_10.pth saved !!!
[2024-07-31 23:23:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.957 (11.957)	Loss 0.4883 (0.4883)	Acc@1 92.188 (92.188)	Acc@5 98.633 (98.633)	Mem 2327MB
[2024-07-31 23:23:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.160 Acc@5 97.886
[2024-07-31 23:23:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-31 23:23:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-31 23:23:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][0/2502]	eta 7:52:40 lr 0.000017	 wd 0.0500	time 11.3351 (11.3351)	loss 0.9047 (0.9047)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:23:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][100/2502]	eta 0:11:22 lr 0.000017	 wd 0.0500	time 0.1646 (0.2839)	loss 1.3932 (1.1500)	grad_norm 0.3270 (0.3356)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:24:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][200/2502]	eta 0:08:41 lr 0.000017	 wd 0.0500	time 0.1688 (0.2263)	loss 1.3202 (1.1707)	grad_norm 0.3398 (0.3356)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:24:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][300/2502]	eta 0:07:35 lr 0.000017	 wd 0.0500	time 0.1665 (0.2070)	loss 0.8446 (1.1609)	grad_norm 0.3285 (0.3354)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:24:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][400/2502]	eta 0:06:55 lr 0.000017	 wd 0.0500	time 0.1713 (0.1978)	loss 1.4007 (1.1591)	grad_norm 0.3478 (0.3355)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:24:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][500/2502]	eta 0:06:24 lr 0.000017	 wd 0.0500	time 0.1619 (0.1923)	loss 0.7273 (1.1586)	grad_norm 0.3205 (0.3354)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:25:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][600/2502]	eta 0:05:58 lr 0.000017	 wd 0.0500	time 0.1820 (0.1887)	loss 1.4539 (1.1592)	grad_norm 0.3311 (inf)	loss_scale 1048576.0000 (1052065.4376)	mem 2327MB
[2024-07-31 23:25:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][700/2502]	eta 0:05:34 lr 0.000017	 wd 0.0500	time 0.1674 (0.1856)	loss 1.0076 (1.1546)	grad_norm 0.3550 (inf)	loss_scale 1048576.0000 (1051567.6576)	mem 2327MB
[2024-07-31 23:25:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][800/2502]	eta 0:05:13 lr 0.000017	 wd 0.0500	time 0.1606 (0.1841)	loss 1.4698 (1.1527)	grad_norm 0.3260 (inf)	loss_scale 1048576.0000 (1051194.1673)	mem 2327MB
[2024-07-31 23:26:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][900/2502]	eta 0:04:52 lr 0.000017	 wd 0.0500	time 0.1775 (0.1825)	loss 0.9941 (1.1554)	grad_norm 0.3481 (inf)	loss_scale 1048576.0000 (1050903.5827)	mem 2327MB
[2024-07-31 23:26:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1000/2502]	eta 0:04:31 lr 0.000017	 wd 0.0500	time 0.1684 (0.1811)	loss 1.1628 (1.1555)	grad_norm 0.3361 (inf)	loss_scale 1048576.0000 (1050671.0569)	mem 2327MB
[2024-07-31 23:26:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1100/2502]	eta 0:04:12 lr 0.000017	 wd 0.0500	time 0.1817 (0.1801)	loss 0.9036 (1.1541)	grad_norm 0.3409 (inf)	loss_scale 1048576.0000 (1050480.7702)	mem 2327MB
[2024-07-31 23:26:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1200/2502]	eta 0:03:53 lr 0.000017	 wd 0.0500	time 0.1684 (0.1794)	loss 0.9217 (1.1510)	grad_norm 0.3295 (inf)	loss_scale 1048576.0000 (1050322.1715)	mem 2327MB
[2024-07-31 23:27:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1300/2502]	eta 0:03:34 lr 0.000017	 wd 0.0500	time 0.1569 (0.1785)	loss 1.3853 (1.1513)	grad_norm 0.3478 (inf)	loss_scale 1048576.0000 (1050187.9539)	mem 2327MB
[2024-07-31 23:27:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1400/2502]	eta 0:03:16 lr 0.000017	 wd 0.0500	time 0.1737 (0.1781)	loss 0.8055 (1.1517)	grad_norm 0.3390 (inf)	loss_scale 1048576.0000 (1050072.8965)	mem 2327MB
[2024-07-31 23:27:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1500/2502]	eta 0:02:57 lr 0.000017	 wd 0.0500	time 0.1883 (0.1776)	loss 1.3899 (1.1525)	grad_norm 0.3389 (inf)	loss_scale 1048576.0000 (1049973.1699)	mem 2327MB
[2024-07-31 23:28:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1600/2502]	eta 0:02:39 lr 0.000017	 wd 0.0500	time 0.1792 (0.1772)	loss 1.2703 (1.1537)	grad_norm 0.3353 (inf)	loss_scale 1048576.0000 (1049885.9013)	mem 2327MB
[2024-07-31 23:28:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1700/2502]	eta 0:02:22 lr 0.000017	 wd 0.0500	time 0.1555 (0.1773)	loss 1.0799 (1.1529)	grad_norm 0.3297 (inf)	loss_scale 1048576.0000 (1049808.8936)	mem 2327MB
[2024-07-31 23:28:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1800/2502]	eta 0:02:04 lr 0.000017	 wd 0.0500	time 0.1600 (0.1770)	loss 0.8286 (1.1529)	grad_norm 0.3248 (inf)	loss_scale 1048576.0000 (1049740.4375)	mem 2327MB
[2024-07-31 23:28:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][1900/2502]	eta 0:01:46 lr 0.000017	 wd 0.0500	time 0.1549 (0.1768)	loss 1.1365 (1.1520)	grad_norm 0.3428 (inf)	loss_scale 1048576.0000 (1049679.1836)	mem 2327MB
[2024-07-31 23:29:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2000/2502]	eta 0:01:28 lr 0.000017	 wd 0.0500	time 0.1588 (0.1766)	loss 1.0569 (1.1522)	grad_norm 0.3157 (inf)	loss_scale 1048576.0000 (1049624.0520)	mem 2327MB
[2024-07-31 23:29:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2100/2502]	eta 0:01:10 lr 0.000017	 wd 0.0500	time 0.1803 (0.1763)	loss 1.3541 (1.1514)	grad_norm 0.3516 (inf)	loss_scale 1048576.0000 (1049574.1685)	mem 2327MB
[2024-07-31 23:29:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2200/2502]	eta 0:00:53 lr 0.000017	 wd 0.0500	time 0.1651 (0.1761)	loss 0.9840 (1.1534)	grad_norm 0.3476 (inf)	loss_scale 1048576.0000 (1049528.8178)	mem 2327MB
[2024-07-31 23:30:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2300/2502]	eta 0:00:35 lr 0.000016	 wd 0.0500	time 0.1543 (0.1759)	loss 1.1520 (1.1515)	grad_norm 0.3424 (inf)	loss_scale 1048576.0000 (1049487.4090)	mem 2327MB
[2024-07-31 23:30:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2400/2502]	eta 0:00:17 lr 0.000016	 wd 0.0500	time 0.1637 (0.1757)	loss 1.3845 (1.1528)	grad_norm 0.3300 (inf)	loss_scale 1048576.0000 (1049449.4494)	mem 2327MB
[2024-07-31 23:30:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [11/30][2500/2502]	eta 0:00:00 lr 0.000016	 wd 0.0500	time 0.1570 (0.1752)	loss 0.8416 (1.1511)	grad_norm 0.3127 (inf)	loss_scale 1048576.0000 (1049414.5254)	mem 2327MB
[2024-07-31 23:30:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 11 training takes 0:07:20
[2024-07-31 23:30:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.406 (12.406)	Loss 0.4873 (0.4873)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-07-31 23:31:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.152 Acc@5 97.864
[2024-07-31 23:31:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-31 23:31:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.17%
[2024-07-31 23:31:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][0/2502]	eta 7:19:10 lr 0.000016	 wd 0.0500	time 10.5317 (10.5317)	loss 0.9492 (0.9492)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:31:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][100/2502]	eta 0:11:26 lr 0.000016	 wd 0.0500	time 0.1624 (0.2859)	loss 1.3066 (1.1548)	grad_norm 0.3287 (0.3356)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:31:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][200/2502]	eta 0:08:45 lr 0.000016	 wd 0.0500	time 0.1915 (0.2281)	loss 1.2892 (1.1522)	grad_norm 0.3285 (0.3358)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:32:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][300/2502]	eta 0:07:42 lr 0.000016	 wd 0.0500	time 0.1653 (0.2099)	loss 0.7442 (1.1646)	grad_norm 0.3526 (0.3352)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:32:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][400/2502]	eta 0:07:00 lr 0.000016	 wd 0.0500	time 0.1667 (0.2000)	loss 1.2223 (1.1565)	grad_norm 0.3262 (0.3354)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:32:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][500/2502]	eta 0:06:26 lr 0.000016	 wd 0.0500	time 0.1507 (0.1930)	loss 1.4246 (1.1566)	grad_norm 0.3317 (0.3350)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:33:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][600/2502]	eta 0:06:00 lr 0.000016	 wd 0.0500	time 0.1593 (0.1894)	loss 1.4059 (1.1549)	grad_norm 0.3451 (0.3353)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:33:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][700/2502]	eta 0:05:37 lr 0.000016	 wd 0.0500	time 0.1746 (0.1874)	loss 1.2057 (1.1529)	grad_norm 0.3343 (0.3353)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:33:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][800/2502]	eta 0:05:15 lr 0.000016	 wd 0.0500	time 0.1784 (0.1855)	loss 1.3617 (1.1539)	grad_norm 0.3268 (0.3352)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:33:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][900/2502]	eta 0:04:53 lr 0.000016	 wd 0.0500	time 0.1882 (0.1833)	loss 1.5264 (1.1513)	grad_norm 0.3330 (0.3353)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:34:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1000/2502]	eta 0:04:33 lr 0.000016	 wd 0.0500	time 0.1592 (0.1821)	loss 1.3798 (1.1533)	grad_norm 0.3464 (0.3353)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:34:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1100/2502]	eta 0:04:13 lr 0.000016	 wd 0.0500	time 0.1653 (0.1810)	loss 0.7982 (1.1549)	grad_norm 0.3286 (0.3355)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:34:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1200/2502]	eta 0:03:54 lr 0.000016	 wd 0.0500	time 0.1903 (0.1803)	loss 1.4292 (1.1531)	grad_norm 0.3379 (0.3356)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:35:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1300/2502]	eta 0:03:36 lr 0.000016	 wd 0.0500	time 0.1637 (0.1798)	loss 1.0414 (1.1539)	grad_norm 0.3230 (0.3357)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:35:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1400/2502]	eta 0:03:17 lr 0.000016	 wd 0.0500	time 0.1548 (0.1793)	loss 1.3127 (1.1536)	grad_norm 0.3246 (0.3357)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:35:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1500/2502]	eta 0:02:59 lr 0.000016	 wd 0.0500	time 0.1590 (0.1790)	loss 0.8085 (1.1509)	grad_norm 0.3512 (0.3358)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:35:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1600/2502]	eta 0:02:41 lr 0.000016	 wd 0.0500	time 0.1571 (0.1785)	loss 1.3578 (1.1477)	grad_norm 0.3191 (0.3357)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:36:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1700/2502]	eta 0:02:22 lr 0.000016	 wd 0.0500	time 0.1601 (0.1782)	loss 1.0600 (1.1499)	grad_norm 0.3424 (0.3359)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1800/2502]	eta 0:02:04 lr 0.000016	 wd 0.0500	time 0.1542 (0.1778)	loss 0.8167 (1.1484)	grad_norm 0.3351 (0.3359)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:36:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][1900/2502]	eta 0:01:46 lr 0.000016	 wd 0.0500	time 0.1586 (0.1776)	loss 1.3003 (1.1492)	grad_norm 0.3389 (0.3360)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:37:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2000/2502]	eta 0:01:29 lr 0.000016	 wd 0.0500	time 0.1793 (0.1773)	loss 1.0491 (1.1500)	grad_norm 0.3326 (0.3361)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:37:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2100/2502]	eta 0:01:11 lr 0.000016	 wd 0.0500	time 0.1594 (0.1772)	loss 0.8050 (1.1493)	grad_norm 0.3033 (inf)	loss_scale 1048576.0000 (1067541.2013)	mem 2327MB
[2024-07-31 23:37:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2200/2502]	eta 0:00:53 lr 0.000016	 wd 0.0500	time 0.2098 (0.1770)	loss 0.8114 (1.1507)	grad_norm 0.3412 (inf)	loss_scale 1048576.0000 (1066679.5384)	mem 2327MB
[2024-07-31 23:37:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2300/2502]	eta 0:00:35 lr 0.000015	 wd 0.0500	time 0.1648 (0.1768)	loss 1.1854 (1.1512)	grad_norm 0.3314 (inf)	loss_scale 1048576.0000 (1065892.7701)	mem 2327MB
[2024-07-31 23:38:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2400/2502]	eta 0:00:18 lr 0.000015	 wd 0.0500	time 0.2008 (0.1767)	loss 1.3589 (1.1515)	grad_norm 0.3207 (inf)	loss_scale 1048576.0000 (1065171.5385)	mem 2327MB
[2024-07-31 23:38:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [12/30][2500/2502]	eta 0:00:00 lr 0.000015	 wd 0.0500	time 0.1545 (0.1762)	loss 0.9326 (1.1508)	grad_norm 0.3370 (inf)	loss_scale 1048576.0000 (1064507.9824)	mem 2327MB
[2024-07-31 23:38:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 12 training takes 0:07:23
[2024-07-31 23:38:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.129 (11.129)	Loss 0.4983 (0.4983)	Acc@1 92.188 (92.188)	Acc@5 98.242 (98.242)	Mem 2327MB
[2024-07-31 23:39:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.198 Acc@5 97.874
[2024-07-31 23:39:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-31 23:39:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-07-31 23:39:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 160): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saving......
[2024-07-31 23:39:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 162): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_best.pth saved !!!
[2024-07-31 23:39:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][0/2502]	eta 7:22:56 lr 0.000015	 wd 0.0500	time 10.6221 (10.6221)	loss 1.3175 (1.3175)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:39:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][100/2502]	eta 0:11:09 lr 0.000015	 wd 0.0500	time 0.1604 (0.2788)	loss 1.3297 (1.2152)	grad_norm 0.3409 (0.3369)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:39:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][200/2502]	eta 0:08:37 lr 0.000015	 wd 0.0500	time 0.1691 (0.2249)	loss 1.3768 (1.2030)	grad_norm 0.3296 (0.3371)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:40:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][300/2502]	eta 0:07:37 lr 0.000015	 wd 0.0500	time 0.1641 (0.2080)	loss 1.3863 (1.2031)	grad_norm 0.3324 (0.3368)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:40:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][400/2502]	eta 0:06:56 lr 0.000015	 wd 0.0500	time 0.1733 (0.1982)	loss 1.4533 (1.1837)	grad_norm 0.3413 (0.3366)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:40:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][500/2502]	eta 0:06:25 lr 0.000015	 wd 0.0500	time 0.1783 (0.1926)	loss 1.2770 (1.1771)	grad_norm 0.3559 (0.3364)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:40:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][600/2502]	eta 0:05:58 lr 0.000015	 wd 0.0500	time 0.1621 (0.1887)	loss 1.1845 (1.1710)	grad_norm 0.3324 (0.3365)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:41:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][700/2502]	eta 0:05:35 lr 0.000015	 wd 0.0500	time 0.1745 (0.1864)	loss 1.1809 (1.1621)	grad_norm 0.3449 (0.3367)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:41:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][800/2502]	eta 0:05:13 lr 0.000015	 wd 0.0500	time 0.1748 (0.1841)	loss 1.2343 (1.1612)	grad_norm 0.3241 (0.3365)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:41:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][900/2502]	eta 0:04:52 lr 0.000015	 wd 0.0500	time 0.1631 (0.1824)	loss 1.3270 (1.1619)	grad_norm 0.3435 (0.3365)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:42:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1000/2502]	eta 0:04:32 lr 0.000015	 wd 0.0500	time 0.1602 (0.1813)	loss 1.3747 (1.1608)	grad_norm 0.3257 (0.3364)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:42:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1100/2502]	eta 0:04:12 lr 0.000015	 wd 0.0500	time 0.1612 (0.1803)	loss 1.3934 (1.1568)	grad_norm 0.3390 (0.3364)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:42:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1200/2502]	eta 0:03:53 lr 0.000015	 wd 0.0500	time 0.1733 (0.1794)	loss 1.1757 (1.1607)	grad_norm 0.3410 (0.3363)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1300/2502]	eta 0:03:34 lr 0.000015	 wd 0.0500	time 0.1814 (0.1786)	loss 0.7238 (1.1592)	grad_norm 0.3371 (0.3366)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:43:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1400/2502]	eta 0:03:15 lr 0.000015	 wd 0.0500	time 0.1645 (0.1778)	loss 1.3457 (1.1578)	grad_norm 0.3363 (0.3366)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1500/2502]	eta 0:02:57 lr 0.000015	 wd 0.0500	time 0.1700 (0.1774)	loss 1.2067 (1.1584)	grad_norm 0.3437 (0.3367)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:43:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1600/2502]	eta 0:02:39 lr 0.000015	 wd 0.0500	time 0.1558 (0.1769)	loss 0.9245 (1.1573)	grad_norm 0.3377 (0.3368)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:44:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1700/2502]	eta 0:02:21 lr 0.000015	 wd 0.0500	time 0.1597 (0.1766)	loss 1.0282 (1.1585)	grad_norm 0.3345 (0.3368)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:44:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1800/2502]	eta 0:02:03 lr 0.000015	 wd 0.0500	time 0.1628 (0.1763)	loss 1.2216 (1.1589)	grad_norm 0.3308 (0.3368)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:44:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][1900/2502]	eta 0:01:46 lr 0.000015	 wd 0.0500	time 0.1707 (0.1761)	loss 1.0871 (1.1591)	grad_norm 0.3377 (0.3368)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:44:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2000/2502]	eta 0:01:28 lr 0.000015	 wd 0.0500	time 0.1559 (0.1759)	loss 1.3823 (1.1590)	grad_norm 0.3198 (0.3369)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:45:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2100/2502]	eta 0:01:10 lr 0.000014	 wd 0.0500	time 0.1748 (0.1755)	loss 1.3449 (1.1590)	grad_norm 0.3385 (0.3370)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:45:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2200/2502]	eta 0:00:52 lr 0.000014	 wd 0.0500	time 0.1753 (0.1754)	loss 1.1920 (1.1576)	grad_norm 0.3353 (0.3370)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:45:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2300/2502]	eta 0:00:35 lr 0.000014	 wd 0.0500	time 0.1615 (0.1753)	loss 1.3343 (1.1565)	grad_norm 0.3467 (0.3370)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:46:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2400/2502]	eta 0:00:17 lr 0.000014	 wd 0.0500	time 0.1849 (0.1751)	loss 0.9307 (1.1592)	grad_norm 0.3173 (0.3370)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:46:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [13/30][2500/2502]	eta 0:00:00 lr 0.000014	 wd 0.0500	time 0.1547 (0.1745)	loss 0.7861 (1.1580)	grad_norm 0.3379 (0.3371)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:46:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 13 training takes 0:07:19
[2024-07-31 23:46:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.720 (12.720)	Loss 0.4724 (0.4724)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 2327MB
[2024-07-31 23:46:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.092 Acc@5 97.902
[2024-07-31 23:46:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-07-31 23:46:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-07-31 23:47:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][0/2502]	eta 7:48:49 lr 0.000014	 wd 0.0500	time 11.2430 (11.2430)	loss 1.4144 (1.4144)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:47:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][100/2502]	eta 0:11:23 lr 0.000014	 wd 0.0500	time 0.1628 (0.2846)	loss 1.1756 (1.1740)	grad_norm 0.3362 (0.3388)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:47:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][200/2502]	eta 0:08:42 lr 0.000014	 wd 0.0500	time 0.1602 (0.2269)	loss 1.4477 (1.1727)	grad_norm 0.3290 (0.3370)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:47:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][300/2502]	eta 0:07:37 lr 0.000014	 wd 0.0500	time 0.1634 (0.2076)	loss 0.9196 (1.1758)	grad_norm 0.3453 (0.3368)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:48:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][400/2502]	eta 0:06:57 lr 0.000014	 wd 0.0500	time 0.1615 (0.1989)	loss 1.5811 (1.1755)	grad_norm 0.3289 (0.3369)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:48:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][500/2502]	eta 0:06:25 lr 0.000014	 wd 0.0500	time 0.1603 (0.1924)	loss 0.9646 (1.1712)	grad_norm 0.3284 (0.3370)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:48:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][600/2502]	eta 0:06:00 lr 0.000014	 wd 0.0500	time 0.1977 (0.1895)	loss 0.9649 (1.1702)	grad_norm 0.3506 (0.3374)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:49:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][700/2502]	eta 0:05:36 lr 0.000014	 wd 0.0500	time 0.1628 (0.1869)	loss 1.1288 (1.1709)	grad_norm 0.3450 (0.3378)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:49:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][800/2502]	eta 0:05:14 lr 0.000014	 wd 0.0500	time 0.1637 (0.1848)	loss 0.7944 (1.1650)	grad_norm 0.3216 (0.3375)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:49:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][900/2502]	eta 0:04:53 lr 0.000014	 wd 0.0500	time 0.1641 (0.1829)	loss 0.8542 (1.1639)	grad_norm 0.3418 (0.3378)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:49:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1000/2502]	eta 0:04:32 lr 0.000014	 wd 0.0500	time 0.1618 (0.1814)	loss 0.9932 (1.1613)	grad_norm 0.3416 (0.3377)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:50:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1100/2502]	eta 0:04:12 lr 0.000014	 wd 0.0500	time 0.1752 (0.1802)	loss 1.3749 (1.1607)	grad_norm 0.3495 (inf)	loss_scale 1048576.0000 (1060004.6213)	mem 2327MB
[2024-07-31 23:50:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1200/2502]	eta 0:03:53 lr 0.000014	 wd 0.0500	time 0.1548 (0.1792)	loss 1.4644 (1.1603)	grad_norm 0.3486 (inf)	loss_scale 1048576.0000 (1059053.0291)	mem 2327MB
[2024-07-31 23:50:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1300/2502]	eta 0:03:34 lr 0.000014	 wd 0.0500	time 0.1938 (0.1786)	loss 1.2365 (1.1635)	grad_norm 0.3377 (inf)	loss_scale 1048576.0000 (1058247.7233)	mem 2327MB
[2024-07-31 23:51:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1400/2502]	eta 0:03:16 lr 0.000014	 wd 0.0500	time 0.1683 (0.1780)	loss 1.2360 (1.1631)	grad_norm 0.3381 (inf)	loss_scale 1048576.0000 (1057557.3790)	mem 2327MB
[2024-07-31 23:51:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1500/2502]	eta 0:02:57 lr 0.000014	 wd 0.0500	time 0.1678 (0.1776)	loss 1.3807 (1.1621)	grad_norm 0.3330 (inf)	loss_scale 1048576.0000 (1056959.0193)	mem 2327MB
[2024-07-31 23:51:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1600/2502]	eta 0:02:39 lr 0.000014	 wd 0.0500	time 0.1507 (0.1772)	loss 0.7619 (1.1611)	grad_norm 0.3378 (inf)	loss_scale 1048576.0000 (1056435.4079)	mem 2327MB
[2024-07-31 23:51:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1700/2502]	eta 0:02:21 lr 0.000014	 wd 0.0500	time 0.1964 (0.1768)	loss 0.9265 (1.1600)	grad_norm 0.3271 (inf)	loss_scale 1048576.0000 (1055973.3616)	mem 2327MB
[2024-07-31 23:52:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1800/2502]	eta 0:02:03 lr 0.000013	 wd 0.0500	time 0.1603 (0.1765)	loss 0.8997 (1.1606)	grad_norm 0.3424 (inf)	loss_scale 1048576.0000 (1055562.6252)	mem 2327MB
[2024-07-31 23:52:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][1900/2502]	eta 0:01:46 lr 0.000013	 wd 0.0500	time 0.2052 (0.1763)	loss 1.1484 (1.1610)	grad_norm 0.3570 (inf)	loss_scale 1048576.0000 (1055195.1015)	mem 2327MB
[2024-07-31 23:52:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2000/2502]	eta 0:01:28 lr 0.000013	 wd 0.0500	time 0.1759 (0.1762)	loss 1.2115 (1.1604)	grad_norm 0.3398 (inf)	loss_scale 1048576.0000 (1054864.3118)	mem 2327MB
[2024-07-31 23:53:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2100/2502]	eta 0:01:10 lr 0.000013	 wd 0.0500	time 0.2015 (0.1759)	loss 1.3397 (1.1597)	grad_norm 0.3396 (inf)	loss_scale 1048576.0000 (1054565.0109)	mem 2327MB
[2024-07-31 23:53:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2200/2502]	eta 0:00:53 lr 0.000013	 wd 0.0500	time 0.1694 (0.1757)	loss 1.3093 (1.1589)	grad_norm 0.3373 (inf)	loss_scale 1048576.0000 (1054292.9069)	mem 2327MB
[2024-07-31 23:53:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2300/2502]	eta 0:00:35 lr 0.000013	 wd 0.0500	time 0.1636 (0.1755)	loss 0.8531 (1.1578)	grad_norm 0.3380 (inf)	loss_scale 1048576.0000 (1054044.4537)	mem 2327MB
[2024-07-31 23:53:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2400/2502]	eta 0:00:17 lr 0.000013	 wd 0.0500	time 0.1601 (0.1754)	loss 0.8536 (1.1569)	grad_norm 0.3534 (inf)	loss_scale 1048576.0000 (1053816.6964)	mem 2327MB
[2024-07-31 23:54:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [14/30][2500/2502]	eta 0:00:00 lr 0.000013	 wd 0.0500	time 0.1544 (0.1748)	loss 1.2395 (1.1558)	grad_norm 0.3340 (inf)	loss_scale 1048576.0000 (1053607.1523)	mem 2327MB
[2024-07-31 23:54:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 14 training takes 0:07:19
[2024-07-31 23:54:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.001 (12.001)	Loss 0.4651 (0.4651)	Acc@1 92.383 (92.383)	Acc@5 98.633 (98.633)	Mem 2327MB
[2024-07-31 23:54:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.184 Acc@5 97.912
[2024-07-31 23:54:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-07-31 23:54:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-07-31 23:54:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][0/2502]	eta 7:38:53 lr 0.000013	 wd 0.0500	time 11.0046 (11.0046)	loss 1.2575 (1.2575)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:55:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][100/2502]	eta 0:11:31 lr 0.000013	 wd 0.0500	time 0.1597 (0.2881)	loss 0.7728 (1.1628)	grad_norm 0.3400 (0.3380)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:55:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][200/2502]	eta 0:08:41 lr 0.000013	 wd 0.0500	time 0.1594 (0.2267)	loss 1.4376 (1.1484)	grad_norm 0.3495 (0.3391)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:55:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][300/2502]	eta 0:07:35 lr 0.000013	 wd 0.0500	time 0.1547 (0.2070)	loss 0.9239 (1.1478)	grad_norm 0.3476 (0.3382)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:56:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][400/2502]	eta 0:06:55 lr 0.000013	 wd 0.0500	time 0.1600 (0.1975)	loss 0.7962 (1.1465)	grad_norm 0.3354 (0.3381)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:56:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][500/2502]	eta 0:06:23 lr 0.000013	 wd 0.0500	time 0.1633 (0.1913)	loss 0.9476 (1.1576)	grad_norm 0.3513 (0.3384)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:56:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][600/2502]	eta 0:05:57 lr 0.000013	 wd 0.0500	time 0.1614 (0.1880)	loss 1.3828 (1.1603)	grad_norm 0.3387 (0.3382)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:56:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][700/2502]	eta 0:05:34 lr 0.000013	 wd 0.0500	time 0.1641 (0.1854)	loss 1.2427 (1.1602)	grad_norm 0.3400 (0.3382)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:57:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][800/2502]	eta 0:05:12 lr 0.000013	 wd 0.0500	time 0.1602 (0.1836)	loss 1.3974 (1.1619)	grad_norm 0.3257 (0.3380)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:57:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][900/2502]	eta 0:04:51 lr 0.000013	 wd 0.0500	time 0.1641 (0.1820)	loss 1.2501 (1.1616)	grad_norm 0.3419 (0.3380)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:57:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1000/2502]	eta 0:04:31 lr 0.000013	 wd 0.0500	time 0.1576 (0.1807)	loss 0.8504 (1.1615)	grad_norm 0.3393 (0.3381)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:57:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1100/2502]	eta 0:04:11 lr 0.000013	 wd 0.0500	time 0.1619 (0.1797)	loss 0.8643 (1.1624)	grad_norm 0.3335 (0.3381)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:58:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1200/2502]	eta 0:03:53 lr 0.000013	 wd 0.0500	time 0.1580 (0.1790)	loss 0.8332 (1.1624)	grad_norm 0.3505 (0.3380)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:58:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1300/2502]	eta 0:03:34 lr 0.000013	 wd 0.0500	time 0.1835 (0.1783)	loss 0.8598 (1.1635)	grad_norm 0.3413 (0.3381)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:58:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1400/2502]	eta 0:03:15 lr 0.000012	 wd 0.0500	time 0.1591 (0.1778)	loss 0.9447 (1.1636)	grad_norm 0.3370 (0.3381)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:59:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1500/2502]	eta 0:02:57 lr 0.000012	 wd 0.0500	time 0.1621 (0.1773)	loss 1.0148 (1.1598)	grad_norm 0.3385 (0.3383)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:59:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1600/2502]	eta 0:02:39 lr 0.000012	 wd 0.0500	time 0.1580 (0.1768)	loss 1.0172 (1.1606)	grad_norm 0.3409 (0.3385)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:59:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1700/2502]	eta 0:02:21 lr 0.000012	 wd 0.0500	time 0.1610 (0.1764)	loss 0.8172 (1.1606)	grad_norm 0.3432 (0.3384)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-07-31 23:59:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1800/2502]	eta 0:02:03 lr 0.000012	 wd 0.0500	time 0.1656 (0.1762)	loss 1.6731 (1.1586)	grad_norm 0.3686 (0.3384)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:00:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][1900/2502]	eta 0:01:45 lr 0.000012	 wd 0.0500	time 0.1601 (0.1760)	loss 0.9619 (1.1569)	grad_norm 0.3615 (0.3384)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:00:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2000/2502]	eta 0:01:28 lr 0.000012	 wd 0.0500	time 0.1662 (0.1759)	loss 0.8376 (1.1582)	grad_norm 0.3437 (0.3384)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:00:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2100/2502]	eta 0:01:10 lr 0.000012	 wd 0.0500	time 0.1743 (0.1758)	loss 0.7850 (1.1570)	grad_norm 0.3412 (0.3384)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:01:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2200/2502]	eta 0:00:53 lr 0.000012	 wd 0.0500	time 0.1671 (0.1757)	loss 1.0711 (1.1565)	grad_norm 0.3283 (0.3384)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:01:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2300/2502]	eta 0:00:35 lr 0.000012	 wd 0.0500	time 0.1991 (0.1755)	loss 1.0104 (1.1568)	grad_norm 0.3474 (0.3385)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:01:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2400/2502]	eta 0:00:17 lr 0.000012	 wd 0.0500	time 0.1603 (0.1754)	loss 1.4634 (1.1572)	grad_norm 0.3247 (0.3385)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:01:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [15/30][2500/2502]	eta 0:00:00 lr 0.000012	 wd 0.0500	time 0.1550 (0.1749)	loss 1.5038 (1.1578)	grad_norm 0.3453 (0.3385)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:02:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 15 training takes 0:07:20
[2024-08-01 00:02:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.189 (12.189)	Loss 0.5156 (0.5156)	Acc@1 92.773 (92.773)	Acc@5 98.242 (98.242)	Mem 2327MB
[2024-08-01 00:02:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.150 Acc@5 97.890
[2024-08-01 00:02:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 00:02:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:02:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][0/2502]	eta 7:46:55 lr 0.000012	 wd 0.0500	time 11.1973 (11.1973)	loss 1.1984 (1.1984)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][100/2502]	eta 0:11:17 lr 0.000012	 wd 0.0500	time 0.1504 (0.2819)	loss 1.0365 (1.1682)	grad_norm 0.3465 (inf)	loss_scale 1048576.0000 (1131631.5248)	mem 2327MB
[2024-08-01 00:03:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][200/2502]	eta 0:08:38 lr 0.000012	 wd 0.0500	time 0.1662 (0.2253)	loss 1.2502 (1.1579)	grad_norm 0.3394 (inf)	loss_scale 1048576.0000 (1090310.3682)	mem 2327MB
[2024-08-01 00:03:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][300/2502]	eta 0:07:35 lr 0.000012	 wd 0.0500	time 0.1634 (0.2071)	loss 1.0126 (1.1510)	grad_norm 0.3327 (inf)	loss_scale 1048576.0000 (1076445.1296)	mem 2327MB
[2024-08-01 00:03:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][400/2502]	eta 0:06:55 lr 0.000012	 wd 0.0500	time 0.1629 (0.1977)	loss 0.9237 (1.1397)	grad_norm 0.3372 (inf)	loss_scale 1048576.0000 (1069495.2219)	mem 2327MB
[2024-08-01 00:04:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][500/2502]	eta 0:06:25 lr 0.000012	 wd 0.0500	time 0.1595 (0.1923)	loss 1.5244 (1.1425)	grad_norm 0.3514 (inf)	loss_scale 1048576.0000 (1065319.7285)	mem 2327MB
[2024-08-01 00:04:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][600/2502]	eta 0:05:58 lr 0.000012	 wd 0.0500	time 0.1677 (0.1886)	loss 1.4337 (1.1433)	grad_norm 0.3196 (inf)	loss_scale 1048576.0000 (1062533.7504)	mem 2327MB
[2024-08-01 00:04:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][700/2502]	eta 0:05:34 lr 0.000012	 wd 0.0500	time 0.1662 (0.1857)	loss 1.0607 (1.1459)	grad_norm 0.3407 (inf)	loss_scale 1048576.0000 (1060542.6305)	mem 2327MB
[2024-08-01 00:04:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][800/2502]	eta 0:05:12 lr 0.000012	 wd 0.0500	time 0.1527 (0.1836)	loss 1.3144 (1.1474)	grad_norm 0.3487 (inf)	loss_scale 1048576.0000 (1059048.6692)	mem 2327MB
[2024-08-01 00:05:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][900/2502]	eta 0:04:51 lr 0.000012	 wd 0.0500	time 0.1620 (0.1820)	loss 1.1336 (1.1485)	grad_norm 0.3378 (inf)	loss_scale 1048576.0000 (1057886.3307)	mem 2327MB
[2024-08-01 00:05:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1000/2502]	eta 0:04:31 lr 0.000011	 wd 0.0500	time 0.1590 (0.1807)	loss 1.3496 (1.1484)	grad_norm 0.3309 (inf)	loss_scale 1048576.0000 (1056956.2278)	mem 2327MB
[2024-08-01 00:05:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1100/2502]	eta 0:04:11 lr 0.000011	 wd 0.0500	time 0.2145 (0.1796)	loss 0.8410 (1.1525)	grad_norm 0.3523 (inf)	loss_scale 1048576.0000 (1056195.0808)	mem 2327MB
[2024-08-01 00:06:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1200/2502]	eta 0:03:52 lr 0.000011	 wd 0.0500	time 0.1602 (0.1787)	loss 0.8686 (1.1503)	grad_norm 0.3371 (inf)	loss_scale 1048576.0000 (1055560.6861)	mem 2327MB
[2024-08-01 00:06:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1300/2502]	eta 0:03:34 lr 0.000011	 wd 0.0500	time 0.1825 (0.1782)	loss 1.2905 (1.1532)	grad_norm 0.3379 (inf)	loss_scale 1048576.0000 (1055023.8155)	mem 2327MB
[2024-08-01 00:06:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1400/2502]	eta 0:03:15 lr 0.000011	 wd 0.0500	time 0.1593 (0.1776)	loss 1.2334 (1.1531)	grad_norm 0.3505 (inf)	loss_scale 1048576.0000 (1054563.5860)	mem 2327MB
[2024-08-01 00:06:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1500/2502]	eta 0:02:57 lr 0.000011	 wd 0.0500	time 0.1639 (0.1772)	loss 1.2851 (1.1554)	grad_norm 0.3318 (inf)	loss_scale 1048576.0000 (1054164.6795)	mem 2327MB
[2024-08-01 00:07:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1600/2502]	eta 0:02:39 lr 0.000011	 wd 0.0500	time 0.1673 (0.1768)	loss 1.1092 (1.1560)	grad_norm 0.3546 (inf)	loss_scale 1048576.0000 (1053815.6052)	mem 2327MB
[2024-08-01 00:07:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1700/2502]	eta 0:02:21 lr 0.000011	 wd 0.0500	time 0.1666 (0.1765)	loss 1.0726 (1.1550)	grad_norm 0.3480 (inf)	loss_scale 1048576.0000 (1053507.5744)	mem 2327MB
[2024-08-01 00:07:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1800/2502]	eta 0:02:03 lr 0.000011	 wd 0.0500	time 0.1601 (0.1763)	loss 0.8626 (1.1540)	grad_norm 0.3203 (inf)	loss_scale 1048576.0000 (1053233.7501)	mem 2327MB
[2024-08-01 00:08:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][1900/2502]	eta 0:01:46 lr 0.000011	 wd 0.0500	time 0.1606 (0.1761)	loss 1.0255 (1.1548)	grad_norm 0.3259 (inf)	loss_scale 1048576.0000 (1052988.7344)	mem 2327MB
[2024-08-01 00:08:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2000/2502]	eta 0:01:28 lr 0.000011	 wd 0.0500	time 0.2108 (0.1759)	loss 1.3464 (1.1555)	grad_norm 0.3196 (inf)	loss_scale 1048576.0000 (1052768.2079)	mem 2327MB
[2024-08-01 00:08:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2100/2502]	eta 0:01:10 lr 0.000011	 wd 0.0500	time 0.1666 (0.1758)	loss 1.3537 (1.1563)	grad_norm 0.3238 (inf)	loss_scale 1048576.0000 (1052568.6740)	mem 2327MB
[2024-08-01 00:08:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2200/2502]	eta 0:00:53 lr 0.000011	 wd 0.0500	time 0.2463 (0.1758)	loss 1.0138 (1.1559)	grad_norm 0.3482 (inf)	loss_scale 1048576.0000 (1052387.2712)	mem 2327MB
[2024-08-01 00:09:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2300/2502]	eta 0:00:35 lr 0.000011	 wd 0.0500	time 0.1781 (0.1757)	loss 0.8928 (1.1541)	grad_norm 0.3547 (inf)	loss_scale 1048576.0000 (1052221.6358)	mem 2327MB
[2024-08-01 00:09:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2400/2502]	eta 0:00:17 lr 0.000011	 wd 0.0500	time 0.1666 (0.1757)	loss 1.2132 (1.1531)	grad_norm 0.3338 (inf)	loss_scale 1048576.0000 (1052069.7976)	mem 2327MB
[2024-08-01 00:09:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [16/30][2500/2502]	eta 0:00:00 lr 0.000011	 wd 0.0500	time 0.1557 (0.1753)	loss 0.8391 (1.1534)	grad_norm 0.3450 (inf)	loss_scale 1048576.0000 (1051930.1016)	mem 2327MB
[2024-08-01 00:09:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 16 training takes 0:07:20
[2024-08-01 00:10:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.784 (11.784)	Loss 0.4866 (0.4866)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 00:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.168 Acc@5 97.894
[2024-08-01 00:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 00:10:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:10:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][0/2502]	eta 7:31:22 lr 0.000011	 wd 0.0500	time 10.8243 (10.8243)	loss 1.3874 (1.3874)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:10:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][100/2502]	eta 0:11:21 lr 0.000011	 wd 0.0500	time 0.1663 (0.2836)	loss 1.2248 (1.1370)	grad_norm 0.3301 (0.3393)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:11:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][200/2502]	eta 0:08:41 lr 0.000011	 wd 0.0500	time 0.1679 (0.2264)	loss 1.2717 (1.1365)	grad_norm 0.3333 (0.3389)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:11:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][300/2502]	eta 0:07:35 lr 0.000011	 wd 0.0500	time 0.1604 (0.2070)	loss 1.2295 (1.1520)	grad_norm 0.3363 (0.3396)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:11:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][400/2502]	eta 0:06:56 lr 0.000011	 wd 0.0500	time 0.1563 (0.1981)	loss 1.4079 (1.1499)	grad_norm 0.3418 (0.3394)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:11:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][500/2502]	eta 0:06:25 lr 0.000010	 wd 0.0500	time 0.1611 (0.1924)	loss 1.3644 (1.1543)	grad_norm 0.3456 (0.3398)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:12:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][600/2502]	eta 0:05:59 lr 0.000010	 wd 0.0500	time 0.1954 (0.1888)	loss 0.8136 (1.1555)	grad_norm 0.3360 (0.3396)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:12:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][700/2502]	eta 0:05:35 lr 0.000010	 wd 0.0500	time 0.1651 (0.1863)	loss 1.2787 (1.1533)	grad_norm 0.3494 (0.3398)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:12:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][800/2502]	eta 0:05:14 lr 0.000010	 wd 0.0500	time 0.1535 (0.1845)	loss 1.1885 (1.1515)	grad_norm 0.3422 (0.3401)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:13:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][900/2502]	eta 0:04:52 lr 0.000010	 wd 0.0500	time 0.1676 (0.1827)	loss 1.2898 (1.1532)	grad_norm 0.3442 (0.3403)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:13:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1000/2502]	eta 0:04:32 lr 0.000010	 wd 0.0500	time 0.1541 (0.1815)	loss 1.0338 (1.1526)	grad_norm 0.3562 (0.3404)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:13:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1100/2502]	eta 0:04:13 lr 0.000010	 wd 0.0500	time 0.1589 (0.1805)	loss 1.0214 (1.1571)	grad_norm 0.3324 (0.3405)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:13:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1200/2502]	eta 0:03:54 lr 0.000010	 wd 0.0500	time 0.1595 (0.1798)	loss 0.7609 (1.1571)	grad_norm 0.3319 (0.3405)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:14:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1300/2502]	eta 0:03:35 lr 0.000010	 wd 0.0500	time 0.1859 (0.1794)	loss 1.4130 (1.1542)	grad_norm 0.3518 (0.3404)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:14:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1400/2502]	eta 0:03:17 lr 0.000010	 wd 0.0500	time 0.1695 (0.1788)	loss 1.3268 (1.1543)	grad_norm 0.3229 (0.3403)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:14:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1500/2502]	eta 0:02:58 lr 0.000010	 wd 0.0500	time 0.1585 (0.1783)	loss 1.4600 (1.1549)	grad_norm 0.3345 (0.3403)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:15:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1600/2502]	eta 0:02:40 lr 0.000010	 wd 0.0500	time 0.1858 (0.1779)	loss 1.3856 (1.1548)	grad_norm 0.3300 (inf)	loss_scale 1048576.0000 (1068224.5197)	mem 2327MB
[2024-08-01 00:15:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1700/2502]	eta 0:02:22 lr 0.000010	 wd 0.0500	time 0.1995 (0.1776)	loss 1.5184 (1.1565)	grad_norm 0.3309 (inf)	loss_scale 1048576.0000 (1067069.4039)	mem 2327MB
[2024-08-01 00:15:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1800/2502]	eta 0:02:04 lr 0.000010	 wd 0.0500	time 0.1689 (0.1773)	loss 0.9701 (1.1560)	grad_norm 0.3541 (inf)	loss_scale 1048576.0000 (1066042.5630)	mem 2327MB
[2024-08-01 00:15:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][1900/2502]	eta 0:01:46 lr 0.000010	 wd 0.0500	time 0.1901 (0.1770)	loss 1.2399 (1.1550)	grad_norm 0.3374 (inf)	loss_scale 1048576.0000 (1065123.7538)	mem 2327MB
[2024-08-01 00:16:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2000/2502]	eta 0:01:28 lr 0.000010	 wd 0.0500	time 0.1670 (0.1767)	loss 1.2077 (1.1553)	grad_norm 0.3413 (inf)	loss_scale 1048576.0000 (1064296.7796)	mem 2327MB
[2024-08-01 00:16:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2100/2502]	eta 0:01:10 lr 0.000010	 wd 0.0500	time 0.1705 (0.1764)	loss 1.4676 (1.1552)	grad_norm 0.3637 (inf)	loss_scale 1048576.0000 (1063548.5274)	mem 2327MB
[2024-08-01 00:16:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2200/2502]	eta 0:00:53 lr 0.000010	 wd 0.0500	time 0.1730 (0.1763)	loss 1.3626 (1.1550)	grad_norm 0.3341 (inf)	loss_scale 1048576.0000 (1062868.2672)	mem 2327MB
[2024-08-01 00:17:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2300/2502]	eta 0:00:35 lr 0.000010	 wd 0.0500	time 0.1716 (0.1761)	loss 0.7929 (1.1545)	grad_norm 0.3484 (inf)	loss_scale 1048576.0000 (1062247.1343)	mem 2327MB
[2024-08-01 00:17:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2400/2502]	eta 0:00:17 lr 0.000010	 wd 0.0500	time 0.1810 (0.1760)	loss 0.9268 (1.1532)	grad_norm 0.3526 (inf)	loss_scale 1048576.0000 (1061677.7409)	mem 2327MB
[2024-08-01 00:17:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [17/30][2500/2502]	eta 0:00:00 lr 0.000009	 wd 0.0500	time 0.1542 (0.1754)	loss 1.4531 (1.1528)	grad_norm 0.3335 (inf)	loss_scale 1048576.0000 (1061153.8808)	mem 2327MB
[2024-08-01 00:17:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 17 training takes 0:07:21
[2024-08-01 00:17:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.770 (11.770)	Loss 0.4780 (0.4780)	Acc@1 92.188 (92.188)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 00:18:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.188 Acc@5 97.882
[2024-08-01 00:18:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 00:18:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:18:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][0/2502]	eta 8:16:13 lr 0.000009	 wd 0.0500	time 11.9001 (11.9001)	loss 1.4315 (1.4315)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:18:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][100/2502]	eta 0:11:31 lr 0.000009	 wd 0.0500	time 0.1635 (0.2877)	loss 1.4809 (1.1870)	grad_norm 0.3449 (0.3405)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:18:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][200/2502]	eta 0:08:48 lr 0.000009	 wd 0.0500	time 0.1565 (0.2295)	loss 1.0261 (1.1789)	grad_norm 0.3702 (0.3408)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:19:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][300/2502]	eta 0:07:41 lr 0.000009	 wd 0.0500	time 0.1535 (0.2094)	loss 1.5024 (1.1778)	grad_norm 0.3266 (0.3408)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:19:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][400/2502]	eta 0:06:59 lr 0.000009	 wd 0.0500	time 0.1774 (0.1996)	loss 0.7487 (1.1765)	grad_norm 0.3447 (0.3404)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:19:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][500/2502]	eta 0:06:27 lr 0.000009	 wd 0.0500	time 0.1688 (0.1936)	loss 1.2851 (1.1680)	grad_norm 0.3212 (0.3403)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:20:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][600/2502]	eta 0:06:01 lr 0.000009	 wd 0.0500	time 0.1673 (0.1899)	loss 1.4896 (1.1606)	grad_norm 0.3390 (0.3401)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:20:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][700/2502]	eta 0:05:37 lr 0.000009	 wd 0.0500	time 0.1610 (0.1875)	loss 1.4621 (1.1656)	grad_norm 0.3335 (0.3403)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:20:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][800/2502]	eta 0:05:15 lr 0.000009	 wd 0.0500	time 0.2058 (0.1856)	loss 1.3198 (1.1686)	grad_norm 0.3289 (0.3404)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:20:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][900/2502]	eta 0:04:54 lr 0.000009	 wd 0.0500	time 0.1736 (0.1841)	loss 0.9172 (1.1685)	grad_norm 0.3519 (0.3405)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:21:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1000/2502]	eta 0:04:34 lr 0.000009	 wd 0.0500	time 0.1647 (0.1827)	loss 1.4189 (1.1642)	grad_norm 0.3362 (0.3402)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:21:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1100/2502]	eta 0:04:14 lr 0.000009	 wd 0.0500	time 0.1707 (0.1816)	loss 0.9939 (1.1661)	grad_norm 0.3317 (0.3399)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:21:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1200/2502]	eta 0:03:55 lr 0.000009	 wd 0.0500	time 0.1708 (0.1807)	loss 1.0441 (1.1593)	grad_norm 0.3452 (0.3398)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:22:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1300/2502]	eta 0:03:36 lr 0.000009	 wd 0.0500	time 0.1581 (0.1800)	loss 1.4678 (1.1595)	grad_norm 0.3497 (0.3400)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:22:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1400/2502]	eta 0:03:17 lr 0.000009	 wd 0.0500	time 0.2062 (0.1794)	loss 1.1564 (1.1597)	grad_norm 0.3233 (0.3401)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:22:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1500/2502]	eta 0:02:59 lr 0.000009	 wd 0.0500	time 0.1710 (0.1789)	loss 1.3081 (1.1593)	grad_norm 0.3298 (0.3399)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:22:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1600/2502]	eta 0:02:41 lr 0.000009	 wd 0.0500	time 0.1725 (0.1786)	loss 1.4527 (1.1613)	grad_norm 0.3546 (0.3401)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:23:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1700/2502]	eta 0:02:22 lr 0.000009	 wd 0.0500	time 0.1818 (0.1782)	loss 1.3226 (1.1631)	grad_norm 0.3488 (0.3400)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:23:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1800/2502]	eta 0:02:04 lr 0.000009	 wd 0.0500	time 0.1623 (0.1781)	loss 0.7205 (1.1635)	grad_norm 0.3389 (0.3400)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:23:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][1900/2502]	eta 0:01:47 lr 0.000009	 wd 0.0500	time 0.1657 (0.1778)	loss 1.4214 (1.1631)	grad_norm 0.3387 (0.3402)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:24:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2000/2502]	eta 0:01:29 lr 0.000008	 wd 0.0500	time 0.1598 (0.1776)	loss 1.0577 (1.1627)	grad_norm 0.3294 (0.3402)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:24:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2100/2502]	eta 0:01:11 lr 0.000008	 wd 0.0500	time 0.1586 (0.1774)	loss 1.3299 (1.1617)	grad_norm 0.3383 (0.3402)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:24:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2200/2502]	eta 0:00:53 lr 0.000008	 wd 0.0500	time 0.1611 (0.1772)	loss 1.0207 (1.1619)	grad_norm 0.3214 (0.3402)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:24:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2300/2502]	eta 0:00:35 lr 0.000008	 wd 0.0500	time 0.1605 (0.1769)	loss 1.3112 (1.1625)	grad_norm 0.3325 (0.3401)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2400/2502]	eta 0:00:18 lr 0.000008	 wd 0.0500	time 0.1627 (0.1768)	loss 1.4248 (1.1631)	grad_norm 0.3590 (0.3401)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:25:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [18/30][2500/2502]	eta 0:00:00 lr 0.000008	 wd 0.0500	time 0.1542 (0.1762)	loss 1.2493 (1.1634)	grad_norm 0.3365 (0.3401)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:25:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 18 training takes 0:07:23
[2024-08-01 00:25:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.243 (11.243)	Loss 0.4885 (0.4885)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 00:25:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.184 Acc@5 97.850
[2024-08-01 00:25:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 00:25:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:26:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][0/2502]	eta 7:54:13 lr 0.000008	 wd 0.0500	time 11.3722 (11.3722)	loss 0.7975 (0.7975)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:26:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][100/2502]	eta 0:11:16 lr 0.000008	 wd 0.0500	time 0.1594 (0.2816)	loss 1.5895 (1.2098)	grad_norm 0.3296 (0.3400)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:26:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][200/2502]	eta 0:08:40 lr 0.000008	 wd 0.0500	time 0.1773 (0.2260)	loss 1.1735 (1.1792)	grad_norm 0.3506 (0.3405)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:27:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][300/2502]	eta 0:07:36 lr 0.000008	 wd 0.0500	time 0.1624 (0.2073)	loss 1.4067 (1.1655)	grad_norm 0.3458 (0.3404)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:27:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][400/2502]	eta 0:06:56 lr 0.000008	 wd 0.0500	time 0.1608 (0.1980)	loss 0.9592 (1.1590)	grad_norm 0.3289 (0.3402)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:27:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][500/2502]	eta 0:06:25 lr 0.000008	 wd 0.0500	time 0.1657 (0.1923)	loss 1.4463 (1.1635)	grad_norm 0.3410 (0.3406)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:27:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][600/2502]	eta 0:06:03 lr 0.000008	 wd 0.0500	time 0.1618 (0.1911)	loss 0.8447 (1.1530)	grad_norm 0.3482 (inf)	loss_scale 1048576.0000 (1052065.4376)	mem 2327MB
[2024-08-01 00:28:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][700/2502]	eta 0:05:38 lr 0.000008	 wd 0.0500	time 0.1564 (0.1879)	loss 1.2419 (1.1511)	grad_norm 0.3314 (inf)	loss_scale 1048576.0000 (1051567.6576)	mem 2327MB
[2024-08-01 00:28:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][800/2502]	eta 0:05:15 lr 0.000008	 wd 0.0500	time 0.1573 (0.1855)	loss 1.5187 (1.1527)	grad_norm 0.3482 (inf)	loss_scale 1048576.0000 (1051194.1673)	mem 2327MB
[2024-08-01 00:28:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][900/2502]	eta 0:04:54 lr 0.000008	 wd 0.0500	time 0.1786 (0.1837)	loss 1.2743 (1.1533)	grad_norm 0.3516 (inf)	loss_scale 1048576.0000 (1050903.5827)	mem 2327MB
[2024-08-01 00:29:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1000/2502]	eta 0:04:33 lr 0.000008	 wd 0.0500	time 0.1708 (0.1822)	loss 1.4480 (1.1548)	grad_norm 0.3284 (inf)	loss_scale 1048576.0000 (1050671.0569)	mem 2327MB
[2024-08-01 00:29:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1100/2502]	eta 0:04:13 lr 0.000008	 wd 0.0500	time 0.1571 (0.1811)	loss 1.1703 (1.1538)	grad_norm 0.3405 (inf)	loss_scale 1048576.0000 (1050480.7702)	mem 2327MB
[2024-08-01 00:29:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1200/2502]	eta 0:03:54 lr 0.000008	 wd 0.0500	time 0.1596 (0.1802)	loss 1.3144 (1.1534)	grad_norm 0.3492 (inf)	loss_scale 1048576.0000 (1050322.1715)	mem 2327MB
[2024-08-01 00:29:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1300/2502]	eta 0:03:36 lr 0.000008	 wd 0.0500	time 0.1805 (0.1800)	loss 0.7780 (1.1524)	grad_norm 0.3527 (inf)	loss_scale 1048576.0000 (1050187.9539)	mem 2327MB
[2024-08-01 00:30:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1400/2502]	eta 0:03:17 lr 0.000008	 wd 0.0500	time 0.1621 (0.1792)	loss 0.8076 (1.1515)	grad_norm 0.3421 (inf)	loss_scale 1048576.0000 (1050072.8965)	mem 2327MB
[2024-08-01 00:30:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1500/2502]	eta 0:02:59 lr 0.000008	 wd 0.0500	time 0.1624 (0.1789)	loss 1.3634 (1.1528)	grad_norm 0.3319 (inf)	loss_scale 1048576.0000 (1049973.1699)	mem 2327MB
[2024-08-01 00:30:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1600/2502]	eta 0:02:40 lr 0.000007	 wd 0.0500	time 0.1705 (0.1784)	loss 1.4090 (1.1545)	grad_norm 0.3395 (inf)	loss_scale 1048576.0000 (1049885.9013)	mem 2327MB
[2024-08-01 00:31:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1700/2502]	eta 0:02:22 lr 0.000007	 wd 0.0500	time 0.1586 (0.1781)	loss 0.8936 (1.1551)	grad_norm 0.3554 (inf)	loss_scale 1048576.0000 (1049808.8936)	mem 2327MB
[2024-08-01 00:31:19 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1800/2502]	eta 0:02:04 lr 0.000007	 wd 0.0500	time 0.1605 (0.1777)	loss 1.3182 (1.1575)	grad_norm 0.3372 (inf)	loss_scale 1048576.0000 (1049740.4375)	mem 2327MB
[2024-08-01 00:31:36 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][1900/2502]	eta 0:01:46 lr 0.000007	 wd 0.0500	time 0.2033 (0.1775)	loss 1.0988 (1.1564)	grad_norm 0.3510 (inf)	loss_scale 1048576.0000 (1049679.1836)	mem 2327MB
[2024-08-01 00:31:53 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2000/2502]	eta 0:01:28 lr 0.000007	 wd 0.0500	time 0.1635 (0.1771)	loss 1.4714 (1.1576)	grad_norm 0.3471 (inf)	loss_scale 1048576.0000 (1049624.0520)	mem 2327MB
[2024-08-01 00:32:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2100/2502]	eta 0:01:11 lr 0.000007	 wd 0.0500	time 0.1576 (0.1768)	loss 1.5732 (1.1590)	grad_norm 0.3346 (inf)	loss_scale 1048576.0000 (1049574.1685)	mem 2327MB
[2024-08-01 00:32:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2200/2502]	eta 0:00:53 lr 0.000007	 wd 0.0500	time 0.1633 (0.1765)	loss 1.1381 (1.1581)	grad_norm 0.3562 (inf)	loss_scale 1048576.0000 (1049528.8178)	mem 2327MB
[2024-08-01 00:32:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2300/2502]	eta 0:00:35 lr 0.000007	 wd 0.0500	time 0.1712 (0.1764)	loss 1.0151 (1.1583)	grad_norm 0.3325 (inf)	loss_scale 1048576.0000 (1049487.4090)	mem 2327MB
[2024-08-01 00:33:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2400/2502]	eta 0:00:17 lr 0.000007	 wd 0.0500	time 0.1689 (0.1763)	loss 1.1046 (1.1574)	grad_norm 0.3485 (inf)	loss_scale 1048576.0000 (1049449.4494)	mem 2327MB
[2024-08-01 00:33:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [19/30][2500/2502]	eta 0:00:00 lr 0.000007	 wd 0.0500	time 0.1535 (0.1758)	loss 0.8043 (1.1577)	grad_norm 0.3416 (inf)	loss_scale 1048576.0000 (1049414.5254)	mem 2327MB
[2024-08-01 00:33:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 19 training takes 0:07:22
[2024-08-01 00:33:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.574 (12.574)	Loss 0.4734 (0.4734)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 00:33:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.156 Acc@5 97.902
[2024-08-01 00:33:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 00:33:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:34:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][0/2502]	eta 7:20:48 lr 0.000007	 wd 0.0500	time 10.5711 (10.5711)	loss 1.5213 (1.5213)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:34:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][100/2502]	eta 0:11:13 lr 0.000007	 wd 0.0500	time 0.1605 (0.2805)	loss 1.0732 (1.1672)	grad_norm 0.3523 (0.3438)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:34:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][200/2502]	eta 0:08:37 lr 0.000007	 wd 0.0500	time 0.1651 (0.2247)	loss 1.3040 (1.1509)	grad_norm 0.3567 (0.3419)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:34:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][300/2502]	eta 0:07:33 lr 0.000007	 wd 0.0500	time 0.1601 (0.2059)	loss 0.6987 (1.1631)	grad_norm 0.3543 (0.3417)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:35:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][400/2502]	eta 0:06:52 lr 0.000007	 wd 0.0500	time 0.1556 (0.1963)	loss 1.3027 (1.1624)	grad_norm 0.3431 (0.3414)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:35:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][500/2502]	eta 0:06:22 lr 0.000007	 wd 0.0500	time 0.1668 (0.1910)	loss 1.2745 (1.1603)	grad_norm 0.3357 (0.3415)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:35:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][600/2502]	eta 0:05:56 lr 0.000007	 wd 0.0500	time 0.1616 (0.1873)	loss 1.2560 (1.1653)	grad_norm 0.3661 (0.3413)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:35:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][700/2502]	eta 0:05:32 lr 0.000007	 wd 0.0500	time 0.1733 (0.1845)	loss 1.2550 (1.1636)	grad_norm 0.3420 (0.3414)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:36:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][800/2502]	eta 0:05:10 lr 0.000007	 wd 0.0500	time 0.1674 (0.1822)	loss 1.1610 (1.1639)	grad_norm 0.3340 (0.3413)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:36:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][900/2502]	eta 0:04:50 lr 0.000007	 wd 0.0500	time 0.1603 (0.1810)	loss 1.2499 (1.1632)	grad_norm 0.3506 (0.3412)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:36:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1000/2502]	eta 0:04:29 lr 0.000007	 wd 0.0500	time 0.1763 (0.1797)	loss 1.3825 (1.1652)	grad_norm 0.3320 (0.3411)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:37:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1100/2502]	eta 0:04:10 lr 0.000007	 wd 0.0500	time 0.1664 (0.1787)	loss 1.0659 (1.1630)	grad_norm 0.3549 (0.3412)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:37:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1200/2502]	eta 0:03:51 lr 0.000006	 wd 0.0500	time 0.1508 (0.1779)	loss 1.2244 (1.1591)	grad_norm 0.3355 (0.3411)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:37:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1300/2502]	eta 0:03:33 lr 0.000006	 wd 0.0500	time 0.1803 (0.1773)	loss 1.3439 (1.1587)	grad_norm 0.3435 (0.3411)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:37:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1400/2502]	eta 0:03:14 lr 0.000006	 wd 0.0500	time 0.1885 (0.1768)	loss 1.0517 (1.1601)	grad_norm 0.3547 (0.3412)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:38:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1500/2502]	eta 0:02:56 lr 0.000006	 wd 0.0500	time 0.1619 (0.1765)	loss 1.3214 (1.1628)	grad_norm 0.3293 (0.3412)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:38:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1600/2502]	eta 0:02:38 lr 0.000006	 wd 0.0500	time 0.1777 (0.1762)	loss 1.2899 (1.1649)	grad_norm 0.3407 (0.3413)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:38:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1700/2502]	eta 0:02:21 lr 0.000006	 wd 0.0500	time 0.1571 (0.1759)	loss 1.6074 (1.1662)	grad_norm 0.3285 (0.3413)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:39:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1800/2502]	eta 0:02:03 lr 0.000006	 wd 0.0500	time 0.1571 (0.1756)	loss 0.7619 (1.1639)	grad_norm 0.3388 (0.3413)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:39:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][1900/2502]	eta 0:01:45 lr 0.000006	 wd 0.0500	time 0.1594 (0.1754)	loss 0.8372 (1.1652)	grad_norm 0.3443 (0.3413)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2000/2502]	eta 0:01:28 lr 0.000006	 wd 0.0500	time 0.1635 (0.1755)	loss 1.1019 (1.1643)	grad_norm 0.3621 (0.3414)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:39:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2100/2502]	eta 0:01:10 lr 0.000006	 wd 0.0500	time 0.1558 (0.1753)	loss 1.2350 (1.1642)	grad_norm 0.3303 (0.3414)	loss_scale 2097152.0000 (1054565.0109)	mem 2327MB
[2024-08-01 00:40:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2200/2502]	eta 0:00:52 lr 0.000006	 wd 0.0500	time 0.1658 (0.1752)	loss 1.3936 (1.1654)	grad_norm 0.3462 (inf)	loss_scale 1048576.0000 (1059056.9959)	mem 2327MB
[2024-08-01 00:40:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2300/2502]	eta 0:00:35 lr 0.000006	 wd 0.0500	time 0.1885 (0.1751)	loss 0.8653 (1.1666)	grad_norm 0.3470 (inf)	loss_scale 1048576.0000 (1058601.4985)	mem 2327MB
[2024-08-01 00:40:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2400/2502]	eta 0:00:17 lr 0.000006	 wd 0.0500	time 0.1987 (0.1749)	loss 0.7779 (1.1659)	grad_norm 0.3555 (inf)	loss_scale 1048576.0000 (1058183.9434)	mem 2327MB
[2024-08-01 00:41:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [20/30][2500/2502]	eta 0:00:00 lr 0.000006	 wd 0.0500	time 0.1545 (0.1744)	loss 1.0480 (1.1678)	grad_norm 0.3323 (inf)	loss_scale 1048576.0000 (1057799.7793)	mem 2327MB
[2024-08-01 00:41:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 20 training takes 0:07:18
[2024-08-01 00:41:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_20.pth saving......
[2024-08-01 00:41:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_20.pth saved !!!
[2024-08-01 00:41:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.997 (11.997)	Loss 0.4829 (0.4829)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 00:41:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.196 Acc@5 97.910
[2024-08-01 00:41:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 00:41:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:41:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][0/2502]	eta 7:42:49 lr 0.000006	 wd 0.0500	time 11.0988 (11.0988)	loss 0.9933 (0.9933)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:42:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][100/2502]	eta 0:11:17 lr 0.000006	 wd 0.0500	time 0.1570 (0.2821)	loss 1.3684 (1.1882)	grad_norm 0.3545 (0.3400)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:42:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][200/2502]	eta 0:08:39 lr 0.000006	 wd 0.0500	time 0.1666 (0.2255)	loss 1.3174 (1.1827)	grad_norm 0.3470 (0.3417)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:42:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][300/2502]	eta 0:07:35 lr 0.000006	 wd 0.0500	time 0.1491 (0.2068)	loss 1.1855 (1.1724)	grad_norm 0.3582 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:42:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][400/2502]	eta 0:06:55 lr 0.000006	 wd 0.0500	time 0.1611 (0.1977)	loss 1.3583 (1.1645)	grad_norm 0.3379 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:43:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][500/2502]	eta 0:06:24 lr 0.000006	 wd 0.0500	time 0.1599 (0.1920)	loss 0.8990 (1.1594)	grad_norm 0.3424 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:43:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][600/2502]	eta 0:05:57 lr 0.000006	 wd 0.0500	time 0.1596 (0.1881)	loss 1.3006 (1.1547)	grad_norm 0.3338 (0.3419)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:43:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][700/2502]	eta 0:05:34 lr 0.000006	 wd 0.0500	time 0.1707 (0.1856)	loss 1.2310 (1.1522)	grad_norm 0.3382 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:44:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][800/2502]	eta 0:05:12 lr 0.000006	 wd 0.0500	time 0.1735 (0.1836)	loss 1.2121 (1.1508)	grad_norm 0.3411 (0.3416)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:44:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][900/2502]	eta 0:04:54 lr 0.000005	 wd 0.0500	time 0.1590 (0.1839)	loss 1.4330 (1.1539)	grad_norm 0.3491 (0.3415)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:44:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1000/2502]	eta 0:04:33 lr 0.000005	 wd 0.0500	time 0.1644 (0.1824)	loss 1.4907 (1.1572)	grad_norm 0.3434 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:44:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1100/2502]	eta 0:04:13 lr 0.000005	 wd 0.0500	time 0.1631 (0.1812)	loss 0.8016 (1.1604)	grad_norm 0.3383 (0.3417)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:45:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1200/2502]	eta 0:03:54 lr 0.000005	 wd 0.0500	time 0.1678 (0.1804)	loss 1.3903 (1.1573)	grad_norm 0.3481 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:45:30 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1300/2502]	eta 0:03:35 lr 0.000005	 wd 0.0500	time 0.1575 (0.1797)	loss 1.1924 (1.1576)	grad_norm 0.3406 (0.3415)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:45:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1400/2502]	eta 0:03:17 lr 0.000005	 wd 0.0500	time 0.1754 (0.1792)	loss 0.8303 (1.1550)	grad_norm 0.3553 (0.3416)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:46:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1500/2502]	eta 0:02:59 lr 0.000005	 wd 0.0500	time 0.1662 (0.1788)	loss 0.8475 (1.1566)	grad_norm 0.3480 (0.3416)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:46:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1600/2502]	eta 0:02:41 lr 0.000005	 wd 0.0500	time 0.1671 (0.1787)	loss 1.0502 (1.1532)	grad_norm 0.3665 (0.3417)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:46:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1700/2502]	eta 0:02:22 lr 0.000005	 wd 0.0500	time 0.1671 (0.1783)	loss 1.3526 (1.1540)	grad_norm 0.3279 (0.3417)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:46:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1800/2502]	eta 0:02:05 lr 0.000005	 wd 0.0500	time 0.1492 (0.1781)	loss 0.7724 (1.1545)	grad_norm 0.3561 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:47:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][1900/2502]	eta 0:01:47 lr 0.000005	 wd 0.0500	time 0.1756 (0.1778)	loss 0.8447 (1.1552)	grad_norm 0.3412 (0.3417)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:47:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2000/2502]	eta 0:01:29 lr 0.000005	 wd 0.0500	time 0.1697 (0.1777)	loss 1.0639 (1.1543)	grad_norm 0.3628 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:47:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2100/2502]	eta 0:01:11 lr 0.000005	 wd 0.0500	time 0.1722 (0.1774)	loss 1.1337 (1.1540)	grad_norm 0.3430 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:48:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2200/2502]	eta 0:00:53 lr 0.000005	 wd 0.0500	time 0.1635 (0.1771)	loss 1.4415 (1.1526)	grad_norm 0.3456 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:48:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2300/2502]	eta 0:00:35 lr 0.000005	 wd 0.0500	time 0.1651 (0.1768)	loss 1.5360 (1.1543)	grad_norm 0.3593 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:48:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2400/2502]	eta 0:00:18 lr 0.000005	 wd 0.0500	time 0.1622 (0.1767)	loss 1.2232 (1.1561)	grad_norm 0.3493 (0.3419)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:48:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [21/30][2500/2502]	eta 0:00:00 lr 0.000005	 wd 0.0500	time 0.1541 (0.1761)	loss 1.4806 (1.1566)	grad_norm 0.3432 (0.3419)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:49:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 21 training takes 0:07:23
[2024-08-01 00:49:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.545 (11.545)	Loss 0.5024 (0.5024)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 00:49:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.122 Acc@5 97.890
[2024-08-01 00:49:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 00:49:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:49:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][0/2502]	eta 7:52:27 lr 0.000005	 wd 0.0500	time 11.3300 (11.3300)	loss 0.9906 (0.9906)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:49:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][100/2502]	eta 0:11:21 lr 0.000005	 wd 0.0500	time 0.1590 (0.2837)	loss 1.4718 (1.1733)	grad_norm 0.3653 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:50:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][200/2502]	eta 0:08:41 lr 0.000005	 wd 0.0500	time 0.1643 (0.2264)	loss 0.7442 (1.1656)	grad_norm 0.3299 (0.3435)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:50:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][300/2502]	eta 0:07:37 lr 0.000005	 wd 0.0500	time 0.1710 (0.2078)	loss 1.3741 (1.1712)	grad_norm 0.3426 (0.3437)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:50:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][400/2502]	eta 0:06:56 lr 0.000005	 wd 0.0500	time 0.1681 (0.1984)	loss 0.7489 (1.1774)	grad_norm 0.3370 (0.3436)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:51:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][500/2502]	eta 0:06:26 lr 0.000005	 wd 0.0500	time 0.1692 (0.1931)	loss 1.1947 (1.1750)	grad_norm 0.3284 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:51:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][600/2502]	eta 0:06:00 lr 0.000005	 wd 0.0500	time 0.1576 (0.1896)	loss 0.8782 (1.1766)	grad_norm 0.3329 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:51:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][700/2502]	eta 0:05:36 lr 0.000005	 wd 0.0500	time 0.1573 (0.1865)	loss 1.3426 (1.1711)	grad_norm 0.3545 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:51:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][800/2502]	eta 0:05:13 lr 0.000004	 wd 0.0500	time 0.1589 (0.1844)	loss 0.9357 (1.1698)	grad_norm 0.3313 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:52:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][900/2502]	eta 0:04:52 lr 0.000004	 wd 0.0500	time 0.1608 (0.1828)	loss 1.1023 (1.1685)	grad_norm 0.3561 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:52:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1000/2502]	eta 0:04:32 lr 0.000004	 wd 0.0500	time 0.1606 (0.1816)	loss 1.2717 (1.1689)	grad_norm 0.3365 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:52:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1100/2502]	eta 0:04:13 lr 0.000004	 wd 0.0500	time 0.1626 (0.1806)	loss 0.8112 (1.1659)	grad_norm 0.3433 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:53:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1200/2502]	eta 0:03:53 lr 0.000004	 wd 0.0500	time 0.1805 (0.1797)	loss 1.3635 (1.1669)	grad_norm 0.3364 (inf)	loss_scale 1048576.0000 (1053814.5146)	mem 2327MB
[2024-08-01 00:53:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1300/2502]	eta 0:03:35 lr 0.000004	 wd 0.0500	time 0.1578 (0.1794)	loss 0.8522 (1.1606)	grad_norm 0.3229 (inf)	loss_scale 1048576.0000 (1053411.8616)	mem 2327MB
[2024-08-01 00:53:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1400/2502]	eta 0:03:16 lr 0.000004	 wd 0.0500	time 0.1716 (0.1786)	loss 1.0015 (1.1552)	grad_norm 0.3458 (inf)	loss_scale 1048576.0000 (1053066.6895)	mem 2327MB
[2024-08-01 00:53:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1500/2502]	eta 0:02:58 lr 0.000004	 wd 0.0500	time 0.1785 (0.1780)	loss 1.2004 (1.1548)	grad_norm 0.3231 (inf)	loss_scale 1048576.0000 (1052767.5097)	mem 2327MB
[2024-08-01 00:54:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1600/2502]	eta 0:02:40 lr 0.000004	 wd 0.0500	time 0.1615 (0.1777)	loss 1.5148 (1.1550)	grad_norm 0.3394 (inf)	loss_scale 1048576.0000 (1052505.7039)	mem 2327MB
[2024-08-01 00:54:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1700/2502]	eta 0:02:22 lr 0.000004	 wd 0.0500	time 0.1810 (0.1774)	loss 0.8953 (1.1531)	grad_norm 0.3510 (inf)	loss_scale 1048576.0000 (1052274.6808)	mem 2327MB
[2024-08-01 00:54:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1800/2502]	eta 0:02:04 lr 0.000004	 wd 0.0500	time 0.1499 (0.1771)	loss 1.3885 (1.1517)	grad_norm 0.3227 (inf)	loss_scale 1048576.0000 (1052069.3126)	mem 2327MB
[2024-08-01 00:55:05 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][1900/2502]	eta 0:01:46 lr 0.000004	 wd 0.0500	time 0.1651 (0.1769)	loss 0.9359 (1.1526)	grad_norm 0.3336 (inf)	loss_scale 1048576.0000 (1051885.5508)	mem 2327MB
[2024-08-01 00:55:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2000/2502]	eta 0:01:28 lr 0.000004	 wd 0.0500	time 0.1614 (0.1769)	loss 0.9225 (1.1537)	grad_norm 0.3365 (inf)	loss_scale 1048576.0000 (1051720.1559)	mem 2327MB
[2024-08-01 00:55:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2100/2502]	eta 0:01:10 lr 0.000004	 wd 0.0500	time 0.1700 (0.1766)	loss 1.3054 (1.1551)	grad_norm 0.3345 (inf)	loss_scale 1048576.0000 (1051570.5055)	mem 2327MB
[2024-08-01 00:55:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2200/2502]	eta 0:00:53 lr 0.000004	 wd 0.0500	time 0.1666 (0.1765)	loss 1.0834 (1.1549)	grad_norm 0.3521 (inf)	loss_scale 1048576.0000 (1051434.4534)	mem 2327MB
[2024-08-01 00:56:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2300/2502]	eta 0:00:35 lr 0.000004	 wd 0.0500	time 0.1758 (0.1764)	loss 1.1441 (1.1550)	grad_norm 0.3311 (inf)	loss_scale 1048576.0000 (1051310.2269)	mem 2327MB
[2024-08-01 00:56:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2400/2502]	eta 0:00:17 lr 0.000004	 wd 0.0500	time 0.1747 (0.1762)	loss 1.3463 (1.1560)	grad_norm 0.3605 (inf)	loss_scale 1048576.0000 (1051196.3482)	mem 2327MB
[2024-08-01 00:56:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [22/30][2500/2502]	eta 0:00:00 lr 0.000004	 wd 0.0500	time 0.1542 (0.1756)	loss 1.1062 (1.1557)	grad_norm 0.3343 (inf)	loss_scale 1048576.0000 (1051091.5762)	mem 2327MB
[2024-08-01 00:56:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 22 training takes 0:07:22
[2024-08-01 00:57:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.056 (11.056)	Loss 0.4861 (0.4861)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 2327MB
[2024-08-01 00:57:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.196 Acc@5 97.902
[2024-08-01 00:57:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 00:57:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 00:57:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][0/2502]	eta 6:52:51 lr 0.000004	 wd 0.0500	time 9.9006 (9.9006)	loss 0.7695 (0.7695)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:57:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][100/2502]	eta 0:11:19 lr 0.000004	 wd 0.0500	time 0.1605 (0.2829)	loss 1.4724 (1.1454)	grad_norm 0.3456 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:58:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][200/2502]	eta 0:08:40 lr 0.000004	 wd 0.0500	time 0.1642 (0.2262)	loss 0.8479 (1.1502)	grad_norm 0.3428 (0.3417)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:58:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][300/2502]	eta 0:07:35 lr 0.000004	 wd 0.0500	time 0.1664 (0.2069)	loss 1.4274 (1.1410)	grad_norm 0.3519 (0.3422)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:58:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][400/2502]	eta 0:06:55 lr 0.000004	 wd 0.0500	time 0.1580 (0.1975)	loss 0.8947 (1.1467)	grad_norm 0.3467 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:58:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][500/2502]	eta 0:06:23 lr 0.000004	 wd 0.0500	time 0.1591 (0.1914)	loss 0.8109 (1.1498)	grad_norm 0.3261 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:59:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][600/2502]	eta 0:05:57 lr 0.000004	 wd 0.0500	time 0.1690 (0.1879)	loss 1.4308 (1.1490)	grad_norm 0.3523 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:59:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][700/2502]	eta 0:05:33 lr 0.000004	 wd 0.0500	time 0.1595 (0.1853)	loss 1.1344 (1.1523)	grad_norm 0.3530 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 00:59:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][800/2502]	eta 0:05:12 lr 0.000003	 wd 0.0500	time 0.1640 (0.1834)	loss 1.4427 (1.1540)	grad_norm 0.3349 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:00:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][900/2502]	eta 0:04:51 lr 0.000003	 wd 0.0500	time 0.1572 (0.1820)	loss 1.1397 (1.1547)	grad_norm 0.3415 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:00:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1000/2502]	eta 0:04:31 lr 0.000003	 wd 0.0500	time 0.1660 (0.1809)	loss 1.1277 (1.1503)	grad_norm 0.3095 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:00:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1100/2502]	eta 0:04:12 lr 0.000003	 wd 0.0500	time 0.1579 (0.1800)	loss 1.0344 (1.1531)	grad_norm 0.3513 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:00:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1200/2502]	eta 0:03:53 lr 0.000003	 wd 0.0500	time 0.1608 (0.1792)	loss 1.3995 (1.1561)	grad_norm 0.3402 (0.3423)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:01:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1300/2502]	eta 0:03:34 lr 0.000003	 wd 0.0500	time 0.1601 (0.1786)	loss 1.2544 (1.1567)	grad_norm 0.3265 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:01:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1400/2502]	eta 0:03:16 lr 0.000003	 wd 0.0500	time 0.1701 (0.1781)	loss 1.2687 (1.1569)	grad_norm 0.3612 (0.3423)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:01:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1500/2502]	eta 0:02:58 lr 0.000003	 wd 0.0500	time 0.1737 (0.1776)	loss 1.2880 (1.1553)	grad_norm 0.3438 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:02:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1600/2502]	eta 0:02:39 lr 0.000003	 wd 0.0500	time 0.1683 (0.1772)	loss 1.1359 (1.1586)	grad_norm 0.3261 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:02:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1700/2502]	eta 0:02:21 lr 0.000003	 wd 0.0500	time 0.1973 (0.1769)	loss 0.7106 (1.1560)	grad_norm 0.3490 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:02:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1800/2502]	eta 0:02:04 lr 0.000003	 wd 0.0500	time 0.1839 (0.1767)	loss 1.3090 (1.1552)	grad_norm 0.3618 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:02:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][1900/2502]	eta 0:01:46 lr 0.000003	 wd 0.0500	time 0.1714 (0.1766)	loss 1.2954 (1.1546)	grad_norm 0.3330 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:03:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2000/2502]	eta 0:01:28 lr 0.000003	 wd 0.0500	time 0.1625 (0.1765)	loss 0.9288 (1.1542)	grad_norm 0.3525 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:03:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2100/2502]	eta 0:01:10 lr 0.000003	 wd 0.0500	time 0.1636 (0.1764)	loss 1.3922 (1.1546)	grad_norm 0.3394 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:03:50 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2200/2502]	eta 0:00:53 lr 0.000003	 wd 0.0500	time 0.1513 (0.1763)	loss 1.3181 (1.1531)	grad_norm 0.3601 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:04:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2300/2502]	eta 0:00:35 lr 0.000003	 wd 0.0500	time 0.1675 (0.1761)	loss 1.3683 (1.1532)	grad_norm 0.3297 (0.3423)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:04:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2400/2502]	eta 0:00:17 lr 0.000003	 wd 0.0500	time 0.1572 (0.1760)	loss 0.8144 (1.1531)	grad_norm 0.3404 (0.3423)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:04:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [23/30][2500/2502]	eta 0:00:00 lr 0.000003	 wd 0.0500	time 0.1544 (0.1754)	loss 0.8706 (1.1523)	grad_norm 0.3365 (0.3423)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:04:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 23 training takes 0:07:23
[2024-08-01 01:04:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.029 (12.029)	Loss 0.5020 (0.5020)	Acc@1 92.578 (92.578)	Acc@5 98.242 (98.242)	Mem 2327MB
[2024-08-01 01:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.158 Acc@5 97.878
[2024-08-01 01:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 01:05:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 01:05:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][0/2502]	eta 8:03:52 lr 0.000003	 wd 0.0500	time 11.6038 (11.6038)	loss 1.2197 (1.2197)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:05:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][100/2502]	eta 0:11:23 lr 0.000003	 wd 0.0500	time 0.1578 (0.2844)	loss 0.9391 (1.1590)	grad_norm 0.3282 (0.3434)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:06:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][200/2502]	eta 0:08:40 lr 0.000003	 wd 0.0500	time 0.1771 (0.2263)	loss 1.2273 (1.1486)	grad_norm 0.3284 (inf)	loss_scale 1048576.0000 (1132044.7363)	mem 2327MB
[2024-08-01 01:06:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][300/2502]	eta 0:07:35 lr 0.000003	 wd 0.0500	time 0.1608 (0.2069)	loss 1.3577 (1.1628)	grad_norm 0.3404 (inf)	loss_scale 1048576.0000 (1104314.2591)	mem 2327MB
[2024-08-01 01:06:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][400/2502]	eta 0:06:55 lr 0.000003	 wd 0.0500	time 0.1711 (0.1976)	loss 0.7212 (1.1624)	grad_norm 0.3478 (inf)	loss_scale 1048576.0000 (1090414.4439)	mem 2327MB
[2024-08-01 01:06:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][500/2502]	eta 0:06:24 lr 0.000003	 wd 0.0500	time 0.1735 (0.1922)	loss 1.4386 (1.1627)	grad_norm 0.3648 (inf)	loss_scale 1048576.0000 (1082063.4571)	mem 2327MB
[2024-08-01 01:07:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][600/2502]	eta 0:05:57 lr 0.000003	 wd 0.0500	time 0.1620 (0.1880)	loss 0.6807 (1.1579)	grad_norm 0.3335 (inf)	loss_scale 1048576.0000 (1076491.5008)	mem 2327MB
[2024-08-01 01:07:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][700/2502]	eta 0:05:33 lr 0.000003	 wd 0.0500	time 0.1619 (0.1852)	loss 1.1952 (1.1565)	grad_norm 0.3272 (inf)	loss_scale 1048576.0000 (1072509.2611)	mem 2327MB
[2024-08-01 01:07:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][800/2502]	eta 0:05:12 lr 0.000003	 wd 0.0500	time 0.1678 (0.1834)	loss 1.2121 (1.1586)	grad_norm 0.3516 (inf)	loss_scale 1048576.0000 (1069521.3383)	mem 2327MB
[2024-08-01 01:08:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][900/2502]	eta 0:04:51 lr 0.000003	 wd 0.0500	time 0.1633 (0.1822)	loss 0.9368 (1.1623)	grad_norm 0.3321 (inf)	loss_scale 1048576.0000 (1067196.6615)	mem 2327MB
[2024-08-01 01:08:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1000/2502]	eta 0:04:31 lr 0.000003	 wd 0.0500	time 0.1729 (0.1808)	loss 0.7988 (1.1594)	grad_norm 0.3495 (inf)	loss_scale 1048576.0000 (1065336.4555)	mem 2327MB
[2024-08-01 01:08:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1100/2502]	eta 0:04:12 lr 0.000003	 wd 0.0500	time 0.1626 (0.1797)	loss 0.8658 (1.1587)	grad_norm 0.3567 (inf)	loss_scale 1048576.0000 (1063814.1617)	mem 2327MB
[2024-08-01 01:08:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1200/2502]	eta 0:03:52 lr 0.000002	 wd 0.0500	time 0.1579 (0.1788)	loss 0.8916 (1.1567)	grad_norm 0.3428 (inf)	loss_scale 1048576.0000 (1062545.3722)	mem 2327MB
[2024-08-01 01:09:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1300/2502]	eta 0:03:33 lr 0.000002	 wd 0.0500	time 0.1631 (0.1780)	loss 0.7619 (1.1538)	grad_norm 0.3585 (inf)	loss_scale 1048576.0000 (1061471.6311)	mem 2327MB
[2024-08-01 01:09:24 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1400/2502]	eta 0:03:15 lr 0.000002	 wd 0.0500	time 0.1603 (0.1774)	loss 1.4548 (1.1549)	grad_norm 0.3353 (inf)	loss_scale 1048576.0000 (1060551.1720)	mem 2327MB
[2024-08-01 01:09:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1500/2502]	eta 0:02:57 lr 0.000002	 wd 0.0500	time 0.1601 (0.1770)	loss 1.2431 (1.1511)	grad_norm 0.3468 (inf)	loss_scale 1048576.0000 (1059753.3591)	mem 2327MB
[2024-08-01 01:09:59 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1600/2502]	eta 0:02:39 lr 0.000002	 wd 0.0500	time 0.1611 (0.1767)	loss 1.2926 (1.1500)	grad_norm 0.3528 (inf)	loss_scale 1048576.0000 (1059055.2105)	mem 2327MB
[2024-08-01 01:10:16 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1700/2502]	eta 0:02:21 lr 0.000002	 wd 0.0500	time 0.1925 (0.1765)	loss 1.5012 (1.1515)	grad_norm 0.3504 (inf)	loss_scale 1048576.0000 (1058439.1487)	mem 2327MB
[2024-08-01 01:10:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1800/2502]	eta 0:02:03 lr 0.000002	 wd 0.0500	time 0.1925 (0.1763)	loss 1.0871 (1.1514)	grad_norm 0.3438 (inf)	loss_scale 1048576.0000 (1057891.5003)	mem 2327MB
[2024-08-01 01:10:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][1900/2502]	eta 0:01:46 lr 0.000002	 wd 0.0500	time 0.1597 (0.1762)	loss 0.9192 (1.1527)	grad_norm 0.3306 (inf)	loss_scale 1048576.0000 (1057401.4687)	mem 2327MB
[2024-08-01 01:11:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2000/2502]	eta 0:01:28 lr 0.000002	 wd 0.0500	time 0.1581 (0.1760)	loss 1.2858 (1.1526)	grad_norm 0.3519 (inf)	loss_scale 1048576.0000 (1056960.4158)	mem 2327MB
[2024-08-01 01:11:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2100/2502]	eta 0:01:10 lr 0.000002	 wd 0.0500	time 0.2033 (0.1759)	loss 1.0063 (1.1529)	grad_norm 0.3474 (inf)	loss_scale 1048576.0000 (1056561.3479)	mem 2327MB
[2024-08-01 01:11:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2200/2502]	eta 0:00:53 lr 0.000002	 wd 0.0500	time 0.1648 (0.1756)	loss 1.0135 (1.1545)	grad_norm 0.3548 (inf)	loss_scale 1048576.0000 (1056198.5425)	mem 2327MB
[2024-08-01 01:12:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2300/2502]	eta 0:00:35 lr 0.000002	 wd 0.0500	time 0.1589 (0.1755)	loss 1.3802 (1.1569)	grad_norm 0.3702 (inf)	loss_scale 1048576.0000 (1055867.2716)	mem 2327MB
[2024-08-01 01:12:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2400/2502]	eta 0:00:17 lr 0.000002	 wd 0.0500	time 0.1760 (0.1753)	loss 1.2187 (1.1548)	grad_norm 0.3316 (inf)	loss_scale 1048576.0000 (1055563.5952)	mem 2327MB
[2024-08-01 01:12:33 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [24/30][2500/2502]	eta 0:00:00 lr 0.000002	 wd 0.0500	time 0.1547 (0.1748)	loss 0.9315 (1.1553)	grad_norm 0.3246 (inf)	loss_scale 1048576.0000 (1055284.2031)	mem 2327MB
[2024-08-01 01:12:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 24 training takes 0:07:21
[2024-08-01 01:12:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.820 (11.820)	Loss 0.4858 (0.4858)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 01:13:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.182 Acc@5 97.894
[2024-08-01 01:13:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 01:13:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 01:13:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][0/2502]	eta 7:30:38 lr 0.000002	 wd 0.0500	time 10.8067 (10.8067)	loss 1.2688 (1.2688)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:13:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][100/2502]	eta 0:11:12 lr 0.000002	 wd 0.0500	time 0.1710 (0.2799)	loss 1.1928 (1.2123)	grad_norm 0.3352 (0.3438)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:13:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][200/2502]	eta 0:08:34 lr 0.000002	 wd 0.0500	time 0.1623 (0.2236)	loss 1.4301 (1.1818)	grad_norm 0.3446 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:14:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][300/2502]	eta 0:07:30 lr 0.000002	 wd 0.0500	time 0.1613 (0.2047)	loss 1.1305 (1.1654)	grad_norm 0.3447 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:14:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][400/2502]	eta 0:06:50 lr 0.000002	 wd 0.0500	time 0.1579 (0.1954)	loss 1.2962 (1.1665)	grad_norm 0.3312 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:14:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][500/2502]	eta 0:06:21 lr 0.000002	 wd 0.0500	time 0.1570 (0.1904)	loss 1.4190 (1.1648)	grad_norm 0.3507 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:15:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][600/2502]	eta 0:05:54 lr 0.000002	 wd 0.0500	time 0.1746 (0.1865)	loss 0.8220 (1.1548)	grad_norm 0.3462 (0.3422)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:15:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][700/2502]	eta 0:05:31 lr 0.000002	 wd 0.0500	time 0.1548 (0.1839)	loss 0.8006 (1.1566)	grad_norm 0.3576 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:15:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][800/2502]	eta 0:05:09 lr 0.000002	 wd 0.0500	time 0.1697 (0.1821)	loss 1.3547 (1.1556)	grad_norm 0.3632 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:15:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][900/2502]	eta 0:04:49 lr 0.000002	 wd 0.0500	time 0.1665 (0.1804)	loss 1.4351 (1.1555)	grad_norm 0.3377 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:16:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1000/2502]	eta 0:04:29 lr 0.000002	 wd 0.0500	time 0.1584 (0.1792)	loss 1.5115 (1.1569)	grad_norm 0.3434 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:16:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1100/2502]	eta 0:04:10 lr 0.000002	 wd 0.0500	time 0.1782 (0.1784)	loss 1.5471 (1.1577)	grad_norm 0.3497 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:16:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1200/2502]	eta 0:03:51 lr 0.000002	 wd 0.0500	time 0.1572 (0.1776)	loss 0.9699 (1.1542)	grad_norm 0.3340 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:17:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1300/2502]	eta 0:03:32 lr 0.000002	 wd 0.0500	time 0.1564 (0.1772)	loss 0.8291 (1.1531)	grad_norm 0.3611 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:17:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1400/2502]	eta 0:03:14 lr 0.000002	 wd 0.0500	time 0.1789 (0.1768)	loss 0.8014 (1.1527)	grad_norm 0.3519 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:17:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1500/2502]	eta 0:02:56 lr 0.000002	 wd 0.0500	time 0.1579 (0.1763)	loss 0.8621 (1.1529)	grad_norm 0.3561 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:17:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1600/2502]	eta 0:02:38 lr 0.000002	 wd 0.0500	time 0.1780 (0.1759)	loss 0.8022 (1.1521)	grad_norm 0.3259 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:18:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1700/2502]	eta 0:02:20 lr 0.000002	 wd 0.0500	time 0.1619 (0.1758)	loss 0.7497 (1.1498)	grad_norm 0.3491 (inf)	loss_scale 1048576.0000 (1055973.3616)	mem 2327MB
[2024-08-01 01:18:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1800/2502]	eta 0:02:03 lr 0.000002	 wd 0.0500	time 0.1586 (0.1756)	loss 0.8767 (1.1517)	grad_norm 0.3447 (inf)	loss_scale 1048576.0000 (1055562.6252)	mem 2327MB
[2024-08-01 01:18:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][1900/2502]	eta 0:01:45 lr 0.000002	 wd 0.0500	time 0.1609 (0.1755)	loss 1.6446 (1.1541)	grad_norm 0.3378 (inf)	loss_scale 1048576.0000 (1055195.1015)	mem 2327MB
[2024-08-01 01:19:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2000/2502]	eta 0:01:28 lr 0.000002	 wd 0.0500	time 0.1652 (0.1753)	loss 0.9875 (1.1545)	grad_norm 0.3371 (inf)	loss_scale 1048576.0000 (1054864.3118)	mem 2327MB
[2024-08-01 01:19:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2100/2502]	eta 0:01:10 lr 0.000002	 wd 0.0500	time 0.1592 (0.1751)	loss 1.2796 (1.1557)	grad_norm 0.3343 (inf)	loss_scale 1048576.0000 (1054565.0109)	mem 2327MB
[2024-08-01 01:19:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2200/2502]	eta 0:00:52 lr 0.000001	 wd 0.0500	time 0.1616 (0.1750)	loss 1.1178 (1.1561)	grad_norm 0.3366 (inf)	loss_scale 1048576.0000 (1054292.9069)	mem 2327MB
[2024-08-01 01:19:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2300/2502]	eta 0:00:35 lr 0.000001	 wd 0.0500	time 0.1792 (0.1751)	loss 1.3416 (1.1555)	grad_norm 0.3388 (inf)	loss_scale 1048576.0000 (1054044.4537)	mem 2327MB
[2024-08-01 01:20:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2400/2502]	eta 0:00:17 lr 0.000001	 wd 0.0500	time 0.1809 (0.1751)	loss 1.3551 (1.1546)	grad_norm 0.3140 (inf)	loss_scale 1048576.0000 (1053816.6964)	mem 2327MB
[2024-08-01 01:20:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [25/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.1764 (0.1746)	loss 0.7392 (1.1534)	grad_norm 0.3739 (inf)	loss_scale 1048576.0000 (1053607.1523)	mem 2327MB
[2024-08-01 01:20:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 25 training takes 0:07:21
[2024-08-01 01:20:42 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.379 (11.379)	Loss 0.5083 (0.5083)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 01:21:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.128 Acc@5 97.898
[2024-08-01 01:21:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 01:21:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 01:21:13 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][0/2502]	eta 7:25:44 lr 0.000001	 wd 0.0500	time 10.6894 (10.6894)	loss 1.1601 (1.1601)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:21:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][100/2502]	eta 0:11:17 lr 0.000001	 wd 0.0500	time 0.1754 (0.2822)	loss 1.3059 (1.1667)	grad_norm 0.3331 (0.3444)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:21:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][200/2502]	eta 0:08:39 lr 0.000001	 wd 0.0500	time 0.1701 (0.2255)	loss 1.0825 (1.1659)	grad_norm 0.3522 (0.3432)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:22:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][300/2502]	eta 0:07:35 lr 0.000001	 wd 0.0500	time 0.1516 (0.2069)	loss 0.9109 (1.1480)	grad_norm 0.3363 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:22:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][400/2502]	eta 0:06:55 lr 0.000001	 wd 0.0500	time 0.1747 (0.1976)	loss 1.3419 (1.1463)	grad_norm 0.3568 (0.3422)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:22:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][500/2502]	eta 0:06:24 lr 0.000001	 wd 0.0500	time 0.1578 (0.1921)	loss 1.3172 (1.1553)	grad_norm 0.3355 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:22:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][600/2502]	eta 0:05:57 lr 0.000001	 wd 0.0500	time 0.1660 (0.1882)	loss 0.7857 (1.1592)	grad_norm 0.3505 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:23:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][700/2502]	eta 0:05:34 lr 0.000001	 wd 0.0500	time 0.1900 (0.1859)	loss 1.4075 (1.1551)	grad_norm 0.3239 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:23:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][800/2502]	eta 0:05:12 lr 0.000001	 wd 0.0500	time 0.1592 (0.1839)	loss 1.2418 (1.1590)	grad_norm 0.3588 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:23:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][900/2502]	eta 0:04:52 lr 0.000001	 wd 0.0500	time 0.1952 (0.1826)	loss 1.2759 (1.1582)	grad_norm 0.3351 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:24:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1000/2502]	eta 0:04:32 lr 0.000001	 wd 0.0500	time 0.1883 (0.1814)	loss 1.1239 (1.1571)	grad_norm 0.3346 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:24:21 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1100/2502]	eta 0:04:12 lr 0.000001	 wd 0.0500	time 0.1743 (0.1803)	loss 0.9312 (1.1571)	grad_norm 0.3357 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:24:38 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1200/2502]	eta 0:03:53 lr 0.000001	 wd 0.0500	time 0.1588 (0.1794)	loss 1.0796 (1.1587)	grad_norm 0.3451 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:24:55 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1300/2502]	eta 0:03:35 lr 0.000001	 wd 0.0500	time 0.1718 (0.1791)	loss 1.2742 (1.1602)	grad_norm 0.3559 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:25:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1400/2502]	eta 0:03:16 lr 0.000001	 wd 0.0500	time 0.1646 (0.1785)	loss 1.3787 (1.1610)	grad_norm 0.3403 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:25:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1500/2502]	eta 0:02:58 lr 0.000001	 wd 0.0500	time 0.1613 (0.1781)	loss 1.4018 (1.1627)	grad_norm 0.3349 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:25:47 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1600/2502]	eta 0:02:40 lr 0.000001	 wd 0.0500	time 0.1593 (0.1778)	loss 1.2071 (1.1602)	grad_norm 0.3385 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:26:04 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1700/2502]	eta 0:02:22 lr 0.000001	 wd 0.0500	time 0.1573 (0.1777)	loss 1.2905 (1.1583)	grad_norm 0.3372 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:26:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1800/2502]	eta 0:02:04 lr 0.000001	 wd 0.0500	time 0.1690 (0.1774)	loss 0.8759 (1.1583)	grad_norm 0.3569 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:26:39 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][1900/2502]	eta 0:01:46 lr 0.000001	 wd 0.0500	time 0.1584 (0.1772)	loss 1.2919 (1.1593)	grad_norm 0.3531 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:26:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2000/2502]	eta 0:01:28 lr 0.000001	 wd 0.0500	time 0.1708 (0.1771)	loss 1.1884 (1.1578)	grad_norm 0.3480 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:27:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2100/2502]	eta 0:01:11 lr 0.000001	 wd 0.0500	time 0.1604 (0.1769)	loss 1.3671 (1.1567)	grad_norm 0.3535 (0.3426)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:27:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2200/2502]	eta 0:00:53 lr 0.000001	 wd 0.0500	time 0.1776 (0.1767)	loss 1.2820 (1.1552)	grad_norm 0.3445 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:27:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2300/2502]	eta 0:00:35 lr 0.000001	 wd 0.0500	time 0.1691 (0.1765)	loss 1.2859 (1.1554)	grad_norm 0.3248 (0.3425)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:28:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2400/2502]	eta 0:00:17 lr 0.000001	 wd 0.0500	time 0.1658 (0.1764)	loss 0.7137 (1.1558)	grad_norm 0.3337 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:28:22 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [26/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.1535 (0.1758)	loss 0.9369 (1.1563)	grad_norm 0.3537 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:28:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 26 training takes 0:07:26
[2024-08-01 01:28:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.319 (12.319)	Loss 0.4912 (0.4912)	Acc@1 92.578 (92.578)	Acc@5 98.633 (98.633)	Mem 2327MB
[2024-08-01 01:29:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.146 Acc@5 97.870
[2024-08-01 01:29:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 01:29:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 01:29:12 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][0/2502]	eta 8:23:48 lr 0.000001	 wd 0.0500	time 12.0817 (12.0817)	loss 1.3956 (1.3956)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:29:29 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][100/2502]	eta 0:11:26 lr 0.000001	 wd 0.0500	time 0.1697 (0.2857)	loss 1.2749 (1.1469)	grad_norm 0.3183 (0.3415)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:29:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][200/2502]	eta 0:08:42 lr 0.000001	 wd 0.0500	time 0.1582 (0.2271)	loss 1.3698 (1.1593)	grad_norm 0.3487 (0.3420)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:30:03 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][300/2502]	eta 0:07:36 lr 0.000001	 wd 0.0500	time 0.1583 (0.2074)	loss 1.3387 (1.1588)	grad_norm 0.3380 (0.3418)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:30:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][400/2502]	eta 0:06:56 lr 0.000001	 wd 0.0500	time 0.1563 (0.1980)	loss 0.9949 (1.1551)	grad_norm 0.3470 (0.3416)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:30:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][500/2502]	eta 0:06:25 lr 0.000001	 wd 0.0500	time 0.2001 (0.1924)	loss 0.6721 (1.1562)	grad_norm 0.3431 (0.3415)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:30:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][600/2502]	eta 0:05:58 lr 0.000001	 wd 0.0500	time 0.1725 (0.1885)	loss 1.3215 (1.1561)	grad_norm 0.3541 (0.3416)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:31:10 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][700/2502]	eta 0:05:33 lr 0.000001	 wd 0.0500	time 0.1594 (0.1853)	loss 0.9801 (1.1578)	grad_norm 0.3480 (inf)	loss_scale 1048576.0000 (1075500.9187)	mem 2327MB
[2024-08-01 01:31:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][800/2502]	eta 0:05:12 lr 0.000001	 wd 0.0500	time 0.1802 (0.1834)	loss 0.9254 (1.1594)	grad_norm 0.3541 (inf)	loss_scale 1048576.0000 (1072139.5056)	mem 2327MB
[2024-08-01 01:31:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][900/2502]	eta 0:04:51 lr 0.000001	 wd 0.0500	time 0.1512 (0.1819)	loss 1.2306 (1.1574)	grad_norm 0.3354 (inf)	loss_scale 1048576.0000 (1069524.2442)	mem 2327MB
[2024-08-01 01:32:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1000/2502]	eta 0:04:31 lr 0.000001	 wd 0.0500	time 0.1497 (0.1806)	loss 1.1244 (1.1541)	grad_norm 0.3096 (inf)	loss_scale 1048576.0000 (1067431.5125)	mem 2327MB
[2024-08-01 01:32:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1100/2502]	eta 0:04:11 lr 0.000001	 wd 0.0500	time 0.1592 (0.1795)	loss 1.1312 (1.1563)	grad_norm 0.3363 (inf)	loss_scale 1048576.0000 (1065718.9319)	mem 2327MB
[2024-08-01 01:32:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1200/2502]	eta 0:03:52 lr 0.000001	 wd 0.0500	time 0.1607 (0.1787)	loss 0.7132 (1.1569)	grad_norm 0.3349 (inf)	loss_scale 1048576.0000 (1064291.5437)	mem 2327MB
[2024-08-01 01:32:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1300/2502]	eta 0:03:34 lr 0.000001	 wd 0.0500	time 0.1663 (0.1782)	loss 0.8038 (1.1537)	grad_norm 0.3474 (inf)	loss_scale 1048576.0000 (1063083.5849)	mem 2327MB
[2024-08-01 01:33:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1400/2502]	eta 0:03:15 lr 0.000001	 wd 0.0500	time 0.1712 (0.1775)	loss 0.8241 (1.1550)	grad_norm 0.3321 (inf)	loss_scale 1048576.0000 (1062048.0685)	mem 2327MB
[2024-08-01 01:33:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1500/2502]	eta 0:02:57 lr 0.000001	 wd 0.0500	time 0.1854 (0.1770)	loss 1.4415 (1.1560)	grad_norm 0.3263 (inf)	loss_scale 1048576.0000 (1061150.5290)	mem 2327MB
[2024-08-01 01:33:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1600/2502]	eta 0:02:39 lr 0.000001	 wd 0.0500	time 0.1591 (0.1766)	loss 1.3125 (1.1577)	grad_norm 0.3428 (inf)	loss_scale 1048576.0000 (1060365.1118)	mem 2327MB
[2024-08-01 01:34:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1700/2502]	eta 0:02:21 lr 0.000001	 wd 0.0500	time 0.1650 (0.1761)	loss 1.3506 (1.1594)	grad_norm 0.3502 (inf)	loss_scale 1048576.0000 (1059672.0423)	mem 2327MB
[2024-08-01 01:34:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1800/2502]	eta 0:02:03 lr 0.000001	 wd 0.0500	time 0.1617 (0.1760)	loss 1.3993 (1.1590)	grad_norm 0.3406 (inf)	loss_scale 1048576.0000 (1059055.9378)	mem 2327MB
[2024-08-01 01:34:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][1900/2502]	eta 0:01:45 lr 0.000001	 wd 0.0500	time 0.1776 (0.1758)	loss 1.3730 (1.1598)	grad_norm 0.3367 (inf)	loss_scale 1048576.0000 (1058504.6523)	mem 2327MB
[2024-08-01 01:34:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2000/2502]	eta 0:01:28 lr 0.000001	 wd 0.0500	time 0.1599 (0.1758)	loss 1.3952 (1.1606)	grad_norm 0.3308 (inf)	loss_scale 1048576.0000 (1058008.4678)	mem 2327MB
[2024-08-01 01:35:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2100/2502]	eta 0:01:10 lr 0.000001	 wd 0.0500	time 0.1621 (0.1756)	loss 1.4981 (1.1596)	grad_norm 0.3468 (inf)	loss_scale 1048576.0000 (1057559.5164)	mem 2327MB
[2024-08-01 01:35:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2200/2502]	eta 0:00:53 lr 0.000001	 wd 0.0500	time 0.1741 (0.1755)	loss 1.1843 (1.1609)	grad_norm 0.3439 (inf)	loss_scale 1048576.0000 (1057151.3603)	mem 2327MB
[2024-08-01 01:35:44 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2300/2502]	eta 0:00:35 lr 0.000001	 wd 0.0500	time 0.1776 (0.1754)	loss 1.2897 (1.1606)	grad_norm 0.3362 (inf)	loss_scale 1048576.0000 (1056778.6806)	mem 2327MB
[2024-08-01 01:36:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2400/2502]	eta 0:00:17 lr 0.000001	 wd 0.0500	time 0.1579 (0.1752)	loss 1.3485 (1.1623)	grad_norm 0.3609 (inf)	loss_scale 1048576.0000 (1056437.0446)	mem 2327MB
[2024-08-01 01:36:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [27/30][2500/2502]	eta 0:00:00 lr 0.000001	 wd 0.0500	time 0.1536 (0.1748)	loss 1.2533 (1.1614)	grad_norm 0.3503 (inf)	loss_scale 1048576.0000 (1056122.7285)	mem 2327MB
[2024-08-01 01:36:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 27 training takes 0:07:23
[2024-08-01 01:36:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.412 (11.412)	Loss 0.4790 (0.4790)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 01:36:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.142 Acc@5 97.890
[2024-08-01 01:36:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.1%
[2024-08-01 01:36:56 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 01:37:07 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][0/2502]	eta 7:38:11 lr 0.000001	 wd 0.0500	time 10.9880 (10.9880)	loss 0.9642 (0.9642)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:37:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][100/2502]	eta 0:11:24 lr 0.000000	 wd 0.0500	time 0.1948 (0.2850)	loss 0.9032 (1.1971)	grad_norm 0.3366 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:37:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][200/2502]	eta 0:08:41 lr 0.000000	 wd 0.0500	time 0.1575 (0.2264)	loss 1.5411 (1.1835)	grad_norm 0.3342 (0.3424)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:37:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][300/2502]	eta 0:07:36 lr 0.000000	 wd 0.0500	time 0.1670 (0.2072)	loss 1.4329 (1.1775)	grad_norm 0.3544 (0.3433)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:38:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][400/2502]	eta 0:06:54 lr 0.000000	 wd 0.0500	time 0.1591 (0.1972)	loss 0.7310 (1.1689)	grad_norm 0.3334 (0.3432)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:38:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][500/2502]	eta 0:06:24 lr 0.000000	 wd 0.0500	time 0.1681 (0.1918)	loss 0.7908 (1.1737)	grad_norm 0.3325 (0.3433)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:38:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][600/2502]	eta 0:05:57 lr 0.000000	 wd 0.0500	time 0.1683 (0.1882)	loss 0.8826 (1.1663)	grad_norm 0.3588 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:39:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][700/2502]	eta 0:05:34 lr 0.000000	 wd 0.0500	time 0.1748 (0.1855)	loss 1.2796 (1.1632)	grad_norm 0.3564 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:39:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][800/2502]	eta 0:05:12 lr 0.000000	 wd 0.0500	time 0.1590 (0.1837)	loss 0.8310 (1.1637)	grad_norm 0.3368 (0.3427)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:39:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][900/2502]	eta 0:04:52 lr 0.000000	 wd 0.0500	time 0.1760 (0.1824)	loss 0.8518 (1.1601)	grad_norm 0.3537 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:39:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1000/2502]	eta 0:04:32 lr 0.000000	 wd 0.0500	time 0.1702 (0.1811)	loss 1.0035 (1.1617)	grad_norm 0.3362 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:40:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1100/2502]	eta 0:04:12 lr 0.000000	 wd 0.0500	time 0.1635 (0.1802)	loss 1.2573 (1.1618)	grad_norm 0.3512 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:40:31 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1200/2502]	eta 0:03:53 lr 0.000000	 wd 0.0500	time 0.1636 (0.1793)	loss 1.4254 (1.1619)	grad_norm 0.3605 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:40:48 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1300/2502]	eta 0:03:34 lr 0.000000	 wd 0.0500	time 0.1600 (0.1787)	loss 1.5015 (1.1631)	grad_norm 0.3486 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:41:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1400/2502]	eta 0:03:16 lr 0.000000	 wd 0.0500	time 0.1558 (0.1782)	loss 0.9513 (1.1625)	grad_norm 0.3386 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:41:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1500/2502]	eta 0:02:57 lr 0.000000	 wd 0.0500	time 0.1835 (0.1776)	loss 1.1841 (1.1610)	grad_norm 0.3416 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:41:40 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1600/2502]	eta 0:02:39 lr 0.000000	 wd 0.0500	time 0.1760 (0.1773)	loss 1.3890 (1.1637)	grad_norm 0.3675 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:41:57 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1700/2502]	eta 0:02:21 lr 0.000000	 wd 0.0500	time 0.1670 (0.1770)	loss 1.2346 (1.1632)	grad_norm 0.3533 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:42:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1800/2502]	eta 0:02:04 lr 0.000000	 wd 0.0500	time 0.1752 (0.1768)	loss 0.7907 (1.1627)	grad_norm 0.3427 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:42:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][1900/2502]	eta 0:01:46 lr 0.000000	 wd 0.0500	time 0.1642 (0.1767)	loss 1.0989 (1.1604)	grad_norm 0.3407 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:42:49 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2000/2502]	eta 0:01:28 lr 0.000000	 wd 0.0500	time 0.1628 (0.1764)	loss 1.1081 (1.1605)	grad_norm 0.3456 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:43:06 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2100/2502]	eta 0:01:10 lr 0.000000	 wd 0.0500	time 0.1826 (0.1762)	loss 0.9739 (1.1604)	grad_norm 0.3512 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:43:23 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2200/2502]	eta 0:00:53 lr 0.000000	 wd 0.0500	time 0.1602 (0.1761)	loss 1.2691 (1.1594)	grad_norm 0.3477 (inf)	loss_scale 1048576.0000 (1049528.8178)	mem 2327MB
[2024-08-01 01:43:41 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2300/2502]	eta 0:00:35 lr 0.000000	 wd 0.0500	time 0.1613 (0.1759)	loss 1.2327 (1.1594)	grad_norm 0.3450 (inf)	loss_scale 1048576.0000 (1049487.4090)	mem 2327MB
[2024-08-01 01:43:58 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2400/2502]	eta 0:00:17 lr 0.000000	 wd 0.0500	time 0.1608 (0.1758)	loss 1.5027 (1.1580)	grad_norm 0.3291 (inf)	loss_scale 1048576.0000 (1049449.4494)	mem 2327MB
[2024-08-01 01:44:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [28/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1536 (0.1752)	loss 1.0761 (1.1573)	grad_norm 0.3280 (inf)	loss_scale 1048576.0000 (1049414.5254)	mem 2327MB
[2024-08-01 01:44:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 28 training takes 0:07:24
[2024-08-01 01:44:32 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 12.215 (12.215)	Loss 0.4866 (0.4866)	Acc@1 92.383 (92.383)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 01:44:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.150 Acc@5 97.890
[2024-08-01 01:44:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 01:44:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 01:45:02 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][0/2502]	eta 7:01:23 lr 0.000000	 wd 0.0500	time 10.1054 (10.1054)	loss 1.2383 (1.2383)	grad_norm 0.0000 (0.0000)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:45:20 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][100/2502]	eta 0:11:19 lr 0.000000	 wd 0.0500	time 0.1793 (0.2829)	loss 1.4069 (1.1687)	grad_norm 0.3476 (0.3445)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:45:37 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][200/2502]	eta 0:08:40 lr 0.000000	 wd 0.0500	time 0.1599 (0.2260)	loss 0.8154 (1.1668)	grad_norm 0.3638 (0.3434)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:45:54 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][300/2502]	eta 0:07:35 lr 0.000000	 wd 0.0500	time 0.1710 (0.2070)	loss 0.8720 (1.1636)	grad_norm 0.3544 (0.3433)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:46:11 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][400/2502]	eta 0:06:55 lr 0.000000	 wd 0.0500	time 0.1556 (0.1977)	loss 1.1209 (1.1572)	grad_norm 0.3458 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:46:28 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][500/2502]	eta 0:06:23 lr 0.000000	 wd 0.0500	time 0.1623 (0.1917)	loss 1.2979 (1.1528)	grad_norm 0.3538 (0.3433)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:46:45 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][600/2502]	eta 0:05:58 lr 0.000000	 wd 0.0500	time 0.1889 (0.1883)	loss 0.7241 (1.1559)	grad_norm 0.3465 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:47:01 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][700/2502]	eta 0:05:34 lr 0.000000	 wd 0.0500	time 0.1566 (0.1854)	loss 1.1212 (1.1567)	grad_norm 0.3380 (0.3428)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:47:18 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][800/2502]	eta 0:05:11 lr 0.000000	 wd 0.0500	time 0.1868 (0.1832)	loss 0.7598 (1.1575)	grad_norm 0.3323 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:47:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][900/2502]	eta 0:04:50 lr 0.000000	 wd 0.0500	time 0.1610 (0.1816)	loss 1.4912 (1.1589)	grad_norm 0.3495 (0.3432)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:47:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1000/2502]	eta 0:04:31 lr 0.000000	 wd 0.0500	time 0.1785 (0.1806)	loss 1.4473 (1.1596)	grad_norm 0.3425 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:48:09 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1100/2502]	eta 0:04:11 lr 0.000000	 wd 0.0500	time 0.1576 (0.1795)	loss 1.2360 (1.1595)	grad_norm 0.3353 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:48:26 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1200/2502]	eta 0:03:52 lr 0.000000	 wd 0.0500	time 0.1745 (0.1785)	loss 1.0252 (1.1572)	grad_norm 0.3264 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:48:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1300/2502]	eta 0:03:33 lr 0.000000	 wd 0.0500	time 0.1639 (0.1780)	loss 1.1275 (1.1606)	grad_norm 0.3626 (0.3431)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:49:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1400/2502]	eta 0:03:15 lr 0.000000	 wd 0.0500	time 0.1757 (0.1773)	loss 1.1179 (1.1557)	grad_norm 0.3546 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:49:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1500/2502]	eta 0:02:57 lr 0.000000	 wd 0.0500	time 0.1572 (0.1768)	loss 1.0228 (1.1559)	grad_norm 0.3425 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:49:34 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1600/2502]	eta 0:02:39 lr 0.000000	 wd 0.0500	time 0.1778 (0.1764)	loss 0.8187 (1.1580)	grad_norm 0.3370 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:49:51 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1700/2502]	eta 0:02:21 lr 0.000000	 wd 0.0500	time 0.1662 (0.1761)	loss 1.3526 (1.1606)	grad_norm 0.3468 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:50:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1800/2502]	eta 0:02:03 lr 0.000000	 wd 0.0500	time 0.1632 (0.1758)	loss 1.1705 (1.1590)	grad_norm 0.3210 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:50:25 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][1900/2502]	eta 0:01:45 lr 0.000000	 wd 0.0500	time 0.1749 (0.1757)	loss 0.7537 (1.1593)	grad_norm 0.3550 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:50:43 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2000/2502]	eta 0:01:28 lr 0.000000	 wd 0.0500	time 0.1574 (0.1756)	loss 0.9067 (1.1611)	grad_norm 0.3513 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:51:00 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2100/2502]	eta 0:01:10 lr 0.000000	 wd 0.0500	time 0.1541 (0.1754)	loss 0.7580 (1.1620)	grad_norm 0.3152 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:51:17 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2200/2502]	eta 0:00:52 lr 0.000000	 wd 0.0500	time 0.1571 (0.1753)	loss 0.8296 (1.1626)	grad_norm 0.3506 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:51:35 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2300/2502]	eta 0:00:35 lr 0.000000	 wd 0.0500	time 0.1693 (0.1752)	loss 0.8525 (1.1597)	grad_norm 0.3532 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:51:52 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2400/2502]	eta 0:00:17 lr 0.000000	 wd 0.0500	time 0.1991 (0.1751)	loss 1.2572 (1.1597)	grad_norm 0.3448 (0.3430)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:52:08 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 240): INFO Train: [29/30][2500/2502]	eta 0:00:00 lr 0.000000	 wd 0.0500	time 0.1534 (0.1747)	loss 1.3175 (1.1590)	grad_norm 0.3309 (0.3429)	loss_scale 1048576.0000 (1048576.0000)	mem 2327MB
[2024-08-01 01:52:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 249): INFO EPOCH 29 training takes 0:07:22
[2024-08-01 01:52:14 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 145): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_29.pth saving......
[2024-08-01 01:52:15 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (utils.py 147): INFO pretrain/diffusion_ft/adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3/diffusion_ft_adapter_smt_l_step_stage3/ckpt_epoch_29.pth saved !!!
[2024-08-01 01:52:27 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 289): INFO Test: [0/98]	Time 11.418 (11.418)	Loss 0.4763 (0.4763)	Acc@1 92.578 (92.578)	Acc@5 98.438 (98.438)	Mem 2327MB
[2024-08-01 01:52:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 296): INFO  * Acc@1 86.158 Acc@5 97.918
[2024-08-01 01:52:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 180): INFO Accuracy of the network on the 50000 test images: 86.2%
[2024-08-01 01:52:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 182): INFO Max accuracy: 86.20%
[2024-08-01 01:52:46 adapter_smt_diffusion_finetune_large_224_22kto1k_step_stage3] (main.py 189): INFO Training time 3:55:21
